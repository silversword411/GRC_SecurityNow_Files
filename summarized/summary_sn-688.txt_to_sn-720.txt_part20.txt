GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#688

DATE:		November 6, 2018

TITLE:		PortSmash

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-688.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we discuss the new "BleedingBit" Bluetooth flaws, JavaScript no longer being optional with Google, a new Microsoft Edge browser zero-day, Windows Defender playing in its own sandbox, Microsoft and Sysinternals news, the further evolution of the CAPTCHA, the 30th anniversary of the Internet's first worm, a bizarre requirement of ransomware, a nice new bit of security non-tech from Apple, some closing-the-loop feedback from our listeners, then a look at the impact and implication of the new "PortSmash" attack against Intel (and almost certainly other) processors.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We're going to talk about PortSmash, a new exploit on Intel's Hyper-Threading architecture.  We've also got a big 30th anniversary to celebrate, the first Internet worm, and some kudos and pats on the back for both Microsoft and Apple.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 688, recorded Tuesday, November 6th, 2018:  PortSmash.



It's time for Security Now!, the show where we cover your security, of course, your privacy, and even explain a little bit about how computers work with our Explainer in Chief, Mr. Steven Gibson.  Hello, Steve.



STEVE GIBSON:  Hello, my friend.  Great to be with you again for this podcast on Tuesday, November 6th, the U.S.'s Midterm Election Day.



LEO:  Yes, yes.



STEVE:  If I were a popcorn eater, I would be sitting down on the couch all evening, watching this all unfold.  Lorrie just thinks I'm nuts.  She says, "Well, I'm going to be working on my Ph.D. dissertation."



LEO:  Good for her.



STEVE:  "You can watch those idiots talking amongst themselves."



LEO:  I watch, even though we won't probably know much till, you know, these things take a long time, especially here in California.



STEVE:  And I was explaining to her, it's like sports, where you don't only watch whatever that last big game is called.



LEO:  You watch all nine innings.



STEVE:  You use that - no, no, no.  You watch all season.



LEO:  All 162 games.



STEVE:  And you like following along and - yeah.  So I'm not, obviously, I'm not a sports spectator.  But I do enjoy watching this human drama.



LEO:  It is a spectator sport.



STEVE:  And I won't be upset.  I don't get upset.  This is just sort of interesting to me, to see how this is all happening.



LEO:  Well, you're in one of the key counties, actually.  You're in Orange County, where Dana Rohrabacher is defending his seat against an upstart Democrat.



STEVE:  Yeah, and he's been, I think, like 30 years?  I mean, the name is just so well-known.



LEO:  Yeah, forever, forever, yeah.



STEVE:  So that'll be interesting to see.



LEO:  But it's a toss-up, even in Orange County. 



STEVE:  Speaking of toss-ups, we have a bunch of stuff to talk about.  I don't know how that relates to toss-ups at all.  But the title is "PortSmash," which we'll be getting to at the end of the podcast, as we often do our title topics.  Believe it or not, Leo, we've got another problem with not only Intel, but pretty much all microprocessors, certainly the AMD from Ryzen on.  Not Spectre; not Meltdown.  This is another one.  So we're going to talk about that and what it means and the consequences of it, and why I'm seeing the industry again kind of overreact to this, although from an academic standpoint it's certainly interesting.  We also have the new - and you've got to have a good name for these, and you've got to have a good logo.  And these guys nailed it:  The BleedingBit Bluetooth flaw, if you will.



LEO:  Oh, lord.



STEVE:  Yes, your bits are bleeding.



LEO:  Ohhh.



STEVE:  We've got JavaScript no longer being optional with Google, and that also made our Picture of the Week that we'll come back to and talk about.  A new Microsoft Edge browser zero-day.  Windows Defender playing in its own sandbox.  And I heard you talking about this over the last few days, and so I wanted to come back to it and sort of do our take on that.  Microsoft and Sysinternals, strangely enough, have some news.  There's also the further evolution of the CAPTCHA, which you and I talked about briefly last Tuesday, but the news dropped on Monday when I was off in reunion mode, and so I didn't have a chance to do a little bit of research.  So I have some follow-up on that.  We've also, actually it was Friday, November 2nd, the 30th anniversary of the Internet's first worm.



LEO:  Oh, wow.  The Morris Worm.



STEVE:  The Morris Worm, 30 years ago last Friday.  Somebody who was there, Steven Vaughan-Nichols, who's now with ZDNet, or writes for them, recounted the experience.  And so I thought it would be fun to do a little blast from the past and talk about the first-ever such incident, and how quaint it was that there were 60,000 servers on the Internet at the time.



LEO:  Wow.



STEVE:  That was in 1988.  We also have a bizarre requirement from a new ransomware which Lawrence Abrams at BleepingComputer covered.  A nice new bit of non-tech security from Apple, which I appreciated.  We've got some closing-the-loop feedback from our listeners, and then we'll take a look at the impact and implication of this new PortSmash attack against Intel and almost certainly other processors.  So once again, I think an interesting podcast for our listeners.



LEO:  Well, fantastic.



STEVE:  So our Picture of the Week.



LEO:  I like it.



STEVE:  It just made me chuckle because of course all of our long-term listeners will remember the days of NoScript.



LEO:  Yeah.



STEVE:  Where we were deliberately disabling JavaScript because it was like, eh, you know, it's there, but you don't really need it, and it could get up to some mischief.  And it's just better, unless you know you need it, turn it off.  And of course several years ago even I - even I, Leo - gave up.



LEO:  I was shocked when you told me that, Steve.



STEVE:  And I was like, okay, I just, you know, because nothing was working.



LEO:  Can't use the 'Net, yeah.



STEVE:  Yeah.  Nothing was working.  So we switched over to uBlock Origin, and we're all happy again, and we've got some control.



LEO:  Yes.



STEVE:  Anyway, I got a kick out of this because the Picture of the Week is from Google, and so it's got the Google logo.  And then it says:  "Google could not sign you in."  And then in a little callout box it says:  "The browser you're using doesn't support JavaScript, or has JavaScript turned off.  To keep your Google Account secure, try signing in on a browser that has JavaScript turned on."  And so it's like, okay, well, there's sort of the final blow is, well, it turns out with this recent change you are no longer able to sign into Google stuff, Google accounts, your Google account, unless your browser supports JavaScript.  It's no longer optional.



LEO:  Why do you think that is?  Is it a CAPTCHA they're using, or something else?



STEVE:  Well, that's - I don't know why it's our second story because I should have led with it since it was the Picture of the Week.  But it's the second thing we're going to talk about.



LEO:  Oh, good, okay.



STEVE:  After we talk in more detail; after we talk about the BleedingBit Bluetooth flaws.  And what I loved about this is that what these guys found - and this is Armis Security found this.  It may have some impact on our listeners.  It's enterprise-class WiFi that also has Bluetooth capabilities.  But when Armis went to TI, which is the maker of these Bluetooth chips where they found the problem, TI said, yeah, uh-huh.  Yeah, we know about that.  And Armis said, uh, what?  And TI said, yeah, it's a bug, and we're going to get around to that. And Armis said no, we could take over any router where this chip is.  And TI said what?



So here again, this is like the difference between the engineers and the security researchers, where the engineers go, "Oh, yeah, it's not good."  And the security researcher says, "No, we've done a multipacket stack overflow with remote code execution from your little bug."  And the engineers are like, "Oh, yeah, that's not good."



So BleedingBit.  It's got everything.  It's got the catchy name, and it's got the nice drippy bleeding logo, so we're off to a good start.  Armis wrote in their explanation of this, they said:  "Armis has identified two chip-level vulnerabilities impacting access points and potentially other unmanaged devices."  They said:  "Dubbed 'BleedingBit,' they are two critical vulnerabilities related to the use of BLE" - which we know is Bluetooth Low Energy - "chips made by Texas Instruments.  The chips are embedded in, among other devices, certain access points that deliver WiFi to enterprise networks manufactured by Cisco, Meraki," I guess is how you pronounce it...



LEO:  Meraki.



STEVE:  "...Meraki and Aruba.  These are the leaders," they write, "in networking, accounting for nearly 70% of the enterprise market."  So first off, most of the listeners of the podcast, I know that we have a lot of enterprise listeners, so pay attention if you are using WiFi access points from those three makers.  And I didn't go into a lengthy enumeration of which models because it's extensive.  Some are and some aren't.  But if you're using these routers, you're going to want to find out and basically update.



Armis said:  "These proximity-based vulnerabilities allow an unauthenticated attacker to break into enterprise networks undetected.  Once an attacker takes control over an access point, he can move laterally between network segments to create a bridge between them, effectively breaking network segmentation."  And of course we know that segmentation is a powerful tool for security.  For example, this breaks through all VLAN segmentation.  And if there are guest WiFi networks in an enterprise, the presumption is those guest networks have no access to the internal, like the IT network, and this breaks that.



Anyway, so Armis wrote:  "Armis has reported the issues to TI [Texas Instruments] and the affected vendors above.  We are also working with additional vendors of various connected devices to ascertain whether they, too, are affected by the BleedingBit vulnerabilities."  So, okay.  So what I loved about this is that, well, the description of what they found, as I was saying to you earlier.  They wrote, under "Disclosure," they said:  "Armis contacted Texas Instruments on June 20, 2018."  So some months ago.



They said:  "Through our discussions, it was discovered that TI was familiar with the bug causing the vulnerability, and issued a fix in their Bluetooth Low Energy Stack v2.2.2.  However, Armis identified it as a security issue.  Once notified, the companies worked together to issue the appropriate updates to the patch and coordinate the announcements.  Cisco was notified on July 24, 2018."



Okay.  So TI knew there was a problem, but apparently they weren't concerned about it.  Okay.  So get a load of how capable security researchers are able to turn a bug into a truly significant vulnerability.  So there's two, as I mentioned above.  The first one is CVE-2018-16986.  And again, our CVE numbers are now in the five digits.



So the first BleedingBit RCE (Remote Code Execution) vulnerability, they wrote:  "....resides in a TI chip embedded in many devices.  Our research focused on access points.  The vulnerability can be exploited by an attacker in the vicinity of the affected device, so naturally within Bluetooth range, provided its BLE is turned on" - so obviously you need the Bluetooth radio active - "without any other prerequisites or knowledge about the device.  First, the attacker sends multiple benign BLE broadcast messages called 'advertising packets,' which will be stored on the memory of the vulnerable BLE chip in the targeted device.  While the packets are not harmful, they contain code that will be invoked by the attacker later on.  This activity will be undetected by traditional security solutions.



"Next, the attacker sends the overflow packet, which is a standard advertising packet with a subtle alteration, a specific bit in its header turned on instead of off.  This bit causes the chip to allocate the information from the packet a much larger space than it really needs, triggering an overflow of critical memory in the process.  The leaked memory contains function pointers, memory that points to specific code segments which the attacker can leverage to point to the code they sent to the vulnerable chip in the previous stage of the attack."



They wrote:  "At this point, the attacker can run malicious code on the targeted device and install a backdoor on the vulnerable chip, which will await further commands transmitted over Bluetooth.  The attacker can also change the behavior of the Bluetooth chip and attack the main processor of the device, gaining full control over it.  In the case of an access point, once the attacker gains control, he can reach all networks served by it, regardless of any network segmentation.  Furthermore, the attacker can use the device in his control to spread laterally to any other device in its vicinity, launching a truly airborne attack."



So anyway, I just loved the fact that clearly they reverse-engineered this.  They first realized that there was some mishandling of some bits in the header of the advertising packets, which resulted in a misallocation of memory.  And apparently TI knew that, too.  But what these guys realized and implemented was that they saw that, by preloading the memory with additional advertising packets that did not have this bit turned on, they would get queued up in a buffer so that they could load much more code than they would otherwise have been able to send in a single advertising packet; thus, using the bug, be able to access all of the previously uploaded packets en masse, all at once.  So just beautiful piece of security engineering, taking a flaw, even one which the designer said, yeah, we're going to fix that, and said, let's - you need to think about this a little more critically, like fixing it now rather than later.



And the second problem is just kind of like you put your head in your hands.  This is one of those where, as I have often said, anybody can make a mistake.  The only thing we require is that mistakes are handled in as responsible a fashion as possible.  Separate from those are the policy problems or decisions which it's kind of hard to forgive those.  This one is the BleedingBit OAD remote code execution vulnerability.  OAD, believe it or not, stands for Over the Air Firmware Download.



They wrote:  "The second BleedingBit vulnerability was specific to the Aruba Access Point Series 300," so not the Cisco and the Meraki devices.  Anyway, so they said:  "This issue is technically a backdoor in Bluetooth Low Energy chips that was designed to allow firmware updates.  The OAD feature is often used as a development tool, but is active in some production access points."  In other words, this was a development backdoor that TI designed into the chip and never intended it to be left on in production.



They wrote:  "It can allow a nearby attacker to access and install a completely new and different version of the firmware, effectively rewriting the operating system of the Bluetooth Low Energy chip, if not implemented correctly by the manufacturer."  They said:  "By default, the OAD feature is not automatically configured to address secure firmware updates.  It allows a simple update mechanism of the firmware running on the BLE chip over a GATT," a G-A-T-T, which is a generic attributes transaction.  "In the case of Aruba's access points, a hardcoded password was added that is identical across all Aruba access points that support Bluetooth Low Energy to prevent the OAD feature from being easily abused by attackers.



"However, an attacker who acquired the password by sniffing a legitimate update or by reverse-engineering Aruba's Bluetooth Low Energy firmware can connect to the Bluetooth Low Energy chip on a vulnerable access point and upload a malicious firmware containing the attacker's own code, effectively allowing a complete rewrite of its operating system, thereby gaining full control over it.  From this point, the malicious potential is identical to that achieved by the first vulnerability."



So anyway, we know that reverse-engineering firmware is just not a high bar any longer.  You stick a debugging probe onto the chip.  You read out the firmware.  You drop it into any of a number of reverse decompiling solutions.  You begin to parse it.  You find the code that stores the password and checks the password, and you're able to get in.  And these guys did just that.  So anyway, for remediation of this problem, TI has provided updates to their OEM partners.  So if you're in an enterprise, if you are using WiFi access points, the first thing you should do, first of all, in every case this Bluetooth can be turned off.  You only need it on for maintenance operations.  So it should have never been left on.  Unfortunately, it's on by default.



So anybody with these should turn them off and/or also at the same time update the firmware to what's the newest firmware available from your vendors, and you should be okay.  So anyway, just a classic, beautifully engineered piece of work on, as they said, those devices are 70% of the market in the enterprise WiFi access point world.  So there will certainly be a lot of companies that want to make sure they're updating their systems to the latest firmware.



And following on from our Picture of the Week about JavaScript no longer being optional with Google, last Wednesday, on Halloween, Google's project manager Jonathan Skelker began his blog by writing:  "It's Halloween, and the last day of Cybersecurity Awareness Month," he wrote, "so we're celebrating these occasions with security improvements across your account journey - before you sign in, as soon as you've entered your account, when you share information with other apps and sites, and the rare event in which your account is compromised."



He says:  "When your username and password are entered on Google's sign-in page, we'll run a risk assessment" - and actually this is feeling very similar to the reCAPTCHA v3 story that we're going to also cover in more depth here in a minute.  He says:  "We'll run a risk assessment and only allow the sign-in if nothing looks suspicious."



LEO:  Hmm.



STEVE:  He said - yeah.  "We're always working to improve this analysis, and we'll now require that JavaScript is enabled on the Google sign-in page, without which we cannot run this assessment."  He said:  "Chances are JavaScript is already enabled in your browser."  He says:  "It helps power lots of the websites people use every day.  But because it may save bandwidth or help pages load more quickly, a tiny minority of users" - and he cites 0.1%, so one in a thousand - "may choose to keep it off.  This might make sense if you are reading static content, but we recommend that you keep Javascript on while signing into your Google" - and actually he says "recommend," but, you know, yeah, because you can't sign in without it - "so we can better protect you.  You can read more about how to enable JavaScript here."  And then he gives a link to various browsers, how to turn it on.



So there was some other stuff, too, in his posting about keeping your Google account secure while you're signed in.  He said:  "Last year we launched a major update to the Security Checkup that upgraded it from the same checklist for everyone to a smarter tool that automatically provides personalized guidance for improving security of your Google Account.  We're adding to this advice all the time, and we recently introduced better protection against harmful apps based on recommendations from Google Play Protect, as well as the ability to remove your account from any devices you no longer use."



So, okay.  So in other words, Google Play Protect is now providing feedback to typically a mobile device's security checkup and will warn the user when they have anything installed that Google no longer feels is behaving honorably.  However, in some of the coverage and feedback, as I was digging into this a little bit, I saw some people saying, well, it'd be nice if people were notified if they've got what essentially amounts to malware already installed, rather than requiring people to go there deliberately.  But I would say, at this point, since this has now been added, anybody with an Android device, it's worth doing the security checkup.  And I took my own advice because I was curious.  I wanted to see what they had done.



And so it's myaccount.google.com/security-checkup will get you there.  And what I found was - it was interesting.  It identified three issues, it said, "with your devices," with mine.  And what it found was three devices that I had not used in an inordinately long time which still had Google account stuff on them.  And so it was, I thought, very usefully saying, hey, you know, if you're not using your Google account with these devices, take it off.  So there were four categories.  There was "Your devices," and it sort of enumerated them.  Then there was "Recent security events," and it said "No events in 28 days."  "2-Step Verification," and it confirmed 2-step verification is on.  And then "Third-party access."



And it turns out there were two things, one which I no longer needed, and in fact I wasn't sure, you know, academia.edu.  It's like, okay, why does that have access to my Google account?  I don't know.  So now it doesn't.  And now only my iAnnotate, as I mentioned, is my very favorite PDF reader.  It has access to my Google cloud for storing and managing PDFs there.



So anyway, I would encourage our listeners to just run, if you're Google property users, take this little security checkup.  It's quick; and if you're not surprised, that's wonderful.  But there may be some things you want to just curate.  As we've said, part of being responsible is occasional curation of the stuff that just sort of tends to accumulate over time because apps don't tend to remove themselves.  They're not, oh, I haven't been using my - the app says to itself, haven't used my access to your Google account for a while, so I'm going to remove myself.  No.  They don't do that.  So it's up to us.  So worth doing.



Oh, and also, the last thing that he said I was curious about.  So I dug into it a little bit more.  And that's this "Helping you get back to the beginning if you run into trouble."  And I thought, what?  So they explain:  "In the rare event that your account is compromised, our priority is to get you back to safety as quickly as possible.  We've introduced a new, step-by-step process within your Google Account that we will automatically trigger if we detect potential unauthorized activity."  And then they said:  "We will help you," and there's four bullet points.



First is "Verify critical security settings to help ensure your account isn't vulnerable to additional attacks and that someone cannot access it via other means, like a recovery phone number or email address."  Two, "Secure your other accounts because your Google Account might be a gateway to accounts on other services, and a hijacking can leave those vulnerable, as well."  Which, if true, that's impressive.  Three, "Check financial activity to see if any payment methods connected to your account, like a credit card or Google Pay, were abused."



And it's like, what?  They're going to do that?  That's, again, sort of amazing.  And then "Review content and files to see if any of your Gmail or Drive data was accessed or misused."  So that seems a little more believable.  But I have to say I'm impressed if Google is saying that they're going to proactively check for transactions on your credit card or Google Pay.  I mean, maybe it's your credit card through Google Pay, in which case that...



LEO:  Yeah.  It'd have to be one that you've given them the information of for Google Pay.



STEVE:  Right, right. 



LEO:  So I can see, if I go to my Google Pay account, I can see credit card receipts and stuff.



STEVE:  Okay, okay.  So they have visibility into that.



LEO:  Yes, that's right.



STEVE:  So basically it's stuff that - they would check for abuse of things that somebody who had your Google account could abuse.



LEO:  Precisely, precisely.



STEVE:  Right, okay.  That makes sense.  Okay.  So there's a hacker, Yushi Liang, who tweeted a couple days ago:  "We just broke #Edge.  Teaming up with Kochkov for a stable exploit.  Brace yourself.  SBX is coming."  Now, SBX is the standard jargon for sandbox.  That's the abbreviation for sandbox.  And we'll be coming back to that when we talk about Windows Defender.  So he's a well-known hacker/developer with some track record.  He's recently broken several other browsers.  So his tweeted announcement of the discovery of a zero-day exploit of Edge with sandbox escape is probably authentic.  He previously developed a remote code exploit for Firefox which required the use of a three-bug exploit chain.  And on the Chromium browser he was able to achieve code execution, though without a sandbox escape.



So upon seeing his latest claim, the guys at Bleeping Computer reached out and received a video from him demonstrating the Edge breach.  In this video, Microsoft's Edge browser is made to launch an instance of Firefox, which should definitely not be possible, and Firefox then loads the download page for Google's Chrome browser.  So the guy has a sense of humor.



Yushi will be attending the forthcoming Pwn2Own Mobile hacking competition, which is in Tokyo next Tuesday and Wednesday - it may generate some security news for us, depending, so we may be talking about that probably the week after - where he can be expected to show off some of his other exploits and take home some prizes.  His recent tweets have revealed exploits against desktop web browsers.  But next week's Mobile Pwn2Own is focusing upon smartphones and IoT devices.  They will be offering prizes for compromises of phones from Google, Samsung, Apple, and Huawei; and IoT devices including the Apple Watch Series 3, the 2nd-generation Amazon Echo, Google Home, the Nest Cam IQ Indoor Camera, and also Amazon's Cloud Security Camera.  More than half a million dollars U.S. in cash and prizes are available to researchers with 10 different devices and categories.  So it looks like it can be profitable for somebody who's able to compromise those.



And it's also interesting and significant that Zerodium, who we've been speaking about because they're the separate entity which offers to buy zero-day exploits, the going rate for an Edge browser zero-day is $50,000, and double that if it's a sandbox escape.  Certainly Yushi knows that that's the case.  Now, this would not qualify for this current Pwn2Own.  And I don't know whether he's looking forward to selling what he's got to Zerodium or how he's going to disclose this.



In some previous tweets it indicates he is looking for some work.  On October 22nd he tweeted:  "Looking for a job (remote).  Need a stable work.  Tired of selling my research for cheap payouts."  He said:  "I did exploit dev and training, browsers most time, for last two years.  Please DM me with offers."  And then actually, just following that other tweet, remember, where he said "We are working."  He said, and he's referring to the Russian guy, Kochkov, he said:  "One of the reasons why I love working with Russian people, @AlexKochkov."  He said:  "Rewrote an exploit of mine from 430 lines to 80."  So props to Alex for his rewrite of that.



So anyway, we may be following up on Pwn2Own.  For what it's worth, there is a - he's got a zero-day in Edge, and we don't know what he's going to do with it.  It's got a U.S. cash dollar value of $100,000, and he's looking for money.  So maybe he'll sell it.  Hard to say.  But one way or another, sooner or later, we will discover it, likely.  Although for something like this, for Zerodium, as we know, to be able to offer $100,000 for a stable, solid Edge zero-day with a sandbox escape, that's valuable.  That means that, for example, in a targeted phishing attack, somebody could, under the default browser of Windows 10, induce someone to click a link which that's all that's necessary.  And we don't even know if they have to click anything.  They might just go visit a page, and that could then run code of the attacker's choosing on the OS outside of the browser, that is, out of the sandbox.  So that's valuable.



And the point I was going to make was that someone like Zerodium, this exploit only has value to its purchasers, the people who purchase it from Zerodium, to the degree that it does not get loose in the wild because, the second it does, the security companies will see it happening, and it'll get known and get fixed.  So these things are valuable to the degree that they are not widespread, that they are tightly controlled by those people who purchase it.  And since Zerodium is in the business of making money from these, whoever's buying it is paying more than $100,000 for it and so wants to have the guarantee that others who purchase it will be responsible, where that means being very selective with its use because, again, once it becomes known, it gets fixed.  So the world we live in today, as I'm saying more and more often lately.



Okay.  Speaking of sandboxes.  Windows Defender gets to play in its own sandbox.  This news came out, I guess a week and a half ago, and I didn't get a chance to catch up on it.  In fact, this may have been on Monday, or I just missed it.  So this is a big deal.  Windows Defender, Leo, as you and I have been talking about, and I heard you talking about this on a couple other podcasts, and as you mentioned - I think it might have been on This Week in Google, in fact, last week - the fact that, as we said,  Defender is one of the five browsers that was reviewed which missed nothing, that is, it equaled four others in having a 100% success rate.  However, it did also have the highest level of false positives.  On the other hand you said then, and we said at the time, we've never had a false positive.



LEO:  No, yeah.



STEVE:  So it's like, okay, fine.



LEO:  I'll take it.



STEVE:  Great.  Exactly, sign me up.  So this is a big deal, and what Microsoft has done, and it was very difficult to pull off although Microsoft isn't saying anything, probably for antitrust and anticompetitive reasons, doing this required deep and careful plumbing into Windows which no other Windows add-on AV will be able to duplicate.  So this is no fault of Microsoft's, that is, the OS is theirs.  It's not open source; it's closed.  So it's just the reality of the nature of the problem that any contemporary AV must face.  So this again sort of suggests that the era of the third-party antivirus is beginning to wind down.  Microsoft is the owner of the OS, and as such they do have a specific position relative to pulling this off.



So what's the big deal?  As we've discussed here a number of times in the past, threat modeling is all about understanding the attack surface.  A web browser presents a large and rich attack surface for our modern PCs.  But, for example, another attack surface is untrained employees who freely click on anything that looks enticing.  People are a widely exploited attack surface, too.



But perhaps the largest attack surface there is is today's antivirus subsystem whose job it is to proactively scrutinize everything coming into the machine through any route.  That's its job.  So things that the users click on in email, things they don't even click on in email, stuff that they download, stuff that their browser sees, stuff they receive in email or download from their browser.  Virtually any - or a thumb drive gets plugged into the computer.  Something has to look at it to make sure it's safe.  And in fact I know that Defender does because I've tested that.



So AV presents one of the richest attack surfaces because, first of all, it's an interpreter, and we know that anything which examines and attempts to understand what it's seeing is at big risk of any mistake being made.  Interpreters are among the most difficult of technologies to secure.  But also an AV process must run with full system privileges because it needs to have total visibility into every nook and cranny of a system's permanent storage, its RAM, and its network connections.  So it can't run as an untrusted process because then bad stuff could hide simply by getting into the system, like getting system privileges through a privilege escalation.



So the AV process itself has to be running with the kernel level permissions because it needs full system visibility.  That means that, if it's compromised, and thus why sandboxing is both difficult and incredibly powerful and important, if compromised, then the compromising entity would get the same privileges as the process that has been compromised, in other words the AV component.  So all of this creates a huge target of opportunity, paints a big target on the back of the AV because it's inherently prone to exploitation and because it has such a position of power within the OS.



Microsoft's blog talking about this further details the many challenges that the Defender engineers faced in pulling this off.  It was a difficult job over a significant period of reengineering things in Windows that were necessary to be able to sandbox.  And it's a big deal.  Tavis Ormandy, our prolific and frequently showering researcher at Google's Project Zero, who previously discovered and disclosed several of these types of flaws in the past year, lauded the Microsoft effort on Twitter, saying it was game-changing.  And I agree.



So all that said, Microsoft is proceeding with caution.  So sandboxing is not, while it is available from 1703, that's Build 1703 of Windows 10, is not yet enabled by default.  Microsoft wrote:  "We are in the process of gradually enabling this capability for Windows insiders and continuously analyzing feedback to refine the implementation."  But anyone - and this is now - that's the end of their quote.  So anyone with Windows 10 v1703 or later, so that's last year's Fall Creators Update, can enable today Defender sandboxing for themselves.  And I am talking to you on a Windows 10 instance where I enabled it immediately.



LEO:  This morning.  Me, too.



STEVE:  Yup.



LEO:  First thing I did as soon as I read that, yeah.



STEVE:  So to our listeners who haven't seen this, you could certainly find it on the Internet.  You basically run a command prompt with elevated permissions, so run command prompt as administrator.  Then you type "setx /M MP_FORCE_USE_SANDBOX 1" and press Enter.  You should receive a confirmation that that change has been applied.  That essentially sets a permanent machine-wide environment variable to tell the AV system, Windows Defender, that you want to sandbox it.



Now, what's interesting is a bug was found.  You cannot shut down your machine and restart it.  You must restart it without shutting it down for that change to take effect.  So once you do that, then just restart, that is, reboot your system, but without powering it off.  And what you can then do, once the sandboxing you believe is enabled, is using Sysinternals' very popular Process Explorer utility, which is free - if you put "process explorer" into the Google, you'll get links.  It's at Microsoft, so you can trust it.



If you run Process Explorer on that rebooted machine, you should find two entries there.  The parent entry is MsMpEng.exe.  And underneath it a child process, indented, named MsMpEngCP.exe, will be attached to it.  The CP stands for Content Process, which is Microsoft's formal name for things that are sandboxed.  So MsMpEngCP.exe.  If you confirm that that's running as a child of MsMpEng.exe, then you have successfully sandboxed your instance of Windows Defender.  And I've not seen any problems with it.  It'll be interesting to hear any reports of anybody who does.



But anyway, I verified all that.  It works great.  And props to these guys.  I mean, they've done something which was very difficult, which is to solve the problem of giving Windows Defender the visibility it needs to do the job it does while, if a mistake was made in it, and given Windows' history last month or Microsoft's history with Windows 10 last month, mistakes can happen.  It means that this very potentially vulnerable process, anything that gets loose will also face containment and won't be able to get up to any mischief.  And again, I'll be interested to see whether other AV vendors start trying to say, oh, yeah, we're sandboxed, too.  I mean, they're going to want to say it.  It's not going to be true.



So, I mean, there just isn't any way, if you're not Microsoft, you can do this because it was really hard and took them a long time, and they own Windows.  Nobody else does.  So you want your AV sandboxed.  We've seen exploits of other AV where they represented, they created a problem more than if you didn't have it there in the first place because of the nature of the threat attack surface.  So anyway, Leo, you and I are even more happy with our decision than we were before.



LEO:  Yeah.  And it really underscores the points we've been making about why you shouldn't be running a third-party antivirus.  I mean, you don't need to now.  We know it's a good antivirus.  It's the only one sandboxed.  Anything else you put on there doesn't improve your security and has a significant chance to make it worse.



STEVE:  Yes, yes.



LEO:  Good.  I'm glad you - because I keep saying that, and I hope it's right because I don't want to give people bad advice.



STEVE:  A hundred percent right.  So we were talking about Process Explorer.  So two guys, Mark Russinovich and Bryce Cogswell, first created Winternals Software Limited Partnership and the Sysinternals website 22 years ago, way back in... 



LEO:  Wow.



STEVE:  Yes, 1996.



LEO:  Amazing.



STEVE:  It was a huge hit with Windows techies everywhere, since Sysinternals offered a unique and unmatched suite of 100% free, no screwing around, I mean, it was just free stuff.  And I remember thinking, what?  It was high quality.



LEO:  It was so good, yeah.



STEVE:  And nothing did the things their stuff did.  So they were system-related tools and utilities for determining much more about what was going on inside Windows underneath the covers than was available elsewhere.  And many of their tools became classics.  And I knew of a number of people who - and I was one of them - who routinely made snapshots of the entire collection, just in case anything might ever happen to make them suddenly unavailable.  I mean...



LEO:  These guys, they can't be giving it away.  That's too good.



STEVE:  Exactly.  So in addition to Process Explorer, which was an advanced version of Windows Task Manager, everyone who was a fan will remember Autoruns, which was a must-have back then.  It was like the best way of seeing what was - in fact, even today it's still the way.  Autoruns, if you don't know about it, try it.  The new one is just like you'll glaze over with all of the information that it shows.  It's like...



LEO:  I recommend it on the radio show all the time because people want to know, how's this stuff starting up?  What's starting up?  I don't get it.



STEVE:  Exactly.  So it gives you a comprehensive view into what is being run at startup or logon.  And of course we will remember, our listeners, especially the old-timers here on this podcast, the Rootkit Revealer.



LEO:  Yes, yes.



STEVE:  Which that was their utility, and it cleverly detected discrepancies between what the Windows Registry showed and the file system API, which essentially suggested, it might suggest the presence of a rootkit.  And it was that utility which first, I mean, the users of that utility which were first alerted that Sony BMG was installing a rootkit into their systems which could be abused, even though Sony wasn't doing anything malicious.  The way it hid files, which is what Sony was using it for, was repurposable by other malicious software to hide their files from users.  And of course that got Sony into lots of trouble.



So, okay.  What did finally happen, to everyone's horror, was the announcement that Microsoft was acquiring Sysinternals and hiring the boys away from us.  And it was like, "No, don't take them away."  And of course at that moment there were many snapshots of the Sysinternals website made because we were afraid that it was going to just disappear.  It was going to be - Microsoft was going to disband it somehow.  But of course many years went by, and their tools just kept growing and getting better and more refined.  So much as with Microsoft's recent purchase of GitHub, everything turned out for the best.



So fast-forward to today.  This is all relevant because we have just learned that Microsoft is in the process of porting this famous Sysinternals tools collection to Linux.  The ProcDump utility has already been ported, and it's up on GitHub, github.com/Microsoft/ProcDump-for-Linux.  So that's ProcDump.  And then ProcMon is next, with more tools to follow.  In their description, Microsoft said:  "ProcDump is a Linux reimagining of the classic ProDump tool from the Sysinternals suite of tools for Windows.  ProDump provides a convenient way for Linux developers to create core dumps of their application based on performance triggers."



And according to Mario Hewardt, I should have practiced his name,  who's the principal program manager for Azure Diagnostics at Microsoft, he wrote:  "These first ports are part of the company's larger plan to make the Sysinternals package available for Linux users in the future."  So just a cool little bit of good news coming for Linux users.



And I wanted to take a look at what Google had said about their evolution of CAPTCHA in the form of reCAPTCHA.  And this one was, it was Monday before last, which I missed because I had prepared the podcast on Sunday, the day before Microsoft's announcement of reCAPTCHA v3.  And I was off in your next neck of the woods, Leo, last week...



LEO:  Microsoft or Google?



STEVE:  Oh, did I say Microsoft?



LEO:  Yeah, it's Google.



STEVE:  I'm sorry, I meant Google.  Yes, Google.  Google, Google.  So CAPTCHA, as our listeners know, is the acronym standing for Completely Automated Public Turing test to tell Computers and Humans Apart.  Woohoo!



LEO:  Talk about a ridiculous acronym.



STEVE:  Exactly.  And so of course we've examined and explored the challenge of doing that in the form of CAPTCHAs any number of times through the years on the podcast.  Now Google, the parent of the reCAPTCHA, has delivered a third major version of their solution.  They announced it to webmasters.  So their announcement is aimed, not at end users, but coming from the direction of website designers.



LEO:  So we've all seen it now, and I think universally loathe it.  But go ahead.



STEVE:  Yes.  They said:  "Today we're excited to introduce reCAPTCHA v3, our newest API" - so again, aimed at webmasters, or web designers, web implementers - "that helps you detect abusive traffic on your website without user interaction.  Instead of showing..."



LEO:  Oh, this isn't that silly one where you find all the bicycles, and then you have to...



STEVE:  Well, actually that's number two.



LEO:  Oh.  So we're going to get rid of that finally.



STEVE:  Yes.



LEO:  Oh, I loathe that.



STEVE:  Yes, I know.  Well, because it's - well, yeah.  We'll get there in a second.  "Instead of showing a CAPTCHA challenge, reCAPTCHA v3 returns a score so you can choose the most appropriate action for your website."



LEO:  "You" the webmaster, yeah.



STEVE:  Yes.  Yeah, exactly.  You the webmaster.  So I have to say that's kind of cool.  The idea being is that it would be sending you information about the entity that is poking around your website.  That's kind of neat.  So they described this sort of like, in describing their history, working towards a frictionless user experience.  They wrote:  "Over the last decade, reCAPTCHA has continuously evolved its technology.  In reCAPTCHA v1, every user was asked to pass a challenge by reading distorted text and typing into a box."  Many times I was looking at it going, uh, what?



Then they said:  "To improve both user experience and security, we introduced reCAPTCHA 2 and began to use" - and this is what you and I intensely dislike, Leo - "many other signals to determine whether a request came back from a human or not.  This enabled reCAPTCHA challenges to move from a dominant" - that is, reCAPTCHA challenges, meaning where you have to bother the user - "to move from a dominant to a secondary role in detecting abuse, letting about half of users pass with a single click."  In other words, okay.  And of course that was the famous and somewhat confounding "I'm not a robot" checkbox assertion. 



LEO:  Yeah, that was so weird, yeah.



STEVE:  And it's like, what?  Okay.  And so you would click it.  And we talked about this at the time, like were they following the cursor around to say, oh, look, he's kind of - it wasn't a constant velocity automated direct beeline to the checkbox.  But it kind of overshot and then came back, more like a person, who knows.  Anyway, and if they could not - if you didn't qualify for the "I'm not a robot" checkbox, then you got that grid, which was like, select all the squares that show crosswalks, or that have headlights or whatever.  And it was like, ugh, again.



Now, they say, "with reCAPTCHA v3 we are fundamentally changing how sites can test for human versus bot activities by returning a score to tell you, the webmaster, how suspicious" - and actually it'd be kind of fun to surface that to the user, too, to say this is what Google says about you, anyway.



LEO:  You are suspicious.



STEVE:  You really are a robot, behave yourself - "...how suspicious an interaction is and eliminating the need to interrupt users with challenges at all."  So even the "I hereby state I not be a robot" goes away.  So they said:  "ReCAPTCHA v3 runs adaptive risk analysis in the background" - thus the need for JavaScript - "to alert you of suspicious traffic to your website while letting your human users enjoy [what they describe as] a frictionless experience on your site."  Then they said:  "More Accurate Bot Detection with 'Actions,'" in quotes.  They said:  "In reCAPTCHA v3 we are introducing a new concept called  'Action,' a tag that you the webmaster can use to define the key steps of your user journey" - and I don't know what that means - "and enable reCAPTCHA to run its risk analysis in context."  Again, whatever that means.



They said:  "Since reCAPTCHA v3 doesn't interrupt users, we recommend adding reCAPTCHA v3 to multiple pages.  In this way, the reCAPTCHA risk analysis engine can identify the pattern of attackers more accurately by looking at the activities across different pages on your website.  In the reCAPTCHA admin console, you get a full overview of reCAPTCHA score distribution and a breakdown for the stats of the top 10 actions on your site" - whatever actions are - "to help you identify which exact pages are being targeted by bots, and how suspicious the traffic was on those pages."



So as with many of the services Google offers, there's a privacy and tracking tradeoff here.  Just as the addition of Google Analytics provides a webmaster with great quantities of doubtless useful information, presented through a beautiful user interface, those same great quantities of useful information flow back to Google.  And it's clear that a similar transaction is going on here.  Google makes clear that they want their new adaptive risk analysis reCAPTCHA to be sprinkled liberally throughout a website to give it greater visibility into user behavior.  But that also gives Google much greater visibility into your website's visitors' behavior, as well.  And I'm not suggesting that there's anything wrong with that.  But it's kind of worth keeping in mind.



I would argue that for sites that have a big bot problem, it probably makes sense to upgrade to this reCAPTCHA v3.  Give your webmaster, if that's not you, some marching orders of figuring out what this v3 is and leveraging it to maximum use.  I mean, it does two things.  It probably better detects bot behavior than ever before.  And it eliminates, as Google says with their zero friction, their zero friction experience, you don't even have to have users say, yeah, I'm not a bot.  It's just me.  In fact, we've got a reCAPTCHA at the moment over on GRC's forthcoming SQRL web forums.  So I'll make some time to see about upgrading that to v3 because why not?  I think it makes a lot of sense.



And, Leo, last Friday was November 2nd, the 30th anniversary of the appearance of the Internet's first worm.



LEO:  Wow.



STEVE:  Nobody knew such a thing was possible.  Named for its progenitor, the Morris Worm.  So Steven Vaughan-Nichols, who's been around forever, writing for ZDNET, on November 2nd he said:  "I was working at NASA's Goddard Space Flight Center in the data communications branch."  This was November 2nd, 1988.  He said:  "Everything was fine.  Then our Internet servers running SunOS and VAX/BSD Unix slowed to a stop.  It was a bad day.  We didn't know it yet, but we were fighting the Morris Internet Worm.  Before the patch was out, 24 hours later, 10% of the Internet was down" - now, again, that's the 1988 Internet - "and the rest of the network had slowed to a crawl.  We were not only facing the first major worm attack, we were seeing the first distributed denial-of-service attack.  Unlike the hundreds of thousands of hackers that would follow, Robert Morris, then a graduate student at Cornell, wasn't trying to 'attack' the Internet's computers.  He thought his little experiment would spread far more slowly and not cause any real problems.  But he was wrong.



"Well, that's what he said afterward," Steven wrote.  "I'm not at all certain that was the case."  He said:  "Consider the Morris Worm had three attack vectors:  sendmail, fingerd, and rsh/rexec.  It also used one of the now-classic methods, stack overflow, in its attack.  It was also one of the first programs to use what we'd call a 'dictionary attack' with its list of popular passwords.  The passwords and other strings hid in the worm's binary through XORing.  So they had been deliberately obscured.  Morris also tried to hide his own tracks," wrote Steve.  "He started the worm from an MIT computer.  It hid its files by unlinking them after trying to infect as many other servers as possible.



"Even without a malicious payload, the worm did serious damage.  Infected systems quickly did nothing but trying to spread the worm, thus slowing them down to a crawl.  Some, most of them running SunOS, a Unix variant and the ancestor of Solaris, crashed under the load.  In the meantime, Morris, who included code to keep the worm from spreading too fast, had realized he was no longer in control.



"Morris called a friend, who subsequently said Morris 'seemed preoccupied and appeared to believe that he had made a colossal mistake.'  He had indeed.  Thanks to the efforts led by Eugene 'Spaf,' as he was called, Spafford, then an assistant professor of computer science at Purdue University, and the current editor-in-chief of Computers and Security, the worm was conquered.  Before the worm was finished, it successfully attacked about 6,000 of the 1988 Internet's 60,000 servers.  In the aftermath, DARPA created the first CERT/CC" - that's the Computer Emergency Response Team/Coordination Center - "at Carnegie Mellon University to deal with future security attacks.



"But the worm's biggest legacy to date was that it started wave after wave of computer and Internet attacks."  He said:  "If Robert Morris hadn't done it, someone else would have.  But regardless, today we live in a world where a day doesn't go by without a serious attack."  So 30 years ago, Leo.  Wow.



LEO:  Wow.



STEVE:  And speaking of not a day going by, Lawrence Abrams at Bleeping Computer brings news of a crazy new ransomware that calls itself "CommonRansom."  And I would argue, as does Lawrence, that actually he says absolutely under no circumstances never ever.  I'm thinking, well - we'll take it as a challenge.  So here's the deal.  The ransom note for CommonRansom reads:  "Hello dear friend."



LEO:  I love these notes.  They're so funny.



STEVE:  Exactly.  "Hello dear friend.  Your files were encrypted!"  Yeah, you're really a good friend.  "You have only 12 hours to decrypt it.  In case of no answer, our team will delete your decryption password."  That's right, because we're your friend.



LEO:  Your friend.



STEVE:  And it says:  "Write back to our email, old@nuke.africa."  And then it says:  "In your message you have to write this ID," and then it provides in the note the "victim ID" for your machine.  So you identify yourself to them by your victim ID.  Then, get this, the IP address and port of the RDP service, that is, the Remote Desktop Protocol of the infected machine.  They want permission to remote into your computer...



LEO:  Oh, please.



STEVE:  I'm not kidding, Leo, to disinfect it.  Number three, the username and password having admin rights.  I know.



LEO:  The one the cure's worse than the fix, I mean the disease.



STEVE:  Yes.  And that of course was exactly Lawrence's point.  And the time of day when you have paid .1 BTC, .1 bitcoin - so what's that, around 650 U.S. at the moment because Bitcoin's been hovering around $6,500 - to the following bitcoin wallet, and then they give you the bitcoin address.  They say:  "After payment, our team will decrypt your files immediately."  Uh-huh.  Meaning they will remote onto your machine.



And Lawrence points out that at that point your screen goes blank because Windows workstations only allow one interactive login at a time.  So they acquire it, you get logged out, they now have admin rights on your machine, and you can't see what's going on.  So lord only knows what's going to happen.  So, okay.  So first of all, who the heck is going to give bad guys a remote desktop protocol connection to this machine?  If you are to be infected by this nightmare, the best advice would be what Lawrence Abrams at Bleeping Computer, who I would argue is the industry's leading expert on ransomware, you know, don't even consider this.



But, okay, what if you absolutely, absolutely, absolutely had to have some files off of that machine?  So obviously the best advice would be not to get yourself infected in the first place.  But if that ship has sailed, and you have no backups, and you absolutely have to; okay?  So as I said, sort of as an exercise, which we're not recommending, I would say take everything, absolutely everything else off your network.  Since so many things are these days WiFi connected, perhaps change the WiFi password and reboot your router so that everything...



LEO:  What if you didn't use a router?  What if you took the router out and direct-connected the computer to the Internet?  And then that way your network's not on.



STEVE:  I think that's probably a better idea, Leo.  I like that better because, yes, because then...



LEO:  He only has access to your computer, that's it.



STEVE:  Just your computer, yeah.



LEO:  But the passwords are in there and stuff like that.  So you want to clear that stuff out, if you can.  If you're encrypted, you're screwed; right?



STEVE:  Yeah, you know, I didn't think about the other information that is available to them that they could have access to.



LEO:  They have your login.  They can see everything.



STEVE:  Yeah, that's...



LEO:  You're kind of out of luck.  I'd say it's done.



STEVE:  Yeah.  Because you're right, depending upon what's there, you absolutely have to consider that they will know everything that is on the decrypted machine after they decrypt it.



LEO:  Right.



STEVE:  Now, technically something, an agent, was already on the machine, which is sort of the position I was coming from.  I was thinking that, okay, you could, if you had to have it decrypted,  they could do that.  But you're going to have LastPass there.  And even though LastPass is encrypted, it's still - you don't want them to have access to your trove of passwords.  And, I mean, again, we know that LastPass is secure.  It uses a PBKDF, so it takes a long time to guess, to decrypt even the local store, et cetera, et cetera.  But you're right.



LEO:  I also would guess that somebody doesn't have a backup, probably doesn't have LastPass either; right?



STEVE:  Yeah, that's a good...



LEO:  This is the naive user who's getting bit, at this point.



STEVE:  If you felt that you could place your decrypted machine's contents in the hands of clearly people who are criminal, not your mom, but a criminal.  My point was, if you got it back, immediately remove the hard drive.  You can never, ever execute anything on that machine.  But you could theoretically treat it as data, as a data drive, put in another drive, reinstall Windows.  If you had an old backup, you could restore that old pre-infection backup and then get a lot of stuff back.  And then, if you had to, again, I mean, again, I know lots of people are like, Gibson, you're crazy.  Okay.  But if there's a file you absolutely have to have, it's a data drive.  You could safely, you know, data can't just suddenly execute itself and jump off the drive.  So even though it's contaminated, and you always have to regard it with skepticism, the data files might be there.



So as an absolute last resort, I could see doing that.  But yes, Leo, you're right.  My position was you'd already lost - you already had bad guys in your computer.  But that's arguably their - they haven't been in, even though maybe their malware has been to encrypt your drive.  So you're right, it's probably just better just to say okay.



LEO:  I lose.  I lose the Internet.



STEVE:  Ouch.  Ouch.  Ouch.



LEO:  Ouch.  Oh.



STEVE:  So I had one last little piece.  I called it "The sounds of silence" because I just got a kick out of this.  On page 13 of the Apple T2 Security Chip Security Overview, which we just got late last month, and I'm going to go through the whole document, I haven't yet, to talk about it probably in some detail next week.  I got a kick out of page 13, which was titled "Hardware microphone disconnect."  And I just thought, bravo, Apple.



They said:  "Security chip feature."  And it's really not, but okay.  "A hardware disconnect that ensures that the microphone is disabled whenever the lid is closed.  This disconnect is implemented in hardware alone, and therefore prevents any software, even with root or kernel privileges in macOS, and even the software on the T2 chip, from engaging the microphone when the lid is closed."  They said:  "The camera is not disconnected in hardware because its field of view is completely obstructed with the lid closed."  So I just thought, you know?



LEO:  Nice touch, huh?  Yeah.



STEVE:  That's a nice touch.  I do appreciate that.  Speaking of appreciating things, I found a tweet Sunday, 9:07 p.m., from Jonathan Hellewell, who tweets from @jrobertbooks, which is also his website.  And he said, "Hey, Steve."  And this is another one of these heartwarming stories where it's like I'm just so glad I can do this for people.  He said:  "Hey, Steve.  I've been a fan of Security Now! since I found it a few years ago, during the Snowden saga.  I get a kick out of it when certain technologies or hardware comes up in other media.  While the news types hype tech stories beyond credulity, I always wonder what your take on those issues is going to be.  Bloomberg's apparent, if unintentional, hit piece on Supermicro comes to mind as a recent example.



"I want to thank you for SpinRite.  I recently had my old Winblows box die," he wrote.  "The computer was no real loss, but a single folder was critical.  I'm a self-published author, and the folder with all of my research, notes, cover images, and drafts was on that computer.  That system was set up for weekly backups, but it died on day six.  I was left banging my head on the desk.  I had written 10,000-plus words on the second book of my series that week, and trying to redo it from memory really wasn't something I wanted to do.  SpinRite came through for me.  Twelve hours later, I was able to get the drive to mount, and I pulled that folder off the drive without any extra problems."



LEO:  Wow.



STEVE:  "So thanks for saving a bit of my sanity."



LEO:  George R.R. Martin.  No.  Okay, good.



STEVE:  "If you find yourself looking for some sci-fi, maybe you could give the Emerging Ascendancy series a look..."



LEO:  All right.



STEVE:  Which is no longer lost, "...at jrobertbooks.com."



LEO:  That's great.



STEVE:  "I'd be more than happy to send you a copy to say thanks, eBook or print.  I'm rather curious to see if you'd like it.  Thanks again, Jonathan."  And Jonathan, thank you for sharing your really terrific report of us having saved...



LEO:  Saved Emerging Ascendancy.



STEVE:  ...a week worth of your sanity.



LEO:  Wow.



STEVE:  I have a quick closing the loop.  Triple Stuf, who tweets from @TripleStufOreo, he said:  "Hi, Steve.  I have a question regarding your vending machine puzzle.  I came up with a similar solution, but I don't understand some of the choices you made.  Wouldn't it be even simpler and more secure to have the following differences?"  He said:  "What if the vending did not have any HSM?"  No Hardware Security Module, no TPM-style chip and so forth.



He says:  "All it has is the server's public key, a bunch of nonces, and a vending machine identifier.  When someone wants to buy something via the app, the app sends the request containing the nonce, vending machine identifier, and product of choice to the server.  The server of course checks the balance, deducts the correct amount, and sends back a signed version of the request, therefore acknowledging the successful transaction.  The app gives the signed request to the vending machine, which it can check."



So he enumerates.  He says:  "Advantages:  Simpler system with less hardware.  Vending machine doesn't have any secrets to keep, even though this shouldn't be a problem for an HSM."  He said:  "Note:  If you wanted to send the inventory securely, this data could also be put in the request that has to come back signed.  Please explain to me what I'm missing.  Thanks for the great show every week."  He says:  "If you would like to use my question in SN" - Security Now!, of course - "please feel free to do so.  Kind regards, Rein from the Netherlands."



Okay.  So for example, he suggested, if you wanted to send the inventory securely, this data could also be put in the request that has to come back signed.  But he just explained the problem.  So, for example, the app in the middle, which is essentially a man in the middle.  We've explicitly allowed, we've designed a man in the middle between the vending machine and home base.  That app could fake the inventory and send it to the home base, headquarters.  Since there's no authentication on what's going from the vending machine to headquarters, there's no way for headquarters to know that the app has deliberately screwed with the inventory.



And notice that, sure, home base can sign what it receives and send it back.  But the vending machine can't do anything with the fact that the inventory that it's sent has been tampered with because it's been tampered with.  And even without that, the idea of one-way authentication, I mean, I get what TripleStuf or what Rein is suggesting.  But it doesn't feel right to me.  SQRL does something more like Rein has suggested, that is, SQRL's protocol is a single-ended public key transaction.  But the server creates an authentication code.  It creates a MAC, a Message Authentication Code, for what it sends to the client.  And it verifies that what the client sends back signed is unmodified, what the server originally sent.



And of course that can't just be a hash like with SHA-256 because, if it was just a hash, then the client could rebalance the hash before signing it.  So it has to be a keyed hash, in other words, an HMAC, using a key unknown to the client.  So a secret key.  So once again, we have a secret in the server.  Which is equivalent to a secret in the vending machine in this case.  And the least you would want to do would be to have the vending machine have a secret that allows it to verify that there was no modification of what it sent to the client.  But now you're back again to having a secret, although it is, it could be a randomly chosen secret key that would not have to be stored in a hardware security module.  It could just be stored in RAM, for example, and only have to live long enough for the transaction to complete.



So I could see softening this somewhat, as long as whatever the vending machine sends is protected from modification.  It just sort of seems like there's going to be ways that the app in the middle could get up to some mischief unless you did that.  And it's actually exactly the architecture that we're using with SQRL.



And one last one.  Mike Calandra, he said, also through Twitter:  "Another thought."  He said:  "I heard you say on the last podcast that someday you would consider switching from Firefox to Chrome, despite the fact that Google uses your browsing history as a primary source of revenue, and hence it is less private.  Your show is on privacy and security.  How do you reconcile this?  It seems that Chrome would have to have significant advantage over Firefox for you, and I wonder what that would be?  I've always been a Firefox user.  Big fan of the podcast, thank you."



Okay.  So that's a very good point.  And I'm not moving anytime soon.  I also really like Firefox.  We don't know what the future is going to hold, whether Mozilla is going to keep their head above water.  I really hope they do.  I guess what I meant to say is I could not consider the move unless Chrome had really good browser tabs, which it doesn't yet.  And I run uBlock Origin even on Firefox.  I would certainly have the same tools, uBlock Origin available for Firefox as an extension.  And so I would certainly further beef up the privacy and security of Chrome and take more responsibility for dealing with and protecting myself against any overage of tracking and inspection.



LEO:  All right.  I'm ready to smash my ports.



STEVE:  So it doesn't have quite the ring as...



LEO:  Actually, it does.  I think that's a good name, PortSmash.



STEVE:  Well, okay.  BleedingBit.



LEO:  BleedingBit's bad, yeah.  I mean, good bad.



STEVE:  I think if it had been my discovery, I might have been tempted to name this next vulnerability YAIPE.



LEO:  YAIPE?



STEVE:  Y-A-I-P-E, for Yet Another Intel Processor Exploit.



LEO:  Sigh.



STEVE:  I know.  But in any event its discoverers have chosen to name it PortSmash.  The short version of this poses the question:  "Is Intel's Hyper-Threading Dead?"  Remember that OpenBSD, out of a characteristic abundance of caution, completely disabled its OS's use of hyperthreading earlier this year, when the Spectre and Meltdown disasters began to unfold.  Near the start of October, last month, PC World executive editor headlined a story:  "Intel's 9th-gen Core i7-9700K [processor] Abandons Hyper-Threading:  What It Could Mean for Performance."  And the subhead was "Intel's 9th-gen gives the Core i7 a demotion in threads, but a promotion in cores."



And PC World explained:  "Intel first introduced Hyper-Threading on consumer CPUs with the Northwood-based Pentium 4" - Leo, it was the Pentium 4.



LEO:  Holy cow.



STEVE:  Yeah, 16 years ago, in 2002.  And I remember thinking, this is the coolest, like, hack.  



LEO:  Yeah.



STEVE:  Because it wasn't a whole core, but you could do two things kind of at once.  And PC World said:  "It works" - and we'll be refining their definition in a second.  "It works," they wrote, "by splitting a single physical core into two logical cores.  Since most compute threads don't consume 100% of a CPU's resources, Hyper-Threading," they wrote, "lets the unused resources do work as well.  Hyper-Threading, of course, is Intel's" - and they called it "fancy pants" - "name for simultaneous multi-threading, SMT..."



LEO:  SMT, yeah.



STEVE:  "...which AMD also began employing with its Ryzen chips.  Although Hyper-Threading's performance boost has been around for 16 years," they write, "it hasn't always been tapped into.  No Core 2 CPUs ever used the feature, for example, and Intel's Atom CPUs have had it off and on."



Okay.  So that was back in October.  And it's clear that Intel has long been juggling and judging the tradeoffs between Hyper-Threading's complexity versus power consumption, thread count, and hyperthread utilization in the real world, in the field, and has not always come down with a purely pro Hyper-Threading position.  So here we are, a month later, now at the beginning of November, and something bad just happened to Hyper-Threading.  The proof-of-concept code called PortSmash comes from researchers at the technical universities in Finland and Havana, Cuba.  PortSmash is a new - that is, it's not yet another variant of Spectre or Meltdown - side-channel attack and exploit with working proof-of-concept code posted on GitHub.



The proof of concept successfully steals a TLS session secret key from OpenSSL across the threading boundary.  In other words, across two separately logical, or logically separate processes, which should be completely isolated from one another.  It is possible for a malicious process to steal the session's encryption key from a TLS session supported by OpenSSL.  The researchers have convincingly demonstrated that cross-hyperthread information leakage can and does occur.



They wrote:  "A CPU featuring SMT" - and they said, e.g., Hyper-Threading, we know it as simultaneous multithreading - "is the only requirement.  This exploit code should work out of the box on Skylake and Kaby Lake.  For other SMT architectures, customizing the strategies and/or waiting times in 'spy' [is the name of their thread] is likely needed.  They named it 'PortSmash' because, at the micro-architectural level, each physical hardware core is subdivided into a number of separable regions, which are, in microarchitecture parlance, called 'ports,' each of which perform different types of tasks within the processor."



And those tasks are being split up into these separated ports and are able to operate separately, sort of autonomously from each other.  So in other words, by granularizing, not only - at the chip level we've got multiple cores; right?  But by granularizing a single core, it's different ports which are sort of like subfunctions of the whole CPU, which no one thread is using all at once.  Another thread can be using other parts of the same core.  So that's what SMT actually is.



So by now, nearly a year after, well, the revelations of Spectre and Meltdown and basically a continuous series of discoveries of the problems of speculation and Meltdown, we know where this is headed.  PortSmash uses subtle instruction timing variations which allow it to detect the contention which inevitably exists when two logically separate threads of execution are sharing a single core's hardware.  So it should hardly come as a surprise.  PortSmash's attacking thread repeatedly executes instructions until the CPU's hyperthread scheduler stops it running and hands that port over to the other thread.  By measuring the time in between its own instructions running on that port, it can measure the time that the other thread takes to process its own instructions.  And this in turn can help it derive another program's secrets over time.  And these guys went from theory to practice.



Now, of course, for their part Intel had to weigh in.  They said:  "This issue is not reliant on speculative execution and is therefore unrelated to Spectre, Meltdown, or..."  Yes, Leo.



LEO:  That supposed to make me feel good?



STEVE:  Uh-huh.



LEO:  Oh, don't worry.



STEVE:  Yes, that's right, "...the L1 Terminal Fault.  They said:  We expect that it is not unique to Intel platforms."  In other words, not just us.  "Research on side-channel analysis methods often focuses on manipulating and measuring the characteristics such as timing of shared hardware resources" - like cash, for example.  "Software or software libraries can be protected against such issues by employing side-channel safe development practices, protecting our customers' data, and ensuring the security of our products is a top priority for Intel.  And we will continue to work with customers, partners, and researchers to understand and mitigate any vulnerabilities that are identified."



Okay.  That was Intel.  Now, OpenSSL was contacted by the researchers and has already updated their code to mitigate the effects of this attack so that updated versions of OpenSSL are already no longer vulnerable.



What about hyperthreading?  Hyperthreading itself is useful since cores can be subdivided into sub-pieces.  Cores can be granularized, and you can get better utilization of the silicon if you let multiple threads wander around on it at the same time.  You get - and estimates vary.  I've seen 10%; I've seen 30%.  So you can get additional performance out of a single core's microcode execution units by keeping them more often busy by giving them more work to do.  So despite its potential for abuse, once the potential downsides are clearly understood, hyperthreading, I argue, can be used safely.  Therefore I doubt we'll be seeing it disappearing altogether.



In the first place, let's not lose sight of the fact that the only place where any danger is present is in a scenario where there could be a hostile instrumented thread simultaneously running on the same core as a thread which is performing security-sensitive work.  So that's the attack model, a hostile thread simultaneously sharing a core with a security-sensitive thread.  So the maximum danger only exists in shared host virtual machine scenarios where unknown and untrusted processes are all cross-sharing the virtualized hardware at the same time.  So no one is denying the danger there.  It's real.  And now we know more clearly how real it is.



But, for example, an enterprise web or database server is only running its own server code.  It's not running random unknown foreign processes.  And the same is generally, though perhaps a bit less explicitly true for enterprise and personal workstations.  As we've often observed, if you have something bad in your machine which could be leveraging Spectre or Meltdown, or now PortSmash, we have already lost the battle.  You've got something bad in there.



Okay.  But also many of today's processes are internally multithreaded, meaning that, within a single process, many things are going on at the same time.  So the thread schedulers in our operating systems can be usefully and relatively easily updated so that no single core is simultaneously shared among multiple processes.  In other words, hyperthreading can be safely employed by multiple threads within a single process, since all of the threads in a process are on the same team.  So the PortSmash danger is only present when separate processes are simultaneously sharing a physical core, and that's easy to prevent.  So with a bit of tweaking of the schedulers of the operating systems running within shared hosting VM systems, even in the presence of hostile processes, the benefits conferred by hyperthreading can be obtained without risk.



So great research.  I love it when something that is theoretically possible is just simply demonstrated.  Here you go, folks, let's take this seriously.  What this will drive is an upgrade of the thread schedulers in OSes to not share cores among processes.  And then we get the benefit of the performance without the downside risk of security.  So PortSmash, yes, it exists.  But I don't think it's really going to be a problem.



And most people who have hyperthreading processors, as I mentioned before, will find an option in the BIOS to turn that off if it makes you uncomfortable.  So if you are in an environment which is subject to hostile processes running amid security-sensitive processes, you can probably simply shut off hyperthreading as an option in the BIOS.  And then you'll lose a little bit of performance.  But until you know that the OSes that you're hosting in that environment are PortSmash safe, you'll have a little bit of a performance hit, but at least you'll have security.



And I have a bit of a tease for next week, Leo.  I believe, unless the world ends in some new and unforeseen fashion...



LEO:  Well, it is Election Day, so anything's possible.



STEVE:  And it is Election Day, so, yes.  I plan to do a deep dive into the inner workings of the encryption built-into self-encrypting SSDs.  A team of researchers has reversed the firmware in a bunch of very popular, I think it's Crucial and Samsung are the two brands they looked at.  And what they found was not encouraging.



LEO:  Yes, especially since we've learned that BitLocker often defers to the built-in encryption on an SSD in lieu of actually doing anything.



STEVE:  Exactly.  Exactly.



LEO:  Whoops.  Yeah.



STEVE:  So that's our topic, tentatively, depending upon what else happens between now and then.  I think we will take a good, deep dive into the technology of SSD drive encryption and why it's not doing what we think it is.



LEO:  Awesome.  Steve, you've done it again.  Two hours of great security information, provided to you all for free by this man here who works very hard to do it each and every week.  So you could reward him by hieing thee hither, though, over to GRC.com and take advantage of SpinRite, the world's finest hard drive maintenance and recovery utility.  You never know when you need to save your next novel.



STEVE:  And if you run it from time to time,  it'll keep it from going away in the first place.



LEO:  Yes, absolutely.



STEVE:  So that's the real takeaway.



LEO:  Had occasion to recommend it a couple of times on the weekend, on the radio show.



STEVE:  Oh, cool.



LEO:  Yeah.  People had problems that were, I think, particularly amenable to running SpinRite on them.  One of them was on a Mac, and none of the Mac recovery tools are very good, and there's nothing low-level like SpinRite.  So I said, take it out, put it on a PC, run SpinRite on it, and I think you'll see that it's probably still there.



STEVE:  Yup.  Yup.  Perfect.



LEO:  While you're at GRC.com you can also get a copy of this show.  He's got audio.  He's got transcriptions.  It's all at GRC.com, plus lots of other great free stuff, including SQRL as it's slowly inching its way.



STEVE:  It's getting there.



LEO:  To a computer near you.



STEVE:  I've solved every problem, Leo.  I've solved just recently the problem of right now when people need to share access to an account, they just give somebody their username and password; right?



LEO:  Oy, yeah.



STEVE:  But, you know, you can't do that in any era where you have a tightly bound identity, like with your retina or your fingerprint.  You can't give somebody else your fingerprint or your retina.



LEO:  Good point.



STEVE:  You don't want to.  So we need, moving forward, a means of dealing with that, and we have that now up and running in a working demo based on a nice API that facilitates it.  So again, when we sit down and talk about this, I think the biggest takeaway is that every, I mean, it's why I've taken awhile.  Every problem that is associated with really solving this correctly we have solved.  So, yeah.



LEO:  Nice.  Lovely.  Lovely news.  You can also get this show at our site.  We have audio and video even at TWiT.tv/sn.  If you get there, you'll find all the previous episodes, and you'll also find information on how to subscribe.  Or just, you know, you can figure this out.  You're smart.  Just go to your favorite podcast app, search for Security Now!.  If you subscribe, you'll get it automatically.  You don't even have to think about it.  Just the next time you're looking for something good to listen to, you'll have it there on your phone or device.



Or you can ask your Echo or your Google Home, your Cortana or your Siri.  You can just say, "Hey, Voice Assistant, listen to Security Now!," or play - people laugh at me because I always say "listen" to Security Now!.  And they say, "You should say 'play' Security Now!."  I don't know why.  But both work.  But I feel like I want to listen.  So I'm saying "Listen to Security Now!."  Together, the two of us.  You and me, Siri, we'll be listening.  In any event, that usually works.  Sometimes you have to add on TuneIn, but almost always that works.  



Thank you, Steverino.  We'll see you next week.



STEVE:  Okay, buddy.



LEO:  Go Red Sox.  Have a good one.  We'll see you next time on Security Now!.



STEVE:  Okay, my friend.  Right-o.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#689

DATE:		November 13, 2018

TITLE:		Self-Decrypting Drives

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-689.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we cover last month's Patch Tuesday this month.  We look at a GDPR-inspired lawsuit filed by Privacy International.  We ask our listeners to check two router ports to protect against a new botnet that's making the rounds.  We look at another irresponsibly disclosed zero-day, this time in VirtualBox.  We look at CloudFlare's release of a very cool 1.1.1.1 app for iOS and Android.  And, in perfect synchrony with this week's main topic, we note Microsoft's caution about the in-RAM vulnerabilities of the BitLocker whole-drive encryption. We also cover a bit of miscellany, we close the loop with our listeners, and then we take a deep dive into last week's worrisome revelation about the lack of true security being offered by today's Self-Encrypting SSD Drives.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Patch Tuesday is here.  He'll cover that, as always; talk about a number of exploits and a number of routers again vulnerable; and then, finally, why you maybe shouldn't trust BitLocker when it comes to hard drive encryption.  SSDs that decrypt themselves?  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 689, recorded Tuesday, November 13th, 2018:  Self-Decrypting Drives.



It's time for Security Now!, the show where we cover your security and privacy online with the guru, the king of security, Mr. Steve Gibson.  Hello, Steve.



STEVE GIBSON:  Yo, Leo.  Great to be with you on this November 13th, the lucky Second Tuesday of the Month.  As I teased at the end of last week, some news had just hit as we were going to podcast about two researchers at a university in the Netherlands who decided to take a look into the details of self-encrypting SSDs, rather than just assuming, because they encrypt, that's all we need to worry about.  It's like, oh, well, they encrypt.  Oh, perfect.



They decided to take a peek under the covers.  And what they found is not one of the seven that they examined, from Crucial and from Samsung - three were from Crucial, the other four were from Samsung, including the very popular 840 EVO and 850 EVO SSDs - none of them were doing the job right.  And in fact it was possible without any information to basically decrypt the drive.  Consequently, the title for this week's podcast is Self-Decrypting Drives.



LEO:  Oh, boy.  Well, the worst part of this is that in some cases Windows relies on these drives' encryption.



STEVE:  Yes, yes.  We will have some takeaways for our listeners.  The recommendation, well, these guys have just - I was glad that there wasn't an overabundance of other news because I wanted to give this topic enough attention.  What's interesting is that, very much as with the vending machine problem, anyone who's been listening with focus to this podcast for a few years would be able to state, as we will, how simple this problem is to correctly solve.  Yet somehow none of these drives solved it correctly.  They solved it incorrectly.  So anyway, so there's lots of good takeaways from this, including what anyone using BitLocker on one of these should probably do.  But these guys go even further.  So we'll get to that.



We also have - we're going to talk about last month's Patch Tuesday this month, that is, today is the Second Tuesday of the month, so it's our Patch Tuesday for November.  We're also going to look at a GDPR-inspired lawsuit filed by Privacy International, just quickly, just to sort of put it on our radar in case it actually happens to develop into something.  There is another new botnet in town making the rounds, and two ports that our listeners should just double-check on that we'll talk about.



We've got another irresponsibly disclosed zero-day, this time in VirtualBox, but probably not a huge problem, but worth, again, taking a look at.  Cloudflare has just released a very cool new app for iOS and Android that allows those mobile platforms to use their now kind of increasingly popular 1.1.1.1 DNS.  We've also, strangely enough, in perfect synchrony with this week's main topic, Microsoft is cautioning about the in-RAM vulnerabilities of BitLocker in the presence of, not surprisingly, Leo, you and I were talking about this a long time ago, the Firewire 1394 or the Thunderbolt interfaces.  And so that's pertinent because we'll see what our guys have to say about whether hardware encryption is actually safer than software.



We're going to close the loop with our listeners, cover a bit of miscellany, and then, as I said, take a deep dive into the question of the actual delivered security of drives that advertise that they perform hardware-level encryption.  And we'll note that 100% of the drives that were examined were found to be deficient.  And that's to say nothing about hard drives.  We don't know what they're doing.  You don't know until you look.  And there has not been enough looking yet.  So I think an interesting and useful podcast for our listeners this week.



LEO:  Can't wait.  As always.



STEVE:  And a fun picture.



LEO:  Yeah, I love the Picture of the Week.



STEVE:  Okay.  So anyway, our Picture of the Week is fun.  I found this, someone sent it to me through Twitter, and I just thought I got a kick out of it.  So it shows two little cars with cameras mounted on their roofs, implying that they are self-driving cars.  And so of course we all know that the famous question that the officer asks you when he pulls you over is "Do you know why I pulled you over?"  And so anyway, the point is that we have a police officer, who's in the second car which is labeled the police car, standing at the window of the first driver, asking, "Does your car have any idea why my car pulled it over?"  Which, yes, the world we live in today.



LEO:  Yeah, that's probably true.  Did you see that - and I don't know if this is more of a rumor than actually an announcement from Google, but their Waymo division is about to launch under a new name, which they're not telling anybody, basically Uber with self-driving cars.  You'll be able to get in a self-driving cab soon.



STEVE:  Wait, who?  Google?



LEO:  Well, Google has a self-driving division called Waymo.



STEVE:  Okay.



LEO:  It's Alphabet.



STEVE:  Right, yeah, yeah.  



LEO:  So they're going to - they may be the first to the table with - they're already testing it in Arizona.



STEVE:  So a no human in the car.



LEO:  No human in the car.



STEVE:  Wow.  That's just...



LEO:  I bet initially they'll have what they call "safety drivers."  Although there was a Waymo accident the other day that was caused by the safety driver because he didn't trust what the car was doing.



STEVE:  Oh, nice, nice.



LEO:  Yeah.  It's almost always the humans that screw things up for computers.



STEVE:  You know, it's weird.  I've mentioned that I had a little bit of a high school reunion a couple weeks ago up in Healdsburg.  And while we were having dinner, you know, we hadn't seen each other for 45 years.



LEO:  Wow.  Wow.



STEVE:  And they knew I was a techie and so forth.  And so one of them said, "You know, Steve, of everything that's happened recently in technology, what would you say you least anticipated?"  And I didn't say what I wish I had said.  I said, "Oh, no way could I have foreseen this crazy expansion in mass storage density.  That's what's enabling so..."



LEO:  Oh, yeah.  That's absolutely true, yes.



STEVE:  It's just unbelievable that we can have half a terabyte on a keychain.



LEO:  I have recordings of me saying, "By the year 2005, we'll all be using solid-state holographic memory.  These ideas of these spinning drives make no sense whatsoever."  So that's one thing.  They've really managed to increase the capacity of spinning drives beyond any reasonable expectation.  And then look at solid-state and the speed.  In a way, that's the solid-state I guess I was thinking of.  Not [crosstalk].



STEVE:  Yeah.  Though in retrospect, what I wish I had followed that up with was the other thing that just - even, I mean, I accept today's storage.  I still can't wrap my head around the idea that cars are driving themselves.



LEO:  Yeah, yeah.



STEVE:  That just seems like, what?



LEO:  It is remarkable.



STEVE:  What?  What?



LEO:  Well, and it was only 10 years ago.  Remember we even talked about on the show the ARPA Grand Challenge where these so-called autonomous vehicles would basically go 10 feet and drive off the road.



STEVE:  Well, yes.  And one of them would win if it could make it to the finish line.



LEO:  If it got to the end.



STEVE:  It wasn't a matter of, like, getting there first.



LEO:  No.



STEVE:  It was like, it didn't wander off into Colorado somewhere and disappear.  No, I'm just amazed.  So, wow.



LEO:  Which - I'm going to put a little sidebar on this - shows you how important government investment is in these technologies.  The ARPA Grand Challenge, ARPA, comes from the Department of Defense.  We wouldn't have self-driving cars at this level, I believe, if it weren't for ARPA.



STEVE:  Advanced Research Projects Agency.



LEO:  Wouldn't have the Internet without them, either.



STEVE:  No, in fact there are major technologies which sane, bottom line-oriented entrepreneurs will not invest in because they're just too far out there.  Well, that was before the days of Elon Musk, so maybe that's no longer the case.



LEO:  No.  But I do think, I mean, if the government didn't do it, it wouldn't have - no, you're right, it's not financially feasible.



STEVE:  It's just too big.



LEO:  Yeah.



STEVE:  Yeah.  Global network?  What?



LEO:  What?  No.



STEVE:  I remember the first time I heard that.



LEO:  Where's the profit in that?  Right.



STEVE:  It was like, I remember the first time I heard that, I was like, that's going to take a lot of wires.  You know?  It's like, how's that going to work?  Anyway, we now, of course, we know how it works.



We also know that Microsoft has today announced the release of last month's update...



LEO:  It's out.



STEVE:  Yes.



LEO:  1809.



STEVE:  Yes.  It's supposed to be out.  I mean, they've said it's out.  But they are releasing it slowly.  I took off...



LEO:  Yeah.  As they should.



STEVE:  Yes.  Once burned.  I took off my hold-offs and did a manual update, and I got the November security rollup and so forth.  But no sign yet of the feature update for Windows 10, which takes us from 1803 to 1809.  And that's not surprising because they stated that they're planning to roll out the 1809 feature update slowly.  So it may not appear immediately for anybody, even if you haven't tweaked your Windows 10 to say I'm going to wait to see how other people do with this before.



LEO:  I already had 1809 on everything, so I can't tell you who's getting it.  But, yeah, that's interesting.



STEVE:  Yeah.  So for what it's worth, a system which was fully qualified, which is the one I'm talking to us over, my Skype machine, I updated it to Pro so I could have control over hold-off.  And I set them to zero, I updated, I got the November stuff, but no feature updates yet.  So it'll be, as they said, I guess they're just going to really watch this one closely as it rolls out.  But they're convinced they understand everything that was happening before relative to data loss.  Now, there were two other problems that just arose that didn't make it into today's conversation because it's sort of off-topic.  But apparently Windows is losing its file associations so that it doesn't know what app is associated with what file.



LEO:  That's always been a - remember, that was a problem all the time in the old days.  All the time, yeah.



STEVE:  Yeah.  And so that came back.  I guess the adhesive on the patch got kind of old, and it just fell off.  So that's now back.  And who knows?  So have fun, you Windows 10 people.



LEO:  So mean.



STEVE:  So Privacy International is one of those companies like the EFF that is maybe a little reactionary, but we're kind of glad they're there because there needs to be somebody to push back on behalf of the consumer when profit-seeking companies don't have anybody to give them any oversight.  So they have just filed complaints against seven companies for what they consider to be wide-scale and systematic, and that is to say deliberate, infringements of data protection law.



Their announcement last Thursday, November 8th, started by saying:  "Today, Privacy International has filed complaints against seven data brokers."  Now, what's interesting is that these are not companies any of us are aware of.  We know about Equifax and Experian.  Those are two of the seven.  But the other ones are, like, huh?  There's one you can't even pronounce, A-C-X-I-O-M.  All the good names were taken.



LEO:  It's Acxiom, though; right?



STEVE:  Oh, yeah, yeah.  All of the good names were taken so it's like, A-C-X-I-O-M.  And Oracle, but I don't think that's...



LEO:  Oracle?



STEVE:  I think that's just some other - I think it's a name collision.  It can't be our Oracle because I don't think Oracle's a data broker; are they?  I think they're...



LEO:  Well, they might be.  They have their fingers in a lot of pies.  That's interesting.



STEVE:  Well, maybe it is Oracle.  And then three ad-tech companies:  Criteo, C-R-I-T-E-O; Quantcast; and Tapad, T-A-P-A-D.  And then of course Equifax and Experian.  They say, with - so these complaints were filed with - wait for it - data protection authorities in France, Ireland, and the U.K.  Meaning GDPR.  "Privacy International urges the data protection authorities to investigate these companies and to protect individuals from the mass exploitation of their data."



So my feeling is, okay, so we knew the GDPR was going to ruffle a lot of feathers.  And it does require, I mean, it's hard to browse the web now without having to acknowledge, yes, I know you're storing a cookie.  Yes, I know, you're storing a cookie.



LEO:  And that drives me crazy.  There's no reason for that.  Come on.



STEVE:  Yeah, it's like, okay.



LEO:  That's absurd.



STEVE:  It's basically they've put a clickthrough on everywhere we go now.



LEO:  Which, by the way, anesthetizes people to any real issues with tracking cookies.



STEVE:  Yup, yup.



LEO:  Okay, okay, okay.  You just click it because you want to get to the site.  It's so annoying.



STEVE:  Yes, exactly.  Fine, get out of my way, yeah, exactly.  Or it's down at the bottom, and you think, okay, what's wrong with the screen?  Something's broken.  And then it's like, oh, there's a cover-over banner at the bottom.  So click it to make it go away.



So anyway, they said:  "Our complaints target companies that, despite exploiting" - this is their language - "...despite exploiting the data of millions of people, are not household names and therefore rarely have their practices challenged.  In tandem with the complaints, we have today launched a campaign to seek to empower people to make it easier to demand that these companies delete our data."  Which of course remember is one of the GDPR requirements.



They said:  "Our complaints argue that the way these companies exploit people's data, in particular for profiling, is in contravention of the General Data Protection Regulation (GDPR), which took effect on May 25th, 2018.  Our complaints are based on over 50 Data Subject Access Requests to these companies, as well as information that these companies provide in their marketing materials" - that's where these companies get in trouble because they brag about all the stuff that they've got; and then a company, a group like Privacy International comes along and says, what? - "and in their privacy policies," they say.  "As such, our assertions are based on evidence that represents only the tip of the iceberg.  We expect and anticipate the regulators will be able to delve more deeply into our concerns regarding wide-scale and systematic infringements of the GDPR.



"PI" - that is, Privacy International - "is encouraged that the UK's Information Commissioner's Office has issued assessment notices to Acxiom" - spelled in a way you could never anticipate...



LEO:  All the consonants.



STEVE:  "...Equifax, and Experian.  We are asking the ICO to take into account our submissions in the context of their ongoing investigation and urge the ICO to widen its investigation to include Criteo, Oracle, Quantcast, and Tapad.  As part of our campaign, PI has made it easier for people to write to companies and demand they delete their data."



Okay.  So I guess this is probably the way this was going to play out is that the regulation occurs, then it probably takes someone to point the enforcers at suspects and cause cases to be filed, or I'm sure that there's like some sort of a questionnaire that'll be sent.  That'll go to the attorneys of the companies that'll respond in some fashion.  And the point is I think it probably is the case that these companies have in the past been playing fast and loose with personal data.  They're opaque, they're unknown, they probably are not actually in compliance today with the GDPR.  So it'll take some enforcement in order to make them, you know, to bring them into compliance.  And as you have noted here and on other podcasts, Leo, I've heard you talking about it, overall this is probably a good thing for the end-user in the long run.  So yay.



LEO:  Yeah, yeah.



STEVE:  Okay.  That's all I wanted to say.  It was just sort of to put that on our radar that there is, you know, Privacy International says, oh, now we have the GDPR, let's go poke a few companies that have been annoying us for a long time and stir the pot a bit, see what happens.  So we'll see.



I have a chart in the show notes showing the BCMP UPnP - BCMP is Broadcom, and UPnP of course is Universal Plug and Play, I have to be careful not to say "pray" - the UPnP Hunter Botnet.  And this is a time sequence diagram which is a nice way to organize protocol of "what happens when" diagrams.  And so we can see on the left there's something known as the Infector.  The first thing it does is send a TCP SYN packet, as in synchronize, which is the way a TCP connection is opened, to port 5431 of a candidate victim.  If the victim responds with a SYN ACK, which is an acknowledgement of the receipt of the synchronized, the TCP synchronized packet, then the Infector says, ah, found somebody who's listening to connections on port 5431.



So it then sends an SSDP Discover packet, which is the Universal Plug and Play services discovery query over the UDP protocol to port 1900.  Which, if the victim has their UPnP enabled to the WAN, will respond with an infectable URL, basically saying here's how to take over.  Here's how to take me over.  Anyway, this is all by way of suggesting to our listeners that this would be a good time because this botnet is now loose to do two things:  to make sure that your port 5431 is not responding to TCP SYN packets.  And it turns out there just happens to be a handy-dandy port probe available...



LEO:  Oh, wonder where that would be?  Where can I find such a thing, Mr. Gibson?



STEVE:  If you put into your browser GRC.com/x, which I just chose because it's short, /portprobe=5431 and hit Enter, GRC will send some TCP SYN packets to your public IP at port 5431 and will notify you a few seconds later whether anybody answered a knock at that door.  You want to either be - preferably you want to be stealth, or you want to be closed.  You do not want to be open for business because that means that your router is almost certainly already taken over.  So the first thing you could do would be - and I have a link in the show notes for anybody who wants it, or you could go to GRC.com, or just put "ShieldsUP" into Google, and it's the first link that comes up.



After you click through the first page, there's a text field in the middle.  You can just 5431 and hit Enter, and it'll also launch a port probe.  So you want to make sure that's closed on your WAN interface.  And that's not something that anything else otherwise checks for you, so it's worth doing.  Then the second stage is something, Leo, that this podcast caused the creation of.  And I think I was able to implement it so quickly that, while it was still news, while we were doing the podcast, I said, oh, and I just added that test to GRC.  That's that instant UPnP Exposure Test, which is still, I haven't touched it since I created it years ago, a big orange square that shows up on the page, and you just click it, and it does that second phase of what the botnet does.  It sends a UDP packet to the user's public port 1900 and looks for any response, and analyzes it if it's found.



That's interesting because as far as I know, this has to be done manually.  It would be possible to automate the use of this test.  But as far as I know, that hasn't been done based on the rate at which we're seeing these occur.  Yet 54,346 positive exposed Universal Plug and Play ports have been found so far since that test went live, 54,000.  And nobody should have their Universal Plug and Play exposed publicly.  So anyway, just sort of an update.  And just so that people understand this, there is a widely used Broadcom chip which exposes a five-year-old flaw, that is, for five years it's been known in lack of Universal Plug and Play authentication.



And just because I wanted to make sure people understood this, I dropped the list of affected routers onto the show notes.  And it's three pages.  So this is not Grandma's dusty router.  This is like the Who's Who of routers, including ADB, ASB, Billion, Broadcom, Clear Access, Comtrend, D-Link - several versions of D-Link, Digicom, INTEX, Linksys, Netcom, Opticom, I mean, I'm just scrolling through three pages.  TP-LINK, Trend Data, ZyXEL, I mean, just it's a Who's Who of routers.  So just it's easy to do.  Just make sure you don't have this exposed because this thing is very aggressive.  I've seen numbers as high as four million, although we believe that those are IPs which are coming and going, it's at least 100,000 at any given time, routers that are exposed.  They're being used as email spamming proxies because we know that that's what Universal Plug and Play is able to do.



We're now seeing the bad guys are bouncing traffic off of other people's routers in order to launch denial of service attacks from that user's router, that is, that appear to be coming from there, and also the analysis of this - it took a while to set up a honeypot because they had to duplicate the behavior in order to get the botnet to believe that the honey pot was actually a defective router.  But they found that they were sending email through this proxy that was being set up, essentially using this for spam.  So you don't want your IP to get blacklisted as a spammer, or you could have a hard time sending email yourself.  So anyway, worth making sure that you haven't been zapped by this.



And we have another irresponsibly disclosed zero-day, this time in VirtualBox.  A security researcher by the name of Sergey Zelenyuk discovered and posted a beautiful disclosure, I mean, I look at this.  This is on GitHub.  And you scroll through this, and it's clear how much effort went into this.  And it's just - it's a lovely disclosure of what is a difficult - you know, Leo, you've got it on the screen, so just scroll down, and you'll see pages of beautiful code snippets that have been pulled out...



LEO:  It's his rsum.  He's trying to get a job.



STEVE:  Well, yeah, you wonder about - although we'll talk about what he's said in terms of his motivation.  But so it's a difficult-to-exploit, not terribly worrisome flaw in the NAT handling code of VirtualBox's Intel E1000 NIC driver.  So, and of course the Intel E1000 is like the very popular gig Ethernet NIC now.  So because he didn't inform Oracle, the guys who were behind VirtualBox, he explains why he chose not to.  And I had to abbreviate one word because I wasn't comfortable saying it on the podcast.  You'll know when I get there.  He said:  "I like VirtualBox, and it has nothing to do with why I publish a zero-day vulnerability.  The reason is my disagreement with contemporary state of infosec, especially of security research and bug bounty."



And so he says - and then he has sort of three major topics and one with some bullet points.  He says:  "First of all, wait half a year until a vulnerability is patched is considered fine."  He says for the second point:  "In the bug bounty field these are considered fine."  And we have then four sub-points:  "Wait more than a month until a submitted vulnerability is verified and a decision to buy or not to buy is made.  Change the decision on the fly."  He says:  "Today you figured out the bug bounty program will buy bugs in a software.  Week later, you come with bugs and exploits and receive 'not interested.'"



Third:  "Have not a precise list of software a bug bounty is interested to buy bugs in.  Handy for bug bounties, awkward for researchers."  And fourth point:  "Have not precise lower and upper bounds of vulnerability prices.  There are many things influencing a price, but researchers need to know what is worth to work on and what is not."  And, finally, his third main point, he says:  "Delusion of grandeur and marketing BS" - that's where I chose to use an abbreviation - "naming vulnerabilities and creating websites for them; making a thousand conferences in a year; exaggerating importance of own job as a security researcher; considering yourself 'a world savior.'"  And then he says:  "Come down, Your Highness."



LEO:  Wow.



STEVE:  Yeah.



LEO:  Geez.



STEVE:  Sour grapes much?  He says:  "I'm exhausted of the first two, therefore my move is full disclosure.  Infosec, please move forward."  So as with the two cases we've seen of SandboxEscaper previously, and her release those two zero-days, and given the quantity and quality of the work that both she and Sergey have done, and Sergey's is every bit as impressive, it's understandable, I guess, that they might feel jerked around by the bug bounty system and decided obtaining their 15 minutes of fame and notoriety is worth more than fighting for some cash.



But reading between the lines it's clear that probably, if they discussed with Oracle, if this guy had discussed with Oracle what he found, they might have - and he would have said it, I guess, maybe in general terms because he wouldn't want to give it away, or they might just jump the gun - they might have said, no, we're not interested.  It's not that big a deal.



So anyway, we're beginning to sort of see this alternative with three of these irresponsible disclosures of zero-days in a relatively short period of time.  So it's certainly true that, if the developer wants attention, they're going to get more attention like this than if they responsibly disclosed, wait for the problem to be solved, and then obtain a footnote of acknowledgment in the eventual security update.



But anyway, to the specifics here, all versions of VirtualBox from 5.2.20 and prior are vulnerable.  This is true for any host because the bug is in a shared codebase.  And it's true for any hosting, that is, for any host OS and any guest OS.  So it's an any/any in terms of what's running on what platform with VirtualBox.  And the VM configuration which is vulnerable is the default if the network card is the Intel PRO/1000 MT Desktop, which is a very popular network card, and running in the default NAT mode.



He says in his disclosure, under "How to Protect Yourself," he says:  "Until the patched VirtualBox build is out, you can change the network card of your virtual machines to PCnet," he says, "either of two, or to Paravirtualized Network.  If you can't," he says, "change the mode from NAT to another one."  He says:  "The former way is more secure."  And then just sort of briefly I'll quote him, saying:  "Introduction," he says, "A default VirtualBox virtual network device is Intel PRO/1000 MT Desktop, and the default network mode is NAT."



He says:  "The E1000 has a vulnerability allowing an attacker with root/admin privileges in a guest" - so you first have to be admin in the guest - "to escape to the host's ring3."  So that is the application layer on the host.  He says:  "Then the attacker can use existing techniques to escalate privileges to ring0 via /dev/vboxdrv."  But again, so you have to have admin in the guest.  Then you get to non-admin in the host.  So I don't know, you know, like what went on behind the scenes.  Maybe this just didn't rise to the level that Oracle felt it was worth getting all worked up about, and so this guy said, fine, I'm just going to go public.



He says, anyway, it is a VM containment breach which allows us to get out of the VM.  That's not good, but it doesn't feel like it's the end of the world.  There's a workaround, and I'm sure, I mean, especially given this beautiful disclosure that he put together, that there's already a fix in the works.  So anyway, another zero-day dropped.  In this case it's not nearly as much of a concern as something we've seen from SandboxEscaper, both of which were quickly leveraged.



And Leo, you're going to love this one, from our friends at Cloudflare.  The site is - I love this - 1.1.1.1.  Yes, you put https://1.1.1.1, which means they got the top-level domain "1."



LEO:  Oh, I like it.  That's right.  They could do other dot one stuff, couldn't they.



STEVE:  Yeah.  So they have...



LEO:  So it's not an IP address.  When you're entering that, that's the actual TLD.



STEVE:  Yes, right.  Wait.



LEO:  Or is it?



STEVE:  Wait, it's https.



LEO:  Let me ping it.  Let me ping it.  Yeah, but that's, well, I don't know.  That's an interesting question.



STEVE:  It's HTTPS, so they're not going to get a security certificate unless they've got a cert for that.  That's a really good question.  I didn't even think to look at who issued the certificate for that when I brought it up.  Let's see.



LEO:  It is at 1.1.1.1, but we knew that.  Let's see.  That's an interesting question.  I guess I could do a WHOIS.  Right?



STEVE:  Ah, DigiCert gave them a certificate.



LEO:  For the top-level domain or for the IP address?



STEVE:  You can't get a certificate for an IP address.  So the web - let's see, let's see.  View, save password, blah blah blah, view certificate.  Okay.  Well, whoa, whoops.



LEO:  No, [crosstalk] AP NIC.  No, this is the Asia Pacific NIC.



STEVE:  Yeah, yeah, yeah.  And the common name is *.cloudflare-dns.com.  So that's where they got their certificate for it.



LEO:  So they don't own dot one because everybody might - by the way, Cloudflare's going to do a domain registrar; right?  For their customers.



STEVE:  Yes.  In fact, we covered that a couple weeks ago.  They have opened it up to all of their customers, not just their enterprise customers.  Okay, so here's the cool thing.  You go to, with iOS or Android, https://1.1.1.1, and it is now offering you an app, either for iOS or Android.  I tried it.  And you download the app.  I used the iOS.  You download it.  It explains it's going to create a VPN profile - it's not a VPN connection, a VPN profile.



LEO:  Oh, clever.



STEVE:  Yes.  That's the way it makes it easy for them to change your DNS settings.



LEO:  And Apple approved this, obviously.



STEVE:  Yes.  And it then establishes DNS over HTTPS, Leo.  It's not just changing your DNS.  It is encrypting and privatizing all of your iOS or your Android mobile devices' DNS queries.  And in the option screen the default is DNS over HTTPS.  You can, if you want, switch it to over TLS.  Both are emerging standards now.  And there's other options and features.  And so it changes your device to 1.1.1.1 and 1.0.0.1, which is their backup DNS.  And essentially it establishes an HTTPS tunnel through which all DNS goes.  And it logs it.  You can look at the log.  And what shocked me was how, I mean, like the moment I set this up, I went to look at the log, and it was like there were already 50 DNS queries from all kinds of crap.



LEO:  Interesting.



STEVE:  In my phone.



LEO:  You might want to note that other VPN apps will not work when you're using their VPN profile.



STEVE:  Right.



LEO:  So it will eliminate the use of VPN for you.



STEVE:  Right.



LEO:  Yeah.  But that's, you know...



STEVE:  Anyway, I was very impressed.



LEO:  This is great.



STEVE:  It's very cool.  So takeaway is anyone who is self-conscious...



LEO:  Oh, it's just a switch to turn it off if you wanted to.



STEVE:  Yeah.



LEO:  So that's good.  So if you wanted to use a VPN, you wouldn't need it anyway.  You could do that.  And then if you wanted you could turn it on.



STEVE:  Correct.  Correct.  And somewhere, see if you can find the log.  I'm not sure where that log was.  It's got a nice little icon that I liked, and I'm...



LEO:  Yeah, here's the log.  It's under the menu.



STEVE:  Ah, good.



LEO:  Already I have quite a few things in here.



STEVE:  That's it.



LEO:  How is that possible?



STEVE:  It's shocking how much DNS a device is making.



LEO:  Most of it's to Apple or Apple-related sites like Akamai.  But still, wow.



STEVE:  Yeah.



LEO:  That's fascinating.  Holy cow.  Nice.  This is great.  So I wasn't going to - I was tempted to make this my pick of the week for iOS today on MacBreak, but I wanted to hear from you first.  So thumbs up; right?



STEVE:  Thumbs up.  Now, on their page they say:  "Privacy First:  Guaranteed."  They say:  "We will just remind our listeners about 1.1.1.1 through Cloudflare," which of course any of us could set our PCs to.  "We will never sell your data or use it to target ads.  Period.  We will never log your IP address the way other companies identify you," they say.  "And we're not just saying that.  We've retained KPMG to audit our systems annually to ensure that we're doing what we say.  And frankly," they say, "we don't want to know what you do on the Internet.  It's none of our business.  And we've taken the technical steps to ensure we can't."



Now, what I take a little issue with is they said "Faster than anything else; 28% faster, in fact."  They said:  "We've built 1.1.1.1 to be the Internet's fastest DNS directory.  Don't take our word for it.  The independent DNS monitor DNSPerf ranks 1.1.1.1 the fastest DNS service in the world.  Since nearly everything you do on the Internet starts with a DNS request" - and obviously many things you don't do start with a DNS request...



LEO:  Right, right.



STEVE:  "...choosing the fastest DNS directory across all your devices will accelerate almost everything you do online."  Now, the reason I take a little issue with that is that I happen to also be the author of the Internet's sort of standard benchmark for DNS.  And when this was announced, it may have been because it was immediately at announcement, there were many of our listeners who reported that they had faster DNS than 1.1.1.1.  But that was because they were still - I think it's because Cloudflare was still rolling out the endpoints, that is, what determines the DNS speed once you've got a fast server is its proximity to you.  And that's what we are finding was that people were reporting that, nice as this sounded, to have all the privacy benefits and all, if there wasn't a node near them, then their traffic still had to go a long way in both directions, and that slowed it down.



LEO:  It's also HTTPS.  Would that be slower, too?



STEVE:  Not once the connection's established.  What this does is it brings up a static connection.  So it does the TLS negotiation.



LEO:  Only once at the beginning.



STEVE:  Yes.  And then it maintains a persistent connection so that it's just quick little packets zipping back and forth in order to get the work done.  So what I would suggest is that maybe it's time to revisit.  If you put in DNS Benchmark to Google, once again GRC is the only thing that comes up because we pretty much own that side of the world.  I solved that problem correctly once.  And it's worth taking a look at again because you may find, as our listeners may have at announcement time, that, yeah, we'd like to use it, but my own ISP's DNS server that's, like, next door to me is still fastest.



On the other hand, your ISP may very well be selling a lot of information about you because they'd like to make money, and there are companies that the folks like PI, Privacy International, are going after that would like to not have people able to do that.  So anyway, Leo, yes, I think this is 100% win for iOS and Android mobile devices for anyone who's interested in the, first of all, speed.  And I absolutely, we know the Cloudflare guys.  There's no question that they are honoring their commitments about privacy.  None whatsoever.



LEO:  So I should point out, it doesn't do this on the iPhone, but on the iPad it does show that it has little VPN designation.  This must be something new they're doing on the iPad.



STEVE:  Ah, didn't notice that.



LEO:  But you're not really on a VPN.



STEVE:  Right, right, right, right.  I see it, too.  I hadn't noticed it.  Actually, I like that as proactive acknowledgment that I've got my DNS redirected.



LEO:  Yeah.  But don't...



STEVE:  Ooh.  Oh, oh, oh, Leo.  Go to GRC Spoofability Test.  I was going to mention in the Miscellany that it's back online.  And it didn't occur to me, and I'll explain when I get there, but just put DNS Spoofability into the Google.



LEO:  Yeah.  The Google always know, yeah.  Okay.  And so if I run this with this on - I've got Cloudflare on.  So if I just initiate standard DNS Spoofability, searching for DNS nameservers used by your systems - and this takes a little time, of course, because it's got to do some round-tripping here.



STEVE:  Oh, it's doing, well, it's doing hundreds of queries.  Yup, I'm doing it now, too.



LEO:  Two servers found; 456 queries received.  I'm doing this on the iPad, which is an interesting...



STEVE:  As I am.  Right now I'm holding my iPad also.  I found I had two servers found, 409 queries received.  So what happens is, if it finds no additional servers, then it'll do three more lines where it - because it's continuing to look for any - oh, I found now I've got four servers found and 1046...



LEO:  Is that good or bad?



STEVE:  Well, it's why I designed the test this way.  It's not bad.



LEO:  It's kind of thoroughly looking, yeah.  



STEVE:  Yes, exactly. So that on the third line it found zero additional ones.  And so it'll do that three times to make sure that no more reluctant servers come in.  But then it's going to show us what it knows about the servers that we're using, and they will not be your ISP's.  They should be, yup, and fourth line through now, zero additional.



LEO:  So to be clear, your traffic is not encrypted through a VPN when you're using this.  What's encrypted through a VPN is merely that DNS query.  And your traffic is traveling normally over the public Internet.  Just so people don't get confused when they see that VPN icon.  You're still on the public Internet.



STEVE:  Correct.



LEO:  Oops, server stopped responding.  Safari did not like the delay.  So I don't know.



STEVE:  Oop, found two more servers on the sixth round of testing.



LEO:  Yeah.  You didn't time out.  I did, unfortunately.



STEVE:  Ah, no, I did time out.  "Could not open the page because the server stopped responding."  Interesting.



LEO:  That's Apple's defaults.



STEVE:  [Growling]



LEO:  [Growling]  So we may not know.  What would I expect to see?



STEVE:  We don't know until we find out.



LEO:  Well, I'll try to run it on the desktop here and see what happens.



STEVE:  And I noticed that our listeners are doing this because I see a large traffic spike. 



LEO:  All of a sudden everybody's...



STEVE:  GRC is sending lots of DNS queries out.



LEO:  At firs they were just downloading your DNS Benchmark.  But now they're doing this, too.



STEVE:  Now they're doing [crosstalk].



LEO:  You really don't mind hitting those servers, do you.



STEVE:  No, no.  So anyway, I think this is all good.  Privacy while you're on the road.  Very, I mean, it's so simple to set up that you can easily tell your weekend Tech Guy listeners how to do this, and it prevents anybody from - now again, you're right, it's not preventing them from seeing where you go.  But it's preventing them from seeing the domain names that you're looking up.  And of course that's a big privacy leakage.  So it's worth patching it.  And if they are right, it's faster.  And of course everybody wants their stuff to be faster.



LEO:  Yeah, cool.  So I'm doing it on the desktop now.  I'll do it on the Mac, and I'll try it with Chrome on a Windows machine, too, just - oh, it won't - it will only let me do this from one IP address, once at a time.  So I'll wait until it's done.



STEVE:  Ah, that's right.  It does.  I remember.



LEO:  You're too smart.  Damn, he's smart.



STEVE:  Well, in fact, you'll like the charts it comes up with because it remembers - I developed this after Dan Kaminsky showed us that DNS servers were not randomizing their queries. And it was the lack of random query data which allowed someone to guess what the query would be and thus spoof a reply in order to do DNS spoofing, and so thus DNS Spoofability.



LEO:  Episode 155, if you want to hear more.



STEVE:  Ah, cool.  So while we're doing that, I'll mention just that it was sort of an interesting bit of coincidence that Microsoft published a note about BitLocker's exposure of its keys in RAM.



I got it working, the Spoofability Test, on iPad.



LEO:  Oh, look at that.  That's cool.



STEVE:  Yeah, that's the - what I did was I remembered there's a custom test.  And so you're able to customize the parameters.  And so I've made it less patient so that Safari wouldn't time out.  And so I used...



LEO:  And so our antispoofing safety is excellent.



STEVE:  Whoa, look at that.  Very nice.



LEO:  That, of course, is because Russell really does a good job of protecting us.  4,264 queries.  14 IP addresses.  Do you want to see a very even distribution like that?



STEVE:  Yeah, because - yeah.  And in fact what the histograms down below are the number of times we saw each of the different bits of the 16 bit.  And so they're all very low and very flat.  You don't want to see any of those being high.



LEO:  Very good.



STEVE:  Very nice.



LEO:  We are not vulnerable to the Kaminsky Exploit, I'm happy to say.  Now, I'm not running 1.1.1.1.  But this is an example of what it would look like.  And you're getting - from 1.1.1.1  you get the same kind of...



STEVE:  Well, I actually saw what looked like some poor query transaction IDs, that is, the right-hand chart showed some lines in it a couple times.  But I just reran it.  It was actually a different server that I got the first time. 



LEO:  Okay.



STEVE:  So it was sort of interesting.  And actually, yeah, I only found one server because I told it not to be very impatient so that it would finish.  But if you want to try it on your iOS device, there's a link at the bottom of the first page that's custom DNS, the custom spoofability test, and you change the last three numbers to 10, 0, and 1, and then it will succeed.  



LEO:  And this is OpenDNS, by the way.  That's what we use.  And that's pretty good.  So OpenDNS would be another good choice for people if they don't get good results from quad ones.



STEVE:  Well, and I think it was just - I retested, and everything came out absolutely great.  So it might have just been a weird fluke the first time.



LEO:  I'm using quad ones now.  I like it.  I'm going to use it on all my mobile devices.  That's great.  That's cool.



STEVE:  Yup.  So coincident with the interesting discovery that hard drives are not doing a good job protecting their own secrets, Microsoft released sort of an advisory, a security advisory, that was warning something that we've talked about for years, actually, which is that it's an interesting fact that both the earlier 1394 so-called, god, I want to say Lightning, but it's not, the Firewire, the 1394 Firewire interface, and then the later Thunderbolt, both allow the connected device to do essentially DMA, Direct Memory Access.



And this has been a security concern because it would mean, and we've talked about this, again, through the years, it would allow somebody to connect a Firewire or Thunderbolt device which is intelligent, it's not just a dumb drive or a camera or something, it's actually a smart controller which then is able to go over the serial interface, the Firewire or Thunderbolt serial interface, and suck out the contents of the system's RAM.



The reason that's an issue, and we'll be discussing this here in a minute relative to drive encryption and decryption, is that most systems, well, in fact BitLocker specifically, in order to do its job it has to have its key in memory.  That is, somewhere in the system memory is the key in use the whole time your computer is on, in order to read and write to and from the drive.



Now, that's not unique to BitLocker.  I mean, this is why, for example, Heartbleed that we talked about a couple years ago, the Heartbleed attack was opportunistically grabbing RAM from a server, and there was some probability or some chance that the RAM it would grab was valuable, that it contained the server's key.  Which is what made it such a problem and a great concern.



So anyway, the point is that Microsoft says:  "A BitLocker-protected computer may be vulnerable to DMA, Direct Memory Access attacks, when the computer is turned on or is in the Standby power state.  This includes when the desktop is locked.  BitLocker, with TPM-only authentication" - which is sort of the default, so just it's encrypted and it's not harassing you - "allows for a computer to enter the power-on state without any pre-boot authentication.  Therefore, an attacker may be able to perform DMA attacks."  And so the idea would be you'd turn the computer on, and there it is.  And now, if you have DMA access, it can get to your key.



They say:  "In these configurations, an attacker may be able to search for BitLocker encryption keys in system memory by spoofing the SBP-2" - that's the Serial Bus Protocol - "hardware ID by using an attacking device that is plugged into a 1394 port.  Alternatively, an active Thunderbolt port also provides access to system memory to perform an attack.  Note that Thunderbolt 3 on the new USB Type-C connector includes new security features which can be configured to protect against this type of attack without disabling the port."



And we've talked about that when we've brought this up before.  Even the MAC and Firewire had some provisions for limiting the range of memory that was available to DMA, though in many cases that wasn't the setup by default.  So then they say:  "This article applies to any of the following systems:  systems that are left turned on; systems that are left in the standby power state; and systems that use TPM-only BitLocker protection."  And they go on to talk about how you can reconfigure what choices you make in order to protect yourself against this.



I've got a link in the show notes for anyone who's interested.  But I thought I wanted to bring this up because the researchers who did not know about this because their research predates the publication of this from Microsoft, bring up exactly this point when they are very soberly talking about what it is they found and how they feel about software versus hardware full-drive encryption.



So that wraps up our news for the week.  I had under Miscellany, interestingly enough, the first item under Miscellany is GRC's DNS Spoofability system is back online.



LEO:  Nice timing.



STEVE:  Yeah.  And what had happened was I didn't realize it had gone down, but someone - I saw a couple mentions in Twitter, and I thought, okay, I've got to look into that.  And then I found a posting, someone had cross-posted from GRC's DNS newsgroup into DNS SQRL because they knew that's where I was spending all my time.  And that really caught my attention.  So finally I thought, okay, I've got to see what's going on.  And so I fired it up; and, sure enough, it didn't work.  And so it's like, oh, crap.



So last weekend I thought, well, maybe I just need to reboot the server.  The GRC server had not been booted since last November because it never crashes, and there are no memory leaks.  And if it's not broke, just leave it alone.  So I thought, okay, well, be a good time to update it and maybe just something got a little cranky.  I brought it back up.  It didn't fix it.  It turns out, and I don't really understand exactly what the sequence is, because I don't understand why the reports are it's been working until just a few months ago.



But the problem was the compression, the HTTP compression, which I've always been a big fan of and has always been enabled, it stopped sending out incremental pieces of the page.  And the spoofability page sends little dots out on the page to show you its progress as it's going.  And the dots represent DNS - the dots come out when the user's browser asks for zero-size GIF or GIF, depending upon what side of the fence you're on, images.  And the GIF images come from a fake DNS server that's 13 characters of random dot DNS dot GRC.com.



So what I built back then was a pseudo DNS server which grabs those queries from the user's browser that needs to look up the IP address of this crazy DNS name.  And when the request is made, I then return a CNAME, which is a canonical name, of a.a.a.a.a.a, like as long as it can legally be.  And then so that goes back to the user's DNS server that says, oh, you're kidding me.  And so it says, okay, what's the name server for that?  And so this is a means of forcing hundreds of queries in a short period of time from all the DNS name servers that the user has serving them, that is, the user's browser has serving them.



Anyway, it broke because something about the compression in Windows stopped doing incremental releases.  I don't know what it was, but as far as I know it's nothing I did.  Anyway, I made an exception to simply turn the compression off for the active code that my site uses, that is, that's underneath the /x, and then it just came back to life again.  So anyway, for anybody, I know that there are listeners of this podcast who have been discomfited by the fact that GRC's DNS Spoofability system which they use from time to time had been offline for months.



So I wanted to let everybody know it's back.  And the timing is nice because people can play with it with their iOS devices.  And if you want to play with it with your iOS device, and you find that Safari times out, I have a custom version, and you can set - the last number, there's like five numbers which characterize the nature of the test.  If you set the last one, it defaults to four, which is how long it waits for more servers to show up.  If you set it to one, then it's less patient, and you'll get some results.  And there's somewhere else where there's a four you can set to a 10.  Anyway, so it's set 47, 48, 10, something, and then one, and then it'll work for you.



Also, Simon Zerafa, a friend of the podcast, pointed me to something that I was just starting to play with, Leo, when you decided it was time to wrap up MacBreak Weekly today, which is really interesting to me.  And so I wanted, to any of our listeners who are webmasters, that is, who are in charge of their own web servers, Content Security Policy, CSP, is a neat facility which allows a website to tighten up its security by telling the browser what resources it's sourcing from where.  And so what it does, the idea is that so the website says this is our Content Security Policy.  For example, scripts should only come from here, and images should only come from there, and so forth.



The problem is, adding that afterwards, that is, on a live site, is nerve-wracking because it's so easy, you know, the point is you want tight access control.  But you don't want too tight access control or you'll block some valid assets of your site.  And so what this does is, if you really have your CSP policy locked down, and it's something you could easily do from the beginning, but it's hard to add afterwards, if it's locked down, it's very difficult for anything that somehow gets loose on one of your pages to get up to mischief because it's like it just can't - it has no freedom and flexibility, or vastly less.



So the point of this is there is a widget, which Simon pointed me to.  It's called - for some reason it's called Laboratory, although it's Laboratory by Mozilla, so maybe it's Mozilla's.  I haven't had a chance to look at it closely.  But it's a widget for Firefox.



LEO:  Yeah.  This write-up is from Mozilla.



STEVE:  Oh, okay, cool.  So what it does is you go to your site.  You activate this thing.  And then you browse around your site.  You visit pages.  You do stuff.  And it's learning where your different types of assets come from, and it builds for you the Content Security Policy for your site, based on its real-time interactive behavior while it's not being attacked.  So the result will be a CSP, like a long, crazy-looking header string, which you could then add to your web server without breaking anything, so long as you didn't do anything that you hadn't done while you were roaming around your site.



Anyway, I'm going to play with it.  I'll know more about in the future.  I would love to have, for GRC, a tighter Content Security Policy because why not?  But I'm just afraid to mess with it because my site's old, and stuff's coming from all over the place.  This would help anybody who wants to add this to an existing site to do so.



And the last piece of Miscellany, Leo, is Peter Hamilton's "Salvation."



LEO:  Have you finished it now?



STEVE:  Finished it.  And I would characterize it - I gave it five stars.  I understand the negative comments in the reviews.  I mean, there are some people who only gave it two because they just want to drop right in to start blowing stuff up, and this doesn't do that.  I would argue this entire first book is setting the stage for the future.  And I don't know whether - I don't know if it's a trilogy, or if it's more.  And maybe three quarters of the way through I was sort of thinking like, okay, I'm not going to be impatient.  I'm just going to know this is Peter, and he's going to take me for a long journey.



And the only problem is now I have to probably wait a year.  And this is what happened with "Pandora's Star," remember, is we got this fabulous first book, and then it was like, okay, wait for the second one.  It's like, oh.  And it was a year later.  I had to reread "Pandora's Star" in order to remember what had gone on so that we could get to "Judas Unchained."



So anyway, for what it's worth, it really is good.  And one of the things that Peter sometimes does, he did this at the end of "Fallen Dragon," is the whole book was a build-up to a surprise which was kind of teased in different ways as you're going along, and it's like, okay, I don't know what that means, but I'll find out.  Okay, I don't know what that means, but I'll find out.  All was answered by the end.  So there's nothing at all cliffhanging except that now I want to know what's going to happen next.  So I've got to wait a long time.  The good news is the tenth book in Ryk Brown's second of five series of 15 books each - so, yes, that's 75 books.



LEO:  Oh, my god.



STEVE:  It came out.  It was released on October 28.  So now that's what I'm reading.  So I've got plenty to keep me busy until Peter Hamilton figures he's going to tell us...



LEO:  I almost don't want to get involved in anything that long because then I won't read anything else for years. 



STEVE:  I know.  His stuff is so good, though, Leo.  I mean, we have always enjoyed it.  I could certainly understand somebody deciding to wait until the whole series is done.



LEO:  The Hamilton stuff, yeah, yeah.



STEVE:  Before starting.



LEO:  I think that's my choice.



STEVE:  Oh, oh, you mean...



LEO:  Oh, you're not going to wait for Ryk to finish 85 books, are you?



STEVE:  He does spit these things out.  I don't know how he does it.  They're not full of typos, I mean, they are really well written.  And they are, for anybody who gave "Salvation" two stars, you've got now 25 books in the Ryk Brown series, the 15 books of the first set and then the 10 books of the second.  So there's 25.  That ought to hold you for a while.  And they are rock 'em, sock 'em, you know, they're like right down space opera, classic space opera.  "Salvation," though, I'm definitely glad I put the time in for this first one because he's created another amazing future.  So I want to know about it.



LEO:  JammerB loved it, too.  He said, "I can't till Steve finishes so I can talk to him about it."



STEVE:  Cool.  Well, speaking of finishing, Nicholas Kasprinski sent me a tweet that is again apropos of today's topic.  He says:  "I have a drive that is encrypted with Windows BitLocker" - well, or maybe not.  But, he says:  "...but is not able to boot up.  Can I still run SpinRite against it to decrypt and recover the drive's data?"  And as we know, the answer is yes.  SpinRite doesn't care what data you have on the drive.  It just cares whether the drive is happy with its ability to read it.  And if the drive is not happy, even when it seems happy, but we ask it a little more, are you really, really sure?  You really sure you're happy with this?  And if it seems a little questionable, then we fix it.



And if the drive says, no, I'm not happy, we say, oh, come one, try again.  No, I'm not, really not.  No, no, no, no, no.  Just give it, you know, look again.  And we do lots of things to get the drive to finally say, okay, fine.  One last time you can have the data back.  And that's all we need, one last time, because then the drive fixes it, it relocates the sector, brings in a spare, we write the good data back, and the drive boots again.  So Nicholas, yes, SpinRite is the only thing that I know of which is able to fix a BitLockered drive which will not boot.  So good luck.



I got four little bits of closing-the-loop data.  Davy Jones, tweeting from @9arsth, said:  "Need a quick yes-no answer.  Applied the sandbox to my Win10 home box.  Tested, it works.  Then applied to W7 Pro.  Command line said it was successful, but when tested, no child process under MsMpEng.  So not possible?"  And so decoding that, I realize he's talking about what we talked about last week, which was the sandboxing of Windows Defender.  And I probably wasn't clear, or maybe I didn't say it at all, that it's only Windows 10.  This is not something Windows 7, any version of Windows 7, in Davy Jones's case Windows 7 Pro.  This is more and more we're going to, unfortunately, see Microsoft offering things that we Windows 7 users wish we could have, but we've got to go to Windows 10 in order to get them.  And, yeah, okay.  No, thank you.



But anyway, so the point is that that command line was a registry entry which only Windows 10 knows about.  You can certainly put that in Windows 7, but it'll have no effect.  So, yes, it is the case that the Defender in the sandbox, which is absolutely what I think is the ultimate solution now for integrated, high-quality AV that doesn't bring with it a risk of creating a larger attack surface for viruses, it's Windows 10 only.



David P. Vallee tweeted, oh:  "In the SQRL paradigm, what happens if there's a system-wide collapse?"  He says:  "People would have hundreds of accounts with no record of their credentials because there was reason to record them" - so he must mean because there was no reason to record them when they were created.  And David, you'll be glad to know that there's no such thing as a system-wide collapse because there's no system to collapse.  So what is an arguably somewhat mixed blessing is that there is no one to go to if, like, you forget how to authenticate yourself to SQRL, or if your SQRL identity is lost.  As a consequence, a huge amount of effort has been put into recovering from those events.  But there isn't a system.  There is no central headquarters.  There's no main arbiter.  And I would argue that's the whole point.  This is a two-party solution between you and the websites you visit.  So there is nothing to collapse.



Skynet says:  "Hi, Steve.  With Windows Defender sandboxing, how do you turn it off if you need to?  Do you retype the command with a zero at the end instead of a one to disable it?"  And that was a great question I forgot to mention.  Yes.  That's exactly what you do.  In the command line which sets the registry key value to one, you change it to zero and then restart your system, and it will no longer be sandboxed.



And, finally, Edward Evans says:  "Hi, Steve.  Regarding SQRL, how would I be able to have a bot associated with my user account without sharing my core secret?"  He says:  "For example, a web watcher that is watching on the far side of the auth wall."  He says:  "Love Security Now!.  Thanks to you, Leo, and the gang, I feel that I actually 'get' security now.  Thanks for all you do."



And that's a really interesting question.  I mean, that's definitely an edge case that I hadn't thought about.  So he's saying how would he be able to have a bot associated with his SQRL identity without sharing his core secret, a web watcher that is watching on the far side of the auth wall?  So, okay.  So first that would assume that he wants to delegate his identity to something which presumably has to log in in order to do whatever it needs to do.  So this is not something that the SQRL client itself supports.



But SQRL creates a per-site sort of - you can think of it like as a sub-identity, that is, from your master identity, which is mixed with the domain name of the site you are wanting to authenticate to.  A secondary identity is created which is per-site.  It's unique for every site you visit.  So, and it doesn't have to be a secret.  So, I mean, for example, that's the identity.  The public version of that is what you give to the site to declare who you are.  The private component of that is how you sign a challenge from the site saying prove to me that this is who you are.



So it would be possible, and somebody could easily create it, I could or anybody could, sort of something that peels off just one site-specific identity from your master SQRL identity that would thus only work on that one site.  And you could give that to a bot, which could then use that identity only valid for that one site in order to operate on your behalf without worrying that it's able to do anything else.  It would not have access to your larger SQRL identity.  So what's cool is that so far all of these sorts of questions we've had answers for, which gives me hope for SQRL.  And after working on it for as long as I have, I do have hope for it.  But we'll see how it goes once we turn her loose.



LEO:  Steve, I just thought before we get back to the show, kind of a tragic note, I know you've been around long enough to know the name Bill Godbout.



STEVE:  Oh, my goodness, yeah.



LEO:  The S-100 computers, the Godbout Electronics and CompuPro.



STEVE:  Yeah.



LEO:  He passed away in the Camp wildfire at the age of 79.



STEVE:  Oh.



LEO:  He was one of the, now we're learning, dozens of victims of these wildfires.



STEVE:  He was a real engineer.



LEO:  Amazing guy, 79 years old, Bill Godbout.  He was an advocate for S-100 computers and industry standards and CPM days.  He was a parts supplier for electronic music projects, and a big part of the computer revolution.  In fact, in "Hackers," Steven Levy's book, he writes about Bill Godbout.  He said he bought junk on a more massive scale; usually government surplus chips and parts were rejected as not meeting the exacting standards required for a specific function, but perfectly acceptable for other users.



"Godbout, a gruff, beefy, still-active pilot, who hinted at a past loaded with international espionage and intrigues for government agencies whose names he could not legally utter would take the parts, throw his own brand name on them, and sell them, often in logic circuitry kits you could buy by mail order."  That was the wild west days of computing.  A legend, and a very sad end to one of the greats in the business.  So I thought I'd pass that along because I knew you would know the name, even if many others in our audience don't.



STEVE:  Okay.  So the title of the 16-page, very well written and wonderfully detailed research paper from two security researchers at the Radboud University in the Netherlands is titled:  "Self-Encrypting Deception:  Weaknesses in the encryption of solid-state drives."  And I will remind everyone  that, essentially, they could also have said weaknesses in 100% of the drives we have looked at, which is to say they looked at three drives from Crucial, three internal and two external drives from Samsung - I'm sorry, two internal and two external drives from Samsung.  Collectively they make up about half of the market.  So they are popular drives.  And they found the security wanting, to put it mildly.



So there are two standards in the industry.  There's the so-called ATA security standard, and what's known as the Opal, O-P-A-L, standard.  Opal was produced by the Trusted Computing Group.  ATA security was the original.  And that's what drives have had for years, which essentially locks and unlocks an ATA drive as a whole, based on a user-supplied password.  And, for example, any of us that have been using computers for a long time, oftentimes our BIOS will allow us to supply a drive password.  And so the idea is that at boot time the BIOS supplies the password, which then unlocks the drive so that there even is a boot sector available to boot from.



And so the protection is that, if the drive is removed from the device, it doesn't have this password that you have assigned in the BIOS.  And oftentimes you have to authenticate, the user has to authenticate to the BIOS in order for the BIOS to then unlock the drive.  So there's boot time protection.  The successor to that original, the so-called ATA Security Feature Set, the ATA spec groups things in feature sets.  So like there's the smart feature set, the security feature set, and so forth.  That was created by this Trusted Computing Group, and they're the same people who did the TPM, right, the Trusted Platform Module.  They are a committee that generates these standards.



Opal defines a much more complex and unfortunately much more difficult to implement encryption facility.  And so, for example, if a much simpler and more straightforward ATA system could be said to be a one-to-one scheme where one password is used to unlock one region, which is to say the entire drive, then the Opal system would be described as a many-to-many system, where a drive's storage space can be freely subdivided into arbitrary bounded regions, each of which can be individually encrypted so that they are accessible under multiple and individually differing passwords so that different passwords can selectively decrypt various subsets of the entire drive.



I'm not kidding.  I mean, that's this disaster that we have.  And it's a disaster because nobody uses it; yet the drives, in order to be compliant, and do have the checkbox on the marketing brochure, have to support it.  And if they're going to support Opal, if they're going to say that they support the TCG, the Trusted Computing Group's Opal standard, then they have to do that.  Despite the fact that operating systems don't care about that.  They just want to unlock the whole drive.  But wouldn't it be nice if it were actually done in a secure fashion?



So, in digging around a little bit, I ran across an article written seven years ago, in 2011, titled "The Pros and Cons of Opal-Compliant Drives."  And in that article its well-meaning but naive author states, quote:  "Hardware-based encryption is very secure, far more secure than any software-based offering.  Software," he writes, "can be corrupted or negated, while hardware cannot.  Software runs under an operating system that is vulnerable to viruses and other attacks.  An operating system by definition provides open access to applications, and thus exposes these access points to improper use.  Hardware-based security can more effectively restrict access from the outside, especially to unauthorized use."



And I wrote here in the show notes:  "This charming view of the world predated the Spectre and Meltdown hardware problems by seven years, and it assumes that hardware is magically perfect because it is harder" than software.  It would be nice to live in that world.  Of course now we no longer do.  So the number one lesson we learned from this terrific research, that is, the research, the 16-page paper, is that the fact that a drive sports a given interface and supports a given security standard whose marketing brochures boast of its security and encryption is entirely decoupled from and means absolutely nothing about the actual delivered security in the face of a determined effort to obtain the drive's content.



Is the encrypted drive unreadable to the casual passerby?  Absolutely.  Is it unreadable to someone who is highly motivated to gain access to the drive's contents?  That's the interesting question this pair of researchers set out to determine.  And the initial surprising successes they had kept them going.  Now, just remember years ago when we were talking about TrueCrypt and how there was a drive in Brazil that was encrypted with TrueCrypt, a pure software solution, which because apparently a good password was used, no authorities there were ever able to gain access to it, despite the fact that they desperately wanted to.  And so they shipped it up to the U.S. FBI and said, "Hey, you geniuses, we need to know what's on this drive."  And as far as we know, that was never possible.  That was software encryption, and a perfect example of it.  And now we know, and we will be hearing a term here in a minute, we know that TrueCrypt then became VeraCrypt, which is the currently maintained, non-BitLocker alternative for - you can use it on Windows and other platforms, as well.



Okay.  So these two skilled researchers from the Netherlands invested significant time and attention to reverse engineer seven mainstream, highly popular SSDs which account for, as I mentioned, collectively nearly half the market.  The abstract of their research reads:  "We have analyzed the hardware disk encryption of several SSDs by reverse engineering their firmware.  In theory, the security guarantees offered by hardware encryption are similar to or better than software implementations.  In reality, we found that many hardware implementations have critical security weaknesses, for many models allowing for complete recovery of the data without knowledge of any secret.



"BitLocker, the encryption software built into Microsoft Windows, will rely exclusively on hardware full-disk encryption if the SSD advertises support for it.  Thus, for these drives, data protected by BitLocker is also compromised."  That is, for these systems.  They said:  "This challenges the view that hardware encryption is preferable over software encryption.  We conclude that one should not rely solely on hardware encryption offered by SSDs.



"We have analyzed firmwares from different SSD models offering hardware encryption, focusing on these flaws.  The analysis uncovers a pattern of critical issues across vendors.  For multiple models, it is possible to bypass the encryption entirely, allowing for a complete recovery of the data without any knowledge of passwords or keys.  The situation is worsened by the delegation of encryption to the drive by BitLocker.  Due to the default policy, many BitLocker users are unintentionally using hardware encryption, exposing them to the same threats.  As such, we should consider whether hardware encryption is a true successor to its software counterpart, and whether the established standards actually promote sound implementations."



Okay.  So what happened?  Regular listeners to our podcast will know the correct way to manage a drive's encryption, and many of us could clearly state it.  So here it is, clearly stated.  This is the way you do it.  At the factory, the very first time the drive is powered up, a high-quality entropy source - whether built-in or external - is used to produce a large (128- or 256-bit) high-entropy symmetric secret.  That key will forever be used to key an in-line AES algorithm which encrypts and decrypts the drive contents on the fly.



So each drive, first time it's turned on, either makes it internally if it has a good source of entropy, or gets it from the factory because the factories are easy.  It's easy to have a good source of entropy.  Every drive, a per-drive unique key which it has and forever uses to key its own AES algorithm.  What that means is that all the data physically stored on the drive is gibberish.  It is indistinguishable from random noise and can only be de-gibberized if you have the secret key.



If the drive is not password protected, it is still encrypted, transparently, under that unique per-drive secret key.  Since a hardware implementation of the AES Rijndael cipher is inline and able to keep up with the performance of the storage medium and the drive's external interface, whichever of the two is slower, that is, so that the inline encryption doesn't slow anything down, there's no performance penalty.  And note another feature of this is all you have to do is change the key, and you have securely completely wiped the drive with cryptographic security, including all the sectors that were ever spared out and taken out of service.  So that's the way drives should be designed.



If the drive's user or operating system wishes to later protect the drive's contents, a user or OS-supplied secret is run through a PBKDF, a password-based key derivation function which is internal to the drive, to produce a password-dependent key which is subsequently used to encrypt the drive's original master encryption key.  The drive's original factory-set master key is physically overwritten with its password-encrypted version so that the original master key no longer exists anywhere on the drive.  Right?  We all know that.  This is not rocket science.  This is the way you solve this problem.



And, finally, to allow a candidate password to be verified by the drive before being applied, a different PBKDF2 hashing function should also be used to independently verify any would-be decryption password.  In other words, when the password is created, it would be run through a separate hash with different parameters, and the proper hashed value would be stored by the drive.  That way the drive can verify in a safe fashion whether the password given to it is correct, yet there's no way to go from the stored verifier to the - there's no way to go from that over to the key used to decrypt the encrypted master key.



So as we know, security is inherently a weakest-link phenomenon.  And if even one of those things that I just described is not done properly, the system's weakest link will be feasibly broken, and the system security guarantees will fail.  Disturbing as it is, among those seven very popular mainstream SSD drives, failures in every one of those aspects was actually found.



So before I go any further, I should note that, when the researchers realized just how bad the situation was in this industry, they elected to handle its disclosure responsibly.  So in their write-up they explained, in their responsible disclosure section, they said:  "After discovering these vulnerabilities, we followed a process of responsible disclosure.  In this case, the National Cyber Security Center (NCSC) of the Netherlands was informed first, which assisted in the responsible disclosure process by setting up the contact with the manufactures involved," in this case Crucial and Samsung.



"We cooperated with both manufacturers to fix their products and agreed not to disclose the vulnerabilities for six months.  Both vendors have confirmed all the reported issues.  For models currently being supported, firmware updates are either released or currently in development."  And I should say that that is now the case, that there is now updated firmware for all of these.  So in the properly designed system I outlined above, we would say that the key is bound to the password, meaning that the information provided by the password is absolutely required to synthesize the key.



Unfortunately, in the majority of these implementations, what the researchers found was that the key was not bound to the password, but that it was protected from access by a password which, as we know, is not the same thing at all because it opens that drive to a full password bypass.  This is what the researchers were often able to achieve and demonstrate.  So in other words, the drive's master key existed, hidden somewhere on the drive - in its firmware, on internal or external nonvolatile memory, somewhere.  And the externally supplied password was being used, not to create the key, but to unlock access to it, which is like, okay, how do you explain that in this day and age?  It just must be that somebody who doesn't understand security has been given the task of implementing security.  Which is never going to be a good idea.



So what were the designers thinking, if they were?  The only way to explain their thinking is that they must have believed that there would never be any way to reverse engineer their implementation.  Assuming - they had to assume - that their code was, first of all, error free; and that the only access anyone would ever have would be through the front door by powering up the drive and accessing its standard ATA command interface.



And speaking for a moment about the issue of error-free code, in their description of the reverse engineering of the very popular Samsung 840 EVO drive, the researchers note of some of the drive's power-up logic, they wrote:  "The key is computed" - this is just one of many keys that they talk about, not any particular super-secret one.  "The key is computed during the drive's boot-up sequence.  However, due to a bug in the firmware, retrieving slot 451 during early boot fails.  Therefore, the value of 'p,' a value being hashed, is in fact a zero buffer.  Consequently, the resulting key is constant for all devices."



That's just an aside, the point being that some piece of logic that was also crucial to the drive's security is completely dysfunctional in the firmware and never has been protecting the drive, and no one apparently ever saw that before or noticed it.  So not only is the fundamental design of the drives flawed, but the flawed design is buggy, so it's not working the way - doesn't even do what its designers intended.



Okay.  But back to the topic of keeping secrets inside the drive.  Our intrepid security researchers took advantage of two things these designers apparently failed to consider, firmware downloads and the microcontroller JTAG debugging interfaces.  Okay, so first, on the issue of the firmware, most of these devices, in fact all of them, have readily downloadable firmware.  In some cases the firmware was not published, but it was downloadable nonetheless.



In some cases it's a bootable ISO image incorporating an OS kernel such as a small Linux and the firmware.  This can be readily reverse engineered, even when the firmware image which is bound into the ISO image has been obfuscated.  And this, of course, as we talk about decrypting the DVD on our living room DVD player, you've got to be able to decrypt it in order to see the movie.  So it's easy to get access to the decrypted content.  So even if the firmware image has been hidden, there's no way for something to successfully prevent it from being obtained.



In other cases, a dedicated firmware utility is provided; but it, too, can be readily reverse engineered to obtain and then reverse engineer the drive's controller code.  And even if neither of those avenues was available, the interface to the drive could be intercepted and the update data captured.  This was actually done in the case of updating the firmware of two of the USB-connected external drives.  Wireshark now has a USB packet capture facility, and so that was used in order to gain access to the firmware.  Oh, and all of the drives use the industry-standard ATA "upload firmware" command, making it easy to capture the firmware passing by.  So one way or another, once the drive's - and I've got air quotes here - "secret" internal operation has been exposed, the location of its secrets can be found.



You know, this podcast has spent a great deal of time talking about the keeping of secrets.  So we know that the greatest breakthrough which occurred in cryptographic thinking was the innovation of a keyed cipher.  Before that innovation, cryptographers created clever, unkeyed ciphers.  But if that cipher's operation ever became known, then every secret it had ever been used to protect would simultaneously be divulged.  Cryptography's greatest innovation, then, was the concept of a keyed cipher where its algorithm could be, and absolutely should be, freely published and studied; and where the secret that must be kept was not the algorithm, but only a tiny key.  Essentially, the use of a key created a near infinity of individual specific ciphers from a single master general cipher.



So the analogy here is to the erroneous thinking that apparently went into the design of these self-encrypting SSDs.  Their designers must have believed that they could keep their internal algorithm secret, or perhaps that no one would ever bother looking because such secrets we know are impossible to keep.  And beyond their exposure of their own firmware was the presence of their microcontroller's serial JTAG interface.  We've touched on this in years past.  JTAG is an industry standard interface which uses only a few wires and thus only consumes a few of a processor's or microcontroller's pins.  It is universally supported in some fashion by all microcontrollers.



The JTAG interface allows the microcontroller to be placed under the control of an external debugger, and through the JTAG interface it's possible to examine the processor registers and its memory.  The JTAG interface can be used to write and execute code on the fly in RAM.  It gives anyone who can connect total control over the microcontroller.  And in their paper they show the PC board layouts of these SSD drives, and where they find a layout of the standard JTAG connector pins.  So it's like it's trivial to hook up a debugger to the microcontrollers, which are standard ARM cores, so they're understandable.  Once you have a JTAG interface, you suck the firmware right out of the chip and then run it through IDA in order to decompile it and reverse engineer it.



So the JTAG interface can usually be disabled after initial testing and manufacture.  Many microcontrollers have a one-time, a one-way fusible link, basically a fuse that is blown which then forever disables the microcontroller's JTAG interface.  And this is done, this is present and done exactly because it is such a powerful and official standards-based backdoor into the operation of every microcontroller.  Yet only two out of the seven SSDs the researchers examined had their JTAG interfaces disabled.  The rest freely accepted JTAG connections.  And there's even something called the "JTAGulator" which is a piece of freeware, an open source platform that automates the process of figuring out which are the JTAG pins on a chip when you don't already know.  So, I mean, the whole thing is just amazing that this is sitting there present in an SSD, pretending to be doing encryption for us.



So in one typical example where the model of a device's firmware was not available for download, its ATA command table could be located through its still functional JTAG interface, which allowed the researchers to learn of an undocumented vendor-specific ATA instruction and the secret parameters it required to unlock that command, which then commanded the drive to export its own firmware through the ATA interface.  And then, of course, the drive's secrets were available.



So anyway, I'm not going to delve in detail into the individual fundamental design mistakes which were discovered in each of the seven drives which these guys dissected.  I've got a link in the show notes for anyone who is interested in their very nicely assembled 16-page research disclosure.  But they did have some interesting things to say about the question of hardware versus software solutions and some salient recommendations which were driven by their discoveries.



So at one point in their paper they write:  "An argument that is often put forward in favor of hardware encryption is that the secret key is not stored in RAM" - that is, main memory, main host accessible RAM - "and therefore is not vulnerable to the aforementioned attacks.  In reality, this argument is invalid for several reasons.



"First, the software running on the host PC controlling the hardware encryption typically does keep a secret key in RAM, introducing the same vulnerability.  The reason is to support Suspend-to-RAM, a low-power state wherein all peripheral devices are shut down.  Since the SSD is completely powered down, it must be unlocked again once the system is resumed, and therefore either the operating system must retain a copy of the drive's secret key at all times, or the user must enter it again.  In virtually all implementations, including BitLocker, the former is chosen."  Meaning the user is not hassled to re-unlock their drive.  It's oh, look.  It comes out of sleep, it wakes up, and everything works.  Well, that means that the drive has been given the secret again in order to re-unlock it.  So it's not any more secure.



"Two, the burden of keeping the secret is moved to the SSD, not eliminated.  The SSD typically keeps the key in the main memory of its controller.  SSDs" - as we know - "are not security-hardened devices by any standard," they write.  "In fact, many have a debugging interface exposed on their PCB" - that's the JTAG I was mentioning - "allowing one to attach a debugging device and extract the secret key from the drive.  Furthermore," they write, "several means of obtaining code execution on the drive exist."



And, finally, three:  "A memory readout attack against software encryption requires physical access.  Given this, the attacker also has the opportunity to carry out a hot-plugging attack against hardware encryption.  This has been demonstrated in practice and poses a realistic threat."



So in their discussion at the end of the paper they wrap it up by saying:  "An overview of possible flaws in hardware-based full-disk encryption was given.  We have analyzed the hardware full-disk encryption of several SSDs by reverse engineering their firmware, with focus on these flaws.  The analysis uncovers a pattern of critical issues across vendors.  For multiple models, it is possible to bypass the encryption entirely, allowing for a complete recovery of the data without any knowledge of passwords or keys.



"The situation is worsened by the delegation of encryption to the drive, if the drive supports Trusted Computing Group Opal as done by BitLocker.  In such cases, BitLocker disables the software encryption, relying fully on the hardware encryption."  What did I do?  I just lost my place.  Oh.  "As this is the default policy, many BitLocker users are unintentionally using hardware encryption, exposing them to the same threats."



Anyway, so this goes on.  Basically what we now know is for these drives which were examined, the people who implemented the system used the password to gain access to the drive's encryption key, rather than using information from the password to uniquely synthesize the key.  That is the only way to do this, to solve this problem securely.  Again, this is not rocket science.  This is just proper security design.



Okay.  So where does this leave us?  I would feel better using one of these drives once its firmware has been audited and fixed than any other drive that has not yet been scrutinized like this.  And all of the drives which were subjected to this scrutiny by these engineers have now been updated.  So if you have a Crucial MX100, MX200, or MX300, a Samsung 840 EVO or 850 EVO, or an external Samsung T3 or T5, significantly more secure firmware awaits you.  Go update your drive.



And I would think at that point, I mean, again, I don't know - we're living in a world where we have to take our manufacturer's word for security until some auditor is given a chance to look at it.  In fact, what these guys wrote was:  "Hardware encryption currently comes with the drawback of having to rely on proprietary, non-public, hard-to-audit crypto schemes designed by their manufacturers.  Correctly implementing disk encryption is hard, and the consequences of making mistakes are often catastrophic.  For this reason, implementations should be audited and subject to as much public scrutiny as possible."  Amen.  "Manufacturers that take security seriously should publish their crypto schemes and corresponding code so that security claims can be independently verified."  Why not?



They said:  "A pattern of critical issues across vendors indicates that the issues are not incidental, but structural; and that we should critically assess whether this process of standards engineering actually benefits security; and, if not, how it could be improved.  The complexity of TCG Opal contributes to the difficulty of implementing the cryptography in self-encrypting drives.  From a security perspective, standards should favor simplicity over a high number of features.  The requirements as specified by the Opal standard, having a many-to-many relation between passwords and keys, and allowing for multiple independent ranges with adjustable bounds, makes it very hard to implement it correctly.



"Finally, TCG should publish a reference implementation of Opal to aid developers.  This reference implementation should also be made available for public scrutiny.  It should take into account that wear leveling is applied for nonvolatile storage.  Opal's compliance tests should cover the implementation of the cryptography, and these tests should be independently assessed."



I forgot, part of what I skipped in the details was that in one case a drive's secrets, about every 20 times the password was changed, wear leveling caused the previous instance to be left unencrypted, and a new sector was assigned for the new security.  And it made it possible at the chip level to go read the unencrypted original key from the nonvolatile memory.  So in this instance wear leveling was a confounder to the attempt to overwrite the unencrypted data with something that was then encrypted.  And in this case it was the master key upon which all the other security of the drive depended.



So Microsoft has responded to this news.  They have an advisory, 180028, titled "Guidance for Configuring BitLocker to Enforce Software Encryption."  Anybody who is using Windows - I'm thinking it's only available in Windows 8 and later, so once again we Windows 7 users don't have access to this.  It reads:  "Microsoft is aware of reports of vulnerabilities in the hardware encryption of certain self-encrypting drives.  Customers concerned about this issue should consider using the software-only encryption provided by BitLocker drive encryption.  On Windows computers with self-encrypting drives, BitLocker drive encryption manages encryption and will use hardware encryption by default.  Administrators who want to force software encryption on computers" - well, and then users - "with self-encrypting drives can accomplish this by deploying a group policy to override the default behavior.  Windows will consult group policy to enforce software encryption only at the time of enabling BitLocker."



Okay.  So there is a command we all have in our machines.  I tried it on Windows 7 just for grins.  It's manage-bde.exe.  That is, manage BitLocker drive encryption dot exe, manage-bde.exe.  If you just type that by itself, you need to use an elevated command prompt.  So right-click on command prompt, then run as administrator, then type manage-bde.  Hit enter.  That'll give you a whole list of switches which that command can accept.  One of them is status, so manage-bde -status.  And that will show you all the drives in your system which are usable with BitLocker and their current status, whether or not they are being encrypted, but whether or not BitLocker is enabled at all on each drive.  And, if so, whether it is encrypted with hardware or software.



If it is encrypted under hardware, as we now know, that may not be very good encryption.  So what you need to do, if you are interested, is first turn off BitLocker on any drive where you wish to switch it to software encryption.  You have to turn it off first.  Then you go into gpedit.msc.  And I have a picture at the last page of this week's show notes showing, because I did this this morning, opened up the Local Group Policy Editor.  I had to do it on Windows 10 because it's not available, this particular option is not available under Windows 7.



And so you follow this under - it's Local Computer Policy > Computer Configuration > Administrative Templates > Windows Components > BitLocker Drive Encryption, and under that is Fixed Data Drives.  It's also available for removable data drives.  And under there is "Configure use of hardware-based encryption for fixed data drives."  And you are there.  You'll only find that if you have Windows 8 or later.  And you can choose the setting there to turn off, that is, disable hardware-based encryption for fixed-data drives.  Then reenable BitLocker, and that will turn on software-based BitLocker encryption for your system.  So again, jumping through some hoops.



And at this point I was trying to decide whether it would be safe to say that this requires local physical access.  But it's probably the case, with the information that has been published, that these drives could have their firmware exported only using the ATA interface, which is available from the host.  So you do not need the JTAG interface.  So I would argue that, if somebody was determined, they could probably gain access to the drive's key, which by the way these guys properly note is available in RAM.  Just because it's in hardware doesn't mean the software key is not also available in RAM.  It actually is under BitLocker in Windows, even if you're using hardware encryption.



And also it's been noted that all the new processors, all of the recent Intel processors have the AES-NI extension which is specific instructions to accelerate the use of the AES Rijndael cipher in software so that it is not producing any slowdown.  And our longtime listeners will remember that I benchmarked TrueCrypt and VeraCrypt, and I skipped over where these guys mentioned VeraCrypt.  They do that a number of times in their paper.  They're big fans of it, and they like it because, as we know, it's open source and has been audited.  And as far as we know, there are no known vulnerabilities in it, unlike this disaster where drives are saying, oh, yeah, don't worry, your data is safe and encrypted, when in fact it is anything but.



So that's the story on self-decrypting drives.  They are, unfortunately, they use the password to unhide the master key, rather than depending upon the password to synthesize the master key.  There's a big difference.



LEO:  Yeah.  What can we do about it?  Use VeraCrypt.



STEVE:  Yeah.  Well, as far as we know, if you have access to Windows and BitLocker, and you'd rather not use a third party, you can disable its use of hardware encryption and then reencrypt your drive under BitLocker, having changed that setting, as long as you have Windows 8.  If you're using Windows 7, then your only recourse is to disable BitLocker and install  VeraCrypt.  It's good.  And as we know, it does not measurably slow things down.  I was unable to detect any benchmark slowdown of the system under TrueCrypt and/or VeraCrypt.  So there doesn't seem to be any penalty in performance these days.



LEO:  Good.  And you can disable BitLocker's use of hardware encryption with a command line; right?  I mean, there's no easier way.



STEVE:  A command line, yes, yes.



LEO:  A quick google will find that.



STEVE:  Yup.  And we have complete documentation in this week's show notes.



LEO:  Yeah.  So you put up the show notes at GRC.com along with the audio of the show and the transcript.



STEVE:  Yeah.



LEO:  So really that's the definitive place to go:  GRC.com.  While you're there, get SpinRite, the world's best hard drive recovery and maintenance utility.  You should also check out all the other cool free stuff Steve offers at GRC.com.  That's his website.  He's on the Twitter at @SGgrc, @SGgrc.  You can leave him messages there.  He accepts direct messaging.



We also have audio and video versions of the show at TWiT.tv/sn.  You can watch us do it live every Tuesday, 1:30 Pacific, 4:30 Eastern, 21:30 UTC.  Tune in at TWiT.tv/live.  Join the chatroom at irc.twit.tv.  Chat along with us as we do the show.  The chatroom always posts the show notes link there, as well, so you can get that and read along, too.



But you can also get on-demand versions, as I said, from Steve's site, our site, or subscribe.  Please subscribe.  That way you'll have a collection.  Start your collection now.  Got to catch them all.  GRC.com.  TWiT.tv/sn.  That's all I need to say except thank you, Steve, and we'll see you next time on Security Now!.



STEVE:  Yay.  Thanks, my friend.  See you next week.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.










GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#690

DATE:		November 20, 2018

TITLE:		Are Passwords Immortal?

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-690.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we cover the action during last week's Pwn2Own Mobile hacking contest.  As this year draws to a close, we delve into the final last word on processor misdesign.  We offer a very workable solution for unsupported Intel firmware upgrades for hostile environments.  We look at a forthcoming Firefox breach alert feature.  We cover the expected takeover of exposed Docker-offering servers.  We note the recently announced successor to recently ratified HTTP/2.  We cover a piece of 1.1.1.1 errata, close the loop with some of our podcast listeners, then finish by considering the future of passwords using a thoughtful article written by Troy Hunt, a well-known Internet security figure and the creator of the popular HaveIBeenPwned web service, among others.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson's here.  And, boy, do we have a big show for you.  He's going to talk about the most recent Pwn2Own and which operating systems fell to hackers' attention; the source behind all of these Spectre/Meltdown and other Intel processor woes and why it's going to be a tough one to fix.  And then he's going to talk about passwords and the future of passwords.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 690, recorded Tuesday, November 20th, 2018:  Are Passwords Immortal?



It's time for Security Now!, the show where we cover your privacy and security online with the guy in charge, Steven Gibson of GRC.com.  Hello, Steven.



STEVE GIBSON:  Hey, Leo.  It's great to be with you again for 690.



LEO:  Yikes.  



STEVE:  Six nine zero, for this, what is it, two days before Thanksgiving in 2018.  I guess this is, what, this is an earlier Thanksgiving than usual?  Is that what happened?  I saw something like on that.



LEO:  I guess it is, yeah.



STEVE:  What's it supposed to be, the third Thursday of the month?



LEO:  Right.  So the earliest it could be would be the 21st, I guess.



STEVE:  So this is certainly close to that.  



LEO:  Yeah.



STEVE:  Our topic is, or the title of the podcast, asks the question, "Are Passwords Immortal?"  That is, will they never die?  And this comes from an article, or this was inspired by an article that a friend of the podcast, Troy Hunt, wrote 15 days ago.



LEO:  He's HaveIBeenPwned. He's the HaveIBeenPwned guy.  



STEVE:  Yes, he is, exactly.  He's the HaveIBeenPwned guy who offers the service that allows people to put their email addresses into his site confidentially.  And Troy gathers all of the breached data from websites that lose control of their data all over the Internet and lets you know if your email is among those.  So anyway, so that's the main topic.  There wasn't a ton of news.  So we have time to go into that in some detail, and I plan to.



I reached another milestone in my proposal for what may ultimately put passwords to rest.  We'll see.  And I'm going to be a little more philosophical about how I feel about that because I don't disagree with the position that Troy has.  I'm going to share what he wrote at the end of this podcast, and then we'll talk about it and also put it into this context.



But we also have, as we anticipated, last week was the 2018 Mobile Pwn2Own hacking contest, which took place in Tokyo.  And we talked - there was a - somebody had done a zero-day which we covered a few weeks before that, but that was for a desktop platform.  And he mentioned in his Twitter feed that he'd be going to this one.  So we have the news from that.



Also, as this painful year of processor design issues winds to a close, we have the ultimate final last word on processor misdesign.  We teased the forthcoming of this research a week or two ago.  It has now been published, and it is amazing and comprehensive, and it's got some surprises, of course, because a bunch of academics really wrestled this whole topic to the ground once and for all.



And there's some very good news.  There's a workaround available for anyone whose motherboard BIOS is not updated, has not been updated, will probably never be updated.  If they're concerned about not having the latest firmware for their own processor, we've got a way around that now.  Which is actually probably very practical for server environments, which as we know is where the danger is greatest in virtualized environments where the chance of hosting hostile code is greater.



As we've been saying all year, yeah, it's not good to have Spectre and Meltdown vulnerabilities, but for the typical end user it's not the end of the world because, if something has already gotten into your system, you've sort of already lost that game.  So anyway, we'll talk about this really cool solution for that.



We've got a forthcoming Firefox breach alert feature that'll be joining Firefox browsers.  We will cover the expected takeover of exposed Docker service offering servers, which again we expected, and it has happened.  Also we will note the recently announced next version or successor to the only just recently ratified HTTP/2.  We now know what HTTP/3 will look like and be.  Also I want to cover a little bit of what I will put it in the errata sector about this 1.1.1.1 that we stumbled over on the fly last week when I was surprised by suddenly realizing that, wait a minute, https://1.1.1.1 was giving us an extended validation secure connection.  And it's like, what?  How did that happen?  So of course I know how.  And even John Graham-Cumming dropped a piece of email to me saying, "Hey, Steve, if you're puzzled, I can explain it to you."  And by that time I knew.  I just had to think about it a little bit more.



Also, as I said, we'll have a chance to close the loop with some of our podcast listeners, some little bits of confusion that exist, and some other points that are useful.  And then we're going to take up this issue of the future of passwords as a consequence of this thoughtful article which you had already encountered.  And you can imagine, I was bombarded by it from every direction.  And what's interesting is the very first response to Troy's posting was someone saying, "Have you heard about SQRL?"  Anyway, and then a lot of the dialogue in the conversation thread that ensued was about that.  So I think a great podcast for our listeners.  And I couldn't resist our Picture of the Week, just become someone happened to send it to me, and I thought it was fun.  So, yeah.  Good stuff.



LEO:  Yes, I'm very excited.



STEVE:  So last week was, as I mentioned at the top of the show, November 13th and 14th was the two-day Pwn2Own, which is an annual, well, actually it's probably more than that because it occurs typically...



LEO:  This is a mobile one.



STEVE:  Right.  And it occurs sort of in concert with various security conferences where people are gathered together.  Trend Micro in this case was behind this one, offering cash and prizes during this competition for vulnerabilities and exploitation techniques against the listed devices.  We talked about those a week or two ago, basically handsets, a couple IoT cameras and so forth.  Those didn't get - oh, and also the Apple Watch was on the list.  Nobody attacked those.



The write-up is fun because it sort of takes the sports announcer style, which I'll share with our listeners.  Trend Micro wrote of each day.  They said:  "The first day of Pwn2Own Tokyo 2018 has come to a close.  Today saw some great action and amazing research as we awarded $225,000 USD and 48 Master of Pwn points.  We had six successful exploits and purchased 13 bugs in total."



They say:  "Our day began with Fluoroacetate" - which is the team of Amat Cama and Richard Zhu - "successfully exploiting the Xiaomi Mi 6 handset via NFC.  Using the touch-to-connect feature, they forced the phone to open the web browser and navigate to their specially crafted webpage.  During the demonstration, we didn't even realize that action was occurring until it was too late.  In other words, a user would have no chance to prevent this action from happening in the real world.  The web page exploited an out-of-bands write in WebAssembly to get code execution on the phone.  This earned them $30,000 and six Master of Pwn points.



"The Fluoroacetate duo returned, this time targeting the Samsung Galaxy S9.  They made quick work of it by using a heap overflow in the baseband component" - that's the cellular radio component - "a heap overflow in the baseband component to get code execution."  They write:  "Baseband attacks are especially concerning, since someone can choose not to join a Wi-Fi network, but they have no such control when connecting to baseband.  The work earned them another $50,000 and 15 more points towards Master of Pwn.



"Next up, Amat and Richard returned to the Short Distance category.  This time, they were targeting the iPhone X over Wi-Fi.  They used a pair of bugs, a JIT" - that's a just-in-time compilation - "a JIT vulnerability in the web browser followed by an out-of-bands write for the sandbox escape and escalation."  Okay, this is from an iPhone X.  "The successful demonstration earned them $60,000 more, and 10 additional Master of Pwn points."  They said:  "This ends the first day of competition with $140,000 and a commanding lead for the Master of Pwn with 31 points.



"Following that attempt, the team from MWR Labs combined three different bugs to successfully exploit the Samsung Galaxy S9 over Wi-Fi.  They forced the phone to a captive portal without any user interaction, then used an unsafe redirect and an unsafe application load to install their custom app.  Although their first attempt failed, they nailed it on their second try to earn $30,000 and six more Master of Pwn points."



They said:  "In our last entry of the day, Michael Contreras made sure his first Pwn2Own attempt was memorable.  He wasted no time in exploiting a type confusion in JavaScript.  In doing so, he earned himself $25,000 and six Master of Pwn points.  We look forward to seeing him in future events."  And they wrote:  "Excelsior!"



Then they start the second day:  "Our second day began with the Fluoroacetate duo of Amat and Richard targeting the iPhone X in the browser category.  After a stellar first day, they kicked off Day Two in style by combining a JIT bug in the browser along with an out-of-bounds access to exfiltrate data from the phone.  For their demonstration, the data they chose happened to be a deleted picture, which certainly was a surprise to the person in the picture.  The research earned them $50,000 and eight more points towards Master of Pwn.



"Next up, MWR Labs team of Georgi Geshev, Fabi Beterke, and Rob Miller also targeted the iPhone X in the Browser category.  Unfortunately, they couldn't get their exploit chain to work within the time allotted, resulting in a failed attempt.  However, they did have some great research, and we acquired," writes Trend Micro, "the bugs through our normal ZDI program."  That's the Trend Micro Zero-Day Initiative program.  



"Following that, the Fluoroacetate team returned, this time targeting the web browser on the Xiaomi" - is it Xiaomi?  I always say it wrong.



LEO:  Xiaomi.  Xiaomi.



STEVE:  Xiaomi, thank you, "Xiaomi Mi 6.  They continued their successful contest by using an integer overflow in the JavaScript engine to exfiltrate a picture from the phone.  They earned themselves another $25,000 and six Master of Pwn points.  The Fluoroacetate team couldn't keep their momentum going throughout the entire competition, as their last entry fizzled out.  They attempted to exploit the baseband on the iPhone X, but could not get their exploit working in the time allotted."



LEO:  To clarify, the baseband is the radio; right?  The SS7 radio software.



STEVE:  Yes, correct.  Yeah, the cellular radio.  And we've often talked about how all of the attention is being focused up at the application processor, where the apps run.  But very little attention gets given to this whole separate processor, the baseband, which does all of the cellular stuff.  And it's believed to be riddled with bugs, which haven't really been looked at yet.  So by offering prizes for exploits down there, we're beginning to see those surface.



So they said:  "Still, five out of six successful demonstrations is pretty remarkable," speaking of the Fluoroacetate team.  They said:  "We're glad to see these two team up and hope to see more from them in the future.



"Our final entry for this year's event saw the MWR Labs team target the web browser on the Xiaomi Mi 6 handset.  The team combined a download bug along with a silent app install to load their custom app and exfiltrate pictures.  This earned them another $25,000 and six additional points toward Master of Pwn."



They write:  "This closes out Pwn2Own Tokyo for 2018.  With 45 points and $215,000, we're happy to announce the Fluoroacetate duo of Amat Cama and Richard Zhu have earned the title Master of Pwn.  Overall," they write, "we awarded $325,000 over the two-day contest, purchasing 18 zero-day exploits.  Onsite vendors have received the details of these bugs and now have 90 days to produce security patches to address the bugs we reported.  Once these are made public, stay tuned to this blog for more details about some of the best and most interesting bugs we saw this week."



So this is a nice way to do this.  You produce meaningful prizes, meaning money, to incentivize talented developers, talented hackers, to dig in and find these problems.  As a third party, in this case Trend Micro is a commercial entity, so they get contributions from others, pool the funds.  They form this in a contest.  The hackers do the best they can in a competitive mode.  So they both want the crown of Master of Pwn, and they certainly want, you know, not bad to come away with $215,000 for a team.  We don't know how long it took them to get these.  And then 18 previously unknown, I mean, like no one knew these problems existed, 18 serious flaws in mobile handsets are then responsibly disclosed to their vendors under the onus that 90 days from then these will go public.  And as a consequence, everybody gets protected.



LEO:  You know what fluoroacetate is?



STEVE:  Fluoroacetate, no.



LEO:  It's bug poison.



STEVE:  Ooh.  Clever.



LEO:  Isn't that?



STEVE:  Very clever.  Very clever.



LEO:  Sodium fluoroacetate or potassium fluoroacetate.



STEVE:  Bugs don't like it.



LEO:  Bugs don't like it.



STEVE:  Cool.  That's neat.



LEO:  I know.  I had to look it up once you said it.  I thought, I bet you there's a reason they call themselves that.



STEVE:  Okay.  So anyway, I just wanted to finish and say, you know, compared to the tweets we've been covering recently from a couple developers who are coming up with good, beautifully engineered exploits that they're just dropping out onto the world, saying here you go, I mean, anybody who wants it can have it, I just - I don't get why they're not willing to operate in a responsible fashion and make some money.  I mean, ultimately that puts bread on the table.



Okay.  So a couple weeks ago, we knew that there were - I think the number was seven, seven previously unknown - everybody was like, oh, you've got to be kidding me - Spectre-ish/Meltdown-ish new problems that a research team had found.  At that point, that's all we knew.  We didn't have the details.  Now we do.  This picture on this next page of the show notes is a tree diagram which shows the consequence of an academic, methodical, careful analysis of contemporary processor design.  Those red boxes are demonstrated new attacks.  The dotted ones were sort of theoretical maybe that they were not able to actually do, but the red ones actually happened.



I want to share the abstract and the beginning of the introduction of this because, more than anybody else, these guys - I wish we had this a year ago because things would be very different if we had this a year ago.  I have to wonder what's going on with Intel because they felt like they were reactive, and we know that there was some understandable CYA behavior with their PR people and all processors have these problems and so forth.  And we've talked about that honestly, the fact that, yes, many things have been done without a proper focus on security.  What annoys me is that ultimately we rely on Intel to be the designers of secure hardware.  And even now, a year later, as we're about to hear, these problems have not been fixed.



So the abstract of this paper, which more perfectly frames this than I've seen anywhere else, reads:  "Modern processor optimizations such as branch prediction and out-of-order execution are crucial for performance.  Recent research on transient execution attacks, including Spectre and Meltdown" - by the way, I should say "transient execution attacks," that's their term for the umbrella that they've created for understanding these.  "Recent research on transient execution attacks, including Spectre and Meltdown, showed, however, that execution or branch misprediction events may leave secret-dependent traces in the CPU's microarchitectural state.  This observation led to a proliferation of new Spectre and Meltdown attack variants and even more ad hoc defenses, for example, microcode and software patches."



They write:  "Unfortunately, both the industry and academia are now focusing on finding efficient defenses that mostly address only one specific variation or exploitation methodology.  This is highly problematic, as the state of the art provides only limited insight on residual attack surface and the completeness of the proposed defenses.



"In this paper we present a sound and extensible systemization of transient execution attacks.  Our systemization uncovers seven new transient execution attacks that have been overlooked and not been investigated so far.  This includes two new Meltdown variants," which they named Meltdown-PK on Intel and Meltdown-BR on Intel and AMD.  "It also includes five new Spectre mistraining strategies.  We evaluate all seven attacks in proof-of-concept implementations on three major processor vendors:  Intel, AMD, and ARM.  Our systematization does not only yield a complete picture of the attack surface, but also allows a systematic evaluation of defenses.  Through this systematic evaluation, we discover that we can still mount transient execution attacks that are supposed to be mitigated by rolled out patches."  Meaning this stuff hasn't been fixed yet.



So to get into it a little bit deeper, in their introduction they say:  "Processor performance over the last decades has continuously improved by shrinking processing technology and increasing clock frequencies, but physical limitations are already hindering this approach.  To increase the performance, vendors shifted the focus to increasing the number of cores and optimizing the instruction pipeline.  Modern processors have deep pipelines, allowing operations to be performed in parallel in different pipeline stages or different units of the execution stage.  Many processors additionally have a mechanism which allows executing instructions not only in parallel, but even out of order.  These processors have a reordering element, which keeps track of all instructions and commits them in order.  In other words, there is no functional difference to regular in-order execution if they have a dependency on a previous instruction which has not yet been executed and committed.  Hence, to keep the pipeline full at all times, it is essential to predict the control flow, data dependencies, and possibly even the actual data."



Now, I'll just pause and say for a minute that the idea of designing that just - it is just mindboggling when you consider that you have interdependent instructions which are themselves very complicated, which are each producing outcomes, which subsequent instructions may or may not be dependent upon.  And you're basically wanting to run ahead as far as you can while slower things are taking longer to execute or having to go fetch things that are not in one of three layers of caches out of main memory, to then get the result, and then that may cause a branch decision to be made which happened in your history.



And then it's like, oh, we're going to have to unwind all of this because we guessed wrong.  Or, oops, wait, we've got to stall this because now we're waiting on a result from something, I mean, I just can't even imagine.  This is in hardware.  Or, well, firmware, with the aid of microcode.  But, I mean, wow.  So just reading that description of today's contemporary microprocessor, for one thing it makes me feel like it's worth the money.  A few hundred dollars for that technology?  Yeah.



LEO:  Good point.



STEVE:  Whoa.  So they said:  "Flushed instructions, those whose results are not made visible to the architectural level due to roll-back, are called 'transient instructions.'  On modern processors, virtually any instruction can raise a fault, for example, a page fault or a general protection fault," you know, like if an instruction has memory that's been paged out.  They say:  "...requiring a roll-back.  Already, without prediction mechanisms, processors sometimes have to flush the pipeline, for example, upon interrupts.  With prediction mechanisms, there are more situations when partial pipeline flushes are necessary, namely on any misprediction.  The pipeline flush reverts any architectural effects of instructions, ensuring functional correctness.  Hence, the instructions are executed transiently:  first they are, and then they vanish..."  Then they say:  "...(i.e., we call this transient execution).



"While the architectural effects and results of transient instructions are discarded, microarchitectural side effects remain beyond the transient execution."  In other words, that's the point, is that all of the focus has been on this, like, not looking at the man behind the curtain, only looking at the instructions being executed and making sure you're correct there.  So correctness at this top level is what Intel has gotten right.  But there was an ignoring of the consequences of the fact that the man behind the curtain that was doing all of this ended up with the right instructions having the proper effects.  But the underlying mechanism is so complicated now that it wasn't being rolled back.  There was debris left in what they called the "microarchitecture," not the macroarchitecture.



So they say:  "This is the foundation of Spectre, Meltdown, and Foreshadow.  These attacks exploit transient execution and encode secrets in the microarchitectural side effects, for example, in cache state, to transmit them to the architectural level to an attacker.  The field of transient execution attacks emerged suddenly and grew rapidly, leading to a situation where people are not aware of all variants and their implications."  And this is the formal way of saying what I said earlier, which is, you know, it's like, everyone went "Holy shit" and ran around and quickly patched some stuff to fix the obvious first things that people found.  But it's until now, like today, that we have a full mature understanding of the broader picture, what really this is all about.



They said:  "This is apparent from the confusing naming scheme that already led to an arguably wrong classification of at least one attack.  Even more important, this confusion leads to misconceptions and wrong assumptions for defenses.  Many defenses, for example, focus exclusively on hindering exploitation of a specific covert channel, instead of addressing the microarchitectural root cause of the leakage.  Other defenses critically rely on state-of-the-art CPU features that have not yet been thoroughly evaluated from a transient security perspective.  We also debunk implicit assumptions, including that AMD processors are immune to Meltdown-type effects, or that serializing instructions mitigate Spectre Variant 1 on any CPU.



"In this paper, we present a sound and extensible systematization of transient execution attacks - Spectre, Meltdown, Foreshadow, and related attacks.  Using our universal decision tree, all known transient execution attacks were accurately classified through a universal and unambiguous naming scheme."  And that's the picture at the top of this article that I put in there, and it's from their paper, and the link to the PDF is here if anyone wants more.



"The hierarchical and extensible nature of our classification methodology allows the easy identification of the residual attack surface, leading to seven new transient execution attacks - Spectre and Meltdown variants - that we describe in this work.  These seven new attacks have been overlooked and not yet investigated so far.  Two of the attacks are Meltdown-BR, exploiting a Meltdown-type effect on the x86 bound instruction on Intel and AMD; and Meltdown-PK, exploiting a Meltdown-type effect on memory protection keys on Intel.  The other five attacks are previously overlooked mistraining strategies for Spectre-PHT and Spectre-BTB attacks.



"We demonstrate all seven attacks in practical proof-of-concept attacks on vulnerable code patterns, and evaluate them on processors of Intel, ARM, and AMD.  We also provide a systematization of the state-of-the-art defenses.  Based on this, we systematically evaluate defenses with practical experiments and theoretical arguments to show which work and which do not or cannot work.  This systematic evaluation revealed that we can still mount transient execution attacks that are supposed to be mitigated by rolled out patches.  Finally, we discuss how defenses can be designed to mitigate entire types of transient execution attacks."



And that's the beginning of 16 pages of all of the details.  So anyway, here we are.  It's been a year of this.  And really it's not surprising because this is not easy stuff.  If this was simple, it wouldn't have taken, what, 20 years for someone to finally say, you know, I can see what's going on over in that other core.  Is that what you meant or intended?  And of course we know this 2018 has been the year of recognizing that it's possible to leak across processors, or across threads in a single processor.



Now, and again, we've also said, as I reiterated earlier in this podcast, that it's not clear to me that, as long as a browser cannot do this stuff, and there has been mitigation immediately added to our browsers, that's really the only place where the typical end user has - it is hosting code in their system in some fashion that they have really no control over.  We're all downloading apps.  And these days, certainly the listeners to this podcast are a little more skeptical than we were 13 years ago, before this podcast began dissecting all of this.  I mean, I double-check where I'm downloading from.  I try to find, if there's some download captain site for some Intel thing like a driver I need, I try to go get it from Intel.  I don't want to get it from download captain.



So we have a lot of control over our own machines.  And as I've said, this is the main concern are cloud providers hosting virtualized systems where multiple entities may be running code in the same platform.  Which is why a lot of this attention has gone to breaking out of sandboxes and breaking out of virtual machine boundaries.  I mean, that's how these things have been applied.  So I don't think it's ever been the case that any of these problems affected end users.  And Leo, as you and I have often reminded ourselves and anyone within earshot, there's never been an instance of any of this in the wild.  No, I mean, unlike zero-day exploits that we're often talking about finding, like, ooh, crap, routers are being taken over by the hundreds of thousands, or Docker is falling.  No, never.  Not once has any of this actually turned into an attack that anyone found.



LEO:  And it's because it's difficult that - tell me if I'm wrong, but my guess and understanding about this is that all these attacks, whether it's the hyperthreading attack that just came out, or Meltdown or Spectre, are timing attacks like Rowhammer where you kind of can deduce, I mean, this is crazy that this even works.



STEVE:  I know.



LEO:  The information in a content stream, not because you can see into it, you're deducing it based on timing.



STEVE:  Yes.  There was a stutter in the execution of your code that was of a different stutter duration.



LEO:  That's, I mean, I don't blame - and honestly, I know Intel, you know, that this was theoretical before they even started doing speculative execution in 1991.  Somebody wrote a paper.



STEVE:  Right.



LEO:  But it seemed so farfetched that anybody could make this work.  Now, the fact is, once somebody does make it work, if they can write a proof of concept, they can then be script-kiddie-ized.



STEVE:  Yes.



LEO:  It's conceivable that it could get out, and somebody who is not in fact capable of figuring out what the content of a stream is from stutters would be able to use it.  But so far nobody's done that.  I don't know if that's because the researchers - and this is all, by the way, highly academic research.  I think most of these guys, really they're just trying to get a PhD.  It's not so...



STEVE:  I would give these guys 12 PhDs.  Wow.  



LEO:  Yeah, I mean, it's brilliant.  But maybe they haven't given enough details that you could weaponize it.  I don't know why it hasn't leaked out.  But it's incredibly hard to do, at least initially.



STEVE:  Yeah.  My guess is this is sort of state actor level.  This is, I mean, where you would put this would be if you could arrange to get your code running on a Google server and get a Google private key, that kind of thing.



LEO:  Right.



STEVE:  Where the value is really high for something really well protected.  And again, it's not somebody who wants to run coin mining somewhere.



LEO:  Part of the reason for that, though, is it's just not worth the effort because there are so many easy ways to do it, to hack somebody's computer, that why go to all that effort.  State actors have a reason to do it and the wherewithal to do it.



STEVE:  Right, yeah.  And I do think, though, that this also, I mean, this is one of the core lessons the industry has learned, as was perfectly framed by Bruce Schneier, who said, famously, and we've often quoted this, "Attacks never get worse.  They only ever get better."  And so if this wasn't addressed, if the bad guys didn't know that it was being addressed, then they could roll up their sleeves, and there would have been an opening to weaponize this.  And so I'm glad this has gotten the attention that it has.



LEO:  Yes, absolutely.



STEVE:  Okay.  With all that said, we have the problem, and it bugs me, I mean, I've got my little InSpectre tool that is woefully now out of date.  People complain that I haven't gone back and kept it current.  But everyone also wants new SpinRite, and so I've got to get SQRL finished first.  But it bugs me that I've got machines which Intel will never update, I mean, not Intel.  The manufacturer will never update them.  They're still useful.  They're still workable.  They're still fine.  But I don't have updated BIOSes for them.  So I can't ever get all happy green on InSpectre.  It's like, sorry, you don't have the firmware.



There's a solution.  There is something open source available called the Intel Microcode Bootloader.  This is not something for non-techies.  This is maybe, I would imagine, this will be of interest to our listeners, which is why I'm bringing it up.  But it could be of serious interest to anybody who's running older hardware in a shared VM hosting cloud that is concerned about these mitigations, and they are not available.



Okay.  So what this is, first of all, the reason a BIOS update can fix the microcode is that all Intel processors have firmware which can be loaded into them when they are powered up.  So they come with a ROM, but the ROM is slower than the internal RAM.  So the ROM is copied into the RAM for maximum execution speed.  That contents of the RAM can be patched or changed.  And so when a BIOS powers up, it contains patches for the processor that it knows is on its motherboard, and it applies those patches on the fly.  In this case, all of these updated BIOSes contain updated microcode that is being reloaded into the processor's firmware RAM every time it powers up.  So the problem is it has to be an "every time you power up" solution.  The BIOS can do that.  But so could something else that boots first and then transfers control to another bootable thing.



LEO:  I'm sure this is trustworthy, but what could possibly go wrong?



STEVE:  Okay.  So a developer has created a preboot patch for Intel microcode providing the latest Intel processor microcode for, get this, 392 Intel CPUs produced from 1996 to 2018.  So way back.  This Intel microcode bootloader provides a workaround for the "my BIOS hasn't been updated for Spectre and Meltdown and probably never will be" problem.  It dynamically updates the microcode every time the system is powered up.  This Intel Microcode Bootloader is based on - and this is news - based on Intel BIOS Implementation Test Suite, BITS, B-I-T-S, BIOS Implementation Test Suite.  Put that in your Google, Leo.  So that users no longer need to modify BIOS UEFI ROMs to stay protected from security vulnerabilities, bugs, and errata.



So I thought, okay, wait.  What is BITS?  So I went looking, and I found an Intel link.  Get this.  The Intel BIOS Implementation Test Suite provides a bootable pre-OS - this is Intel.  I'm reading from Intel.  This is Intel's page.  The Intel BIOS Implementation Test Suite provides a bootable pre-OS environment for testing BIOS and in particular the initialization of Intel processors, hardware, and technologies.  BITS can verify your BIOS against many Intel recommendations.  In addition, BITS includes Intel's official reference code as provided to BIOS manufacturers, which you can use to override your BIOS's hardware initialization with a known good configuration, and then boot an OS.



So they say, under who should use BITS, they said:  "You might want to use BITS if you're a system or BIOS developer, and you want to validate that your system meets Intel's recommendations; you're an OS or application developer building on technologies provided by Intel platforms, and you want to check if your system, or one of your user's systems, has configured those technologies correctly; you're an advanced user or developer, and you want to check your BIOS to see if it configures Intel hardware correctly and, if not, to make a stronger case to your BIOS vendor to get it fixed; or you need to poke hardware in a low-level way, and you need a pre-OS environment to work in to avoid OS interference."  Anyway, I'm just stunned that this thing exists, and it's available:  biosbits.org, B-I-O-S-B-I-T-S dot org.



LEO:  There's apparently something similar for AMD.



STEVE:  Cool.



LEO:  Does the same thing, yeah.



STEVE:  Cool.  So they said:  "BITS consists of a modified GRUB2 bootloader, with many additional commands to probe and manipulate hardware configuration, as well as scripts using these commands to test and reconfigure hardware.  BITS supports scripting via Python and includes Python APIs to access various low-level functionality of the hardware platform" - oh, can you get in trouble now - "including ACPI, CPU and chipset registers, PCI and PCI Express.  You can write scripts to explore and test platform functionality using the full power of Python in 32-bit ring 0, without any OS in the way, and without recompiling BITS or writing custom C code.  See our Python scripting guide for more information."  Anyway, I'm just...



LEO:  Oh, boy.  Oh, boy.



STEVE:  So on top of this real foundation, there is now an Intel Microcode Bootloader.  The guy has on the page that I've got a link to, and I saw you brought it up during the podcast, instructions:  "Format a USB flash drive with a FAT32 file system.  Extract the archive to the USB flash drive and run install.exe to make it bootable.  Enter the BIOS/UEFI, assign the USB flash drive as the first boot device, and enable legacy boot mode.  The bootloader will regularly update the microcode and load the OS."



So the idea would be, if you were an end user, or if you were somebody with a cloud system, you would get a little thumb drive, set this up, stick it into a USB slot, and forget it.  Or actually, there are motherboards now with internal USB, so you could just stick the little thumb drive on the motherboard on one of the internal USB sockets and just arrange to have it boot first and then let it go ahead and continue to boot your OS as it usually would.  That thing will update your microcode where your motherboard wouldn't, and then execute your OS afterwards.  So anyway, I just wanted to put it on everybody's radar.  I thought it was very cool.  And this BITS thing is making me think about, ooh, SpinRite, hmm.



LEO:  Hmm.  Oh, yeah, you're right.  Now you're going to write SpinRite in Python.  Hmm.



STEVE:  Hmm.  So Firefox Monitor is a service that we've never really talked about.  It's not been on our radar.  But a forthcoming feature in Firefox, if you'll pardon the pun, popped up, which is available in multiple languages, and Mozilla's excited about it.  Last Wednesday they announced the addition of a new feature for Firefox.  The first time and only the first time a Firefox visitor goes to a site that has suffered from a publicly reported data breach within the past 12 months, a pop-up notification will appear notifying the user of a prior data breach at that site.  And this is like - and I have a picture of a sample from their announcement in the show notes where they just use example.com, and it shows a little popup, and it asks the question, have an account on this site?  More than - and then there's a number that would be filled in based on the real data, here more than 500,000 accounts from example domain were compromised in 2018.  Check Firefox Monitor to see if yours is at risk.



And so there is a - you can click on Check Firefox Monitor, or you can dismiss it, or you also have the option to turn off this breach notification system for all future.  So the user can take note of and dismiss the notification, or it can elect to immediately jump over to the Firefox Monitor site, where they can confidentially enter their email address and have Troy Hunt's excellent HaveIBeenPwned site as a service check for any exposures of the user's email.  And anyone not wishing to receive these alerts on any site can simply choose "Never show Firefox Monitor alerts" by choosing, as I mentioned, that dropdown arrow on the notification.  Mozilla has said that this functionality will be gradually rolled out to Firefox users over coming weeks.



And we've not talked about Firefox Monitor before.  That's at monitor.firefox.com.  And it looks like it offers its own front end to Troy's, you know, the HaveIBeenPwned facility, and Troy publishes a web API to facilitate such third-party access, making it a service.  At monitor.firefox.com, users can sign up with Firefox Monitor using their email address to receive proactive notification if a breach occurs which does involve their email address.  Which that's kind of cool.  Troy does not himself offer that service.  But if you are a Firefox user, or I guess even if you're not, you can go to monitor.firefox.com, give them any email addresses you want them to watch.  And if breaches occur, they'll proactively check to see whether your email address is among them and send you a note, if so.  Which is very cool.



So a couple weeks ago we talked about the unpatched Docker flaws which would potentially open the door to anybody who found them.  And not surprisingly, Juniper Networks Threat Labs recently discovered and reported on malware in the wild which is searching for exactly that:  misconfigured, publicly exposed Docker services on the Internet, which it is infecting with Monero miners.  I won't get into any super detail.  I'll note that Docker by default is not on the Internet.  It is only using Unix sockets, which is to say using the sockets interface in Unix to do within single machine communications, not on the Internet.  So it's necessary for someone to explicitly bind the Docker APIs to Internet sockets in order to make them available.  If done, then those are 2375 and 2376, which if enabled provide both unencrypted and unauthenticated access to the Docker APIs.



So the point is that - and maybe people doing this don't understand that, because the presumption was that this API would never be publicly exposed, it is not authenticated by default.  The presumption is, if you're running on the same machine, you inherently have access to the APIs of that machine.  But when bound to Internet ports, and if it's on the WAN interface, facing the public, everybody can get them.  And it looks like many people have made that mistake.



Once a new host is infected, it starts looking for other accessible hosts, both on the public Internet and on any internal networks the host has access to.  So that of course will typically be a corporate Intranet, and the infected victim machine ends up servicing as an unwitting bridge from between the public Internet and the internal private Intranet.  Juniper has gone through and decrypted all of the scripting that is going on.  They described the infection chain as largely living off the land because it leverages well-known system-provided utilities to spread its infection and carry out all its activities.  They wrote that some of the system utilities that it uses are Docker, Wget, cURL, Bash, iproute2, Masscan, apt-get, yum, up2date, pacman, dpkg-query, systemd, and so forth, all which are familiar names to people who use Unix or Linux machines.



Juniper has dissected and described the operation of the malware in detail and noted that it downloads a stock version of the Monero miner bash script.  It runs MoneroOcean's Monero miner bash script, which is hosted on Pastebin, and executes it.  And then once that's done, it begins scanning all available networks for any port 2375 and 2376.  When it finds any, it dumps it to a local.txt file, and that contains the list of IP addresses which it then proceeds to infect.  So anyway, again, what we're seeing is, if something can be infected on the public Internet like this, it's not making people lots of money, but it's free after the set this up and turn it loose.  So that's what happens.



I mentioned at the top of the show that we had recently ratified HTTP/2.  For years we've been at HTTP/1.1.  That's often the protocol which is recognized when a server responds to an HTTP query.  It'll send back an okay, 200 okay, and also HTTP/1.1.  Now HTTP/2 is where we are.  And as a consequence of some recent communications among the IETF, the decision has been made and consensus reached about HTTP/3.  And it will use, sort of dramatically, actually, UDP, not TCP, as its underlying... 



LEO:  Wait a minute.  What?



STEVE:  Yes.  Not TCP as its underlying transport.  Mark Nottingham, who is the chair for both the HTTP working group and the QUIC (Q-U-I-C) working group for the IETF proposed renaming what is now known as HTTP-over-QUIC (Q-U-I-C) to HTTP/3, and the proposal appears to have been broadly accepted.  HTTP/3 will have QUIC as an essential, integral feature such that HTTP/3 will always use QUIC (Q-U-I-C) as its network protocol.  Okay.  So let's back up here a bit.



LEO:  Ooh.



STEVE:  Yeah.



LEO:  That's a big change.



STEVE:  That's a huge, huge change.



LEO:  The reason I thought they used TCP is for reliability; right?  Because you have a SYN and ACK packet.  UDP is used for streaming because you don't have to have the ACK.  You just keep streaming.



STEVE:  Right.  It was reliability in the sense of remember that the [crosstalk]...



LEO:  Not caring if you got it, basically.



STEVE:  Right, right.  Exactly.  Like you and I have this conversation going over UDP because it is lighter weight.  And if a packet doesn't arrive, it only is a tiny bit of sound worth of packet that was lost, and the codec at the receiving end is smart enough to just sort of repeat what it already had and fill the missing time.  And if the loss is bigger, then people will hear kind of a twang, like we've often in the older days heard from cell phones.  But in general, the idea being that UDP is unreliable.  Well, you could build reliability on top of UDP if you're smart.  Okay.  So let's back up a bit.



As we know, traditional secure connections over the Internet start with a DNS lookup to map the domain name to the Internet address.  Originally that was IPv4, and increasingly it's IPv6.  But address lookup is typically cached, and it's not really about connections.  So that's out of scope for this.



I have a picture here in the show notes of how a secure TLS connection is currently set up on the Internet.  Once we have an IP address, multiple packets are sent back and forth to first establish a TCP connection.  Sequence numbers are exchanged, and IP level features are negotiated between the end points.  And as we know, there's the famous TCP SYN, where the client, the connection initiator, sends its sequence number, that is, a random 32-bit number from which it will number all bytes that it then subsequently sends.  The server responds with an ACK for the client's SYN, and its own SYN in one combined packet with two of those bit flags sent in the TCP header.  So that's called the SYN ACK packet.  And so upon the client receiving it, they each have each other's sequence number.  Then the client responds with its ACK to acknowledge the server's SYN, and at that point the TCP connection is said to be open.



The client then typically like in this case in a web browser scenario wants to establish - it wants to go from a TCP connection to a TLS connection.  It wants to establish security.  So it sends a client hello packet, which contains a cryptographic nonce which it has chosen, along with a list of all the cipher suites for the version of TLS that it supports.  The server receives the client hello.  It looks through its prioritized list of cipher suites in the order it would like to use them for the first one which is in the client's list.  So it chooses one that it likes and supports.  It also chooses its cryptographic material, its nonce, and returns that in a packet with multiple flags.



A server hello, the certificate - oh, I forgot.  The client's hello also specifies in the SNI, the Server Name Indicator extension to the client hello packet, the name of the service it wants.  That allows a multiply home server to rummage around in its certificates and find the one that the client wants and to return the certificate to assert its identity to the client.  So it sends a certificate, the server hello, back to the client, which contains its crypto information.



So now they've both exchanged crypto information.  They've agreed upon a cryptographic protocol.  The client needs to respond with an acknowledgement of the cipher spec that the server chose with a so-called "change cipher spec" message to say, okay, I accept what you've returned.  I am changing over to that cipher.  The server needs to do the same thing.  It needs to acknowledge the client's message and say everything from now on is under the new cipher that we have agreed on.



So they both bring their ciphers up using the nonces that they've exchanged, which allowed them to negotiate a secure secret key under which they then encrypt all data over not only - so the first layer's TCP.  The second layer is TLS.  Now the TLS tunnel is up, and they can exchange application data with authentication and secrecy.  Whew.  QUIC, Q-U-I-C.



LEO:  Such a short acronym for so much stuff.



STEVE:  Well, QUIC does that in one packet.



LEO:  Well, that's not QUIC.  That's [crosstalk] old school.  That's the old way of doing it.



STEVE:  So believe it or not, QUIC, which stands for Quick UDP Internet Connections, Q-U-I-C, is able under the proper conditions and preconditions to achieve the same with a single packet from the client to the server, everything bundled up into one packet.  So for this we have Google to thank.  Recall that Google's earlier SPDY, S-P-D-Y, technology also proved itself worthy and formed the basis of HTTP/2, which has now become an IETF standard, that is, in the process of getting fully supported.  HTTP-over-QUIC, which is now - now the IETF has said that will be HTTP/3, is a rewrite of the HTTP protocol that uses Google's QUIC instead of TCP as its base.



QUIC is Google's complete rewrite and rethink of that entire protocol stack which I just went through which combines everything - IP, TCP, UDP, TLS, and HTTP/2 - into a single amalgam.  So Google has proposed that QUIC might eventually replace both TCP and UDP as the new protocol of choice for moving binary data across the Internet.  Tests have demonstrated that QUIC is both faster and more secure because it also is an encrypted by default system, that is, there is no nonencrypted version.  And it uses the just recently ratified TLS v1.3 protocol built in.  So it was proposed as a draft standard at the IETF in 2015.  The version of HTTP-over-QUIC, which was a rewrite of HTTP on top of QUIC instead of on top of TCP, was proposed a year later, in July of 2016.



So these things take time.  Here we are, nearly 2.5 years later, and it now looks like it's going to become the next big standard when we're ready to move away from HTTP/2.  The support for HTTP-over-QUIC, that is, this next generation, was added to Chrome 29 and Opera 16.  It's also supported in the LiteSpeed web servers, and Facebook has started adopting the technology.  So it looks like it's going to happen.  And essentially, as we know, anyone who's looked at the developer console of their browser and brought up any contemporary web page knows that stuff is coming from all over hell and gone.  I mean, it is just, you know, today's web pages are nuts with where they're pulling all of their assets.  Every one of those requires a negotiation with a different domain and remote server, bringing up TCP, bringing up TLS, and then finally being able to make the query in order to get going.



What this does, again, given some cached agreements among endpoints, and QUIC manages all of that, it allows a single packet to be sent to a service in order to request an asset and have that asset returned.  So we're talking about once we get this, once both ends have made the move over, significantly quicker loading web pages.  Which of course is why Google was behind this from the beginning, as they would like everything to be faster.  And who wouldn't?



Last week, as I mentioned at the top of the show also, we talked about the cool new app from Cloudflare for iOS and Android devices, and https://1.1.1.1, or one dot one dot one dot one.  And I stumbled over my recognition during the podcast that it was a secure connection.  And it was like, what?  So my first erroneous assumption was that, well, if it's a secure connection, it has to have a certificate.  And if it has a certificate, then dot one...



LEO:  It's a domain, yeah.



STEVE:  It has to be a domain name.  And of course that was wrong because we all - how many times have we put http://192.168.0.1 or .1.1 or something into our browser in order to access our router.  As we know, dotted numbers, dotted quad numbers are IP addresses.  They're not domain names, and they're not legal for domain names.  So "1" cannot be a domain name.  It has to be part of a dotted quad.



LEO:  This is John Graham-Cumming's email.  He probably sent it to you, too.



STEVE:  Yeah.



LEO:  Thanks for viewing.  Happy to tell you more.  1.1.1.1 in the IP address of the site; and, yes, we have an SSL certificate for an IP address.  That's the key; right?  You can get one for an address.



STEVE:  Well, yes.  And the secret is...



LEO:  They have friends.



STEVE:  Well, yeah.  They have friends.  And it's the SAN field.  We've talked about this extension to certificates often.  That's the server alternative name.  And, for example, like GRC's certificate has GRC.com and I don't know what, www.grc.com.  I don't think it has star.  It might have www.grctech.com.  Anyway, the point is multiple alternative names for the same service.  And it turns out, there IP addresses are legal.  So you can put an IP address in the SAN field.  Let's Encrypt will not allow that.  And I had forgotten, if I ever knew it, that it was possible to put an IP address in there, but it is.



So the answer to this mystery is that when we went to look at it, that wasn't the name of the certificate.  The name was *.cloudflare or whatever it was dot something or other dot com.  And this was a DigiCert certificate.  And they arranged to have 1.1.1.1 placed in the enumeration of alternative names in that certificate.  Thus they're able to offer a TLS connection for their stuff.  So very cool.  And mystery solved.  And I wanted to close the loop on that.



Christian Alexandrov on the 19th tweeted:  "@SGgrc One short SpinRite testimonial."  He said:  "If an HDD [hard disk drive] wants to die, it will die.  My regular use of SpinRite on a drive gave me a lot of warning.  So I had the time to save all my data which I care about.  One more lesson for people:  Regular use of SpinRite warns you when something is going to happen."



And anyway, Christian, thank you for that.  And I will reiterate that we know that drives have that SMART data, the self-monitoring analysis and reporting tool, S-M-A-R-T.  The problem is it's only meaningful when the drive is being asked to do something.  If you look at the SMART data while the drive is sort of idling, maybe there's some information there, but it doesn't really tell you what's going to happen when you ask the drive to do some work.  If you make it sweat, that's when things happen.  And it's years of having that in, like, what, now, 14 years of that being available because I added that in SpinRite 6, has demonstrated that, when you're running SpinRite, if the drive is in trouble, you will start seeing red in the health bars that SMART is reporting.



Essentially, if running SpinRite pushes the drive's health down, that gives, I mean, the drive is still running.  Everything is still fine.  But a healthy drive won't have its health pushed down just by asking it to do what it was born to do.  So it really does provide a sort of an analog sense for how much left there is in this drive.  And as Christian says, if you use SpinRite and take a look at that SMART data, after the drive's been running for a while under SpinRite, if everything is still happy in the green, you're okay.  At least the drive is not saying, boy, you know, I'm having to work here in order to just read some data, which was what I'm supposed to be able to do easily.  That's, as Christian says, a nice early warning system for telling you maybe it's time to make sure you have a current backup.



Terry Daniel tweeted:  "@SGgrc Since SQRL tokens are linked to website name, is there a solution in SQRL for the problem that occurs when a web property changes its name," he says, for example, "United to Continental, or Ofoto to Kodak Gallery."  It's a great question, Terry.  And it's one that we've talked about in the SQRL forum and have a solution for.  It is the case that all of the security that SQRL offers, which as far as we know is absolute, that is, I haven't talked about it except in passing, we'll get around to it here in the future once I'm able to stop teasing people with this and actually let people play with it or have it more widely available to play with.



As far as we know, it is unspoofable.  I mean, really, we solved that problem.  And but that requires a tight binding between your identity and the domain name, or that absolute unspoofability guarantee would be broken.  But there is the ability to have an entity who has had to move itself to a different domain to bring SQRL users from the old domain to the new domain.  So the idea would be, if United purchased Continental, what would happen would be if a SQRL user then logged into United, United server would notice that, oh, someone's trying to log in with SQRL, but we don't recognize them.  The result of that would be a page that says, "Hi there.  You're trying to log in here with SQRL, but you might be a Continental user.  Try again."  And so that second page would be the SQRL at the Continental domain which United purchased.



And so the idea is that one last authentication at the domain that's being left, that's being abandoned, that is being moved from, allows United to identify the user under both the United.com and Continental.com domains because, after all, the person just tried to use SQRL at United, so United has their SQRL identity there.  That allows them to know who they were under the Continental domain and essentially migrate them without the user having to do anything except just try to log in under the old domain, and it's able to happen automatically.  Or, if they go to Continental.com, which is still up, then it's able to move them over.  So great question, Terry.  And SQRL does have a solution for that, which is a little bit of a double step, but only has to happen once, and then you're known under the new domain.



John Baxter says:  "Hi, Steve.  Do you have any information on how long external SSD devices can sustain their contents while not plugged in to power?  Specifically, the Samsung T5.  Clearly they aren't permanent, as the capacitors have to be leaking charge at some rate.  Two weeks seems to be okay.  I haven't found information at Samsung or in reviews."  He says:  "At two weeks, I could be running through spares way too fast, but I think I would have seen complaints.  One solution might be to store the devices plugged in on power, but at some point the better choice would seem to be spinning drives."



Okay.  So there are a couple misconceptions here.  I've referred to SSDs as having electrons stranded on a little pad, essentially.  That's different from a capacitor.  DRAM is a capacitor which is leaking charge very quickly.  So we know that it's surprisingly slow so that, for example, if you spray it with Freon, you can reduce the leakage of the DRAM's capacitors long enough to move it into a different piece of hardware sometimes and then bring it back to life and not have lost a lot of the DRAM's data.  SSDs are not capacitors.  They are an isolated gate technology where there is some leakage from electron tunneling, but it is really, really low.  And we've talked in the past about how, for example, there is a temperature sensitivity.  The hotter they are, the faster they leak.  But it is in years, not in weeks.



LEO:  Camera flash is not getting refreshed, and it sits for hours, days, weeks, months, and years.  And, I mean, as far as I can tell, if I took a picture and leave it on an SD card, it's there a year or two later.



STEVE:  Yes.



LEO:  So it's the same, basically, the same kind of technology; right?



STEVE:  Yes, it is the same technology.  And for what it's worth, Leo, if your refrigerator has any space available...  



LEO:  I would, but it's filled with, I don't know, what is it, Palm Pilots?



STEVE:  Palm Pilots, yes.



LEO:  Yeah.



STEVE:  So anyway, so there is a temperature-sensitive leakage.  So actually not having them plugged in would be better than having them plugged in.



LEO:  Oh, interesting.  [Crosstalk].



STEVE:  The colder, yes, the colder they are, the less quickly the stranded charges would tend to bleed off.  But it is in years.  And what you really need to do is like every year run SpinRite on them.



LEO:  Oh, good idea.



STEVE:  Because it is only by reading and rewriting an SSD that its charges are refilled.



LEO:  Ah, interesting.



STEVE:  That's the thing that recharges the SSD's storage over time.  And what's happened, of course, is that as we've pushed density higher, those SSD cells that used to be SLC, Single Level Cell?  Now they are MLC or TLC.  MLC was four levels of charge, and TLC is eight levels of charge.  So each bit cell now stores three bits.  Which means that it's more important that you distinguish the exact amount of charge in the cell than ever before.



That is, unfortunately, a tradeoff has been made for density over reliability, which is, for example, why I can't get them anymore, but the SSDs that the GRC server is running on are SLC SSDs.  And even then they're in RAID 6 because you know I'm belt-and-suspenders and SpinRite in order to have the greatest level of reliability.  But anyway, not to worry, and do keep them cool.  If you really are worried, stick them in the fridge.  It won't hurt them, and it will reduce the rate at which the charge is leaking from an offline SSD.



James tweeted:  "Listening to this week's episode of Security Now!.  Could you produce an app-specific public key that could be given to the bots" - oh, he's referring to our discussion about having a bot use SQRL if it needed to log into a site on your behalf - "could be given to bots that use your SQRL identity and have the main protocol authorize the use of that key for a specific app when you're connected to an authenticated session.  You could then manage on a per-site basis the apps that have authority to use your identity on that site, much the same way we use app-specific passwords on Google or other services.  You could then prune your app-specific keys in the SQRL app with any revocations being communicated to the site via the protocol and signed with your master identity to authorize the revocation."  Whew.  Okay.



So what he's saying essentially is could you authorize a different SQRL identity, which would be the bots, to log in on your behalf at a site?  And the answer is something we called "managed shared access."  And it is what I brought online Saturday, three days ago.  That's this thing that I've been working on for the last 90 days.  We had a definition of it in mind.  We had never created an implementation.  That's what I wrote.  I wrote a whole new server and an API, essentially a design to support that.



The problem is right now in the world where identity is, I was going to say loosely bound to people, but really not bound to people at all because it's not even a retina or a fingerprint or your face.  It's just your username and password.  People commonly share their usernames and passwords with other, for example, family members, when they need to share, they want to grant access to a website like Netflix, for example, or Mom and Dad both know the username and password for the bank.  Well, in a world which we are arguably moving toward, where identity is more tightly bound because we've got biometrics of one form or another, or something like SQRL, which is a proxy for you, that's inherently hostile to sharing your identity.  You don't want to share your SQRL identity with anyone because it doesn't just represent you at one site.  It represents you everywhere.  That's the benefit and the power.



But with that comes this problem that we're used to just giving away usernames and passwords when we want to share.  So what we need is we need services that support SQRL to have as an option, not necessarily, but as an option, the ability to allow multiple SQRL identities to all share an account.  And we have that.  It's a well-defined spec.  Everybody who's listening to this will be able to play with that.  The guys in the SQRL newsgroup since Saturday have been playing with it.  And it's very cool.  It is no decrease in security, and it provides a means for sort of an owner of the account to invite others to use their SQRL identities there and to curate who has what level of access and to eliminate people after they've wandered off or you no longer want to share your access.



So managed shared access.  It's in the API.  It exists.  And James, you foresaw, essentially, a need that we did, too.  And it's part of the solution.  And as I've said before, and I'll be talking about it again in a minute when we talk about what Troy Hunt was saying, I believe every problem has been solved.  Other solutions have not solved every problem.  I think every problem has been solved.  And it's why I think it has a chance to succeed.  We'll see.  I'm happy to let the world judge that.



Yodar44 said:  "Steve, in SN-689 you focused on BitLocker on SSDs."  That of course was last week's podcast, "Self-Decrypting SSDs."  He says:  "Do the same concerns apply to BitLocker on spinning drives?"  To which I would answer the same concerns, absolutely.  What we don't have is research for whether spinning drives have been as negligent, frankly, as at least we know Samsung, and was it Corsair, the two brands of SSDs that those guys dissected.  It's not until you look, unfortunately, that we know what's going on underneath the covers.  Because these guys looked, we realized, oh, crap, this is not providing actual security, which is one of the reasons why it's a problem that these things are black boxes to us.  These guys went to great lengths to look inside.  So I would say yes, until we know definitively that a drive has implemented its encryption properly, you cannot trust a drive.  You have to revert to software solutions.  And CPH...



LEO:  Which drives, just to recap, do it right?



STEVE:  Well, we don't know of any that do it right.



LEO:  Oh.  There you go.



STEVE:  Yeah.  But we're assuming that Samsung, and was it Corsair?  Doesn't feel like it was Corsair.  Corsairs are...



LEO:  Crucial?



STEVE:  Crucial, yes.  Samsung and Crucial have updated - actually I think I saw that the MX300, that's the Crucial drive, had not yet been updated.  But when they update their firmware, we hope they do so in a way that would satisfy these academics who looked into them and found them wanting.  But, I mean, what we really need is - because this is not, like, implementing this correctly, that's not proprietary.  It's not something that the drive needs to keep secret from everybody else.  We ought to have, and the Trusted Platform Computing group ought to provide, an open source reference for here's how you implement this overly complex security standard which we defined.  So just code this into firmware and do it.



And if we knew that that had been done, then good.  It'd be like the reason we trusted TrueCrypt and we trust VeraCrypt is it's open source.  And it's been vetted multiple times.  So we know what it does.  Drives, we've been assuming, oh, they say it's hardware, so it must be better.  No.  Now we know that, you know, that research put the lie to that assumption.



LEO:  Yeah.  The research was Samsung and Crucial didn't do it right, and those companies responded by patches.  But I'm reading Samsung's support page.  It says for non-portable SSDs like the Samsung EVO drives that are very widely used, in fact it's my number one choice...



STEVE:  Yes, yes.



LEO:  ...we recommend installing encryption software.  So they did update.  That's the portable SSDs like the T5 our previous guy was talking about, and the T3.  And I use those.  But you first have to reinstall the portable SSD activation software and then update the firmware.  And if you have a T1, well, you have to contact the Samsung Service Center.  So the portable SSDs are, with maybe some effort, made secure.  But it doesn't look like they have any plans for making the most common SSDs in use, the EVOs, secure.



STEVE:  Wow.  Wow.



LEO:  So you should use, I mean, the answer is you should use VeraCrypt.



STEVE:  Yes, that is the answer.



LEO:  That's what Samsung says.



STEVE:  Yes.  Or BitLocker, when you tell it not to use hardware.  Which...



LEO:  Oh, yeah, you can tell BitLocker not to do that, yeah, okay, yeah.



STEVE:  Yes.  CPH said:  "Steve, Windows 7 BitLocker is unaffected by the hardware encryption problem because Windows 7 BitLocker doesn't support leveraging hardware encryption in the first place, so VeraCrypt is not the only recourse there."



LEO:  [Crosstalk] Microsoft updates; right?  Because now they know there's a flaw, and they should just update all BitLocker implementations to do software encoding.



STEVE:  Yes, they ought to just back off of, yes, that's exactly right.  I think that makes a lot more sense.  I'm going to skip one because it's overly long and not necessary.  Mikael Falkvidd said:  "Re SSD encryption," he says, "the key cannot be derived from the password."  He says:  "The password must be used to unlock the key.  Otherwise it would be impossible to ever change the password."



LEO:  Good point.



STEVE:  "And it would be impossible to start using a password without wiping the entire drive."   He says:  "I might have misunderstood your description," so he says, "/rant," meaning closing rant.  He says:  "If that's the case, I'm sorry, but I would love a clarification."  So the clarification is there's an additional what we would call a "level of indirection."  So the key that you are using is not the one that encrypts the drive.  That one is the drive's master key, which never changes.



So the way you change your password is you give it the old password, which is used to decrypt the key, not unlock it, to decrypt it.  And now it's holding its breath, and it's decrypted.  Now you give it the new password, and that decrypted key is then reencrypted under the new password.  So that's how you perform a change of password without losing the contents of the drive is you have both, you know, you always want the old password to prove you're you, and then the new password.  So that's a similar sort of a straddle, kind of like what SQRL does from one domain to another, is you decrypt under the old password and then immediately reencrypt under the new password.  And that way you're able to make a change.



Tinzien says:  "Hi, Steve.  In the past you advocated using Windows Defender plus Malwarebytes.  With the new Win Defender sandbox, are you suggesting no Malwarebytes is needed/is a vector for problems?  Or is this an omission?  Thank you for your continued great work."  And when I have suggested Malwarebytes it's only to use it transiently as a scanner, never to install it.  So that's the source of the confusion is I do like Malwarebytes, and I have used it sometimes to scan a system just as an additional check when something seemed a little weird, just to make sure nothing was getting away from Windows.  So I use it only to scan, never to install it permanently.  And I think, as we've been saying now, you and I, Leo, for a while, I think Windows Defender is the answer for this.



LEO:  Now for the fun final - what are you laughing at?



STEVE:  I'm just smiling.



LEO:  Are you laughing at Mark Zuckerberg, who is enjoying the show today?



STEVE:  So Troy Hunt on passwords.



LEO:  Oh, this is good.  I read this article.  I thought, Steve's going to have something to say about this.



STEVE:  Yeah.  So just to recap, before I share what Troy said, three days ago, Saturday afternoon, I put what I've been working on for the past three months online, to be pounded on and commented upon by our wonderful group of testers, techies, and developers over in GRC's SQRL newsgroup.  So far, since then, I've fixed a few bugs, and I have a short to-do list of features to add or tweak.  This was an important piece of work since it defined and has now proven and verified that I got it right a minimal and workable generic SQRL service provider API which allows a web server to query an external SQRL provider for all of the site's SQRL support, so it creates a well-defined boundary.  The provider still needs to be present on the web server's domain, but this allows for a clean externalization and a well-defined boundary between SQRL's authentication functions and an existing website.



Fifteen days ago, on November 5th, while I was working on this, so I didn't respond to it until I've come up for air, Troy Hunt, who as we know is the HaveIBeenPwned guy, and he's a prolific writer about security, and he knows what he's talking about, posted an article which he titled "Here's Why," and then he says "[Insert Thing Here] Is Not a Password Killer."  And I have a link in case anyone's interested.  But I'm sure if you put "here's why insert thing here is not a password killer" into Google, it'll find it.  So this is what Troy wrote.



"These days I get a lot of messages from people on security-related things.  Often it's related to data breaches or sloppy behavior on behalf of some online service playing fast and loose with HTTPS or passwords or some other easily observable security posture.  But on a fairly regular basis I get an email from someone which effectively boils down to this:  'Hey, have you seen [insert thing here]?  It's totally going to kill passwords!'"  And Troy says:  "No, it's not.  And to save myself from repeating the same message over and over again, I want to articulate precisely why passwords have a lot of life left in them yet.  But firstly, let me provide a high-level overview of the sort of product I'm talking about, and I'll begin with recognizing the problem it's trying to solve:  People suck at passwords.



"I know.  Massive shock; right?  They suck at making genuinely strong ones, they suck at making unique ones, and they suck at handling them in a secure fashion.  This leads to everything from simple account takeover (someone else now controls their eBay or their Spotify or whatever), to financial damages (goods or services bought or sold under their identity), to full data breaches (many of these occur due to admins reusing credentials).  There is no escaping the fact that passwords remain high-risk security propositions for the vast majority of people.  Part of the solution to this is to give people the controls to do password-based authentication better, for example by using a password manager and enabling two-factor authentication.  But others believe that passwords themselves have to go completely to solve the problem, which brings us to proposed alternatives."



He says:  "I don't want to single out any one product out there because the piece I'm writing is bigger than that, so let's talk about patterns instead.  I'm referring to passwordless solutions that involves things like QR codes, pictorial representations, third-party mobile apps, dedicated hardware devices, or 'magic' links sent via email.  I'm sure there are others; but for the purposes of this post, any pattern that doesn't involve entering a username and password into a couple of input fields is in scope.  To their credit, some of these solutions are genuinely very good, technically very good.  But what proponents of them seem to regularly miss is that 'technically' isn't enough.  Despite their respective merits, every one of these solutions has a massive shortcoming that severely limits their viability, and it's something they simply can't compete with.  Despite its many flaws, the one thing that the humble password has going for it over technically superior alternatives is that everyone understands how to use it.  Everyone."



LEO:  Yeah.  Not how to use it securely or well.



STEVE:  Yes.



LEO:  But they do know how to use it.



STEVE:  Yes.  It is literally the definition of the lowest common denominator. 



LEO:  Yeah, yeah.



STEVE:  And he says:  "This is where we need to recognize that decisions around things like auth schemes go well beyond technology merits alone.  Arguably, the same could be said about any security control.  And I've made the point many times," he writes, "before that these things need to be looked at from a very balanced viewpoint.  There are merits, and there are deficiencies.  And unless you can recognize both, regardless of how much you agree with them, it's going to be hard to arrive at the best outcome."



He says:  "Let me put this in perspective.  Assume you're tasked with building a new system which has a requirement for registration and, subsequently, authentication.  You go to the marketing manager and say, 'Hey, there's this great product called'" - and he has, you know, "[insert thing here]" - "'that replaces passwords, and all you have to do to sign in is...'    And you've already lost the argument because the foremost thing on the marketing manager's mind is reducing friction.



"Their number one priority is to get people signing up to the service and using it because, ultimately, that's what generates revenue or increases brand awareness or customer loyalty or achieves whatever the objective was for creating the service in the first place.  As soon as you ask people to start doing something they're not familiar with, the risk of them simply not going through with it amplifies and defeats the whole point of having the service in the first place."



And he goes on, and it's long, and I don't want to skip anything, but everyone should have the point of what he's talking about.  I mean, I just saw him saying right here:  "What I often find when I have these discussions is a myopic focus on technical merits.  I'll give you an example from earlier last year where someone reached out and espoused the virtues of the solution they'd built."  And I should mention I've never spoken to Troy, so it wasn't me.  Because SQRL's not out yet.



He says:  "They were emphatic that passwords were no longer required due to the merits of [insert thing here] and were frustrated that the companies they were approaching weren't entertaining the idea of using their product.  I replied and explained pretty much what's outlined above.  The conversation is going to get shut down as soon as you start asking companies to impose friction on their users.  But try as I might, they simply couldn't get the message.  'What barrier?  There's no barrier.'  They went on to say that companies not willing to embrace products like this and educate their users about alternative auth schemes are the real problem, and that they should adjust their behavior accordingly."



Troy says:  "I countered with what remains a point that's very hard to argue against:  'If your product is so awesome, have you stopped to consider why no one is using it?'  Now, in fairness, it may not be precisely 'no one.'  But in this case and so many other of the [insert things here], I'd never seen them in use before, and I do tend to get around the Internet a bit.  Maybe they're used in very niche corners of the web.  The point is that none of these products are exactly taking the industry by storm, and there's a very simple reason for that:  There's a fundamental usability problem.  This particular discussion ended when they replied with this:  'I think it is only negativity that doesn't allow positiveness to excel.'"  He says:  "Ugh."  He says:  "I'm negative about stuff that's no good, yes."  He says:  "I dropped out of the discussion at that point."



Anyway, he says:  "This is why passwords aren't going anywhere in the foreseeable future and why [insert thing here] isn't going to kill them.  No amount of focusing on how bad passwords are or how many accounts have been breached or what it costs when people can't access their accounts is going to change that.  Nor will the technical prowess of [insert thing here] change the discussion because it simply can't compete with passwords on that one metric organizations are so focused on:  usability.  Sure, there'll be edge cases, and certainly there remain scenarios where higher friction can be justified due to either the nature of the asset being protected or the demographic of the audience.  But you're not going to see your everyday ecommerce, social media, or even banking sites changing en masse.



"Now, one more thing," he says.  "If I don't mention biometrics and WebAuth, they'll continually show up in the comments anyway."  And so he talks about them and how there are some emerging standards, but he acknowledges they're many years out yet.  Anyway, he concludes:  "This is why [insert thing here] is not a password killer; and why, for the foreseeable future, we're just going to have to continue getting better at the one authentication scheme that everyone knows how to use:  passwords."



Now, okay.  So that's what he said.  And I just wanted to address it head on.



LEO:  Yeah, but you could use the same words about SQRL.  I mean, it's not out yet, so it's not like anybody could have adopted it.



STEVE:  Correct.



LEO:  We do have somebody with a license plate "SQRL," however.  So you've got that going for you.



STEVE:  Yes.  Our Picture of the Week shows the back of a Jeep with an Arizona-registered license plate, "SQRL."  I don't know what the...



LEO:  You don't know this guy.  



STEVE:  Don't know who that is.  But bravo.



LEO:  SQRL.



STEVE:  And so I will admit that I got a kick out of the fact that the very first reply posted to Troy's article was from a David A. Leedom, 15 days ago, who just wrote:  "Has anyone looked at SQRL?"



LEO:  Thank you.



STEVE:  Yes.  And there was pretty much, I guess maybe, I don't know, half of the dialogue, there was a lot of dialogue, were people going back and forth.  And actually some online names I recognized from GRC's SQRL newsgroup and Twitter were present.  The point is, I think, skepticism is a healthy trait, and it doesn't offend or annoy me in the slightest.  But what I have also observed is that anyone who has actually seen SQRL and experienced it is immediately and irrevocably converted into a true believer.  I think that what confuses people is how patient I am.  I deeply believe in the truism that we only get one chance to make a first impression.  So I'm working as hard as I can, day and night, so that everyone's first impression will be all that it can and should be.



When I first talked about this on Security Now!, there were a bunch of people who didn't pay close attention, who created blog posts that attacked it, not because there was anything wrong with it, but just because they didn't know what it was, or sort of like this.  It's like, well, [grumbling sounds].



So anyway, a few points to Troy's article.  First of all, as we know, it's not a product.  I think that's probably why it has a chance to succeed.  It isn't something you have to subscribe to or pay for or anybody is making any money from, which frankly boggles the minds of friends who I talk to about it when I say, no, it's free.  No, I'm just doing this because someone has to so we'll have it.



So as we know, it's a solution and a protocol which by design no one owns.  I have simply built an implementation of a SQRL client and now several implementations of SQRL servers.  That was done to work out all the details and the edge cases.  If I were to do it again, I would likely have built my SQRL client as a web extension, since SQRL really should be integrated into every web browser.  Someday, if it succeeds, it will be.



But back then web extensions didn't really exist, and they weren't unified when I began this.  And there is a web extension in the works by someone else for Firefox and Chrome.  And as for no one owning it, there is also a working Android client over on GitHub whose SQRL implementation is currently available in Arabic, Chinese, Dutch, English, French, German, Hebrew, Japanese, Norwegian, Russian, Spanish, and Swedish.  It's working great.  There's a native client coming for Linux, and there's a working client for iOS.  GitHub also has a crypto library for SQRL that people can use to build into their own apps.  And there are server implementations for Java and PHP in the works.



And I'll note that all of these other people are not nuts.  They're not crazy.  They have one thing in common, which is what for the moment separates them from the rest of the world.  They have seen SQRL work firsthand, and they immediately realized what it means.  And I know I'm teasing everyone, but this has been a passion of mine for years.  I'm beginning to get excited about it because it won't be long now until everyone will have the chance to play with it for themselves and see and judge what this might mean.  And as for replacing what we have now, SQRL is quite happy to coexist alongside usernames and passwords, to simply be there as an option and benefit offered by websites who choose to offer it.



And like at any website, if someone is a SQRL user, having created a SQRL identity, and wishes to associate their SQRL identity for that site with their existing identity at a website, they can easily do so.  The demos that I have online and which everyone who's listening to this will be playing with before long, allows you to experience that.  You can use a username and password to create an account at this demo site.  And then it says, oh, you don't have your SQRL identity associated.  And so you can do that, and then you log in with SQRL.



So this has all been designed to allow SQRL adoption to occur incrementally.  And, for example, I used this a long time ago, but it still holds.  How many times have you read someone's blog, and you've thought, ooh, I have something to add to that.  And so you go to click on Reply, and the first thing it asks you to do is create an account.  What's your email address?  And you look at it, and you think, oh, forget it.  No, I'm not.  Not one more place on the Internet.  It's not worth it.



Well, there's a perfect example of where friction, account creation and login friction, authentication friction is a limiting factor.  For those sites to have SQRL means you don't need to do anything except click on the "Login with SQRL," and you are now uniquely identified at that site, now and forever, never having to do anything else.  So my point is I believe there are places where account creation friction matters enough that adding it, which is easy to do, will be enough.  And what'll happen is, as people incrementally begin to get used to it - and, I mean, what you ask yourself when you experience this is, wait a minute, what just happened?  This is secure?  And the answer is yes.  There's no more usernames and passwords being breached and lost and getting out in the wild because SQRL doesn't give web servers any secrets they have to keep.  There's nothing that they have to do.  So, I mean, it really is a completely different solution.



So anyway, I agree with Troy 100% that nothing yet has been good enough to supplant usernames and passwords, and that they are here, usernames and passwords, because they've always been.  And I also agree that any real meaningful change takes time.  We all know about inertia.  That's the lesson we're learning on this podcast all the time.  IPv6.  How many decades old is that?  And it's still trying to happen.  And TLS 1.0, 20 years old, and it's only now beginning to get itself deprecated.



So anyway, I guess my main point is that until and unless there is a perfect alternative available, just available, and where "perfect" in this context means it's open, it's free, it's zero-friction, and it answers every problem, which I believe we're going to see SQRL does.  Until such a thing exists, nothing will ever change.  That change cannot be forced.  We know that.  I agree with Troy 100%.  And I would be happy to let SQRL speak for itself.  I think we're going to find that it will be able to.  And I will be surprised if it doesn't happen.



On the other hand, I'm looking forward to getting back to SpinRite, once this thing is launched and out of my hands because I want to get back to SpinRite.  I've had enough of SQRL.  And really, I don't have a horse in the race.  I just wanted to solve the problems.  And I think we're going to see that they're solved.  So we'll see.



LEO:  I think a lot of people's questions about it just really come from at this point not really having any idea how it works.  I mean, we've showed it.



STEVE:  Yup.



LEO:  It's just, even myself, I keep going - on and on.  But soon.  Soon.  Soon.



STEVE:  Yes.  Yes.  Yes.



LEO:  Anything that makes authentication work will be much improved.  Much improved.



STEVE:  Well, and I wouldn't be surprised if things like password managers, LastPass could easily adopt, I mean, it's a perfect place to add SQRL support.  So for sites that don't yet support SQRL...



LEO:  That's where you've got to go, yeah.



STEVE:  You use LastPass, and let LastPass be your SQRL client for what I imagine will be an increasing number of sites that will adopt it over time.  Again, it's not an all-or-nothing.  It doesn't have to, you know, no one's trying to make any money on this.  It has to be free.  But it also has to be done right, or it'll just stumble, as other solutions have.  I don't think this one will.  We'll see.



LEO:  I'm rooting for you.  That's the case.  I can't wait to get rid of passwords.  I think that's a good - actually, that's an interesting implementation.  If it goes in your password manager, then SQRL gets used wherever it's available.  And over time your password manager shrinks.



STEVE:  Yeah.



LEO:  It gets simpler and simpler, yeah.



STEVE:  Yeah.  It just - yup.



LEO:  Does SQRL allow for or require two-factor?  Or is it just...



STEVE:  No. 



LEO:  It doesn't need it.



STEVE:  It doesn't.  And that's the magic.  As I've said, the only reason we need multiple factors is none of them by themselves are secure.



LEO:  Right.



STEVE:  So what SQRL is, it is secure single-factor.  That is, I mean, and it's not - it's both username and password.  That's what's freaky.  You don't have to tell it who you are, then prove who you are.  It asserts and proves your identity all at once.  So, I mean, I showed it to Lorrie because I just, on Saturday, I brought up this new demo site.  So I had the iOS client on my iPhone X, and I brought up the site on an iPad, and it was just sitting there.  And I let the phone see the QR code, and then I let the phone see my face, and the page on the pad went blip, and I was logged in.



LEO:  So cool, yeah.



STEVE:  And she said, "Why wouldn't anyone want this?"  I said, "I know."  Well, for one thing, nobody has it yet.  I mean, no one has had a chance to because it needed to get done right.  And also remember that the QR codes, that's only for optical - to use your phone as the authenticating device.  Most people will just click on the "Login with SQRL" on their computer, and so there's no phone involved.  Anyway...



LEO:  It's basically SSH login, the public/private key login that I use on my SSH server; right?  Basically?



STEVE:  No.  No.



LEO:  No?  Okay.



STEVE:  No.  No.  I mean, it's...



LEO:  It's more than that.  Okay.



STEVE:  Well, I mean, if you lost your key, if someone got a hold of your SSH key, you'd be screwed.



LEO:  My private key, yes.



STEVE:  Yes.  With SQRL you're not.  You can get it back.



LEO:  Oh.  All right.  That's cool.



STEVE:  Oh, I mean it's, you know, again, what we'll do is I want to do a confrontational, you know, you, Mike Elgan, and Jason probably, or whomever.  John, if he wants.  Anybody.  Have a little roundtable and have you guys - I'll show it to you.  I'll explain it to you.  And then hit me.  Because there is an answer for every possible but what about, but what about, but what about.



LEO:  You know what I want?  You know what I'd like to do is maybe get Troy in or Joe Siegrist, somebody like that.  Because honestly, what we really have to do is just get the folks from LastPass to implement it, and it would just take off.  They're going to say, but we don't own it.  Well, that's right.  You don't own passwords, either.



STEVE:  Exactly.



LEO:  That's right.



STEVE:  Exactly.  But if you want a nice integrated solution, I mean, they could say, if they see the handwriting on the wall, it's like, wait a minute, our only chance not to lose people forever is to be offering the alternative at the same time.



LEO:  Plus it would help the transition from - because a password manager would continue on legacy sites to unlock the password. 



STEVE:  Yes.  They're never going to go away, ever, ever, ever.  I recognize that.  And in fact one of the really cool things, Leo, is once you're comfortable with SQRL, in the UI you can set a flag, an option button that says request only SQRL authentication or something.  I don't remember the exact jargon.  But the idea is that then, once you understand how SQRL works, as you visit sites and use SQRL there, the site sees the request saying shut down non-SQRL authentication.  So your username and password no longer works there.  But not only for you, but for anybody else.  That is, it locks the bad guys out.  Remember that having SQRL authentication, one aspect is that it's a convenience.  But we also want it to be much more secure.  Well, the only way for it to be much more secure is if the old crap that isn't secure goes away.  And we know that username and password isn't going to go away.  But SQRL can ask that it be disabled.  And so sites that want to fully support that can, and increase their security.  I mean, it's just - it's full of goodies.



LEO:  Well, soon; right?  Soon; right?



STEVE:  We'll get there.  We'll get there.



LEO:  I understand your reluctance.  I think you actually for the first time have really clarified that it's because you only get one chance for a first impression.  You really want to make sure it is locked down.



STEVE:  Yes.  Because you can't...



LEO:  And absolutely perfect.



STEVE:  You can't come back and have people go, oh, well, remember?  Oh, yeah, we fixed that.  It's like, unh-unh, no.



LEO:  That makes a lot of sense.  All right, Steve.  Another great show in the can, as they say.  Thank you for joining us.  We do this each week on Tuesdays, 1:30 Pacific, 4:30 Eastern, 21:30 UTC.  You can watch live at TWiT.tv/live, watch or listen.  We've got streams, both audio and video.  You can also chat in the chatroom at irc.twit.tv.  Those are the people watching and listening live at the same time as you.  It's a nice community in there.



If you want on-demand versions, Steve's got them at GRC.com, that's his website, the Gibson Research Corporation, along with SpinRite, the world's best hard drive recovery and maintenance utility.  If you want to know more about SQRL, it's all there, too, along with a ton of other free stuff Steve gives away.  It's very simple, just go to GRC.com.  He also has transcripts of every show there.



We have audio and video at TWiT.tv/sn, and you can of course subscribe in your favorite podcast application.  That way you'll get it automatically.  I think this is one show, unlike any other show we do, where you want to collect every episode.  Get them all.  Keep them because it's a library of good information.  Thank you, Steve.  We'll see you next time on Security Now!.



STEVE:  Thank you, my friend.  Bye.  Happy Thanksgiving.



LEO:  Thank you.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#691

DATE:		November 27, 2018

TITLE:		ECCploit

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-691.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  Hackers and attackers apparently enjoyed their Thanksgiving, since this week we have very little news to report.  But what we do have to discuss should be entertaining and engaging:  Yesterday the U.S. Supreme Court heard Apple's argument about why a class action lawsuit against their monopoly App Store should not be allowed to proceed; Google and Mozilla are looking to remove support for FTP from their browsers; and from our "What could possibly go wrong?" department we have browsers asking for explicit permission to leave their sandboxes.  We also have some interesting post-Troy Hunt "Are Passwords Immortal?" listener feedback from last week's topic.  Then we will discuss the next step in the evolution of RowHammer attacks, which do, as Bruce Schneier once opined, only get better - or in this case worse.



SHOW TEASE:  Hey, it's time for Security Now!.  Kind of a slow week in security.  I guess the hackers took Thanksgiving off.  But Steve does have a "What could possibly go wrong?" segment, a picture of something that really did go quite wrong, and a look at the next-generation Rowhammer.  They're calling it "ECCploit."  Stay tuned.  Security Now! is next.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 691, recorded Tuesday, November 27th, 2018:  ECCploit.



It's time for Security Now!, the show where we cover your security and privacy online with our friend, it's kind of like your neighbor, Steve Gibson, if your neighbor were like the king of computing.  Hello, Steve.



STEVE GIBSON:  Hey, Leo.  It's great to be with you again for this post-Thanksgiving episode of Security Now!.  In gathering together all of the news of the week, I came up kind of short.  Luckily we had a bunch...



LEO:  Oh, good.  I don't mind.



STEVE:  Yeah, I think the hackers and the attackers all...



LEO:  They took the week off.



STEVE:  They enjoyed their long Thanksgiving week.  Of course that's not, I guess, globally celebrated.  But anyway, it was slow.  We do have some interesting things to talk about.  I wanted to give my own take, I saw that you guys led off MacBreak Weekly with it, to talk about what was initially my concern over Apple's presentation yesterday in front of the Supreme Court because we know how important it is to have a curated App Store.  And I was like, oh, goodness.



LEO:  That's a good point.  There's a security angle on this.



STEVE:  Oh, it's like all - it's all there is.  And we also have Google and Mozilla looking to remove support for, believe it or not, I mean, you will believe it because who uses it anymore, FTP from web browsers.



LEO:  Oh, I guess, yeah.



STEVE:  It's not gone yet, but it's always been there.



LEO:  It's still in there?  What a surprise.



STEVE:  Yeah, I know.  Like whoever uses that?  Which is not to say there's anything wrong with FTP.  Calm down, you old-timers.  It has its purposes.



LEO:  I still use it all the time, yeah.  I use SFTP.



STEVE:  Well, and there are, yes, there are the alternatives people want.  Then we have, from our "What could possibly go wrong?" department, browsers considering looking for permission to leave their sandboxes.  And it's like, oh, really?  We have some interesting post-Troy Hunt "Are Passwords Immortal?" listener feedback from last week's topic.  And then we're going to take a close look at the next step in the evolution of Rowhammer attacks.



LEO:  Oh, boy.



STEVE:  Which do, as Bruce Schneier once opined, only get better.  Or maybe worse.  Anyway...



LEO:  More successful.



STEVE:  Yes.  As a consequence, and this comes from our friends Herbert Bos and his team, although a different team, at VU [Vrije Universiteit] in Amsterdam.  Anyway, they named it ECCploit.  And when I was kind of looking at that, I realized, okay, how would you pronounce that?  Well, ECC is "eck," so it's "exploit."



LEO:  Oh, "exploit."  Oh, clever.



STEVE:  Oh, and Leo, we have a Picture of the Week that, again, the old-timers here, they'll be like, "Oh, I recognize that.  That looks like what I have."  Anyway, a lot of fun I think this week for our listeners, and a bunch of good stuff.



LEO:  Awesome.  I guess do you want to do the Picture of the Week?



STEVE:  I've been staring at it.  And I was imagining you accept a job, a new job as the IT guy at some company, and your boss says, oh, and we have an intermittent connection somewhere.



LEO:  Can you find it?  This is somebody's wire closet.  But you know what?  I've been in that closet.



STEVE:  Oh, I think we all have.  I don't know what it is about these closets that evolve this way.  Almost without exception they start off neat and planned.  In fact, you can see on the back wall it looks like there are some nice loops of a black something, like they're all tacked down.  There was like, somebody had this dream of we're just going to have the nicest wire farm.  And oh, my goodness.  I think my favorite one is this tan one that's coming across the walk path.  I mean, it's one thing for them just to be kind of going down on the sides.  But, yeah, you have to duck under this one that's going kind of diagonally from left to right.



LEO:  Oh, lord.  This is not as bad - we weren't as bad.  But it is a little bit what it looked like at the Brick House because we rushed over there, and we added stuff.  But when we moved here - and really kudos to Alex Gumpel, Burke McQuinn, and John Slanina, our engineering team; and Russell, too.  When we moved here, we already kind of knew all the connections, all the boxes.  We already had everything.  And they took the time.  And I'm proud to say our server room does not look like this.



STEVE:  Ooh, god.  I just noticed.  Look at the top in the middle where there's other wires that are looping over and pulling down.



LEO:  Oh, no, no.  That's ridiculous.



STEVE:  So they had to cross over, but they didn't want to block the walk path.



LEO:  Oh, geez, Louise.  Oh, man.



STEVE:  So they hung them over the other wires.



LEO:  Oh, man.



STEVE:  It's, like, pulling them down.  It's like, oh, yikes.  Yeah, I think, you know, remember the old saying, no battle plan survives first contact with the enemy.  And there's an analogy there.  It's like you initially know where everything should go, and you lay things out, and then something comes along, and you've got to change things, and you don't have time.  So it's like, okay, well, I'll just do this.  Just a kind of hack here, real quick.  We'll just run these wires over here, and then I'll get back to this when I have the time.  And then something else happens before you are able to fix that last one, and it piles up.  And the result is this.



LEO:  This is our old wire closet before we moved.  And, well, it's not that bad.  Although if you go around behind it, it is pretty...



STEVE:  That was the basement below?



LEO:  Yeah, this is the basement in the Brick House.



STEVE:  Yeah, it was, well, and you guys really, you had an advantage of an entire floor that was essentially your wiring closet.



LEO:  Right.



STEVE:  I mean, you were able to drill down wherever you were, and you'd be down in a completely available space to then run cables to somewhere else.



LEO:  Do you have a picture anywhere of - by the way, there's the engineer, Scotty, who is the patron saint of our closet.  Do we have a picture anywhere of our current setup?



STAFF:  I can go take one.



LEO:  Go take one and send it to me because I want - I'm very proud - John just came in.  I'm very proud of what our team did because they made it look beautiful.



STEVE:  And it is a challenge.



LEO:  And you know it's important because if you have, exactly as you said when you began, if somebody says, hey, there's some intermittent problem, you don't want to be the one that has to go in there.



STEVE:  It's like, I can't even see the lights.  What are you talking about?  They're just, like, totally obscured.  So, okay.  So while we're waiting for the photo of...



LEO:  Much improved.



STEVE:  ...the pride of your current wiring closet, I wanted to bring up a topic that you guys mentioned on MacBreak Weekly at the top of that podcast, which I took from a Security Now! angle.



LEO:  Yeah.  And that actually - I'm thrilled you did because we were discussing the Supreme Court hearing yesterday over the - really it was more about the standing of the plaintiffs in a class action lawsuit against Apple, claiming that the App Store was a monopoly.



STEVE:  Right.



LEO:  And that people should be able to buy apps from other sources on iPhones.



STEVE:  Exactly.  And that's of course what set me off because we know how unable to make those kinds of decisions end users are.  They're just not - they shouldn't have to be, and they're not, equipped to protect themselves.  So at issue is whether Apple's App Store, through which as we know it alone can offer and sell applications for its mobile devices, constitutes a monopoly and gives it monopoly power over all sales of iOS platform apps.  And as you mentioned before correctly, the lawsuit, which is known as Apple Inc. v. Robert Pepper et al., it's a class action.  And I feel as we all do, and as you said on the podcast, these things are generally just for ambulance-chasing attorneys to line their pockets, and the people who are members of the class typically receive pennies.



So this suit has been around for almost a decade, and it argues that Apple's 30% cut, which they take of all App Store sales revenue, is excessive and anti-competitive and amounts to price gouging since consumers have no alternative other than to purchase apps for their devices at Apple's "inflated" prices.  On the other hand, come on, like, really?  But this issue is, as I said, of interest to us on this podcast because we've often noted that consumers can act unwisely and may be poorly informed about the true dangers of obtaining applications which have not been rigorously scrutinized, vetted, and curated - which is, I think, a vital service that Apple performs, and not easy.  I mean, we've often talked about how difficult it is.  And yes, they make mistakes.



But largely, for example, it is that and some of the other lockdown, one could argue, monopolistic things that Apple does are what makes the iOS a more secure platform today than Android.  And we know that Google has been making efforts to sort of catch up in that regard.  So as is often done in the law, and as you mentioned, Leo, Apple is not defending itself against that allegation.  But their attorneys said, oh, here's how we'll spin this.  What they've been arguing is whether the consumers who are bringing this complaint have grounds for that complaint in the first place.  And this is known as "standing."  Apple is arguing with the plaintiffs, essentially, of these litigants, whether or not they have standing to sue Apple over any of this.  So, I mean, aside from even looking at the issue.



Apple's taken the position that only its developers who are being charged a 30% sales "commission," as Apple frames it, on the retail sales of their apps, would be damaged if Apple's conduct was found to be unlawful, and that it would be up to them to complain, that is, "them" the developers, not the end consumer.  The precedent which the Supreme Court is revisiting was - and that's the reason it's gone to the Supreme Court after Apple has already won in the appellate court.  I mean, so the U.S. Department of Justice has already come down on Apple's side.  So it's like, okay, we're taking this to the Supreme Court.



So the precedent was set back in 1977 in a case known as Illinois Brick Co. v. Illinois, which was a dispute in which the court ruled in favor of concrete brick manufacturers.  The state of Illinois had sued the brickmakers for allegedly inflating their prices, causing an increase in the cost of public building projects, which of course the State of Illinois was upset about.  The court ruled then that, even though the increased brick costs might hurt Illinois indirectly, only the contractors who actually bought the bricks had standing to sue.  And so that established what has become known as the Illinois Brick Doctrine, which gets hauled out every time an issue like this comes up.  And that says that only the direct purchaser of a good can collect damages from a monopoly holder.



So as I said, the U.S. Department of Justice has already ruled that the Illinois Brick case was controlling here, whereas they're arguing that the Court of Appeals misapplied the Illinois Brick Doctrine, which of course is what they want the Supreme Court to agree to.  So Apple argued yesterday that it is not directly selling apps to iPhone users, rather that they're acting as an agent for app developers who ultimately are selling their wares to consumers.  And so they're saying, in exchange for the commission Apple takes on app sales, the company provides access to its vast user base and performs other services such as malware detection and so forth.



So anyway, we know where we are on this.  One way or the other, it would be a disaster if, I mean, we're a long way away from this happening.  But it would be a disaster if the government ever forced iOS apps to be available from anywhere that a user wanted them.  The presumption is, oh great, we'll save 30%.  First of all, no.  It's not, you know, you're not going to get something useful for less than $0.99 or free with in-app purchases.  That's just not going to happen.  But mostly we lose the ability for Apple to control this very attack-prone surface that our mobile platforms represent and just stop being able to protect consumers from themselves because we just know people would just be downloading stuff from everywhere, and it would be a disaster.  So, yikes.  And, you know, that's pretty much what you guys said on MacBreak.  I thought it was interesting though, that bricks came into it.



LEO:  Yeah, I mean, the disaster you're describing is what happens on PCs today.  You can download an app anywhere, from anywhere, on a PC.  Microsoft has tried with Windows S, Windows 10 S, to make a version of Windows that is only available to the  App Store.  And the first thing anybody does who gets a PC with Windows 10 S on it is disable that so they can download Chrome and other things.  I mean, when you get Chrome, you're not getting it from the Microsoft App Store, you're getting it from Google.



STEVE:  Yeah.



LEO:  And I think the right to do that is great.  But maybe a mobile platform is a little bit different and should have a restricted App Store.  I honestly think, if that's what you want, you should get an Android phone that's more appropriate.



STEVE:  Right, right, right.



LEO:  But it's an interesting case.  I mean, a class action case is not going to change anything.



STEVE:  No.



LEO:  Even if they win.  Oh, let me show you our wire closet.



STEVE:  Oh, yeah, yeah, yeah.



LEO:  This is the current - so let me just - just so you remember the wire closet...  



STEVE:  Oh, come on, really?  



LEO:  Yeah, from earlier, this is the Picture of the Week.  This is our - it's called a Video Hub, which has all the cameras are going into one side and then all of the outputs are going to another side.  The whole server room looks like this.



STEVE:  That's a thing of beauty.



LEO:  Isn't it nice?



STEVE:  That really is.



LEO:  They're not even showing the best stuff, and there's some beautiful cable bundles and things going through conduit, those cable trays up above, everything is done beautifully. 



STEVE:  Very nice.



LEO:  And it's important to do it that way.  We even custom cut these RCA cables to be custom lengths so you wouldn't have a lot of extra stuff.



STEVE:  Yup.  Very nice.



LEO:  They did a beautiful job.  I just want to make sure they get - Alex Gumpel and Russell and Burke McQuinn get credit because they just did a fabulous job.



STEVE:  Sure, sure.



LEO:  Yeah.  It's clean; right?



STAFF:  They're not RCA.



LEO:  They're not, I'm sorry, did I say RCA?  They are not RCA cables.  They are, what are those things called?  BNC, thank you.



STAFF:  It's video, SDI.



LEO:  SDI BNC cables.  So those are the ones that lock.  We don't use any of that cheap RCA crap.  Isn't that nice?  I'm proud of them.  I wanted to give them kudos because they did a nice job.



STEVE:  Now, if we only knew who was running that closet that's the Picture of the Week, we could send your team over there and...



LEO:  Oh, no.  They can't have them.  They can't have them.



STEVE:  You'd see them about five years later after having nervous breakdowns.



LEO:  This is only because we moved; right?  We were able to - because nobody was going to go in that old rat's nest until we moved.



STEVE:  That's right.  That's what you do.  Once it gets out of control, you just find a new location.  



LEO:  Just move.



STEVE:  Exactly.  And you just do it, okay, now this time we know how to do it.  So, very nice.



LEO:  I mean, those labels, guys, didn't we give you a Brother label maker?  Couldn't you do a better, I mean, come on.  They're a little handwritten.  But other than that.



STEVE:  So Bleeping Computer had an interesting posting about the endangered status of web browser support for the age-old FTP protocol.  And I'm sure, again, the old-timers among us have put in ftp:// and the name of an FTP server, and today's browsers will understand that, will act as an FTP client, will bring up a connection to a remote FTP server - for those who don't know, that's File Transfer Protocol - and display the files that are in a directory.  And you can click on a file and download it.  So, I mean, it's not that different than a browsable directory on a web server, on an HTTP server.



The trouble is that FTP is barely and rarely used anymore by the mass of Chrome and Firefox users.  I saw some stats that were something like 0.1% over some period of time.  So it's not absolutely zero, but it's like - the problem is that there have been various attempts to secure FTP in the way that HTTP was secured.  But they never took hold.  And there are some technical challenges.  I recall talking about the NAT traversal challenges of supporting FTP many years ago on this podcast.



The trouble is, unlike HTTP, where a single connection is made to a server, FTP by default, there is a variant that is known as "passive FTP," but its normal operation uses a pair of TCP connections, one for the so-called "control channel" and a second connection for the data channel.  That second channel is allocated dynamically where the FTP server that answers the incoming data channel connection, in its response after they negotiate, tells the client which service port it should use to connect to for its data link.  Who knows why that was the way it was designed in the old days, but that's the way it was designed.



So this of course creates a problem for firewalls and NAT routers, which adopted the practice, which is what we talked about years ago, and which remains in use today, of actively snooping on the FTP protocol control messages in order to determine which secondary connection they need to allow for inbound.  So when they see the server that is behind them telling an external FTP client, oh, make your inbound data connection on port whatever, the NAT router sees that and goes, ooh, and opens up.  It performs its own NAT mapping, opening up on the fly a port to receive the now-expected incoming FTP client data connection.



So, okay, now, that gets complicated when you add security.  In fact, it becomes impossible if the FTP control connection is encrypted using something like TLS, since what is then a deliberate man-in-the-middle firewall or NAT router cannot determine the TCP port number for the secondary data connection which has been negotiated between the client and the FTP server because it's encrypted.



So there have been, as I mentioned, some attempts to secure FTP.  But mostly, first of all, they have succeeded, like doing FTPS.  Simply layering TLS on top of FTP in a way analogous to the way we added TLS to HTTP wouldn't work for that reason, but the fact that we're currently depending upon being able to snoop on that traffic in order to handle the FTP protocol in its standard mode.  And of course there's a whole issue of the web browser's attack surface being kept open and expanded for this use for a protocol that virtually no one uses through their browser any longer.  The point is that the browsers are currently supporting this and thus have an available surface for attack that they have to keep maintaining, even though it's not in use.



So it's not that FTP itself isn't a useful application protocol for cross-Internet file transfer, but first of all, standalone full-featured FTP clients are available for every platform.  I don't know if our listeners know, but Windows has long had an FTP client command line.  If you open a command line and enter FTP and hit Enter, you're in an FTP client.  And if you type "help," you'll get a large list of available commands, and you can do FTP stuff.  And then if you type "quit," you'll exit the client and come back to the Windows command prompt.  And of course all operating systems that are in use today on some level support it.



And so what's happened is that the insecure and unsecurable FTP has become sort of the lowest common denominator for Internet file transfer.  First of all, oftentimes just HTTP with S, HTTPS will allow you to download files.  GRC has some directories where I explicitly make them available for developers to grab stuff that I'm working on.  When I was working on SpinRite 6, I had a SpinRite 6 directory, and people could browse around in that.  And in fact I had to deliberately put a text file in the directory starting with an underscore so it would be at the top of the list that says "This directory is deliberately publicly accessible."  Because I would get helpful notes from people saying, "Steve, you realize you've got an exposed directory on your...."  It's like, yes, yes, yes.



Anyway, so the point is you can do this in a secure fashion already with HTTPS if all you want to do is make files available for download, and even with directory browsing and the other things that FTP allows.  And as you mentioned, when we were talking about this at the top of the show, there have been successful variations.  There is SCP, which is the Secure Copy.



LEO:  Actually, that's the one I use.  I use that one all the time, yeah.



STEVE:  Yes, yes.



LEO:  That's SSH; right?



STEVE:  Exactly.  And then there's another one.  There is an SFTP, which is another use of SSH for file transfer.  There's even FISH, F-I-S-H, which is File transfer over Shell protocol.  So there have been solutions to this.  It doesn't affect anyone today.  But just sort of as an early note to our listeners, if something you're doing, and I can't imagine why, but it seems like, okay, if something you're doing requires FTP through a browser, just an early heads-up that both Google and Mozilla are kind of looking at this thinking, you know, we're going to have to retire this somehow.  So it may well be that at some point in the future we're discussing the announcement that version X of Chrome and Y of Firefox are planning to remove their native support for this venerable and increasingly obsolete protocol.  So it may happen.



From our "What could possibly go wrong?" department, you're not going to believe this, Leo.  I just like, okay.  There is a discussion underway at Google and on public forums of the so-called "Writable Files API," intended to simplify local file access.  Okay.  So from our "What could possibly go wrong?" department...



LEO:  What could possibly go wrong?



STEVE:  Okay.  So I think this is actually going to happen at some point.  So yikes.  "The Writable Files API:  Simplifying Local File Access" is the heading of this sort of proposal.  And I'm just reading from Google's - I've got the link here under the developers.google.com.  "What is the Writable Files API?"  They say:  "Today, if a user wants to edit a local file in a web app, the web app needs to ask the user to open the file.  Then, after editing the file, the only way to save changes is by downloading the file to the Downloads folder, or having to replace the original file by navigating the directory structure to find the original folder and file."  They say:  "This user experience leaves a lot to be desired and makes it hard to build web apps that access user files.  The writable" - oh.  Oh, good.  Okay.



"The Writable Files API is designed to increase interoperability of web applications with native applications, making it possible for users to choose files or directories that a web app can interact with on the native file system, and without having to use a native wrapper like Electron to ship your web app."  In other words, Electron creates an environment that a web app can run in natively so it's not in the browser.  Google of course is wanting to release the restraints on web browsers.



They say:  "With the Writable Files API, you could create a simple single file editor that opens a file, allows a user to edit it, and save the changes back to the same file; or a multi-file editor like an IDE or CAD-style application where the user opens a project containing multiple files, usually together in the same directory.  And there are plenty more."



Okay, now, as I was thinking about this, I thought, rather than filing this in the "What could possibly go wrong?" department, maybe this should really have been in the "We should have seen this coming" department.  You know, our web browsers first began pulling crap from all over the Internet to fully monetize the pages that we visited, sometimes by mistake.  Then they got us hooked on JavaScript so that nothing works anymore without it.  But compared to native OS apps, JavaScript performance was bad.



So first we got NaCl - that was the native client - which was very clever, but never achieved critical mass.  Then consensus was finally reached around Web Assembly, so that now code running from random sites on the Internet that we visit can much more efficiently run Monero mining on our machines while we're trying to figure out what that popup says that's asking for permission to store cookies on our browser after we leave the page that we visited by mistake.



So what we see is a history of desperation on the side of the web browser apps to become a first-class native-born citizen of our OSes.  And we know that the whole sandboxing thing has to really be chafing the web browser guys.  They want to be real apps on our machines.  They don't want to be stuck behind any safe enclosures.  They've got speed now.  So next up is access.



Anyway, so there is a discussion.  And the good news is they recognize they're paying lip service to the security issues.  Under "Security Considerations" they said:  "The primary entry point for this API is a file picker, which ensures that the user is always in full control over what files and directories a website has access to."  Okay.  Good.  "Every access to a user-selected file, either reading or writing, is done through an asynchronous API, allowing the browser to potentially include additional prompting and/or permission checks.



"The Writable Files API provides web browsers with significant access to user data and has potential to be abused.  There are both privacy risks, for example, websites getting access to private data they weren't supposed to have access to; as well as security risks, for example, websites able to modify executables, encrypt user data, and so forth.  The Writable Files API must be designed in a way as to limit how much damage a website can do, and make sure that the user understands what they are giving the site access to."  Oh, lord.  So there is, on GitHub, there is a discussion of this and an explainer.  And although the following is a little bit redundant, we have time, so I want to explain what they're thinking.



So under "Proposed Security Models" they say:  "By far the hardest part of this API" - okay, now I would just say no would probably be a good idea.  But they said:  "By far the hardest part for this API is of course going to be the security model to use.  The API provides a lot of scary power to websites that could be abused in many terrible ways."  Okay.  People, I'm saying to Google, are you hearing yourselves?  It's like, yes.  And it's going to happen.  But okay.  But, oh, we can write a multi - we could do a CAD app.  We could do a multi-file editor.  We could open a project that then has access to all the files in the project.  What could possibly go wrong?



They said:  "There are both major privacy risks (websites getting access to private data they weren't supposed to have access to), as well as security risks (websites modifying executables, installing viruses, encrypting the users' data and demanding ransoms, et cetera).  So great care will have to be taken to limit how much damage" - how much damage.  Yeah, let's limit how much damage a website can do.  By all means.  I'm for that.



And they say:  "And make sure a user understands what they are giving a website access to."  Oh, yeah, right.  Our moms are going to understand this?  Can you just see the website saying, in order to do what you want, we need to have - oh.  Anyway.  "Persistent access to a file could also be" - get this.  "Persistent access to a file."  So they're suggesting that websites have persistent access to files on our systems.  "Persistent access to a file could also be used as some form of super-cookie; but of course all access to files should be revoked when cookies/storage are cleared, so this shouldn't be too bad."  Oh, okay.



"The primary entry point for this API is a file picker, i.e., a chooser.  As such, the user always is in full control over what files and directories a website has access to.  Furthermore, every access to the file, either reading or writing, after a website has somehow gotten a handle, is done through an asynchronous API, so browser could include more prompting and/or permission checking at those points.  This last bit is particularly important when it comes to persisting handles in indexed databases.  When a handle is retrieved later, a user agent might want to re-prompt to allow access to the file or directory."  So this suggests that they're proposing that websites would, by default, have enduring access to specific files or projects or whatever on the user's machine.



They say:  "Other parts that can contribute to making this API as safe as possible for users include limiting access to certain directories."  Oh.  Well, that would be good.  That would be handy.  "For example," they say, "it is probably a good idea for a user agent" - that is, the browser - "not to allow the user to select things like the root of a file system" - you think? - "certain system directories, the user's entire home directory, or even their entire downloads directory.  Also, limiting write access to certain file types.  Not allowing websites to write to certain file types such as executables will limit the possible attack surface."  And then "other things user agents come up with."



Okay.  So, you know, we've often observed here that just because it's possible to do something, doesn't automatically mean that it should be done.  So it occurred to me as I was putting this together that a better name for this proposal might be the Security Now! Four-Digit Podcast Numbering Assurance Act of 2018, since this guarantees that we are never - we're going to get into four-digit Security Now! podcast numbers, if we give websites the opportunity to say to their users, "Okay, now, here's what we need you to do."  Oh, lord.



So for our more technical listeners, there is a discussion of this with a lot of opinions echoing mine, like just back away from your computer, you developers.  Find something else to do to add to our browsers.  So I've got a link to the discussion at discourse.wicg.io, where anybody who's interested can throw their two or three cents in, if you want to spend some time and just look at what's going on over there.  I just - oh, yeah.  I mean, these show notes are produced in Google Docs.  I download the PDF for them, and that's where I get the PDF that is then posted online, which Leo is looking at right now, and our listeners are looking at, those who bring it up while the podcast is underway.  And I know that lots of people grab it.



So it's absolutely the case that web apps are useful.  And we're seeing, now that we've got scripting, now that we've got WebAssembly that allows very close to native performance of an app running on code that we got from a website, I mean, I can see the next step is to allow this thing to step out of its boundary into our file system.  That's, like, that's the last straw.  But I'm afraid it might be true.  I mean, I'd be tempted just to turn that off because, well, we'll see how it develops.



But the good news is they clearly recognize the danger.  But the one thing we see that is the thing we have never been able to solve is social engineering attacks.  Phishing attacks continue to be successful because they leverage people's inherent trust in - they just assume that there's authority coming from the other side.  These computers, they don't understand the workings.  They don't understand.  They want something to work, or they want to do something, and the website says, oh, your version of Flash is out of date.  Click here to, I mean, that's how all of these problems happen.  And so you could just imagine the website that says, oh, in order to do this, click here and then go do the following.  And in order to be useful, the app has to have access to some directories on your file system, and that's what it's going to try to get.



So I don't know.  I get it.  But it really does sound like it's not a matter of the technology being a problem.  It's the users who will unwittingly follow instructions.  Historically they've been following instructions to download updated Flash players to do what they want, or click here to get what they want.  One of the core pieces of advice from the podcast that I really, I just love this one, is never download anything you are told to.  Never download anything you didn't go looking for, meaning if something says you need something, just say no.  Just no.  It's not worth the risk.  And this to me, I think it's the social engineering abuse potential which this raises.  I mean, yeah, someone could say "Download this and run it," and we can already do that today.  Maybe this isn't any more dangerous.  But bad guys keep coming up with clever ways to trick users.



Last week's podcast asked the question "Are Passwords Immortal?"  And we shared Troy Hunt's blog post.  It generated a huge, really interesting set of responses from our listeners, both through Twitter and the mailbag, and I wanted to share four of them.



Dan Stevens wrote:  "Hi, Steve.  I listened to Security Now! Episode 690, where you discuss Troy Hunt's blog post about passwords, with great interest.  I think good points were made on both sides.  But I'm optimistic SQRL will eventually become widely used.  When credit cards came out in the 1970s," he says, "before my time, I bet everyone was saying 'They'll never replace cash,' and they'd be right.  Cash is still widely used even in developed nations.  Yet over the last few years, perhaps since contactless payments became available, I've needed cash less and less, so much so that I now leave my wallet at home and just carry my payment card.



"I think this shows if people are able to perceive the benefits of a new system over the status quo, a large portion will eventually switch to it.  It's just a matter of time.  Yes, passwords will never die, but they don't need to.  If SQRL becomes widely available enough for me to 'leave my password vault at home,' so to speak, that's good enough for me.  I'm really looking forward to the official release of SQRL and excited to see what becomes of it."



And I really thought that was a great analogy.  I'm a huge, for what it's worth, credit card user.  And I do remember, he was right, when credit cards first happened, there was some skepticism about it.  I mean, it was like, what?  And it was weird, and it was like, not everyone took them.  And you had to ask restaurants, do you take this, or do you take that?  And it was like, you know, there was - definitely it was anything but friction-free.  Yet they did happen.  And they are now, I mean, that's the way we buy stuff on the web.  And that's a huge, huge portion of commerce.  Yet cash is still in place.  And so really I thought that was a great analogy for the notion of the adoption of something new over time.



And on the issue of friction, one thing I forgot to mention when I was talking about this last week, but I just did want to note, is that, unlike any of the other multifactor systems, SQRL's setup is needed once.  No question about it.  You've got to get the app.  You create an identity.  It asks you to do some things to protect yourself, to write down what we call a "rescue code," which is your "Oh, crap, I forgot my password," your "get out of free" pass.  You can back up or print out your master identity and fold it and put it away in a drawer so that you have it.  Normally you'll spread that one identity across your devices.  We have a means of importing and exporting identity among computers and smartphones and things.  So you sort of get an automatic backup of your identity in a multi-device mode that many of us have today.  But the point is there is some one-time stuff.



But I will argue, and everyone will soon see, that is literally the only friction you ever get.  And so the point I really didn't make last time is that, unlike any other multifactor system, all subsequent use is literally zero friction.  I mean, even multifactor, where you're having to snap a QR code or write down the key for your time-varying multifactor authentication.  Each time you do that, every time you set that up with a new site, that's certainly not zero friction.  So much so that I've discussed how to solve the problem of setting up a new device that now needs to know what all of your codes are.  And my recommendation was, while you have that device or that site's QR code up, print it so you end up with a sheaf of papers that you can easily allow another device you're setting up to see.



But the point is, after this is just - after identity is created, the real joy of SQRL, what it really does, is from then on nothing can touch it.  Nothing can compete with it in terms of ease.  And I think that will probably be the main reason it succeeds is in the example that I gave you go to a blog site, and they say, oh, we need to know who you are.  It's just like instantaneous to do that.  And once people see it, it's like, okay.  We need everybody to offer this.



Don Williams said:  "Does SQRL have an option for the following scenario?"  He says:  "My employer contracts with a third party for services to be used by all employees.  The third party permits access to only those users who can authenticate to the employer's security system.  Therefore the third party is depending on the employer's authentication system."



And the answer is yes.  The example I have given is, for example, imagine that in the future we had a U.S. identity system, that is, we wanted to try online voting; or, well, we do have things like the IRS and Medicare.  We have government-provided services.  The government could create a site, say identity.gov.  And when you go to Medicare's site, or you go to the IRS, or you go to Veterans or whatever, those government sites could all use the SQRL domain of identity.gov.  So you automatically get, within your control, centralized identity and authentication.



So it is entirely possible for one site to use the authentication of another site.  You see that in the SQRL dialogue.  It comes up and shows you the domain to which you are authenticating.  So it's very clear what's going on.  But this does allow for one site to use the identity of another.  And in fact that's sort of the example that I gave last week in answering the question about what happens if Ford purchases some other car company and wants to move its users over.  We're able to use that flexibility in order to make that happen.  And the user who's doing it would see that they are authenticating to the other site in order to transport their identity across.



And, finally, James said:  "Hi, Steve.  I was delighted to hear my feedback being discussed regarding delegation of access on SQRL, and even more delighted to hear that it has been addressed already.  Listening to SN-690, I was enjoying your discussion of Troy Hunt's blog post.  You mentioned the importance of a frictionless process being key for the adoption of any potential successor to the current security model of username and password.  In particular you articulated the often encountered situation where people will refrain from leaving a comment on a blog post or site simply because they do not have an account, and the process of creating one falls into the 'too hard' drawer."  Or I would say "not worth it" drawer.  "Consequently, people just move along, and their insightful comment or reply is lost to the world.  Which got me thinking (again)."



He says:  "The beauty of SQRL, as you have explained before, is that it allows a person to anonymously identify and assert said anonymous identity with a site.  It allows them to do so quickly and easily.  But," he says, "many, many, MANY sites base their entire business model on knowing something tangible about their visitors - their name, their email address, a human-friendly username, et cetera.  In the case of ecommerce then, delivery addresses, billing addresses, and credit card information would also be required in due course.  In short, most sites probably will not accept the creation of an account with just one anonymous handle.  They will want more.



"My question," he says, "finally is, is there a provision in the SQRL spec to allow this to be handled in a 'frictionless' manner?"  And then he proposes one.  He says:  "I envision or propose something along the following lines:  On connecting with a site for the first time, the recognition is made that the user is unknown."  Okay.  SQRL has that.



He says:  "The site passes its challenge to you, asking you to provide and assert your identity.  Could at this moment the protocol also allow the site for the first time to request other information about you that the site would want to know?  Your preferred handle, your email, your real name, your billing address, and even (cringe)," he says, "your CC details.  The SQRL application could then display, one time only, a list of the details that are requested by the site and which of those are considered by the site to be 'mandatory conditions of account creation,' much the same way that Android requests app permissions at install time.



"The user could then choose what they wish to disclose to the site at that point, and those they wish to withhold.  Or, if they feel that the site is being too nosey, they can withdraw, and the identity creation process is aborted.  This would also likely assist in compliance with GDPR, as the user is being made explicitly aware of what information they are being asked to disclose and providing tacit consent in the process."  And he goes on, but everyone gets the idea.



So I put that in because he raises a really good point, and it has been a source of a lot of discussion over the years in our SQRL group.  And that is, what is it?  The earlier specification made what I now consider to be a mistake of doing too much.  There wasn't all of that in there, but there was more.  And in one of the major revamps I made, it was in pulling way back from that.  We decided that what it should be is, with very little exception, only authentication.  That is, that's what it should - it should only be we should keep it clean and simple, and it just is authentication.  Many people have observed, as James has, that sites want more.  And that's absolutely the case.



It's also the case that, if it turns out people want more, it could certainly be added.  But it's not how we're going to start at the beginning.  I don't want this to be - I don't think it should be more than just a pseudonymous assertion of an anonymous identity, which solves and addresses all the cryptographic aspects.  Because it's certainly possible then to do a form fill-out thing of some sort to add that or to have that be separate.  I just didn't want to confuse the specification by bundling that in.  And then it becomes - it's much trickier from a privacy standpoint, from a GDPR standpoint, from a multilingual standpoint.



And so the answer is no, we have decided to keep it with very little exception.  There is one or two exceptions we'll eventually talk about.  But I think it's better if it's just that.  Because then the site can say, oh, hi.  You've established your SQRL identity.  If you would like to receive notification when someone responds to your blog posting, then we'll need an email address from you.  Feel free to provide it and so forth.  Different sites will have different requirements.  I think it makes sense to let the site handle it at the site end, after SQRL is used to establish you as an identity, rather than mixing it all together.



And, finally, closing the loop.  Joe Petrakovich said:  "Shout out to @SGgrc Steve Gibson who I think gave me the idea to use Amazon S3 instead of Dropbox."  He said:  "Folder auto-sync job set up.  Monthly bill reduced from $10 to $0.10."  So I've mentioned before that I'm using S3 to store all kinds of stuff.  And I have gigabytes of stuff.  But because it's just archiving, my bill is like $2.43 a month, and it's just a great repository.  So I just wanted to acknowledge, to thank Joe for the note and to note that that's an effective solution.



So I like the name "ECCploit."



LEO:  ECCploit.



STEVE:  Rowhammer attacks against ECC protected memory.  Okay.  So to sort of remind ourselves and set the context for this, as we'll recall, it was Professor Herbert Bos, B-O-S, and a team of researchers at VU Amsterdam who were the first to demonstrate how potent and potentially practical Rowhammer attacks could be.  And I'll explain again what Rowhammer attacks are.  But at the time, everyone took some relief from the observation that parity-protected RAM would tend to catch and thwart any single bitflips, though not dual bitflips; but that at least the presence of full ECC error-correcting RAM, which is ECC, would provide much stronger protection.



Well, now Professor Bos and a different team of researchers are back with the results of their examination of just how safe we should feel about ECC's protection from Rowhammer attacks.  Their paper is titled "Exploiting Correcting Codes:  On the Effectiveness of ECC Memory Against Rowhammer Attacks."  And to give everyone a hint of what they found, the headlines in the technical press that covered this included "Rowhammer Data Hacks Are More Dangerous Than Anyone Feared," "Not Even ECC Memory Is Safe from Rowhammer Attacks," and "Potentially Disastrous Rowhammer Bitflips Can Bypass ECC."



Okay.  So first of all, let's remind ourselves about Rowhammer.  That was four years ago, in 2014, that this jumped onto our radar.  And over the course of four years there has been a bunch of research produced that demonstrated that this problem is real.  As we have said - in fact, we were just talking about it last week, the difference between SSD and DRAM.  SSD is relatively permanent storage because electrons are stranded on a little chunk of conducting material, but stranded there by insulation, so that they have nowhere to go; and their presence, their electrostatic presence can be sensed, thus creating a permanent one or zero data bit.



DRAM, the reason DRAM is as dense as it is, where you can get just a phenomenal amount of data, billions of bytes of data on a strip of circuit board, is that the mechanism of storage cannot be further simplified.  It is an itty-bitty little tiny capacitor which is just essentially two electrical plates separated by an insulator, technically called a "dielectric."  And that capacitor is charged, meaning that electrons are pulled off of one side and put on the other to create this charge, which the plates are so small, and they are so close together, that there is leakage.  And so this little capacitor, this imbalance in charge, tends to rebalance itself.  This capacitor discharges rather quickly.  Consequently, it's necessary to refresh the memory in the DRAM continuously.



The idea is that in order to store at such high density, we have sacrificed the ability to store data permanently in favor of storing much greater data.  And that requires the cells to be small, that they cannot retain the data for long.  So what happens is, before this grid has a chance to discharge, row by row the data is read out, and the capacitors are essentially recharged.  The ones that have no charge in them have a zero bit, and so they're left discharged.  The ones that have a one bit in them are read before they have a chance to discharge so that we can't tell that they have a one bit stored in them, and they're essentially recharged.  So that's really what refreshing is.  It's recharging the capacitors, essentially by dumping them all into an end-of-row buffer, and then reading what they had, and then recharging them all, and then going to the next row.  Okay.  So this is a large matrix of itty-bitty tiny cells that cannot be made any smaller, or they would have been.  And they need to be constantly refreshed.



In the same way that hard drives are sort of on the edge of viability and rely on error correction to kind of bring them back out of the gray zone, thus the reason SpinRite is still useful today because it's not physical defects on the platters that SpinRite is now compensating for.  Twenty-five years ago it was.  I was dealing with physical storage problems on the platters.  That's no longer the case.  Now we're dealing with the fact that densities have gone up so high, and the drives have been engineered so tightly, that they are counting on error correction in order to keep the data readable.  So, I mean, the analogy is exact here.



So what's happened is the noise margins are so tight in DRAM that what was discovered four years ago is, if somebody hammered on a row of memory, and by that I mean continuously read from the memory, that would create enough noise, electrical noise in the neighborhood that it could cause adjacent cells of DRAM to be misread.  That's Rowhammer, hammering on a row, and the row next to yours could be misread.  And the clever, again, remember attacks only ever get better, or worse, or stronger, or more powerful, whatever adjective you want to use, or adverb I guess.  Wait.  An attack?  Yeah.



So anyway, what the designers realized was you could hammer on both sides of a target victim row and make an even stronger opportunity of inducing what are known as bitflips, actually flip the bit, cause a bit stored in memory to change.  And then once they were able to do that, they figured out how to spray page tables through memory and cause the operating system to switch its page tables from the real ones to the ones they had created, and thus get write and read access to regions of memory that they weren't supposed to.  So the point is, it turns out by starting at just, if we arrange to read from DRAM hard enough, over and over, they took that and went all the way to engineering truly effective attacks.



So memory can have a parity bit added to it.  You can take, for example, a row of 64 bits of memory, which is generally the size that DRAM is read out in; and you can have, for example, a parity bit added to that 64 bits to make 65.  And it's a very simple matter to have the hardware look at the bits that are being written, because the DRAM is also - it's read in a whole row and written in a whole row at a time.  So it's easy to have hardware add a 65th bit, which will be set to one or zero depending upon the number of one bits, the even and oddness of the number of one bits set in the rest.



The point is that - and this is known as "parity."  It's a well-known process.  Back in the days of paper tape we often had seven channels of holes punched in tape, and the eighth channel was the parity bit.  And the idea being that, if you punched the tape with, for example, even parity, then every row of eight had seven data bits to contain the ASCII characters, and the eighth bit guaranteed that the total number of punches would be an even number of holes.  That way, when the paper tape was being read back, the paper tape reader would always verify even parity, and it would stop if it read something that was wrong.  And in fact when we got to optical tape readers, you would see them stutter because they were able to read and reverse the tape direction.  And so they'd back up and come forward again and might stutter a little bit in order to correct the misread of a row of tape.  And so you dramatically improved your reliability.



So the idea with parity memory, which I want to talk about first, is that it would detect a problem.  If you always know that any row of memory is supposed to have even parity, that is, the number you add up, you sum the number of one bits and include the extra one you added, it should always be even.  And of course everyone should understand that you need the extra bit so that you don't - because you can't force the 64 bits to be even.  If the 64 bits happen to have an odd number of ones, then you set the parity to one to increase the number, the total number of ones by one, thus making it even.  So that's the simple logic.



And the point is, the parity memory allows for the detection of a single flipped bit in the row of 64 bits.  Note that it does not detect two bits that are flipped because, if you change two bits, then you have not upset the evenness and oddness of the parity of the 64.  But hopefully no bits are ever being flipped in the normal case.  I mean, we need our system's memory to remember what we wrote there.  And for the most part it does.  The idea being, though, that if a cosmic ray, which is literally one of the causes of DRAM problems, you just get a random cosmic ray come shooting through your DRAM, and it'll blast the bits.  You'd like in a system which is really caring about the work it's doing, it's better to have it stop and just say, whoops, we had a parity error, than to go blithely along and do stuff wrong.



So the next level up from parity is error correction.  And error correction, the short version of this is it uses many more added-on bits, typically eight added to 64, so you get 72 bits in a row.  And using some very complex math, which is also done in hardware, and interestingly is proprietary, it's possible to figure out which bit flipped and fix it.  And it's tricky again because you've got 64 bits, and you have a relatively small number of ECC bits compared to the whole, and the math that's used is able to determine which one of any of the 64 bits was wrong, if one of those bits flipped.  And whereas parity memory cannot detect a pair of bitflips, ECC memory will detect, but it's unable to correct, a pair of bitflips.  So it just sort of ups the game on the whole DRAM error issue.



And so it was originally felt that parity memory would be protection, well, protection from Rowhammer to the extent that it would prevent abuse of single bitflips by stopping the system; by saying, whoops, we had an error here.  Don't go any further.  So bad guys couldn't get anything done.  Then the Rowhammer attackers figured out how to do two bitflips and to make it practical.  There was something I was reading when I was digging through this 17-page research paper that these guys wrote, where they talked about a number of instructions which were found in the pseudo command which is used in Linuxes and Unixes to elevate the privilege of a non-privileged user.  There were, I think it was 13 locations where they had identified instructions where a pair of bits flipped in the instruction would, for example, convert the instruction from a conditional jump which was testing whether you had authenticated properly into a move, thus neutering the jump and essentially authenticating any user who then used that instruction to elevate their privilege and thus bypass authentication.



So, I mean, this stuff has been turned into practical attacks, amazing as it is.  So the addition of error correction code was believed to really up the ante.  It would detect single bitflips and correct them, and it would detect and not be fooled by dual bitflips.



So I'll share the abstract from their research to give you the context for this.  They said, "Given the increasing impact of Rowhammer and the dearth of other adequate hardware defenses, many in the security community have pinned their hopes on error correcting code (ECC) memory as one of the few practical defenses against Rowhammer attacks."  Meaning we haven't come up with a solution for this.



"Specifically, the expectation is that the ECC algorithm will correct or detect any bits they manage to flip" - meaning attackers - "manage to flip in memory in real-world settings.  However, the extent to which ECC really protects against Rowhammer is an open research question due to two key challenges.  First, the details of the ECC implementations in commodity systems are not known."  Which came as a surprise to me.  I just assumed we all knew about that.  Turns out it's proprietary.  "Second, existing Rowhammer exploitation techniques cannot yield reliable attacks in the presence of ECC memory.



"In this paper we address both challenges and provide concrete evidence of the susceptibility of ECC memory to Rowhammer attacks.  To address the first challenge, we describe a novel approach that combines a custom-made hardware probe, Rowhammer bitflips, and a cold-boot attack to reverse engineer ECC functions on commodity AMD and Intel processors.  To address the second challenge, we present ECCploit, a new Rowhammer attack based on composable, data-controlled bitflips and a novel side channel in the ECC memory controller.  We show that, while ECC memory does reduce the attack surface for Rowhammer, ECCploit still allows an attacker to mount reliable Rowhammer attacks against vulnerable ECC memory on a variety of systems and configurations.  In addition we show that, despite the non-trivial constraints imposed by ECC, ECCploit can still be powerful in practice and mimic the behavior of prior Rowhammer exploits."



I won't go digging deeper into this except to note that these guys pulled this off.  It turns out that ECC details are contained on chip and are undocumented by Intel or AMD.  So it was first necessary for them, in order to figure out how to bypass error correction, to determine to reverse engineer the error correction algorithms, that is, to figure out exactly what math was used on the 64 data bits to create the ECC "tag," as it's called, to compute ECC in order to know how to flip bits deliberately that would bypass error correction.  They did that through a brutal process of reverse engineering.  They reverse engineered one AMD and three Intel chipsets to figure out what was going on.  They recognized that normally the chipset that software is running on is made aware or is not obscured from software that is running on the hardware platform.



So the point being it's possible for software to know what hardware architecture it's on.  That would then tell it what the respective ECC algorithm is, which would then allow it to choose the proper Rowhammer attack in order to flip bits on that particular platform.  So while it is definitely the case that error correction code does make Rowhammer attacks more difficult, it does not solve the problem of Rowhammer.  And one thing I learned from reading this paper, which just made me sort of close my eyes and shake my head, was it turns out that the refresh rate of DRAM, we have always known that it has a performance impact, that is, since refreshing is taking some of the RAM system's bandwidth for no good purpose, rather than just running through row by row by row, reading and rewriting that memory, that's going to consume some of the bandwidth.



Thus we had known that, whereas one of the mitigations for Rowhammer is increasing the refresh rate in order to have the cells' data be stored further from the noise margin, that is, having the one bits more firmly one, thus making it more difficult to have noise induce a flip, we've known that.



It turns out that there are instances where systems are very power critical.  They absolutely have to run at low power.  So it turns out that refresh is deliberately reduced because not only does it reduce performance to refresh at a high rate, it increases power consumption.  So there are places where refresh rate has been deliberately compromised, and ECC is used as a prophylactic to - and I was of course again reminded of hard drives that have done the same thing.  Hard drives are now relying on error correction to make up for the fact that they're no longer able to guarantee that they're able to read back the data without correction.



Similarly, it turns out, error-correcting DRAM is now being used as a performance prophylactic to solve the fact that, where power has been reduced, DRAM refresh rate has been reduced to compromise on power, and error correction is then assumed to be part of the system to bring back the reliability that was lost from having DRAM refresh rates reduced.  Which, again, as an engineer, I just shake my head, and I go, well, okay, good luck to you.



So anyway, that's the news.  The headlines were correct, although we're really out in the academic weeds at this point.  Again, we're sort of where we were with Spectre and Meltdown in that the real danger is in shared hosting environments where bad software has an opportunity to run alongside good software and is able to, by being mischievous, get access that it shouldn't have across protection boundaries.  It's true that we don't want bad software which we inadvertently run in our own workstations to have greater rein in our system than the OS wants to give it, and Rowhammer attacks allow that.  That's why our browsers were immediately hardened against Rowhammer attacks because the browser is the way that you have untrusted stuff running in your system which again is why I'm not so sure I'm bullish on the idea of giving our browsers, like releasing our browsers from their sandbox and allowing them to have access to the native file system that they're running on.  But we'll see how that goes.



Similarly, if you've got something that is already hammering away at your system's RAM, you've already got malware running in your system, and that's not a place you want to be.  So anyway, I wanted to address this because the popular press, the technical press, has talked about how Rowhammer is now functional in ECC memory.  And it's like, yes.  If the chip is one whose ECC has been reverse engineered, if it knows where it's running, I would argue anyone who, like state-level actors, where a huge amount of resources could be mounted to attack a specific system, a specific thing, where it could be known what the hardware platform is, people could be put on the task of reverse engineering the ECC algorithms for that particular hardware platform.



Then the Rowhammer attack could be designed to flip the bits specifically necessary to achieve a specific goal on that specific platform.  Look at all the times I had to use the word "specific."  I mean, it's a really specific, really focused attack.  Then we have a problem.  But for us, for most of us, it's of academic interest to see how something from four years ago keeps coming back, attacks keep getting stronger against all the mitigation that is brought to bear.  From a theoretical security standpoint, really interesting.  But I don't think it really affects us directly very much.



LEO:  Well, I hope those guys got their Ph.D.  That's all I can say.



STEVE:  Well, four years ago, I think that whole team probably graduated.  They've got their doctorates, and they're gone.  And so Herb has a new team of Ph.D. students.  Okay.



LEO:  No, let's do it again.



STEVE:  You want to be doctors?  Let's do it again.



LEO:  No Nobel Prize for that.  But, you know, a Ph.D. is a good start.



STEVE:  That's right.  



LEO:  Steve, we've done it again.  We've completely dissected and hammered upon the rows of information, of security information from this week.  And there's nothing left but pulp.



STEVE:  Among us and our listeners.



LEO:  Yeah.  Well done.  Bravo.  I hope you enjoyed watching the show.  Now, some of you watch live.  And if you'd like to do that, you're more than welcome.  That's the best way, if you want to interact with the show, to get in the chatroom at irc.twit.tv, do it while the show's live.  That's Tuesday afternoons, 1:30 Pacific, 4:30 Eastern, 21:30 UTC.  You can watch at TWiT.tv/live.



You can also get the downloads.  We offer a variety of ways to download it.  Steve's got the 64Kb audio at his website, GRC.com, along with transcripts, which are handy if you like to read along as you listen.  I know a lot of people do.  And, you know, I bet you there are not a few people who have a complete collection of all the audio and a complete collection of all the transcripts so that they can search it, refer back and so forth.  That's a very good idea.  Put it in binders.  Put it on your shelf.  You'll impress your friends.  GRC.com.



While you're there, pick up a copy of SpinRite, world's best hard drive recovery and maintenance utility.  Find out more about SQRL.  Everything else is free.  Lots of freebies on there including ShieldsUP to test your router, Perfect Paper Passwords, Password Haystacks, everything.  It's fun.  And even health advice, and Vitamin D information, and it goes on and on.  It's a treasure trove:  GRC.com.  If you want to get a hold of Steve, you can leave a feedback message there at GRC.com/feedback.  But you also can DM him at Twitter.  He's @SGgrc on the Twitter.



We have audio and video at our website, TWiT.tv/sn.  You can download whatever you like there.  For most people, the most convenient thing to do is get a podcast app on your phone or your tablet and subscribe.  That way you'll get every episode automatically, the minute it's available.  It's the fastest way to get the downloads without having to think about it.  We should actually make a feed of the show notes, too.  I bet you people would appreciate that at some point.  But for now the show notes are at GRC.com.



I think that's everything.  Thank you, Steve.  Have a great weekend.



STEVE:  And we'll be back in December.



LEO:  Holy cow.  How did that happen?  I will not, actually.  I'm going to be in New York City next week for our annual trip to see shows on Broadway.



STEVE:  Must be Jason time.  I do remember that you were going to be...



LEO:  It will be Jason time.  It'll be Jason time.



STEVE:  Cool.



LEO:  And then I'll be back for the week after that.  And we have planned a great Best Of.  I don't think we've ever done a Best Of with Security Now!.



STEVE:  We haven't, but we're going to this year.



LEO:  We're going to let you take a couple of weeks off because Security Now! falls on Tuesday, which is Christmas and New Year's.



STEVE:  Yup. 



LEO:  So we're just going to play that Best Of twice.  And we'll be back doing live shows on the second Tuesday of 2019.  That's like Tuesday, December 8th, I mean January 8th.



STEVE:  Cool.



LEO:  Thank you, Steve.



STEVE:  Thank you, my friend.  See you next week.



LEO:  Have a great night.  See you next time.



STEVE:  Bye.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#692

DATE:		December 4, 2018

TITLE:		GPU RAM Image Leakage

HOSTS:	Steve Gibson & Jason Howell

SOURCE:	https://media.GRC.com/sn/SN-692.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  This week we discuss another Lenovo Superfish-style local security certificate screw-up; several new, large and high-profile secure breach incidents and what they mean for us; the inevitable evolution of exploitation of publicly exposed UPnP router services; and the emergence of "Printer Spam."  How well does ransomware pay?  We have an idea now.  We talk about two iOS scam apps, a false positive Bing warning, progress on the DNS over HTTPS front, and rumors that Microsoft is abandoning their EdgeHTML engine in favor of Chromium.  We also have a bit of miscellany, news of a cybersecurity-related Humble Book Bundle just in time for Christmas, and a bit of closing-the-loop feedback.  Then we discuss some new research that reveals that it's possible to recover pieces of web browser page images that have been previously viewed. 



SHOW TEASE:  It's time for Security Now! with Steve Gibson.  I'm Jason Howell filling in for Leo, who is out this week.  Steve's going to talk about a whole bunch.  This has been a busy week for security.  Obviously the huge Marriott and Quora breaches.  Also Lenovo Superfish strikes again, but this time with Sennheiser's own version of it.  Gorilla printer marketing, thanks to rabid PewDiePie fans.  That might not make a whole lot of sense now, but I promise it will.  And Steve explores the insecurity of GPU memory.  All that and more coming up next on Security Now!.



JASON HOWELL:  This is Security Now! with Steve Gibson, Episode 692, recorded Tuesday, December 4th, 2018:  GPU RAM Image Leakage.



It's time for Security Now!, the show where we talk about all the latest security news happenings and a whole lot more diving deep.  I'm Jason Howell, filling in for Leo.  Diving deep because of this man right here, Steve Gibson.  How you doing, Steve?



STEVE GIBSON:  Hey, Jason, great to be with you this week while Leo is back on the East Coast as we open December.



JASON:  Absolutely.  I kind of can't believe that it's December.  We were talking before the show that 2018 just as a year for security, about how scary security can be at times.  And it really seems, especially looking at today's rundown, that doesn't seem to be anything that has like an end in sight.  It's just going to get more and more complicated as we go along.  I've just got to get used to it.



STEVE:  I ought to point our listeners, who are obviously interested in security, to last week's Triangulation podcast, which you recorded with our often-mentioned Bruce Schneier and was all about his book.  And it was fun to sort of listen to it because it sounds a lot like this podcast.  And one of the things that he pointed out is that, as we've often said here, that one of the root causes of the problems we're seeing with security is that it's often an afterthought that security is really expensive.  I mean, it's not free.  Because of the fact that security is the weakest link sort of problem, every link has to be strong because any one of them being weak gives you an opportunity to breach the system.



So what that means is that, in order for every possible link in a chain of dependency to be strong, you have to focus on security for all of those.  And it raises the cost.  And he made, I think, what was a very good point, which was that people say they want security.  And it's like, yeah.  Yeah, I want security.  And then you ask them, okay, how much are you willing to pay for it?  It's like, what?  Uh, well, let's just hope that nothing bad happens.  And of course we know how that turns out.  This is Episode 692 of the result of hope.



JASON:  Yes.  This is what hope gets you, 692 episodes of Security Now!.  People are like, okay, cool.  Give me more.  Yeah, it is interesting.  He talks a lot about regulation as well in the interview.  That's kind of the second part of his book is just like, this problem is too big, obviously, for the companies themselves to make these changes and to prioritize it.  And so maybe regulation is needed.



STEVE:  Yes, they would rather it were somebody else's problem.  And, well, and he also made the point, we will be talking about the massive Marriott breach as one of our news items.  He mentioned that also last Friday.  And he made the point that one of the problems is it isn't clear that breaches are truly expensive for companies.  Microsoft Research, the research branch of Microsoft, wrote a paper years ago titled something like "The Rational Lack of Concern Over Security."  It was a title like that.



And it was weird, but they made the argument and supported it that people are rational actors.  And, for example, and it was written back in sort of the pre-strong password/password manager days, where they were saying, look, yes, something bad could happen.  But the cost of having a different long password for every site you visit and the overhead of managing it and all of that mess is such that people acting out of their own rational self-interest will still choose "monkey" as their password.  Or you can't anymore.  You're not allowed to.  The website says no, no.  But that was Leo's password for years.  Worked just fine, and he didn't have to worry about what it was.  It was monkey until the bar really changed.



But the point being that it's not the case that people are irrational.  It's that we haven't made it easy enough and practical enough to be secure that that's the rational choice.  And so this is what we get.  And so, for example, yeah, Marriott suffered a 500 million user data breach through a property acquisition they made two years ago, a breach that we'll be talking about in a second, that predated their acquisition of the Starwood properties by another two years.  And it's like, oops, sorry.  And they'll move forward.  They'll still be Marriott.



JASON:  Yeah.



STEVE:  So anyway, this week we're going to talk about some interesting new research out of Korea, South Korea, about another sort of stone that's been turned over and what was found underneath.  And so the title of today's podcast is GPU RAM Image Leakage.  It turns out that GPU RAM, of which there's ample because images are big, isn't being managed well from a security standpoint.  OS RAM is.  We all understand.  Oh, you know, you've got to be careful about your OS RAM because you could have secrets in it, and so you don't want other processes running in the same machine to have access to another process's RAM.  That's what memory protection, memory management is all about.  That doesn't exist in the GPU.  Nobody ever really thought about that.  These guys said, hmm, I wonder what's in there?  And they found out.  So we're going to talk about that.



But whereas last week was like this post-Thanksgiving security dead zone, I don't know what happened, but the point was nothing much happened.  We still had a good podcast.  But this week, whoo, we're making up for that.  We've got another Lenovo Superfish-style local security certificate screw-up that I'll remind our listeners who maybe have joined us since that all happened with Lenovo, what that was about, but then also what has happened recently.  We've got several new large and high-profile security breach incidents, of which the Marriott is only one.  And I want to talk a little about what that means for us, sort of in aggregate.



We've got the inevitable evolution of exploitation of publicly exposed UPnP router services.  We've been talking about this happening; and, sure enough, those chickens came home to roost.  The emergence of, believe it or not, printer spam.  Also some indication of how well ransomware pays.  As a consequence of the U.S. Treasury going after a couple ransomware culprits, we now have a sense for that.  We also have the story of two iOS scam apps, a false-positive warning that Bing was producing, some very encouraging progress of running DNS over HTTPS rather than UDP where it's all exposed.  Mozilla has been experimenting with that with great results.



Also the rumor that Microsoft, believe it or not, is abandoning all of the work on and their Edge HTML engine in favor of, believe it or not, Chromium.  We also have a bit of miscellany, a new cybersecurity-related Humble Book Bundle just in time for Christmas that has some tasty goodies on it.  We have a bit of closing-the-loop feedback.  And then we'll discuss this interesting new research from some people who took a peek into GPU RAM, wondering what they would find, and realized it was possible to determine what web pages people had been visiting, even though that's a no-no from a privacy standpoint.  So I think another great podcast for our listeners.



JASON:  Absolutely jam-packed.  And I knew it was going to be, especially because of the Marriott and the Quora news today.  I was like, oh, boy.  We've got a lot of ammo.  There's just so much more in here.  We're going to talk about that.  All right.  We start with a picture, don't we.



STEVE:  We do.  Now, the reason is there's a little back story here that you wouldn't be aware of.  Last week's Picture of the Week was a truly hysterical wiring closet which, I mean, it was just the wiring closet from hell.  It was, I mean, wires were crossing back and forth; and, I mean, it was a disaster.  And it really generated a lot of feedback from our listeners, who got a kick out of it.  Many of them shared some of their own favorite pictures of similar closets.  And this one stood out from all of them.  I gave this one the caption:  "Shelves?  Who needs shelves?"  Just because...



JASON:  Where we're going, we don't need shelves.



STEVE:  Where we're going, we don't need - exactly.  And because this is a much more modest event.  This was some church networking closet of some sort.  You can sort of in the upper right is a Panasonic, so that's probably the church's phone system.  But what I got a kick out of was that there's a silver D-Link consumer router that is just suspended in midair from one of its network cables.



JASON:  Oh, that poor cable.



STEVE:  And it's got, like, what, yeah, exactly, it looks like it has about eight of them.  Like all of his little ports are full.  But it's just sort of floating there in the middle of the space saying, yeah, you know, we don't need a shelf.  I mean, and there is a shelf right down below.  But that network cable coming from above won't let it quite get down to the shelf, so it's just floating in midair.



JASON:  Too bad there's no way to extend that network cable.  There's just no way.  This is the only option.



STEVE:  Isn't that a shame?  Isn't that a shame that they only come in one size, and that one was too short.



JASON:  I don't know if I've ever done that exactly.  But I will say that I looked at this image, and a part of me could totally identify with it.  I was like, yeah.  I mean, you know, it's in a room that you never go in.  And, I mean, it's probably going to work.  So, okay, fine.



STEVE:  Well, and you can imagine, too, that when this was lashed together, the person doing it probably had the best of intentions.



JASON:  Sure.



STEVE:  It was probably, okay, this is just...



JASON:  Temporary.



STEVE:  ...till I get to Fry's and pick up a long cable.  I'm definitely going to do that real soon now.



JASON:  And then they realized, maybe I don't need to.  It's still working.



STEVE:  There actually is dust on the D-Link router.  You can sort of see some fingerprints from where...



JASON:  Yes.  Oh, totally.  That thing's been there a long time.



STEVE:  So, yeah.



JASON:  And that's a good point.  Virgil in the chatroom says airflow going around the router, I mean, it's keeping it cool entirely.



STEVE:  Ah, convection cooling for a fanless experience, absolutely.



JASON:  Good point.  Very good point, Virgil.



STEVE:  Yeah, I like that.



JASON:  Love it.



STEVE:  Okay.  So believe it or not, we had four years ago what was known as the Lenovo Superfish debacle.  To remind our listeners what that was, Lenovo began bundling something known as Superfish with some of its computers back in September of 2014.  That didn't last long because by the end of February of 2015, the U.S. Department of Homeland Security was advising its removal along with its associated root certificate.  So Lenovo came under fire, I mean, immediately upon the discovery of this, for preloading this Superfish, that was the name of this advertising system from some bulk advertising people, I think they were maybe Israeli and operating out of San Francisco, if memory serves.



Anyway, this was part of this preinstalled crap that a lot of PCs are bloated down with to varying degrees.  It powered something called Visual Discovery.  And this Visual Discovery benefit was meant to help shoppers by analyzing the images on the web pages that they were visiting and then presenting similar products offered at lower prices, thus "helping users search for images without knowing exactly what an item is called or how to describe it in a typical text-based search engine."  So this was going to be just a breakthrough.



But to do this, the adware, which was installed locally on the person's PC, on their Lenovo laptop, needed to intercept, decrypt, and inspect all web browser connections, even HTTPS/TLS-protected communications.  And to do that it installed its own self-signed root certificate to allow it to impersonate other websites to the PC user's browser.  In other words, it inserted itself as a man in the middle.  And in order to do that, even with secure connections, and to allow them to remain secure, it needs to be able to produce certificates on the fly for the remote websites you're visiting so that your browser thinks, oh, I'm at Amazon.com, https://www.amazon.com.



So that meant that the certificate that this Visual Discovery, the Superfish stuff, created had to be signed by a trusted cert.  Well, it's not a CA, and what it's doing is bad.  So it installed its own essentially CA certificate so that it could sign any certificates that it needed to create on the fly.  All of this is bad.  So it's true that our machines already have a bunch of self-signed root certificates.  That's what root certificates are.  They are signed by the certificate authorities so that they sort of trust themselves.  And that's what trusted CA certs are.  But that's why what's in our root store is really crucial.



I mean, unfortunately, we could argue it's not well enough protected.  And actually, in some of the commentary and dialogue on the 'Net in the wake of this new event that I'll explain about in a minute, people have been talking about the fact that this should not be so easy for stuff to be installed on the fly.  In this case it was preinstalled in the machines that people received.  So you say, okay, well, fine.  But we'll be talking about something that does it on the fly.  So we already have root certificates in our machines.  What makes that system safe is that the server which is asserting its identity with a Certificate Authority signed cert, is able to keep the certificate's associated private key safe, since it's safely locked up at the far end of the connection.  We have no access to the Amazon.com servers.  That's where their private key is that makes all this go.  We get the certificate from it, but we don't get the private key.



And even those annoying, and we've talked about these often, these TLS-intercepting middleboxes which enterprises use to peer into all of their employees' Intranet traffic, they're at least self-contained and located in a secured environment somewhere.  What made the Superfish transgression so bad was that the equivalent of that server software which is, in the case of Amazon, distantly located, and we have no access to it, it was right there in the same PC because it had to perform the equivalent of being the server for that domain in order to do its man-in-the-middle interception.  And that meant that the certificate's matching private key had to also be right there in the PC.  And since the private key needed to be used for the software to operate, the private key's own usage key, that is, to decrypt it also had to be present.



So back then it wasn't very surprising when Robert Graham, who's the CEO of Errata Security, he announced that he'd been able to crack the private key for the Superfish certificate, thus effectively breaking the HTTPS security for all affected Lenovo laptops.  What this meant was that, even remotely, any site could then be spoofed for anyone using one of these Lenovo laptops.  So thus a lot of people were upset.  And in fact, it turns out that in the news just recently we've learned that these slowly turning wheels of justice have finally clicked over, and Lenovo has just now agreed to a settlement in a 32-state class-action lawsuit.  A federal court has approved a large payout fund for Lenovo, and Lenovo will be required to create a $7.3 million reservoir which is set aside to settle this class-action lawsuit over exactly this, that they installed this stuff without users' explicit knowledge or permission, and it was a bad thing to do.  Okay.  So that was then.



Okay.  Believe it or not, this has been repeated by something you just wouldn't expect.  To give you a taste for it, Ars Technica's headline reads:  "Sennheiser discloses monumental blunder that cripples HTTPS on PCs and Macs."  Bleeping Computer titled their coverage:  "Sennheiser headset software could allow man-in-the-middle SSL attacks."  So believe it or not, a headset.  The same mistake has just been made, not in trade for some grand sweeping advertising scheme, as was in the case of this Visual Discovery, but only so that the Sennheiser headset software could run in the user's web browser and securely connect to their own local software also running in the same machine.



In other words, with web browsers becoming increasingly fanatical about HTTPS in preference to HTTP, even when the connection is to the same PC made through the unroutable localhost IP, which our listeners know as 127.0.0.1, and actually the whole 127 block, Sennheiser decided that the easiest way to skin that cat would be to run their own secure local web server on the user's machine in order to get like a padlock, to get https://127.0.0.1.  And in fact that's the common name on the cert that they created is the localhost IP.  And they created the same error.



I mean, there's only one way to do this.  If you're going to do that - oh, the other thing that Superfish did I skipped over, I forgot to mention, is that they didn't create a separate certificate for every PC.  They could have done that, and it would have still been annoying, but it would have been kept local.  And that would have meant that somebody could have only spoofed connections for the PC whose specific certificate they had obtained in order to do so.  You could argue, well, if they're able to obtain the certificate from the PC, they would be equally able to plant their own spoofed root CA in the PC, and thus perform the same sort of hack.



So there was really - that wouldn't give them much leverage.  The leverage came from the fact that all Lenovo PCs shared the same certificate, thus getting it from any Lenovo PC meant you could spoof for all Lenovo PCs.  Sennheiser made the same mistake.  Rather than minting, as they could, a unique certificate for every machine, their installer just has one.  And even worse, after you uninstall the Sennheiser stuff, it leaves it behind.



So anyway, so the same thing has happened.  The guys who discovered this, Secorvo Security Consulting, ginned up a custom certificate of their own.  In their report about this they wrote, and I skipped the preamble, they said:  "Then we created a new key pair and used our fraudulent CA to issue a wildcard TLS server certificate for hosts in the domains of Google, Sennheiser, and some of Sennheiser's competitors," which allowed them to remotely impersonate any Google or other domains for anyone who had ever installed that Sennheiser software, even after uninstalling it, because it left it behind.



So this was all disclosed responsibly.  Secorvo reached out to Sennheiser, said this is not good.  So there are updates for the Sennheiser software now.  Microsoft felt that this was enough of an issue that they waded in with their own remediation of this.  They have an advisory 180029 titled "Inadvertently Disclosed Digital Certificates Could Allow Spoofing."  And Microsoft wrote:  "Microsoft is publishing this advisory to notify customers of two inadvertently disclosed digital certificates that could be used to spoof content and to provide an update to the Certificate Trust List (CTL) to remove user-mode trust for the certificates.  The disclosed root certificates were unrestricted and could be used to issue additional certificates for uses such as code signing and server authentication.  More details are here."  And then they list a couple links.



"The certificates were inadvertently disclosed by the Sennheiser HeadSetup and HeadSetup Pro software.  Consumers who installed this software," writes Microsoft, "may be vulnerable and should visit HeadSetup Update for an updated version of the HeadSetup and HeadSetup Pro software.  As a precaution, Microsoft has updated the Certificate Trust List to remove user-mode trust for these certificates.  Customers who have not installed Sennheiser HeadSetup software have no action to take to be protected.  Customers who have installed Sennheiser HeadSetup software should update their software via the links above."



Now, this brought me back to something that we talked about.  I mean, we've talked about this before, the sort of the background danger of malicious certificates creeping into our own PC's root store because the whole bargain here is that we trust the certificates in our root store because the owners of the matching private keys for those certificates are being trusted with their ability to create certificates for any domain.  I mean, this is the big danger, this is the Achilles heel of the whole system is that it's an any-to-one mapping.  Any single CA that we trust has the ability to create a certificate for any domain.



There was a tool, RCC, which we talked about before.  I went looking around, and the domain that was hosting it is gone now.  And it's for sale if anyone cares, but I wouldn't bother.  There's a better solution from our friends.  Mark Russinovich and Sysinternals have a tool called Sigcheck.  And I've got the links in the show notes to the Microsoft page of documentation and also for the zip file containing it.  And it's, as always, as are all these little tools, two tiny EXEs, one for 32-bit, one for 64.  It's a command line utility, and you give the command "Sigcheck -tv," which is kind of easy to remember.  And what it does is it dynamically pulls the most recent CTL, the Certificate Trust List, from Microsoft and then cross-checks it against the root certs you currently have in your trust store.



I, of course, ran it to see, and I was initially surprised by four things that popped up.  And then I realized, oh, that's okay because I, for the work that I'm doing with SQRL, I created certificates for my own localhost and my own machine name so that I am able to create HTTPS connections to myself, which is fine.  They can't be used for anything else.  But what was cool was that this thing found them and sort of reminded me that I had them there.  And so I would, just sort of as an audit, as something to do here as we close a busy security year 2018, might be worth - I'm sure you can just google S-I-G-C-H-E-C-K, Sigcheck, and grab it and open a command line and run it.  And just make sure that, if it shows you anything, it should be a null list.  It should just say no, nothing.



If anything pops up, take a look at what that is and make sure you know why, as I just did, because what it means is that that thing has the ability to, you know, the owner of the matching private key to the certificate that it shows can synthesize certificates that your browsers will trust.  And just before I leave I should mention, though, that remember that this won't work on the Chrome browser for Google's own properties.  The Google Chrome browser knows, it has the fingerprints of every correctly synthesized Google certificate.  So this spoofing of certs never works on Chrome for Google properties.  It works for other properties, but just not - you know.



So Secorvo Security Consulting said that they created a wildcard TLS certificate for hosts in the domains of Google, and it's like, yes.  And so you could do mail.google.com, but don't try going there with a Chrome browser or all kinds of alarms go off, and that's not good.  So anyway, it's interesting how these mistakes keep getting made.  I would argue a headset installer has no business mucking with anybody's root certificate store.  I mean, it's just ridiculous.  There's no need.  First of all, there's this confusion about localhost.  It's not actually networking.  It's using the packet-pushing, IP-addressing, networking architecture sort of to create interprocess communications.



That's all it is when it's staying within your local machine.  If you connect with localhost, nothing goes out on any wires anywhere.  There's nothing happening external.  So all it is, is interprocess communication.  Unix has used this with Unix sockets forever.  So this notion that they needed a secure connection to the same machine is nonsense.  I mean, it's crazy.  And because it's often done without HTTPS, browsers are honoring the ability to not have HTTPS for localhost connections.  So it was unnecessary in the first place.



And if they for some reason really, really, really felt that they had to do it, then the installer should have made a certificate on the fly so that the same mistake made by Superfish wasn't being made again, and at least the danger would have been contained to somebody who had planted a certificate in a machine or somehow got the private key for the one that had been minted.  The damage could have been constrained.  And who knows?  Maybe this is another class-action lawsuit.  And if a few of them happen, maybe people will learn that this is just not something that can be done.  And really it ought to not be possible for an installer to surreptitiously drop stuff into our root store.  All kinds of warnings should have been produced.  And so that's on Microsoft.  This ought to be closed down.



JASON:  No kidding.  And just to be clear, that Sigcheck, that is not necessarily - so you're encouraging anyone to use this, whether they've installed or used these Sennheiser products or not.  This is broader than that.  This is checking everything including.



STEVE:  Yes, yes.  Yes, because this story is, of course, about Sennheiser.  Superfish was about Superfish.  But other stuff, I mean, the fact that we've just learned as an example that installing Sennheiser headset software can plant a bogus root cert in your trust store suggests, ooh, what else might that hasn't hit the news that might be in your machine.  And malware could do this, as well.  And in fact I think there have been, not malware, but there have been some AV tools which have done this for the same purpose because they want to go in and intercept communications.  So it's just it makes sense to audit  the root store, our PC's root store periodically just to make sure nothing has crept in when we weren't looking.  And Sigcheck, I would argue, would be the official way to do this.  RCC was good, but it looks like it's just been abandoned.  And again, it's from a third party.  The domain has expired.  So it's like, eh, let's just stick with Mark Russinovich's stuff, which we know we can trust.



So as we talked about at the top of the podcast, Marriott two years ago in 2016 purchased the so-called Starwood Properties, which was a whole bunch of different things.  I know that in your conversation with Bruce Schneier on Friday he mentioned that he was a customer of some Starwood assets.  That's W Hotels, the St. Regis, Sheraton Hotels & Resorts, Westin Hotels & Resorts, Element Hotels, Aloft Hotels, the Luxury Collection, Tribute Portfolio, Le Meridien Hotels & Resorts, Four Points by Sheraton, Design Hotels, and even the Starwood-branded timeshare properties.  So a whole bunch of very well-known hotel chains purchased two years ago, acquired by Marriott.



It turns out that what we are just now learning is that two years before then, something apparently crawled into the Starwood network and set up shop, a classic APT, Advanced Persistent Threat, which had been there.  As a consequence of that, Marriott has been forced to disclose that they're still sort of determining the scope of this.  And they said that they had not yet finished identifying duplicate information in the database, but believe it contains information on up to approximately 500 million guests who made a reservation at a Starwood property going back to as early as 2014.



They said for approximately 327 million - 327 million - of these guests, the information includes some combination of their name, mailing address, phone number, email address - oh, so physical mailing address, phone number, email address, passport number, Starwood preferred guest account information, their date of birth, their gender, their arrival and departure information, reservation date, and communication preferences.  They said for some the information also includes payment card numbers and payment card expiration dates, but the payment card numbers were encrypted using 128-bit AES.



There are two components which are needed to decrypt the payment card numbers; and, they wrote, at this point Marriott has not been able to rule out the possibility that both were also taken.  So the fact that the payment card information was encrypted doesn't mean that the bad guys can't decrypt it.  So they said for the remaining guests the information was limited to name and sometimes other data such as mailing address, email address, and other information.  They reported this incident to law enforcement and continue to support that investigation, which is ongoing, and they've already begun notifying various regulatory authorities, as needed.



So this doesn't rise to, believe it or not, the largest breach ever; however, it's now solidly in number two.  The largest breach ever was that massive three billion user breach reported by Yahoo! back in 2016.  But Marriott is now number two.  And I guess predictably, because Marriott's well known, the breach is so big, there was an immediate response from Congress.  Senator Mark Warner said in a statement:  "We must pass laws that require data minimization, ensuring companies do not keep sensitive data that they no longer need.  And it is past time we enact data security laws that ensure companies account for security costs, rather than making their consumers shoulder the burden and harms resulting from these lapses."



And I have to say, you know, we sort of roll our eyes when Congress gets involved in anything, or the government, or bureaucracy.  But this notion of data minimization, I think that makes a lot of sense to me.  I've spoken of it before.  And this idea that data doesn't self-expire, and as a consequence databases which are never pruned are able to just accrue data without end for no reason except nobody has said we need data minimization.  We need data to expire itself when you could argue it no longer has any business value, and after some length of time.  Certainly things, for example, credit card information.  After its expiration date, it ought to be removed.  It's no longer valid.  And you could argue, well, it doesn't have to be removed if it's no longer valid.  But it just ought to be part of the process.



So what I like about it is that, if there was legislation in place to require data minimization, it would be real protection, unlike other policies which are sort of too soft, because its enforcement would be easily tested.  When a breach occurred, and the information that had escaped was revealed, the responsible company can be asked why exactly are you still retaining all of that data from X time ago, from a decade ago or five years ago or two years ago.  What is its ongoing business purpose?  And so the point is it would expose companies to a clear violation of a law to which they could then be held accountable, which we don't currently have.  And that would force it to be enacted, and consumers would clearly benefit from that happening.  So I just think that that part of it makes sense.  



JASON:  I think that makes sense, although the cynical part of me is like, yeah, but then the companies are going to come back, and they're going to figure out a reason why they keep, you know what I mean, they're going to make it part of their business to keep that longer and to justify it.  And I guess that's the point; I guess that would be the point.  But once businesses have this data, it's like next to impossible for them to be okay not having it anymore.  They would resist very hard.



STEVE:  I know.  They want it.  They want it.  In fact, we'll be talking about one here in a minute, a company that's called Data & Leads, as in like sales leads.  And their business model is aggregating all of this.  So, yeah, they don't want to let go of any of it, ever.  Maybe it'll be useful.  And of course in their brochure they're able to brag about how many people they have in their database.  And I think it was 57 million.  And so, yeah, they don't want to have those migrate away.



But before we get to that we have, as you mentioned also, this Quora breach.  Quora, probably everybody knows, they're the Internet's leading question-and-answer site.  They just announced a problem.  Adam D'Angelo posted on the Quora blog with the heading:  "Quora Security Update."  He said:  "We recently discovered that some user data was compromised as a result of unauthorized access to one of our systems by a malicious third party.  We are working rapidly to investigate the situation further and take appropriate steps to prevent such incidents in the future.  We also want to be as transparent as possible without compromising our security systems or the steps we're taking; and, in this post, we'll share what happened, what information was involved, what we're doing, and what you can do."



And he finished his little introduction saying:  "We're very sorry for any concern or inconvenience this may cause."  Then, under "What happened," he said:  "On Friday" - and that's only four days, four or five days ago.  "On Friday we discovered that some user data was compromised by a third party who gained unauthorized access to one of our systems.  We're still investigating the precise causes; and, in addition to the work being conducted by our security teams, we have retained a leading digital forensics and security firm to assist.  We've also notified law enforcement officials.  While the investigation is still ongoing, we have already taken steps to contain the incident, and our efforts to protect our users and prevent this type of incident from happening in the future are our top priority as a company."



Under "What information was involved," they said:  "For approximately 100 million Quora users, the following information may have been compromised:  account information, meaning name, email address, encrypted hashed password, data imported from linked networks when authorized by users; public content and actions, in other words, questions, answers, comments, up votes; non-public content and actions, for example, answer requests, down votes, direct messages."  And he wrote:  "Note that a low percentage of Quora users have sent or received such messages."



He said:  "Questions and answers that were written anonymously are not affected by this breach as we do not store the identities of people who post anonymous content."  He said:  "The overwhelming majority of the content accessed was already public on Quora, but the compromise of account and other private information is serious."  Under "What we're doing:  While our investigation continues, we're taking additional steps to improve our security.  We're in the process of notifying users whose data has been compromised.  Out of an abundance of caution, we're logging out all Quora users who may have been affected; and, if they use a password as their authentication method, we are invalidating their passwords."  So a forced password reset.



"We believe we've identified the root cause and taken steps to address the issue, although our investigation is ongoing, and we'll continue to make security improvements.  We will continue to work both internally and with our outside experts to gain a full understanding of what happened and take any further action as needed."



And then, under "What you can do," he said:  "We've included more detailed information about more specific questions you may have in our help center, which you can find here."  And there's a link that he provided.  I have it in the show notes if anyone is an active Quora member.  You could find the link in the show notes.  Or actually I know that you have probably received email from them because, Jason, you and I got a forward from someone who was affected.  And so Quora has been proactive.  Actually in this he didn't say that they are proactively notifying everybody who may have been affected, but obviously they are.  And, finally, they said:  "While the passwords were encrypted," he said, "hashed with a salt that varies for each user" - so per-user salt - "it is generally a best practice not to reuse the same password across multiple services, and we recommend that people change their passwords if they're doing so."



So my takeaway is here is a textbook case of a company with state-of-the-art policies, doing everything right, taking quick action.  We don't know how early somebody got in, that is, we don't know how quickly they figured out that there was something wrong.  But we do know that it's not four years.  Well, we don't know.  But we do know because - I drew four years from the Marriott instance.  In the case of Marriott, we do know that somebody has been in the Starwood stuff from 2014.



So we don't know at this point how soon this happened.  But it's certainly the case that immediately upon determining there was a problem, Quora jumped and took every possible appropriate step.  And we know that their internal technology policies per user salt on passwords is the best that we know how to do.  They didn't mention PBKDF2, if their salting process and their password hashing is also deliberately slowed down to prevent brute force cracking of specific account hashes, but we can assume and hope that that's the case.  Given everything else that they've done, I wouldn't be surprised.  So I think that's all anybody can do.



And just in the third and final of this lump of news we have the records of at least 57 million records in a massive 73GB data breach.  It turns out that Elasticsearch is currently the number one searchable indexed database in use by enterprise.  The bad news is it is intended for internal server access, does not have an authentication mechanism by default, and it can be and has been exposed, instances of it have been exposed on the public Internet in the past.  The Shodan search engine found at least three IPs where identical Elasticsearch clusters were misconfigured for public access.  The first of these IPs was found and indexed by Shodan on November 14th of this year, and that open Elasticsearch instance exposed the personal information of 56,934,021 U.S. citizens with information including their first name, last name, employers, job title, email, their physical address, state, zip, phone, and IP address.



Another index of the same database containing more than 25 million records with more of the so-called "Yellow Pages" details had name, company name details, zip address, the carrier route, longitude and latitude, the census tract, their phone number, web address, email, employees count, revenue numbers, SIC codes; and so that looks like a Yellow Pages business directory of some sort.  They said:  "While the source of the leak was not immediately identifiable, the structure of the field source in the data fields is similar to those used by a data management company, Data & Leads, Inc.  And the people who discovered this were unable to get in touch with Data & Leads Inc.'s representatives.  And shortly before the publication of this, the Data & Leads website went offline and is now unavailable."



So the coincidence is suspicious.  As of today, the database, this Elasticsearch database is no longer exposed to the public.  However, we don't know for how long it had been online before the Shodan crawlers indexed it for the first time on November 14th, and who else may have accessed and acquired the data.  So anyway, the people who put this up wrote that:  "Our goal is to help protect data on the Internet by identifying data leaks and following responsible disclosure policies.  Our mission is to make the cyber world safer by educating businesses and communities worldwide on ethical vulnerability disclosure policy.  We regularly publish reports," blah blah blah.  So they did the right thing by attempting to get a hold of this, not disclosing that this was public until it was no longer public, even though they were never able to officially determine that it was Data & Leads that were the source of this.



So stepping back from this a little, we have three different instances, all that vary in their details.  It looks like Quora's system was designed correctly and responsibly, but the bad guys found a way inside even so.  It sounds a little bit as though Marriott made a somewhat diseased or ill-advised property acquisition two years ago, which then came back to bite them.  In retrospect, maybe they should have performed a more careful forensic analysis of their Starwood acquisition beforehand.  And who knows?  Perhaps they did, to some degree, and this problem was missed.  But if it was, then this would suggest that their analysis was not deep enough.  Maybe it was just pro forma, just for show, just so that they could say that they had done that.



But the Sony incident demonstrates that an advanced persistent threat can get into a network and hide itself and live there for a long time.  And then we have what looks like just irresponsible configuration of an Elasticsearch service.  Elasticsearch is in use heavily by enterprises, and they are not exposed by default publicly, or there would be many more such instances.  So this was a company that probably had theirs misconfigured, and it's disappeared now.  But it really means that, as users of the Internet, as participants online, to varying degrees our information is exposed publicly.  And as you said, Jason, it is in databases, and these companies don't want to let go of it.  And large sums of money are being leveraged for access to this data, this personal data which, I mean, there are companies that are now selling the fact that they have acquired these databases and are making them available.



So I think what most of us do now is to be aware of the realities of our use of the Internet, that to varying degrees our online lives are subject to exposures.  Maybe it makes sense to create silos, individual trust silos where we use different identities, different email addresses so that to some degree, when something is exposed, it's not our entire world that is exposed.  And I guess just be cognizant of the fact that what is online, anything placed online is subject to eventual disclosure to varying degrees.  Even companies doing their best to keep that information safe can still have breaches.



JASON:  I think what I learn or what I realize in all of this, taking a look at all of these incidents of hacking, and the Starwood hotels is just one example, like do I think that consumers are going to see this and say, well, I'm not going to stay there anymore?  And I think people are just getting so complacent at this point, you know what I mean?



STEVE:  Yup.



JASON:  Like it feels, and I know you know this better than anybody, it feels like an everyday occurrence at this point.  And so as that happens, it just becomes more and more muted.  So thankfully, you know, hopefully there are actions to hold their feet to the fire because I'm not sure the consumers are even doing that anymore. 



STEVE:  No, I agree.



JASON:  Consumers are like, oh, this is part of technology.  I guess we just get used to it.  And that's a problem.



STEVE:  Yeah.  Well, and one could argue, too, that big as these numbers are, if the credit card information is old, well, I mean, no one wants their credit card information to get lost, but it happens.  For whatever reason, I mean, I was an early adopter of credit card purchasing on the Internet.  And I've told this story before.  I used to fly to Northern California, I still do, annually to visit my family on the holidays.  And back in the day I used a travel agent.  And when I would give her a call, she'd say, "So, Steve, same credit card, or did that get compromised?"  And as often as not I would say, "Well, Judy, yeah, I got a new card because there were some bogus charges on my last one, and so I've had to change my number."



Happily, that hasn't happened.  Well, for one reason I'm using PayPal that masks a lot of my purchases now behind that PayPal API, which is a great benefit.  But I think that the point is that, even though that can happen, exactly as you said, consumers are still saying, well, the benefits outweigh the risks.  And so I would argue a rational consumer is still going to stay at a Marriott-owned property if the hotel is in the location that they want and is available and for the best price.  I mean, irrespective of the security side, that just doesn't factor in.  I doubt that it's going to.  And I would argue it probably should not, that even though these things can happen, it's like, yeah, well, okay.  But the convenience of, I mean, how can you even book a hotel?  I guess you can call them.  But they're going to enter it into their computer, and you're going to be in their database anyway.  So it's unavoidable.



JASON:  Absolutely.  It's depressing where we are, but we are where we are.  So, Akamai.  Haven't thought about Akamai in a while.  What's up with Akamai?



STEVE:  Well, they're keeping an eye on things for us.  They named something that they found "EternalSilence" because it combines EternalBlue - which is the NSA-created, I guess you'd call it "attack ware."  EternalBlue is this exploit for moving among Windows file and printer sharing Windows networks, which has been weaponized and been wreaking havoc.  Anyway, EternalBlue and Silent Cookie.  Silent Cookie is the name that an exploit of Universal Plug and Play calls itself because of the entry placed in the NAT routing tables of the routers that it exploits.



So Akamai says that over 45,000 routers have been compromised with this what they call EternalSilence because it's EternalBlue plus Silent Cookie.  They detected a malware campaign that alters the configurations on home and small office routers to open connections inward toward internal networks so bad guys can access and infect previously isolated computers.  And we've been saying this is going to happen.  It's just a matter of time.  While bad guys were using UPnP only to bounce traffic off of exposed routers, it was like, okay, well, it's a good thing that that's all they're doing, but it's always been the case that they could decide to get meaner if they wanted to.



And that's now happened.  They're doing this using the so-called UPnProxy, U-P-N-P-R-O-X-Y, UPnProxy technique, which we've talked about before.  Unfortunately, a surprising, I mean, an amazing number of NAT routers have the UPnP port, port 1900, open to the WAN.  I just - I do not see any possible reason for that.  It is meant to be an internal protocol on the LAN interface that allows, for example, the Xbox famously to access Universal Plug and Play - and I have to make sure I don't say "Universal Plug and Pray" - in order to allow the Xbox for people who want to do Internet networked gaming to accept unsolicited incoming connections.  So an Xbox is able to interface with Universal Plug and Play in order to say, okay, open the following ports through to my internal IP on the Intranet, that is, the internal LAN behind the router.  That was its intended purpose.



The moment that it appeared, I raised the alarm that there was no authentication in this protocol.  And yes, that was done for ease of use so that it was zero config.  Unfortunately, if you also make the mistake of binding this protocol to the WAN interface, then you're exposing a zero config, authentication-free-by-design protocol to the public Internet.  And this is the result.  So when we first talked about this last April, we saw that hackers were using this technique to convert routers into proxies for bouncing DDoS, spamming, phishing, and other traffic.



But in a report published last Wednesday, Akamai says it's seen a new variation of this UPnProxy where hackers are now leveraging UPnP services to insert different rules into these exposed routers' NAT tables.  The rules still function as a proxy, but instead of relaying the traffic back out, they allow an external hacker to connect to the SMB, that's the Server Message Blocks, ports 139 and 445, of devices and computers located behind the router on the internal network.  Akamai says that something on the order of 277,000 routers, more than a quarter million, 277,000 routers they have found with vulnerable UPnP services are now exposed online.  Of those, 45,113 have been modified in this recent campaign they've uncovered.



Okay.  So that says Akamai themselves have been connecting to port 1900 and pulling the NAT routing table out and examining it.  They found that one particular hacker or hacker group has spent weeks creating custom NAT entries across these 45,113 routers named "galleta silenciosa," which they said is Spanish for silent either cookie or cracker, thus the name of this exploit.  So what they're doing is, once this hacker is able to create this mapping that allows them to access port 139 and 445, then they're injecting the NSA's EternalBlue weaponized SMB exploit into the networks in order to then expand laterally and find all of the machines that have those ports open and are internally using file and printer sharing on Windows machines.  Which then Akamai says they have detected "millions of successful injections" during which these crooks connected through these ports to the devices beyond the router.  Akamai said that these devices number somewhere in the 1.7 million range.



What the hackers did, Akamai cannot determine as they don't have visibility inside the networks.  That would be, I mean, it's one thing to take a peek at the exposed routing tables in the NAT routers.  It would be going too far for them to go inside the network.  Though the point is anybody can, if you happen to have one of these routers with an exposed UPnP.  This has been going on for a long time, that is, the exposure has been.  Now, only recently, hackers have woken up to it and said, oh, let's have some fun doing this.



Anyway, so I guess if there's any good news, it's that this does not appear to be a large-scale, nation state-orchestrated hacking operation with any larger goal in mind.  Akamai said that the recent scans suggest that these attackers are opportunistic intruders.  The goal isn't a targeted attack.  It's an attempt at leveraging off-the-shelf exploits to cast a wide net into a relatively small pond in the hope of scooping up a pool of previously inaccessible devices.



So anyway, they wrote that companies and individuals who don't want to be victims of these and future attacks are advised to either disable UPnP service on their routers - yes, please do - or, if UPnP must be exposed on the WAN side, and again I know of no use case for that, they said at least obtain a well-secured router that doesn't use a vulnerable UPnP implementation.  And by all means give it an impossible-to-guess username and password.  So anyway, Akamai refers to this particular router hacking campaign as EternalSilence, and I have a feeling we've not seen the last of campaigns of this sort.  



JASON:  Have we seen the last of PewDiePie?  This is the most important question I have to ask you today.  I know that the episode theme is on GPU RAM Image Leakage.  I really think it should have been on PewDiePie.  Just my opinion.



STEVE:  It was a close call, I'll say that.  Because even, I mean, for even I to know that PewDiePie, I mean, I've heard that before.  I've heard the phrase.  I don't know what one is, but I know that there's something...



JASON:  What is a PewDiePie?



STEVE:  There is something.  There is a PewDiePie.  That's a something.  So imagine this.  You're apparently a big fan of the Swedish YouTuber and comedian and videogame commentator whose real name is Felix Kjellberg, known on YouTube as PewDiePie.  You learn that his number one position on YouTube by subscriber count is being endangered by T-Series, some apparently lame Indian music record label and film company that simply uploads videos of Bollywood trailers and songs.  Both YouTuber channels are right around 72 million subscribers.  But at the moment, PewDiePie is narrowing.  It's down to only about 300,000, and you want to support PewDiePie.



You decide to take matters into your own hands.  You want to send out a message to everyone.  But you don't have President's Trump's "the world is ending" universal cell phone presidential SMS blaster code.  But you are in possession of some modest hacking skills.  And you're apparently not very troubled by questions of ethics, morality, or legality.  How do you proceed?  You surf over to Shodan and poke around to find a bunch of something.  Turns out you find a lot of printers, publicly exposed printers.  Perfect.  PewDiePie, here we come.  According to the hacker, he found three different vulnerable printing protocols on Shodan - IPP, LPD, and JetDirect - with up to, get this, 800,000 vulnerable printers in total.  Jason, what is becoming of this world?



JASON:  I don't really know.



STEVE:  It's just [muttering].  So this guy tweets.  And what's his name?  Oh, his handle is @HackerGiraffe.  So @HackerGiraffe tweets:  "I was horrified [uh-huh] to see over 800,000 results show up in total.  I was baffled," he tweets, "but determined to try this.  So I picked the first 50,000 printers I found running on port 9100" - yup, that's the printer port - "and downloaded the list off Shodan."  He says:  "I then used PRET [P-R-E-T], the PRinter Exploitation Toolkit, on GitHub, which gives hackers the ability to access files, damage the printer, or access the internal network."  Whoo, how nice.



However, @HackerGiraffe said that he only wanted to use the kit to print out messages about PewDiePie to "spread awareness" and make sure that T-Series doesn't overtake PewDiePie as number one because of course.  He tweeted:  "PRET [PRinter Exploitation Toolkit] had the scariest of features:  ability to access files, damage the printer, access the internal network, things that could really cause damage.  So I had to do this to at least help organizations and people that can protect themselves."  So, oh, I guess it doubled as a public service announcement.



The hacker typed up a bash script which runs an exploit kit against the impacted IP, that is, 9100, with commands to print a message, then quit.  He then uploaded the script onto his server and left it running.  The printed message said:  "PewDiePie is in trouble" - which is not something you see every day - "and he needs your help to defeat T-Series!  PewDiePie, the currently most subscribed to channel on YouTube, is at stake of losing his position as the number one position by an Indian company called T-Series, that simply uploads videos of Bollywood trailers and songs."  I guess they're popular, though.



The message then urged readers to unsubscribe from T-Series and subscribe to PewDiePie, and concluded the message by telling readers to tell everyone they know.  And Jason, you are dutifully showing this message on the video of the podcast at the moment.  And it went on to say other things.  Many people were surprised by this.  In fact, one person tweeted that it appeared on the ticket printer of the police station.  So, yes, there are many exposed printers on the Internet.  Even police ticket printers are online.  So, you know, never to allow a good opportunity to go unexploited, within a day or two, the PewDiePie hack, which did generate lots of attention in the Twitterverse, has apparently spawned a new web service over the weekend.  There is now a Printer-Spam-as-a-Service, known as just the unimaginative name "Printer Advertising."  We have a picture in the show notes of...



JASON:  Guerilla marketing.



STEVE:  ...printer adver- exactly, guerilla marketing.  It says:  "Secure your spot in the most viral ad campaign in history.  We have the ability to reach every single printer in the world."  Well, not mine, and I'm sure not those of most of our listeners.  And then they said:  "Reservations are limited."  Uh-huh.  So the good news is, if this actually happens, the  end result will be certainly the removal of at least some of Shodan's inventory of apparently as many as 800,000 currently exposed accessible and perhaps vulnerable printers from the public Internet.



Andrew Morris, the founder of GreyNoise Intelligence, detected the message, that is, this printer advertising message, in one of his company's honeypots on Sunday.  But the spam campaign pushing this ad to exposed Internet-connected printers has continued through Monday, through yesterday.  All of the printer spam originates from an IP address which is quite well known to those who monitor this sort of Internet background radiation, which our listeners know is the term I coined long ago to describe this junk on the Internet that's just never going to go away.  It's just background radiation, stuff scanning for, you know - Code Red and Nimda worms, there are still some instances of them alive and scanning.



The IP address is 194.36.173.50, which is known for generating quite a lot of bad traffic.  It's scanning for routers for UPnP services, ColdFusion plugins, exposed LDAP servers, web servers, DNS servers, and Memcached servers.  So just sort of a potpourri.  Anyway, if you start getting spam on your printer, please take that as a heads-up that somehow your printer is exposed to the Internet, and either you or your IT people should fix that because we have talked about the fact that printers are incredibly complex interpreters.  And as we know, interpreters are virtually impossible to secure.  And printers have lots of known vulnerabilities.  They should not be exposed to the Internet.  It'll just be another way into your internal private network.  Not good.



So we've talked about ransomware, of course, a lot because - I guess it's maybe been supplanted to some degree by cryptocurrency mining that seems to have caught the attention of people.  The problem for a long time was how to get paid, or how the ransomers who had encrypted somebody's computer would get paid.  Then of course along came bitcoin and solved that problem.  It was like, oh, we'll ask for some bitcoin.  So one of the questions has been, is this profitable?  Does it pay?  One of the problems we've noted has been that, first of all, if anybody's got a current backup, then you're certainly more assured to use your current backup to restore your computer than to pay sketchy ransomers for maybe restoring the contents of your files.



There have been instances where, speaking of police stations, all of a police station have had all their computers encrypted with ransomware, and they have paid to have them decrypted because they had to have access to police records that had to be current and had not yet been backed up.  So there are instances where ransoms have been paid.  As I mentioned at the top of the show, and we have the details here, this week the Department of Justice unsealed a grand jury indictment against two Iranian hackers who are alleged to be responsible for the SamSam ransomware.



As part of this indictment, for the first time, the U.S. Department of Treasury's Office of Foreign Assets Control, OFAC, also publicly attributed cryptocurrency addresses to individuals who were involved in converting ransomware-generated cryptocurrency payments into fiat currency.  The Department of Treasury's announcement stated:  "While OFAC routinely provides identifiers for designated persons, today's action marks the first time OFAC is publicly attributing digital currency addresses to designated individuals."  So that alone is sort of noteworthy.



In this particular case the cryptocurrency addresses are being attributed to Iran-based individuals named - and I'll just call them Ali and Mohammad because their last names are unpronounceable by me - who the U.S. government states have facilitated the exchange of ransomware payments into Iranian currency.  The addresses attributed to these individuals are, and I have them in the show notes, they are now public record, and here's where you want to sit down.  They contain a combined total of 5,901 bitcoin.



JASON:  Dang.



STEVE:  Yeah, baby.



JASON:  Rolling in it.



STEVE:  Which puts the value of that cache of bitcoin at over 23 million USD.  So I hate to say crypto ransomware pays, but, boy.  If all of this money was generated from ransom payments, and it seems likely, I mean, we don't know where, may not be U.S. consumers who have been paying because crypto ransomware is a global thing.  But, yikes.  These guys have made some money.



What's interesting here is that OFAC has also added these two guys and their bitcoin addresses to the specially designated nationals and blocked persons list known as the SDN, Specially Designated Nationals.  Which means that U.S. individuals and companies are legally blocked from doing business or conducting any transactions with these individuals, dot dot dot, or with their bitcoin addresses.  These sanctions, that means, could also affect non-U.S. businesses and individuals who conduct transactions with them due to secondary sanctions.  But on the primary side, that means that it is now illegal for a U.S. citizen to pay the ransom to these guys if you're encrypted with their SamSam ransomware.  Which is sort of an odd twist on all of this.  I mean, I don't know what you do.  I guess back up before you get hit by this annoying stuff.



JASON:  I bet they're wishing they had sold it a year ago.



STEVE:  Oh, yes.  Or even two months ago.



JASON:  Twenty-three million now.



STEVE:  Two months ago, because it's dropped in half.



JASON:  Oh, I know.



STEVE:  You're right.  It was north of 20,000, but it sat around 6,500 for a long time before this recent crash in value.  So, yeah.



JASON:  Totally stable.



STEVE:  Two iOS apps were caught stealing money from their victims.  I just sort of wanted to put this on people's radar.  It's already been remediated.  Apple was quickly notified.  The apps were taken down.  But it's just sort of an interesting hack.  And it would be nice if Apple were to do something to make this less likely to occur.  They were both highly rated.  Fake rated, but still they had like 4.3 stars, many five-star reviews, glowing reviews, so that anybody in the iOS App Store looking for a fitness app, these things were titled "Fitness Balance" and "Calories Tracker."  You did your due diligence.  You looked at the reviews.  They looked positive.  They were fraudulent reviews; but, still, how could you know?



And so what happened was when you installed the app and sat down to use it, it would ask for your fingerprint to access the stored data in the app in order to use it.  If you gave it, there was a brief popup showing an Apple Pay transaction of around $100.  It was 139 euros or between 99 and 119 USD, depending upon your currency.  And then it would disappear.  And if you weren't paying attention, you would get billed $100 is the point, is that these apps were charging its users $100 and nagging them if they closed it and didn't do it and kept trying to do it.  And the problem is on the iOS phones that have the Home button, that's the same as the fingerprint.  And so you put your fingerprint there, and that authorized payment while the phone is on.



The iPhone X, where the button has been removed, there's an option that's on by default that makes it very clear you're making a payment.  The screen darkens.  It says, just over to the right where the power button is, "Double-click to pay."  And so you'd be going, like, whoa, wait, stop.  So Apple has on the iPhone X and subsequent devices that don't have the button, they have made it more difficult to have an inadvertent payment.  But, I mean, and I saw screenshots of the popup.  Reddit carried a bunch of conversations about this because this exploded quickly while it was there on the App Store before Apple took it down.



And the good news is, if you have a credit card as opposed to a debit card associated with your Apple Pay, we know that credit cards indemnify their owners against fraudulent payments, and so you could certainly get this charge reversed, as long as you knew that it had happened and you then challenged the charge on your statement when it showed up.  And maybe you could go to Apple and say, hey, I just got dinged by a malicious app from your store, and see if they could make you whole again also.  But anyway, just worth noting that every so often things do sneak past the Apple Store's scrutiny and their curation of apps.  Response is generally quick, but you want to keep an eye on these things.



JASON:  All right.  Got a few little bits here before we head into the main event.



STEVE:  Yup.  So the good news is Mozilla's experiment with DNS over HTTPS is turning out to be a win.  They recently posted, late last week, next steps in DNS over HTTPS testing.  They wrote:  "Over the past few months, Mozilla has experimented with DNS over HTTPS, also known as DoH.  The intention is to fix a part," they write, "of a DNS ecosystem that simply isn't up to the modern, secure standards that every Internet user should expect."  They said:  "Today we want to let you know about our next test of the feature.  Our initial tests of DoH studied the time it takes to get a response from Cloudflare's DoH resolver.  The results were very positive.  The slowest users show a huge performance improvement."  And that's significant.  The slowest users, meaning that there were users whose DNS servers were more than average slow, and this fixed them so that they saw a big bump in performance.



They said:  "A recent test in our beta channel confirmed that DoH is fast and is not causing problems for our users.  However," they wrote, "those tests only measure the DNS operation itself, which isn't the whole story.  Content Delivery Networks (CDNs) provide localized DNS responses depending upon where you are in the network, with the goal being to send you to a host which is near you on the network and therefore will give you the best performance.  However, because of the way Cloudflare resolves names, this process works less well when you're using DoH with Firefox.  The result is that the user might get less well localized results that could result in a slow user experience, even if the resolver itself is accurate and fast.



"This is something we can test.  We're going to study the total time it takes to get a response from the resolver and fetch a web page.  To do that we're working with Akamai to help us understand more about the performance impact.  Firefox users enrolled in the study will automatically fetch data once a day from four test web pages hosted by Akamai, collect information about how long it took to look up DNS, and then send that performance information to Firefox engineers for analysis.  These pages aren't the ones that the user would normally retrieve and just contain dummy content.



"A soft rollout to a small portion of users in our release channel in the United States will begin this week and end next week.  As before, this study will use Cloudflare's DNS over HTTPS service and will continue to provide in-browser notifications about the experiment so that everyone is fully informed and has a chance to decline participation in this particular experiment.  Moving forward, we are working to build a larger ecosystem of trusted DoH providers, and we hope to



be able to experiment with other providers soon.  We don't yet have a date for the full release of this feature.  We will give you a readout of the result of this test and will let you know our future plans at that time.  So stay tuned."



And I think this is great.  What this says is that traditional DNS over UDP will probably end up ultimately only being used by our OS stuff, not by our browsers.  Browsers can themselves, as all of this demonstrates, choose not to use the underlying OS-provided DNS services, but to do their own.  And browsers also arguably represent by far, I mean, I don't know what the number is, probably 99% of all DNS activity because, as we know, when we go to some random page on the Internet, these days pages are composed of crap coming from every direction, all which has a domain name that needs to get looked up, to be turned into an IP address for the browsers to go pick up all the stuff that now composes contemporary web pages.  As opposed to the underlying OS that, yeah, it's got a few links to Microsoft or various repositories to check for updates and so forth.  But not nearly the crazy spread, the heterogeneous access to domains all over the Internet that our web browsers have.



And this sort of automatically secures - not only provides us privacy protection, but it centralizes all these accesses so that it makes sense to push 99% of all DNS over a one-time established secure tunnel to some DoH provider.  So I just think we're seeing what is going to end up being an inevitable migration of browser-based DNS to a secure channel to prevent it from being intercepted and snooped on and to give everyone, especially people who had been suffering with slow DNS from typically their ISPs to give them a big performance boost.  So I just say yay.



And an interesting piece of gossip, which is all it is at this point.  There's no confirmation.  Microsoft hasn't said anything.  This comes from Zac Bowden over at Windows Central.  He writes that:  "Microsoft Edge web browser has seen little success since its debut on Windows 10 in 2015.  Built from the ground up with a new rendering engine known as EdgeHTML, Microsoft Edge was designed to be fast, lightweight, and secure.  But it launched," he writes, "with a plethora of issues that resulted in users rejecting it early on.  Edge has since struggled to gain traction, thanks to its continued instability and lack of mindshare from users and web developers."



Now, I should just say, since I'm not primarily a Windows 10 user, I'm using Windows 7, and as we know I'm a Firefox user, I haven't experienced any of that.  When I have used Windows 10, I've used the default Edge.  It seems fine to me.  But anyway, Zac would know.  He says:  "Because of this," he writes, "I'm told that Microsoft is throwing in the towel with EdgeHTML and is instead building a new web browser powered by Chromium, which uses a similar rendering engine, first popularized by Google's Chrome browser, known as Blink.  Codenamed 'Anaheim,' this new browser for Windows 10 will replace Edge as the default browser on the platform, according to my sources, who wish to remain anonymous."



He says:  "It's unknown at this time if Anaheim will use the Edge brand or a new brand, or if the user interface between Edge and Anaheim is different.  One thing is for sure, however; EdgeHTML in Windows 10's default browser is dead."  Whoa.  He says:  "Many will be happy to hear that Microsoft is finally adopting a different rendering engine for the default web browser in Windows 10."  Although on the other hand we know that most people are using Chrome already, so okay.



He says:  "Using Chromium means websites should behave just like they do on Google Chrome in Microsoft's new Anaheim browser, meaning users shouldn't suffer from the same instability and performance issues found in Edge today.  This is the first step toward revitalizing Windows 10's built-in web browser for users across PCs and phones.  Edge on iOS and Android already uses rendering engines native to those platforms, so not much will change on that front."



Oh, and he said:  "In addition, Microsoft's engineers were recently spotted committing code to the Chromium project to help get Google Chrome running on ARM.  Perhaps some of that work will translate over to getting Anaheim running on Windows 10 on ARM, as well."  And he concludes, saying:  "I expect we'll see Microsoft introduce Anaheim throughout the 19H1 development cycle" - which is the current one - "which insiders are currently testing in the fast ring.  This is a big deal for Windows," he writes.  "Microsoft's web browser should finally be able to compete alongside Chrome, Opera, and Firefox; and those who are all-in with the Microsoft ecosystem will finally be getting a browser from Microsoft that works well when browsing the web."  He says:  "There's still lots we don't know about Anaheim.  I'm sure we'll hear more about it officially from Microsoft in coming weeks."



So that's really interesting.  First of all, I guess I agree that having the behavior integrated seems like a good thing.  And of course, as I mentioned before, there is, in my own little camp, there is work being done on a web extension for SQRL which is being developed for Firefox and Chrome.  Maybe this would mean it would automatically run under Edge, as well, which would be cool.



Just a quick note that Bing was for a while generating a false positive warning about VLC Player.  VideoLAN, the creators of VLC, tweeted:  "Supposedly, @bing now consider vlc-3.0.4-win64.exe to be malware, which gives an annoying popup."  VideoLAN said in their tweet:  "This appeared two days ago, and we have no clue how to fix it yet.  We've checked, and the binary has not changed and is still correctly signed.  TBC."  I guess to be continued.  Anyway, that was on November 27th.  That has since been fixed.  But, you know, this happens to all of us.  I think it was my Never10 app was generating a false positive malware warning for a while, and it just sort of happens.  As we know, malware is becoming a little more heuristic, a little more guessing in its nature, and so false positives are a possibility.



I have two little items in our Miscellany.  First off, from Ian Wills, this is probably the single best abbreviation rename I've ever run across:  "GDPR renamed Greatly Disproportionate Privacy Response."  So anyway, I got a kick out of that.



And many of our listeners have tweeted the news of this Humble Bundle Cybersecurity book deal.  When I checked this morning, the time remaining was counting down from just under six days.  So anybody listening to this podcast by next week the same time should be okay.  Well, minus a day.  I'm not sure.  So that would suggest that Monday morning of next week it will have expired.



As Humble Bundle purchasers know, these are amazingly good deals at $1, $8, and $15, depending upon which level you decide to purchase.  You get DRM-free multiple format, both PDF, EPUB, and MOBI format book deals.  I saw among them "NMAP:  Network Exploration and Security Auditing"; "Network Analysis Using Wireshark 2"; "Cryptography in Python"; "Hands-On Penetration Testing on Windows"; "Metasploit Penetration Testing Cookbook"; "Mastering pfSense," which that's my favorite router for people to install on various hardware to create a very secure, highly feature-complete premises router; "Mastering Kali Linux"; "Metasploit for Beginners"; and "Mastering Linux Security and Hardening."



And that's like a third of all the books that are available for a very low price.  So just in time for Christmas.  And you could buy them and give them on a thumb drive to somebody who is interested in security also because you buy them and you own them.  And all of the proceeds are donated to charity.  So as we know, Humble Bundle does this from time to time, and this looks like a nice lineup.



JASON:  Yeah, I love Humble Bundle.  They're good.



STEVE:  Yeah, yeah.  And something has never happened before, believe it or not, in the SpinRite world, which is EE, whose "from" line says he's from AU in SF, by the name of Anthony May, wrote a very nice article on, of all places, Quora.  And it's long, so I'm not going to drag our listeners through it all.  But it's at Quora.com, "What Needs Repair on a Computer That is Harder Than You Think."  And it starts out talking about hard drives and their bad sectors.  And I'll just read a little bit.  I'll read the beginning and the end.



He said:  "Yes, 'spinning rust' hard drives whose design goes back to the 1950s, still going strong throughout this decade despite the rise of solid state drives, will continue well into the next decade."  He said:  "To be clear, there's not much real repair goes on with modern computers, or tech in general these days.  It's deemed not economically viable to repair compared to the cost of replacement, swapping out some module, card, motherboard, drive, et cetera.  But hard drives?"  He says:  "They're spinning death Frisbees, just waiting to gobble your precious data stored on them.  And more goes wrong in them than most people realize, their data kept safe only thanks to mathematics."



He says:  "'Bad sectors' is a term you hear occasionally, but it's rare to hear someone who actually knows how to fix them because in reality there's only one way I know that has any likelihood of fixing the problem unless you opt for thousands of dollars at a data recovery specialty business, and it's a commercial software utility" - uh-huh, guess what - "which automatically makes people dubious.  'How could software fix hardware?' they scoff."  And I'll stop reading at that point.  I did encounter the phrase which people have heard me utter before:  "Remember, a hard drive doesn't know there's a problem with a sector of data until it tries to read it and discovers that the math doesn't add up anymore."



And then he finishes this posting - I've skipped a bunch of other stuff which listeners may be curious to read.  He finishes, saying:  "Again respecting the analog-y nature of the magnetic alignment of ferrous particles on the surface of the hard drive and that they can weaken over time, you can exercise the physical hard drive medium by reading the data from the sector, inverting its ones and zeroes and then rewriting that data back to the sector; then reading that data back, reinverting its ones and zeroes, and finally writing that data back to the sector.  The net result is the data is exactly the same, but you've pushed every bit through a write of a one and then a zero and then a one or vice versa, leaving the sector freshly written.  But at each step the hard drive is monitoring the error-detection math to see if there's any sign of surface defect.



"This is what SpinRite does, the commercial software I mentioned earlier.  No, I'm not affiliated with," and he says, "Gibson Research Corporation and Steve Gibson's SpinRite software.  But I've used it for nearly 30 years, as have countless other computer pros, and it's rescued countless amounts of data from presumed death because of this analog-y nature of spinning disk hard drives.  And," he says, "incredibly" - and I agree - "the same results are now being achieved for SSDs, even though their failure mechanisms are entirely different.  But the forward error correction math is still there.  The spare sectors are still there.  And so corrupted data can still be recovered from both spinning rust hard drives and modern solid state drives."  So anyway, very cool posting, Anthony.  Thank you for sharing it and letting me share it with our listeners.



Two quick bits of closing-the-loop feedback with our listeners.  I'll try to pronounce his name, looks like Frode Burdal Klevstul.  He said:  "@SGgrc Is there anything in the SQRL protocol that makes CAPTCHAs obsolete?"  And, oh, boy, do I wish.  But no.  It's something that we had discussed over in the newsgroup where we've been hashing this out and nailing down all the details for quite a while, as our listeners here know.  There wasn't anything obvious that we could do.  And as I mentioned last week, the decision was made to, as much as possible, with only a couple exceptions, keep SQRL minimized to authentication, not burden it with a bunch of superfluous non-auth-related features.



And separating a human from a bot is just not authentication related.  It's human body related.  I mean, it's are you human as opposed to which human are you, or actually which bot are you because bots could use SQRL also, as we have also talked about in recent weeks.  So anyway, nope, unfortunately, that we decided was outside of SQRL's purview.



And then @themainapp tweeted @SGgrc - and I love this, this is probably my favorite tweet of all time - and @GibsonResearch.  He said:  "Just finished listening to 'password immortal' podcast.  Enjoyed your rebuttal of the paper, but I think you missed a few of the paper's points.  I think SQRL will run into usability issues because it's too easy."  Okay, well, let's hope it has that problem.  Oh, yeah.



JASON:  No kidding.  It sounds like a good problem to have.



STEVE:  Sounds like the right problem.  We could always throw in a little wrench there to make it a little less easy to use, if that turned out to be a problem.



JASON:  Make that a toggle, though, in the settings.  Like do you wish for this to be more difficult to use?



STEVE:  Are you sure you want to log in this easily?  You know, exactly.  Are you sure you want to be done so quickly with logging in?  Okay.



JASON:  There you go.



STEVE:  Okay.  So this research, really interesting.  Again, as I said, some guys from South Korea turned over another rock, and what did they find?  I've got the link to the entire research paper, if anybody wants to go any further than I'm going to here because here I'm just going to, as I do, just sort of cover the big points and share what they discovered and, again, how clever hackers are.  So I titled this, well, and this is the title of their paper:  "Stealing Webpages Rendered on Your Browser by Exploiting GPU Vulnerabilities."



The abstract of their paper reads:  "Graphics processing units (GPUs) are important components of modern computing devices for not only graphics rendering, but also efficient parallel computations.  However, their security problems are ignored despite their importance and popularity.  In this paper, we first perform an in-depth security analysis on GPUs to detect security vulnerabilities.  We observe that contemporary, widely used GPUs, both NVIDIA's and AMD's, do not initialize newly allocated GPU memory pages which may contain sensitive user data.  By exploiting such vulnerabilities, we propose attack methods for revealing a victim program's data kept in GPU memory both during its execution and right after its termination.



"We further show the high applicability of the proposed attacks by applying them to the Chromium and Firefox web browsers which use GPUs for accelerating web page rendering.  We detect that both browsers leave rendered web page textures in GPU memory, so that we can infer which web pages a victim user has visited by analyzing the remaining textures.  The accuracy of our advanced inference attack that uses both pixel sequence matching and RGB histogram matching is up to" - get this - "95.4%" accurate.



They said, okay, in their introduction:  "This work considers how attackers can disclose sensitive data kept in graphics processing unit (GPU) memory.  We aim to obtain rendered web page textures to uncover web pages a victim user has visited.  We successfully reveal such data from modern GPUs NVIDIA and AMD when we enable GPU-accelerated web page rendering in recent web browsers Chromium and Firefox."  They said:  "For example" - and I have a picture in the show notes, and they refer to that here - "Figure 1 shows the Google logo image of Google.com and a partial dump of rendered web page textures extracted from an NVIDIA GPU used by the Chromium web browser."  And it kind of makes it clear that there's a relationship between Google's logo and the debris left behind.



"Although the GPU has rearranged the textures according to its undocumented hardware characteristics, we can infer that the dump originated from the web page because their color patterns are similar.  Especially, our combined matching attack can successfully infer up to 95.4% of randomly visited 100 front pages of Alexa Top 1000 websites when a victim uses the Chromium web browser with an NVIDIA GPU."



They explain a little further:  "GPUs are important and powerful components of contemporary computing devices.  Personal computing devices including desktops, laptops, and smartphones use GPUs for supporting various graphics applications.  Graphical user interface, multimedia players, and video games all use them.  Large-scale computing devices including workstations, servers, and clusters also use GPUs for energy-efficient massive parallel computations.  GPUs utilize a large number of processing cores and a large amount of independent memory for efficiently processing graphics operations and computational workloads.  For example, an NVIDIA Kepler GPU can have up to 2,880 cores and 6GB of memory, and its floating-point operation performance is nine times better than that of recent CPUs.



"Programmers can use two types of application programming interfaces (APIs) to access GPUs:  the graphics APIs (DirectX and OpenGL) and the computing APIs (CUDA and OpenCL).  First, the graphics APIs provide functions for graphics operations such as projection, shading, and texture mapping.  Second, the computing APIs provide functions for non-graphics applications such as financial, medical, or weather data analyses; database query optimizations; packet routing; intrusion detection systems; and cryptographic engines.



"The most significant differences between the graphics APIs and the computing APIs are sharing and memory manageability.  The computing APIs allow different users to share the same GPU, whereas the graphics APIs only support a single user.  A number of users can share the same GPU using the computing APIs in a time-sharing fashion, as the computing APIs demand no dedicated screens, and current GPUs only support sequential execution of different GPU processes.  Although some techniques, like VirtualGL, allow remote users to share the same GPU when using the graphics APIs, they warn users of potential security problems, for example, logging keystrokes and reading back images through an X server.



"Second, while GPU drivers manage GPU memory with the graphics APIs, programmers can manually manage GPU memory with the computing APIs, including allocations, CPU-GPU data transfers, and deallocations."  In other words, the computing API gives programmers direct access to raw GPU memory.  "In contrast," they write, "the graphics APIs provide no functions to manage such memory while providing a set of optimized functions to perform memory-efficient graphics operations."



And they conclude, finally, or they said:  "Unfortunately, the sharing and high memory manageability of the computing APIs may incur critical security threats because GPUs do not initialize newly allocated memory buffers.  Although numerous studies consider such an uninitialized memory problem in operating systems, no study deals with the uninitialized GPU memory problem.  If similar security threats exist with the computing APIs, the threats have much larger impact as multiple users may share the same GPU."



And they explain three points:  "In this paper we first perform an in-depth security analysis on GPUs regarding their architectures and computing APIs to reveal any potential security threats.  We identify that the computing APIs have a serious uninitialized memory problem because they do not clear newly allocated memory pages, have memory types that programmers cannot delete, and have in-core memory without security mechanisms.



"Second, we develop effective security attacks on GPUs applicable to the most widely used GPUs by NVIDIA and AMD.  By exploiting the revealed security threats, our attacks can disclose sensitive data kept in GPU memory of a victim program both during its execution and after its termination."



And they said, finally:  "Third, we demonstrate the high applicability of our attacks by inferring browser history of the two most widely used web browsers, the Chromium and Firefox web browsers.  Both browsers support GPU-accelerated web page rendering acceleration, which uploads web page textures to GPU memory to increase rendering speed.  Our attacks can extract rearranged web page textures of both browsers from NVIDIA and AMD GPUs."



So very much like the muck that we've been mired in all of this year, all of 2018, the very first podcast of this year introduced Spectre and Meltdown.  Actually, I think Meltdown was first, and then Spectre soon followed.  And we've been dealing with the consequences of subtle flaws in the fundamental engineering of our processors for the sake of their performance all year.  Here we have what is essentially an off-chip large memory where an API has been created, the computing API, to leverage the GPU's computational capability when it's not being used for graphics intensive applications.  And no thought has been given to the fact that there could be privacy sensitive information.  And who knows about, I mean, who knows like if anybody is using the GPUs for crypto acceleration.



One hopes that they are proactively wiping that memory of any crypto keys or any intermediate results from that computation memory before they release it back to the GPU.  Otherwise, there's a very good chance that is not just image textures from web browsers, but it's potentially serious information being left over from anybody else who has not proactively wiped their own memory before releasing it back to the OS.  So this may not be the last we hear of GPU attacks, thanks to the computing API.  Who knows?



JASON:  Interesting stuff, Steve.  So if somebody wants - is there is anything that you know of as far as automating this?  Like clearing it out in an automatic sort of way?  I mean, if it's just filling up all the time, automating it seems to be a great approach for the majority of people, anyways.



STEVE:  Yeah.  Now, so if it were deemed important, an update to the API could cause an allocation to be cleared prior to releasing a handle to it for the computation API.  What Windows does, and what it sounds like the GPU should start doing, after all, it's got all this computation capability, Windows has a background process which is zeroing unallocated memory in the background all the time.  So it's possible to make an allocation of memory and ask for it to be zeroed by Windows.  I do.  I use it exclusively in the code that I write because you're able to then assume that this will have zeroes and not spend time yourself zeroing it.  And it takes no time because Windows typically has lots of memory that it's not actively using so that it just leaves it zero.  It goes through and wipes it to zero and then moves it to a pre-zeroed queue where it's allocated from.



That's not currently being done in our GPUs, but this kind of research is what would drive NVIDIA and AMD to say, you know, it wouldn't hurt to have a background thread just going along, zeroing out memory.  And if it can, when a computation API call requests memory, give it zeroed memory if it doesn't care, rather than non-zeroed memory.  And that could potentially shut this down immediately.  So that would be a good strategy.



JASON:  Well, there you go.  And, boy, you're saying that wouldn't really impact the performance along the way.



STEVE:  No.



JASON:  Because it's happening as-is.  It's not like some bulk process that happens.



STEVE:  Yup, should be completely in the background, yup.



JASON:  There you go.  Awesome.  I think we did it, Steve.  We made it.



STEVE:  Yup.  Once again.



JASON:  Really appreciate you letting me come on the show with you, man.  It's always a lot of fun, and I always learn a ton.



STEVE:  Oh, Jason, appreciate having you on with us.



JASON:  So you can go to GRC.com for all sorts of Steve's amazingness - SpinRite, of course, the best hard drive recovery and maintenance tool that you can get your hands on.  Get a copy there.  Information, of course, on SQRL, which you talked about a little bit on this show.  Details can be found there.  Also audio and video of this show can be found at GRC.com.  And then, yes, transcripts, which you can't find that on the TWiT site.  So go to GRC.com, and you'll find transcripts of this show, and I imagine all previous episodes, all those transcripts, if people want to...



STEVE:  Yup, they're all there.  All 691 previous episodes.



JASON:  So people can print them out onto paper and read them on the train as if it was a book.



STEVE:  That's right.



JASON:  So do that.  GRC.com.  Our website, of course, is TWiT.tv/sn.  That's the Security Now! page that houses all the episodes here, audio and video, published on that page, so you could check that out.  Record this show live every Tuesday starting at 1:30 p.m. Pacific, 4:30 p.m. Eastern.  And I know that time zones have changed since the last time I did this.  It's probably not 20:30 UTC anymore, is it.  Or maybe it still is.  I'm not really quite sure.  But look it up.  Go to WorldTimeBuddy.com, I think is the site that I usually use, and look up 1:30 p.m. Pacific, and find out when the show is for you.  Thank you, Steve, once again.  Really appreciate it, man.



STEVE:  Thanks, Jason.



JASON:  We will talk to you soon.  We'll see you all next week on another episode of Security Now!.  Bye, everybody.



STEVE:  Bye.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#693

DATE:		December 11, 2018

TITLE:		Internal Bug Discovery

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-693.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm 



DESCRIPTION:  This week we take a look at Australia's recently passed anti-encryption legislation; details of a couple more mega breaches, including a bit of Marriott follow-up; a welcome call for legislation from Microsoft; a new twist on online advertising click fraud; the DHS's interest in deanonymizing cryptocurrencies beyond Bitcoin; the changing landscape of TOR funding; an entirely foreseeable disaster with a new Internet IoT-oriented protocol; a bit of errata; and some closing-the-loop feedback from our truly terrific listeners.  Then we look at a case where a prominent company discovered one of their own bugs and acted responsibly - again - and what that suggests for everyone else.  



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here with lots to talk about.  He's going to have an update on the Australian encryption law and what it could mean to a smartphone near you.  We'll talk about massive hacks, of course, and a Microsoft president suggesting there should be legislation and limits on facial recognition now.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 693, recorded Tuesday, December 11th, 2018:  Internal Bug Discovery.



It's time for Security Now!.  Hey, better an hour late than never.  Sorry to keep you waiting.  Steve Gibson, he's our security guru.  But as often is the case on Tuesday, we get a little backed up.  Hey, Steve.  Good to see you.



STEVE GIBSON:  Yo, Leo.  Yeah, you know, there are things you can do for that little backup, but...



LEO:  Are you telling me there's shots for that?  Anyway, sorry to keep you waiting, but welcome.



STEVE:  Not a problem.  Glad to be back on.  Well, because we've got a lot to talk about.  It's funny because I guess a couple of hours ago I looked over in Twitter, where I had not looked for a while, and I was overwhelmed by the quality of the incoming tweets.  And as it was, we already had, like, too much to talk about. So I thought, okay, shoot.  But I'm going to spend some time over there and catch up because we just have so many great listeners who are doing a great job of finding stuff and making comments and providing feedback.



I wanted to talk about, today, we titled this podcast number 693 for the 11th of December "Internal Bug Discovery," which was motivated by a posting made by a prominent company in the last week who discovered a rather significant privacy breach in their own API and dealt with it.  And what this put me in mind of is that there are four different sources of discovery of bugs, four different sort of like discoverers.  And only one of them is really the way you want the world to work, which is what we just saw.  But it's also the most expensive of the four for a company.  So anyway, I just wanted to spend some time and sort of get a little philosophical about this aspect of the world which is becoming more important, after we catch up with a lot of security.



We've got, of course, the most tweeted topic was this Australia recently passed anti-encryption legislation.  We also have details of a couple more mega breaches, including a bit of Marriott follow-up.  A welcome call for legislation from the president of Microsoft, of all places.  A new twist on online advertising click fraud that just made me chuckle.  The DHS, the U.S. Department of Homeland Security, has put out a document asking for proposals for the possibility of developing a range of technologies, one of which is doing, I was going to say four, but maybe it ought to be two, the other cryptocurrencies, what has been done to Bitcoin in terms of deanonymizing it in order to track people.  We want to touch on that.



We've also got the welcome and changing landscape of Tor's funding, The Onion Router anonymizing network funding.  An entirely foreseeable disaster with a new and forthcoming Internet IoT-oriented protocol.  We have a bit of errata, some closing-the-loop feedback from our truly terrific listeners, and then we'll take a look, as I talked about at the beginning, a look at this case where a prominent company discovered one of their own bugs and acted responsibly and what that means.  And we have just a very fun Picture of the Week.  One of our listeners spotted this, took a picture, sent it to me with the caption that I'm using, essentially.  So I think we'll have fun sharing that, as well.



LEO:  That's funny, yeah.



STEVE:  So another great podcast, I think.



LEO:  Awesome.



STEVE:  So anyway, our Picture of the Week is just a kick.  It's a photo that one of our listeners took of the back door of LogMeIn's location.



LEO:  Oh, that's funny.



STEVE:  We know that because there's the LogMeIn logo prominently shown, and then the door below that says "USE MAIN ENTRANCE" with an arrow pointing to the left.  And so this was sent to me with the caption, "Apparently LogMeIn has a backdoor."



LEO:  Okay.



STEVE:  Okay.



LEO:  They do not have a real backdoor.  Well, they do have a real backdoor.  They don't have a virtual backdoor.



STEVE:  Just to be clear, yes.  Their backdoor is in the physical world, not in the cyber world.



LEO:  Yes.



STEVE:  So unfortunately, speaking of backdoors, Australia's Telecommunications Assistance and Access Bill...



LEO:  Oh, yeah, I was wondering what you'd say about this.



STEVE:  Yeah, of 2018.  Last Thursday Australia's House of Representatives has finally passed what is known as, formally, the Telecommunications Assistance and Access Bill of 2018, less formally known as the anti-encryption bill.  And once Australia's upper house votes the bill into law, which is expected, since the bill enjoyed wide bipartisan support, it would go into effect during the next session of Australia's Parliament, which occurs in early 2019.  So this looks like it's happening.



It would provide, and I'll go into some, you know, this podcast is about interesting details, so I want to cover a little bit of probably what the regular mainstream press doesn't talk about.  It would, as we would expect, I mean, and we've been talking about this sort of thing happening now for quite a while, provide law enforcement with a legal means, that is, lawful means to compel anyone using encryption - so Google, Facebook, WhatsApp, Signal, and others - to assist them in accessing the encrypted communications of their products and platforms.



So the way this thing is written, they use the term "notice" in an unusual way, but it takes the form of three so-called "notices."  There's the TAR, the T-A-R, which is the Technical Assistance Request.  And that's described as a notice to request tech companies for providing voluntary assistance to law enforcement, which includes removing electronic protection, providing technical information, installing software, putting information in a particular format, and facilitating access to devices or services.  Okay, so "voluntary" is like the key word there.  That's the Technical Assistance Request.



Then there's the TAN, the Technical Assistance Notice, which the bill states is a notice "requiring" rather than "requesting" technology companies to provide assistance they're capable of providing that is reasonable, proportionate, practical, and technically feasible, giving Australian agencies the flexibility to seek decryption of encrypted communications in circumstances where companies have existing - and that's key - existing means to do it.  Like, for example, in the bill it states "points where messages are not end-to-end encrypted," meaning either before they have been or after they have been decrypted at the other end.  And then the TCN is the so-called Technical Capability Notice, which is issued by the Attorney-General, requiring companies to "build a new capability" to decrypt communications for Australian law enforcement.  So we can all read that.



Collectively, these so-called "notices" would compel tech companies to modify their software, where modification is necessary, and their service infrastructure to essentially backdoor encrypted communications and data that would otherwise not be obtained.  The bill itself I looked at, and it is incomprehensible to a lay technologist.  I mean, it is just - I think it was 218-some pages.  However, I found a sort of a digest of it which is more conversational and has some back-and-forth and breaks these things down.



So I want to share a couple things because, for example, Apple was involved in providing feedback, and the industry was invited to provide that feedback, although all of this has been relatively fast-tracked.  I mean, it's as if suddenly there's a great deal of urgency.  And a lot of the opposition to this, and you can imagine there's plenty from the civil rights people and the industry itself that is saying, whoa, hold on, let's not rush into anything.



And the other problem we have is that there is, as always seems to be where legislation bumps into technology, there is ambiguity left in the legislation which of course ultimately ends up needing resolution in courts when someone says, "Well, we don't think that's what the bill says," and the people who wrote it say, "Well, yes, but that's what we meant."  And then the people opposing it say, "Well, then, why didn't you say so?"  Well, the reason they didn't say so is they know they couldn't have gotten it passed if it had been a lot more explicit.  So it's deliberately left in the gray in order to get it through the legislative process and dump it at the feet of the courts.



So from this really good digest, they said:  "Following earlier industry consultations, the Government released an Exposure Draft" - which is what they called it - "of the Bill on 14 August 2018."  Okay?  Right?  So August, September, October, November, December, not a long time ago - "and sought public submissions by 10 September 2018."  So whoops, wait a minute.  August, September.  So public submissions within shy of four weeks.



"The Department of Home Affairs [in Australia], DoHA, received almost 16,000 submissions, of which over 15,000 were classified as standard campaign responses" - so I guess politically motivated - "743 were [described as] unique individual responses classified as appropriate for consideration, and 55 were considered substantive submissions from industry groups, civil society, government bodies and individuals."



They wrote:  "While some stakeholders raised concerns about other schedules" - oh.  This thing has, I think it was four, they called them "schedules," which are essentially the meat of the legislation.  It's Schedule 1 which is where all of these three notices were described.  So this says:  "While some stakeholders raised concerns about other schedules, the majority of submissions focused primarily or exclusively on Schedule 1 of the Exposure Draft."  And that is the so-called "industry assistance" schedule.



They wrote:  "Many stakeholders provided submissions that included general and specific recommendations on the proposed industry assistance scheme, including" - and I don't know what these acronyms are - "IGIS, AHRC, LCA and applied cryptography academics" - so those must be Australian interest groups of some sort - "and cryptography academics Chris Culnane and Vanessa Teague.  There was significant concern that the scheme in its current form has very wide application, and that amendments to offer greater definition, narrow the scope, or clarify processes are necessary.



"From a technology perspective, Apple submitted that Schedule 1 'remains dangerously ambiguous with respect to encryption and security.'  Further, Apple stated:  'We encourage the government to stand by their stated intention not to weaken encryption or compel providers to build systemic weaknesses into their products.'"  This is still Apple talking.



"'Due to the breadth and vagueness of the Bill's authorities ... the Bill could allow the government to order the makers of smart home speakers to install persistent eavesdropping capabilities into a person's home, require a provider to monitor health data of its customers for indications of drug use, or require the development of tools that can unlock a particular user's device, regardless of whether such tool could be used to unlock every other user's device as well....  While we share the goal of protecting the public and communities, we believe more work needs to be done on the Bill to iron out the ambiguities on encryption and security to ensure that Australians are protected to the greatest extent possible in the digital world.'"



And none of that happened.  So this thing was pushed through and rushed, and the bill as it stood is what is in the process of being enacted.  So for its part the government argues the new legislation, naturally, we've heard this before in various forms from multiple governments, "for national security and an essential tool to help law enforcement and security agencies fight serious offenses such as crime, terrorism, drug trafficking, smuggling, and [of course it's in there] the sexual exploitation of children."



Since the bill had support from both major parties, the Coalition and the Labor parties, the upper house could vote in support of this Assistance and Access Bill to make it law, which as I said at the top is expected to come into effect in early 2019.  The bill states that the tech companies cannot be compelled to induce a systemic weakness or systemic backdoor into their legitimate software or hardware, or to remove electronic protection like encryption to satisfy the government demands.  So again, here we have this weird collision of nontechnical intent and the statement of what the bill cannot be compelled to cause companies to do, yet they're being compelled to essentially break encryption in some form or fashion.  So, okay.  So who knows how this is going to end up coming out.



"Instead, the new legislation contains measures aimed at facilitating" - this digest says - "lawful access to information through two avenues, decryption of encrypted technologies [okay]  and access to communications and data at points where they are not encrypted."  So I guess somebody has told these legislators that, oh, it's not necessary to decrypt the data if you just get it before it's encrypted or after it's decrypted.



So the bill stipulates, quoting from the bill now:  "We encourage the government to stand by their stated intention" - oh, I'm sorry, that's what Apple had said before.  So we have, as we've said before, this Five Eyes Nations and the so-called "going dark" problem that we've talked about.  And I've looked around at what various technical outlets were having to say about this.  One said that since Australia is a member of the Five Eyes alliance, along with the United States, the U.K., Canada, and New Zealand, which last month declared that "privacy is not an absolute" and the use of end-to-end encryption "should be rare," the new bill could be a stepping stone towards new encryption laws in other nations, as well.



I mean, and that's what we're seeing.  We're seeing everything, well, look at the GDPR.  We're seeing everything sort of creeping forward.  Australian Prime Minister Malcolm Turnbull has previously made his position on encryption clear, last year saying that the laws of mathematics are very commendable, but the only law that applies in Australia is the law of Australia.  And Apple at some point responded to the new bill by making their submission to the Australian government, saying that encryption is simply math, and any process that weakens the mathematical models that protect user data for anyone will by extension weaken the protections for everyone.



So anyway, I really do think that this is the interesting thing, I mean, this will be probably the most interesting issue in 2019, given that Intel's processors have now been fully examined and don't have any more disasters waiting for us, and surprises, because even as we've talked about, in the U.S. there is this notion of warrantable examination.  In the U.S., privacy is not an absolute.  You need to get a search warrant, in which cases you're able to search under that warrant.  And we do have a philosophical collision here between math, which when properly implemented, as Apple says, is absolute.  It can provide absolute encryption that a third party, you know, it's unlike the front door of a house that can be broken into.



So anyway, it just looks like this is a battle between governments and the forces that want to protect our privacy.  And I expect governments to win because they're able to ask for what they want.  And Leo, I like your take when you talk about that the amount of visibility that now exists as a consequence of the incredible use of electronic communications compared to decades ago.  And your analogy, I think, is really good about a few dark pixels on a high-res screen.



LEO:  And they don't like it.



STEVE:  And they don't want any pixels that prevent them from seeing absolutely everything they want.



LEO:  That's Phil Zimmermann's - the creator of PGP - analogy.  And he actually coined Zimmermann's Law that, with the advance of technology, surveillance would advance comparably.



STEVE:  Ah.



LEO:  We were talking about the "Bodyguard," that four-part or six-part episode on...



STEVE:  Oh, yeah, yeah, yeah.



LEO:  From the BBC.  And one thing I noticed watching the "Bodyguard," and I think it's accurate, especially in the U.K., the amazing amount of information they can get just because everybody's on camera.



STEVE:  Yes.



LEO:  Pull the camera for that.  Pull the camera from that.  And I think that that's a lot of what police work is these days.  Well, let's see the video from last night because we have it.  We've got eyes everywhere.



STEVE:  Right.  Right.  And, I mean, it's now become a staple of TV and movies where the detective goes out to the scene and then slowly does a 360, looking for all the cameras that happen to have that region in their field of view, and then says to their underling, okay, go pull the tapes.



LEO:  Go get those, yup.



STEVE:  Yeah, exact.



LEO:  So, I mean, talk about surveillance.  And then you've got all of the information that you can get from a phone that the carrier has, including location information.  The encrypted messages seems to me to be the smallest part of the information.



STEVE:  Well, and in fact we've talked about this in other contexts where we might call everything else the "metadata."  Like the envelope on the letter.  And sure, you may not know what's in the letter.  But the fact of the metadata tells you who's talking to who.  You get to build a whole network of intercommunications.  And as we know, none of that is protected.  It's only the content.  And sure they'd like to know what's there.  But even absent that, it's a surveillance dream for law enforcement.



LEO:  And as you and I have both said, I mean, of course we want bad guys to be caught.  We don't want terrorists to act with impunity.  But they have a lot of information, A.  And the bigger issue is that any compromise of the encryption integrity of any of these devices is going to not only let law enforcement read it, but bad guys will get access to it, too.  And who knows what the security of a phone would be like if it's been compromised like that.  It's just...



STEVE:  Yeah.



LEO:  It would be bad for everybody.  I don't know what the answer is.  You at one point thought that it would be possible, your position on this was it would be possible to somehow let government to get access to this without compromising everybody's safety.



STEVE:  So in a very selective ecosystem, which is Apple's.  And I use them as an example because it's in place, and it exists.  It would be absolutely possible for Apple to curate a master key for each of their devices.  Which does create a concentrated massive region of responsibility.  But we know how to protect secrets.  And, I mean, that's what Apple will end up doing.  If this legislation happens, then the model on the iOS platform will change.  And when the device is created, I mean, everything's already in place.  Devices have unique secrets.  They've got unique keys.  They've got Secure Enclaves.  All of the infrastructure is in place.  All that has to happen is that, under encryption, the device sends an additional unlock key to Apple, who then stores each of its customers' devices unique unlock key.  It doesn't weaken anything.  All of the technology is there except it does create, inarguably, a single point of failure and a great deal of responsibility.



But given that, what it means is that Apple could respond to a subpoena for a specific device so that, for example, after the terror attack that generated so much news here in Southern California for Farood, was it?  Farook?  I don't remember his name now.  You know, the guy who had the cell phone that could not be cracked.  Well, the FBI could generate a subpoena, and they would say to Apple, we need this one phone unlocked.  Apple looks up the unique key for that one device and unlocks it for law enforcement.  In no way, except that Apple maintains a master key for each of their devices...



LEO:  So they'd have to keep that secure, but I think they could do that; right?



STEVE:  Yes.  And that's my argument is that, you know, I get the theoretical ivory tower, there's no way to do this without installing a backdoor or weakening security.  That's just not the case.  I mean, with the under...



LEO:  Here's the problem.  You could say, oh, yeah, well, we could trust Apple.  But then there's Huawei and Samsung and...



STEVE:  That's why I started, yes, that's why I start off by saying this only applies to Apple's ecosystem because this is the way Apple has built this out.  You're right, it's entirely - like for example the Android platform would be, you know, that doesn't exist because you've got all...



LEO:  Each manufacturer would be a separate issue; right?



STEVE:  Yes.  And they're not even updating these devices.



LEO:  No, no.



STEVE:  I mean, that's like, you know, it's nuts.



LEO:  So this Australian law is nothing like that, really.  It's requiring everybody to have some way to get that data.



STEVE:  Yes.  Yes.  And, I mean, from a technology standpoint we will be talking about this, I think, because what will happen?  Now, in some of the dialogue that I've seen that I didn't discuss, there has been the discussion of major - I think there are also very steep fines.  I think it was $10 million is one of the fines that I saw referred to.  Yet it was noted that, well, that doesn't put a dent in one of these massive multinational technology companies, so it's like, eh.  And they could react by saying, fine, we'll pull out of Australia.  Well, that hurts Australia, not to have the services of a company that says we either protect the privacy rights of our customers who may happen to be Australians, or we're not going to have any Australian customers.



So, I mean, but on the other hand, what have we seen before?  We've seen Google initially pull out of China when China said, you know, we need you to filter and play by our rules.  Google initially said no thanks and pulled out.  Well, then they came creeping back in a few years later.  So I think, ultimately, I do believe technology can provide answers.  And I get it that the ivory tower cryptographers would rather say it's impossible. But nothing is impossible for technology, for math.  So I imagine we'll end up finding some solutions.  And it's going to be a great topic for Security Now!.



LEO:  Well, we'll see.  As you say, we'll be reporting on this more in future.



STEVE:  Yeah.  So one of the questions is how do we get companies to be more responsible?  And I'd noted over the past week that the attorneys general of 12 states - Arizona, Arkansas, Florida, Indiana, Iowa, Kansas, Kentucky, Louisiana, Minnesota, Nebraska, North Carolina, and Wisconsin - have gathered together to file a lawsuit against an Indiana-based medical informatics engineering company.  Well, in fact their name is Medical Informatics Engineering, MIE.  And they have a subsidiary, NoMoreClipboard, NMC.  And they're a company that is automating medical records management, as NoMoreClipboard sounds like, so that instead of having all paper records, everything's online.



MIE, the parent company, sells this web-based electronic health record services.  And no one has to be told that you have to take security seriously if you're going to do electronic medical records in the cloud because the cloud is the cloud.  It's very different from a local network in a doctor's office, where everything is kept local.  So the wheels of justice turn slowly.  This all began back a little over three years ago, on May 7th of 2015, when hackers stole very personal information of 3.9 million people from MIE's backend database using a simple SQL injection attack.



And of course being medical records, it's not just LinkedIn information.  It's names, addresses, Social Security numbers, also health data including lab test results, health insurance policy information, past diagnoses, disability codes, doctors' names, medical conditions, the names and birth certificates and birth statistics of their children - in other words, everything.  So this joint complaint accuses MIE of failing to properly secure its computer systems, not telling people about its system weaknesses of which it was aware, and then failing to provide timely notifications of the incident.



MIE never bothered to actually encrypt this information, even though it said it did, so it was in violation of HIPAA regulations just there.  It also provided public accounts for sharing the passwords "tester" and "testing," which were established specifically so that a client's employees did not have to log in with a unique userID.  Yeah.  Why bother with that?  Penetration testers uncovered the issue and highlighted the risk to MIE, but the lawsuit says that MIE took no action.  And then, using one of these test accounts which was discovered, the thieves explored the health record database using SQL injection attacks to gain further access to more privileged accounts, so using the technique we've described in the past, pivoting from one access point to another.



MIE allegedly didn't have any data exfiltration alarms in place.  Nothing alerted them to the fact that data was being taken out of their system.  It was a network performance monitoring alarm, well, of sorts, that finally raised the red flag because the attackers were dumping so many large records from the database at such volume that it choked off the network bandwidth, and they started to wonder, hmm, why is the Internet so slow today?  Uh-huh.



Anyway, the states which have brought this suit allege that once the breach was discovered, MIE only had a draft incidence response plan, and that there was no evidence that it even followed that in any case.  And they added that the notifications, had they been followed, were inadequate.  They discovered the breach on the 26th of May in 2015 and informed the public of the breach via a notice on its website, not directly, 15 days later, on June 10th; and then finally began email notifications another five weeks after that on July 7th; and then finally followed up with printed paper letters in December.



So anyway, the 12 states that are bringing the suit claim that MIE and their subsidiary, NMC, violated federal HIPAA legislation protecting the privacy of health information; also accused MIE of breaking 27 state-level laws concerning data breach notification, abusive and deceptive practices, and personal information protection.  The states are proposing a consent decree to clear up the matter before getting into litigation.  And they're calling for an as-yet undefined payout from MIE, along with its commitment to clean up its act and follow several security measures, including the use of multifactor authentication, not making generic accounts accessible via the Internet, using strong passwords, training their staff properly in cybersecurity, using a security incident and event monitoring solution, and putting SQL injection attack detection measures in place.  And they'll have to do responsible things like conduct regular security audits with help from a qualified professional, file reports, and take actions on them.  In short, the settlement asks the company to do what any competent cybersecurity team charged with protecting sensitive data should be doing.



So in their coverage of this, Sophos notes the collaborative nature of the settlement, and they said:  "As voices call for stricter federal privacy protection laws, this could be a sign that states are getting fed up with these mega breaches and are taking things into their own hands."  They noted that in October Uber settled with all 50 states over the handling of its 2016 data breach, paying $148 million.  So Sophos wonders whether this suit might herald more coordination between attorneys general to hold companies accountable.



And for our part, let's hope that other companies observe this.  We don't know yet what the fine is going to be.  But, I mean, this is a company clearly in violation of existing laws who has been caught doing this.  And I think the fine needs to be hefty so that other companies look at that and think, whoops, it would be much less expensive for us to follow the law and put these measures in place than it would be to be caught not doing so and then have to pay the fine.  Yikes.



And speaking of databases, the director of Cyber Risk Research at Hacken, Bob Diachenko, has been tracking a publicly accessible instance of MongoDB, which could be accessed without authentication, for some time.  Over the course of several months, October and November, Bob initially discovered this database which was open in October, containing 66,147,856 unique records containing the full name, personal or professional email address, user's location, details of their skills, a phone number, employment history, also a link to the individual's LinkedIn profile was present, all of which led him to believe that this was likely scraped data from online LinkedIn profiles.  He was unable to determine the owner of this database, just having found it at some IP on the Internet and unable to attribute it to whomever.  But it is now no longer up.



He had an interview with Bleeping Computer where he noted that the scraping of personal data, presuming that it was just scraped from the web, is legal; and that making a copy of it publicly available is legal, as long as it's not used against the best interests of its owner, which is considered an offense.  There were, as I mentioned, 66-plus million records, including email addresses.  That entire database has been uploaded to Troy Hunt's HaveIBeenPwned service.  So any of our listeners who are interested could make another visit over to Troy's HaveIBeenPwned service, put any of their email addresses in to see if they are newly apparent, which would indicate that this data had gotten loose since the last time they checked.



And, you know, I think what we're seeing is that this is a - things like this, incidents like this, because it's not the first time it's happened, it's not going to be the last time, it's a natural consequence of there now being a global network of commercial entities that are wanting to share and exchange data.  And that, coupled with the incredible dropping cost of long-term high-volume mass storage, which means that everybody can have a copy of everything, and the inherent resale value of aggregated personal information on millions of individuals.  And as has been observed before, we're no longer the consumer who is buying, but rather the details of our existence is the product which is being sold, completely without our knowledge or permission.  You know, we've talked about Equifax that has all of this data that they have vacuumed up and assembled and then are reselling for their own profit, completely without our knowledge or permission.



So I get it that many people will not and do not care about the circulation of their personal information.  But for those who do, I think the only recourse we have is minimizing what is put online where possible, and in many cases it's not possible to do that.  But also, for example, in the case of the firms that provide us the ability to lock access, like the large credit clearinghouses, preventing third parties from using them to acquire credit in our name, thus essentially effecting a form of identity theft, really does make possible.  You know, Leo, you and I have talked about this, the idea of locking access if we ourselves are not applying for credit actively.  I'll say again, it really makes sense.



LEO:  The right thing to do, you bet.



STEVE:  To keep anybody from applying for credit.  So this was interesting.  On the 6th of this month, Brad Smith, the president of Microsoft, posted another piece sort of on something he's been thinking about, clearly.  This one was titled "Facial Recognition:  It's Time for Action."  But what was interesting was that this, as I said, was the second of two.  And we didn't talk about this.  When he brought it up the first time back in the summer, July 13th, was sort of his foundation-laying statement.  And that's the one I wanted to focus on because he's just sort of reiterating that position last week.  But on July 13th he said, and this is what I think is interesting, he says:  "Some emerging uses are both positive and potentially even profound.  Imagine finding a young missing child by recognizing her as she is being walked down the street."  And Leo, of course, this connects right into our discussion of cameras for surveillance a second ago.



LEO:  They're even more valuable if you've got face recognition on them; right?



STEVE:  Yeah.  And he says:  "Imagine helping the police to identify a terrorist bent on destruction as he walks into the arena where you're attending a sporting event.  Imagine a smartphone camera and app that tells a person who is blind the name of the individual who has just walked into a room to join a meeting.



"But other potential applications are more sobering.  Imagine a government tracking everywhere you walked over the past month without your permission or knowledge.  Imagine a database of everyone who attended a political rally that constitutes the very essence of free speech.  Imagine the stores of a shopping mall using facial recognition to share information with each other about each shelf that you browse and product you buy, without asking you first."  He says:  "This has long been the stuff of science fiction and popular movies like 'Minority Report,' 'Enemy of the State,' and even '1984,' but now it's on the verge of becoming possible."



He says:  "Perhaps as much as any advance, facial recognition raises a critical question:  What role do we want this type of technology to play in everyday society?"  And he has a separate topic then, the need for government regulation.  He says:  "The only effective way to manage the use of technology by a government is for the government proactively to manage this use itself.  And if there are concerns about how a technology will be deployed more broadly across society, the only way to regulate this broad use is for the government to do so.  This in fact is what we believe is needed today" - this is Microsoft speaking - "a government initiative to regulate the proper use of facial recognition technology, informed first by a bipartisan and expert commission."



And he says:  "So what issues should be addressed through government regulation?  That's one of the most important initial questions to address.  As a starting point, we [Microsoft] believe governments should consider the following issues, among others."  And these are the bullet points that I wanted to bring up.



He said:  "Should law enforcement use of facial recognition be subject to human oversight and controls?  Should restrictions on the use of unaided facial recognition technology as evidence of an individual's guilt or innocence of a crime?  Similarly, should we ensure there is civilian oversight and accountability for the use of facial recognition as part of governmental national security technology practices?



"What types of legal measures can prevent use of facial recognition for racial profiling and other violations of rights while still permitting the beneficial uses of the technology?  Should use of facial recognition by public authorities or others be subject to minimum performance levels on accuracy?



"Should the law require that retailers post visible notice of their use of facial recognition technology in public spaces?  Should the law require that companies obtain prior consent before collecting individuals' images for facial recognition? If so, in what situations and places should this apply?  And what is the appropriate way to ask for and obtain such consent?



"Should we ensure that individuals have the right to know what photos have been collected and stored that have been identified with their names and faces?  Should we create processes that afford legal rights to individuals who believe they have been misidentified by a facial recognition system?"



So from a technology standpoint, which is of course the approach we take primarily, I think these are really interesting points.  And you know, for example, Leo, that we've all seen signs when we enter a retail establishment, a notice like "video surveillance in use" sort of thing, which is taken to a different level if, it seems to me, if it's facial recognition, automated facial recognition running behind those surveillance videos.  I mean, if you look around in any department store or in any large public facility, you often see those little black domes that are presumably cameras pointing somewhere.  And it's different to imagine that they're going into, being spooled on hard drives in case there's some reason to later run back at some point and see what happened somewhere, in order to recreate an incident that happened in a public place.  That seems different than them running facial recognition and logging the names, well, more than the names, the unique identities of the people that the camera believes it has seen within its field of view on a streaming basis.



But as Brad notes, this really is where we are.  I mean, this is not science fiction any longer.  It's probably already happening somewhere, and we don't know about it.  So we were once upon a time, like I guess it was just after Edward Snowden's revelations, it came as some shock that the NSA had mega nodes on the Internet where they were sucking up all of the unencrypted packet traffic, well, even the encrypted traffic, assuming they could decrypt it in the future.  And that was a little bracing for us.  And so now we're in a place where, thanks to computational capability, the crazy drop in prices of sensors and processing which makes this feasible, that there could be recognition happening pervasively in the public sphere, not just on the Internet.  Frightening.



So I got a kick out of a bit of news of some ad click fraud which was going on in a set of Android apps.  A long time ago, somewhere here or on the TWiT network, we noted the interesting fact, I remember this being discussed, either I was discussing it with you, Leo, or it was being discussed on one of the podcasts, the interesting fact that in this very tightly optimized online advertising LAN, ads clicked by iOS device  users were considered to be more valuable than those clicked by Android users.



LEO:  Yeah, absolutely, because they spend more money.



STEVE:  Exactly.  And so consequently, advertisers were shelling out fractionally more money for iOS clicks than clicks from non-iOS devices.  So it should come as little surprise that, since there's already no honor among thieves, Sophos recently uncovered 25 apps on the Google Play Store.  They're in the Google Play Store, so they're infecting Android platform, but they are spoofing what is already their highly spoofable user-agent headers to lie about the device.



LEO:  Oh, golly.  Of course.



STEVE:  Of course.  Of course.  Whose user was supposedly, though not actually, clicking on web page ads, and of course they are claiming to be various iOS devices rather than the devices they actually are because it's already ad fraud, so why not get paid more for the spoofed click on a ad?  You know?  Of course, why not?  So anyway, I just got a kick out of that.  The users, the Android devices are already blissfully unaware because this is happening offscreen that these clicks are being sent.



LEO:  This is a really valuable click, honest. 



STEVE:  Yeah, exactly.  So it's from somebody on an iOS platform, so pay us more.  Wow.  Anyway, that just was a quickie, but I got a kick out of it.  And we talked about the big Marriott breach as a consequence of their Starwood property acquisition.  It just came out that a Marriott spokesman said the hotel was working on a way to reimburse some of their prior guests the cost of obtaining replacement passports.



LEO:  Oh, my.



STEVE:  Yeah, if they can show that they've been victims of fraudulent operations where the passport number was involved.  It turns out that New York Senator Chuck Schumer publicly called on Marriott over the weekend to pay for people's passport replacement fees.  He said:  "The data breach at Marriott compromised millions of travelers' U.S. passport info, and a new passport costs $110.  So Marriott must personally notify customers at greatest risk, and Marriott should pay the costs of a new passport for victims who request it."



Whereupon Connie Kim, a Marriott spokesperson, wrote in an email to the Washington Post:  "We are setting up a process to work with our guests who believe that they have experienced fraud as a result of their passports being involved in this incident.  If through that process we determine that fraud has taken place, then the company will reimburse guests for the costs associated with getting a new passport."  So there.  Anyway, yeah.  It was a big breach.  And as we know, it happened four years ago.  Marriott acquired Starwood, this collection of Starwood properties about two years ago.  So the breach happened not directly under their control, but maybe they didn't fully vet the properties that they acquired.  Who knows what the back story is.  But it's nice that they're stepping up and being responsible.



Meanwhile, the U.S. Department of Homeland Security hopes to de-privacy-ify, if that's a word, the explicitly private Zcash and Monero currencies.  There's this thing known as a pre-solicitation document which - and I have a link to it here in the show notes for anyone who's curious for all the details.  And it also wants some other interesting technologies, like AI for image recognition of potential explosives in luggage.  So the idea would be you would run luggage through some imaging scanner, and they're wanting to see if they can develop AI to potentially - and I would imagine a human would get brought in, depending upon how certain the AI was that an image might represent something explosive.



But there's a whole bunch of cool stuff in this pre-solicitation document.  I only wanted to talk about the last one on the list of items.  They're looking for the same sort of transaction information for Zcash and Monero that they have been able to obtain for Bitcoin.  As we know, law enforcement has had great success in tracking Bitcoin transactions because, although the Bitcoin system cryptographically protects itself and is secure against external tampering, you know, Bitcoin transactions are not private since they are recorded in a public ledger, but it is tamperproof.



So the whole point of it is there's no way to go back and change the public ledger, but the transactions themselves show publicly available Bitcoin addresses, so everyone can see where the money is going to the level of the Bitcoin address.  Not so with the deliberately privacy enhancing Zcash and Monero currencies.



So this DHS pre-solicitation document begins by explaining:  "The Department of Homeland Security (DHS) Small Business Innovation Research (SBIR) Program, comprised of the Science and Technology (S&T)" - we're big on acronyms here - "Directorate's SBIR Program and the Countering Weapons of Mass Destruction (CWMD) Office SBIR Program" - and remember that's the Small Business Innovation Research - "invites small business concerns to review this pre-solicitation notice, which is intended to lead to the FY19" - that would be Fiscal Year 2019 - "DHS SBIR Phase I solicitation.  This notice is not itself a solicitation or Request for Proposals.  This notice is merely an opportunity for interested parties to comment on, or request information about, the attached topic areas."



And as I said, there are a list of topics.  The last one is what caught my attention.  The last item on the list says "Objective:  Design a product to support the implementation of blockchain-based forensics, data analysis, and information sharing."  And under description they said:  "Blockchain and Distributed Ledger Technology (DLT) are emerging technologies being leveraged for a wide range of commercial and governmental applications.  The most well-known use case would likely be Bitcoin within the newly emerged cryptocurrency arena, which has spurred further interest and developments.  Prior efforts have addressed Bitcoin analytics, which covers only a limited scope within the greater realm of cryptocurrencies.  This proposal seeks applications of blockchain forensic analytics for newer cryptocurrencies such as Zcash and Monero.  And ongoing research within the field also contributes to new technological implementations and technologies that continue to multiply the specific types of consensus, privacy, security, and proof mechanisms."



They said:  "A key feature underlying these newer blockchain platforms that is frequently emphasized is the capability for anonymity and privacy protection.  While these features are desirable, there is similarly a compelling interest in tracing and understanding transactions and actions on the blockchain of an illegal nature.  To that end, this proposal calls for solutions that enable law enforcement investigations to perform forensic analysis on blockchain transactions.  This analysis can be approached in any number of ways and may consider different data situation use cases depending on whether additional data from off-chain sources are available.  Furthermore, with the proliferation of new blockchain variants, the desired solution should either attempt to show generality or extensibility, or at least provide working approaches to treating newer blockchain implementations."



And I had in the show notes here, but I won't go through it, different details.  They break this into three phases:  the design of an analysis ecosystem where they mention Zcash and Monero; phase two, the prototype and demonstration of these forensic technologies designed during phase one; and then phase three for commercial or government applications.  So it doesn't take much reading between the lines to see that law enforcement is a little unhappy that the newer non-Bitcoin so-called distributed ledger technologies are thwarting their ability to see into the transactions to the same degree that they're able to see into Bitcoin transactions.



In the case of Bitcoin and the tracking of those transactions, thanks to the help of the private sector, U.S. law enforcement authorities, for example, were able to determine that 95% of all ransomware ransom payments were cashed out and converted into fiat currency through the BTC-e cryptocurrency exchange.  n international arrest warrant for the owner of this BTC-e portal was issued.  But the U.S. lost the extradition fight with Russia.  So the point is that, in the case of Bitcoin, it is the case that we have the technology, and the government is using forensics tools to, as we know, track where the money is flowing with bitcoin.  They very much want to be able to do the same thing with the next generation of emerging cryptocurrencies such as Zcash and Monero.  But at this point they don't have those tools.  So they would like to have them.



And just I thought that was interesting, that this is now, not surprisingly, on their radar, and an opportunity for some enterprising individuals to see if they can help the government to produce those tools, or produce those tools for the government.  It's always been a little bit of a concern to especially people who are skeptical, that the Tor Project was created by DARPA, the Defense Advanced Research Project Agency, and that its initial funding and its funding for quite a while through its startup was from the U.S. State Department.  It's like, okay.  But on the other hand, its technology has been vetted.  It's open.  The source is open.  It's been scrutinized extremely well.



And so I really do think - we know that the people in the Tor Project, we know that their hearts are in the right places.  And just to step back for any new listeners, what we know is that the Internet, in the same way that it was never designed with secrecy and privacy in mind, thus the underlying protocols do not support encryption, that had to be added on top later.  Similarly, the Internet was never designed with anonymity in mind.  We've talked about it often how the system is inherently a point-to-point system.  Your browser needs to contact Microsoft, so it asks your DNS server for the IP address of Microsoft.com.  And when it gets it, it sends a packet to that IP address.  The source of the packet is your IP so that answers can come back to you.  And the destination is the IP that it got from DNS.



And so what's established over this amazingly heterogeneous network of linked routers is essentially a point-to-point communications link where each end knows the address of the other.  In other words, zero anonymity.  And for what it's worth, anybody looking at packets can also see their source and destination addresses, even when they're encrypted.  When the packet itself contains an envelope whose contents is encrypted and cannot be seen, the addressing information, as we were talking about earlier, the metadata is the source and destination IPs which are out there for everyone to see because the routers have to have it in order to send the data back and forth between the two parties.



So Tor came along as a unique and truly cool question, which is, in the same way that security and privacy has been added on top of the existing Internet, can anonymity be somehow added on top of an existing routing system that doesn't itself support anonymity?  And if anyone is interested, we did a really cool podcast way back in the day on Tor.  It was called "The Onion Router," which is what the initials T-O-R originally stood for, about the technology that solved this problem to a very, very good degree.



Tor added an anonymity layer on top of the Internet such that somebody who very much wanted to communicate with some other entity on the Internet, but very much wanted to protect their identity, was able to choose a bunch of routers that their data would bounce around, Tor routers, before emerging onto the public Internet, and would create this so-called "onion," a series of encapsulations, successive encapsulations of their data such that each router that their data went to could only take the outer wrapper off before forwarding it.  The router would know where the data came from and where the data was going to, but not where it came from before that, and not where it was going to after that as a consequence of the successive encapsulations.  And so after a few hops the actual source of the data is deeply obscured.



And subsequently we've talked about various academic attacks on this Tor network and how that anonymity guarantee or hope can be penetrated.  But it's fundamentally very good, and it takes somebody with a huge amount of resources and money and will in order to sort of even then weakly deanonymize.  And I think the most interesting thing that came out of the research is that it is much easier to prove an assumption about who someone is than it is just to collect data on everybody.  So even in efforts to deanonymize, it's first necessary to make an assumption about who the endpoints belong to and then, with a great deal of effort, it's kind of possible to confirm the assumption.  But even that is a weaker success on breaking anonymity.



Okay.  With all that said, the Tor Project recently published, because they are a nonprofit, they published their tax exemption Form 990 which reveals a lot about the source of their funding.  And the really nice bit of news here is that in 2017, which is what this recently filed form covers, the U.S. support had almost hit 50%.  It was at 51%, with the balancing 49 being sourced, not from the public sector U.S. government, but from various private sector entities or other governments.  The cofounder and the developer of the original software was a guy named Roger Dingledine.  And he said...



LEO:  Really?



STEVE:  Yeah, I know.



LEO:  Poor guy.



STEVE:  Roger Dingledine.  Sorry, Roger.  Anyway, he said, in terms of percentages, while 2015 saw 85% of our funding coming from the U.S. government, 2016 saw the fraction drop to 76%, and 2017 we're down to 51%.  So, I mean, even the fact that Roger's talking about this suggests that he, too, as a representative of the Tor Project, understands that people have always been made a little uncomfortable that the U.S. government is funding all of this.  He said:  "I should take a brief moment to explain how funding proposals work, for those who worry that governments come to us wanting to pay us to do something bad.  There is never any point where someone comes to us and says, 'I'll pay you X to do Y.'  The way it works is that we try to find groups with funding for the general area that we want to work on, and then we go to them with a specific plan for what we'd like to do and how much it would cost for us to do that, and if we're lucky they say okay."



So in 2017, breaking down the funding, was just shy of $800,000, $798,000, came from the U.S. government-backed Radio Free Asia; $635,500 came from the similarly U.S.-backed SRI International;  $594,000 from the Swedish International Development Cooperation Agency, SIDA; $548,000 from the U.S. NSF, the National Science Foundation.  And the State Department, which as we noted financed Tor's initial development and has sustained the project through its first years by covering most of its costs, has been reducing its involvement in Tor funding.  In 2015 it was in for $200,000.  It went up a little bit in 2016 to $218,000.  But in 2017 it was down to $133,000.



So at the same time, last year the Tor Project raised a record $425,709 from its users, which is more than twice the funds it raised from users in 2016.  But so far this year it's looking a little bit bleak at only $95,000 so far.  As I think we've mentioned in the past, Mozilla has been a good partner with Tor.  They've pledged matching funds for user donations.  And overall, Mozilla independently of that has stepped up, boosting its contributions from $24,500 in 2016 to a whopping more than half a million, $522,188 last year.  And they are expected to be a top contributor in 2018, as well.  So yay for the Mozilla Foundation for doing that.  And DuckDuckGo also contributed $25,000 to the Tor Project in 2017, just because they wanted to support that.



It's also, Tor Project has also received in-kind service donations such as free cloud computing, free hosting, volunteer coding, translation services, and legal services, which they estimate for the purposes of their filing at around $806,000.  So all told, the Tor Project's 2014 total revenue was $2.5 million.  It grew to $3.3 in 2015; dropped by 0.1 to $3.2 million in 2016; hit an all-time high of $4.2 million last year in 2017.  So Roger said that Tor's budget, even at the 2017 level, which seems like a lot at $4.2 million, remains modest, considering the number of people involved and the impact they are having.



He said:  "It is dwarfed by the budgets that our adversaries are spending to make the world a more dangerous and less free place."  So for what it's worth, their donation page is donate.torproject.org.  And it's kind of a cool page.  It's live.  And if you watch it for a while, you might see it jump a bit.  I went there and looked at it and happened to see the numbers change.  So somebody had just donated while I was there.  So it is really nice to know that a network designed not to provide anonymity, I mean, it's nice to know that in a network like the Internet, which was designed not to provide anonymity, such a facility like Tor exists and that it really does provide strong anonymity, which as we discussed previously can only be penetrated at great cost and with tremendous resources.  So yay for those guys.



I mentioned an entirely foreseeable disaster at the top of the podcast.  Believe it or not, a recently designed protocol for IoT devices uses UDP with no authentication.  Which pretty much tells you all you need to know, at least our audience, about what is going to happen and in fact has started to.  Leo, I know you jumped on the NTP protocol, the Network Time Protocol, for its use in amplifying DDoS attacks. 



LEO:  Right.



STEVE:  And of course we've previously talked about how DNS can be used.  You make a simple query to a DNS server, and it returns a larger response.  So if you spoof the source IP of your query, then the DNS server's response goes to the IP you spoofed rather than back to you.  Same is true for NTP.  In the case of DNS, it can be restricted so that only the users of the DNS server's local LAN have access.  The same is true in the case of NTP.  The problem is for protocols which are intended to be globally accessible and want to use UDP.  Well, it turns out that there's a new one on the field.  It's called C-O-A-P, CoAP, the Constrained Application Protocol.  It is, self-described, it is at CoAP [C-O-A-P] dot technology, describes itself as a "specialized web transfer protocol for use with constrained nodes and constrained networks, meaning low-cost, low-power devices in the Internet of Things.  The protocol is designed for machine-to-machine applications such as smart energy and building automation."



And as I said, here's the bad news:  It supports the use of unauthenticated UDP packets in a simple query/reply mode, and a very small query can return a much larger, 50 times larger, reply.  So it's a dream come true if it exists publicly on the Internet for DDoSers.  It is essentially, in fact it can be thought of as a compactified HTTP.  The protocol can support DTLS-style certificated-based endpoint authentication and encryption, like TLS over UDP.  But when that's done, it's no longer tiny and simple, nor is it lightweight.  So all that extra goodness has been eschewed in favor of just sending a query and returning a reply.



CoAP is new, having only recently been ratified into a standard four years ago in 2014.  So we would expect uptake to follow the creation of libraries and then its manufacturing and deployment into devices.  So what does Shodan have to say about it?  After several initially quiet years, the number of publicly exposed, accessible, and answering CoAP devices has only in the last few months exploded, actually over the last year, exploded since a little over a year ago, beginning in November of 2017.  It first appeared on the map in November of 2017, so 13 months ago, with about 6,500 devices.  A month later, it was at 26,000 devices.  Then by May of this year that number had grown to 278,000 devices.  And today we're in the somewhere between 580 to 600,000-device range, according to Shodan.



Dennis Rand, who's the founder of eCrimeLabs, who has been monitoring the explosion of this protocol, watching this happen, believes that the reason for this explosion is CoAP's use as part of something known as the QLC Chain, formerly known as Qlink, which is a project aimed at building a decentralized blockchain-based mobile network using WiFi nodes throughout China.  If you look at Shodan for a breakdown of the IPs where CoAP is available, China has 573, almost 574,000 of them.  The next biggest, okay, 574,000, next biggest is the U.S. at less than 4,000, 3,831; Russia, 1,675; Canada, 327; Germany, 193.  So these are virtually all in China.



And this recent rise in readily available and poorly secured CoAP clients, which is to say no security, has not gone unnoticed.  Over the past few weeks, the first DDoS attacks carried out via CoAP have started to appear.  ZDNet reported that a security researcher who deals with DDoS attacks, but who couldn't share his name due to employment agreements, stated that CoAP attacks have occurred occasionally during the past few months, and now with increasing frequency, reaching 55Gb on average.  Remember, this is a factor of 50 amplification attack.  So it is very simple to take compromised routers, bounce traffic off of them using their external UPnP exposure, bounce a little bit of traffic off of them from a spoofed IP, and have that traffic then hit a CoAP device, of which there are now 600,000, and have that device then amplify the traffic by a factor of 50 and send it on to its victim destination.  So, yikes, 55Gb on average.



And one of the CoAP, the largest CoAP attack was seen at 320Gb; 55Gb, to put this in perspective, is an order of magnitude larger than the average size of a normal DDoS attack, which is about 4.6Gb, according to the DDoS mitigation firm Link11.  Of these 580-some CoAP devices currently available on Shodan today, the same researcher told ZDNet that roughly 330,000 of them could be abused, so more than half, to relay and amplify DDoS attacks with an amplification factor of about 46, so roughly 50.  And of course all this was foreseeable.  It turns out the people who were doing this understood that UDP could be spoofed, but they did not build in a lightweight means of preventing that.  What they unfortunately build in was, oh, gee, you could bring up DTLS.



And so there is a security protocol for CoAP, but nobody uses it because it requires - I saw it.  It looked like maybe eight or nine roundtrips.  And basically it's full weight DTLS, TLS on top of UDP protocol.  Which is just sad.  I mean, it takes no rocket science to figure out how to solve the problem.  When I was writing the show notes I thought, well, okay, how would I solve it?  One way would be to have the initiator first ask for permission by sending a small permission request packet to the endpoint it's intending to make a request from.  The recipient would simply encrypt the requesting IP with its own locally unique private key and return that as a permit.  It needs to retain no state.  All it does is there's an incoming request for essentially permission to ask you questions.



When it powered up, it generated a random encryption key.  While it's powered up, it uses that to encrypt any incoming permission requests of the requesting IP and sends that back as a permit.  Then only requests containing the proper permit which match the requesting IP would be replied to.  This would require one extra UDP roundtrip, very lightweight, much lighter weight than certificates, and that extra roundtrip would only be required once per endpoint pair, so the overhead would be minimal.  The recipient never needs to retain state, and only the endpoint asking the questions needs to retain permits for however long it chooses to, and it's able to request a renewed permit if it has decided to discard it.



So again, it's not like these are hard problems to solve.  There's a minimal implementation.  This was very much like the stateless SYN solution that I came up with, which it turns out the industry had, Dan Bernstein had also come up with previously for Linux, which solved the problem of needing to maintain state when setting up a TCP stack.  So again, this stuff is out there, but people are not bothering to take advantage of it.  And as a consequence, now we have a rapidly growing number of new IoT endpoints that will happily multiply attack traffic by a factor of 46, and we're getting much larger DDoS attacks as a result.



So a little bit of, I guess this is errata because it corrects something that I just assumed was still true, but I'm very thankful for it.  We talked last week about this debacle that Sennheiser has gotten themselves into, where they were found installing a single common root certificate in their HeadSetup application, very much the way Lenovo did back in the - what was it?  I'm blanking on the name of the Lenovo mess.



LEO:  Oh, yeah.  Chatroom [crosstalk].



STEVE:  Somebody, yeah.  Anyway, I got email from - so these were the guys at Secorvo who found the Sennheiser root CA.  And I got email from Andre Domnick.



LEO:  Superfish.



STEVE:  Superfish, yeah, the Lenovo Superfish debacle.



LEO:  Thank you.  [Vetman] wins.  He was the first.



STEVE:  Thank you.



LEO:  Then [Chickenhead].



STEVE:  So Andre Domnick of Secorvo send me a note saying:  "Hey, Steve.  Thanks for mentioning our small finding" - which nobody thinks was small.  It was very cool.  He says:  "But just a short comment on the Chrome stuff.  It is often mentioned in the media that Google Chrome is performing certificate pinning for the Google domains.  But Google apparently has backed away from the idea.  In our report you can find a picture of the current Google Chrome accepting the certificate.  Additionally, we did not detect any unusual behavior of Chrome when presenting our cert to the browser."



So we can't take that as definitive, but it's interesting because I've been saying on the podcast every time it comes up that the Google browsers know, they have the fingerprints of the Google certificates.  In fact, we know that it at least used to be true because this is how fraudulent certificates have been found, have been spotted in the wild in the past, was that Chrome said, hey, that's not from Google.  So I just wanted to thank you, Andre, for the correction, and we'll see if we get any more information about that because maybe Google did, I mean, I've wondered how they could pin all their certificates because they're creating them like crazy and minting them on the fly.  And it seems difficult to do that.  But maybe they've backed away from that.  I don't know.  But I did want to mention that Andre says maybe we can't be counting on that.



Greg K. in Columbus, Ohio, regarding CA certificates, he says:  "I'm fine with your rants about adding certs to trusted CA stores and middleboxes.  However, I'm an advanced user.  If I want to use my own internal root CA or my own middlebox to knowingly scan my own web traffic, I should be able to do that.  I should not be restricted from adding whatever I want to my trusted cert stores on my network through group policy or whatever means.  So I agree with your comments, but I am opposed to restricting the ability to do these things.  I know what I'm doing, but others do not.  So please be careful and responsible when you call for banning these activities to those individuals or organizations that can do it responsibly.  I know you didn't exactly do that, but I would hope that you are being careful with your opinions."



And I'll just say, okay, I did mention that I also have my own root certificate.  I've got one for this machine and one for localhost, which I discovered when I ran the Mark Russinovich or Sysinternals test against the root stores to make sure that nothing creepy had crawled into my root store.  So Greg, I take your point.  And I guess what I would opt for would be something like the dark screen, whoa, hold on, what you're about to do is installing a root certificate into your store.  Are you sure you want to do this?  That doesn't exist, a UAC-style "whoa" dialogue.  That doesn't exist right now.  And as a consequence, these things are just able to slip in without our knowledge.  So I think it's necessary for some control to be provided.  But I agree with you.  An expert end user still needs to have the ability to do this.



Craig in Edinburgh said - his subject was "Self-signed cert surprise all the way from Azeroth."  So I guess that's got to be a location in Edinburgh.  [Reference is to WoW world.]



LEO:  Edinburgh.



STEVE:  Edinburgh, oh, okay.



LEO:  They don't say the "gh."  You've got to say Edinburgh.



STEVE:  And he actually did say, I'm not kidding, after this, "No accent, please, Leo."



LEO:  Oh, no.  Well, I already blew it.  I already blew it.



STEVE:  He said:  "Given your audience, I bet I'm not alone in alerting you to this, but thought it was worth flagging up.  After you mentioned Sigcheck" - that's the Sysinternals tool - "I ran it, expecting to be clean, as I don't install much, and was surprised to find one cert listed from Blizzard for Battle.net."



LEO:  Uh-huh.  Uh-huh.



STEVE:  He said:  "Yes, I'm a longtime Warcraft addict.  I could quit anytime I like, honest."  He says:  "I wondered what it was doing there and came across this post from a Blizzard employee."  And I have the link in the show notes.  I'll read it in a second.  He says:  "I don't think it's anything to really worry about as the last line says the desktop app generates a self-signed certificate that's unique to your machine and configures your system to trust it."



LEO:  Uh-oh.



STEVE:  "So it's not the same as the Lenovo screw-up, but I had no idea it was there until today.  It probably ought to be, at the very least, opt-in; don't you think?"



Okay.  So Blizzard posted:  "Our recent update to the Blizzard Battle.net desktop app made sure players could properly use features like logging into Battle.net via a social network, or joining a Blizzard group via an invite link.  To facilitate these features, we updated the local web server to use a self-signed certificate to be consistent with current industry security standards.



"For those interested in more detail, using these features requires your web browser to communicate with the Blizzard Battle.net desktop app.  Previously, the desktop app used a certificate signed by a public Certificate Authority, meaning that no modifications to your system certificates were necessary; however, this technique is incompatible with Certificate Authority policies, and we can no longer use it."



So, first of all, yay to Blizzard for not continuing to do something that is not safe because that would have required that their server running in the desktop had a private key for the certificate that they're using, and that's a big no-no.



Then they said:  "While some browsers such as Chrome and Firefox are equipped to handle browser-to-app communication techniques, the changes were necessary for other browsers.  For the time being, the desktop app generates a self-signed certificate that's unique to your machine and configures your system to trust it."  And that is exactly the way it should be done.  So again, bravo to Blizzard.



What unfortunately Superfish did and what, amazingly, Sennheiser repeated, was using a single certificate globally, such that in the case of back when Lenovo made this mistake with the Superfish software, which was an add-on, there was only the Superfish system installed a certificate system that was common to all Lenovo users who had that Superfish software, which then allowed spoofing of any site to all of those users.  Same thing for Sennheiser.  But a per-system certificate - and what I said last week, the mistake that Sennheiser made was not creating a per-system cert, which their installer could easily have done, which Blizzard did.  So Blizzard did what they wanted to do correctly.  So, yay.  And Craig, thanks for sharing that experience and example.



Okay.  So an important lesson and teachable moment, thanks to more troubles with Google+.  David Thacker, VP, Product Management of G Suite, posted yesterday under the title "Expediting Changes to Google+."  And I will quickly share what he wrote.



He said:  "In October we announced that we'd be sunsetting the consumer" - the consumer, that's one key word - "version of Google+ and its APIs because of the significant challenges involved in maintaining a successful product that meets consumers' expectations, as well as the platform's low usage.  We've recently determined that some users were impacted by a software update introduced in November that contained a bug affecting a Google+ API."  And of course our listeners will remember that we covered a different exactly similar problem not that long ago.



He says:  "We discovered this bug as part of our standard and ongoing testing procedures and fixed it within a week of it being introduced."  So the point I'll get back to later is that they have something called "standard and ongoing testing procedures" which brought this to light.  And that's significant.  He says:  "No third party compromised our systems, and we have no evidence that the app developers that inadvertently had access for six days were aware of it or misused it in any way.



"With the discovery of this bug, we have decided to expedite the shutdown of all Google+ APIs.  This will occur within the next 90 days."  Which essentially accelerates this by about half a year.  He said:  "In addition, we have also decided to accelerate the sunsetting of consumer Google+ from August 2019 to April 2019.  While we recognize there are implications for developers, we want to ensure the protection of our users."



Under "details about the bug and our investigation," he wrote:  "Our testing revealed that a Google+ API was not operating as intended.  We fixed the bug promptly and began an investigation into the issue.  Our investigation into the impact of the bug is ongoing, but here is what we have learned so far:  We have confirmed that the bug impacted approximately 52.5 million users in connection with a Google+ API.  With respect to this API, apps that requested permission to view profile information that a user had added to their Google+ profile like their name, email address, occupation, age, and so on were granted permission to view profile information about that user even when not set to public."  So it was a glitch in this "should this be public or not," and the upshot was it published information that the user had put in, but had marked non-public.



"In addition, apps with access to a user's Google+ profile data also had access to the profile data that had been shared with the consenting user by another Google+ user, but that was not shared publicly."  So again, another publication mistake.  "The bug did not give developers access to information such as financial data, national identification numbers, passwords, or similar data typically used for fraud or identity theft.  No third party compromised our systems, and we have no evidence that the developers who inadvertently had access to this for six days were aware of it or misused it.  We have begun the process of notifying consumer users and enterprise customers that were impacted by this bug.  Our investigation is ongoing as to any potential impact to other Google+ APIs."



"So," they said, "we will sunset all Google+ APIs in the next 90 days.  Developers can expect" - so basically this is the last straw.  They're like, nobody is using this.  All it represents is the potential for exposure.  We've already said we're going to shut this down.  Nobody cares.  We're just going to do it sooner.  And probably after the initial announcement, and everyone just yawned, they said, oh, maybe we could have done it  a lot sooner.



"So," they said, "developers can expect to hear more from us on this topic in the coming days and can stay informed by continuing to check the Google+ developer page."  They said:  "We have also decided to accelerate sunsetting consumer Google+, bringing it forward from August to April 2019.  We want to give users ample opportunity to transition off of consumer Google+; and, over the coming months, we will continue to provide users with additional information, including ways they can safely and securely download and migrate their data."  And they had a note in here which I'll skip to enterprise customers saying, don't worry, this is not going for you.



The importance to me of this phrase:  "We discovered this bug as part of our standard and ongoing testing procedures and fixed it within a week of it being introduced."  I want to formally congratulate Google for this, and I think it brings up an extremely and increasingly important issue.  If a bug exists or is introduced at some point, it can subsequently be discovered, and history is showing us will eventually be discovered by any one or more of four possible entities.



It could be found by a malicious black hat hacker who leverages it to damage the company who created the software and/or their users, or customers who have trusted the company's offerings to be safe to use.  As we've recently seen several times, it might also be discovered by a self-aggrandizing gray hat who does not themselves leverage the discovered vulnerability, but who discloses the bug's existence irresponsibly to both embarrass the company and endanger their users or customers until such time as it can get fixed.



And a perfect example are the three recent zero-days which we've seen tweeted.  Or the bug might be discovered by a security researching white hat who responsibly discloses their discovery and quietly notifies the company to the problem so that it can be repaired.  Or, finally, the bug might be discovered by the company's own employees, who can, as Google just has, immediately repair the oversight, determine the nature and extent of its impact on their users or customers' exposure, and notify them proactively of any possible consequences.  While inadvertently outsourcing the discovery of unknown bugs to third-party security researchers is often what happens, it also often happens that those discoveries are made by those whose hats are not white.



So it's a gamble.  And unfortunately it's a gamble that many companies make because they do not, cannot, or choose not to build in processes to find those bugs themselves.  We know that those who create a system that's intended to be secure and bug free are the least able to attack it and find fault in their own work.  I mean, all of our experience demonstrates that.  So some kind of internal red team facility needs to be created.  It could be a separate group within the company who pride themselves on punching holes in the company's own offerings, or an automated system which continually scans, fuzzes, and searches for defects and odd behavior.



The point is the establishment and maintenance of any such proactive vulnerability discovery system is not free, and the value it returns to the company can be difficult to justify.  One of the lessons we have ample opportunity to learn and relearn on this podcast is that security is not free.  And companies do not make headlines for not having serious security incidents.  They only make headlines when their security fails.  So I want to salute Google for clearly having such processes in place, which are working, and often find problems themselves rather than relying even inadvertently for them to be discovered by others.  We learned from Paul and Mary Jo that Microsoft had much more than ample feedback from users of their most recent Windows 10 feature update disaster, which was warning of every problem that later surfaced, at great cost and embarrassment to them and their users.



So in this instance Microsoft had such a system, but it was there apparently pro forma, just in name.  They weren't actually looking at any of the feedback and the data that it was producing.  So you also have to heed what the system you have put in place is producing.  And we also know that they understand that now.  Hopefully their behavior will change.  And I was reminded of this way back 30 years ago when SpinRite was young, and I had an entire bullpen of technical support people on the phone with customers.  We learned that there's no such thing as a one-off.  Anything that happens to one user will happen to others.  But that's a lesson that takes some time to sink in.  And it's not good news.



So some people in some organizations prefer not to believe it.  They think, oh, well, we don't know why that happened.  It probably won't happen to anybody else.  No.  Given enough time and enough people, it will.  So you have to pay attention to every one of them.  So anyway, props to Google.  In this regard, I think they are a company worthy of emulation.  And I really do hope that, as the technical press continues to note the scope and the cost of these breaches, that companies understand that it is possible for them to find them themselves, rather than to rely on outsourcing it to, in many cases, black hats who can also hurt their users.  So there.



LEO:  So there.  Is that it?



STEVE:  That's it.



LEO:  That's all we're going to say?  You took me by surprise.  Thank you, my friend.  Steve Gibson comes to us courtesy of the Gibson Research Corporation, providers of the finest hard drive recovery and maintenance utility known to man, a.k.a. SpinRite.  Go to GRC.com.  Get your copy.  And while you're there you might check out all the other great stuff Steve offers, including this show - not only audio of the show, but also transcripts.  It's the only place you can get those, and they're searchable, which makes it really easy to go through all 693 episodes, find the topic you're interested in, going back all the way to Honey Monkeys, honeypots and all of that, 13 years ago.



You can also get audio and video from our site, TWiT.tv/sn.  And then I think the best thing to do is start your collection by subscribing to Security Now!, getting every episode the minute it's available on a Tuesday evening.  You can collect all 693, or 694 next week.  I will be back, so will Steve, as we are most Tuesdays about 1:30 Pacific.  We were late today, I apologize, but usually around 1:30 Pacific.  That's 4:30 Eastern, 21:30 UTC.  If you want to watch live, go to TWiT.tv/live.  Chat with us in the chatroom at irc.twit.tv.  That's all the people watching live.  So that way you can, when Steve and I say, "What was the name of that [Lenovo] rootkit?" you can say, "Superfish, Superfish, Superfish."  Patrick Delahanty says:  "Collect 'em and trade 'em with friends."  Got to get them all.  Steve, we will see you next week.  And I thank you so much for another great show.



STEVE:  Thank you, buddy.  That will be the last podcast of the year since we have two weeks off, since Tuesday is when Christmas Day and New Year's Day fall.  So I'll try not to forget how to do them in that time.



LEO:  I doubt you will.  I might.



STEVE:  I was joking with Lorrie.  She said the same.  She says, "I don't think you're going to forget."



LEO:  Thanks, Steve.



STEVE:  Okay.



LEO:  See you next time.



STEVE:  Right-o.  Bye.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#694

DATE:		December 18, 2018

TITLE:		The SQLite RCE Flaw

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-694.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we look at Rhode Island's response to Google's recent API flaw; Signal's response to Australia's anti-encryption legislation, the return of PewDiePie; U.S. border agents retaining travelers' private data; This Week in Android hijinks; confusion surrounding the Windows v5 release; another Facebook API mistake; and the eighth annual most common passwords list, a.k.a. "How's monkey doing?"  Why all might not be lost if someone is hit with drive-encrypting malware; Microsoft's recent four-month run of zero-day vulnerability patches; the Firefox 64 update; a reminder of an awesome train game for iOS, Mac, and Android; some closing-the-loop feedback with our listeners; and a look at a new and very troubling flaw discovered in the massively widespread SQLite library, and what we can do.  



SHOW TEASE:  It's time for Security Now!, our last episode of 2018.  Wow, what a year it has been, and we're not done yet.  Microsoft says this was the worst year for zero days, and there are more coming in Windows.  A flaw that potentially could be disastrous in a software component almost everybody has on every machine you have.  And of course a Picture of the Day, just for the season.  Plus I might sing a song.  Don't be afraid.  It's Security Now!, next.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 694, recorded Tuesday, December 18th, 2018:  The SQLite RCE Flaw.



It's time for Security Now!, the show where we protect you, your privacy, your loved ones, and your security online with the help of the man in charge over here, Steve Gibson of the GRC, Gibson Research Corporation, at GRC.com.  Hello, Steve.



STEVE GIBSON:  I'm not sure what I'm doing.  Am I doing a Vulcan salute, or am I shooting myself in the head with my first two fingers?  I'm not quite...



LEO:  It's kind of crazy, yeah.



STEVE:  Just not really sure.



LEO:  Last show of 2018, Steven.



STEVE:  It is indeed.  Yes, and this is 694, so we'll be into the 700s pretty soon, the 700 club.



LEO:  Wow.



STEVE:  Wait, what?  Anyway...



LEO:  Huh?  Well, and as I pointed out, because you have so widely pointed out that your last show will be 999...



STEVE:  Oh, well, I wrote that in - it's been written in assembly language, Leo.  I have no flexibility there.  I've only got...



LEO:  I think it can only do three digits.



STEVE:  I have a three-digit format string, and it's just, you know, when it hits 999, it's like, oh, well, it's been...



LEO:  I, as many, feel it should go to AAA, but that's another - or A00.  But that's another matter entirely.



STEVE:  Been nice knowing you.



LEO:  I feel like people are going to start counting backwards, like this is going to be, like, once we get to 700, people are going to say, oh, there's only 299 shows left, only 298, 297.



STEVE:  Yup, bottles of beer on the wall.  So we'll...



LEO:  It'll be the countdown, yeah.



STEVE:  So while I was assembling today's news, I got to one of the stories that I had run across and dug into it a bit more.  And I thought, ooh, this is really not good.  There is a, well, the absolute number one most popular embedded database is SQLite, you know, SQLite.  And, I mean, it's everywhere.  It's in - I meant to get some...



LEO:  It's in iOS.  It's in every Mac.  I don't know if it's on Windows, but it's everywhere, you're right.



STEVE:  Well, yeah.  And what happens is that apps bring it with them.  I found four instances in my own system.  We'll be talking about this at the end of the show.  It's the topic, the main title.  Well, the title of the show is "The SQLite RCE Flaw."  A remote code execution flaw was found that allows execution of an attacker-provided code if they have any way to get access to a SQLite command.  And, for example, browsers allow that.  Until Google was notified of this, you could crash  Chrome and take over Chrome.  Chrome uses SQLite.  It turns out that it's pervasive.  The Chromium engine was using it.  But it is, I mean, well, we'll get there.



So I think a really interesting and important way to ring out this new year which has been, you know, that has wrung us out.  You and I were saying before we began recording that, without question, the number one main issue that we dealt with in 2018, and it started right off the top, it was like the first podcast of 2018, was the Spectre and Meltdown problems, the idea that the industry learned that the fundamental architecture of our processors was allowing cross-thread, cross-core, basically interprocess leakage of information, which is, you know, it dominated the news this year.



But there's a lot of stuff to talk about.  We're going to take a look at Rhode Island's response to Google's recent announcement of their second API flaw; Signal's response to Australia's anti-encryption legislation; the return, believe it or not, you were fortunately in New York, Leo, when Jason and I had to deal with PewDiePie.



LEO:  Oh, geez.  What does he have to do with security?



STEVE:  I know.  You're going to find out.



LEO:  Oh, boy.



STEVE:  The return of PewDiePie we have.  We also have news that U.S. border agents - and this really does get me - are retaining travelers' private data.



LEO:  Oh.



STEVE:  Not just, yes, not just like checking it while you're crossing, but they're not performing post-transit elimination of the data.  We also have This Week in Android Hijinks with another interesting exploit for Android and a little bit of a takeaway there.  Some confusion that I saw in the industry surrounding the recent WordPress 5 release.  We have another Facebook API mistake that I know you've been talking about, this photo API.  We have, oh, and Leo, the eighth annual most common passwords list, also known as "How's monkey doing?"



LEO:  Yeah, really.  True.



STEVE:  We also have why all might not be lost if someone is hit with drive-encrypting malware.  The observation on Microsoft's recent four-month run of zero-day vulnerability patches.  Every single of the last September, October, November, December, they were patching things that were found in the wild, so that's an interesting bit of news.  We've also got the move of Firefox to 64 with some new features.  A reminder of an awesome train game that I fell in love with.  You did, too.



LEO:  I remember, yeah, yeah.



STEVE:  Many years ago.  Available on iOS, Mac, and Android; and I was recently reminded of it.  Also we have a little bit of closing-the-loop feedback with our listeners, including a SpinRite song for the holidays.  And then we're going to dip into this SQLite Remote Code Execution flaw, and with a takeaway for our listeners.  It turns out that it's probably worth doing a bit of an audit on your system to find the instances of SQLite and see if there are updates available.  The danger is purely a function of attack surface.



And so, for example, apparently Thunderbird uses it, which is the email client that I'm using.  And of course email is all attack surface.  It's just one big exposed attack surface.  So that's not good.  But it turns out it's pretty simple to find the instances of it.  And I think people will be surprised how prevalent it is.  I mean, the network monitoring app, the little NetWorx thing that I use has SQLite in it.  Like, okay, why?  Who knows.  But anyway, lots of fun stuff to talk about, and a fun Picture of the Week also for the holidays.



LEO:  You do Pictures of the Week but today I'm going to do a Mug of the Week.  This was sent to me by Spencer [Dudzik].  Actually his dad is a big fan of this show and iOS Today and Know How, and his sons watch.  Spencer really liked a joke that you told on the show and sent us a mug.  Well, he sent it to me.  I'll send you one if you want, too.  Remember this one?  You can't use "beef stew" as a password because it isn't "stroganoff."  Oh, ow, oh.  So Spencer, thank you.  I really appreciate that.  It's got a nice green inside, so I'll be using it for my green tea.



STEVE:  Much as you might want to forget that one, Leo, now...



LEO:  Every day I'll see...



STEVE:  Every time you bring it up to your face.



LEO:  You can't use beef stew as a password because it wasn't stroganoff.  I blame you.  I blame you.  It's your fault.



STEVE:  I do like a good stroganoff, though, Leo.



LEO:  Oh, Stroganoff's fantastic.  Just don't use it as a password.



STEVE:  No.  So I've been sitting on this Picture of the Week for quite a while.  It's just fun.



LEO:  It's festive for the holiday season.



STEVE:  Apropos of, yes, the holidays.  This is just a little ditty familiar to people who are familiar with Christmas songs.  This says:  "He's making a list.  He's checking it twice.  He's gonna find out who's naughty or nice.  Santa Claus is in contravention of Article 4 of the General Data Protection Regulation (EU) 2016/679."  Bad Santa.



LEO:  Bad Santa.



STEVE:  You've got to, if you're making a list...



LEO:  Four percent of annual revenue out the door, Santa.



STEVE:  He needs to be careful about data retention.



LEO:  Yes.



STEVE:  And, you know, be careful with what information he does about who's naughty and nice because that could be abused if it fell into the wrong hands.



LEO:  So funny.  So funny.



STEVE:  You never know.  So we're going to have to have some sort of new protocol for Santa.



The Rhode Island government - when I saw this, I thought, huh?  What?  Rhode Island government sues Google after the latest Google+ API leak.



LEO:  Oh, boy.



STEVE:  And I'm thinking, okay.  So clearly not everyone was as happy as I was over Google's conduct in handling their most recent API leak.  Of course it was...



LEO:  And shutting down Google+ even sooner as a result.



STEVE:  Yes.  It was the topic of last week's podcast.  It's like, bravo, Google.  Anyway, within a day of this announcement of the second leak, a class action lawsuit was filed by the Employees Retirement System of Rhode Island (ERSRI), a government-owned investment fund that provides retirement, disability, survivor, and death benefits to state and municipal employees and public school teachers.  So I'm thinking, okay.  So they're suing Google over an API flaw where there's no evidence that any information was stolen.  Because remember Google said, as far as we know, you know, we've audited this.  There was no indication that anything happened.  But we caught it quickly, we fixed it, thank you very much.  So how was anyone harmed?



Well, this ERSRI accuses Google of intentionally misleading shareholders and federal regulators by failing to disclose its Google+ data leaks in a timely fashion.  And although the lawsuit was filed immediately following the second leak report, it also cites both this most recent and the October API incident that we covered at the time.  So remember we talked about it in October that Google was kind of caught in the wringer because, as I recall, I think it was The Wall Street Journal who broke the news that there was a problem that Google had chosen not to go public with, and so that was a bit controversial.  Anyway, the second incident was announced Monday, a little over a week ago, eight days ago, when Google said that - wait, no.  Was it a week ago or yesterday?



LEO:  It might have been a week ago.  Yeah.



STEVE:  Yeah, yeah, yeah, when Google said that this API update that they had performed, or an API update had introduced another potential for data leakage which the company patched within a week.  Google said that this second leak wasn't abused to harvest user data.  But if someone did, the data of 52.5 million users was exposed.  So not good, but they fixed it.  So again, okay, how is this grounds for a lawsuit?



The ERSRI officials  are claiming that Google's recent string of leaks and supposedly delayed disclosures have harmed Google's own company stock value, which has in turn incurred losses to investors such as itself.



LEO:  No, no, no.



STEVE:  It's like, really?



LEO:  Really?



STEVE:  Really?  So you're going to...



LEO:  Really?  Sue Equifax.  Sue Marriott.  Sue Yahoo.



STEVE:  Exactly, somebody who, like, really deserves a good kick.  Okay.  So Rhode Island's general treasurer, Seth Magaziner, said:  "Google had an obligation to tell its users and investors that private information wasn't being protected. Instead, Google executives decided to hide the breaches from its users and continued to mislead investors and federal regulators. This is an unconscionable violation of public trust by Google, and we are seeking financial restitution on behalf of the Rhode Island...."



LEO:  It's not a bad idea to establish a precedent that forces companies to be open about this stuff.



STEVE:  Well, so I wanted to bring it up and focus our attention on it a bit because, as we know, responsible and timely disclosures of mistakes such as this second one by Google, which they caught and killed quickly, then disclosed, is good for our ecosystem.  So I'm a little worried that, if we're starting to establish an "admit culpability and be sued for it" precedent...



LEO:  Oh, that will put it back under the rock.



STEVE:  Yes.  And it's finally coming out from under the rock where it's been.



LEO:  Right, right.



STEVE:  So if companies start being sued the instant they acknowledge that they made and fixed a mistake, where nobody was hurt, over the reputation damage their own responsible disclosure incurs, then it's foreseeable.



LEO:  That's a very good point.



STEVE:  That companies are more like, I mean, they're going to say, hey, wait a minute, we don't want to expose ourselves to a class action lawsuit.  And of course you and I feel the same way about class action lawsuits.  The plaintiffs get nothing, and the attorneys - I mean, clearly, if this thing launched a day after, it had already been written.  Somebody was ready to just fill in the blank, "Google+," and launch this thing.  So anyway, I just wanted to, when I realized what the suit was about, I thought, oh, this could have a chilling effect.  And we don't...



LEO:  So in effect they wanted them to keep it secret because it hurt the stock price.



STEVE:  Yes.



LEO:  Huh.  Huh.



STEVE:  Yes.



LEO:  Geez, Louise.



STEVE:  Yes.  So I guess the only way to resolve this ultimately is maybe eventually, because mistakes are going to happen, is for the industry to mature so that it's understood that fixing and acknowledging an error should not cause reputation damage.  It's the reason for last week's podcast celebrating Google's conduct.



LEO:  Companies should be indemnified against liability for reporting flaws.  This is good.



STEVE:  Yeah, yeah.  Meanwhile, we have - I love the title of this post.  This was from Joshua Lund, who is a sysadmin, programmer, privacy enthusiastic, security fan, writer, occasional cyclist, and one of the Signal developers, who posted at Signal.org last week.  The title of his posting was "Setback in the Outback."  And I lightly edited what he wrote for the podcast.



He said:  "Like many others, we have been following the latest developments in Australia related to the Assistance and Access bill with a growing sense of frustration.  The widespread adoption of strong cryptography and end-to-end encryption has given people around the world the ability to protect their personal information and communicate securely.  Life is increasingly lived online, and the everyday actions of billions of people depend on this foundation remaining strong.  Attempting," he writes, "to roll back the clock on security improvements which have massively benefited Australia and the entire global community is a disappointing development.



"More than eight years have passed since we released the public beta of what is now known as Signal.  Throughout the entire development process, the project has faced resistance from people who struggle to understand end-to-end encryption or who seek to weaken its effects.  This is not a new dynamic."  He says, in a paragraph by itself:  "We can't include a backdoor in Signal, but that isn't a new dynamic either.



"By design, Signal does not have a record of your contacts, your social graph, conversation list, location, user avatar, user profile name, group memberships, group titles, or group avatars.  The end-to-end encrypted contents of every message and voice/video call are protected by keys that are entirely inaccessible to us.  In most cases now we don't even have access to who is messaging whom.



"Everything we do is open source, and anyone is free to verify or examine the code for each release.  Reproducible builds" - which is not easy, by the way - "and other readily accessible binary comparisons make it possible to ensure the code we distribute is what is actually running on users' devices.  People often use Signal to share secrets with their friends, but we can't hide secrets in our software.



"Everyone benefits from these design choices, including Australian politicians.  For instance, it has been widely reported that Malcolm Turnbull, the 29th Prime Minister of Australia, is a Signal user.  He isn't alone.  Members of government everywhere use Signal.  Even if we disagree with Christian Porter, we would never be able to access his Signal messages, regardless of whether the request comes from his own government or any other government.



"Although we can't include a backdoor in Signal, the Australian government could attempt to block the service or restrict access to the app itself.  Historically, this strategy hasn't worked very well.  Whenever services get blocked, users quickly adopt VPNs or other network obfuscation techniques to route around the restrictions.  If a country decided to apply pressure on Apple or Google to remove certain apps from their regional stores, switching to a different region is trivial on both Android and iOS, and popular apps are widely mirrored across the Internet.  Some of them can even be downloaded directly from their official website.



"One of the myriad ways that the 'Assistance and Access' [in quotes] bill is particularly terrible," he writes, "lies in its potential to isolate Australians from the services that they depend on and use every day.  Over time, users may find that a growing number of apps no longer behave as expected.  New apps might never launch in Australia at all."



He finishes:  "Technology organizations looking to open offices in a new country could decide that AEST (Australian Eastern Standard Time) isn't such a great time zone after all.  As remote work continues to become more prevalent, will companies start saying 'goodbye,'" he writes, "instead of 'g'day' to applicants from Australia, who are unable to sufficiently secure and encrypt their corporate communications?"  He says:  "This doesn't seem like smart politics, but nothing about this bill seems particularly smart.  We remain committed to fighting mass surveillance worldwide.  We encourage users in Australia to reach out to their representatives and express their opposition to the Assistance and Access Bill."  And it's interesting.  I looked around for other reaction to that, and haven't so far found anything.



LEO:  1Password posted a blog post.  ProtonMail posted a blog post.



STEVE:  Good.



LEO:  So a number of people have responded.  I hope LastPass will say something.  But it does raise some questions in my mind.  So I have some questions for you.



STEVE:  Okay.



LEO:  About this law, the Triple A law, which hasn't yet gone into effect.  I think it doesn't go into effect till next year.



STEVE:  Until Parliament meets at the beginning of 2019, yes.



LEO:  And so there's some chance that maybe they'll change it or retract it.  And we don't know how strongly, assiduously it'll be enforced.  But it says, essentially, that anybody who provides encrypted services must be able to provide unencrypted cleartext versions for law enforcement if they ask.



STEVE:  Yup.



LEO:  Which means Signal provides encrypted services.  ProtonMail provides encrypted services.  LastPass and 1Password provide encrypted services.  It sounds like they'd have to, you know, LastPass, we just did the ad, says we don't ever have access to your vault.  Only you do.



STEVE:  We absolutely know for a fact that they don't.



LEO:  Yeah.  It's trust, what you call "trust no one," end to end.  Or another way to put it is end-to-end encryption.  So the question is, does that mean they have to then modify their code to sell it in Australia, so that when requested they can provide cleartext?  And it's my understanding that it does.



STEVE:  So one interesting thing that I - because I've been thinking about this for the last week.  One of the ways the world has evolved is that applications no longer talk to the hardware directly.  They talk to an API which the operating system publishes.  So it could be, you know, we've often talked about this idea of accessing communications either before they're encrypted or after they're decrypted.  Well, there is a common place where the keyboard API exists.  The video API, the screen output API, and that's this OS layer.



And so it could be, I mean, I don't know, we don't know how this is going to evolve.  But it could be that Android and iOS themselves could provide a pre-encryption and post-decryption interface because, after all, as we've often said, I mean, the user is entering plaintext into their keyboard, and they are viewing plaintext on their screen.  That plaintext transits from the encrypted tunnel through the operating system after it's been decrypted.



So what we may end up with is a general design which would - the advantage, first of all, it means that our phones do have this - our operating systems have a designed-in monitoring facility.  It isn't a backdoor into any of the encryption.  It doesn't weaken any of the encryption.  It just gets to it before it's been encrypted or decrypted, much like the user, typing on their keyboard, like a keystroke monitor.  And so that's a means by which a universal solution could be found.



LEO:  With the cooperation of Google and Apple.



STEVE:  With the cooperation of the operating system.



LEO:  Actually, it wouldn't even be Google.  It would have to be the manufacturer because Samsung would have to say, okay, we'll do that.  Yeah.



STEVE:  Right, right.



LEO:  And what we don't know is, yes, that would be a solution, but that would require Australian law enforcement to know and do it, as opposed to going to Signal and saying, no, you've got to do it.



STEVE:  Well, and then there's also the question of storage; right?  Because, for example, we know in a point-to-point system there is some storage of received messages at the receiving end.  But there's no central storage of prior messages by the provider.  I mean, basically, Signal is providing a system that allows two people to interchange messages securely.  The only storage occurs in the message stream at each end.  So, for example, there doesn't exist, it's not like they don't want to provide it in the case of Signal's being able to provide past message traffic.  It doesn't exist anywhere in a third location.  So, as I said, I really think...



LEO:  But wouldn't they have to - couldn't they be forced to rewrite Signal to provide that?



STEVE:  Well, yes.  And what was...



LEO:  That's the question.  And what are the penalties?  We don't know what that is.



STEVE:  And we sort of heard Signal's...



LEO:  We just pull out of Australia.  We wouldn't do it.



STEVE:  Exactly.  If you guys don't want to allow security, we're about security.  We are not going to compromise our security.  If you don't want access to Signal, I mean, if the only way you'll allow us to be there is breaking encryption, we're just not...



LEO:  Bye-bye.



STEVE:  We're just going to say no.



LEO:  Apple wrote a seven-page letter in October to the Australian government, saying it raises cybersecurity concerns, gives the states power to abuse users' privacy.  "We encourage the government to stand by their stated intention not to weaken encryption or compel providers to build systemic weaknesses into their products."  That's always been Apple's point of view with the FBI and others.



So my email provider - it's a little different within Proton Mail because ProtonMail promises end-to-end encryption.  My email provider, FastMail, is in Australia.  And it worries me that they may be somehow compelled using this to - the thing is, none of my email is encrypted.  It's in plaintext.  So they wouldn't be doing anything.  And if I want it encrypted, I don't use them, I use PGP to do it.



STEVE:  Right.  So you know it's been five years since the summer of 2013 when Edward Snowden surprised us all with what we now have as an awareness of, for example, just how much the NSA's breadth and depth of technological surveillance was in place.  And, I mean, it's almost kind of quaint now, Leo, when we think of going to a coffee shop and using Firesheep to intercept a decrypted browser session and impersonate someone.  I mean, you can't do that anymore.  A lot has changed in, I mean, I would argue our, especially this podcast's, our world has changed in these five years.  Encryption has now become, I mean, it is now in place.  Back then you only encrypted to log on, just to hide the username and password, and then you dropped back to an unencrypted connection, which is the way Firesheep was able to grab the browser session cookie across all these different services.



So I really think that - here we are at the end of 2018.  I suspect that 2019 and 2020 are going to be the years where the slow-to-respond governments, I mean, the governments, as we've been covering, they're not happy with this going dark problem, that they're unable to any longer see into what's going on when they need to.  We had the famous fight between the FBI and Apple that we talked about last week with the San Bernardino terrorist when they were unable to get into the iPhone with Apple's help.  So, boy, I just think this is really interesting, the next couple years.  I think this is going to get settled one way or the other.



LEO:  Yeah, hope so.



STEVE:  Yeah, because countries don't want...



LEO:  If a law like this is passed in the U.S., that makes it really challenging because companies might be able to walk away from Australia, but they can't walk away from every nation.



STEVE:  No, no.  And I mentioned Google.  When China said we need you to filter your results, Google said no.



LEO:  They walked away, yeah.



STEVE:  They walked away, but then they're coming back because it's like, ooh, it's a pretty big market over there, so...



LEO:  Actually, the latest news is that, while they developed this Dragonfly, that employees were so upset that they decided to not do it and put it on the backburner.



STEVE:  Oh.  Well...



LEO:  Which is good.  That's great.  Employees need to stand up.



STEVE:  Yes.  And in the U.S. we have this notion of warranted search, which it's in the Constitution, and it's inconceivable to me that our legislature won't decide that, if a court orders communications to be intercepted for lawful purpose, that there doesn't have to be a way for that to be done.  I mean, it seems like where we're headed.  And again, so far we've just sort of been slipping by, I think.



LEO:  I like it because you're not dogmatic on this.  You recognize the need to do it.  And I think you believe, and I think you've convinced me, that there is a way to do it technologically that doesn't harm our general safety and security.



STEVE:  Yes.  And yes, the absolutists are right.  And from an absolute sense, any anything is bad.



LEO:  Any weakness is bad, yeah.



STEVE:  Any weakness is any weakness.



LEO:  Right.



STEVE:  And so it's no longer absolute.  But in the U.S. we don't have an absolute guarantee of privacy.  We know that the Constitution keeps the king's men from barging in without warrant.  But if you get a judge to say, yeah, we think there's reasonable cause to look in there, then, you know, so we don't have...



LEO:  We need to find a way to do it without compromising everybody, without weakening the entire system.



STEVE:  Yes.  And as I have said, in the unique case of the Apple ecosystem, as a consequence of the way Apple has designed their stuff, they could have a per-device key, which they protect, which would allow them to unlock - in this case, for example, it was Farook's phone after the San Bernardino attack, where they said, oh, we have no way to do it.  Well, presented with a specific device, they could unlock that specific device if they used that technology, without in any way weakening everybody else, except that, yes, there would be a central repository of everyone's master key. 



LEO:  Well, that's an inherent weakness; right?



STEVE:  It is.  It's an inherent weakness.  It concentrates the responsibility.  Apple doesn't want it.  But it may be that the decision is made, sorry, if you're going to provide encryption, we have to have a way in.  And there it is.  I mean, it may be what we end up with.



LEO:  Yeah.  I suspect it will be.  



STEVE:  I do, too.  Okay.  So Leo, two weeks ago you were spared my first...



LEO:  PewDiePie, oh.



STEVE:  My first encounter with PewDiePie.



LEO:  With Pewd, okay.



STEVE:  Yes.  And I thought, what?  PewDiePie?  What the heck?  So, okay.  So we know that - we discussed it two weeks ago.  A big fan of YouTube's sensation and top subscription magnet...



LEO:  And anti-Semite and racist.  Might as well throw that in, too, yeah.



STEVE:  Unfortunately.  PewDiePie, whose Twitter handle - the fan's Twitter handle is @HackerGiraffe.  He was concerned that PewDiePie, oh no, might be about to slip into second place, eclipsed by T-shirt.  No, I mean, T-Series, which is in number two place.



LEO:  Yeah.  Because they're installed on every phone in India.  So it's a kind of inorganic way to become number one.



STEVE:  Okay, yeah, that's...



LEO:  And I think that's what's offensive to PewDiePie's fans.



STEVE:  That makes sense.  See, Leo, you're bringing a lot more information to this than I had.



LEO:  Oh, I know way too much about PewDiePie, believe me.



STEVE:  So @HackerGiraffe zipped over to Shodan to get the IPs of some exposed printers.  This is a couple weeks ago.  He reported then finding - and this is what we talked about two weeks ago - 800,000 printers publicly exposed on the Internet.



LEO:  Oh, I remember this, yeah, yeah.



STEVE:  Of which he grabbed 50,000 IPs.



LEO:  Geez, Louise.



STEVE:  He then used PRET, the Printer Exploitation Toolkit, freely available on GitHub, which gives hackers the ability to access files, damage the printer, or access the internal network.  He used it to send out a mass plea for recipients of his illicitly printed page to please subscribe to PewDiePie.  And while you're at it, please unsubscribe from T-Series.  Which, as you said, if you're in India, apparently you already are.



LEO:  Right.



STEVE:  Okay.  So two weeks later, we're back.  This time he grabbed 100,000 printer IPs, sent out another blast over the weekend, and in fact he tweeted that he would be taking a university exam at around the time Monday morning that employees will be arriving at work to potentially find the printouts.  Now, I'm a little confused about time zones there because we know that these prints were received globally everywhere.



This printout says:  "PewDiePie is in trouble, and he needs your help to defeat T-Series."  Then under "What is going on," he printed:  "PewDiePie, the currently most subscribed-to channel on YouTube, is at stake of losing his position as the number one position by an Indian company called T-Series that simply uploads videos of Bollywood trailers and campaigns.  What to do?  Unsubscribe from T-Series.  Subscribe to PewDiePie.  Share awareness of this issue.  Tell everyone you know, seriously.  Oh, fix your printer.  It can be abused."  And "BROFIST."



And I have in the show notes a screenshot of part of the printer page, for anyone who is interested.  So anyway, this is, I mean, I certainly agree with point five, fix your printer.  And as I mentioned two weeks ago, I can't vouch one way or the other for PewDiePie.  I don't know why he has 75 million subscribers at this point.  He doesn't seem to be in any great danger because T-Series is at 70 million.  As I recall, two weeks ago, they were a lot closer.  So unfortunately, I'm afraid that all of the attention that PewDiePie gained as a consequence of this printer blast does seem to have allowed him to pull well ahead of the T-Series folks.  So anyway.



Oh, and The Wall Street Journal website was also defaced, as if that wasn't enough, saying that Wall Street Journal - and this is of course not true - Wall Street Journal would like to apologize to PewDiePie due to misrepresentation by our journalists, those of whom have now been fired.  We are sponsoring PewDiePie to reach maximum subscribers and beat T-Series to 80 million.  Well, that didn't happen.  Wall Street Journal fixed it.  But anyway, wow.  The world we live in...



LEO:  Just consider the kind of people he's attracting as his fans might tell you a little something about him.



STEVE:  Exactly.  Border agents, Leo.  Now, okay.  So I should mention I saw this last week, the idea that border agents were looking at users' private data.  And I thought, yeah, okay.  I don't like it, but okay.  It's going to happen.  Then looking more closely, the other shoe dropped, which is that they are not demonstrating proper behavior with the private data that users who are crossing the U.S. border are being compelled to provide. 



You know, in my case, I have absolutely zero contraband of any sort on my phone.  But it's my phone.  It's private, as are my conversations with friends and family.  It's nothing happening.  But it's, you know, I guess I feel a bit proprietary toward them.



LEO:  The good news is you'll never be stopped because you're a white male. 



STEVE:  Yes.



LEO:  Wear a turban, see what happens.



STEVE:  It is true that the percentage of people who are stopped is very low.  It's not like 100% of people are having their...



LEO:  It's very, very low.  And I've never had anything but welcome arms when I've come back into the U.S.  I've never had - I've been faced with more searches going into Canada.



STEVE:  Yeah.



LEO:  But that's because I'm a white, you know, male.  It's who you are.



STEVE:  Yes.  So anyway, there was a report from the U.S. Office of the Inspector General, the OIG, which was published by the Department of Homeland Security website last week, that our own U.S. Customs and Border Protection Agents are retaining the data that they copy from users' devices and not deleting it.  So for the past three years, since 2015, when this Trade Facilitation and Trade Enforcement Act, TFTEA, went into effect, CBP, that's the Customs and Border Protection agents, are allowed to carry out warrantless device searches at all 328 ports of entry to the U.S.  And so this means that agents are allowed to visually inspect any traveler's devices - smart phones, laptops, and so forth - without any reason.  And they're supposed to look for suspicious content of any sort.



Okay, moreover, in 67 of those 328 ports, they're also allowed to copy, and I'd be interested to know what 67, like why?  What makes those special?  But in those 67, they're also allowed to copy the device data onto a USB thumb drive for uploading onto a search platform called the Automated Targeting System, ATS - yikes - on which more complex searches are carried out against the user's copied data.  Now we'll put aside the fact that the license expired for the automated targeting system for about seven and a half months, during which time it wasn't available.  But that got renewed, and apparently that little oversight has been fixed.



So according to this recently published OIG report, CBP agents have not been deleting user data from the thumb drives after they've uploaded the data onto this ATS.  So the idea is the thumb drive is just being used as a shuttle to transport it to some other machine, where it's then uploaded.  And standard procedure dictates that the data be deleted.



The OIG report said:  "We physically inspected thumb drives at five ports of entry.  At three of the five ports, we found thumb drives that contained information copied from past advanced searches which had not been deleted."  So anyway, I just - it bothers me that anyone's data is being copied.  And of course it's more worrisome or annoying that after being copied it's not being treated in a responsible fashion in a way that preserves the privacy of the people who just had us poking into their devices.



LEO:  Don't worry.  They're only saving your music.  They don't - it's not the data they care about.  They just want the music.



STEVE:  Oh, goodness.  So we could talk about malicious Android apps till the cows came home.  This one just sort of stood out because it reminds me that asking users to be ever attentive while they're using apps is probably a fool's errand.  ESET posted their discovery of an Android trojan which steals money from PayPal accounts, even for users that have two-factor authentication turned on.



LEO:  To your point, your phone knows all that stuff.



STEVE:  Well, exactly.  And so what they discovered in November, last month, was malware that combined the capabilities of a remotely controlled banking trojan with a novel misuse of Android's accessibility services, which targets users who use the official PayPal app.  So this app is called Optimization Android v1.0.  It pretends to be a battery optimization app, does nothing with your battery.  After it's launched, it terminates without offering any functionality and hides its icon.  So it just basically sort of disappears from your phone.  It's now a trojan.



From then on, it watches what the user does with the primary goal of stealing money from victims' PayPal accounts.  It requires the activation of what turns out to be a malicious accessibility service.  The request is presented to the user as being from an innocuous-sounding Enable statistics service.  So a dialogue pops up that says:  "Use Enable Statistics?"  And then it says:  "Enable Statistics needs to, one, observe your actions.  Retrieve notifications when you're interacting with an app.  Two, retrieve window content.  Inspect the content of a window you're interacting with."  And so you've got the "okay" or "cancel."  So users say okay.



Well, what's just happened is you've given this trojan now access to your screen and keyboard, essentially, which it abuses.  It's now on the machine and able to intercept any launch of the Android PayPal app and attempts to maliciously transfer a thousand euros or whatever the local currency is whenever the user logs into PayPal.  The use of two-factor authentication doesn't prevent this since the trojan simply waits for the two-factor authentication to complete and the logon to succeed, at which time it jumps in using essentially an alternative API, an alternative UI, that allows it to talk to the PayPal app on behalf of the user, abusing accessibility in order to transfer the funds.



ESET has notified PayPal of the existence of the trojan and of the PayPal account to which funds were being transferred.  And the hack will fail only if the user has no balance and no card connected to their PayPal account from which funds could be pulled.  So stepping back from this, I think our takeaway is that we are asking for all kinds of features from our smartphones.  They've become very smart.  And we've seen, I mean, this is just not an Android problem.  We've seen instances of this on iOS also where the addition of important features such as those required in support of accessibility can be abused in clever ways that was never their intent.



So the safe use of desktop PCs requires everything we're seeing.  The safe use of desktop PCs requires some care about what apps are loaded and from where.  So much so that even Microsoft is now moving toward this idea of a store where there will be curated apps that, rather than just this free-for-all of downloading things for Windows from wherever, although Microsoft is having a hard time selling that concept because we all like the freedom that we have now as Windows users.



But I don't think it's fair to ask users to be constantly vigilant in their actions as they use apps.  I think it is the case that the only solution is to keep these apps from getting into our machines in the first place.  So really the location where we need vigilance is in not installing these things capriciously, I mean, being skeptical of the apps that we install.  And of course one of our favorite slogans on the podcast is never install something that something tells you you need.  Only install things that you go looking for.  So anyway...



LEO:  Of course those are on third-party app stores, so that's the other thing is to stick with the Play Store.



STEVE:  Yes, yes, yes, yes, yes.



LEO:  Not that there hasn't been malware on there.  But they have to go through a lot more hoops to get that stuff on there.



STEVE:  Right.  And in fact this was on the Play Store.



LEO:  Was it?  Oh, I thought it was a third-party store only.



STEVE:  I think, the screenshot I saw I think showed it on Google Play, but I'm not sure.



LEO:  Okay.



STEVE:  So, but yes, you certainly only get things from Google Play.  WordPress just released v5 of their number one website blogging system.  And there was some confusion surrounding it.  First of all, I thought it was kind of fun.  I didn't realize until I saw the name of it and then dug in a little bit that as they have done since v1, which was the Miles Davis release back on January 3rd of 2004, they've named their releases after famous musicians.  Version 2.0 was Duke Ellington; 2.1 Ella Fitzgerald; 2.2 Stan Getz.  They had John Coltrane, George Gershwin, Count Basie, Charlie Parker, Bennie Goodman.  Anyway, 5.0 is the Bebo Valdes, who is, they say, a pioneering Cuban jazz musician.



What was confusing is that in the press just last week, late last week, was the news that, as a consequence of this major v5 release, Google could index users' passwords.  And so it's like, whoa, wait a minute.  Okay.  So looking closely at this, I wanted to sort of, well, first of all, give people a heads-up that there is an important security release, but there is one for v5.0.1.  But it had actually nothing to do with this big release of v5.  There is a new release of 5.0.  If anyone's interested, the big new feature is they have a completely brand new, extremely flexible block-based editor.  And is WordPress still a sponsor of the...



LEO:  They sure are, WordPress.com, yeah.



STEVE:  Yes.  So I did want to mention that.



LEO:  Now, if you're on WordPress.com you don't even have to think about this because they just keep it up to date automatically.



STEVE:  Right.



LEO:  It's only if you're running a self-installed version of WordPress.



STEVE:  Correct.  So the truth is that, although the headlines made this look like this was a problem with 5.0, in digging deeper into this, it looks like the release of 5.0 was asynchronous to a regular security release where WordPress fixed seven different problems.  One of the seven was that in an unusual circumstance which required nonstandard configuration that probably actually nobody would ever encounter in reality, it was possible that a search engine might index the initial setup screen where an email address and a default password could be found.  But it was like, you know, the tech press really went clickbait overload on this.



So but the important thing is that there was a week after - the release of 5.0 was on December 6th.  On December 13th, a week later, was a security release announcement.  And it does affect 5.0 just because it affects them all, but it also runs all the way back to the Count Basie release, which was v3.7 back in October of 2013.  So for the last five years they have a security update for everything.  So I did want to give our listeners a heads-up, as you said, Leo, for people who are running their own WordPress installations, that's it just a standard security update that affects all versions for the last five years.



So despite the way the press handled it, it was not something devastating that, first of all, was even likely to happen for anyone using v5.  It just happened to be that they released this security update a week after their major release.  The security fixes, as I said, were comprehensive.  And for what it's worth, it does look like a nice update to WordPress.



I know you've talked about the Facebook Photo bug.  This again is an instance of I think Facebook doing a good job.  Last Friday they announced another security incident affecting millions of their customers.  For the 12-day period from September 13th through the 25th, a bug existed in one of Facebook's APIs which exposed the private photos, or I should say potentially exposed the private photos of nearly 6.8 million users.  Their forensic analysis of the bug revealed that up to around 1,500 apps, which were built by 876 developers, could have had access to the private photos of these 6.8 million users.



I have here in the show notes a link to Facebook's posting.  A Tomer Bar is the Facebook employee who last Friday posted under the title "Notifying Our Developer Ecosystem About a Photo API Bug."  Facebook was completely forthcoming in this.  He basically stated the things I've just stated.  And remember that last week's podcast was titled "Internal Bug Discovery."  So I will note that, despite Facebook's current residence being a doghouse from their past conduct...



LEO:  Good way to put it.



STEVE:  Yeah.  They, like Google, clearly have an internal red team of some sort whose job it is to find their own mistakes before anyone else does.  Rather than faulting them for making a mistake, I salute them, as I did Google last week, for finding and fixing it themselves.  This is not a role that anyone wants to outsource to hackers.  So he explained that the nature of this was that:  "When someone gives permission to an app to access their photos on Facebook," he writes, "we usually only grant the app access to photos people share on their timeline.  In this case, this [12-day window] bug potentially gave developers access to other photos such as those shared on Marketplace or Facebook Stories.



"The bug also impacted photos that people uploaded to Facebook but chose not to post.  For example," he says, "if someone uploads a photo to Facebook but doesn't finish posting it, maybe because they're lost reception or walked into a meeting, we store a copy of that photo for three days so the person has it when they come back to the app to complete their post."



He said:  "Currently, we believe this may have affected" - and then, you know, those numbers and so forth.  And he finishes, saying:  "We're sorry this happened.  Early next week we'll be rolling out tools for app developers that will allow them to determine which people using their app might be impacted by this bug.  We will be working with those developers to delete the photos from impacted users."  And presumably those apps who use Facebook's tool would be able then to notify their users that they may have been affected by this app or not, depending.



He says:  "We'll also notify the people potentially impacted by this bug via an alert on Facebook."  Again, so I think they're being completely responsible and acting the way we need our large Internet providers of services like this to act.  As a society we are building ever more complex and capable systems, and these systems will be imperfect.  I fully expect that some day, because this is science, this is math, we will figure out how to economically make these systems far more correct than we know how to today.  But until we get there, the best we can do is find the problems and fix them as they arise.  And so here's another example of I think it being done right.



And speaking of monkeys, Leo, oh, the passwords we use.  SplashData, the publisher of the password management applications TeamsID, Gpass, and SplashID, has just released their eighth annual list of the Worst Passwords of the Year.  Appearing for the first time on the list in 23rd place is "donald," which we haven't had before, and we can imagine the source for that password.  Somewhat sadly, our longtime podcast favorite "monkey" has slipped five places, down to 18th most popular; while "princess" is debuting for the first time in the 11th slot.



So what have we?  The top two, surprisingly, have not budged.  The password 123456, ranked top last year, still in first place this year.



LEO:  I was going to use "princessdonaldmonkey," but I guess I can't now.



STEVE:  Oh, well, you know, when you combine words...



LEO:  It's better.



STEVE:  ...you can get a better password.  So yes.



LEO:  123456princessdonaldmonkey.



STEVE:  That would be good.  Well, no.  Don't anybody use that.  Okay.  In number two place, yes, hasn't moved, "password."



LEO:  Oh, man.



STEVE:  I know.  Can you believe?



LEO:  But if I spell it with a zero instead of an "O," is that okay?



STEVE:  Oh, in fancy h4xx0r fashion?



LEO:  In h4xx0r fashion.  In h4xx0r fashion.



STEVE:  So, and we also have the slightly less popular, yet growing in popularity for those who are slightly less lazy, 123456789.



LEO:  Oh.  Now we're talking.



STEVE:  So you add, yeah, you add that 789, it does take a little bit longer.  That's moved up three into third place this year, while 12345678 has dropped one rung to fourth place.  Apparently it got kicked out by 123456789's having moved up to number three.  Wow.  There is real competition on these number ones.



LEO:  Tight fight, yeah, mm-hmm. 



STEVE:  It is, you know, you never know.  And in fact 12345, just five digits, retains its fifth place position.  So you can always remember, 12345 is the fifth most popular password.



LEO:  Easy to remember.



STEVE:  Wow.  111111 is new, has just appeared on the list.



LEO:  Oh.  Oh.



STEVE:  Yup.  Oh, 1234567, not 5, not 6, not 78, not 789, but just 7, has moved up one.  "Sunshine," how happy, sunshine has just appeared on the list.  "Qwerty," Q-W-E-R-T-Y, we know where that came from, that has dropped five because it's been pushed down because some of these number runs have apparently run up.  "Iloveyou," unchanged, in 10th place.  That's nice.  "Princess" has appeared in 11th.  "Admin" dropped down one.  "Welcome" also down one.  666666, newly appeared.  Believe it or not, among all this jumbling, abc123 still in 15th place, just sitting there, not budging.  Everything's churning around it, princesses are popping in, and sunshine is new.  Abc123 holding at 15.



"Football" dropped down seven.  123123 also holding at 17.  And as I said, sadly, "monkey" has dropped down to 18.  For someone who's getting really clever, 654321 in 19th place.  And then we have the, wow, how could this actually be - oh, I see.  You hold the shift key down.  It looks like cartoon swearing:  !@#$%...



LEO:  Let me guess the next one.  Caret?



STEVE:  Yes.



LEO:  Ampersand?  Asterisk?  



STEVE:  You're a h4xx0r.  Like, how did you do that?  Amazing.  The great Kretzkin is among us.



LEO:  Oh, man.



STEVE:  Yes.  And Leo, it's new on the list, that one you just guessed all by yourself.  New.



LEO:  Well, welcome.



STEVE:  "Charlie" is new in the 21st position.  And here we have something really clever.  This is like one you were going to suggest:  aa123456.  And what's amazing to me is that not one person did it, but that's the 22nd most popular password.  Twenty-third, "donald."  Twenty-fourth, "password1."  Okay, that's not very imaginative, but it has just appeared.  And then, for those who - maybe a browser, oh, no, I was going to say maybe a site objected to "qwerty," but no, because that's the ninth most popular.  So "qwerty123."



LEO:  Oh, there's improvement, yeah.



STEVE:  Rounds out the list in the 25th position.



LEO:  Nice.  Strong.



STEVE:  So SplashData - I wanted to say "Slapdash," but no.  SplashData estimates almost 10% of people have used at least one of the 25 worst passwords on this year's list.  So 10% - no, certainly zero of our listeners, none of our listeners, except maybe "monkey," but actually we know that one of our listeners has used "monkey" in the past.  And apparently nearly 3% of people have used the worst password:  123456.  Not even going as far as adding a 789, Leo, which would have made it really not much harder to get.



LEO:  Where do they get these?



STEVE:  This is from leaked lists of passwords, which they aggregate over the course of the year and then sort and then provide for our entertainment on the podcast.



LEO:  They take the adult words out.



STEVE:  Oh, oh, yes, yes.  I was just going to say, and I have that in the show notes here.  We should note that passwords leaked from hacks of adult websites were not included in this report, doubtless being NSFW.  That would make for a different list.



So, oh.  Bleeping Computer had two stories last week that I thought were very interesting, and I wanted to absolutely put it on our listeners' radar.  Just because some malicious drive encryption is done right, and because it would be trivial to do it right, doesn't mean that all of it has been done right.  Far from it, apparently.  There were two stories that Bleeping Computer had.  One was titled "How to Decrypt the InsaneCrypt or Everbe 1 Family of Ransomware."  I've got the link in the show notes for anyone who's interested.



And they wrote:  "InsaneCrypt or the Everbe 1.0 Ransomware is a family of ransomware infections that were based off an open source project.  This ransomware family is distributed through possibly spam and hacking into Remote Desktop Services, but that has not been confirmed.



"The good news," they write, "is that the variants of this ransomware family can be decrypted for free using a decryptor created by Michael Gillespie and Maxime Meignan.  In order to use the decryptor, a victim just needs to have" - I thought this was really interesting - "an encrypted file and an unencrypted version of the same file."  This has got to be the lamest encryption [crosstalk].



LEO:  Got to be.  You can reverse the hash, in other words.



STEVE:  Right.  This can be typically achieved through the sample pictures provided by Windows.  I thought that was clever.



LEO:  Oh, yeah.  Those will be encrypted, and you'll be able to get another copy of those easily, yeah.



STEVE:  Exactly.  The second story:  "How to Decrypt Hidden Tear Ransomware Variants."  They said:  "If you've been infected by a Hidden Tear Ransomware variant, then you're in luck" - well, kind of.  But, you know, I mean, it's never good to be encrypted at all.  But given that you have been, this is the one you want to be encrypted with because it's not really, apparently.  "You're in luck," they say, "as a program called Hidden Tear Decryptor has been created [also by Michael Gillespie, who apparently does this] that allows you to recover your encryption key without having to pay the ransom."



Hidden Tear is the name of a ransomware family whose full source code, apparently not very well engineered, was published on GitHub.  This allowed hackers to download the source code and create their own ransomware variants that could be used to infect victims.  Due to the source code's wide availability, there are many ransomware variations under different names that utilize the same Hidden Tear codebase.  As the original code was decryptable, this means all other ransomware created from the same code are decryptable, as well.



Some, get this, some of the Hidden Tear variants supported by this tool include:  8lock8, AnonCrack, Assembly, Balbaz, BankAccountSummary, Bansomqare Wanna, Blank, BloodJaws, Boris, CerberTear, CryptConsole2, CryptoKill, CyberResearcher, Data_Locker, Dev-Nightmare 2xx9, Diamond, Domino, Donut, dotRansom, Executioner, Executioner2, Executioner3, Explerer, FlatChestWare, Frog, one I can't say on the podcast, but we're in the F's.



LEO:  In the F's, yup, yup.



STEVE:  You can imagine, uh-huh.  Gendarmerie, Horros, JobCrypter, Jodis, J-Ransomware, J-Want-To-Cry, Karmen, Kraken 2.0, Kratos, LanRan, Lime, Lime-HT, Luv, Matroska, MireWare, MoonCrypter, MTC, Nobug, Nulltica, onion3cry, OpsVenezuela, Paul, PayOrDie, Pedo, PGPSnippet, Poolezoor, Pransomware, Predator, Qwerty, Random6, Random6 2, Randion, Ransom - anyway, it just keeps going.  We're only to the R's.  So the point is there's a lot of this stuff out there.  First of all, who knew?  But secondly, there's a very good chance, if you get hit by Donut or PayOrDie, or Kraken, or Sorry - there was one called "Sorry" - then by all means you, your friends or family, check in with Bleeping Computer because, as we know, they're on top of this stuff.  Maybe there's a simple way to get yourself decrypted, which would be wonderful because apparently there's a lot of poorly written crypto malware out there, thanks to there being a dumb one on GitHub.  So yay.  Maybe that's pushing the bad stuff out of the way.



I mentioned at the top that Microsoft is encountering a run of zero-day vulnerabilities.  Last week's Patch Tuesday closed a zero-day vulnerability that was being actively exploited by two different state-sponsored cyberespionage groups who are also behind the zero-day that Microsoft patched the month before in November.  And before that, in October, Kaspersky Lab discovered a zero-day being used for an elevation of privilege by the FruityArmor state-sponsored cyberespionage group.



LEO:  What state is that, FruityArmor?



STEVE:  I know, I know.  Remember, I don't think they name themselves.



LEO:  Oh, okay, good.  Oh, that's a relief.



STEVE:  You know, they're like names we give them.  And it's annoying when the same group is known by 12 different names, as we've seen before.  So in this case, yes, it's FruityArmor.  And before that, in September, Microsoft patched a zero-day that was in use by hackers who were using it to install and spread a backdoor.  So mostly these appear to be high-end, highly  targeted, state-level espionage attacks.  Their discoverers, or the discoverers of these zero-days, don't want to have them discovered.  I mean, the users of these zero-days don't want them discovered because they're too useful for targeted attacks.  So they are not being used in widespread campaigns.  You know, they're really being kept under the radar.  We don't know, when they're found, how long they've been in use.



And given that two different zero-days were both discovered and fixed in the most recent Patch Tuesdays, one wonders how many as yet undiscovered zero-days are also in use because, again, we don't know that they are until they're found.  And they're not being sprayed around the Internet.  They're not being tweeted by hackers who are having a bad day.  They're being developed by the likes of the NSA and other countries' equivalent services for the purpose of getting into other people's computers.



So this spate of zero-days affecting Windows, our increasingly old and creaky operating system, is a bit worrying.  Windows is coming under increasing pressure, and it's not holding up as well as we would like or hope.  And in fact, Leo, I listen to you with Paul and Mary Jo on Wednesdays, and it's a little sobering. 



LEO:  Oh, yeah.  Oh, yeah.



STEVE:  They're not happy any longer.



LEO:  Unh-unh.



STEVE:  They're really saying, wow, you know, what the heck is going on up there in Redmond?  Okay.  Our last little bit of news is that Firefox 4 was released.  An interesting note is that Firefox's full distrust of the Symantec SSL/TLS certificates had been postponed from the previous release of 63 until finally now, Release 64.  Remember that Firefox brings along its own certificate store.  Unlike Chrome, for example, which relies on Windows' underlying root store, Firefox does its own.  It uses its own security suite.  And so it's up to them when they were going to finally roll out full mistrust, that is, no longer accept a certificate from any website using those certs.  They postponed it because up until finally now, their telemetry had indicated that too many Firefox users were still encountering websites that were still using the Symantec certs.  So, boy, I can't imagine why anyone wouldn't just re-up with DigiCert.  And finally I guess everyone has.  So there's that change.



Also there is something known as a Contextual Feature Recommender, which is a bit of a tongue-twister, the CFR.  The good news, you can turn it off.  But the idea is that - and if any of our listeners who are Firefox users are annoyed by this, you can turn it off.  It's a new feature that will display recommended extensions that correspond to a particular website you're visiting.  So, for example, if you're at Facebook, there is a Facebook Container extension which this thing will say, hey, you know, there is an extension to Firefox that's available for Facebook.  There's also an enhancer for YouTube and To Google Translate.  You can, however, if you don't like that, uncheck the "recommend extensions as you browse" under the Browsing category of Firefox options.  So, eh.



Also a new feature is Multiple Tab Selection.  You can either mark a range of tabs with shift, or toggle tabs to select multiples with control-click.  And once chosen, you can bookmark them as a group, move them, or pin them.  Also they've added Native Windows 10 Share Support so that you're able to - I guess Windows 10 has a web page sharing with a variety of applications installed.  And so to access the feature, you click on the little dot dot dot menu in the address bar, then click on the share button, which will open the native Windows 10 sharing dialogue.  Not being a big Win10 user yet, I haven't encountered that myself.



A little controversial for old folks, Live Bookmarks and RSS Feeds have been removed.  They've removed support for both Atom and RSS feed subscriptions from the base browser.  So anyone who was actively using RSS feeds will simply need to install an add-on to provide the feature.  So you can get it back, it's just not built into the default.  And I also talked about a Remove Extension context menu option.  I'm sure I mentioned this before, so I must have been talking about it coming.  It allows you, if you right-click on any of the Firefox add-on extensions, in the dropdown menu is now a remove extension that makes it just instant to get rid of it.



And my favorite add-on is you can get to it by putting about:performance in the URL bar.  That gives you a task manager that shows which tabs are using how much of your processor.  So if you have a misbehaving tab which has slowed things down or seems to be consuming a lot of resources, about:performance will allow you to identify it and tell it to knock itself off or just kill it.



LEO:  Steve Gibson is going to sing for us.



STEVE:  Oh, no.



LEO:  Oh, no.



STEVE:  So I'm not sure how I'm going to handle that.  But first, first Rails, R-A-I-L-S.  I was reminded of it.  It's been years since we talked about it.  I fell in love with this game for my iPad.  It's also available on Android and for the Mac platform.



LEO:  Oh.



STEVE:  Yes.  And it's just a great game.  It's sort of a puzzle game.  Stations are placed on the board.  You then interconnect them using a rail outlet with switches of your own design.  And then trains are emitted from the stations, and you're responsible for flipping the switches in order to route the trains around to keep everything moving.  And it's just...



LEO:  I haven't played it in ages.  I just loved it.  We both loved it, I remember.



STEVE:  Yes, yes, yes, yes.  It's great.  So I just wanted to put it on - it's in time for Christmas, so maybe you could give somebody who has a phone or a pad something fun to occupy themselves with.  I mean, and again, all ages.  And that video you're showing right now, those are some beautiful track layouts.



LEO:  Yeah.  I have certain patterns that I like to use.  But I have to say it gets harder and harder.



STEVE:  Yes, yes.  Well, the trains get longer, and it's like...



LEO:  And things happen, and trains break down, and you just get - I didn't realize, they've added snow, which I like.  That's nice.  They've got some seasons in there.  That's...



STEVE:  Different seasons, yeah.



LEO:  It's called Rails, R-A-I-L-S.



STEVE:  Rails, yeah.  There are a bunch of cheesy ones, so be careful.  You want...



LEO:  There are.  This is a category, I guess.



STEVE:  Yeah.  Okay.  So Kevin Sears, a listener from Eagle, Wisconsin, offers us the Holiday Hard Drive Song, he says, a winter holiday remake sung to the tune "Let It Snow."  So I'm not going to sing.  But so he renamed it "Let It Go."  And he said:  "Oh, an aging hard drive is frightful."



LEO:  [Singing] "But my photos are so delightful. Got no plan you know so, I let it go, let it go, let it go.  It doesn't show signs of stopping, 'cause error correction is working.  The green light is still on so, I let it go, let it go, let it go.  The drive wants to say goodnight, but everything seems to be all right.  Bits are aged five years or more, seems slower than it did before.  The spinning surface is dying, and S.M.A.R.T. says it's goodbying.  I think I backed up a year ago so, I let it go, let it go, let it go.  Now bits are just ones or zeroes, trying hard to be our data hero.  But soon they'll be unreadable - NO!  'Cause I let it go, let it go, let it" - dot dot dot dot dot.



STEVE:  And it died.



LEO:  And it died.



STEVE:  And so have our listeners, I'm sure.



LEO:  I had to do that.  You can't just read it.  You've got to sing it.



STEVE:  I was discussing this with Lorrie last night, and I thought, I can't do this.  And then it occurred to us, oh, but Leo could.



LEO:  Leo will do it.



STEVE:  So Leo, you stepped up.  Thank you for that.



LEO:  Let it go, let it go, let it go.



STEVE:  Or in the case of SpinRite users, just run SpinRite on your drive and keep those data bits happy.



LEO:  It's very true, really.  I mean, I think it's a good lyric.  Nicely done.  Well done.  Sorry about the singing.



STEVE:  No, no, don't apologize.  Someone had to do it, Leo.  Better you than me.  Mike S. in New Hampshire, his topic was router infection.  He said:  "I have a Netgear router, an R6300v2, if you care.  I routinely clicked on the Router Update > Check for new version link.  It told me that there were no updates available.  Last week I started seeing very slow response from WiFi, so I checked again, and it said the same thing.  I decided to check with Netgear, and their website had updates."



LEO:  Ooh.



STEVE:  Uh-huh.  "I downloaded the update and applied it, and the performance went back to normal.  So far it's been good."  He says:  "I'm thinking that some malware got into it and wouldn't let it check for updates itself."



LEO:  Oh, interesting.  Ah.  That makes sense.



STEVE:  Yes, yes, yes, yes.  Yes, it totally does.  That's what you would expect something nasty to do because it didn't want to get itself flushed away.



LEO:  Right.



STEVE:  So anyway, I just wanted to share that with our listeners.  That's a very good point is that, when you think about it, you can't actually trust the internal "check yourself for updates."  It's worth going to the manufacturer's website and verifying.  So Mike, thank you very much for that little tip.  I'm glad to pass that on to our listeners.



And Chris Peterson in Ham Lake, Minnesota, the subject of his note was "It's even simpler."  And I have to agree with him.  He said:  "Regarding the unauthenticated UDP IoT protocol."  That was the CoAP protocol we talked about last week.  He said:  "Since it's IoT, we want to minimize processing and storage."  Agreed.  He said:  "The IoT device only needs to return a pseudorandom number to the requestor."  Okay, now let's step back a bit.  Remember, what I suggested last week was that the IoT device could come up with a random key under which it would encrypt the incoming IP to create an access token, which it would return to the requestor, such that every subsequent request coming in had to carry that permission token in order to be serviced.  So that was my suggestion.



Chris has a better one.  And this is better.  He says:  "This can be of poor entropy," meaning this random number.  So he says:  "The IoT device only needs to return a pseudorandom number to the requestor.  This can be of poor entropy.  The IoT device is then paired with the requestor.  Any further requests from that IP must include the same PRN" - the same pseudorandom number - "or they're rejected.  A request from another IP resets the pairing, and the process is repeated.  We can do this because spoofing requires a response to a fake IP address.  This fake address, the victim, won't be responding.  A short pseudorandom number of 32 bits or less should be sufficient, thus minimizing traffic sent to the victim.  Obviously not 'perfect' security, but it's 'good enough' security, and way better than what they're doing now."  And he said "73, Chris, K9EQ."



Okay.  So this is clever.  My solution was overdesigned because your typical IoT device, first of all, it's not fielding stuff from all over the Internet.  My solution would have allowed it to do that, to have no state saved in the device, always returning a unique token based on the source IP of the incoming request.  So as IPs all over the place asked for permissions, it would dutifully return the token for them.  But that's not the model.  Your typical IoT device is going to talk to one thing.  And if there were two or three, well, over time this would cause it to reissue a random token and then essentially establish a temporary one-to-one pairing with the most recent requestor of a connection to the IoT device.



Anyway, he's right.  No crypto.  Simple PRN that you could just take, I mean, I think most chips will allow you to read the number of clock cycles since they've powered up.  That would be completely fine.  Just send that back and save it along with the IP address so you can check it in subsequent requests.  Anyway, Chris, tip of the hat to you.  That's simpler and, I think I agree, better.  And again, much better than the nothing that we have now in this dumb CoAP protocol that somehow happened when it shouldn't and is now being abused for, what is it, times 50 DDoS reflection attacks, which is just crazy.



Okay.  So a critical SQLite flaw which leaves millions of apps vulnerable.  I should say "app instances."  The Blade Group at Tencent has named this "Magellan."  And they write - I've got a link to their full disclosure posting.  They said:  "Magellan is a remote code execution vulnerability discovered by Tencent's Blade Team that exists in SQLite."  And I want to remind everyone again, this is probably in all of our machines.  I had four instances in my machine, and that took me somewhat by surprise.  So I'll explain what it is, and then I'll explain how to find it.



Okay.  So they say:  "As a well-known database, SQLite is widely used in all modern mainstream operating systems and software, so this vulnerability has a wide range of influence.  After testing, Chromium was also affected by this vulnerability.  Google has confirmed and fixed this vulnerability.  We will not disclose any details of the vulnerability at this time, and we are pushing other vendors to fix this vulnerability as soon as possible."



So what we have now is the knowledge of a problem, the confirmation of the problem.  In fact, there was a page on the Internet which is now kind of irrelevant that allowed you, for Chrome 70, the Chrome browser 70, you could crash any tab by clicking it because it would invoke SQLite that the Chrome browser was exposing to any website that you went to.  So that's bad.  This was a remote code execution vulnerability.



So what they have is simply a Q&A at this point.  Am I affected by the vulnerability?  Answer:  "If you use a device or software that uses SQLite or Chromium, it may be affected, depending on whether there is a suitable attack surface."  And of course that's important.  The malware has to have a means of executing an SQL command against the database which is present.  So it's the embedded database itself where the vulnerability exists.  As I mentioned, my email client, Thunderbird, uses SQLite.  And of course email is an attack surface.  So that's pretty much all it is.



Question two, what is the danger of this vulnerability?  "Remote code execution, leaking program memory, or causing program crashes."  Three, does the vulnerability have exploit code?  They say:  "Yes, we successfully exploited Google Home with this vulnerability, and we currently have no plans to disclose exploit code."  That is, everything needs to get fixed.  Four, what are the conditions for exploiting this vulnerability?  They answer:  "This vulnerability can be triggered remotely, such as accessing a particular web page in a browser or any scenario that can execute SQL statements."



Five, has Magellan, which is what they call this, been abused in the wild?  Their answer:  "We have not seen a case of it yet."  Six, is there a fix or workaround?  And the answer:  "We have reported all the details of the vulnerability to Google, and they have fixed the vulnerability.  If your product uses Chromium" - and remember there are many other browsers that do - "please update to the official stable version."  And so in the case of Chrome it's 71.0.3578.80.  And I already have one, my Chrome is a little bit further than that.  They said:  "If your product uses SQLite, please update to 3.26.0.  The CVE number is pending."



So on the 14th, late last week, there was a Crash Chrome 70 with the SQLite Magellan bug.  That was at WorthDoingBadly.com.  That's obsolete now because, as we know, Chrome is at 71.  You want to make sure, though, that your Chromium-based browser is safe.  So for what it's worth, it might be worth going to WorthDoingBadly.com and trying this.  The site is WorthDoingBadly, all one word, dot com slash sqlitebug.  So WorthDoingBadly.com/sqlitebug, and make sure that your browser doesn't crash.



Okay.  So SQLite is a library, essentially, that many different things use that, for whatever reason, need to store user data, and even things that I'm not quite sure why they're storing user data.  I found it in four places on my system.  It was in NetWorx, N-E-T-W-O-R-X, which is an app I've mentioned and I use.  I love it.  I have it running on my screen all the time because I have it using SNMP, Simple Network Management Protocol, to continuously check the counters, the byte counters on my pfSense router to watch the incoming and outgoing bandwidth.  I just kind of like to get a sense for data coming and going in and out of my network so if something seems anomalous, I can go, okay, wait a minute, and go check why a huge amount of data is transiting.



Python, my installation of Python brought sqlite3.dll along.  And Adobe Reader - shock - is vulnerable with sqlite.dll.  Adobe Reader, even though mine is 9.something, which is not super recent, but it had v1.0.0.1 of sqlite.dll dated 2/27/2009.  Also my favorite PDF printer, which is Nova PDF, that has a .NET version, sqlite.interop.dll, also vulnerable, v1.0.82.0 dated 6/8/2018.  So that's not very old, but it's not yesterday.



Okay.  So anybody can find these who is a Windows user by changing your directory to the root.  So do cd\ to go to the root.  And then you want to do a recursive dir, a recursive file search for sqlite*.dll.  So cd\ to move to the root, and then dir /s, which causes a search through all subdirectories, space sqlite*.dll.  Hit Enter, and pretty quickly you should be rewarded with a list of all the files that begin with sqlite.  You'll find sqlite.dll, probably sqlite3.dll, and maybe some that have interop.dll, which are the .NET variety.



So this doesn't immediately mean that you're in trouble.  What it means is that those apps are using this embedded database for their own storage purposes on your system.  It means that the API, that is, the SQL API has a vulnerability that could allow code to be injected on your system and run.  The danger - now, for example, Adobe Reader, there's the danger, Will Robinson.  For some reason, who knows what it's doing, but of course why wouldn't Reader have SQL built into it because it has everything else, Flash and SQL and oh my god.  And JavaScript and stuff that no one needs.  But it's there.  So if there's a means for a document to invoke this SQL embedded database in Reader, that's a worry because that represents an attack surface.



So I haven't looked at why my install of Python has SQLite 3.  Probably it's just that it supports that database engine API, so it's part of the Python library.  It doesn't mean that it's itself vulnerable, or maybe it's part of the IDE.  Who knows?  Anyway, so it's probably worth just doing a little audit of your system by recursing through your drive for sqlite*.dll, see what you find, and then maybe, if it looks like there might be an attack surface exposed, like I would be worried about my Adobe Reader very much.  I'll probably actually pursue this over the holidays and have some information at the beginning of 2019 just because that seems like an obvious attack surface.  And it might be the fact that I'm deliberately using an old version because I don't want to be the SaaS, sign up and join the Adobe fan club, so I'm using one that's entirely offline.  I think it's 9.5 was the last one that was not hooked into the cloud.



But anyway, the SQLite folks immediately fixed this.  They have an update.  The problem is that that doesn't mean that the libraries that we use are going to get updated or in a timely fashion.  If it turns out that there are means of leveraging this into an attack surface, we're probably going to see this in coming weeks and months next year.  So I wanted to put it on everyone's radar.  It's easy to audit your system and get a sense for what apps you have installed which are using this embedded database.



Again, it doesn't mean it's a problem.  It's only a problem if there's a way for a malicious use of that through the app.  So it may be Python is a problem.  But Python's an example.  I mean, so, for example, the SQLite 3 in my Python is v3.21, relatively recent, dated 6/27/2018.  Probably it's already been updated, or it will be shortly.  So that kind of thing can be fixed.  I guess I would say, where possible, where you see that an app is using SQLite, maybe give them a few weeks to catch up, to get their use of it updated, and then update the app so that you're not vulnerable.  Anyway, I don't want to...



LEO:  Scare anybody?



STEVE:  Yes, I don't want to overstate this.  But it could become a popular means of attacking people's systems.



LEO:  It's everywhere.  It's everywhere.



STEVE:  The good news is - it is everywhere.  The good news is that it absolutely requires access to SQL.  There is a web SQL interface.  That's what Chrome supports.  Firefox uses SQLite, but they weren't exposing the interface.  Chrome was.  So actually it allowed JavaScript to issue SQL commands to an embedded database on your system.  And that was the way in.  So, briefly, Chrome could be exploited with this.  No longer.  So you want to make sure that, if you are a fan of a Chromium-based browser, that you get yourself patched.  And again, I'm sure that anyone who is producing a browser already has updates and fixes, just as Google did instantly with Chrome.



LEO:  Yeah.  My Chrome is safe.



STEVE:  Yup.



LEO:  Good to know.



STEVE:  And Leo?



LEO:  Actually, this is Safari.  Safari's safe, too.  Good.



STEVE:  Not only is this the end of the podcast.



LEO:  The end of the year.



STEVE:  This is the end of the year.



LEO:  We're done.  Next week is Christmas Day, and we're going to have a "Best Of."



STEVE:  Cool.  Cool.



LEO:  Some great stuff.  We've never done this before with Security Now!, but I think it's going to be fun.  New Year's Day we will do the same because there's no point in - but we'll be back January 8th for a whole new season of Security Now!.  Lots of new stuff.  There'll be exploits galore.



STEVE:  Who knows?



LEO:  Who knows?



STEVE:  This same podcast last year, the last podcast of the year in 2017, we would have never predicted that 2018 would have been the year of a disaster of the Intel chipset.  And other processors, too.  Not fair just to pick on Intel.  But, you know, every processor architecture collapsed this year.



LEO:  Yeah.



STEVE:  Who knows what we have in store for us for next year?



LEO:  Something wonderful.



STEVE:  We'll be here.  We'll be here.



LEO:  And I am relieved.  Thank goodness Steve's here.  Find everything Steve does at his website, GRC.com, including SpinRite, the world's best hard drive maintenance and recovery utility; and this show and transcriptions thereof.  GRC.com.  He's on Twitter.  If you want to leave him a message or ask him a question:  @SGgrc, or GRC.com/feedback.  Do people still use that?  Or they just use Twitter probably.



STEVE:  Oh, yeah, yeah.  That's where I got both of the notes that I shared, yes.



LEO:  There you go, GRC.com/feedback.  You can also get audio and video of the show from us, TWiT.tv/sn.  Subscribe in your favorite podcatcher, you just get it automatically and download it the minute it's done.  Collect all 694 episodes.  Make sure your collection is complete.  Steve, have a great holiday.  Have a good two weeks off.



STEVE:  Same to you, my friend.  I'm going to enjoy a little time off, and we'll be back to kick some security butt here.



LEO:  Yeah.  Thank you.  We'll see you next time on Security Now!.



STEVE:  Thanks, buddy.



Copyright (c) 2018 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#696

DATE:		January 8, 2019

TITLE:		Here Comes 2019!

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-696.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we look at the NSA's announced forthcoming release of an internal powerful reverse engineering tool for examining and understanding other people's code; emergency out-of-cycle patches from both Adobe and Microsoft; and, yes, we do need to mention PewDiePie again.  We also need to mention our prolific zero-day dropper SandboxEscaper, a new effort by the U.S. government to educate industry about the risks of cyberattacks, some welcome news on the ransomware front, some VERY welcome news of a new Windows 10 feature, and a note about a just-published side-channel attack on OS page caches.  Then we'll wrap with an update on my work on SQRL and my discovery of a VERY impressive and free large file transmission and sharing facility.



SHOW TEASE:  It's time for Security Now!.  We've been off for a few weeks.  Oh, my goodness.  That means there's lots to talk about, including new Microsoft zero-day exploits.  Aye-aye-aye.  Steve has discovered a great file transfer utility.  An update on SQRL:  It has an API.  And GHIDRA - or is it GHIDRA, or is it GHIDRA - the NSA's latest tool.  All the deets coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 696, recorded Tuesday, January 8th, 2019:  Here Comes 2019!



Happy New Year!  Time for Security Now!.  Yes, there's still a need for security in 2019.  Thank goodness Steve Gibson's here.



STEVE GIBSON:  Yes?



LEO:  What would we do without Mr. G, Steve Gibson of GRC.com, the Gibson Research Corporation - our amanuensis, our maitre d', our chief cook and bottle washer.  You enjoyed three weeks off, huh?



STEVE:  Yes, well, I'm apparently a little fuzzy because I got the episode number wrong and the year number wrong.



LEO:  In your show notes.



STEVE:  But I did get my facts straight, so I think that's really what...



LEO:  That's all that matters, Steve.



STEVE:  ...matters more than anything else.  There was no major theme that came through.  I mean, I did have three weeks' worth of news to catch up on because we skipped two episodes; I did.  There was no news for a total of three weeks spanning the weeks on either side of the two that we missed.  So I just labeled this one "Here Comes 2019."



LEO:  Don't hackers take the holidays off?



STEVE:  Actually, it did seem to be a little slow.  I think maybe everyone just said, "Eh, we'll hack things later.  We'll get around to it.  All those routers, those routers aren't going anywhere.  There are more of them every day."  We're going to take a look at the NSA's announced forthcoming release.  The upcoming RSA conference is in March.  They have announced they're going to release their own internal powerful reverse engineering tool for examining and understanding other people's code.  So we'll talk about that.



LEO:  Huh.



STEVE:  We've got emergency out-of-band, out-of-cycle patches, both from Adobe and Microsoft, that now have occurred, but I just want to touch on those.  And, yes, starting off 2019 with a bang, we do need to mention PewDiePie once again.



LEO:  Oh, dear.



STEVE:  It does not go away.  We also need to mention our prolific zero-day dropper, SandboxEscaper.  And Leo, you are going to love her travel photos.  Oh, my goodness.



LEO:  Oh, no.



STEVE:  Apparently she's a hiker, and she's got the coolest little tent.  It's like a little personal shelter tent kind of thing.  Anyway, I'm sure you'll be scrolling through them when we get down to that.



LEO:  I will.



STEVE:  But anyway, so actually there were two things that she did, and we didn't tell our listeners yet, but I killed another processor this morning, and that threw off my game a little bit.  The good news is I got a lot done last night and some this morning before I decided, oh, I really should reboot the system, and it didn't come back from the reboot.



LEO:  Oh, boy.



STEVE:  Anyway, I've scrambled around a bit.  But there are actually two new zero-days that SandboxEscaper dropped.  And I meant to get to the fourth one...



LEO:  Wow.



STEVE:  Yeah, I know, she's busy.  And somebody, I noticed somebody tweeting who saw the show notes already, who said, "Steve, you really shouldn't refer to her as a" - well, her as a she, or this person as a female.  Except I didn't know when - now I have her locked in here as a woman.  I didn't know the first time who this was, and somebody said, like, "We know this is a female."  So I adjusted myself for all the subsequent zero-day drops that she has dropped.  So until I actually learn definitively otherwise, now as a consequence, well, we'll get to this.  But her Twitter account and her GitHub account have both been suspended or canceled or deleted.



LEO:  Oh, wow.  Oh, wow.



STEVE:  Yeah, she's finally really upset people.



LEO:  It's fine if we don't - whether we know or not, the default doesn't have to be "he."  It could be "she."  That's fine.  Or "they."  I mean, who knows?



STEVE:  I do see people reading, I mean, I read people who write "she" for some reason when there's no reason to think either way.  And I think, whoa.  But I don't know, I guess I think of "he" as being more generic.  But anyway.



There's a new effort by the U.S. government to educate the industry about the risks of cyberattacks.  We have some welcome news on the ransomware front, some very cool announcement from BleepingComputer I'm going to talk about.  And some very welcome news about a new Windows 10 feature.  And also a note about, just as I was getting ready to wrap things up, the news of a new side-channel attack on OS page caches was announced, which I had no chance to look at.  It didn't look - I scanned it quickly.  It didn't look like it was going to be a big deal.  It was, like, really out on the fringe.  But I'll have more about that next week.



And then I want to wrap up with a brief update about my just-finished next and actually final phase of work on SQRL; and, as a consequence of something about that, my discovery of a very impressive and free large file transmission and sharing facility, which I needed in order to move a 6GB VM to a developer in Denmark.  And you probably know about it already, but I was very impressed with it, so I just wanted to give them a shout-out.  So something for everybody.



LEO:  Nice.  Now, I have delved into SandboxEscaper's blog.



STEVE:  Ah, okay.



LEO:  Where I found, I'm going to say "her," profile page.  She's transgender.  And it looks from the picture of it that she's chosen to be a woman.  But she doesn't state what pronoun she prefers.  So I think it's appropriate to say "she" unless she states otherwise.



STEVE:  Okay.



LEO:  Based on her picture.



STEVE:  And did you scroll down through her hike history?



LEO:  Yes, yes.  The blog is called "The Blog About Polar Bears."



STEVE:  Yeah, we don't know why. 



LEO:  I don't see any polar bears.  But she says:  "I currently do not own any social media accounts after losing @Evil_Polar_Bear.  People have tried to impersonate me in the past after having account banned.  So whatever, you're warned.  I'm a retired vulnerability researcher.  I make a living writing travel blogs now.  Besides that, I'm also transgender.  Hobbies include fencing and hiking long distance trails."  So what she doesn't do is say which pronoun she prefers, and certainly we would use whichever she prefers.  But given that it looks like she's decided she's a woman, so we're going to call her "her."



STEVE:  Well, and relative to polar bears, she did tweet a picture of what she decided was an angry polar bear.  And I looked at it, and I thought, it looks very comfortable to me.  It doesn't look like it's in any way upset with seals or dolphins or anything else in its, you know, penguins or anything.



LEO:  Maybe it's an ironic polar bear, probably.



STEVE:  Anyway.



LEO:  All right.



STEVE:  Never a dull moment on the Security Now! Podcast.



LEO:  Yeah.  Anyway, that's the deal from here, the view from here.



STEVE:  Thank you.  And in the meantime, I want a readout from you, Leo, on this tent that she is using.  It's very cool.  It's a little personal thing.  Anyway, see what you think.  Meanwhile, so speaking of passwords.



LEO:  Yes.



STEVE:  That is apropos of the Picture of the Week.  We have a picture of a little beagle sort of forlornly looking up at the person taking the picture, and the caption says:  "Someone figured out my password.  Now I have to rename my dog."  So sort of - is that cart before the horse or something?  Anyway, I do use LastPass, Leo, and I don't know any of my passwords anymore because we're not supposed to.



LEO:  No.



STEVE:  They're impossible to know.  And the only real downside is when I try to, you know, LastPass, I think my default is 20 characters of upper, lower, and special case and everything.  And something says, oh, you can only use 15.



LEO:  I hate it when that happens.  I hate that.  But then I'll use, like, 12 or 13 because I don't want to use 15.  You don't want any hints.



STEVE:  Ah, that's true.



LEO:  Does that make sense?



STEVE:  You've been paying attention, my friend.



LEO:  I have.



STEVE:  That's very true.  I didn't want to give away my secret, but you did.



LEO:  Never mind what we just said.



STEVE:  So on Podcast Tuesday of March 5th, so that's, what...



LEO:  Three months.



STEVE:  I'm really off my game.  February, March.  Two months from now, Robert Joyce, a senior advisor with the U.S. National Security Agency, will be offering his talk titled - this is at the RSA conference - "Come Get Your Free NSA Reverse Engineering Tool."



LEO:  Oh, geez.



STEVE:  Yeah.  Robert's talk abstract reads:  "NSA has developed a software reverse engineering framework known as GHIDRA."  And I did some googling, thinking, okay, I mean, it's got IDA in it, so that was a clue because that's Interactive Disassembler, IDA.  And so they kind of crammed - and Ghidra is something from Final Fantasy V, "an enemy fought at Ronka Ruins alone and can also be fought in the battle with an Alchymia.  It is a dangerous foe, and it has auto-reflect and battles using poison breath and lightning."



LEO:  Well, it was also a many-headed serpent in Greek mythology.  I mean, it was...



STEVE:  Well, that was just a hydra; right?



LEO:  Yeah, H-Y-D-R-A.



STEVE:  Yeah.



LEO:  So this is more letters.



STEVE:  G-H-I-D-R-A.  Yeah, we've got more letters in there.



LEO:  Oh, G-H-I-D-R-A.



STEVE:  Anyway, so whatever they have, it will be demonstrated...



LEO:  This sounds like GHIDRA, maybe GHIDRA.



STEVE:  Okay.



LEO:  I don't know.  We'll find out.



STEVE:  I don't know.  I'm sure there's got to be some Final Fantasy people around listening.  Sorry about that.



LEO:  So it's a Final Fantasy character.



STEVE:  Yeah.  So it's going to be demonstrated for the first time at the RSA Conference 2019.  He says:  "An interactive GUI capability enables reverse engineers to leverage an integrated set of features that run on a variety of platforms, including Windows, Mac, and LINUX" - and it's written in Java - "and supports a variety of processor instruction sets."  The whatever we call it, GHIDRA platform includes all the features expected in - this is the NSA bragging - "in high-end commercial tools, with new and expanded functionality NSA uniquely developed, and will be released for free public use at RSA."



LEO:  It was also in Vault 7.



STEVE:  Yes.  We first learned of it in the Vault 7 WikiLeaks.



LEO:  So we've had it for a while.



STEVE:  Well, we've known of it, but apparently it's been evolving.  So the WikiLeaks quoted version was 7.0.2.  We know that it requires Java 1.7.  So according to the Vault 7 documents, GHIDRA, for lack of a better pronunciation, was initially developed by the NSA in the early 2000s; and a Reddit user named "hash_define" who claimed to have had access to GHIDRA said that the tool had been shared with several other U.S. government agencies, mentioning the CIA.



LEO:  That's why it's in Vault 7.



STEVE:  Right, over the past few years.  So while there's no explicit announcement yet that the NSA plans to open source GHIDRA as opposed to just make it available, some believe the agency will also publish GHIDRA source code on the NSA's GitHub code repository, which we've mentioned a couple times.  It's an amazing trove over there.  Currently there are 32 released projects, NSA projects over on their GitHub port.  So the open source community may have access to this and be able to maintain it for free.



So for what it's worth, the buzz about this within the reverse engineering community is the promise of a good solid user interface, which apparently is what's missing, even from the IDA Pro, which is sort of the standard, although it's pricey.  And the belief is that it has a strong typing feature set.  I'll explain what that's about in a second.  But they're suggesting that this would fill an important gap that's currently lacking in the current reverse engineering tools.  IDA Pro that I've mentioned, where IDA stands for Interactive Disassembler, is the current favored tool.  And I'm sure it's pirated like crazy because consider its audience.  But if you license it, it's $1,866.99.



LEO:  What?



STEVE:  Yeah.



LEO:  How did they come up with that?



STEVE:  That was my thought, too:  $1,866.99.  That's like, okay.  That's what they want.



LEO:  I'm sure it means something.



STEVE:  So the company that produces IDA Pro is Hex-Rays, which is kind of cute.  And I'm sure that everybody but them is delighted at the idea of the NSA releasing something that apparently blows it away.  Okay.  So think about this.  For those of our listeners who don't know - so we're all familiar with the term IDE, Integrated Development Environment.  An IDA is sort of the reverse.  Its challenge, the IDA's challenge, the Interactive Disassembler's challenge is that the act of compiling source code discards so much of the programmer's originally supplied information that what you end up with, of course, is the famous pile of ones and zeroes.



So there have been mistakes made in the past where the debugging information has been almost certainly mistakenly left in the code.  That can happen, in which case basically you get the names of all of the things.  You get the names of the so-called "symbol table," which is the mapping between what the programmer called a variable and where the variable is located, what the programmer called a subroutine or a jump point and where that is.  But normally - and that's big.  That is, leaving the debugging information in the object code makes for a much larger result.  So it's almost always the case that that is stripped out.  So what you're left with is just ones and zeroes.  And so you're just looking at it, and it's just an opaque blob.



So the automated portion of the disassembler can analyze the program's flow.  It can essentially step through the code.  It's not actually doing what the code says.  It's not executing the code.  But it can step through it to find the branches and see where the branches branch to, and then essentially build a tree of all of the bytes in the executable that could be executed if every branch was followed and not followed, and every jump was taken, and every subroutine was called.  So it's able to do - that's typically called a "static analysis."  It doesn't run the code.  It analyzes it like crazy.  Essentially what that does is it creates a flow of the code, but it also shows what areas cannot be executed.  And so it assumes those are data.  And in some cases then it can go further.



Once it's sort of done that, then it can look and see, oh, look, we're moving something with an instruction that moves something to memory, and it knows the address of that.  So it's able to provisionally label that as a variable of some sort.  It doesn't know what to call it yet.  So it just gives it sort of a temporary name.  So the idea is that the automated portion is able to go through and, without understanding what the program does, without ever executing a single line, it's able to sort of unfold it, sort of unwrap it into pieces.  And it's able to say, you know, this is data.  These are instructions.



And we've showed snippets of this, of a disassembler output, many times in the past in some of our coverage.  It's able to then graphically create, like, blocks, where typically there's an arrow coming in, and then a block of code, and then maybe one or two arrows going out to places that block jumps to, that goes to other blocks.  So it can sort of allow you to move around graphically.



But the other thing that can happen now that we have operating systems with known APIs is that a good disassembler will see, oh, look, the OS is being called to read the registry here, because that's a known operating system call, and the parameters that that operating system call requires are known.  So that allows it to further reverse engineer and go, well, this has to be the handle to the key which has already been opened, so we can label that a registry key handle.  This has to be the name of the key that's being opened underneath the handle so we can label that now.



So as a consequence of the fact that we know everything about the functions that the program calls in the operating system, that again further really gives us a foothold on what's going on.  But there's a limit, of course, to how much can be done with automation.  It can't understand anything, but it can really go a long way toward setting things up and getting it ready to be understood, which is where the interactive portion comes in.  Then a human, a hacker, somebody at the NSA or somebody, basically, anyone who wants to reverse engineer and understand somebody else's compiled code, I mean, certainly all of the people that are taking malware apart in order to understand what it's doing and how it works, and we're talking about this all the time, they're using this sort of tool.



So then they'll take a look at one of these blocks and see what registry, for example, to continue the example I was drawing, see what registry functions are being used, look at the actual name of the key that's being read or written, and then think, aha.  And that tells them what's actually happening.



And so what you can do is you can, with all of the help that the automation portion was able to provide, you sit there, and you start filling in the blanks.  You say, well, if this registry key is reading this into here, then this here must be named that.  And so you start giving things names.  And as you do, the disassembler will propagate all the other instances of that throughout the code and, basically, although it's not completely automated, working interactively with one, it's very possible to make a really good stab at reverse compiling, essentially reversing the process of all of the information that was lost when the compiler discarded all of the labels that the programmer had put on things in order to make their own code intelligible to themselves, which the computer doesn't need.



So anyway, I'm sure we're going to be talking about this on the 12th, which is the podcast after March 5th where this will be announced and made available.  And I have a feeling it's going to be very popular.  I mean, if they've done, you know, they're touting this as commercial-grade GUI; and arguably it is that aspect of it, it's the ease of use GUI side which most of these reverse assembly tools don't really finish out.  So this is very exciting.  I think it's going to probably - and it's interesting that they're choosing to do this.  Maybe, Leo, as you noted, it's because, well, the cat was already out of the bag a little bit with the CIA WikiLeaks Vault 7 stuff.  So they thought, well, what the heck, let's formally announce it at RSA.  But very cool.  



LEO:  Yeah.



STEVE:  And kind of fun to play with, if you're interested in understanding how stuff works.



Last Thursday, January 3rd, Adobe released a pair of emergency out-of-cycle patches for their Acrobat and Reader PDF system.  The good news is that, while the bugs are rated critical and do allow for remote code execution and privilege elevation, Trend Micro, who found and reported the problems to Adobe, was unaware of any ongoing exploitation in the wild.  However, an attack leveraging these flaws only requires the victim to open a specially crafted PDF and remote code execution on your system with privilege elevation.  So really not good.



So this makes these high value targeted for phishing attacks, so they would be unlikely to be used widely.  They would be too valuable to just spray out in an email blast or something.  So if they were in use, it would be unlikely in this day and age that they would have been spotted by Trend Micro.  Which is to say we don't know that, because they haven't been seen, they aren't in fact in use.  So for what it's worth, if you haven't already, this was last Thursday, just check for updates in whatever version of Adobe Reader or Acrobat you may be using, and make sure you're up to date.  Again, unlikely that this would bite you unless you would be a target of a phishing attack.  But still, it's better to be safe.



And believe it or not, Leo, on this first 'cast of the new year, we're going to talk about, yes, PewDiePie.



LEO:  Oh, PewDiePie.



STEVE:  This time Chromecast users have had PewDiePie-pushing video content interrupting their viewing.



LEO:  Whoa.



STEVE:  If you can believe it or not.



LEO:  Wow.



STEVE:  Yes.  On Reddit, someone posted:  "TV randomly switching to some PewDiePie video."  He posted:  "Every 20 minutes or so,  my TV switches to some crappy YouTube video about PewDiePie with [and he said] rap music and a #ChromeCastHack hashtag."



LEO:  That's not annoying at all.



STEVE:  "Anyone know how to stop this?  It's driving me bonkers."  Now, Google's response to this from GraceFromGoogle, the Google Community Manager, Grace wrote:  "Hi everybody.  We know how frightening this is.  The good news is your Chromecast hasn't actually been 'hacked,'" in quotes.  "Rather, somebody was able to cast to your Chromecast due to an opening in your home network."  Oh, yeah.  That's a great comfort.  "This is the result of your router making some smart devices, including Chromecast, publicly reachable."  No, actually it's a consequence of Chromecast telling your router that has Universal Plug and Play enabled that it would like some ports mapped to it, pretty please.



Okay, so it is both.  It is Universal Plug and Play.  How many times must we say?  I will say that the instant it appeared, on this podcast we said turn it off.  This is really bad.  Turn it off.  And we know that Xbox people, that was a hardship for them because arguably this was created so that Xbox would be able to have ports mapped into it.  So anyway, what we have is the return of @HackerGiraffe.  Remember that twice now, Leo, once while you were gone, then the second time, for you the first mention on this podcast of PewDiePie, was his second hack of a greater number of printers which are exposed on the Internet, all sending out the - there was like, the No. 2 YouTuber was some Israeli or...



LEO:  Yeah, it's an Indian channel, Teapot or something, yeah, was starting to beat PewDiePie.



STEVE:  Right, right, right.  Okay, so...



LEO:  It worked, by the way.  I think PewDiePie has - T-Series has ceded its lead to PewDiePie.



STEVE:  That's what it was.  It was T-Series.  So anyway, on the FAQ for TheHackerGiraffe.com, they ask themselves:  "What's going on?  If you came here because you're a victim of CastHack, then know that your Chromecast/Smart TV/Google Home is exposed to the public Internet and is leaking sensitive information related to your device and home.  What information is leaked?  What WiFi your Chromecast/Google Home is connected to, the Bluetooth devices it has paired to, how long it's been on, what WiFi networks your device remembers, what alarms you have set, and much more.  What can hackers do with this?  Remotely play media on your device, rename your device, factory reset or reboot the device, force it to forget all WiFi networks, force it to pair to a new Bluetooth speaker or WiFi point, and so on.



"What can't hackers do?"  He says:  "Assuming the Chromecast/Google Home is the only problem you have, hackers cannot access other devices on the network or sniff information besides WiFi points and Bluetooth devices.  They also don't have access to your personal Google account, nor the Google Home's microphone.  They do have access to the noise level in the room," however, which is interesting.



Then the FAQ that they wrote for themselves asks themselves, "Who are you?  Your friendly neighborhood @HackerGiraffe.  We just want to have a bit of fun [uh-huh] while educating and protecting people" - of course, this is breaking the law, but what the heck - "and protecting people from open devices on the Internet.  We were also behind the #PrinterHack and #PrinterHack2.  Why are you doing this?  We want to help you, and also our favorite YouTubers, mostly PewDiePie.  We're only trying to protect you and inform you of this before someone takes real advantage of it.  Imagine the consequences of having access to the information above.  What do you want?  Well, fix your device.  And also subscribe to PewDiePie on YouTube.  Also Pyrocynical, Dolan Dark, and Grandayy."  I'm not going to spell it.



LEO:  This is why you're never going to succeed on YouTube.  You've got to be able to read these.



STEVE:  Wow.  "Don't forget good old Keemstar."



LEO:  Keemstar, whoo.



STEVE:  Okay.  "How do I fix my device?"  He says:  "Disable UPnP on your router.  And if you're port forwarding ports 8008, 8443, 8009, then stop forwarding them."



LEO:  Will your Chromecast stop working if you do that?



STEVE:  No.



LEO:  Okay.



STEVE:  "Thank you.  Any way to show support?  Yes," he says, "I, HackerGiraffe, have other things to do."



LEO:  Oh, good.



STEVE:  Please, yes.  "Use my free time teaching people cybersecurity and ethical hacking."  Okay.  "If you want to support personally or enjoyed this hack" - oh, yeah, it was enjoyable - "consider becoming a patron on my Patreon page."  And, by the way, I went, and it had been canceled or closed.  So I don't think that worked.  Okay.  So if you go to the page that is listed in the show notes, casthack.thehackergiraffe.com, you get a status page.  Total exposed devices, 72,341.  Of those, 8,254 were renamed, and 65,283, that is, the balance, have been forced to play video.  So more than 65,000 Chromecast or Smart TV devices received this annoying YouTube video.  That same number of 72,341 breaks down as 1,542 were Google Home devices, and the remaining 67,049 were Smart TVs or Chromecast devices.



So anyway, Google did say that this was the fault of UPnP being enabled.  This Grace from Google continued, after blaming it on UPnP, and I don't because this is Chromecast saying, "Open ports to me, please," she wrote:  "To make your network more secure, you can disable UPnP to avoid any unwanted content being played on your devices."  How convenient.  "The instructions are different from router to router, so we suggest checking with the manufacturer of your particular device.  However, this may affect other apps and devices that use UPnP to function."  Meaning Chromecast doesn't really need it; but we thought, what the heck, let's open some ports because why not?



So what I would note, there's been coverage of this.  As we have  mentioned before, the port mapping done by the UPnP API typically is hidden from the user so as not to confuse them, like wait a minute, I didn't map these ports.  So they don't appear in the UI.  What I would recommend, first of all, remember that GRC - the first time this became apparent to me on the podcast, Leo, that Universal Plug and Play was being enabled externally, it was like, okay, wait a minute, let's check for that.



So although this doesn't require an external presence, but our ShieldsUP! service still has, from that day on, a quick test for external access.  That's what the bad guys are using for proxying traffic and setting up bots and so forth, and now turning around and using it to look inside people's networks.  So you absolutely want that disabled from external access.  But this is internal use; and it is the case that, as Grace notes, that there may be some things you are doing on your network that will stop working if you disable it.  Typically, however, it is possible to manually configure the ports to the devices that need it.



The problem with having Universal Plug and Play enabled is there's - and this was the first point we made about it when it first hit our radar was there is no authentication.  There's no username, no password, nothing.  It is wide open so that anything inside your network has access to it if it's enabled and can do anything it wants to with your router behind your back.  I mean, it's unconscionably bad.  It always was.  And we keep seeing instances where it's biting people.



So the point is that, if you disable it, you should reboot your router.  The Universal Plug and Play mappings are typically dynamically made.  They are not statically stored in nonvolatile memory.  So a reboot should flush them.  But turning it off won't necessarily close these holes because it'll prevent new ones from being mapped, but probably leave the existing ones in place.  And if you're going through all that, take the time to check the router's manufacturer's website with the version number.



Remember that we've seen already an instance where the router itself wasn't aware that there was a newer version.  So even a router that is trying to check to see whether it should update itself may not have the latest information about what is available for its own firmware.  So check the manufacturer.  Always update to the latest.  Shut down Universal Plug and Play if you believe you can, and then do a reboot in any event so that, if you're trying to close things down, you have a chance of doing so.



LEO:  And don't forget to subscribe to Pyrocynical, Dolan Dark, and Grandayy.



STEVE:  That's right.  Give props to PewDiePie.



LEO:  And good old Keemstar.



STEVE:  So also...



LEO:  Sorry I threw you.  I apologize.



STEVE:  I did, I had one last...



LEO:  By the way, PewDiePie is beating T-Series, but it's really close as of seconds ago.



STEVE:  Wow.



LEO:  Yeah, PewDiePie has about 400,000 more subscribers.  He's got 80 million subscribers, by the way.  It's amazing, the number, although probably those aren't real people.  Or maybe they are.  I don't know how it works.  Yeah, he's got, if you really care, he's got 80,224,368 subscribers; T-Series 79,486,103 subscribers.  I know you're glad.  Now, I was just doing that to give you time to find your place.  Go ahead.



STEVE:  And I did.  Thank you, Leo.



LEO:  You're welcome.  See?  You see?  I'm helpful.  I'm helping.



STEVE:  What's annoying is that Google knows about this.  They have known about this for years.  There are a number of YouTube hacks dating from 2014 showing Chromecast being commandeered, like play something on your neighbor's Chromecast sort of thing.



LEO:  You have to be in physical proximity?  Or can you do it anywhere in the world?



STEVE:  Well, at least this way here, you can do it anywhere in the world because it's publicly open.  And that's what this guy did.  This HackerGiraffe, how many was it, 72,341 devices were found.  Of those, more than 65,000 had been forced to play videos.



LEO:  Geez.  Do you use Shodan to find that?  How would you find that?



STEVE:  Shodan may very well because we know that that's a tool he used to find the printer exploits before.  So he may have just said, oh, what else can I do to push PewDiePie?  Because, after all, it's neck and neck with T-shirt.  It's actually T-Series, I know.



LEO:  It's T-Series.  Pewd versus T-shirt.  Okay, Steve.  Back to you.



STEVE:  So you know I've been using, as a consequence of having killed another processor this morning, I've been using a laptop that I have Windows 10 on that I don't use very much.  And in the bit of browsing that I've been doing this morning, and even just now during this announcement, I was wondering, I wanted to get some better information on overclocking killing CPUs.  Websites are unusable without uBlock Origin.  I have no affiliation with them.  We've talked about them a lot, you know, Gorhill and...



LEO:  It's your adblocker that you love.



STEVE:  Oh, my lord.  Because I don't have it installed on this other laptop that I'm actually using right now because I just haven't ever used it very much.  But it's like, oh, this is what people tolerate on a normal basis?  So, uh-huh, yeah, I'm just saying, you know, uBlock Origin.



LEO:  It's true.  It's true.



STEVE:  Or whatever.  Boy.  So Microsoft issued an emergency out-of-cycle patch for IE.  It was a while ago now, so hopefully everybody has received and updated, although it's comprehensive.  It was the day after our last podcast they issued an emergency out-of-cycle patch to close a zero-day vulnerability in IE that was under active attack and being used to attack Windows users.  It was discovered by Google's Threat Analysis Group.  It's a remote code execution flaw in IE's JavaScript engine.  And we've often talked about how difficult it is to get these right.  You know, JavaScript is interpreting code that the browser downloads from wherever you go.  And all pages have it, are using it these days, so you're stuck with it.



Famously, back in the day, before uBlock Origin, it was NoScript, and it made things a lot saner, and I liked it.  But of course we gave up using it because you have to have JavaScript these days.  Everything is using code that your browser is running.  So when this particular vulnerability is exploited, it allows hackers or attackers to execute arbitrary code in the context of the current user, which is not as bad as also being able to elevate privilege, but you don't want random code running even under your context because it can still get up to mischief.  And if the current user were logged on with administrative rights, then the attacker who exploited the vulnerability could take control of the entire system - install programs, view/change/delete, create new accounts even for themselves.



So what makes this more worrisome is that our browsers, as I was just saying, run JavaScript code all the time.  And in this instance, not only is it a specially designed web page, it could be HTML email, a mail attachment, an MS Office document, a PDF file, anything else that supports embedded IE scripting engine content, and lots of things do.  And this exists in all instances of IE, from IE9 on Windows Server 2008, which would have been what version of Windows, 7 or maybe Vista, I think, IE10, IE11.  So just across the board.  So just make sure that, I mean, I would imagine a lot of us are using Firefox or Chrome and not IE.  But still the point is that it's the underlying engine component, not the browser itself.  It's the JScript.dll library, and other things will invoke that in your build of Windows, even if you're using another non-IE browser.



So here we are.  We are the second Tuesday of the month.  This is Patch Tuesday.  I have not had a chance to check to see whether this thing, well, we know that it was an out of-cycle patch, so I would imagine - my point was that, since then, and certainly if not before, then today, everybody's machines are getting caught up and patched.  And so definitely worth doing.  This is the kind of thing that would be sprayed on, you know, an advertisement on an unwitting web page could use this.  So this explains why Microsoft jumped the gun and put this out immediately.



And the day after that, SandboxEscaper strikes again.  And she's apparently not any happier, Leo, than we've seen her in the past.



LEO:  She's very angry, yeah. 



STEVE:  I had to blur the F-bomb in her tweet because, you know, I thought, okay, this is an adult podcast, and we all know what the other three letters are after "F."  So it's like, okay, why, really.  But still it seems appropriate.



LEO:  No, no, no.  We have youngsters who want to learn about security, too, and protect their little ears.



STEVE:  Yes.  So she was at that time tweeting as @Evil_Polar_Bear.  And Leo, you noted that that account had been closed.



LEO:  Yeah.



STEVE:  So she tweeted sandboxescaper.blogspot.com, which is where her blog is now.  She said:  "New zero-day.  My GitHub got taken down.  And screw it, I'm not going to get anything for this bug anymore.  So you can all go, you know, 'f' yourselves.  Bye, happy holidays."  Charming person.  Tweeted on the 20th of December.



LEO:  However, I have to say, pretty good at finding zero days.



STEVE:  Oh, Leo, that's what's so sad.  And in fact, the first link on her blog is a list of all of the CVEs that she's responsible for.  And so she's been active for, I think it was like three or four years, going back a ways, and has found a bunch of stuff.  And as I have said before when we've covered the two previous ones, looking at them, they were really good work.  I mean, it's very nice work.  So, yeah, you've got them on the screen now.  There's a nice chunk of work.  So I don't know what the back story is.  She's never happy when she tweets.  So we know people like that.



Anyway, so just to recap quickly, back in late August was the first of these that came to mind.  She exposed details and provided a proof-of-concept exploit for a local privilege escalation flaw in Windows Task Scheduler.  We talked about it at the time.  It was present due to errors in the handling of the Advanced Local Procedure Call service in Windows.  And a few days after the release of that proof of concept, a zero-day vulnerability was found, based on her work, actively being exploited in the wild.



Microsoft addressed it the following month in the September 2018 security patch, but people were being actively hurt in the interim before their systems were caught up.  Remember we offered - there was one of those little jiffy patches, whatever they were called, where there's a company that does the little quick patches in order just to fix it until Microsoft catches up.  But of course nobody who's not listening to this podcast knows about those or that there was a problem.



Okay.  Then, two months later, in October of last year, she released a proof of concept exploit for another privilege escalation vulnerability in Microsoft's data sharing that allowed a low-privileged user to delete critical system files from a targeted Windows system and then demonstrated how that could be leveraged into something damaging.



So anyway, here we are again.  There was even another one that, as I mentioned, I had intended to get to, but everyone gets the idea.  Unfortunately, she is not as well retired from this as we wish she were.  And apparently she either has a very good connection to the Internet while she's in the middle of hiking with polar bears on some frozen tundra somewhere, or she comes home and then does more about finding zero-days.



So this particular one, this zero-day, which did result in her Twitter and GitHub accounts being taken down because - and frankly, at the time, I remember being surprised that a damaging proof of concept was being left on GitHub.  I was kind of impressed that it was because, well, okay, I guess we're going to be fair.  But not anymore.  So this one is a zero-day in the MsiAdvertiseProduct.  And I am tempted to say, well, Microsoft, maybe you deserve this one.



LEO:  At least they're honest.



STEVE:  Yeah, right, they didn't name it something funky.  MsiAdvertiseProduct function of Windows that's responsible for generating, quote, this is their explanation:  "An advertise script or advertises a product to the computer and enables the installer to write to a script the registry and shortcut information used to assign or publish a product."  So this is a little glitch in Microsoft's Windows as a service feature set, which she found and has disclosed.  Due to improper validation, the affected function can be used to force the Windows installer service into making a copy of any file with system privileges and read its content, resulting in arbitrary file read vulnerability.



So anyway, another zero-day.  Microsoft, if they didn't fix it today, because it's now been about three weeks, I'm sure they are.  They're aware of it.  It was removed from GitHub.  Her Twitter account has been removed.  Maybe this is the end of it.  But she did have a run of four very nice zero-days over the last few months.  And the sad thing is she's finding important vulnerabilities.  Better for her to find them and just report them than turn them loose.  So I don't know.  It's unfortunate.



Okay.  What is fortunate is that the U.S. National Counterintelligence and Security Center, the NCSC, has begun distributing materials ranging from brochures to videos to privately held companies around the U.S.  These promote and encourage heightened awareness of the rising cybersecurity threats from nation-state actors.  Certainly, again, everybody on this podcast is well aware.  But your random companies in the U.S. are like, oh, well, you know, how's our security?  And the IT guys says, "It's great, boss."  And it's like, okay, fine.  Next item on the bullet point.  How about something that makes money for us?



NCSC Director William Evanina wrote:  "Make no mistake, American companies are squarely in the cross-hairs of well-financed nation-state actors who are routinely breaching private sector networks, stealing proprietary data, and compromising supply chains.  The attacks are persistent, aggressive, and cost our nation jobs, economic advantage, and hundreds of billions of dollars."  So this campaign provides detailed information on the growing threat from foreign state hackers.



The NCSC is an Office of the DNI that we've talked about, the Director of National Intelligence, which is designed to provide counterintelligence and security expertise in several areas, ranging from insider threat and supply chain risk management to personnel security.  So to push back against what they perceive as and we know is a growing threat to our commercial enterprises, the NCSC decided to provide the U.S. private sector with information it needs to understand and defend against cyber intrusions initiated by foreign governments.  In their tweet yesterday, Monday, January - what?



LEO:  I'm just looking at the card.  Go ahead.



STEVE:  Oh, I know, I know.  



LEO:  Go ahead.



STEVE:  I know.  In their tweet yesterday, on January 7th they tweeted:  "The National Counterintelligence and" - oh, and in fact, Leo, you should click the link because it's animated.  The actual image is a GIF.  They said:  "The National Counterintelligence and Security Center is today disseminating a series of videos, tips, and other materials to help U.S. industry guard against growing counterintelligence and security threats from foreign nation states and other actors."  And we have a link to the tweet, and the tweet has a pic at the bottom of it.  I managed to capture it after it had fully populated because it's five bullet points, and they populate with some animated glee.  First one:  Strengthen your...



LEO:  I can't get it to play, unfortunately.  I think I have video blocked everywhere somewhere.



STEVE:  It's just a GIF, though, so it ought to - but it's good.  I'm glad that it's not playing, Leo.  That's good.



LEO:  Yeah, it's good security.



STEVE:  Yes.  So "Strengthen your Passwords."  And they have a capital P-@-$-$-w-0...



LEO:  In other words, use leet, and all will be well, yeah.



STEVE:  Exactly.  That's right.  No one's ever going to figure that out.  "Lock down your social media accounts."  Okay.



LEO:  What do you mean, lock them down?



STEVE:  Exactly.



LEO:  Like use a password?  Okay.



STEVE:  Exactly, what does that mean?  "Delete suspicious emails."  Instead of what, like archive them?  Click on them?  Hold them?  What?  Yeah.  This is cutting-edge security.



LEO:  Oh, my god.  This is security for fifth-graders.  Oh, my god.



STEVE:  Yeah.  Oh, and look at the title bar.  "Know the Risk.  Raise Your Shield."  Doot-doo-doo.  Oh, number four.  "Don't expect privacy when you travel," especially from U.S. border agents.  Yes.  Especially when you're trying to come back in the U.S.  Good luck to you.  We covered this, what, last podcast?  Yes, they are downloading your personal data, your private data from your phone, and not deleting it from their thumb drives.  That's right.  And then number five:  "Know who you're talking to."  What?  Okay.  Anyway, yes.  More tips coming in the future.  We'll have six through 10 in our next GIF slide.



LEO:  I've got a tip I should pass along to the lawyers who put out the redacted Manafort filing.



STEVE:  Uh-oh.



LEO:  Wait a minute, I've got to show you this one.  This just broke.  It's from @nycsouthpaw.



STEVE:  It is unredactable?



LEO:  It's unredactable.



STEVE:  Oh, I hate when that happens.



LEO:  Unredacted redaction.  Basically, he or she says you can copy the black highlighted redactions in the Manafort team's filing, paste it somewhere else, and then see what it says.



STEVE:  Yeah.



LEO:  Nice job with the redactions.



STEVE:  That's right.  What you want to remember to do is to print the result of the redaction, and then that's what you use.



LEO:  Or maybe just use a felt pen.  I don't know.  There's ways to do it.  Holy cow, yeah.



STEVE:  Wow.  So this announcement, this cutting-edge announcement from your NCSC government agency working to protect you and remind you to shield yourself from its own border agents comes on the heels of a statement made before the U.S. Senate Judiciary Committee last month by Bill Priestap.  He's the Assistant Director of the FBI's Counterintelligence Division.  He said:  "Many American businesses are just now starting to understand the new environment in which they are operating.  The continued proliferation of cyber hacking tools and human intelligence capabilities means that this environment will only become more hostile and more treacherous for our companies.  Our businesses face competitors in the form of foreign enterprises assisted or directed by extremely capable intelligence and security services."  So anyway, everybody worry.  Thank you very much.



LEO:  Aye-aye-aye.



STEVE:  Thank you very much.  Okay.  So, Leo, there's another picture for you here.  As we know, Apple no longer attends the industry's annual Consumer Electronics Show - that is, they don't set up a big booth and do that - which is normally held and has historically been held in Las Vegas every year because Apple is large enough now to create their own shows.  But they couldn't resist this year plastering a huge graphic on - it's one of those, like, it's black and white, but it's one I guess where it's like a tint or perforated or something?



LEO:  Yeah.  You can see through the windows, yeah.



STEVE:  Yeah, because the people in the rooms of this hotel have to still be able to see out.



LEO:  By the way, note the hotel.



STEVE:  I know, I know, I know, I know, I know.  That's the little glitch here.  So we'll ease into this.  So this year, while still absent from CES, Apple decided to have some fun with the famous "What happens in Vegas, stays in Vegas" slogan, at the intended expense of their less privacy enforcing consumer electronics competitors, probably Amazon and Google, who as we know both last year suffered from some embarrassing privacy slips.  So this huge, I mean, it is the entire side of the hotel, black and white, building-sized graphic facing the Las Vegas Convention Center, shows the distinctive outline of the iPhone with the slogan "What happens on your iPhone, stays on your iPhone."  And then it has apple.com/privacy at the bottom.



However, as has been noted, Leo, by you and the industry, there's a tiny and somewhat ironic aspect to this which has been noted.  The building-size graphic appears on the side of a Marriott Hotel, which as we know suffered its own embarrassing, years-long, or at least years-old, data breach which we learned of last year, as a consequence of them having purchased the Starwood Properties which had - what was it, 2014?  I think Marriott bought it in 2016; but in 2014, two years before, they had suffered a breach.  And I did see a little thing that didn't really make it into our bullet points for the podcast, but it was that the passport numbers, I want to say 518-some thousand passport numbers were leaked from that breach.  So Marriott had reduced the size of the breach a little bit, but also said, yeah, we did, we have to confess, 518-some thousand passport numbers were divulged.



Okay.  Now, the good news from our friends at BleepingComputer.  We recently, it may have been the last podcast of 2018, ran through some of the many variations of ransomware that had all been derived from a single poorly written starter which was on GitHub, and the fact that someone named Michael Gillespie was creating a series of decryptors for the ransomware descendants of that poorly written starter because, being poorly written, it was possible to decrypt the file contents without paying the ransom.



Since then, BleepingComputer, which as we know closely follows and reports on this ransomware universe, has been following and reporting on Michael's subsequent developments.  On the second of January, Lawrence Abrams, the founder of BleepingComputer, posted:  "How to Decrypt the FilesLocker Ransomware with FilesLockerDecrypter."  Although Michael Gillespie did not produce a decryptor for this ransomware, well, not as a descendant, it turns out that on December 29th an unknown user released the master RSA decryption key for version 1 and 2 of FilesLocker, which allowed a decryptor to be produced, which has been done.  Then, two days later, on the fourth of January, Larry again posted on BleepingComputer:  "How to Decrypt the Aurora Ransomware with AuroraDecrypter," and provided a complete walkthrough of the use of Michael's decryptor for Aurora ransomware.



Anyway, so I was doing a bit more digging and catching up on everything that had happened since our last podcast, and I discovered an article that Lawrence had posted on Thursday the 20th, two days after our last podcast:  "BleepingComputer.com is now a partner with No More Ransom."  And Leo, you're going to want to click on NoMoreRansom.org.  I've got the link on the next page in the show notes.



Larry wrote:  "BleepingComputer is humbled and honored to announce that we have joined the No More Ransom project as an associate partner.  We've been providing ransomware information, support, and the amazing decryptors from Michael Gillespie since the beginning, and this partnership will enable more victims to receive the help they need."  And indeed it will.  He says:  "No More Ransom project is a joint project created by Europol, Politie, and McAfee to provide information and assistance to those affected by ransomware.  Since its creation, numerous other law enforcement agencies, security companies, and supporters have joined the project, which now supports 35 different languages."



In fact, when you go to the home page, NoMoreRansom.org, you are asked to choose a language, which of 35.  And so /en/ then takes you to - there is a decryption tools listing with a long list of tools that can be used for decrypting ransomware, if you are, and I guess I would say "lucky enough" to be encrypted with something that can be decrypted without paying the ransom.  So I wanted to put this on everybody's radar, NoMoreRansom.org.  If you or anyone you know or care about is hit by ransomware, there's a chance, I mean, first thing is you don't want that to happen.  You want to be safe about it happening by somehow arranging to have really, really current backups.



And although I'm annoyed, for example, that I fried another processor this morning, I am not the least bit worried about any loss of data.  My backups' backups have backups, and the images' images have images.  And so I am, like, after having been caught by my XP machine dying last year, I'm not going to ever be in that position again.  So I'm good.  I even have drives that are not online, but briefly come online and then disappear so that, if anything did get me, it would have no way of knowing that there was a drive that technically was accessible that cannot otherwise be accessed.



So, I mean, I take this danger seriously.  In my opinion, this is the biggest concern that exists now is the threat from software that encrypts, I mean, basically it's like potentially losing all, I mean, not just a drive crash.  That we can recover from.  We've got SpinRite.  Or the motherboard dies.  And that's fine.  You still have your drives.  But, you know, the idea of something trying to get into your system and to maliciously encrypt your data, no one wants that to happen.  That's worse than a virus.



But remember, this only works on poorly implemented ransom crypto.  If the crypto is done right - and the first ransomware, before this became a fad, the first ransomware, as we discussed at the time, was done right.  A high-entropy symmetric key was obtained.  It was used with AES-256 cipher with a varying initialization vector, which was stuck on the front of all of the encrypted files in order to do proper encryption of the byte stream that the file represents.  And then that symmetric key was completely wiped and removed from the system.  There was no trace of it left behind.  You had to pay the ransom in order to get your data back.



So understand that this isn't universal decryption of ransomware.  It's only if the ransomware that you happen to get bit by was not done properly, not done correctly, that you are able to back yourself out.  But it's worth knowing, I mean, it's cool that we now have NoMoreRansom.org as an aggregation site for these tools.  And props to BleepingComputer for joining up with them and helping them to do a better job by funneling Michael's work to them.  Very cool.



Last August we covered the "mixed blessing" news that Microsoft was introducing something that I thought was really tasty.  At the time they called it "InPrivate Desktop."  And their description at the time read:  "InPrivate Desktop (Preview) provides admins a way to launch a throwaway sandbox for secure one-time execution of untrusted software, basically an in-box speedy VM that is recycled when you close the app."  And under prerequisites, and here's where I was disappointed, first bullet point under prerequisite, "Windows 10 Enterprise."  And it ran on builds 17718 and beyond, any branch.  It required hypervisor capabilities enabled in the BIOS, at least 4GB of RAM, 5GB of free space, and two CPU cores.  So this was very cool, the idea of having an easy-to-use, readily accessible, throwaway VM for safely doing stuff.  But of course it was also disappointing because who among us has Windows 10 Enterprise?



We're discussing this again because, shortly before Christmas, Santa Microsoft left news of a welcome present under our Windows 10 tree.  I've got the link to the announcement in the show notes.  Microsoft's Hari Pulapaka, I guess that's how I pronounce his name, he posted:  "Windows Sandbox" - it's renamed.  "Windows Sandbox is a new lightweight environment tailored for safely running applications in isolation."  His posting says:  "How many times have you downloaded an executable file, but were afraid to run it?  Have you ever been in a situation which required a clean installation of Windows, but didn't want to set up a virtual machine?



"At Microsoft we encounter these situations regularly, so we developed Windows Sandbox - an isolated, temporary desktop environment where you can run untrusted software without the fear of lasting impact to your PC.  Any software installed in Windows Sandbox stays only in the sandbox and cannot affect your host.  Once Windows Sandbox is closed, all the software, with all its files and state, are permanently deleted."



And then five bullet points:  "Windows Sandbox has the following properties:  Part of Windows.  Everything required for this feature ships with Windows 10" - and here it is - "Pro and Enterprise.  No need to download a VHD.  Pristine:  Every time Windows Sandbox runs, it's as clean as a brand-new installation of Windows.  Disposable:  Nothing persists on the device.  Everything is discarded after you close the application.  Secure:  Uses hardware-based virtualization for kernel isolation, which relies on the Microsoft's hypervisor to run a separate kernel which isolates Windows Sandbox from the host.  And lastly, efficient:  Uses integrated kernel scheduler, smart memory management, and virtual GPU."



And under prerequisites now, Windows 10 Pro or Enterprise Insider Build 18305 or later.  Oh, actually they reduced the requirements, too.  It does require a 64-bit architecture, virtualization capabilities enabled in the BIOS, at least 4GB of RAM (8 recommended), 1GB of free disk space - that's nice, that's down from five - and at least two cores.  So this is extremely good news, I would imagine, for all of us listening to this podcast, that Windows 10 Pro will soon have this feature.  It's currently shipping in the Fast Ring of 18305, which is also known as 19H1.  And it's hidden under that Programs and Features dialog.



So you go to Settings > Apps > Apps and Features, then Programs and Features, which gives you a dialog you could stretch out where there's a bunch of checkboxes, and look for - you want to Enable Windows Sandbox.  Then you close it, it rummages around for a while, and then you'll have it.  And very, very cool that we're going to have this built into Windows 10 Pro.  So congrats to Microsoft for this not just being in Enterprise.  I'll bet you that they took a look at this and thought, okay, this is definitely worth giving to everybody.



And as I mentioned before, there's a new page cache side-channel attack.  From the looks of it, it doesn't look like a big deal.  The page cache is something that, being a cache, that our OSes use in order to boost performance in a number of ways.  I will dig into it, and I will have news either way, to either confirm that it's not a big deal or get a sense for what it means.  The news just hit as I was putting the podcast together.



So I wanted to share a fun testimonial that I found from someone named Dunbar Pappy.  He said:  "Steve, we'd swapped emails a few years ago, but today I write with my profound thanks for your SpinRite creation."



LEO:  Aww.



STEVE:  Yeah.  Well, and get this.  "Recently I'd been transferring all my patent work and other high-value documents and settings to an older Acer laptop with Ubuntu OS, which I use as a dedicated secure workstation.  During this massive, multi-platform undertaking, an OS update came through; and, after restarting, the Linux system was completely locked up."



LEO:  Oh, great.



STEVE:  Yeah.  "Few files would even open, and no system operation would execute.  It wouldn't even shut down.  I envisioned hours of reconstructing files from other backups," he says, "(if even possible), a massive headache with profound monetary implications if unsuccessful.  I decided to run my years-old copy of SpinRite 5 on it."  And just so everybody knows, I mean, SpinRite 6 was released in '04, so...



LEO:  Where did he get 5?



STEVE:  SpinRite 5, he's had it for more than 15 years.  He says:  "I decided to run my years-old copy of SpinRite 5 on it, just to reassure that the bits would be readable with my adapter transfer hardware during the laborious HDD swap-and-read attempts.  Finished after 12 hours of plodding along, I decided to reboot the Acer just to see if anything would even function.  Then lo and behold, it was all systems go, files recovered and system operations normal."  He says:  "The English language is inadequate to express my relief; and, further, in today's techno-centered disposable world, it is nearly impossible to find any product that actually does what it says it will, and at a fraction of the cost compared to the alternative.  But SpinRite is one that delivers.  My eternal thanks, Steve.  Dunbar Pappy."



LEO:  Yay.  What a great story.  Nice.



STEVE:  So the point is, you know, you write the software right, and it doesn't die, and it's still useful years later.  And, yes, as soon as SQRL is behind me I will be back to updating SpinRite 6, since nobody doubts it's time for me to do that.  But as we know, it's still helping a lot of people in the meantime.



What I wanted to mention about SQRL was that I have finished all of the work on a piece that I didn't expect to have to do.  Having done it now, I'm glad.  Which is, as I've mentioned before, I want to put up web-based forums for SQRL users who are wanting to understand it better, who want help with this or that SQRL client; for developers who want feedback on the work that they're doing.  Basically, the equivalent of what we now do almost offline in our old-school, Usenet-style NNTP newsgroup.  I wanted to create a website.  The problem is that this thing is, well, it doesn't obviously natively support SQRL.  And how could I have the SQRL web forums not allow you to use SQRL to log in?



So I mentioned this on the podcast.  I had also posted a couple questions quite a while ago to the developers of this - I chose the XenForo forum software.  I like it a lot.  I don't regret the choice.  This is like the third iteration of forum software they've written, and it shows.  But it's written in PHP.  And so what I was looking for was a couple questions because I assumed I would have to integrate SQRL into their forum software.  I found a reply from an existing developer in Denmark who is a Security Now! Podcast listener.  He's been listening, well, he's been coding since he was 15.  He's now 29.  And he's been listening for years.



He said:  "Hey, I know XenForo.  Do you need any help?"  Well, it would be crazy to implement SQRL from scratch in PHP.  Actually, it would be wonderful if someone eventually does it.  But that didn't seem to be the shortest path.  So what I decided to do was to create something that didn't exist yet, which was to create and define an API for an external SQRL authentication service that would allow any integration into a web server with virtually no knowledge of SQRL.  That is, to basically move all of the work over to the SQRL API so that it would then be easy to make a couple calls to have all of the SQRL crypto done by this API.



So if anyone's curious - and Leo, you could bring it up right now if you were interested - there's a link in the show notes:  GRC.com/sqrl/sspapi.htm.  That's the SQRL Service Provider API.  And so that page documents the API which now exists.  And in fact we have a demo site, actually two demo sites, which are using the API, which I use in order to bring it up and verify it.  And the reason this is a little bit interesting is that this developer is not a Windows user.  He runs on Mac and has some Linux stuff around.  But of course the API, my implementation of this API, it's my hope, for example, that the API will be implemented in Java and in other things because this creates a uniform interface to any web server that wants to avail itself of SQRL authentication, really minimizing the work that has to be done on the server side.



But the developer needed to use the API that I've written.  Of course, I wrote it in assembly language for Windows.  So it turns out that we could have used an unlicensed version of Windows, but Windows 10 allows you to install it without a license key.  It asks you to eventually license it, but it runs forever, apparently.  It disables some of the configuration stuff, you know, like the desktop background.  And if you go to the control panel and try to do something, it says it's all disabled, and says you should license this copy of Windows.  And it works just fine.  And if you've been around Windows for a while, you know that you can pretty much do anything you want to with the registry and group policy.  So needless to say, I have it running just the way I want.  All of that incredible videogame junk that's in Windows 10 when you install it now - Leo, have you seen a fresh install of Windows 10 recently?



LEO:  I have a few here and there, yeah.



STEVE:  It's unbelievable.  It's like, how do people tolerate this?  I just cannot believe it.  Anyway, mine is all stripped down.  There's nothing in it except a Windows core.  There's also something called IIS Express, which is a really cool IIS server which just runs as a desktop app.  So I configured everything, put the API in it, bundled it up into a VM because he has VMware.  I think he's using Fusion, and it works just fine there.  So there was all that.



Then the problem was how to get it to him.  I thought, okay.  First of all, it was slow.  I thought I'd stick it on my server, and then he could grab it.  But it turned out that that was pretty slow.  Then I thought, okay, I'll stick it up on Amazon.  Well, it was 5.8GB for the super-minimized, compressed, optimized, dejunkified, like it was really stripped, 5.8GB.  Amazon has a limit of 5GB for files on AWS.  So I looked around.  All of this by way of saying I found a very cool service, Leo.  You probably know of it, but I wanted to make sure our listeners did.  It's called Filemail.com.  The guys are in Oslo, Norway.  It is free forever for files up to 50GB.  Fifty, 50GB.



And I'm always suspicious of something free.  Why is it free?  Well, they offer for commercial entities or people who want to keep files there longer or bigger than 50GB, I can't imagine who, but for $15 a month you get a bunch of additional features, like you can password protect the file.  Of course I RARed this in order to - with a large dictionary and used a RAR password, since the file was going to be leaving my control, so that it would be protected.



But anyway, I wanted to make sure people knew.  I was so impressed.  I don't remember now the time of day it was.  But I went to Filemail.com.  I chose, rather than sending email, I wanted a link.  I dragged and dropped this 5.8GB file on the browser, and it saturated my upstream cable modem at 33Gbps straight.  I mean, it could not, I mean, I have a 30GB upstream bandwidth on my cable modem.  It saturated it at 33.  I was very impressed.  And he got the same saturation performance at the other end when he downloaded it.



For free you're able to allow - you can select how long you want the files to stay there - a day, a week.  I don't remember now, but I remember that it was either one day or one week.  Maybe there were other choices.  If you check that you want to password protect, it tells you that you need to be a licensed user, you know, you need to register and do the pay as you go.  But, boy, I see no downside to using this for sharing large files, if you just want to send somebody a link with no privacy - the reason I didn't want to have their service do this by email is I didn't want to divulge even his email address.  So I don't see any downside to this.  This looks like a great service.  So I wanted to make sure our listeners knew of that.  And there's our podcast.



LEO:  C'est fini.  Thank you, Steve.  A great beginning to 2019.



STEVE:  Yes.  I will be beginning to - I'm not far away from being able to tell people where to go to get a copy of SQRL and play with it, I mean, to make it public and start having people get to know what it is I've been doing for the last five years on this crazy project.



LEO:  Five years.  Wow.



STEVE:  Yes, it is.  And then get back to, yes, get back to SpinRite, everybody.  I absolutely can't wait.



LEO:  Steve's show, Security Now!, comes to you every Tuesday.  We do it about 1:30 Pacific, 4:30 Eastern, 21:30 UTC.  You can listen or watch as you choose, live at TWiT.tv/live.  If you do that, join us in the chatroom, irc.twit.tv.  You can also get on-demand audio from Steve's site, GRC.com.  That's where you'll find, not only SpinRite, the world's finest hard drive recovery and maintenance utility...



STEVE:  Going strong after 35 years.



LEO:  Yeah.  Also everything you ever wanted to know about SQRL, Perfect Paper Passwords, Password Haystacks, ShieldsUP!, and on and on.  Somebody called me the other day about, believe it or not, the Click of Death.  They called the radio show about the Click of Death.



STEVE:  Wow.  And you knew all about it.



LEO:  I did, and I referred them to your Trouble in Paradise application.



STEVE:  Yup.



LEO:  See, there are still people with ZIP drives out there.  I don't know why.



STEVE:  I'm sorry.



LEO:  GRC.com.  He also has transcripts, which is always a good way to listen to the show is you can read along.  We have audio and video at our site, TWiT.tv/sn for Security Now!.  Or subscribe in your favorite podcast application.  That way you'll get it the minute it's available.  We're going to send this off to the editors now.  It should be out in just an hour or two.  So if you subscribe, you'll have it hot and fresh off the presses, ready for your commute tomorrow.  Steve, have a great Tuesday, and I will see you next week on Security Now!.



STEVE:  Thank you, my friend.  Till then.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#697

DATE:		January 15, 2019

TITLE:		Zerodium

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-697.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm 



DESCRIPTION:  This week we examine the intended and unintended consequences of last week's Windows Patch Tuesday; and, speaking of unintended consequences, the U.S. government shutdown has had some, too.  We also examine a significant privacy failure in WhatsApp, another ransomware decryptor (with a twist), movement on the DNS over TLS front, an expectation of the cyberthreat landscape for 2019, a cloudy forecast for The Weather Channel App, a successful 51% attack against the Ethereum Classic cryptocurrency, another court reversing compelled biometric authentication, and an update on the lingering death of Flash, now in hospice care.  We then look at a bit of miscellany and errata and finish by examining the implications of the



recent increase in bounty for the purchase of zero-day vulnerabilities.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Lots of security news.  He'll have an update on last week's Patch Tuesday for Microsoft Windows users.  We'll talk about a major court decision when it comes to unlocking your smartphone.  And, finally, who's willing to pay up to $2 million for zero-day exploits?  Zerodium.  We'll have all the details coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 697, recorded Tuesday, January 15th, 2019:  Zerodium.



It's time for Security Now!.  Yay.  The best part of your week has just begun, featuring this fellow right here, Mr. Steve Gibson, the host and moderator of our show.  Hello, Steve.



STEVE GIBSON:  And a big surprise this week, Leo.  I know not only what episode number this is, but also what year it is.



LEO:  Congratulations.



STEVE:  And those will all be correct on the show notes and in all communication going forward.



LEO:  Yay.



STEVE:  Normally I have a hard time - well, not normally.  Sometimes I have a hard time focusing down on a single topic.  And actually there was some competition because there was a lot of interesting news this week.  But the huge inflation of reward for zero-days paid by Zerodium - also sort of I thought this was apropos because we've been talking about SandboxEscaper, who seems so annoyed by the fact that she's unable to cash in her zero-days.  It's like, what?  Anyway, and again, we've talked about this before, but I wanted to kind of come back to it, especially in light of the fact that, well, I'll save it.  I won't step on this.  But the bounty has gone insane.  And it really does sort of create an ethical question, I think, about the idea of an entity paying an insane amount of money for zero days, which is what Zerodium purchases.  And it's like, okay.  So anyway, we're going to talk about that.



But before we get to that, we've got some intended and unintended consequences of last week's Windows Patch Tuesday.  And sometimes I'm feeling self-conscious that we don't talk about Patch Tuesday on Tuesday, which is when the podcast is.  It turns out that the aftermath of what happens is almost more interesting than the news of what's being fixed.  It's what the patches broke and what the remediation is for those.  Because Microsoft, anyone who listens to Windows Weekly with Paul and you and Mary Jo on Wednesdays is knowing how they're now feeling about Microsoft's attempts to keep this sinking ship afloat.  Anyway, so we've got intended and unintended consequences of last week's Patch Tuesday.



Also, speaking of unintended consequences, the U.S. government shutdown has had some, as well, that we'll talk about.  We've also got a significant privacy failure in WhatsApp, which of course is based on the Signal protocol, which itself is excellent.  But someone's found somebody else's communications in her new phone, and we look into and explain that.  And we actually have a corollary to the TNO - Trust No One - slogan for this podcast as a consequence.



We've got another ransomware decryptor with a twist; some forward motion on the DNS over TLS front, that is, this general movement to move DNS away from unencrypted UDP over into encrypted and authenticated TLS or HTTPS.  We've got two different takes on expectations of the threat landscape for 2019.  And of course I couldn't resist saying a "cloudy forecast" for The Weather Channel app.  I heard you also referring to them in the last week, so we'll touch on that.  And then also one of the things that was competing for the title of this week's podcast is a successful, what's known as a "51% attack."  In this case, it was against the Ethereum Classic cryptocurrency.  We'll talk about that, and also a little bit of the background behind 51% attacks, which we haven't touched on for quite a while.



I heard you mention at the end of MacBreak Weekly something else that we need to talk about, which is another court reversing compelled biometric authentication.  We have an update on the lingering death of Flash, now in hospice care.  And then we'll take a look at a bit of miscellany, some errata, and finish by examining, as I mentioned at the top, the implications of this recent stunning and literally headline-pulling increase in bounty for the purchase of zero-day vulnerabilities.  So I think another great week for our listeners.



LEO:  That's Zerodium.  That's what that's all about.



STEVE:  Yeah.



LEO:  Zero-day-ium.



STEVE:  Right.  So our Picture of the Week is just - it's one that I had in my collection of images that people have sent me over time.  And it was apropos of one of the stories that we'll be talking about.  I shook my head when I saw this because this is for a domain, RelianceCP.com, and the tab is Reliance Capital Partners.  And I haven't gone there recently, and I don't know if this is the image you get.  But it makes it very clear that they are still one of those old-school Flash-based sites.  Apparently when you go without Flash, you get a big banner that says:  "To enjoy this site" - because of course enjoyment is the goal - "you'll need to update your Flash Player.  It's easy, painless, and will take just a moment."



Then, of course, they pretty much go against that by saying:  "1.  Download and install the latest version of  Adobe's Flash Player."  Then two starts with "Unfortunately," which is really not the first word you want in the second step of "easy, painless, and will just take a moment."  But "Unfortunately, you'll then need to close your web browser," which means you are unable then to read steps three and four because your web browser is gone.



LEO:  Memorize them.  Memorize them.



STEVE:  Yes.  Then "Go back to this site."  And there is this whole issue of retention which they're sort of fighting against here because maybe they were hoping you would stay if you just came by mistake.  But anyway, "Go back to this site after you restart your web browser."  And then "4.  That's it.  Have fun."  And of course that is if you then survive what it is that having Flash loaded in your system - because obviously you didn't have it before.  And in order to go to RelianceCP.com, at least when this picture was taken...



LEO:  No, no.  I just went there.  This is the current site.



STEVE:  No.



LEO:  What's really frustrating, by the way...



STEVE:  Oh, boy.  Wow.



LEO:  No, that's it.  What's really frustrating is you don't get anything if you don't have Flash.  I mean, you don't get - they don't degrade gracefully.  They just don't show you anything.  So I don't know...



STEVE:  So again, we remember those days where somebody built an entire website in Flash.  And so...



LEO:  That's what this is, obviously; right?



STEVE:  Yes, exactly.  And so the assumption was those browsers, you know, who wants text?  We want Flash.  And so, okay.  And actually this - I was really tempted just to jump down, I don't even know how deep it is in our notes, because I talk about this.  Well, we'll get there.  Anyway, great Picture of the Week.  And, boy, whoever you are, Reliance Capital Partners...



LEO:  Really they don't want any viewers because I don't think - is there any modern system that has Flash on it by default?  No.  My Windows 10 machine doesn't.  My Mac doesn't.  No iOS device does.  No Android device does.  So basically what they're telling people is go away.



STEVE:  Yeah, really.  If you're visiting this century...



LEO:  I don't know who these guys are, but they mustn't get much web traffic.



STEVE:  I'll bet they're kind of thinking, gee, I wonder what happened?  Maybe the Internet just kind of was a fad after all because no one's coming by anymore.



LEO:  It's a real estate investment company focused on acquiring, repositioning, renovating, and stabilizing multi-family properties.  I only know that because of the Google cache.  Oh, boy.



STEVE:  Yeah, boy.  Well, that's how to make sure you don't get any business.



LEO:  Yes.



STEVE:  Okay.  Last Tuesday was the second Tuesday of the month, the earliest second Tuesday possible being the 8th, and 51 vulnerabilities were fixed.  So a good number of vulnerabilities for the first Patch Tuesday of the new year.  Seven of them were rated critical.  However, it also broke Windows file sharing for Windows 7 and its matched companion, Server 2008 R2.  But we'll get to that in a second.



First the good news.  There was a surprising DHCP vulnerability discovered internally by one of Microsoft's own guys in their enterprise security group, and it could allow an attacker to send a specially crafted DHCP response to a client to perform arbitrary code execution.  Now, that's scary because DHCP is sort of promiscuous.  I mean, it's on all the time if you are in any normal mode, especially if you have a laptop, and you're roaming around.  When you go into the airport or the coffee shop or pretty much anywhere, it's DHCP which is reaching out and saying, "Hey, I'm somewhere new.  Give me an IP.  And it turns out that a malicious DHCP server could have been, for Windows systems, installing code on those systems.



They wrote:  "A memory corruption vulnerability exists in the Windows DHCP client when an attacker sends specially crafted DHCP responses.  An attacker who successfully exploited the vulnerability could run arbitrary code on the client machine.  To exploit the vulnerability, an attacker could send a specially crafted DHCP response to the client."  So the good news is they discovered it internally.  They have no knowledge, as opposed to it being found by one of our AV companies in the wild being used, and so hopefully foreclosed this before anyone got bit by this.  So that's good news about last Tuesday's updates.



They also found a pair of Hyper-V vulnerabilities which could have allowed powerful escapes from the virtualized containment.  Microsoft wrote:  "To exploit the vulnerability, an attacker could run a specially crafted application on a guest operating system" - meaning in one of the Hyper-V VMs - "that could cause the Hyper-V host operating system to execute arbitrary code."  So not only an escape, that is, like poking a hole, but actually getting the external Hyper-V host to do something for you, so not good.  And those are closed.



There were also three other critical flaws were patched in the ChakraCore scripting engine, which was failing to handle memory objects in Edge.  And there was also a problem in their Jet database engine which was publicly disclosed, but had not been observed in the wild.  So all those things got fixed, and that's good.  In addition to, what, 46, 47, 48 others.  However, as has become all too commonplace lately, included in last Tuesday's release were two updates that caused problems connecting to network shares on Windows 7 and its server version, as I mentioned, Windows Server 2008 R2.  Three days later, that would have been on Friday the 11th, Microsoft released a standalone update to resolve that problem which had been introduced by Tuesday's patches.



So first of all, if you didn't have any problem, then you're okay.  If you did, the two patches that caused the problems were KB4480960 and KB4480970.  So you could remove those, or you could install the subsequent fix, which was 4487345.  And if anybody hasn't known what's going on, you haven't dug around or found an answer, I do have the link to that update in the show notes.



There was also a registry fix, I mean, because this was, for a lot of people, this was a big problem.  So there was some scurrying around.  A quick fix was found that involved adding a DWORD value to a key in the registry under LocalAccountTokenFilterPolicy.  If you did that in order to temporarily solve this problem, you should remember to take that out because you don't want to leave that in there, and instead apply this KB4487345 fix.



Also last week Windows 7 machines being activated through Microsoft's Key Management Service, KMS, began receiving "Windows is not genuine" notifications, indicating that the Windows license was not valid, which of course you can imagine upset some of the valid Windows users.  It turns out that, due to its coincidence with Patch Tuesday, the initial suspicion was that something else that Patch Tuesday had done had broken this.  But it turns out that we later discovered that there was a change that Microsoft had pushed to their activation servers that broke that, which Microsoft later backed out.



They wrote:  "A recent update to the Microsoft Activation and Validation unintentionally caused a 'not genuine' error on volume-licensed Windows 7 clients that had" - and there was some interaction - "that had KB971033 installed."  They said that the change was introduced at 10:00 UTC on January 8th and was reverted at 4:30 UTC on the 9th.  So made a mistake, realized what had happened, and backed out of it.  So anyway, as I said, it's almost more useful now deliberately holding off a week just to see what happened after our monthly Patch Tuesday, since Microsoft has, I mean, I don't think we've had one that's been uneventful for quite a while.



I mentioned unintended consequences to the U.S. government shutdown.  Certainly, as our U.S. listeners know, unless you're really uninvolved with the news of the day, at the time of this podcast the U.S. government is partially shut down over a political funding dispute about the proper nature of the enforcement of security at our southern U.S. border.  In a classic example of unintended consequences, a growing number, somewhere around 80 as of the most recent reporting and counting that I've been able to find, 80 different government websites are no longer accessible or have been marked as insecure connections because, during this government outage, their servers' TLS certificates have expired and cannot be renewed, apparently, until the government reopens.



I have a shot in the show notes here of a notice:  "Your connection is not secure."  And this was Mozilla.  This was Firefox last night.  It said:  "The owner of [and the site is] ows2.usdoj [that is the United States Department of Justice] .gov has configured their website improperly.  To protect your information from being stolen, Firefox has not connected to this website.  This site uses" - here's another little twist that we'll get to in a second - "HTTP Strict Transport Security" - as we know is HSTS - "to specify that Firefox may only connect to it securely.  As a result, it is not possible to add an exception for this certificate."  So you the visitor using Firefox have no recourse.



And this U.S. DOJ site was not alone; .gov websites with expired certificates which are also on the HSTS preload list, including all of the U.S. Department of Justice .gov subdomains, are completely inaccessible by Chrome and Mozilla because the U.S. DOJ, wanting to strengthen their security, has been declaring historically and is on the preload list for these browsers.  So you can only get to them this way.  So naturally, first of all, I'm sort of surprised that this happened so quickly, but I guess certificates are constantly rolling over and needing to be renewed.



There was also a rockettest.nasa.gov site that is gone, and a Lawrence Berkeley National Lab site, d2l.lbl.gov.  What's interesting is that I was curious about those because the rockettest.nasa.gov site, its cert expired on January 5th, and this Lawrence Berkeley National Lab site expired on the 8th.  They initially showed warning messages like I read before.  Oh, but they did not have HSTS, so you were able to push past the warning, saying, you know, I want to go there anyway.  I know what I'm doing, or I'll take the risk or whatever.  The problem is those sites also required authentication.  And so if you pushed past that, you were then sending your username and password credentials without encryption to the site.



So even though it has apparently been impossible to issue new certs during the shutdown, at least the sites have been taken off the air.  In the case of rockettest.nasa.gov, when I looked this morning, DNS had been pulled from it.  And the ows2.usdoj.gov site had had its traffic blocked.  So you can't get there at all.  It's still resolving DNS; but they said, okay, we can at least pull the plug on this so that people are not trying to log in insecurely.



So clearly the takeaway here is that we should learn a lesson.  We're having these temporary government funding shutdowns over political disagreements now, relatively routinely.  I think this is the third one under the Trump administration.  And we've broken a record in terms of its length.  So certainly it's a function of the size of the window during which certificates cannot get renewed.  If these are now breaking length records, there's a greater opportunity for certificates to expire during this time.  The good news is we always see these approaching.  So I'm hoping that this experience and the fact that we've moved the Internet forcefully to HTTPS connections, which is all for the better, and it's certainly good that sites are on the HSTS preload list so that, even when their certs expire, there's no loss of security, except for the fact that you can't get there.



But my point is that, since we always see these potential shutdowns approaching, it would be great if there was some look at the certs on the servers.  Because we know that all of the certificate authorities will credit us with the amount of remaining time on a certificate when they are renewed.  So there's no downside to renewing the cert ahead of time.  It doesn't cost more money.  You're not losing the amount of time that was remaining on the certificate when you renew.



So I hope that there's enough awareness of this that the people in the government, as we see the next shutdown approaching, will say, oh, gee, we only have three weeks left on this cert, and that may not be enough time.  So let's renew now so that the site can stay on the air.  Although, frankly, I think it was - I remember a previous one where several sites just put up a banner and said:  "We're sorry.  During the funding dispute and the government shutdown, our services are not currently available.  Come back when you hear otherwise."  So anyway, the good news is at least the sites that were offering you the ability to ignore security, at least in the two that I looked at, they had been taken down.  They were off the air, which is an improvement.



An interesting case of WhatsApp security being broken.  We've talked about, of course, a lot about messaging security.  We did a podcast on the Signal app, whereas I have said a number of times, as I was reading through the detailed protocol spec, I remember thinking initially, boy, this thing is overdesigned.  And then, as I got into the details more, I realized why the bullet point features that were mentioned at the beginning were there, and I came away with a lot of respect for the Signal protocol.  The problem is it's still up to the implementer to deal with some of the details.  And at least WhatsApp has failed in one way to handle some of this.



This came to light on the 10th, which was, what, last Thursday.  An Amazon employee, Abby Fuller, tweeted:  "Logged into WhatsApp with a new phone number today and the message history from the previous number's owner was right there.  This doesn't seem right."  And apparently there was - I don't know how many people followed her.  The news got out.  It drew some attention to her tweets.  She followed up with additional tweets.  She said:  "Now I'm wondering how many other times it's happened.  Like does whoever has my old number now have my WhatsApp history?"  And she also tweeted in response to others:  "Yes, it was a new device.  No, it wasn't second-hand.  It was not a second-hand SIM.  Yes, I'm sure they weren't my messages or groups that I was added to.  Yes, they were in plaintext.  I'm sure it's my new phone number.  It was not restored from a backup."



Okay.  So we know what happened.  The apparent leakage of someone else's WhatsApp messaging stream into Abby's phone should raise privacy concerns.  As we know, WhatsApp uses our phone number as our authentication in lieu of username and password.  The argument has been that WhatsApp only sends to that number, and so our phone is our authenticating device.  So the fact that it just uses our phone and our phone number is not a vulnerability.  But what exactly happens when phone numbers change hands?  It's clear from an online FAQ that WhatsApp is aware of this issue.  The problem is that its users aren't aware, and WhatsApp has made everything so simple and automatic that it's difficult to then ask users to pay attention to something that's far from obvious because its security implications have been deliberately hidden in order to make this system easy to use.



On their FAQ, I've got a link to it in the show notes for anyone who's interested, they have a subject, "Changing phone numbers and/or phones," and then the subhead "Changing your WhatsApp phone number.  Before you stop using a particular phone number, you should migrate your WhatsApp account to the new number.  For a simple way to do this, use our Change Number feature.  By using this feature, you'll be able to migrate your account information, including your profile information, as well as your groups."



They say:  "Make sure your contacts delete your old number from their phone's address book and input your new number, as it is a common practice for mobile providers to recycle numbers.  You should expect that your former number will be reassigned."  In other words, this is a complete failure of the privacy guarantees that WhatsApp is promoting as a consequence of the fact that it's phone number tied, yet people are not necessarily tied to their phone numbers when they change.  So Abby's tweets indicated that the chat history she received on her new phone was "not full, but definitely actual threads/DM conversations," she said elsewhere.



So we know that WhatsApp doesn't archive messaging on their servers, but we also know that - and really WhatsApp is Signal because it's the Signal protocol.  And this is something that we explained and covered when I talked about the Signal protocol on our podcast of that name.  We know that undelivered messages will persist in encrypted form for up to 45 days.  The other problem is that once a device's SIM and phone number have been used to establish the local device's encryption keys, the SIM can be removed.  Yet that device, now absent any cellular telephony, can continue to use the encryption keys it still has, until such time as the phone number associated with its absent SIM becomes assigned to some other WhatsApp user.



So that means the binding, the real-time binding between the phone number and WhatsApp encryption is weak.  I mean, there is no real-time binding.  It's a first-use establishment.  Which does create a rather large window during which time there's a presumption that you still have this phone number, even though it's on a device that may have no phone number at all, no cellular telephony service.  And so that creates a rather glaring loophole.  When you combine that with storage and catch-up delivery of pending messages, it creates an opportunity for some significant privacy leakage.



So this is the way WhatsApp operates.  Oh, it also trusts new encryption keys broadcasted by a contact and uses them to automatically reencrypt undelivered messages and send them to the recipient without informing or leaving an opportunity for the sender to verify the recipient.  Again, it's doing a lot of things behind the scenes so that it just works.  Unfortunately, we're seeing a perfect example of how this could be broken.  And of course this brings us back to my number one complaint about ease of use versus security and privacy tradeoffs, which we inevitably encounter anytime someone else manages our keys for us.



This made me go back and visit Threema.  I haven't looked at the Threema website for a while.  And I've always liked them because they keep this in the hands of their users.  Yes, there's a little more setup in the beginning.  You are asked to do - you remember that Threema's the one that has the green, yellow, and red sort of stoplight signal for the level of authentication of the other person's keys that you have achieved.  So, yes, a little more setup.  Also it's not free.  It's a few dollars in order to purchase this.



LEO:  And it's not open source.



STEVE:  Is it still not?



LEO:  No.  They use NaCl, but we don't know how they use NaCl; right?



STEVE:  Okay.  Anyway, so all these...  



LEO:  Unfortunately, Signal also uses a phone number.  I wish they didn't.



STEVE:  I know.  And yes, and they do.



LEO:  It's a drawback, yeah.



STEVE:  Yeah.



LEO:  They don't have the same problem WhatsApp does, I'm sure.  Because once the keys are invalidated, they say, well, that's that, you know.



STEVE:  Right.  So as I said at the top of the show, if I were to coin a phrase to be a perfect corollary to our TNO, Trust No One, it would be, "If you're not managing your own keys, someone else is."  Keys are the way, encryption keys are the way our systems work today.  They need to be somewhere.  And someone needs to manage them.  Someone needs to create them, curate them, verify them, use them, destroy them.  And if you're not doing that, then that's being done somewhere else.  So again, it's a perfect corollary to Trust No One.  If I'm using some sort of communications tool - and again, nothing I'm doing needs it.  But if someone's really concerned about privacy, you want something where you're managing your own keys.  And Leo, I would agree with you that it should also be open source and, as we know, have been audited, closely looked at by people who have an adversarial role.



LEO:  They claim they're audited, but you have to sign an NDA to audit it.  So, I mean, it's a commercial program.  Let's just say it, call a spade a spade, yeah.



STEVE:  I thought I was going to have another story about a free, useful, ransomware decryption tool.  We've been talking about those over the last few weeks.  This one is PyLocky, P-Y-L-O-C-K-Y.  It turns out - I got a kick out of the headline.  It was "Unlock files for free."  And I thought, okay, well, this is probably another encryption done wrong.  Turns out that's not the case.



LEO:  We should mention it comes from Cisco, from Talos; right?



STEVE:  Well, actually their research does.  What they discovered was that, if you happened to be capturing your network traffic...



LEO:  Oh, okay.



STEVE:  Uh-huh.



LEO:  Well, there you go.



STEVE:  ...at the time that this thing grabbed a hold of your computer and had its communications with its command-and-control server, so that you had a PCAP file, a packet capture file of the network traffic between your machine and the command-and-control server, because the initialization vector and the password were transiting the network at the time, well, the good news is...



LEO:  Yeah, we got it.



STEVE:  What's the problem?



LEO:  Yeah, you've stored it.



STEVE:  Yeah, thanks very much, but I wasn't capturing my traffic.  And you might argue also that anybody who is doing, whoever they are, who actually has Wireshark running or something capturing their packet traffic, is probably astute enough not to get infected with this in the first place.  So I'm not exactly sure what the overlap is between people who are being caught out by this ransomware and happen to have packet capture running at the same time.  To me this felt like, oh, hey, look, we were playing with this ransomware.  We noticed that, oh, look, we could capture the packets and reverse the decryption.  Okay.  But that isn't really going to help anybody. And other than that, the encryption was done correctly.



On the 9th, which was Wednesday, Google Public DNS began to support DNS over TLS.  So Google has added themselves to the ranks.  We know that Cloudflare has been doing this for a while.  Google now is there, too.  In their posting, under their security Google blog, they said:  "We implemented the DNS over TLS specification along with the RFC 7766 recommendations to minimize the overhead of using TLS.  These include support for TLS 1.3," they said, "for faster connections and improved security, TCP fast open, and pipelining of multiple queries and out-of-order responses over a single connection."



And actually I would argue that that's probably one of the most important things to have for DNS because the user is going to be establishing a semi-persistent connection to a remote DNS server.  But remember that, even though it's caching, you may very well be making queries for DNS domains that it doesn't have in its cache, meaning that it's going to have to go out and recursively resolve that query in order to get for you the IP address that you're asking for.



If you're going to a web page, where oh my god, you know, just scores of domains are now appearing on individual web pages, that creates a flood of DNS queries.  So you're definitely not going to want to wait for an in-order resolution of DNS.  You really need out-of-order resolution.  So you need to be able to dump a whole bunch of queries onto that remote DNS server and start getting answers back immediately for IPs that it has in its cache while it goes out and looks for other things so that you can get those back and start making your queries and not be stuck in a serialized pipeline.  So that's all for the good.



They also note - this is Google, of course - that Android 9 (Pie) users can use DNS over TLS today.  So I dug down a little bit because I was curious to see where we are in the state of the deployment of this.  And so what they have is known as a "stub resolver."  The stub resolver is - essentially it replaces, or I guess I would say it's a local proxy for DNS.  So it's a little resolver that runs on your whatever, on your smartphone in the case of Android.  There are stub resolvers for other platforms I'll get to in a second.  So what it does is it rewrites your DNS servers to itself, so typically 127.0.0.1, the local host.  So it creates a local DNS server on your machine so that your machine then makes queries to it, which it then proxies out securely over to the DNS server, the DNS over TLS server that it's been configured with.



In their notes they explained that the stub resolver is configured with DNS over TLS, resolver name dns.google in their case.  The stub resolver obtains the IP addresses for dns.google using the local DNS resolver because of course first it's got to get the IP of where it's going.  It makes a TCP connection to port 853 - as we know, 53 is the normal port for DNS, so this is 853 - at one of the IP addresses that it has received from the normal DNS resolver.  It then initiates a TLS handshake with the Google Public DNS resolver.  And the Google Public DNS server returns its TLS certificate along with a full chain of TLS certificates which chain up to a trusted root certificate.



So the stub resolver verifies the server's identity based on the certificates that it receives over the TLS connection.  If the identity cannot be validated, the name resolution fails, and the stub resolver returns an error.  So naturally you want to make sure that you are connecting to the real dns.google server and that you're not subject to any sort of spoofing because the whole, you know, not only are you hoping to get privacy by running your DNS over TLS, but you're also wanting to solve the problem of DNS spoofing through any sort of a man-in-the-middle attack or interception.



As we know, DNS over UDP provides essentially no protection for that because, if someone can somehow get into your connection, it's trivial to spoof DNS.  This solves that problem, but you want to make sure that your other end is anchored at the real trusting or trustable DNS server.  And so after the TLS connection is established, the stub resolver then has a secure communications path between you and Google's Public DNS server over which these queries can be sent.



So we have DoH, which is the acronym or the abbreviation of DNS over HTTPS.  Chrome and Firefox already support that.  So that gives us already a lot of these benefits.  But as we know, our computer systems are doing lots of DNS queries also.  So DNS over HTTP supported by our browsers only solves the problem for browsing.  DNS over TLS, if our OS has a DoT resolver, that is, DNS over TLS, then our whole system gets protected, and all of the DNS queries that are being made are going to be running over TLS.  And so long as the DNS over TLS server is giving us good performance and is well connected and is nearby, it can run very quickly, and we get absolute authentication of the queries subject to its security and privacy so that nobody can see what we're doing.



There is DNSPrivacy.org for anyone who is interested.  There are now resolvers, there's a stub resolver for Mac and Linux and Windows.  They're still in the early stages of development.  But this would allow somebody who really wanted that kind of privacy, now that we're seeing some good heavyweight support for DoT, DNS over TLS, to get privacy for all of their systems' DNS lookups.  So yay.



Their paper is titled "A First Look at the Cryptomining Malware Ecosystem:  A Decade of Unrestricted Wealth."  And of course we're all familiar with the expression "Crime doesn't pay."  The point I think is that getting caught eventually makes any crime which may have appeared to be going along nicely, right up until that point, suddenly a source of regret.  That adage would certainly apply to those who are behind efforts to steal others' computing resources for the mining of cryptocurrency.  We've been talking about cryptocurrency mining now for, well, browser insertion.  For a while websites were voluntarily putting the script on their sites, like in lieu of advertising, to say, "Hey, while you're here, we'd like to borrow some of your processer or GPU in order to mine some coin for ourselves."



Anyway, these guys, two researchers - I'm trying to look for it in my show notes.  I don't see where they are.  I have their original research link in the show notes.  In their abstract they said:  "Illicit cryptomining leverages resources stolen from victims to mine cryptocurrencies on behalf of criminals.  While recent works have analyzed one side of this threat, i.e., web browser cryptojacking, only white papers and commercial reports have partially covered binary-based cryptomining malware.  In this paper, we conduct the largest measurement of cryptomining malware to date, analyzing approximately 4.4 million malware samples," they said, "one million malicious miners over a period of 12 years from 2007 to 2018.



"Our analysis pipeline applies both static and dynamic analysis to extract information from the samples, such as wallet identifiers and mining pools.  Together with open source intelligence data, this information is used to group samples into campaigns.  We then analyze publicly available payments sent to the wallets from mining pools as a reward for mining, and estimate profits for the different campaigns.



"Our profit analysis reveals campaigns with multimillion dollar earnings, associating over 4.3% of Monero with illicit mining."  So, what, one in 25, over 4.3%.  "We analyze the infrastructure related with the different campaigns, showing that a high proportion of this ecosystem is supported by underground economies such as pay-per-install services.  We also uncover novel techniques that allow criminals to run successful campaigns."  So what's the cash-out value of 4.3% of Monero?  $53 million.



LEO:  Wow.



STEVE:  Yes, exactly, Leo.  Unfortunately, we know that money is what drives this.  And if you've got a $53 million paycheck on the other side of figuring out how to get mining on other people's hardware, you have a lot of incentive to do so.  And unfortunately, one of the themes we keep coming back to here is the degree to which security is porous.  We've seen instances where, for example, where routers are compromised in order to install cryptomining on any browsers that they're able to perform an injection on behind the router.



So it's just, in aggregate, we're looking at $53 million spread out among these campaigns.  And in fact in their outlook for 2019, Check Point Security sees cryptocurrency-stealing software continuing to be the number one most commonly distributed form of malware.  That is, they have a Top 10 list, and all of the top slots in Check Point's Top 10 list are filled with cryptocurrency miners.  Coinhive continues to be the most prominently distributed malware, followed by XMRig, both which use the victim computer to mine Monero with the profits directed into the cryptocurrency wallet of the attacker.  Those two are followed by JSEcoin, which is a JavaScript miner embedded into websites; and then the not very imaginatively named CryptoLoot, which is a direct competitor to Coinhive.  CryptoLoot was second only to Coinhive last November, but since then its distribution has dropped a bit.



So, I mean, this is where the pressure is at the moment.  I guess it's somewhat better than it being ransomware, which is encrypting all of someone's files on their entire computer.  Still, it's sort of nickel-and-diming people.  If you get it into your phone, it runs your battery down.  We've seen instances where it's causing phones to overheat because it's pushing them so hard.  Those things tend to give away the presence of something that's gone wrong in one's computer.  And then you scan it with some AV software and figure out what's going on.



But it does say that that's what the bad guys are trying to do is just get this stuff into your machine any way possible.  And of course advertising is one of the ways this happens because our browsers are pulling in ads from everywhere, and those ads are then able to run JavaScript on our machine.  And unless there's proactive measures being taken to prevent those ads from consuming undue resources, there's some slight chance that they're going to be mining cryptocurrency and score a fraction of a coin, or be part of a mining pool, in which case it's just incremental wins.



And speaking of Top 10 lists, Avast's Threat Landscape Report for 2019 is out.  It was a 26-page report.  I'm not going to go over the whole thing.  But one topic that has been a big one for us all of 2018 in the Internet of Things section was a subsection on router-based attacks stating that the worst is yet to come.  It may not surprise anybody, but we've talked about how porous our routers are and how unfortunately subject to compromise.  In their report, I won't go through it in detail - if anyone's interested I've pulled the whole section out, it's in the show notes - because we've been covering it to such degree.



Their research, however, shows that 60% of users around the world have never updated their router's firmware.  And while we can hope that newer routers are - and I want to believe that newer routers are doing a better job, there's this pressure from the routers to enable lots of features.  And one of the trends we are seeing is routers becoming increasingly sophisticated, offering an increasing number of features, and those features ending up biting people.



So all of our listeners know you want to make sure you do not have Universal Plug and Play exposed publicly.  You want to have it disabled on the LAN side because it can be abused.  If you know you don't need it, if you can disable it, you want it disabled, as I mentioned last week, it really is worthwhile, especially if your router is a few years old, to go out and make sure that there isn't newer firmware available.  And you really ought to, if you can, purchase a router from a reputable source.  They mentioned, they said, they're talking about an increase in router-based malware in 2018.  They said they've also seen changes in the characteristics of the attacks, where router-based malware has traditionally taken over a device for the purpose of carrying out a DDoS.



In other words, we talked about that earlier in 2018, where they were using UPnProxy in order to bounce packets off of routers in order to just distribute their attacks.  The Mirai botnet was doing that.  Avast said that today's attacks use malware that infects a device and then opens up a line of communication to a command-and-control server without taking any immediate action.  They said:  "We saw this with VPNFilter."  Now, remember, that's the malware that the FBI alerted everyone to and said please reboot your routers, bizarrely enough, because if you did that, you would at least flush it out of RAM.



LEO:  Temporarily.



STEVE:  Exactly, temporarily.  Once the router's infected, these malware strains, they wrote, listen to the network traffic, fingerprint the network and the devices behind it, and allow for the command-and-control server to send new payloads or instructions to the devices.  So basically it's elevated itself to an advanced persistent threat-style infection of a router.  And they said:  "In this, the malware acts more like a platform and less like a virus."  They said:  "This 'platformification' of IoT malware opens up many possibilities for bad actors who can repurpose it for a multitude of nefarious activities including pay per install, DDoS for hire, cryptomining, or even good old-fashioned spam."



They wrote:  "This evolution replicates how PC malware counterparts have evolved and indicates the sophistication of new strains of IoT targeted malware."  So I don't think it's possible to overstress the fact that we talk about attack surfaces a lot.  One reason to be wary about AV, as we've discussed, is that it can present an attack surface, an increased attack surface, because if it's trying to scan everything coming in, if the AV itself has made any mistakes, it can increase the opportunity for compromise.  Well, there is no larger attack surface than our router.  That is the face of our network to the Internet.  So absolutely keep it secure.



And I wanted to mention also that I was just shopping recently for - what I went looking for was Netgate's SG-1000, which is this cute little tiny box, no moving parts, just two network interfaces, a WAN and a LAN.  What's significant about it is that it's a perfect platform for running pfSense, which is my favorite and chosen platform because it is completely open, open source, and it's running FreeBSD.  And literally anything you can imagine that you might want to run on it, will.  Anyway, I found that it had been discontinued, but it's been replaced with the SG-1100.  The price is $159, so it's not a $49 piece of plastic.



But it is, I mean, it's what you want if you want something secure on your perimeter where you know what software is running in it and a platform that can grow with you.  I'm using it to establish persistent VPN links using OpenVPN.  But I'm also doing port mapping, translating from one port to another in order to avoid some of the things that my local ISP is doing, and a bunch of other things.  Anyway, I just wanted to point people to it.  The SG-1100 has got five times the packet processing performance of the SG-1000.  So it turns out that the trade tariffs forced the price to go up a little bit, but you're getting five times the performance.  It's got three 1GB interfaces.  So if you're interested in setting up a segmented network, this supports that fully.



And it is also the first product equipped with Microchip's CryptoAuthentication device which provides assurance that the system is running authentic, unaltered pfSense software.  So the software itself is signed by pfSense.  And when you download it, it's verified and cannot be changed.  So anyway, just on the topic of routers, I was poking around, looking for something.  And first of all, I was sorry to see that the SG-1000 was gone, but I ended up with an SG-1100, which gives me an extra Ethernet interface and five times the performance, which is good because, as we know, our cable modems are increasing in speed.



LEO:  So do you then hook this up to a WiFi access point?  Or how do you...



STEVE:  Yeah, yeah.  So the cable modem plugs into its WAN interface.  And then I turn what was my WiFi router into an access point.  Typically they're able to run either way.  And I plug that into the LAN side in order to get access.



LEO:  All the DHCP is performed by the SG-1100.



STEVE:  Yes, yes.  And, for example, you can run a DNS server there.  You can run DNS over TLS there.  So it could provide - so it could be running your DNS over TLS proxy, connected to the strong DNS provider of your choice.  And then all of your devices just use DHCP, get it as the DNS provider, and your entire network then is doing DNS secure.  So, I mean, as an example of a perfect use for this.  It's just it's a perfect little FreeBSD box.  And there it is.  It's a cute little thing, too.



LEO:  It's not really a box.  It looks more like an Altoids tin.  



STEVE:  Yes, just barely big enough to hold the connectors that it needs.



LEO:  That's really cool.  So would you recommend - you'd recommend this over the EdgeRouter.  I mean, this is what the EdgeRouter X does kind of.  And it's got pfSense built in.



STEVE:  Yeah.  I like - yes, it's got - this has a beautiful web interface.  The problem with the EdgeRouter is that it really, I mean, you've got to really have your propeller wound tightly on your beanie in order to deal with the EdgeRouter.  The EdgeRouter is very powerful, but it isn't easily configurable.  This is really - you log in with a browser, and it's got a beautiful web-based GUI where you're able to fill out forms, create static and dynamic mappings, set up connections, I mean, it's just amazing.



It's got packet capture.  You're able to do - you know how in Linux you have a top that shows the assorted list of processes by how much power, by how much processing time they're using.  You have the same thing for your network so you can see what connections out to the outside world are using how much bandwidth in order to monitor what your entire network is doing.  And, I mean, it just goes on and on and on.  It is, for someone who wants to play with their network, I couldn't recommend anything better than this.



LEO:  Oh, I might have to get it.



STEVE:  It's a cutie.



LEO:  The only problem is the mesh routers that I use like to be their own DHCP server.



STEVE:  Well, and this doesn't prevent them from doing that.  So you could [crosstalk].



LEO:  But this is bridge mode.  Or you could have double NAT.



STEVE:  You could have, well, so they want to be DHCP or NAT.



LEO:  Oh.  I've always used those interchangeably.  They're not?



STEVE:  No.  DHCP is Dynamic Host Configuration, which is giving machines IP addresses and so forth.  NAT, of course, is stateful routing of packets across.  Typically those are in the same box.



LEO:  I understand the different functions, yeah, yeah, yeah.



STEVE:  But they wouldn't need to be, yeah.



LEO:  So you have to let it do NAT for it to do the pfSense functionality.  Otherwise it couldn't really, you know, do packet inspection.



STEVE:  Well, you could set it up as a bridge, in which case it would have some functions.  But it would really, as you say, it would make more sense for it to run your network.



LEO:  It should do the NAT, and then you let the Eero do the DHCP.  I don't know if I can do that, but I'll try.



STEVE:  That would be interesting, yeah.



LEO:  Yeah.  A lot of these mesh routers want to see all the traffic because they want to do QoS.  They want to be able to control it more directly, as opposed to just being a dumb access point or radio. 



STEVE:  Yup, that does make sense.  So I want to say it right up front that I get it that not everybody is going to be concerned about this.  But as an indication of where practice and the law are colliding, I think this was interesting.  And it does - there is a sort of a little bit of a creep factor in this also.  And tongue-in-cheek I called this "A cloudy forecast for The Weather Channel App."  The City of Los Angeles has sued The Weather Channel, claiming that it's been posing as a "personalized local weather data alerts and forecasts" app; but, they say, in truth makes profits by tracking users throughout the day and night, selling their private personal location data.



And I should say that's my favorite app.  It's the one I use.  I like it.  Even though it's not necessarily super accurate, I'm a little bit seduced by the fact that you can touch Today and Tomorrow and see a guess about what the weather's going to be hour by hour, which is just sort of seductive.  Anyway, this lawsuit, which was brought by, and I have it lower in my show notes, Domenic Venuto - no, no.  He's on the other side.  Okay, I'll just stick with what I have here.  The lawsuit calls The Weather Company's practices "fraudulent and deceptive" and says they violate California's unfair competition law.  TWC fails to disclose that it collects users' location data and sends it to third parties, the suit maintains.



So the City of L.A. says:  "It isn't about analyzing the clouds above our heads for a personalized weather forecast.  Rather, it's about collecting location data for 'advertising and other commercial purposes unrelated to weather data, alerts and forecasts.'"  None of the marketing purposes of collecting geolocation data are disclosed on either Apple's App Store or Google's Android Play Store in their versions of the free app, which is also available in an ad-free version for $3.99, notes the lawsuit.



Now, first of all, I didn't know you could buy it for four bucks and be free of the ads.  Now I'm tempted to do so because you can see how concerned I am about the...



LEO:  I guess you don't care, do you.



STEVE:  About being tracked, yeah.



LEO:  Take a look at Dark Sky on iOS.  That's another very good app that does much the same thing.  You know, IBM owns The Weather Company.  This is an IBM company.



STEVE:  Yes, yes.  And they're not happy about being sued by this.  What was a little - the thing that was a little creepy is that in the lawsuit they did some digging, and they found that this Domenic Venuto, who is the general manager of the consumer division at TWC, admitted in an interview that:  "If a consumer is using your product and says, 'Hey, wait a minute, why do they want to know where I am?' because it isn't an organic fit with the app, you're going to have some problems."



LEO:  Wait a minute.  It is an organic fit with the app.  It's a weather app.



STEVE:  Yes, but that's the point.  The whole idea is this is about tracking your location under the guise of providing you weather information.



LEO:  But it needs your location to give you the weather information.



STEVE:  It does.  So yes.  But his point is that, well, okay.  So what they found was...



LEO:  I mean, I could see if, I don't know, a music app wanted to know your location, which by the way they all do.  That you might be a little, well, why does it need to know that to play music for me?



STEVE:  Exactly.  And so that's the point is he was acknowledging that by being a weather app, it's not going to raise suspicion that they want to know where you are.  And again, so that was the point he was making.  So last month The New York Times investigation found that The Weather Channel was - first of all, as we know, it's a big pack.  As you just noted, music apps do the same thing - was one of at least 75 companies getting purportedly anonymous but pinpoint precise location data.  And this is what I heard you mention when you were talking about this earlier, Leo, like within feet of where you're located, from about 200 million smartphones across the U.S.



In their coverage of it, they said they're often sharing it or selling it to advertisers, retailers, or even hedge funds that are seeking valuable insights into consumer behavior.  In one example, a company known as Tell All Digital, which is a Long Island-based advertising firm, buys location data, then uses it to run ad campaigns for personal injury lawyers that it markets to people who are in emergency rooms.  So the point is that the location data is that precise that they're able to determine if you're in an emergency room of a hospital and, if so, arrange - I'm sure in the whole advertising bidding deal - arrange to serve you personal injury lawyers ads because you may be in the mood for needing one at that time.



So anyway, I'll just note that iOS gives us good control over when we are feeding apps location data, and that you're able to say blackout location data completely for an app.  Let it know where we are all the time, or only while we're using the app.  And I presume that means only when it's in the foreground and has not been put to sleep by having been switched to the background.  And for what it's worth, these things are very accurate.  They're accurate to within a few yards and in some cases are tracking us 14,000 times a day, so basically creating a continuous stream of where we go over time.  So again, Leo, as I said, I'm thinking of deleting it so I can purchase the $4 version and get it without ads.  But if Dark Sky is doing the same thing, I'll probably switch to that.  So it's Dark Sky?



LEO:  Yeah, I like that one a lot.



STEVE:  Okay, cool.



LEO:  That's the old Forecast.io.  And, yeah, I think it's good, yeah.



STEVE:  Good, good.



LEO:  In fact, I think you might even like it better, and there's no ads.



STEVE:  I'm glad we got that tip.  I will check it out.  Is it a for-purchase?



LEO:  Yeah, I think it's a few bucks.



STEVE:  Oh, good.  I'm happy to pay.



LEO:  You know, if you don't pay, you've got to wonder who's paying.



STEVE:  Yup, exactly.  Somebody's paying. 



LEO:  Somebody's paying.  



STEVE:  Exactly.  Okay.  So the Ethereum Classic blockchain was hit - I think it began on the 5th, I've got it in the notes, and we'll come to it - with what is an expensive, known as "51% attack."  When this podcast first described the detailed operation of the Bitcoin blockchain, in what was actually a classic podcast for us - because I remember I dug into it.



LEO:  Oh, it was great.



STEVE:  And read Satoshi's original whitepaper.  And I just was...



LEO:  You were raving about it.



STEVE:  Oh, my goodness.



LEO:  You thought this was amazing.



STEVE:  This is the coolest thing I've ever encountered.



LEO:  And you know what?  You were right because blockchain, you know, you can weigh in or out on cryptocurrencies and Bitcoin, but blockchain there's no doubt is a very valuable innovation.



STEVE:  Yeah, yeah.  So we mentioned that one of the key assumptions, in fact the cornerstone assumption for the security and trustworthiness of any proof-of-work, blockchain-based technology is a large community pool of honest participants who mutually concur and authenticate blockchain events.



LEO:  It says that?  Because that really rules - if I'd seen that, I would have said, well, this will never work.  Everybody's got to be honest?



STEVE:  Yeah.  Actually, that's my jargon.



LEO:  Oh, that's your line, all right.



STEVE:  Page 3 of Satoshi's original whitepaper, which was titled "Bitcoin:  A Peer-to-Peer Electronic Cash System," it stated:  "If a majority of CPU power is controlled by honest nodes, the honest chain will grow the fastest and outpace any competing chain."



LEO:  Oh, that's intriguing.



STEVE:  So he understood even before this had happened, when it was just a whitepaper, that this was important.  So stated another way, for the blockchain to remain secure, no single actor must ever be able to obtain a majority of the chain's total processing power because someone who is able to dominate the chain rules the chain.



LEO:  Uh-oh.



STEVE:  Uh-huh.  And is thus able to cheat others.  And that's what has been happening since January 5th to the Ethereum Classic blockchain.  At the expense, that is, they had to expend significant computation.  But at the expense of significant computation, which was expended, attackers have been able to rewrite history.  They rolled back and reorganized the Ethereum Classic blockchain and were thus able to double spend by recovering previously spent coins and transferring them to a new entity.  I've got a link in the show notes to the Coinbase.com blog.  Coinbase's security engineer, Mark Nesbitt, wrote in the blog about these events.



He said:  "The function of mining is to add transactions to the universal shared transaction history known as the blockchain.  This is done by producing blocks, which are bundles of transactions, and defining the canonical history of transactions as the longest chain of blocks.  If a single miner has more resources than the entirety of the rest of the network, this miner could pick an arbitrary previous block from which to extend an alternate block history, eventually outpacing the block history produced by the rest of the network and defining a new canonical transaction history."  



And that's what happened.  They posted a timeline:  "Late on the evening of Saturday, January 5th, our systems," he wrote, "alerted us to a deep reorg in ETC that contained a double spend.  Our on-call engineers responded to the alert and worked to confirm the report through the night.  We determined that we would temporarily halt send/receive interaction with the ETC blockchain in order to safeguard customer funds.  This meant that customers who tried to send or receive ETC on Coinbase Consumer or Pro were unable to complete their transactions.



"On the morning of Sunday, January 6th, we posted an update on status.coinbase.com" - and Leo, you should go there - "stating that, 'Due to unstable network conditions on the Ethereum Classic network, we have temporarily disabled all sends and receives for ETC.  Buy and sell is not impacted.  All other systems are operating normally.'  We performed an analysis on Sunday afternoon and evening to confirm the pattern and determine the key details of the double-spend attacks.  Beginning Sunday afternoon, we observed eight more incidents, all containing double spends.  Out of an abundance of caution, we did not put out a blog post prior to legal and technical review.  A false alarm could have inadvertently caused market instability.



"On Monday, January 7th morning, after legal and technical review, we finalized our public analysis and posted to our blog and social media accounts."  And of course that went into the news.  I didn't put it in the show notes; it is there on status.coinbase.  Or in this blog posting, the individual breakout of double spends, which ended up - I thought I had it.  Oh, yeah, here it is.  219,500 previously spent coins were re-spent, netting the attackers $1.1 million in ETC.  And on that page that you showed, of note is the fact that right now, today, everything is green except that one.  ETC is still - their buy and sell, or their send and receive is still disabled, and they have it down for "maintenance."  But really what's happened is...



LEO:  We don't control it anymore.  It's gone.



STEVE:  Yes.  That Ethereum chain can no longer be trusted.



LEO:  Now, this is not - this is a fork of Ethereum, Ethereum Classic.



STEVE:  Yes, it's the classic.



LEO:  Didn't affect Ethereum.  However, the real question is, obviously it's a smaller cryptocurrency.  Could this happen to Bitcoin or one of the bigger ones?  It would be awfully hard to do; right?



STEVE:  Yes.  Historically, it's happened in the past.  There was a 51% attack, I mean, these are generically called "51% attacks" on blockchains for exactly this reason.  As Satoshi observed, you have to have a majority of honest nodes involved in validating these transactions.  If any one entity obtains majority control, they're able to take over.  And in the past we've touched on this where it's been like at risk.  There have been single entities that sort of were approaching 50%, and that got people worried.  And there were brief instances where someone had more than 50%.  They had a majority.  But as you note, the bigger these get, the more they sprawl, and then the more difficult it becomes for any one actor to obtain a majority.  So there is safety in size because it just becomes an insane amount of processing power in order to pull this off.  But it can happen.



LEO:  Steve Gibson, back to you.



STEVE:  So a court recently ruled that we needn't give law enforcement the finger, I mean, our finger.



LEO:  Our finger.  Or face.  Or iris.



STEVE:  So Thomas Brewster, who is Forbes' cybersecurity reporter, yesterday ran a story with the headline:  "Feds Can't Force You to Unlock Your iPhone with Finger or Face, Judge Rules."  A California judge has ruled that American law enforcement - Thomas wrote "cops" - can't force people to unlock a mobile phone with their face or finger.  The ruling goes further to protect people's private lives from government searches than any before, and is being hailed as a potentially landmark decision.



Now, of course we've talked about this at length, where the standing law before was you could not be compelled to divulge something you knew like a password because that was being called "testimonial."  But something you were, like your thumbprint, your iris, whatever, was not testimonial.  So under that argument, you could be compelled to produce your fingerprint to unlock a device.  So this order - and again, this is the most recent order.  There is no last word until this thing goes to the Supreme Court for final judgment.



This order came from the U.S. District Court for the Northern District of California in the denial of a search warrant for an unspecified property in Oakland.  The warrant was filed as part of an investigation into a Facebook extortion crime in which a victim was asked to pay up or have an embarrassing video of the publicly released.  The police had some suspects in mind and wanted to raid their property.  In doing so, the feds also wanted to open up any phone on the premises via facial recognition, a fingerprint, or I don't know of any phone that uses an iris, but that was in there, too.  While the judge agreed that investigators had shown probable cause to search the property, they did not have the right to open all devices inside by forcing unlocks with biometric features.



On the one hand, the magistrate judge, which was Kandis Westmore, ruled the request was overbroad as it was neither limited to a particular person nor a particular device.  So they were just saying, whatever we find of whoever it happens to be, we want to be able to see inside it.  But in a more significant part of the ruling, Judge Westmore declared that the government did not have the right, even with a warrant, to force suspects to incriminate themselves by unlocking their devices with biological features.



And as we know, previously courts had decided that biometric features, unlike passcodes, were not testimonial.  That was because a suspect would have to willingly and verbally give up a passcode, which is not the case with biometrics.  A password was therefore deemed testimony, but body parts were not, and so not granted Fifth Amendment protection against self-incrimination.



So then this created a paradox.  How could a passcode be treated differently from a face or finger when any of the three could be used to unlock a device and expose a user's private life?  So where we are now is that Judge Westmore focused there on her ruling.  She's declared that technology is outpacing the law, writing that fingerprints and face scans were not the same as physical evidence when considered in a context where those body features would be used to unlock a phone.  So in other words, specifically saying that, for the case of unlocking, these are being held as different.



So anyway, as I said, this is a flip from where we've been.  But various judges, as we have seen, in various part of the country, make various decisions based upon their particular reading of the case and specific examples.  I think it's going to take the Supreme Court ultimately to make a decision as we move forward about what is and is not usable for unlocking our devices.  And who knows the way it'll come down.  We're seeing judgments now on both sides.



Firefox 69 will finally disable Adobe Flash plug-in by default, in which case our Picture of the Week company is pretty much SOL.  They're finally going to have to have somebody design a regular website for them, rather than asking people to go download Flash in order to use their site.  The Flash plug-in is the last remaining NPAPI.  The NPAPI is, believe it or not, the Netscape Plug-in Application Programming Interface.  Yes, Netscape, brought to us back in 1995 with Netscape Navigator 2.0.  It was later adopted by other browsers.



The developer.chrome.com site says of NPAPI:  "NPAPI plug-in support for extension has been discontinued.  The documentation below is preserved for historical purposes only."  So Chrome wants nothing to do with this.  And at this point, Flash is the only thing still using the NPAPI, which Firefox reluctantly continues to support.  Over on developer.chrome it says:  "Warning:  NPAPI" - and I'm not making this up, it says - "is a really big hammer that should only be used when no other approach will work.  Code running in an NPAPI plug-in has the full permissions of the current user and is not sandboxed or shielded from malicious input by Google Chrome in any way.  You should be especially cautious when processing input from untrusted sources, such as when working with content scripts" - gee, like a web browser - "or XMLHttpRequest," the XHR, the standard way that JavaScript reaches out and performs queries, doing AJAX and so forth.



"Because of the additional security risks NPAPI poses to users, extensions that use it will require manual review before being accepted in the Chrome Web Store."  So it's like, yeah, it's still there.  But really, try not to need it.  So of course this is why continued Flash support is so inherently dangerous.  Flash uses the NPAPI, which is non-sandboxed, which means that anything that gets into your Flash uses one of its many security vulnerabilities - I mean, we know they've still got to be there because anytime anyone looks, someone finds some - would allow malicious activity in your browser.  So it's not sandboxed because sandbox didn't exist, and you could argue was not needed, back in 1995.  We were just lucky that our computers booted back then.  And sometimes they didn't.



LEO:  Right.



STEVE:  So once Flash has been disabled by default in Firefox, users will not be prompted to enable Flash.  But even then they will be able to activate Flash on certain sites using browser settings.  So this crazy company does continue to operate, is able to operate through 2019.  The final step for Flash on Firefox is due in early 2020, when Adobe also officially end-of-lifes Flash and it is completely removed from the consumer versions of Firefox.  Flash will continue to be supported in the Firefox extended support release, the ESR version, until the end of 2020.  And in 2021 Firefox will refuse to load the plug-in completely.



So Microsoft will also be disabling Flash by default in Edge and IE in mid to late 2019, so mid to late this year.  Google will be disabling Flash by default in Chrome 76, which is due for stable release around July.  Chrome users will be able to enable Flash in settings, but the plug-in will require explicit permission.  And then, as of Chrome 69, users need to give permission for each site to use Flash every time the browser is restarted, which is another nice deterrent.  So I'm reminding everyone of this.  I mean, the boom really is being lowered on Flash.



I'm reminding everyone because, aside from the security win of removing this longstanding nightmare from our browser ecosystem, every time I mention this I receive notes and email from our knowledgeable listeners within various enterprises who are still, who today remain seriously dependent upon this creaky old technology.  And it's not its age that I have a problem with.  I'd still be using Windows XP if my machine hadn't died and forced me to 7.  So it's not age that I have a problem with.  It's that it has always been buggy as hell, and Adobe never really cared to expend the time or energy to fix it.  They just kept patching it as people kept poking holes in it, and people kept getting hurt by it.  So its existence has hurt countless innocent Internet users, and the sooner it dies, the better.



So those knowledgeable listeners of this podcast whose enterprises - the people who write me every time I talk about this, go tell somebody that, if this is mission critical, someone's got to rewrite this for you.  And what often happens is I hear about they're using something that, yes, it's 15 years old, but they lost the source code, or the only people who knew how it worked are gone or died or who knows what.



LEO: Happens all the time, yeah. 



STEVE:  We've got these apps in COBOL, and we need to update them.  It's like, okay.  Anyway, so, I mean, at some point it's just, well, I mean, it looks like you could continue to use it through next year if you really have to, but not past that.



LEO:  I mean, if you've got an Intranet, you could force people to use old browsers and stuff like that; right?



STEVE:  Yeah.  Although you'd have to make sure they didn't go reach out to see if there was an update because the browser would go, whoa, am I old.



LEO:  Yeah, we have that problem.  We have a banking app, a check reader app that we've got to keep using old Mozilla.



STEVE:  Yeah.  So I got a piece of errata, Leo.  Mike D., I actually saw his note in the Security Now! newsgroup at GRC.  The subject was:  "Steve, it is pronounced 'Gee-Drah.'"



LEO:  Yeah, that's what I thought.  The "H" is silent.



STEVE:  Yes, "Gee-Drah."  He says:  "Love SN, have been listening from the beginning.  In terms of the current episode, clearly you are not spending enough time watching bad cinema," he says, "<chuckle>.  It - Ghidra, King Ghidra, King Ghidorah, and/or Monster Zero..."



LEO:  Don't.  Just stop right here.  Stop trying to read those names.



STEVE:  Thank you, one of the monsters from the same lineage of Japanese monster movies as - oh, Godzilla.  Godzilla I know how to say.



LEO:  Yeah, yeah.



STEVE:  "Mothra and many others.  It appears in several of those movies and even takes on Godzilla in one."



LEO:  Well, well.



STEVE:  Okay, I'm not being sucked into this.  I'm not going to go find out.



LEO:  Ghidra is good.  Oh, no.



STEVE:  But now we know it's Ghidra.  And also I forgot to mention last week, I had it in my notes - well, actually it was one of those things that - remember that my system was dead last week.  It was in fact the processor.  I've killed two of them.  I'm no longer overclocking because, you know...



LEO:  No more.



STEVE:  Even though nothing was getting hot, everything seemed fine, but it took about six months, and then it was the death knell.  So what I had forgotten in my notes was just a little note that you'll remember that when Windows 10 Disk Cleanup utility added the Downloads folder checkbox, I cautioned our listeners that, be careful, because we may have all had the habit of turning them all on, as I certainly had.  But make sure that, if you were turning the Downloads folder on, you intended to have it delete all your prior downloads, or at least all those that were in the Downloads folder.  I got a kick out of the fact that this was apparently causing so much trouble that there is now a warning dialog that pops up in Windows 10 Update to the Disk Cleanup if that's turned on.  So yes, it was biting people.



And my last little piece of miscellany before we talk about Zerodium was from our friend, Leo, Evan Katz.



LEO:  Oh, yes.



STEVE:  Who said:  "P.S.:  Yes, Filemail is amazing, and the best large transfer service that exists," exclamation point.  "I have used it for years."



LEO:  Good to know.



STEVE:  And I certainly trust Evan and his opinion on this.  I only had that one experience I mentioned because I had to move a nearly 6GB VM to Denmark, and I was just stunned by the fact that it saturated my upstream cable modem.  I've never seen anything do that for an hour or two at 33Gb.  It just - it was amazing.  So anyway, Evan, thank you for the confirmation.  And I've got someone whose opinion I trust who has used it to move things around for a long time.  So yay.



Okay.  I've been biting my tongue, not wanting to let the cat out of the bag because this is a ridiculous amount of money.  And who are we fooling about who are buying these?  I had to dig, in their own FAQ, like all the way to the bottom, every other possible question they had answered themselves.  But first let me step into this from the front door.



So their website proudly proclaims "We Are Zerodium, the leading exploit acquisition platform for premium zero-days and advanced cybersecurity capabilities."  Then their slogan is "We pay BIG [all caps] bounties, not bug bounties."



So, okay.  Under "Our Exploit Acquisition Program," they say:  "Zerodium is the leading exploit acquisition platform for premium zero-days and advanced cybersecurity vulnerabilities.  We pay big bounties to security researchers to acquire their original and previously unreported zero-day research.  While the majority of existing bug bounty programs accept almost any kind of vulnerabilities and proof of concepts, but pay very low rewards, at Zerodium we focus on high-risk vulnerabilities" - meaning, and I'm adding this, the juicy ones - "with fully functional exploits, and we pay the highest rewards."  And I'm going to skip the number here for a minute.



"Eligible research:  Zerodium is currently acquiring zero-day exploits and innovative security research related to the following products."  And so it's pretty much all of the mainstream everything everyone uses.  Operating systems we've got Microsoft Windows 10, 8.1, and servers.  Oh, look, but not Windows 7.  Good.  Apple macOS, Mojave, High Sierra.  Linux, BSD, CentOS, Ubuntu, et cetera.  VM Escape, VMware.  Web browsers:  They want remote code execution, or sandbox escape and bypass, or both in Google's Chrome, Microsoft Edge, Firefox, Tor browser, Apple Safari.  For clients and files, remote code execution or sensitive information disclosure from MS Office files, Word, Excel, PowerPoint.



PDF readers:  Adobe or Foxit.  Email clients:  Outlook and Thunderbird.  File archivers:  WinRAR, 7-Zip, and WinZip.  Smartphones:  Apple iOS, Android, Blackberry, Windows 10 Mobile.  Web servers:  Apache, Microsoft, Nginx, PHP and ASP, OpenSSL, mod_ssl.  Email servers:  MS Exchange, Dovecot - never heard of that one - Postfix, Exim, and Sendmail.  Web apps and panels:  cPanel, Plesk, Webmin; WordPress, Joomla, Drupal; vBulletin, MyBB, phpBB; IPS Suite, IP.Board; Roundcube and Horde.



And, finally, research and techniques:  Any other security research, exploit, or technique related to WiFi, Baseband RCE; routers, IoT remote code execution; antivirus remote code execution, exactly the attack surfaces we were talking about, AV; Tor de-anonymization.  Okay, who cares about that?  Well, we know.  Mitigation bypass.  All the notable mobile brands are listed by name.  Eligible Linux/BSD distributions, all of them.  Eligible routers brands, all the biggies:  ASUS, Cisco, D-Link, Huawei, Linksys, MikroTik, Netgear, TP-Link, Ubiquiti.



They said:  "Note:  If you have zero-day exploits for other products or systems not listed above, feel free to submit minimal details, and we will gladly discuss the opportunity."



So now what brought the news was last Monday, January 7th, under New Payouts Highlights, they said:  "Payouts for the majority of desktops, servers and mobile exploits have been increased.  Major changes are highlighted below."  So get this.  An Apple iOS remote jailbreak, meaning zero click, with persistence, now $2 million.



LEO:  Whoa.



STEVE:  $2 million.



LEO:  To whom would that be worth that much?



STEVE:  Exactly.  Exactly.  If these guys are paying $2 million and making a profit.



LEO:  Unless they're - maybe they're a front, though, for a nation state.  Maybe they're not resellers.



STEVE:  We don't know.



LEO:  We don't know.



STEVE:  That's a very good point.  If you need one click, if you have an Apple iOS remote jailbreak which does require a click, well, you used to be able to get a million.  But, no, now it's 1.5.  So $1.5 million if you're not quite skilled enough to do this with zero clicks, but you can do it with one click.  You could still get $1.5 million.  For WhatsApp, for iMessage, or SMS/MMS remote code execution, $1 million, doubling what it was previously, $500,000.



A Chrome remote code execution with LPE, that's of course Local Privilege Elevation, for Android, including a sandbox escape, used to only net you $200,000, now half a million.  Safari with a local privilege elevation on iOS, including a sandbox escape, same story.  Used to only be 200 grand, now half a million.  A local privilege escalation to either kernel or root for Android or iOS jumps, it's doubled, from 100,000 to 200,000.  And a local PIN or passcode or Touch ID bypass for Android or iOS went from - oh, this is, in terms of percentage, a big jump, from 15 grand to 100,000.



LEO:  Wow.



STEVE:  They said:  "Note:  Payouts were also increased for other products, including remote code elevation via document and media, remote code execution via man in the middle, ASLR or KASLR bypass information disclosure, et cetera."  And then on the server/desktop side -  that was all mobile.  And you'll note that because it is the hardest to get into, Apple iOS is at the top of that pack at $2 million for a zero-click or 1.5 for a one-click.  Chrome is down at 500,000 and Safari at the same.  So getting yourselves into Apple iOS, that's still the crown jewel.  And, wow, two million.



On the server or desktop, a Windows remote code execution with zero clicks via SMB - they give an example - or remote desktop protocol packets has doubled from half a million to one million.  A Chrome remote code execution on the desktop and a sandbox escape, including - so you can do something useful once you get out or once you get code to execute, that's doubled also from 250 to half a million dollars.  Apache or MS IIS, so either of those two major servers, a remote code execution, remote exploit via HTTPS requests - good luck with that, but maybe - doubled, quarter million to half a million dollars.



Outlook remote code execution from 150,000 to 250,000.  PHP or OpenSSL remote code execution went from 150 to 250,000.  MS Exchange Server, 150 to 250.  VMware, VM escape, a guest-to-host escape has doubled from 100,000 to 200,000.  And Windows local privilege escalation or sandbox escape, okay, now, this is exactly what SandboxEscaper had.  She had a Windows local privilege escalation and sandbox escape.



LEO:  She blew it.  She could have been rich.



STEVE:  Went from 50,000 to 80,000.



LEO:  That's a lot of one-man tents you could buy with that.



STEVE:  That's a lot of hikes you could do out in the wilderness.



LEO:  Yeah.



STEVE:  Yeah, I don't get it.  Anyway, so they say of their payouts:  "Zerodium payouts for eligible zero-day exploits range from $2,000" - I don't even know what that is, that's probably, I mean, there's nothing here that's less than $50,000, so I guess that's got to be easy to make two grand.  Anyway, "up to $2 million per submission.  The amounts paid by Zerodium to researchers to acquire their original zero-day exploits depend on the popularity and security level of the affected software and system, as well as the quality of the submitted exploit - full or partial chain, supported versions/systems/ architectures, reliability, bypassed exploit mitigations, default versus non-default components, process continuation, et cetera.  For more information, please read our FAQ.



"The payout ranges listed are provided for information only and are intended for fully functional, reliable exploits meeting Zerodium's highest requirements.  Zerodium may pay even higher rewards for exceptional exploits and research."  And I love they had this chart that I put into the show notes because it shows the Zerodium submission process.



We start with "You discover a high-risk zero-day vulnerability and manage to exploit it."  Then, "You submit minimal technical details about your research to Zerodium."  That's Step 2.  Step 3, "Zerodium confirms its interest in the research and sends a pre-offer."  Step 4, "You submit the full technical details and exploit to Zerodium."  Step 5, "Zerodium evaluates the research and sends the final acquisition offer."  Step 6, "You accept the Zerodium offer," so have a party.



LEO:  And profit.



STEVE:  "And receive your payment within one week."



LEO:  Fast.  Fast payments.



STEVE:  And then 6 leads back into 1 because, having gone through this circle once...



LEO:  Reinvest.



STEVE:  The cycle of life.  You start looking for the next really bad $2 million.



LEO:  SandboxEscaper needs to get to work here.



STEVE:  Yeah.  Now, exactly.  I don't get what's going on with her.  Make some money, honey.  Okay.  So we had, down in their timeline, they noted September 19th, 2018.  I wanted to dig around a little bit to see what more I could find out about who they are.  September 19, 2018, so late last year they wrote:  "We are acquiring pre-authentication remote code execution exploits affecting the following Routers:  ASUS, Cisco, D-Link, Linksys, MikroTik, Netgear, TP-Link, and Ubiquiti.  Exploits leading to authentication bypass or credentials disclosure are also accepted.  Exploits relying on cross-site scripting or cross-site request forgery are not eligible."  So here they are proactively soliciting a specific class.



LEO:  We have a client who would like to buy.



STEVE:  Exactly.  Exactly.



LEO:  Is this illegal in any way?  I mean, is it...



STEVE:  I mean, it really raises my hackles because it just seems like it ought to be.  On December 20, a few months later, late last year:  "We are currently looking for code execution exploits via USB drives on Windows and/or macOS.  The exploit must achieve code execution immediately after the USB key or drive is plugged into the system without relying on visible keystroke injections or user interaction."



And so I read these, and then I went looking, I went digging in their FAQ.  And all of the first 20 questions are people rubbing their hands together about how they get paid and can you transfer directly into my Swiss account and so forth.  Anyway, finally, down near the bottom:  "How the acquired security research is used by Zerodium."



LEO:  Oh.  How is it used?



STEVE:  Uh-huh.  They say:  "Zerodium extensively tests, analyzes, validates, and documents all acquired vulnerability research and reports it, along with protective measures and security recommendations, solely to its clients subscribing to the Zerodium Zero-Day Research Feed."  And that was the second to the last question.  And then we wrap with "Who are Zerodium's customers?  Zerodium customers are mainly government organizations in need" - they need them, Leo - "in need of specific and tailored cybersecurity capabilities and/or protective solutions to defend" - that's right, it's like the U.S. Department of Defense - "to defend against zero-day attacks.  Access to Zerodium solutions and capabilities is highly restricted and is only available to a very limited" - yeah, those with deep pockets - "number of organizations.  Zerodium does not have any sales partner or reseller.  Our solutions are only available through our direct channel."



So, yeah.  I guess, I mean, I would love to get some sense for the level of activity in this channel, even if we have nothing else, if just an alarm kind of went "bong" every time one of those transited, it would be interesting because we don't know what's going on.  I'm sure part of this research is, I mean, certainly the discoverers are locked up in an NDA.  They can't share with anybody else.



LEO:  Right.



STEVE:  In return for what they have discovered.  If a government organization is paying big bucks to be on the Zerodium feed, they're being careful with it because its entire value to them is only to the degree that it remains unknown to the world.  I mean, gee, what could you do with a USB drive that ran code on a Windows or Mac when you just plug it into a USB without requiring any keystrokes?



LEO:  Hmm.



STEVE:  Don't know.  Or why could you want to take over an Apple iOS mobile device with either zero or one keystroke?  Hmm.  How could that come in handy?



LEO:  Wow, that's really interesting.



STEVE:  Really, yes, to me it just seems bizarre that there is a marketplace now, I mean, an out-front, in-your-face, we will pay $2 million.  I mean, the thing that's a little annoying is that we know how porous security is.  The harder you look, the more you find.  And $2 million is a heck of an incentive for someone to dig down into somebody else's code and see if they can find a mistake somewhere.



LEO:  Yeah.  Yeah.



STEVE:  Wow.  Wow.



LEO:  Wow.  Well, there you have it, ladies and gentlemen.  Fascinating.  It's fascinating.  We have no idea who they - and when did they appear?



STEVE:  They've been around for a few years now.  We've talked about them several times.  And so an increase in bounty probably  means that the government feed, that may have increased in number or in cost, so that they can afford to pay more.  It might mean that security is tightening up, so they're having to incent the "researchers," unquote.



LEO:  That could be, too.  It could be supply and demand.  If the supply shrinks, maybe...



STEVE:  Yeah.



LEO:  Yeah.  They might have to pay more.



STEVE:  So it's like, please look harder because we really do want these.  Our customers are clamoring.



LEO:  It also means there's competition from other sources like the companies themselves.  Remember Apple for a long time refused to do this because they were afraid of ratcheting up the marketplace.



STEVE:  Well, Leo, Pwn2Own, I mean, Pwn2Own is developers basically saying, "Hey, I got a laptop," rather than $2 million, or for a lesser exploit, 200,000.  "I got a really cool sweatshirt."  Okay.



LEO:  I owned this Macintosh, and all I got was this T-shirt.



STEVE:  That's right.



LEO:  What is your judgment?  Is it harder to find these than it has been in the past, or easier?



STEVE:  Yes.



LEO:  It is harder.



STEVE:  Yes, yes.



LEO:  Because we're all aware of it, and companies are working much harder to protect security, Microsoft chiefly doing a lot to lock its operating system down.  Yeah.



STEVE:  Yeah.



LEO:  Well, what a great subject, and thank you for another great show.  I appreciate it.  You'll find Steve at GRC.com, the Gibson Research Corporation.  That's where he does his main work, which is of course SpinRite, the world's best hard drive maintenance and recovery utility.  And he also gives away a lot of stuff.  ShieldsUP.  He talks a lot about passwords there.  There's also health information and of course the latest on SQRL.  Soon, soon, SQRL will emerge into the world.  The little baby will be born.



STEVE:  A little bushy little tail.



LEO:  Bright-eyed and bushy-tailed SQRL.  Steve also has the podcast there, audio and transcripts of every show at his website, GRC.com.  You can get them there or get them from us at TWiT.tv/sn.  We also have video.  And he can be reached and followed at @SGgrc, that's his Twitter handle, @SGgrc.  You can DM him there, too, if you've got a tip.



If you are interested in watching us doing the show live, you want the latest, freshest security as we bake it, so to speak, security news, you can go to TWiT.tv/live every Tuesday around 1:30 Pacific, 4:30 Eastern, that's 21:30 UTC.  And you can join the folks watching live in our chatroom at irc.twit.tv.  But as I said, you can always get on-demand versions on our site, on Steve's site, or your favorite podcast application.  Subscribe, and you'll get it automatically.  I mean, it's available, and you can listen at your leisure.



Steve, have a great week.  We'll see you next time on Security Now!.



STEVE:  Thank you, Leo.  I'm sure the week will bring us lots of new things to talk about next week.  So till then, bye.



LEO:  Juicy stuff.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#698

DATE:		January 22, 2019

TITLE:		Which Mobile VPN Client?

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-698.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm 



DESCRIPTION:  This week we examine a very worrisome WiFi bug affecting billions of devices; a new fun category for the forthcoming Pwn2Own; Russia's ongoing, failing, and flailing efforts to control the Internet; the return of the Anubis Android banking malware; Google's changing policy for phone and SMS app access; Tim Cook's note in Time magazine; news of a nice Facebook ad auditing page; another Cisco default password nightmare in widely used, lower end devices; some errata, miscellany, and listener feedback.  Then we answer the age-old and apparently quite confusing question:  Which is the right VPN client for Android?



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We'll talk about the upcoming Pwn2Own contest.  You won't believe what the hackers are being asked to attack next.  We also have details on Steve's favorite OpenVPN client for Android, and a good reason to be very careful about which banking tools you use on Android.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 698, recorded Tuesday, January 22nd, 2019:  Which Mobile VPN Client?



It's time for - oh, wait a minute, let me make sure I'm recording.  I am.



STEVE GIBSON:  Oh, please do press the red button.



LEO:  Did I ever not record Security Now!?



STEVE:  Once.



LEO:  Once.



STEVE:  Once, actually maybe twice in the entire 13-year history we would get all done, and then it was like, ooh.  But, you know, I have to say, Leo, the second time was better than the first time.



LEO:  It often is, yeah.



STEVE:  It was painful, but we did have a dry run rehearsal, so...



LEO: So sorry.  Well, that will never happen again because I am no longer in charge of recording shows. 



STEVE:  And it hasn't happened for a long time.



LEO:  No.  No, at least a decade.  I remember the worst one was a Floss Weekly episode where I erased it twice, and I had to call the guy back for a third time.  But we are recording.



STEVE:  So we are closing in on Episode 700.  We're at 698 today, the last podcast - wait, wait, no.



LEO:  It has been a long time.



STEVE:  It'll be the 29th will be the last podcast of the month, so next Friday.



LEO:  Your first podcast in February.



STEVE:  So a piece of news that Bleeping Computer ran caught my attention because it was about the 150 bad VPN clients for Android in the Google Play Store.  And I thought, what?  Well, first of all, that there were bad ones, I guess, wasn't that big a surprise.  But that there were 150 VPN clients, just I couldn't believe it.  So there is only one.  One.  And I thought, okay, we're going to call this podcast "Which Mobile VPN Client?"  We're going to answer that question once and for all for the ages, Leo, if you want to use a VPN, what client should you use and, of course, what VPN.  So that's what we'll get to.



But there was a lot of news.  There was an extremely worrisome WiFi bug which affects, if we believe Marvell that makes the chip which was found to be defective when they were boasting in their marketing material about their chip being in billions of devices, then this WiFi bug could not be worse, and it's in billions of devices.



LEO:  Oh, my goodness.



STEVE:  It's not good.  So we're going to start with that.  Then we've got a new, and you're going to love this one, this one is like in your wheelhouse, a new fun category for March's - the end of March is the forthcoming Pwn2Own competition.  And you might be able to guess.  But anyway, definitely fun new topic or new category for Pwn2Own.



We've got Russia's ongoing failing and flailing efforts to control the Internet, and they're now setting their sights on two new players.  We talked previously, remember, about how they tried to block Telegram and what a disaster that was, that ended up blocking ranges of Google and Amazon's cloud services and making a mess of things.  Well, they're now going after a couple more players, probably without much more success.



We've got the return of the Anubis Android banking malware, which was found to be using a clever new trick to avoid detection that I just had to share with our listeners because it's like, okay, this, I mean, we really are in an unending cat-and-mouse game here.  We've got Google's announcement of their changing policy for phone and SMS app access.  They're clamping down.  I also wanted to talk about the first thing you guys talked about on MacBreak Weekly, which is Tim Cook's note in Time magazine.



LEO:  Oh,  yeah, good, yeah.



STEVE:  We also have news of a nice Facebook ad auditing page I wasn't aware of.  But it resulted in the creation of the first bit.ly link I have created in a long time, snfbads.  I tried to not have "sn" in there, but every kind of variation of Facebook or FB or anything was already taken.  So it's bit.ly/snfbads, for anyone who wants to jump ahead.  And so we're going to talk about that.



There's another, believe it or not, Cisco default password nightmare.  But this time not in some obscure high-end switch that no one's ever heard of, but in widely used, lower end SOHO consumer devices.  We've got some errata.  I said something last week I'm just so embarrassed about, I can't believe I said it.  But we're going to fix that.  We have some miscellany, some listener feedback, and then we're going to answer the age-old  and apparently quite confusing question:  Which is the right VPN client to use for Android?  And once we get back from our first break, we need to talk about Oscar's latest DEC PDP reconstruction resurrection because, boy, this guy has just done an amazing job.



LEO:  Nice.  Well, we've got a full lineup of stuff to talk about.  I can't wait.  Picture of the Week?



STEVE:  So Oscar is someone I've mentioned before.



LEO:  Yeah.



STEVE:  He's this amazing craftsman.  He produced a clone of the venerable DEC PDP-8 initially.  And I own several of them.  I talked about it.



LEO:  They're over Steve's right shoulder right now.



STEVE:  Well, no.  Those are actually different.  Those are, shoot, I can't...



LEO:  Is that that kit that you built?



STEVE:  Well, Oscar's is different.  The ones over my shoulder were based on the Intersil 6100 chip, which was an actual PDP-8 on a chip.



LEO:  Oh.



STEVE:  The problem is those are all gone.  They're like hen's teeth now.  Do hens have teeth?  Anyway.



LEO:  They don't.  That's the whole point.



STEVE:  Ah.



LEO:  Get it?  Rarer than hen's teeth?



STEVE:  I get it.



LEO:  They've got gullets.



STEVE:  So the problem is, if you can't get a PDP-8 on a chip, what are you going to do?  Well, what you're going to do is you're going to emulate the machine on a Raspberry Pi.



LEO:  Of course.  Which is probably more powerful than a PDP-8 was.



STEVE:  Oh, my god, yes.  



LEO:  A lot.



STEVE:  Yes.  Way faster.  And in fact there is a whole underground, well, I guess they're not really under the ground, but they're off to the side.  They're out of the mainstream, that's what we want to say.  They're nonmainstream.  But they're fascinating people who are keeping old machine architectures alive by creating mature simulations of their architectures.  I mean, to the point where they can run the original operating systems and mount disk packs, virtual disk packs, and do everything.  Like you used to with the raised floor in the air conditioned environment and everything.



LEO:  Wow, that's awesome.



STEVE:  Okay.  So Oscar's first piece of work was a PDP-8.  This one is a PDP-11.



LEO:  This is beautiful.  This is gorgeous.



STEVE:  It is just stunning.



LEO:  Oh, man.  Including the toggle switches on the front.



STEVE:  He did custom injection molding for the toggle switches and the panel.  The whole thing is an exact duplicate of the PDP-11/70.  And in fact, on his page, it's Obsolescence is his website, .wixsite.com.  I have a link in the show notes for anyone who's interested.  And for some of our listeners you will be when you understand - and I don't understand this at all, Leo.  I don't get it that he wants to sell these kits for $250.  I mean, he is selling them for $250.



LEO:  Wow.



STEVE:  Apologizing.  He says:  "I apologize because that's almost $90 more than the PDP-8.  But if you knew the upfront cost of making that injection-molded case..."



LEO:  Oh, yeah.



STEVE:  "...you would understand it's not because I am turning commercial.  This is still very much a hobby project, although one that got slightly out of hand."



Anyway, okay.  So again, there's a Raspberry Pi behind the panel.  So what he did was he exactly duplicated the functioning of the front panel, you know, blinking lights and switches.



LEO:  Oh, I've got to get one of these.  This is amazing.



STEVE:  It is just gorgeous.  Anyway, so he sat with a buddy in front of an actual 1170 and, like, pushed switches while video recording in order to get any edge cases of the way the actual 1170 front panel operated so that he could exactly emulate it.  And then look at the screen on his laptop.  That's - he, like, faked a CRT bezel on his ThinkPad to show the way a CRT would look with the bezel for the actual console when you're logged in and giving commands to the PDP-11.



Now, okay.  So there's elegance to both the 8 and the 11.  The elegance of the 8, the PDP-8, was could you make a computer with five AND gates.  Okay, well, not quite, but still.  Remember that the opcode was three bits on a PDP-8.  So you had a total of eight instructions, and there's no subtract.  You have to jump through hoops to pretty much do anything.  So the PDP-8 is arguably a perfect platform if you just kind of wanted to get started to show someone the concept of ones and zeroes and with a few instructions how you can do something.  And frankly, I'm stunned by the fact that there's OS/8 is an operating system for the PDP-8, written with eight opcodes, I mean, eight instructions.  So, okay.



But the 11 is different.  The 11 is the other end of the spectrum.  It is a stunningly elegant instruction set.  And it was the birthplace of Unix.  It was on a PDP-11 that the C language was born.  Thompson and who was the other guy...



LEO:  Kernighan.  Kernighan.



STEVE:  Kernighan did C.  Thompson...



LEO:  Ritchie.



STEVE:  ...and Ritchie, yes, did Unix.  And they wrote Unix, toggling it in through the front panel because you had to start somewhere.



LEO:  I have their books, you know, the Unix programming language and the C.  I should get this.  I even have the source code.  You know they released the source code in a big bound book.  That would be fun.



STEVE:  Well, and what's so neat about this is that it's not just a front panel that does nothing.  You get all of the PDP-11 code and source and OS and Unix and everything the way it was back then.  For $250.



LEO:  I'm ordering it right now.  Wow.



STEVE:  You do have to add a Raspberry Pi, but what are those, 40 bucks or something.



LEO:  Yeah, 35.  I've got one lying around.  Most people do these days.



STEVE:  Yeah.



LEO:  Wow.



STEVE:  Anyway, and so I just, you know, I'm afraid we're going to bury Oscar in orders.  He is sending me one because I have to have one of these.  It's just stunning.  So I just wanted to share.  I know from the past that our audience was interested in the PDP-8 and that my talking about it and just, I mean, once this is gone, it's gone.  I mean, we don't know that nobody else in the future is going to ever do this again, but I don't think anyone's going to do it like this.  This is a work of art.  I mean, this is just a masterpiece.  So take a look at the show notes.  It's PiDP-11.  I didn't check to see whether Google would find it if we put in PiDP-11.  Let's see what happens.  No, didn't come up.



LEO:  I think if you just type "obsolescence" and "PiDP-11," something like that, because the site is obsolescence.wixsite.com.  So I bet, if you added the word "obsolescence," that would find it.



STEVE:  Yup, that brought it up.  "Obsolescence PDP-11."



LEO:  That's awesome.



STEVE:  And anyone can find it that way.  And oh, my god, it's just - what a toy.  And so...



LEO:  I just added my name to his email list because you can't order it directly.  You have to email him and all that.  Yeah, that would be fun to have here.



STEVE:  He's a good guy.  And oh, yeah, I need more blinking lights behind me, Leo.



LEO:  Well, I tried to get the 8, and he forgot to send it to me.  And then by the time he remembered it was gone.  So Oscar, I want one of these.  And I'll send you the money, don't worry.



STEVE:  I'm sure Jason could throw this together and get it up and going and have some fun with it.



LEO:  Oh, it'll be fun, yeah, yeah.



STEVE:  Okay.  So on a much more serious note, we have an incredibly widespread WiFi firmware bug which by all measure affects probably billions of devices.  The title of Embedi's disclosure was "Remotely compromise devices by using bugs in Marvell Avastar Wi-Fi:  From zero knowledge to zero-click RCE."



Okay.  So this is one of the most popular, if not the most popular WiFi chipset on the market.  In digging around, I wanted to find out whether it was possible for its firmware to be reflashed, and I could not get an answer.  You have to go under NDA with them in order to get access to the pinouts and the programming of the chip.  But it's highly unlikely that an SOC, as this is called, a System on a Chip, has reflashable firmware because there would just be no need for it.  Or even if it did, you probably can't get to it from the main processor of the system that it's mounted in.  You would need to physically hook on a JTAG debugger in order to get to reflash the firmware, if it's even possible.  So we're probably all - "we" the industry, the world - stuck with this WiFi chip as it is.



But in the process of digging in to see what I could find, I discovered, I mean, it's been selling for a decade.  So, I mean, this particular chip, the 88W8897, it's everywhere.  It's in the Sony PlayStation 4, the Xbox One.  Microsoft's Surface laptops use it.  Samsung's Chromebooks use it.  The Samsung Galaxy J1 smartphones, the Valve Steam Link cast devices, some other laptops.  There are consumer routers and embedded devices and other network hardware.  So, I mean, it's sort of the go-to WiFi chip.  What drives it is an embedded RTOS, a real-time operating system known as ThreadX.  And there's been some confused reporting about whether the bug is in ThreadX or in the implementation of it.  And it looks like it's actually in the implementation, in the code that was written for this real-time operating system.



So the story begins late last spring, when a researcher, Denis Selianin, who is with the embedded security firm Embedi that I mentioned before, he was experimenting with fuzzing that very highly popular WiFi chipset.  And we've talked about fuzzing before.  The idea is in some cases you can't find problems by inspection.  So what is done is you sort of automate the process.  You just throw a whole bunch of stuff at it.  And if something you throw at it makes it crash, then you go, oh, what just happened?



So the idea is that's the process known as fuzzing, where you record what you're throwing, and you also watch to see if what you threw crashed the chip.  And, if so, then you say whoa, and you back up, and then you fuzz again, verify that it's reproducible, and then from there - so basically you're using just a random process to find edge cases, things that the programmers missed.  So he was doing that.  And I saw the log.  It's like for four point some days of this.  And then he found a problem, dug into it, figured out what was going on, ended up finding four problems, two of which are critical.  And when we say "critical," we're not kidding.



So if any of these WiFi devices that I mentioned are powered up, the bugs he found would allow malicious attackers to force them to execute arbitrary code of the attacker's choice without requiring any action on the part of the device's owner.  In other words, this could not be worse.  Well, okay.  It would be worse if you could do it from Russia.  The good news is, well, sort of the good news is it is WiFi, so you need to be within WiFi range.



LEO:  Well, that's good.



STEVE:  Well, okay, yeah.  So it means that it's not...



LEO:  It's more of a concern for a business than an individual, though.



STEVE:  True, well, yeah, unless - well, certainly you could be targeted.  And if you went to Starbucks there's a lot of, I mean, the idea is that any device with this chipset that receives this malformed message will execute code. 



LEO:  So is this chipset in a WiFi access point?  Or it's in computers; right?  This is a...



STEVE:  Both.



LEO:  It's both.



STEVE:  It's like, yeah.



LEO:  It's everywhere.



STEVE:  It's widely used.  It's MIMO.  It's Bluetooth.  I mean, it's a super popular chip because these guys did a beautiful job, except there's a little bug in the firmware of the chip.  So one of these bugs is specific to this particular 88W8897 WiFi controller.  But the other bug may be based on the ThreadX  operating system.  Now, details are being withheld because he did then inform - Embedi informed Marvell, I think it was in May, yeah, May of 2018 that they had found the problem.



But again, so here's the problem is that it could certainly be the case that chips moving forward will have this fixed.  But there is almost, I mean, I don't know for sure.  I could not get an answer.  Everybody is being tight-lipped about this because this is really bad.  This is in billions of devices all over the world.  And the reason we're not hearing more about it yet is that proof of concept has not been released.  There is a demo of this happening.  The guy showed it at a conference in November.  So there's a demo of it happening online, but no proof of concept required.



He says it requires no user interaction.  It can be triggered in - oh.  What happens is every five minutes the typical WiFi chip reaches out to enumerate WiFi signals, WiFi base stations, access points, users, whatever, within its range.  So at that point the device could be compromised.  It requires no knowledge of the WiFi network name or passphrase or key.  You need to know nothing about it because it's such a low level.  It's down at the low level, you know, the ether link protocol level of the chip and gives the attacker arbitrary code execution on this WiFi SOC, this system on a chip.



LEO:  This is in the Surface computers.



STEVE:  It is, yes, it's in the Microsoft Surface machines.



LEO:  Oh, boy.  Oh, boy.



STEVE:  Yeah, and so here's the problem is it's not clear this can be fixed.  I mean,  I don't know one way or the other.  I've got a link in the show notes to the demonstration proof of concept video.



LEO:  It's in my Xbox One.



STEVE:  Yeah.



LEO:  Oh, my god.



STEVE:  Yeah.



LEO:  The J1 smartphones, fortunately, the Galaxy J1's aren't super popular.  But Samsung Chromebooks are.  Surfaces are.  Xbox Ones and PlayStation 4's are.



STEVE:  Yeah, uh-huh.



LEO:  Holy cow.  Yikes.



STEVE:  So I have a feeling we will be following this story for the balance of the year.  The problem is that there's enough information that is already disclosed that it's probably possible for bad guys to independently follow in the footsteps of Embedi and reconstruct this.  And once they do release something, once Marvell has some sort of response to this, these guys have said they're going to release a proof of concept and the details of their tools.  But the problem is we know billions of devices are not going to get their firmware updated.  And this is just so juicy, the idea of no click, no authentication, over the air.  You don't have to physically touch a device.  You just have to zap it with the proper packet when it reaches out, and you can take it over.  It's going to be, just for like the real hackers, it's going to be - they're not going to be able to control themselves.  We will see exploits for this.



LEO:  So I could go sit in a coffee shop.  Now, nobody - of course, the code's not out there.  But when you say "owned," could I just start capturing all the packets coming out of that device, for instance?



STEVE:  Yes, for instance, exactly. 



LEO:  Okay.  Okay.



STEVE:  And you could also turn around and then access the system that that WiFi chip is sitting on because the chip itself is on the bus.  So you could then access probably main memory of that chip and rifle around and get private keys and so forth.  Because we don't want anybody in our main memory.



LEO:  Yikes.



STEVE:  Yeah.  Because the way the architectures are now, the bandwidth of these chips is so high that they have direct DMA access into main memory, that is, when they're receiving data, it is being streamed directly into the main memory of the system, which means that the chip itself...



LEO:  They can read it.  They could read it.



STEVE:  Exactly, has access.



LEO:  Oh, man.



STEVE:  Yeah.



LEO:  Yikes.  So that's worse than capturing your packets.  That's seeing everything that's going on.



STEVE:  Yeah.  They can rummage around.  I have a feeling this is going to be a field day for the bad guys.  And in a lot of machines that, as you said, that people have.  You have a bunch of them already.



LEO:  I do.  I have all of them.  Oh, boy.



STEVE:  Okay, Leo.  Get this.  Pwn2Own adds a Tesla Model 3.



LEO:  Oh, crap.  Please don't pwn my Tesla. 



STEVE:  No, that's good news because you want these guys to find and fix the bugs.



LEO:  Oh, yeah.



STEVE:  And in fact, if they do, $900,000 worth of prizes for the Tesla.  So there's money in them thar hacks.  This will be at March's forthcoming - I think it's March 2022nd - forthcoming, the CanSecWest in Vancouver, which we talk about every year because it's so much fun.  It will for the first time add automobiles to its hacking target.  So the biggest prizes will be a quarter million dollars for hacks that execute code on the Tesla Model 3's three primary systems.  There's something that's generically known as the "gateway," there's the autopilot, and the VCSEC.  The gateway is the central hub that interconnects, as it sounds, as you would expect, the car's power train, the chassis, and other components; and processes the data that they are exchanging.  The autopilot, of course, is the driver assistance feature that helps control lane changing, parking, and other functions.  This VCSEC is the Vehicle Controller Secondary.  That's the term used.  It's responsible for security functions, including the car's alarm.



So these three systems represent the most critical parts of a Tesla, so it's clear why hacks successfully targeting them would be eligible for big payouts.  To qualify, the exploits must force the gateway, autopilot, or this VCSEC to communicate with a rogue base station or other malicious entity.  Then stepping down from that, a denial of service attack which could take out, for example, the car's autopilot, will pay $50,000.  So they want to know if you can do that.  But that's a step down from forcing one of those three critical systems to have a communication with a malicious external entity.



Also, Pwn2Own will pay $100,000 for hacks that attack the Tesla's key fob or phone as key, either by achieving code execution, unlocking the vehicle, or starting the engine without the key.  It'll pay $100,000 add-on prize for winning hacks in another category that attack the car's Controller Area Network.  We've talked about this in the past, the so-called CAN bus, which is the bus that interconnects literally everything.  It's no longer the case that there's a big wiring harness like you and I used to have in the first cars that we owned, where you could peel back the floorboard mat, and you'd see this huge bundle of wires going to the taillights and the brake lights and the backup lights and the turn signals and everything.  That's all gone.  Now everything just gets power and a signal, and all of the light-on, light-off stuff is multiplexed over this CAN bus.



So hacks targeting the car's infotainment system is also on the menu, which will earn a successful exploiter $35,000.  And hacks which escape the security sandbox or escalate privileges to root or access the car's OS kernel will fetch $85,000.  WiFi or Bluetooth hacks get $60,000.  And there's a separate add-on payment of 50 grand which will be paid for winning hacks that achieve persistence - which means, of course, that once they get in, they set up shop in your car and survive a reboot of the system.  So anyway, at the end of the month, let's see, the Tuesday following happens to be my birthday, March 26th.  So as I turn 64, we're going to find out about how the Tesla Model 3 withstood the CanSecWest Pwn2Own.  And of course there's always, well, as always, there are attacks against things without wheels.  We have a quarter million dollars for a success - boy, these contests are...



LEO:  This is a business.  You've got to get in this business.  There's some money in there.



STEVE:  They're making some money.  Quarter million dollars for a successful Hyper-V client guest-to-host escalation.  And respectively, $150,000, $70,000, and $35,000 for hacks of VMware's ESXi, VMware Workstation, and Oracle VirtualBox, respectively, so stepping down.  The web browser attack category will pay $80,000 for hacks of Chrome and Microsoft Edge, and in the case of Edge, with a Windows Defender application guard specific escape, so you've got to get out in order to do something.  And a Firefox exploit will net $40,000.  And the coverage of this, Trend Micro puts this on.  The server-side category is much smaller this year, with Microsoft Windows Remote Desktop Protocol, the RDP, as the only target.



Oh, and Trend Micro noted that most of their server-side targets had moved to their targeted incentive program, so they're no longer needed to be in Pwn2Own.  They've sort of changed where that's being done.  But still, a successful exploit of Remote Desktop Protocol will bag its user or its exploiter $150,000.  Of course, that's significant because we have been talking about cross-Internet RDP exploits, which it's surprising to me how many people have Remote Desktop Protocol exposed, and it's just not secure enough to let people get to it.  So at the end of March we'll have some news about how all that's going.



Russia is not doing so well with blocking Internet services they dislike.  As we discussed at the time, back in April, and as I mentioned at the top of the show, the Russian agency responsible for censoring Russian citizens' access to the Internet - and I can't pronounce this, Roskomnadzor.



LEO:  Exactly right.



STEVE:  Thank you, Leo.



LEO:  Perfect.



STEVE:  They attempted to block, as we know, Telegram, after Telegram ignored their threats of blocking the service.  We'll recall that, after that initial block, Telegram moved their servers into the cloud network space, you know, being served by Amazon and Google, which resulted in Russia blocking wide swaths of IPs, which blocked many more critical services than just Telegram.  So it turns out it's easier said than done to block a service.  And of course, for their part, Telegram users evaded the blocking by using VPNs and various available proxy services, after which Russia again countered by expanding its block list and ended up blocking even more.



Oh, and Reuters later reported, in August of 2018, that Russia then started testing, and kind of went off of the news map, "more precise technology to block individual online services," though really it didn't make much news after that.  And way back before that, back in 2016, Russia had also attempted to block LinkedIn with limited success.  So they want control over what happens in their country.  And the problem is, as I said, it's easier said than done.  So we're talking about this now because Russia has now set their sights on Facebook and Twitter.



LEO:  Oh, boy.  Oh, good luck.



STEVE:  Yeah, I know.  Exactly.  Good luck.  Roskomnadzor...



LEO:  Roskomnadzor.  If you say it like that, it just rolls off the tongue.  Roskomnadzor.



STEVE:  Much better, Leo.  Last December 17th that agency who you just mentioned convincingly sent letters to both Facebook and Twitter accusing them of failing to comply, as indeed they were, with a law requiring all servers that store personal data to be stored in Russia.  They just said "Eh.  No."  The letters gave each company 30 days to provide a, quote, "legally valid response," unquote.  Well, that time has passed.  That was up last week.  And neither company even bothered to reply.



So the Wall Street Journal now reports that, today, that agency begins administrative proceedings against both companies.  The Russian censorship agency said:  "The social media networks hadn't submitted any formal and specific plans or submitted an acceptable explanation of when they would meet the country's requirements that all servers used to store Russians' personal data" - I mean, come on, give me a break, for Russian privacy's sake - "to store Russians' personal data be located in Russia."  Russia has previously threatened to block Facebook over its ongoing noncompliance with this data storage law in 2017 and in 2018.  So this is not new.



Anyway, this should provide some interesting fodder for the podcast because, I mean, in the case of Telegram, you would think that would be more easy.  And, you know, I wouldn't be surprised to learn that these large global mega services had deliberately adopted network architectures.



LEO:  Oh, yeah.  They're multi-homed.  They're all over.  Yeah.



STEVE:  Exactly, that made blocking them extremely difficult.



LEO:  By the way, Roskomnadzor is the FCC of Russia.  And they call it RKN, if you just want to make it simple for yourself.



STEVE:  Roskomnadzor.  See, now that you said it right, I can see it.



LEO:  Yeah, it's easy.  Everybody knows Roskomnadzor.



STEVE:  Roskomnadzor, of course, Roskomnadzor.



LEO:  In Soviet Union, Roskomnadzor calls you.



STEVE:  You taught me how to pronounce Huawei, Leo, and I held onto that.



LEO:  I have no idea if I'm saying it right.  I'm just spelling it out.  But...



STEVE:  I like it.



LEO:  Literally, RKN is allowed.



STEVE:  Okay, good.  I'm going to name a podcast, one of our episodes, Roskomnadzor.



LEO:  And then you'll have to say it.



STEVE:  Mark my words.  When we have our big story about the failure of Russian censorship, "Roskomnadzor" we will proudly proclaim.



LEO:  It's funny because we think of the great firewall of China.  I mean, China blocks Twitter and Facebook.  You know, you figure, well, it must be doable.  But of course that requires a country where the entire Internet access goes through the government.



STEVE:  Yeah.



LEO:  And Russia's not, I guess, in that position yet.  Yet, I say, yet.



STEVE:  Yeah, well, and, I mean, the fact is the Internet was designed to resist this.



LEO:  Yes.



STEVE:  I mean, it was, like, on purpose.



LEO:  Routes around damage.  That's considered damage; right?



STEVE:  Exactly, yeah, yeah.  So speaking of damage, Anubis is the banking malware you really don't want in your Android smartphone if you do any online banking with your smartphone.  Which really, you know...



LEO:  Who doesn't?



STEVE:  I would - yeah, okay.  So get this.  Trend Micro found two instances of this Anubis malware using what they called "motion-based evasion tactics."  So first of all, this Anubis trojan has been observed, I mean, it is very competent.  It has been observed to attack 377 different bank applications, from 93 countries around the world, aimed at a bank I've never heard of, Santander?



LEO:  Santander is, yeah, no, that's a big bank.



STEVE:  Okay.  Santander, RBS, NatWest, Citibank, as well as non-banking apps such as Amazon, eBay, and PayPal, among obviously many others.  So it is an aggressive and capable banking malware trojan.  Trend, as I mentioned, recently discovered it hiding inside two Android Google Play Store apps, which each had dozens of fake five-star ratings and thousands of installations into Android devices.  So this thing had gotten into people.  So the two apps are Currency Converter and BatterySaverMobi.  So again... 



LEO:  Don't install those two.



STEVE:  Yeah, be a little skeptical, yes.  What sets these apart from other malware-carrying Android apps is their use of their host's motion sensors to detect whether they've been installed in a malware analysis sandbox, in which case their malicious behavior is suppressed, and they behave themselves.  Isn't that clever?



LEO:  So if it's in a sandbox, it doesn't get carried around.  It's not like a real phone; right?



STEVE:  Exactly, and you don't have some random motion sensor sending data, accelerometer data, into the OS.  So that'll just be sitting there doing nothing.  Now, again, this is all a cat-and-mouse game because, now that the sandbox guys know this is a possibility, it's like, ah, we'll add emulation of motion to the motion-sensing API in the sandbox to trigger motion-sensitive awareness in malware and be able to catch this happening.  But again, this is why it's just like this back-and-forth game.  Creating a secure system, which we keep seeing that creating a secure system which is also powerful and flexible is something which has so far eluded the best brains in computer science.  Because you could argue, you know, this is an important thing.  And we haven't figured out how to do it yet.



We know that we could have a system which is closed and secure, like the original Apple iPhone.  But people want apps.  We want open stuff.  I certainly do.  And we want capable apps.  But so far, with app capability comes app liability and exploitability.  And despite all of our best efforts, we haven't yet figured out how to get the one that we want, which is capability, without inviting the others we don't want.  And then there's social engineering.  I put on the previous page of the show notes, Leo, this is a dialog that this Anubis app presents because it needs...



LEO:  It would fool me.  It would fool me.



STEVE:  Yes.  It would fool anybody.  Yes.



LEO:  It's a system update dialog which you see all the time on Android.



STEVE:  Exactly.  And when you click it, you're giving the Anubis malware admin permissions on your system.  So, I mean, the very fact that that is possible demonstrates that we're not there yet because who would not click that?  It'd be like, oh.



LEO:  I'd click it, yeah.



STEVE:  Yes, yes.  Anybody would.



LEO:  Now, you have to download one of these apps onto your phone from the Play Store?  Or, no, they're not in the Play Store.



STEVE:  Yes, they're in the Google Play Store.  



LEO:  Not any more they're not.  They can't be.



STEVE:  Well, no.  They've been yanked.  But thousands of people did download them from the Google Play Store, where they were, with fake five-star ratings.



LEO:  And they do have to ask permission, so they pop this up.



STEVE:  Yes.



LEO:  And then I presume you get a legit permissions dialog.  But you assume it has to do with a system update, so you say okay.



STEVE:  Yeah, I mean, who knows, yeah, exactly.



LEO:  Wow.



STEVE:  Yeah, I mean, so the problem is, of course, social engineering.  We're going to click on that, and that's going to give this thing rights.  So we have a ways to go before we figure out how to do this stuff in a secure fashion.  And to that end, Google is cracking down on Android phone- and SMS-using apps, that is, apps that ask for phone and SMS permissions.  Last Monday Paul Bankhead, who's the director of product management for Google Play, posted to the Android Developers Blog, so this is written, as you will hear, to developers.  But I wanted to share this with our listeners because I'm glad Google is taking these steps.  It's going to cause problems; but, unfortunately, we've been too permissive in the past.



So his first little paragraph, TLDR, says:  "As previously announced and directly communicated to developers via email" - in other words, don't blame us, we warned you - "we'll be removing apps from the Google Play Store that ask for SMS or Call Log permission and have not submitted a Permissions Declaration Form.  If you have not submitted a Permissions Declaration Form, and your app is removed, see below for next steps."



And then he explains, he says:  "We take access to sensitive data and permissions very seriously."  Well, yeah.  They do now, or increasingly do.  He says:  "This is especially true with SMS and Call Log permissions, which were designed to allow users to pick their favorite dialer or messaging app, but have also been used to enable many other experiences" - I would, like, put air quotes there - "'experiences' that might not require that same level of access.  In an effort to improve users' control over their data, last October we announced we would be restricting developer access to SMS and Call Log permissions.



"Our new policy is designed to ensure that apps asking for these permissions need full and ongoing access to the sensitive data in order to accomplish the app's primary use case, and that users will understand why this data would be required for the app to function."  And of course we've also often talked about the overbroad, like you install an app and it needs access to all kinds of stuff that seem completely tangential to the app's intention or purpose, or like why would it need that.



So, he says:  "Developers whose apps used these permissions prior to our announcement were notified by email and given 90 days to either remove the permissions, or submit a Permissions Declaration Form to enable further review."  Anyway, he says:  "We take this review process seriously and understand it's a change for many developers.  We apply the same criteria to all developers" - so you know they're not taking sides - "including dozens of Google apps.  We added to the list of approved use cases over the last few months as we evaluated feedback from developers."  So in other words, they allowed developers to explain why they needed access to this, even though you wouldn't at first blush maybe have thought so.



He says:  "Our global teams carefully review each submission.  During the review process, we consider the following five points.  First, likelihood that an average user would understand why this type of app needs full access to the data.  Second, user benefit of the feature."  That's really good.  I mean, I hope Google's really, I mean, it sounds like they're really going to honor this, and they should, the user benefit of the feature.  "Third, the importance of the permission relative to the core functionality of the app.  Four, risks presented by all apps with this use case having access to this sensitive data," meaning if everybody asks for this, what does that mean?  "And then, fifth, availability of more narrow alternatives for enabling this feature."  In other words, is there a better way to achieve the same thing that doesn't require giving this permission?  So this is all for the best.  This is great.



They said:  "With this change, some use cases will no longer be allowed.  However, many of the apps we reviewed with one of these permissions can rely on narrower APIs" - in other words, they were just, you know, the developer was kind of lazy and said, yeah, just give me all this - "reducing the scope of access while accomplishing similar functionality.  For example," he writes, "developers using SMS for account verification can alternatively use the SMS Retriever API, and apps that want to share content using SMS can prepopulate a message and trigger the default SMS app to show via intents."  In other words, that's a perfect example of, instead of just getting global access to SMS, use a specific narrow API designed to do just and only that aspect, the SMS account verification through this SMS Retriever API.



He says:  "Tens of thousands of developers have already resubmitted their apps to support the new policy or have submitted a form.  Thank you.  Developers who submitted a form received a compliance extension until March 9th."  And he goes on about next steps.  But anyway, I just think this is great.  I mean, this feels like lessons learned from the road and from the real world.  And it's always painful to take things away which had previously been given.  But Google is learning through interaction with their platform out in the world that, ouch, we need to take this more seriously.  There are all kinds of clever ways we hadn't thought of for the things that we were permitting apps to do to abuse those permissions.  So we're going to create narrower APIs first, and then we're going to force apps to use those, or to explain to us why they can't get by with using those.  So I just, you know, yes.  It's what we need.  So props to Google for that.



As I mentioned at the top, Leo, you guys started MacBreak Weekly this week talking about Tim Cook's note in Time magazine.  You read it into the podcast.  I'm going to read it into ours because it's short, and my take is a little different.  First of all, I couldn't understand everybody's take on MacBreak because everyone, it was such a hot topic, everyone was talking at once.



LEO:  I know.



STEVE:  And it was like, whoa, okay, what happened?



LEO:  Everybody had to get their word in there.



STEVE:  What happened?  So, okay.  So last week Time magazine printed a statement by Apple's Tim Cook which took aim at - and this is I think the significant part of this.  This is not Apple versus Google and Facebook, which is how - it's so easy to paint that with a broad brush because that meme has been established.  What Apple did, what Tim Cook did was take aim at what is the largely hidden data brokerage industry, which has quietly sprung up over the past decade.  And we've touched on it on this podcast from time to time.



Sometimes as I'm doing some research I'll encounter one of those chilling websites where they tout everything they know about us because they're trying to sell this intelligence gathering capability to an audience different than ours.  And I often will share that on the podcast because it just kind of gives me the creeps.  So I think that Tim raises some important points.  And as I said, this is not the accepted Apple versus Facebook and Google profit models where Apple is saying we don't profit from the collection of your data, but they do.  And that's not what he wrote.



So here's what he said.  He said:  "We all deserve control over our digital lives.  That's why we must rein in the data brokers.  In 2019 it's time to stand up for the right to privacy - yours, mine, all of ours.  Consumers shouldn't have to tolerate another year of companies irresponsibly amassing huge user profiles, data breaches that seem out of control, and the vanishing ability to control our own digital lives.  The problem is solvable.  It isn't too big, too challenging, or too late.  Innovation, breakthrough ideas, and great features can go hand in hand with user privacy, and they must.  Realizing technology's potential depends on it.  That's why I and others are calling on the U.S. Congress to pass comprehensive federal privacy legislation, a landmark package of reforms that protect and empower the consumer.



"Last year, before a global body of privacy regulators, I laid out four principles that I believe should guide legislation:  First, the right to have personal data minimized.  Companies should challenge themselves to strip identifying information from consumer data or avoid collecting it in the first place.  Second, the right to knowledge, to know what data is being collected and why.  Third, the right to access.  Companies should make it easy for you to access, correct, and delete your personal data.  And, fourth, the right to data security, without which trust is impossible."



He says:  "But laws alone aren't enough to ensure that individuals can make use of their privacy rights.  We also need to give people tools that they can use to take action.  To that end, here's an idea that could make a real difference.  One of the biggest challenges in protecting privacy is that many of the violations are invisible.  For example, you might have bought a product from an online retailer, something most of us have done.  But what the retailer doesn't tell you is that it then turned around and sold or transferred information about your purchase to a 'data broker,' a company that exists purely to collect your information, package it, and resell it to yet another buyer.  The trail," he writes, "disappears before you even know there is a trail.  Right now, all of these secondary markets for your information exist in a shadow economy that's largely unchecked, out of sight of consumers, regulators and lawmakers."



And he finishes:  "Let's be clear.  You never signed up for that.  We think every user should have the chance to say, 'Wait a minute.  That's my information that you're selling, and I didn't consent.'"  Oh, and he says:  "Meaningful, comprehensive federal privacy legislation should not only aim to put consumers in control of their data, it should also shine a light on actors trafficking in your data behind the scenes.  Some state laws are looking to accomplish just that, but right now there is no federal standard protecting Americans from these practices.  That's why we believe the Federal Trade Commission should establish a data-broker clearinghouse, requiring all data brokers to register, enabling consumers to track the transactions that have bundled and sold their data from place to place, and giving users the power to delete their data on demand - freely, easily and online - once and for all."



And he finishes:  "As this debate kicks off, there will be plenty of proposals and competing interests for policymakers to consider.  We cannot lose sight of the most important constituency, individuals trying to win back their right to privacy.  Technology has the potential to keep changing the world for the better, but it will never achieve that potential without the full faith and confidence of the people who use it."



So anyway, I just think yes, you know, he's right.  We know that this shadow economy exists.  And once again I find, as I read this, I find myself feeling as though we're still in the very early days of this explosion in processing power; the collapse in the cost of mass storage, which has enabled endless compilation of these profiles, and every scrap and tidbit of data can be sucked in and retained, which you couldn't do if it was prohibitively expensive to do that; and of course the connectivity created by the Internet.



And not surprisingly, the regulatory framework that's needed to govern the implications of these changes lags far behind.  And any of us who have, and we often have, listened to our policymakers talk or listened to congressional testimony and hearings and the questions that they ask, demonstrate that those who would create the regulations barely have any idea how this stuff works.  And I don't have any idea how powerful the lobbying clout is of these data brokers, but it might be significant.  And unfortunately, as we know, money drives a lot of this country's politics.  So was there any sort of a conclusion from the discussion that you guys had, Leo, in MacBreak?



LEO:  Well, "conclusion" like is this a good idea?  I think everybody thinks it's a good idea; right?



STEVE:  Is there any chance it could happen?



LEO:  Not a chance in hell.  But we didn't actually talk about that.  The fear, of course, is - so I think you're a trifle, I don't want to say naive.



STEVE:  That's okay.



LEO:  But you're a nice guy in thinking that Tim Cook's just talking about data brokers.  Because really, he may not say Google and Facebook, but any time Apple talks about privacy, there is always the subtext of "We do it right."  Always.



STEVE:  Yes.  I do agree that this brilliant marketing.  I mean, this is brilliant...



LEO:  Okay, so you're not naive.  You understand.



STEVE:  Oh, no, no, I can...



LEO:  They may not say it explicitly, but it's always about us versus Google and Facebook.  That's, you know, yeah, it's about data brokers.  And one thing we did point out is that, if Apple were really serious about this, they would, for instance, not require everybody to use Google as your search in Safari on the iOS device.  But the reason they do it is because Google gives them $9 billion a year - actually this year it'll be, according to some estimates, $12 billion - to be the search tool.  And if you want to protect privacy, you don't give it to Google.



STEVE:  So I guess that means, what, you use a different browser and use DuckDuckGo.



LEO:  Yeah.  DuckDuckGo has a browser.  Apple should, if Apple really cared about this, it seems to me, they would at least give you the option to use DuckDuckGo in Safari.  Right?



STEVE:  Yup.



LEO:  They don't because it's billions of dollars in the pocket.  So it's a little, you know, that's a point to be made also.  As soon as you use an iPhone, the minute you put Facebook on it, forget it.  Doesn't matter how secure Apple is.  You've got Facebook on there.  You're being spied upon immediately.  So it's reasonable for Tim to say we want these regulations because users are going to put Facebook on our beautiful, pristine, private phones, and then it's just as bad as any other phone.



STEVE:  Right.



LEO:  Finally, the other thing that we talked about is that one of Apple's big fears, every company's big fears, not that there'll be federal regulation.  Everybody, by the way, including Google and Facebook, are calling for federal regulation.  But the reason they do that is because the states will individually, as California has, impose their own privacy rules.  And then you have a crazy quilt of 50 different rules.  I mean, you already have to do GDPR.  That's the other thing is that every company that does business in Europe is already doing most of those four points because GDPR...



STEVE:  And you probably saw that Google got hit with a massive GDPR fine.



LEO:  Yeah, 50 million euros, yeah.  By the way, for kind of something dumb.



STEVE:  Yeah, I know.  It's like, okay.  But you're right.  We talked about the lack of federal oversight in the case of Net Neutrality, where again, having every state have their own legislation, it's just a nightmare for the carriers.



LEO:  No, it isn't a nightmare.  They know that they don't have to worry about it.  And the feds have already said the states can't do it.



STEVE:  Yup.



LEO:  Marco Rubio from Florida has already proposed in Congress a rule that says states can't make their own privacy regulations.  Leave it to the big boys.  Leave it to the grownups.



STEVE:  That's right.  We'll take care of it.



LEO:  And the presumption is, I mean, maybe, I mean, that is probably the right thing to do.  But the presumption is that they will then, by getting all the power in the federal Congress, be able to write their own laws, these data brokers, and they won't have to worry about anybody.  So, yeah, I don't know if it'll happen.  There's definitely a current going in the country that people want this.  So maybe it will.



STEVE:  Well, as long as the head of the data brokers association is not put in charge of drafting the legislation.  Where have we seen that before?



LEO:  That would never happen.



STEVE:  Let me think.  Yeah, never happen.



LEO:  I think ultimately we're going to have to protect our own privacy.  And we're going to use DuckDuckGo and things like that; right?



STEVE:  Exactly.  And that's what this podcast, one of the things this podcast is about.



LEO:  You bet.  Why we're here.



STEVE:  Speaking of which, this brings us to a piece that I only saw being covered, or actually I guess it's really not as much news as sort of a public service announcement from Sophos that is a sponsor of - it was a sponsor that appeared last week on this podcast.



LEO:  Yes, our new sponsor.



STEVE:  Huh?



LEO:  Yes, our new sponsor, yeah.



STEVE:  Yeah, new sponsor, Sophos.  I've got a link in the show notes.  And they are the source of this bit.ly link I talked about, bit.ly/snfbads, which expands to a link, also in the show notes, for a page that I don't know, going in normal like in the front door, how easy it is for a Facebook user to find this.  But this shows your so-called "ad preferences" that you may have never known that you had.



I'm not a Facebook user.  I do have an account because I needed once upon a time to look at the privacy and security settings that Facebook was offering.  So I logged into that dusty old account in order to bring up two pages that I have here in the show notes.  So if you do bit.ly/snfbads, you can learn what Facebook is doing for you.  And so there are six main topics:  your interests, that is, what Facebook has somehow independently determined your interests are; advertisers; your information; your ad settings, which you may not have known, I didn't know that you had; hide ad topics, your ability to hide ad topics, like parents could hide alcohol ads from minors, for example; and then an explanation of how Facebook ads work.



And when I saw ad settings, I clicked on that.  And so it's got three sub-settings.  And so, for example, the first one, which is default allowed, it says "Ads based on data from partners," which is exactly what we've been talking about, that says:  "To show you better ads, we use data that advertisers and other partners provide to us about your activity off of Facebook Company Products."  And that's allowed, but you can click it and say no.



Then the second one:  "Ads based on activity on Facebook Company Products that you see elsewhere."  And that is explained:  "When we show you ads off Facebook Company Products, such as on websites, apps, and devices that use our advertising services, we use data about your activity on Facebook Company Products to make them more relevant."  And that of course is also default allowed, and you can say no.



And then the third is "Ads that include your social actions."  And they explain:  "We may include your social actions on ads, such as liking the page that's running the ad.  Who can see this info?"  And then I had this set to "No One."  And I don't know why it's set to No One.  Maybe because I don't have any links to something, or who knows what.  But anyway, that's just one of the six things that I opened.  I thought it would be of interest to any of our listeners who didn't know that this was there.  This is some nice disclosure and some controls, but also for our listeners to forward this to their friends and family that might want to know.



In Sophos's coverage of this, they said:  "Fitbit?  Pollination?  Jaguars?  Snakes?  Mason jars?"  They said:  "Okay, fine, Facebook.  I'm not surprised that I've clicked on those things."  In fact, on mine, under I think it was on advertisers, mine were cars and realtors.  And I have no idea why because I'm driving a 2011 car, and I've owned my home since '84.  And nothing, there's no movement on any of those two categories.



LEO:  They've got the wrong guy on that one, yeah.



STEVE:  So it's like, okay.  But there were like 12 different ads, and they were all realtors or car sales.  So anyway, they said, Sophos says:  "But when did I ever click on anything related to 'Star Trek: Voyager' or cattle?"  And anyway, so the guy...



LEO:  Cattle?



STEVE:  Cattle.  The guy writing this on Sophos says:  "My 'this feels weird' reaction makes me one of the 51% of Facebook users who report that they're not comfortable that the ad-driven company creates a list that assigns each of us categories based on our real-life interests.  It's called 'Your ad preferences.'"  And they say:  "You can view yours here.  If you drill down, you can see where Facebook gets its categorization ideas from, including the things we click on or like, what our relationship status is, who employs us, and far more." 



So anyway, just a FYI of some useful information.  I thought this was a nice page to know about.  And I don't know, if you're just looking at your Facebook, if it's easy to get there from the front page.  But it's your ad preferences on Facebook.  And some interesting information is there.



Okay.  One last piece before we take our last break and get into the final bits.  Cisco's small business switches have a serious problem.  And so anyone using small business switches, pay attention.  This is sort of "gob smacking," as they would say in the U.K.  If you, your organization, or anyone you know are using Cisco 200 or 250 Series Smart Switches, 300 or 350 Series Managed Switches, Cisco 350X, 500, or 500X Series Stackable Managed Switches, there's a real problem.  And we've talked about this recently, like last year, a number of times.  Cisco was apparently auditing their own source and kept finding backdoors that had been written into their source code.



LEO:  What?



STEVE:  I don't know if they were by developers who left this behind or what.  In this case, this did not sound like the cause of the problem.  This sounds deliberate.  But it's a little stunning.  In Cisco's summary of this, they said:  "A vulnerability" - that's not what it is.  But they said:  "A vulnerability" - I mean, I guess they want it to be - "in the Cisco Small Business Switches software could allow an unauthenticated" - that's also not what it is, but I'll explain in a second - "remote attacker to bypass the user authentication mechanism of an affected device."  Almost none of that is true.  But they go on.



"The vulnerability exists because, under specific circumstances" - which are the default circumstances, but anyway - "the affected software enables a privileged user account without notifying administrators of the system.  An attacker could exploit this vulnerability by using this account to log into an affected device and execute commands with full admin rights."  Sounds bad.  "Cisco has not released software updates that address this vulnerability.  This advisory will be updated with fixed software information once fixed software becomes available.  There is a workaround to address this vulnerability."



Okay.  So what's really going on?  Unbelievably, as I said, it's another of those Cisco default built-in passwords.  The vulnerability, which has been assigned a CVE last year, it's CVE-2018-15439, has a critical base CVSS severity rating of - ready? - 9.8.  So, you know...



LEO:  Is 10 the highest?



STEVE:  Yes.  Yes.



LEO:  Okay.



STEVE:  And if it were 9.8 on IMDB, it'd be the most famous and popular movie anyone had ever seen.  So, baby, you know, it's up there.  It exists, get this, because the default configuration of these devices, okay, these are highly popular, widely sold, inexpensive Cisco networking gear.  The default configuration of these devices includes a privileged user account that is used for the initial login and cannot be removed from the system.  An administrator may disable this account by configuring other user accounts with access privilege level set to 15.  However, you don't need to do that in order to use this so it's often not done.  If no user configured privilege level 15 accounts exist in the device configuration, the default privileged user account is enabled without notifying administrators of the system.



So there's no notification given that this default login exists.  Cisco says:  "Under these circumstances" - which is the default circumstances - "an attacker can use this account to log into" - so that's not bypassing authentication, it's logging in, it's using authentication - "log into an affected device and execute commands with full admin rights.  It could allow an unauthenticated" - okay, well, no, an authenticated - "remote attacker to bypass the user authentication mechanism" - no, to use the user authentication mechanism - "of an affected device."



So anyway, the workaround is - there is no patch.  There is no update.  There's nothing, there's no firmware available yet.  And unfortunately, as we know, even once it's created, I mean, these are widely used, low end, Cisco networking gear that someone finally woke up to having, probably because they realized people were getting in somehow, and someone looked to figure out how, and it's like, oops.  Unless someone adds an account with privilege level 15, this default account is enabled.  So if any of our listeners know of or are responsible for or knows anybody who is responsible for any of these low end 200, 250, 300, 350, 500, and 550 series switches, you absolutely want to create your own account with privilege level 15 in order to disable the default account.  There is no other way to do it on these switches.



So, wow, just unbelievable oversight on, I mean, and it feels like the fact that creating one disables it, I mean, maybe there's a disable which is broken?  But it feels like this was on purpose, like by design, and they're finally getting around to saying, oh, maybe we shouldn't have done that.  So I hope our listeners will protect themselves.



So a couple little bits of miscellany.  I wanted to let our listeners know, I was looking around for a file sync solution.  I'm now routinely working from two different locations, and since I hadn't traditionally been doing that, I was initially shuttling, like emailing my development directory to myself in each direction.  And the other day I sent the wrong directory, so when I got to my other end it was like, oh, I don't have all my latest work.  So I decided, that's crazy.  I need to be able to keep directories in sync between different machines.



And so I did a little bit of looking around, and I think I've settled on a cross-platform solution that fits me because, I mean, Google kind of offers this kind of facility.  Dropbox does.  In my case, I have a server that could be a central location for synchronization that both of the endpoints are able to access.  Anyway, Syncthing looks like a nice solution for this.  So I just sort of wanted to put it on our listeners' radar.  I have not started to play with it yet.



LEO:  I've used it for a long time.  It's fantastic.



STEVE:  Ah.  Then thank you.



LEO:  Highly recommended.



STEVE:  Yay, yay, yay, okay.



LEO:  And it's free.



STEVE:  Yes, yes.



LEO:  It's open source.  It's great.



STEVE:  It felt right.  They have a donation link at the bottom.  I'm going to make sure that I use it.



LEO:  It's kind of like the old BitTorrent Sync that we liked so much.



STEVE:  Yes.



LEO:  Similar idea.  Each place has a code; each source has a code.  You could share that code with people.  And I really like it, yeah, yeah.



STEVE:  Good.  Syncthing.  So as a result of my digging around, that's where I settled on.  So I just wanted to say that I haven't started to use it yet, but I will provide a report to our listeners once I have.  And I'm glad for the early heads-up.



LEO:  Yeah.



STEVE:  I'm giving a presentation on SQRL this coming Sunday to a really neat ethical hackers group.  And normally they would welcome more of our listeners who are in the area, so I would talk about it.  But many are already listeners, and the interest expressed so far by those wishing to attend is already at 180% of the venue's capacity.  So I'm going to wait until next week to talk about it any more, although it will be recorded.  We'll have a video made from it.  So I'll be able to share that link once it's ready.  And apparently, based on the postings to the meeting signup board, I'll be signing many copies of SpinRite while I'm there.  I guess these are old-timers, and maybe they have got disks of SpinRite because certainly we used to produce those in the old days.  Or maybe they'll have me sign their, I don't know, their thumb drive, where they've downloaded SpinRite.



LEO:  I like it.



STEVE:  But a whole bunch of people said, oh, I can finally get my copy of SpinRite signed, if Steve's going to be there.  It's like, yeah.  And so in order to prepare, I've been updating the presentation slides.  In fact, that's one of the things that I mis-sent myself the other day.  I've been updating the presentation slides that I first created for the DigiCert Security Summit when I did the presentation of SQRL then.  Then I updated them when I demonstrated SQRL to Stina and her technical colleagues at Yubico.  The latest slides now add the news of this SQRL service provider API that I mentioned last week and I've talked about a little bit.  But I am increasingly excited about it as I understand how much it will mean for SQRL.



I had never really thought about the server-side implementation until I considered what someone who didn't know SQRL at all would need to know.  I mean, it's true, as I've mentioned before, that there's only one cryptographic function that the server needs, and that's to verify a signature.  But there are a lot of other subtleties associated with rekeying a SQRL identity, allowing a user to disable their use of SQRL, and a bunch of other stuff.  And while it's not rocket science, it's completely unfamiliar to anybody who doesn't know the SQRL protocol.



And so asking a whole bunch of different, I mean, it was foreseeable that there would be server-side implementations created.  I have just ended up doing it, and in the process defining a very simple API that abstracts all of that stuff on the other side of the API, making it very simple for someone to add SQRL to an existing web server.  And in fact this developer I've mentioned before in Denmark, who knows the XenForo forum software we're using, was able to quickly and easily add SQRL support using the API, and now he's working on finishing up the configuration dialogs and adding some touches to the UI.



And one of the guys in the GRC newsgroups who already knows the SQRL protocol inside and out has received from me my assembly language source code for the API, and he's reimplementing it in portable open source C.  So that'll give us a multiplatform, compile it on any platform you want, encapsulated SQRL API, making it probably a day or two worth of work on any programmer's part to add SQRL to an existing web server.  And I'm excited about that because, as we know, it's one thing to like all the things that SQRL does, but for it to actually go anywhere it needs to get adopted.



And so this will - I'm really, I mean, I didn't even foresee the need for this service provider API in the beginning until it came time for me to have a website that wasn't natively supporting SQRL, meaning these SQRL forums, support SQRL.  And then it was like, oh, okay.  How should I do that?  And we ended up with another really - what's going to end up being a super useful component of this SQRL ecosystem.  So I'm jazzed about that.  And I'll be talking about that and more at this conference on Sunday, or this hacker meeting on Sunday, and I'll talk about it a little bit how it goes next Tuesday.



Okay.  So errata.  As I said, I could not believe what I said last week.  This was relating to the expiration of certificates on web servers.  When I talked about pushing past the "certificate is expired" notice, I said that what this meant was that all of the communications would then be in the clear.  Which is of course not the case.  It means you say to your browser, yes, I understand the certificate is expired, but let's use it anyway.  And in which case...  



LEO:  It's still encrypted, but it's just expired, yeah.



STEVE:  Yes.  And the problem is I was, I mean, I was wrong.  So okay, let's fix that.  So yes, it's still encrypted.  It does mean that you have blown your security guarantee, that you are subject to man-in-the-middle attacks.  A man in the middle could cause that warning to occur, and you say, okay, I don't care, blah blah blah.  Okay, but still I was - there was some of the coverage of this was talking about logon credentials being now sent in the clear, and I did not drill down far enough.  I just repeated it without thinking.  So my apologies for, I mean, what I know is not the case.  We've talked about this so many times before.  And a bunch of people wrote to me saying, uh, what?



LEO:  Oh, come on.  That's just a [indiscernible].



STEVE:  Anyway, you know me, I needed to fix that.



LEO:  Get it right, okay, that's fair.



STEVE:  Yes.  And so it's right.  So it is the case that you are bypassing security, but it is not the case that someone doing passive sniffing, as I said, is then going to be able to see your username and password in the clear.



Gary Foard in England said - the subject line was "30 gig?"  And he said:  "Dear Steve."  This is also errata.  He said:  "Love the show and SpinRite.  But last week, 697, you mentioned again, as before in 696, about Filemail.com and that you have a 30-gig upstream connection.  Really?"  He says:  "Bloody hell!"  He says "in English accent."  He says:  "I get along with 6-megabits downstream and 1-megabit upstream.  Some people have 100-megabits downstream, which is good.  I think I've heard Leo brag of his business connection being 10-gig, but you say 30-gig upstream?"



He says:  "I don't know how much SpinRite you sell, but it ain't that big that you need 30-gig up."  He says:  "You're a nice guy and deserve it, but maybe you mean 30-megabits.  That would still be impressive.  You've said it twice now, and it's bugging me.  Have a nice day."  And yes, I will now be very self-conscious about 30-megabits, not gigabits.  I do not need, I do not want, I can't imagine 30-gig upstream.  So yes, I have always been meaning to say 30-megabits.  If I make a mistake again, just correct it, everybody, in your head:  meg, not gig.  So Gary, thank you.



LEO:  I just assumed you did have 30 gigs.



STEVE:  No, no.



LEO:  I know.  I just thought, well, he goes through Level 3.  Maybe they gave him a big fat glass pipe.



STEVE:  Yeah.  No, this is at home.  This is like, you know, Filemail when I was - it was 30-megabits, or actually it was 33-megabits that it was saturated.  And I could see how I could have said gigabits because it's easy to make that mistake.  So yes, 33-megabits up, and 300-megabits down.  But rarely do I see that.  It's only, you know, sometimes I'm downloading something from someone big with a really good CDN or something, and it's like, whoa, I actually do get 300.  But mostly they're feeding it slower than that at the feeding end, and so I don't really ever see that.



LEO:  Even at 100 you're faster than most people are sending.



STEVE:  Yeah, exactly.  So Ben asks about having a dedicated SpinRite machine.  Wat?  W-A-T?  And he said:  "Hello, Steve.  Long-time listener of Security Now! and all Gibson goodness wherever it's available.  Crawler of GRC.com.  Owner of SpinRite and recommend it to everyone on the train.  Thank you for all of those."



He says:  "I came across an interesting use case and wondered how to best do it.  A friend of mine works at a 'photo shop.'"  He has that in quotes.  He says:  "That's what they're called here.  They do a variety of photo services, as well as a few other digital services.  One of them is restoration - restoring deleted content and/or corrupted media.  I visited him at his workplace and saw that they're using Recuva, R-E-C-U-V-A."



LEO:  Recuva, yeah.



STEVE:  Recuva.  "Knowing what the answers will be, I asked him what the success rate is.  Expectedly, it's so-so.  They use several other such utilities with about the same results.  I recommended they buy SpinRite.  My friend trusts me, and he convinced the boss to buy it.  Success rates are, unsurprisingly, far better now.  This got me thinking.  They could use a dedicated machine for SpinRite.  Since SpinRite consumes the whole machine, and they do this often enough, then a machine dedicated for SpinRite makes sense.  So what hardware would this machine feature?  From my own experience with SpinRite, I know that it can become very CPU intensive, depending on what it's dealing with on the target media.  So, CPU.  Lots of power?  Cores versus clock speed?  What else?  Machine will of course be diskless for itself.  Is there anything to avoid?"



He says:  "While in normal circumstances the machine's hardware wouldn't be a big focus - you start SpinRite, let it run - here it's different.  The machine cannot do anything else, and SpinRite does one thing at a time.  So giving it the resources to complete its work quicker is the whole point.  I recall that you did say that one can try running SpinRite in a VM and connecting the media to the VM.  But you also said that narrows down SpinRite's range of functionality.  So what's optimal for SpinRite?  If SpinRite can consume the universe, what would it consume?  If one had no other considerations, what would that machine look like?  I'd love it if you can bring this up in an SN episode.  Would also like to thank Leo and the heroic team at TWiT, as well.  Special thanks to Elaine.  Her transcripts come in very handy."



So Ben, and to all of our listeners, first of all, a dedicated SpinRite machine is a very popular thing.  I mean, I've been hearing about them for 30 years.  Many people do it.  The good news is you actually can use an old piece of junk hardware that is barely useful any longer for anything else.  It turns out that it's actually not at all CPU intensive.  It is throughput intensive.  So it is intensive of the connection between the drive and the computer.  Which is why, if at all possible, for example, you want to mount the drive directly to the interface on the motherboard; not, for example, run it through a USB connection which can be a bottleneck.



So connecting the drive to the motherboard is really the only thing you need to do.  At that point, anything that has the proper connection, old-school IDE interface or a SATA interface which is typical now, that'll do it.  But, I mean, really it can be a machine that you've stopped using because it wasn't fast enough to even run a GUI or any kind of a desktop OS.  SpinRite needs no RAM, like literally 640K it runs in.  6.1 will start using more RAM, but even then less than a gig.  I think it was - I have it in my mind, yes, it was a 30MB buffer because I'm allocating a 64K sector buffer, and a sector is half a K.  So it's a 32MB buffer that the next version will use in order to suck in 32MB at a crack, which is where SpinRite 6.1 will get its huge speed boost.



But until then, any hardware you've got you can use.  And so what it means is also, if you had a couple of older machines, you could have several running on different drives at the same time.  But anyway, SpinRite is very, very noncritical about the hardware it uses because it's just running DOS on the chip itself, and so it doesn't actually need much processing power.



Steve Fosmire in Denton, Texas, asked about Java Update offering to remove itself.  And so I'll share his note.  He said:  "I just got a Java update notification that popped up, so I clicked on Update."  And at that point, you know, as all of our listeners used to refer to it, I haven't heard the term for a while, but we called this a "Gibsonian response."  It was like, whoa, stop everything.  A Java update notification popped up.  At that point, you know, if you've been listening to the Security Now! podcast for long, you're thinking, whoa, what?  Wait a minute.



So anyway, he goes on:  "Please remove unused versions of Java."  And he says the update read:  "It appears that you have not used Java on your system in over six months.  We recommend that you uninstall it by clicking the Remove button below.  If you later decide you need Java, you can reinstall it from Java.com."  Okay, now, that kind of sounds better.  He says:  "If you wish to keep Java on your system, please update it by clicking the Update button below."  And he says:  "How about that?  I don't remember what thing I needed Java for, but to have it tell me 'go ahead and remove it' is a new thing entirely.  Thought you would want to know. I did take a screen shot of the Java Update window, so if you decide you want to put this on the SN show, I could send it to you.  Thanks for over a decade's worth of free security knowledge goodness.  Steve."



He says:  "P.S.:  Proud SpinRite site license" - he says in parens "(4)" meaning four copies - "owner from my previous IT consultant company."  He says:  "Closed up shop a year ago and went to work full-time as a network admin at my biggest client."  So this is interesting.  So this was not - so I don't know what triggered the update.  That would be interesting.  But if this is legitimate, if Java noted that it had not been used and suggested it remove itself or you remove it, that's very cool.  I agree, Steve.  So I think maybe I misunderstood initially what you said.



But again, I know that our listeners know that anything that pops up and asks you to take action, depending upon where you are, what you're doing, are you on a website, or like what's going on, just be very skeptical.  So I wanted to share that that had apparently been possible to have happen.



Okay.  Topic of our show.  Which mobile VPN client?  As I mentioned at the top, Bleeping Computer provided some coverage from a Simon Migliano, who is the head of research at Metric Labs.  In Bleeping Computer's coverage, they said:  "One in five apps" - okay, one in five - "from the top 150 free VPN Android apps in Google's Play Store was flagged as a potential source of malware, while a quarter of them" - so one in four - "come with user privacy breaking bugs such as DNS leaks which expose users' DNS queries to their ISPs."



They wrote:  "As found by Simon Migliano, Metric Labs' Head of Research, the company behind the Top10VPN service, these VPN Android applications have already been installed approximately 260 million times" - in 260 million phones - "according to the numbers reported by Google's official store."  Okay.  So we've got several problems here.  First of all, the fact that there are 150 free VPN Android apps.  That suggests this is a popular thing, that running a VPN in your Android device is something people want to do.



In the show notes I took a snapshot from Bleeping Computer's page where it shows an app by installation.  And so, for example, Hotspot Shield Free version, more than 50 million downloads.  Risky permissions were detected, no DNS leaks, risky functions were detected, no virus or malware.  SuperVPN also had 50 million downloads, also risky permissions detected, also risky functions detected, and DNS leaks in this one.  Hi VPN - hi, VPN - 10 million downloads, risky permissions detected, risky functions detected, DNS leaks detected.  And so on and so on, going down the list.



Lots of risky permissions detected.  Lots of risk functions detected.  Most of them, all but, let's see, one, two, three, four, five, six, seven, eight, nine, ten.  So this is the top 10 VPN clients.  And of those, one, two, three, four did not leak DNS.  The other six did.  Eight of the 10 had risky functions detected, and a different eight of the 10 had risky permissions detected.  Bleeping Computer reported that the research team found the following intrusive permissions and user privacy breaking code.



So when I talked about risky functions or risky permissions, get this.  Location tracking, a quarter of the VPN apps are doing location tracking.  Access to device status information in 38% of them.  In a fewer number, use of camera and microphone and the ability to secretly send SMS messages.  In a VPN client, which makes my head explode.  Over half, 57% featured code to get a user's last known location.



So this brings us to the main topic and title of today's podcast:  Which Mobile VPN Client?  There is only one anyone ever needs.  It's called, not surprisingly, OpenVPN for Android.  I have a link in the show notes.  OpenVPN.  It's maintained, compiled from OpenVPN source by Arne Schwabe, who lives in Germany.  And it's like, maybe it's not flashy.  I don't get why it's only five million downloads, whereas the other ones are 50 million.



LEO:  Well, Steve, because it doesn't offer an OpenVPN server.  It's just a client.



STEVE:  Yes, and that's all anyone needs.



LEO:  Okay, but the other ones you were talking about were clients for commercial services in many cases.  So like Hotspot is a commercial VPN provider.  So be clear when you download this, folks, you still need an OpenVPN server.



STEVE:  Right.



LEO:  This is just a client.



STEVE:  Right, right, right.  And I was going to say that.  I didn't understand that those other ones are - so they're providing you...



LEO:  Can't say for all of them.  But Hotspot for sure, that's a commercial VPN service.



STEVE:  Right, right, right.



LEO:  So I'm not sure what Bleeping Computer is trying to, yeah, I'm not sure what they're - I think they're confusing the category, as well.  I'm not sure exactly what they're saying.  There are a lot of bad VPN companies, that's for sure.



STEVE:  There are bad VPN companies.  And what we now know is there are also bad VPN clients.  That is, nobody who wants to use a VPN would want to have something that was tracking their location, using their camera and microphone and so forth.



LEO:  Yeah.  I mean, but there's nothing wrong with using OpenVPN.  I mean, this is OpenVPN; right?  Use the client.



STEVE:  Exactly.



LEO:  But you need to have a server at the other end.  Otherwise...



STEVE:  Yes, yes.  And so I would, I guess, I would separate the two.  I would use no other client than this OpenVPN client for Android.  It is on GitHub.  It's produced by a reputable guy who's doing this in open source.  I mean, that's all this thing is, is a clean, simple OpenVPN client for Android.  And that was where I was headed with this is then go choose whatever OpenVPN service you feel comfortable with.  And there are, like, many.  In fact, all of these VPNs are probably flavors of OpenVPN because this problem, the whole VPNing problem, has been resolved.  I mean, it's been solved already.



So anyway, so that was my takeaway was the idea that there were this incredible number of clients where you've got to ask yourself, and as you and I have often talked about, Leo, why are they providing this to you for free?  I wouldn't install anything on an Android device other than this one OpenVPN client, and then choose whatever service you like.



LEO:  Most good VPN services have their own client.  They have their own tied app.  It's a dedicated app.



STEVE:  Yeah, although you are able to configure whatever you want to.



LEO:  I guess so, yeah.  I mean, but if you're using NordVPN or our sponsor, ExpressVPN, you're using the Nord client, or the ExpressVPN client, which means you don't have to do any configuration.  It just connects to their servers.



STEVE:  Right.



LEO:  So if you know what you're doing, OpenVPN for Android makes sense.  But you have to know what the server is.  You have to know how to configure it, et cetera, et cetera.



STEVE:  Right, right.  So anyway, that was my message.  Just use OpenVPN.  This problem has been solved.  You really don't need to look any further.  And you can have a clean client in your device.



LEO:  Is DNS leakage something that the client does?  Or is it something that the service does, the server does?



STEVE:  Yes, it's something that the client does, if it's not designed properly.  And even OpenVPN, you can control with a config file whether DNS is routed through the tunnel or not routed through the tunnel.  So it does need to be set up properly so that all of the network access routes through the tunnel, and DNS isn't being provided by the OS's underlying platform service.  So it is worth verifying that, I mean, if you're concerned about that from a privacy leakage standpoint, that you've got a client that's doing it; it's configured the way you want it to be.



LEO:  Right.  I know many companies don't like OpenVPN because of previous bugs; right?  There are some companies that use other VPN technologies like IPSec; right?



STEVE:  There are.  The problem with IPSec is that it can be difficult to get out of certain locations.  There are a lot of firewalls that block IPSec; whereas OpenVPN is able to run over standard ports because it just uses TLS and [crosstalk].  Yes, exactly.



LEO:  Port 443, yeah.



STEVE:  Yeah.



LEO:  Okay, cool.  So just to be clear, that's a great client for OpenVPN, but you have to have a server somewhere that you're connected to.



STEVE:  Correct.



LEO:  Many people do.  I mean, lots of routers support it, for instance.



STEVE:  That's what I've got.  Every one of my locations has an OpenVPN server.



LEO:  They're already running it, yeah.



STEVE:  And pfSense, which is my preferred firewall, it's got OpenVPN server.  You just click a button, and then it's turned on.  So it's great.



LEO:  That's very handy.  Very good, Steverino.  We have concluded our proceedings for the day, for the week.  Episode 698 in the can, as they say.  You could find copies of Security Now! past and present at Steve's site, GRC.com, the Gibson Research Corporation.  He also has, as he mentioned, Elaine Farris's great transcripts.  And as long as you're at GRC.com, make sure you get a copy of SpinRite, the world's best hard drive recovery and maintenance utility, plus all the other freebies Steve gives away.  There's lots of good content.



STEVE:  You can run it on an old computer, on one of your little wind-up shoe leather computers.  It'll just go just fine.



LEO:  If it's got BIOS, you can run it.



STEVE:  That's right.



LEO:  You also might want to check out SQRL, which is apparently .999999 ready.



STEVE:  It is just painfully, painfully close to happening.  Yup.



LEO:  Will you post your talk?  Will there be a video?  Will you post it?  



STEVE:  Yes, yes, yes.  There'll be a video.  I'll post the talk.  And before long Lorrie and I are going to come up and hang out with you and Lisa.



LEO:  Oh, good.



STEVE:  And then we'll do the full presentation.



LEO:  Good.  We have to check this young lady out.



STEVE:  Yes.



LEO:  If you want to get all of that, again, GRC.com.  Steve's Twitter handle is @SGgrc.  You can DM him there.  And of course he tweets the show notes before every show, so you can get a copy of those there, @SGgrc.  We have audio and video of the show, oddly enough, for no apparent reason, on our website, TWiT.tv/sn.  You can also subscribe.  You just search for Security Now!, and you can get a copy of it automatically downloaded every Tuesday, the minute it's available.



We do the show 1:30 p.m. Pacific, 4:30 Eastern, 21:30 UTC on Tuesdays.  If you want to watch us do it live, TWiT.tv/live has audio and video feeds.  And if you do that, hang out in the chatroom.  It's where everybody else watching live is, in irc.twit.tv.  Steve, we'll see you next week on Security Now!.



STEVE:  My friend, till then, bye.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#699

DATE:		January 29, 2019

TITLE:		Browser Extension Security

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-699.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm 



DESCRIPTION:  This week we look at the expressive power of the social media friends we keep, the persistent DNS hijacking campaign which has the U.S. government quite concerned, last week's iOS and macOS updates (and doubtless another one very soon!), a valiant effort to take down malware distribution domains, Chrome catching up to IE and Firefox with drive-by file downloads, two particularly worrisome vulnerabilities in two Cisco router models publicly disclosed last Friday, some interesting miscellany, a particularly poignant SpinRite data recovery testimonial, and then some close looks at the state of the industry and the consequences of extensions to our web browsers.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Lots to talk about this week.  Of course, iOS flaws, including the FaceTime flaw.  He's also going to talk about DNS hijacking, why so many sites host malware, and he has a little bit to say about an interesting study that shows how your online friends say a lot about you.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 699, recorded Tuesday, January 29th, 2019:  Browser Extension Security.



It's time for Security Now!.  Here he is, ladies and gentlemen, the man you've all been waiting for for a whole seven days, Steve Gibson, the man of the hour.  Let me get this nonoperational Facebook portal out of your way here.  I don't know what I'm going to do with this thing.  I don't even have a Facebook account anymore.  And the last thing I want to do is give Mark Zuckerberg a 4K camera into our house.  So you want one?



STEVE GIBSON:  No.



LEO:  Okay.



STEVE:  Thank you.



LEO:  I didn't think you would.



STEVE:  Thank you so much.



LEO:  Hi, Steve.



STEVE:  Yo, Leo.  So we are one episode away from 700.



LEO:  Woohoo!



STEVE:  It's funny when I talk to people about - like people who don't know what I'm into.  I met with some friends late last week, and I mentioned, yeah, I've been, you know.  Like whatever happened with that podcast?  I said, oh, not whatever happened.



LEO:  Whatever happened to that podcast.



STEVE:  I said it didn't go anywhere.  It's still right here.



LEO:  Whatever happened to it.



STEVE:  How did that turn out?  Oh, well, it's still in the works.  And I said, oh, yeah, we're in year 13.  They go, what?  And I said, yeah, we just recorded 698.  They go, episodes?  I said yeah.  And, you know, problem is I just have to figure out what to put in each one because there's just so much competition for stuff to talk about.



LEO:  It's not that you have too little, it's you have too much. There are so many things, yeah.



STEVE:  Yes.  And in fact on Sunday, two days ago, I presented SQRL for the first time in its finished form to this LETHAL, they call themselves LETHAL, Los Angeles Ethical Hackers and Leets group.



LEO:  Oh, interesting.



STEVE:  And a lot of them were Security Now! Listeners, so they're smiling right now because they go, yeah, we saw him.  And several of them reminded me of, oh, yeah, remember when those podcasts were, like, 18 minutes?  It's like, no, I don't - can't really say that I - I can't say.  And remember the honey monkeys?  It's like, oh, yes, we all remember the honey monkeys.



So anyway, there were several pieces of news, a really interesting security analysis of browser extensions, and then some rumblings about Chrome's movement from a version 2 to a version 3 of the browser extension API that made headlines.  So I thought, let's call this one "Browser Extension Security," since it's all kind of about that.  So that's our main focus for the day.



Also there was an interesting article, a study that was done - it was published in something in a branch of Nature magazine, but gosh, I can't remember now the name, but a reputable magazine.  It was an analysis of the expressive power of the social media friends we keep and what an analysis of who we friend or who friends us can say about us, even when we don't say anything about ourselves.  And the strength of what they found was what was interesting, so I thought I would just touch on that.



We've got a persistent DNS hijacking campaign which has really got the U.S. government upset, so much so that they imposed some 10-day deadlines on some performance.  And the good news is the government is up and running again, so there's a chance that those deadlines can be met.  So we'll talk about that.  We have also last week's rather significant iOS and macOS updates.  And of course, as we all know, we'll have another one very soon now as a consequence of the Facebook bug that was found.



LEO:  FaceTime.



STEVE:  FaceTime, right, sorry, FaceTime bug that was found, which we'll talk about.  Also a Swiss site has made a valiant effort since I think it was March of 2018 to take down malware distribution domains.  We got some interesting stats from that which just sort of focus on, I mean, really put a point on how big a problem this is.



We've got Chrome catching up to IE and Firefox with its handling of and suppression of drive-by file downloads.  Two worrisome vulnerabilities in two Cisco VPN routers that are so bad that, if anyone has them, they should just stop listening to the podcast and...



LEO:  Oh, wow.



STEVE:  I mean, it's, you know...



LEO:  Go fix.



STEVE:  Continue later because there's still more stuff to talk about that's good.  But, oh, goodness, I mean, this is bad.  Cisco released the patch on Wednesday, and then the exploit was published on GitHub two days later on Friday, and this attacking and scanning immediately began.  So this is the world we live in now.



We've also got some miscellany.  Leo, you're going to want to have a hanky handy.



LEO:  Oh, I do.



STEVE:  I'm not kidding.  A particularly poignant SpinRite data recovery testimonial that - yes.  And then we're going to take, as I said, a close look at the state of the industry and consequences of the extensions we're making to our browsers.



LEO:  Nice.



STEVE:  And so it's just sort of a fun Picture of the Week.  It was in my archive of things to get to that I just got a kick out of.  So I think another great podcast for our listeners as we approach number 700 with a podcast that isn't yet history.



LEO:  Somebody in the chatroom is already saying, "Please don't quit after 1000.  Please, please."



STEVE:  That would be after 999.



LEO:  999, yeah, because he apparently can't handle four digits.



STEVE:  No, that'll be my - I wrote it in assembly language, and that's all there is to it.



LEO:  I only have four registers.  Or one 32-bit register or whatever it is.



STEVE:  I have this...



LEO:  What?



STEVE:  Our Picture of the Week.



LEO:  I have it.  Oh, it's so funny.



STEVE:  It's just good.  It's showing a big, sturdy-looking brick building with a big stop sign on the side and the guard with the little gate down, and it's labeled "U.S. Cybersecurity."  We've got even razor wire spiraled along the top.  And then we also see a side view of the building where the Windows logo with its four panes is shown with the blue one cracked through, with a ladder propped against the building.  So it's like, yeah.  And I put my own caption on it, "The Weakest Link?"  So, yeah.



LEO:  Mm-hmm, very good.



STEVE:  And I've said before, I guess we've talked about there being some migration away from Windows.  I think it was China, was it, that they're working on a Linux-based...  



LEO:  They have a Red Linux that's their own distribution, yeah.



STEVE:  Right.  And I just - I'm stunned that anyone who isn't a clear ally of the U.S. would just pervasively use a closed source operation system from a U.S. company.  It's just - I just, you know, there are just too many opportunities for, I mean, even if there weren't any deliberate backdoors, and as far as we know there are none, we know Patch Tuesday, it's happening every Tuesday just like the podcast.



LEO:  Yeah, every month.



STEVE:  Well, every month.



LEO:  And we do talk a lot about our fear that Huawei and other Chinese companies are watching us.  But remember there are a lot of countries around the world that don't trust our software and our hardware for the same reason.  And we know we engage in this.  That's what we learned with Vault 7 and Edward Snowden's revelations and all of that.



STEVE:  Yeah, yeah.  So this interesting piece of work, I titled it "The Acquaintances We Keep" because the research is titled "Information flow reveals prediction limits in online social activity."



LEO:  Oh, well, now that you put it that way.



STEVE:  Much simpler, yeah.  And in fact I am going to also paraphrase what they said in the abstract after I read it.  But I thought it'd be an interesting little tidbit to tuck away that our listeners could bring out at the proper time, perhaps when standing among a group of non-techie people, like with a drink in your hand or something.



The abstract of the paper says:  "Modern society depends on the flow of information over online social networks, and users of popular platforms generate substantial behavioral data about themselves and their social ties.  However, it remains unclear what fundamental limits exist when using these data to predict the activities and interests of individuals, and to what accuracy such predictions can be made using an individual's social ties.



"Here" - and that is in this research - "we show that 95% of the potential predictive accuracy for an individual is achievable using their social ties only, without requiring that individual's data.  We used information theoretic tools to estimate the predictive information in the writings of Twitter users, providing an upper bound on the available predictive information that holds for any predictive or machine learning methods.  As few as eight to nine of an individual's contacts are sufficient to obtain predictability comparable to that of the individual alone."



They said:  "Distinct temporal and social effects are visible by measuring information flow among social ties, allowing us to better study the dynamics of online activity.  Our results have distinct privacy implications.  Information is so strongly embedded within a social network that, in principle, one can profile an individual from their available social ties, even when the individual themselves forgoes the platform completely."



In other words, what they're saying is that the people that we are linked to has such strong predictive value that just knowing eight or nine of them and knowing about them is as predictive as knowing everything about us.  Which I just thought was an interesting data point, not something that I would have imagined possible.  But in this day and age with unlimited computing power and data storage, these are the kinds of mischief that academics are getting up to.  So not a big deal.  I just thought it was sort of interesting that, yeah, if we know who eight or nine of your contacts are, knowing about - well, and the other thing, too, Leo, is that it strongly suggests the nature of how we choose our friends is they're people who tend to align with us in various ways.



LEO:  Yeah, yeah.



STEVE:  So there has been, boy, I don't know if I have the date where it began, where it was first seen.  But like a year?  An ongoing - I've seen references to it as I've been dipping around the security world.  But nothing really - it didn't really come up to the level of needing to say something because there wasn't anything definitive.  It's been a DNS hijacking campaign which has sort of been in the background.  It isn't widespread.  It's targeted.  But it's interesting.



What got it onto my radar was where I saw how the U.S. had reacted to US-CERT's warning about it, finally warning about it last week.  So we have the National Cybersecurity and Communications Integration Center, the NCCIC, which is part of the Cybersecurity and Infrastructure Security Agency, the CISA, which is aware of a global Domain Name System, DNS - so we have acronym soup here - infrastructure hijacking campaign.  Using compromised credentials - and that's the key, like how they get in.



Using compromised credentials, an attacker can modify the location to which an organization's domain name resources resolve.  In other words, they hack into your DNS provider.  Which is why, for example, I use Hover, and Hover uses an authenticator.  And as annoying as it sometimes is to go, okay, fine, and put in the six-digit code that's only good...



LEO:  Oh, I do that.  Oh.



STEVE:  You betcha.  It's like, this is probably more than anywhere else.  Well, LastPass does it.  Hover does it.  Google, you know...



LEO:  I have Gmail.  Whatever your email provider, that's the other one.  Yeah, those are the big three, aren't they.



STEVE:  Yeah.  So there are places where it's like, yes, this is annoying, but it's better.  It creates a dynamic code that makes it much more difficult to attack.  So anyway, but there are lots of DNS providers who don't do that, don't offer that.  For example, Network Solutions, no.  So thank you very much.



LEO:  Among other reasons.



STEVE:  Yeah, well, exactly.  I think I have, like, one, maybe two domains there still.  I completely moved off of them, where I was from day one, over to Hover.  But the only ones that are there are the ones that only they provide.  I just can't get them anywhere else.  So it's like [grumbling sounds].  So by compromising someone's DNS login, the attacker points their DNS, their authoritative real DNS to a different IP away from their servers, typically to where they've got a spoofed website already set up in advance.



So this of course enables the attacker to redirect all of the traffic subject to the global DNS caches expiring, as they will over the course of, depending upon how long the records have been allowed to be cached, for maybe eight hours, maybe 24.  But traffic immediately begins to flow.  I've seen that myself because back when GRC was having to change its IP in order to avoid long-term persistent DDoS attacks, we would change the IP and then be like, nobody there.  It's like, okay, hello.  We're here.  Then, of course, change the DNS record, and then gradually over the course of some number of hours the traffic would begin to pick up as people were like, oh, we don't know where GRC was, but they're back.  It's like, well, actually, no, you kept trying to go to the old IP because we changed to a different one, and your DNS only now caught up as a consequence of the fact that DNS is caching.



Anyway, so this is bad.  You don't want your DNS to get hijacked for many reasons.  But this is one of the things that's been going on.  It allows the attacker to redirect the traffic that would normally go to the true servers, to an attacker-controlled infrastructure; and, what's interesting, to obtain valid encryption certificates for an organization's domain names, which then enables man-in-the-middle attacks.  And that's one of the things that has sort of raised the level of concern and is where I'm going to focus some of our attention as we talk about this more.



So this NCCIC in their coverage encourages administrators to review FireEye and Cisco Talos Intelligence blogs on global DNS infrastructure hijacking for more information.  Additionally, NCCIC recommends the following best practices to help safeguard networks against the threat.  Well, I'm skipping that because we're going to talk about what the CISA, which is the subpart of the NCCIC, said, which is where these, like, respond within 10 days or else.



Anyway, I had the links to FireEye's report, which I will share because they've got some detail.  They said:  "FireEye's Mandiant Incident Response and Intelligence teams have identified a wave of DNS hijacking that has affected dozens of domains belonging to government, telecommunications, and Internet infrastructure entities across the Middle East and North Africa, Europe and North America.  While we do not currently link this activity to any tracked group, initial research suggests the actor or actors responsible have a nexus to Iran.



"This campaign has targeted victims across the globe on an almost unprecedented scale, with a high degree of success.  We have been tracking this activity for several months, mapping and understanding the innovative tactics, techniques, and procedures" - and now we have a new acronym, the TTPs, the Tactics, Techniques, and Procedures, okay - "deployed by the attacker.  We have also worked closely with victims, security organizations, and law enforcement agencies where possible to reduce the impact of the attacks and/or prevent further compromises.



"While this campaign employs some additional tactics, it is differentiated from other Iranian activity we have seen by leveraging DNS hijacking at scale.  The attacker uses this technique for their initial foothold, which can then be exploited in a variety of ways.  In this blog post we detail the three different ways we have seen DNS records being manipulated to enable victim compromises.  Technique one, involving the creation of a Let's Encrypt certificate and changing the 'A' record, was previously documented by Cisco's Talos team.  The activity described in their blog post is a subset of the activity we have observed."



Okay.  So as I said, let's think about what this means.  If a site's DNS record can be changed, then subject to DNS caching expiring and needing to be renewed, all traffic to the domains controlled by the altered DNS record will be redirected to an attacker-controlled IP address.  And since that redirection includes the lookups being performed by the Let's Encrypt services, the attacker is able to immediately auto-obtain and auto-install a valid certificate for their fraudulent site at its fraudulent IP.  So this, of course, further defrauds every visitor to that site who will see a fully correct https://.  Did I spell the domain correctly?  Yes, everything is correct.  And look, it's HTTPS.  It's valid. 



So we've often talked about the need to have secure domain name lookup.  And as we can see, this was made even more important with the advent of Let's Encrypt because incorrect DNS is Let's Encrypt's greatest weakness.  We've raved about the idea of having Let's Encrypt and being able to sort of have the equivalent of opportunistic security for connections, even though Let's Encrypt has weakened the identity guarantee, or I guess I should say it's transferred it entirely to DNS.



Once upon a time, where you had a human in the loop - and you still do, of course, with other types of certificates that provide a much stronger assurance about the identity of the entity to whom you are connecting.  What Let's Encrypt has done by completely automating the lookup or the certificate issuance process is it has - essentially it's transferred all of the veracity of its identity assertion over to DNS.  And unfortunately, DNS is not up to the task yet.  Someday, when DNSSEC is fully deployed, then we'll have secure domains, and this kind of fraud will be dramatically mitigated, if not completely eliminated.  We're not there yet.



So we're sort of in this awkward stage where everyone's rushing to HTTPS.  Browsers are becoming really militant about not having an HTTPS connection, where developers are being inconvenienced because they can't see into their traffic any longer, which used to be really convenient.  So we're having to set up our own proxies in order to intercept traffic, or we're able to typically look at the web browser developer mode in order to see what's going on with our browser.  But it is difficult to look at it now on the wire because it's all being encrypted.



So the predictable effect this has had, that is, that Let's Encrypt's automated certificate issuance has had is that what once was relatively useful identity assertion value of an HTTPS certificate has been reduced significantly.  And it is interesting, I mean, that these bad guys have taken the time to obtain certificate certs for their DNS hijack domains, which suggests that, as we have seen, HTTPS really has become the de facto must-have property for websites.  And certainly the high-value sites they're apparently going after are HTTPS sites.  They may be pinned as HTTPS in the web browser using HSTS so you no longer have the option of trying to do, for example, what we've talked about before, an HTTPS to HTTP downgrade attack, where you just switch all the URLs down to HTTP and assume that no one will notice.  Browsers just won't do this.



And of course the problem is that Let's Encrypt is now really being strongly compromised.  We've talked about this before.  Back in March of 2017 we covered the news of an analysis that showed that Let's Encrypt had issued, get this, 15,270 certs containing the string "PayPal."  



LEO:  Oy.



STEVE:  Yeah.  So that just sort of shows you that...



LEO:  No good deed goes unpunished is what that shows you.



STEVE:  Exactly.  Exactly.



LEO:  Geez.  But it doesn't mean if you use Let's Encrypt that you're vulnerable.  It just means people are misusing it.



STEVE:  Well, yes.  It means that because it took the man out of the loop, it took all human verification out of the certificate issuance process and automated, that's both its benefit and its liability.  What it means is that we can't really any longer rely on automated certificates to assure us that we're connecting to the authentic domain that we believe we are.



And in fact that takes me to one of the conclusions of this is I'm wondering if web browsers, well, if the certificate standard should not be augmented with a flag that is set, if the certificate was issued automatically, that is, certs that do not have a human intervention that Let's Encrypt, and presumably in the future there will be other automated certificate issuers, if automated certs shouldn't be required to set a flag in the certificate properties indicating that it was issued through automation.  And then that would allow browsers to provide some indication of some sort, optionally, that yes, you're secure.  But just FYI, this certificate was issued automatically, not with a human oversight.  Just, again, as sort of a beacon, not saying that that's bad, but it is subject to abuse, and this is what's happened.



LEO:  Isn't that what EV certs, though, are?  I mean...



STEVE:  Yes, yes.



LEO:  And they're trusted at a higher level because, I mean, I know DigiCert for our EV certs calls me and asks, I mean, there's a lot of verification.



STEVE:  Yeah, yeah.



LEO:  But even with non-EV there's not that, I mean, they might send an email to check your email; right?



STEVE:  So there's DV, which is domain validation, which is what this is.  Essentially, we always had domain validation certs.  But even then you had to go through some hoops.  They weren't automated.  You had to, like, they would email you a blob to put on your website's home directory, and then you'd say it's there, and then they would go retrieve it from the root of that domain, which would serve to prove that you, the recipient of that email, were in control of that domain.  And they said, well, if they're able to put a blob of text we provided them on the domain, then they must be okay to give the cert.



LEO:  Let's Encrypt gives us something like - they've tried to do something like that.  They have kind of some sort of domain control verification process, automated.



STEVE:  Oh, yeah.  Well, it's automated, and that's the problem is that it's all you need is DNS.  If you have DNS - and of course the whole point of this is that DNS is not yet robust.  Well, and we could argue that even the system is stronger than the management because this is the attack of the management interface, which is why we were talking about Hover and time-based tokens and how very important it is for people to maintain good control over their DNS records because it's the way these exploits happen.



So in their background that they wrote:  "In coordination with government and industry partners, the Department of Homeland Security Cybersecurity and Infrastructure Security Agency (CISA) is tracking a series of incidents involving Domain Name System infrastructure tampering.  CISA is aware of multiple executive branch agency domains that were impacted by the tampering campaign and has notified the agencies that maintain them."  I mean, these are dot gov.  "Using the following techniques, attackers have redirected and intercepted web and mail traffic, and could do so for other networked services.



"One.  The attacker begins by compromising user credentials, or obtaining them through alternate means, of an account that can make changes to DNS records.  Two.  Next, the attacker alters DNS records, like Address (A), Mail Exchange (MX), or Name Server (NS) records, replacing the legitimate address of a service with an address the attacker controls."  Actually, changing the name server records, that's diabolical because that would allow them to sort of set up an unobserved persistent ability because those are the records to which the ultimate reference is made.  So they could change those then later, at a time of their choosing.  So that's kind of tricky.



"This enables them to direct user traffic to their own infrastructure for manipulation or inspection before passing it on to the legitimate service, should they choose."  So it allows them to sort of basically set up a clean man-in-the-middle attack.  This allows a risk that persists beyond the period of traffic redirection.



"Three.  Because the attacker can set DNS record values, they can also obtain valid encryption certificates for an organization's domain names.  This allows the redirected traffic to be decrypted, exposing any user-submitted data.  Since the certificate is valid for the domain, end users receive no error warnings.  To address the significant and imminent risks to agency information and information systems presented by this activity, this emergency directive" - that's what this was, an emergency directive - "requires the following near-term actions to mitigate risks from undiscovered tampering, enabling agencies to prevent illegitimate DNS activity for their domains and detect unauthorized certificates."



So this is required actions.  "Within 10 business days for all dot gov or other agency-maintained domains, audit public DNS records on all authoritative and secondary DNS servers to verify they resolve to the intended location.  If any do not, report them to CISA.  CISA recommends agencies prioritize NS records and those associated with key agency services offered to organizational users and the public.  For example, websites that are central to the agency's mission, MX records, and other services with high utilization."



Action two:  "Change DNS Account Passwords.  Within 10 business days, update the passwords for all accounts on systems that can make changes to your agency's DNS records.  CISA recommends the use of password managers to facilitate complex and unique passwords."



Action three:  "Add Multi-Factor Authentication to DNS Accounts.  Within 10 business days, implement multi-factor authentication for all accounts on systems that can make changes to your agency's DNS records.  If MFA cannot be enabled, provide CISA with the names of systems, why it cannot be enabled within the required timeline, and when it can be enabled.  CISA recommends using additional factors that are resilient to phishing.  Consistent with NIST SP 800-63B, Short Message Service SMS-based multifactor authentication is not recommended."



Action four:  "Monitor Certificate Transparency Logs.  Within 10 business days, CISA will begin regular delivery of newly added certificates to Certificate Transparency logs for agency domains via the Cyber Hygiene service.  Upon receipt, agencies shall immediately begin monitoring CT log data for certificates issued that they did not request.  If an agency confirms that a certificate was unauthorized, it must report the certificate to the issuing certificate authority and to CISA."



And then the CISA said:  "CISA will provide technical assistance to agencies that report anomalous DNS records.  We will review submissions from agencies that cannot implement multifactor authentication on DNS accounts within the timeline and contact agencies as needed.  We'll provide regular delivery of newly added certificates to CT logs for agency domains, and we'll provide additional guidance to agencies through an emergency directive coordination call following the issuance of this directive, as well as through individual engagements upon request."



So the government is taking it seriously and basically is taking clearly some affirmative action here to remediate any yet-undiscovered but pending attacks as a consequence of previous or getting underway attacks against the dot gov and other important infrastructure.  So it's nice to see something like you've got 10 days to do this, to get this done.



But also interesting that what we're seeing is attacks becoming more sophisticated because they have to be.  Basically, coming up with a way to get admin control over DNS, then using that to set up an alternative infrastructure to obtain Let's Encrypt certs through automation and then leverage that in order to create a man in the middle and gain access to data for as long as it can be had.



And of course, you know, lots of communications is trusted.  And you can imagine that that is, if you're able to establish yourself in a man-in-the-middle position, for as long as you can keep that you have a pipeline for all of the sensitive information that flows over that.  So it's not just the badness of getting yourself into that position, but the huge vulnerability that is created when you're able to see all the data that flows over that link during that time.



LEO:  Makes sense, yeah. 



STEVE:  Yeah.  So I don't normally talk about iOS and macOS security updates.  I have a few times in the past.  This one caught my attention just because, when I searched on the word "arbitrary," the page lit up.



LEO:  Arbitrary.



STEVE:  Yes.  Because that's the phrase - Apple uses the phrase "arbitrary code execution."



LEO:  Ooh.  That does not sound good.



STEVE:  So I believe I heard Rene say last week that he was surprised that the update was to 12.1.3 because he was expecting, I think, it to go to 12.2.  He's the Mac guru follower genius guy, so I don't know what that's about.  But anyway, what we got was 12.1.3.  Presumably, maybe Apple already has other plans for 12.2, and that hasn't happened yet.  So these things apply to iPhone 5s and later, iPad Air and later, the iPod Touch sixth generation.  And that caught my eye, as I said, because when I searched the security news details page - I have the link here in the show notes for anyone who's interested - for the word "arbitrary," I got...



LEO:  There's a lot.



STEVE:  ...Bluetooth.  Yeah.  I got Bluetooth:  An attacker in a privileged network position may be able to execute arbitrary code.  And they describe an out-of-bounds read was addressed with improved input validation.  But until then you've got an over-the-air remote code execution vulnerability.  In FaceTime a remote attacker may be able to initiate a FaceTime call using arbitrary code execution.  That doesn't seem that bad.  But still you don't want that.  A bunch of kernel impacts.  A kernel arbitrary code execution.  Those are never good.



There was, in Apple's libxpc, which is part of the iOS process management system, there was an arbitrary code execution.  Also another arbitrary code execution in SQLite.  WebKit had a bunch, and those are not good because of course that has a lot - WebKit is Internet-facing.  So there was paliciously - paliciously.



LEO:  Palicious and delicious both.



STEVE:  Yeah, exactly.  Processing maliciously crafted web content may lead to arbitrary code execution.  Actually all three of them say that.  So there was a memory corruption issue was addressed with improved memory handling.  A type confusion issue was addressed with improved memory handling, and multiple memory corruption issues were addressed with improved memory handling.  As we know, Apple doesn't ever give us any details.  It's just be happy these are no longer going to bite you.  And Fluoroacetate was involved.  And I remember we talked about him before, or she or whoever, working with Trend Micro's Zero Day Initiative.  Fluoroacetate reports to Trend, who then reports to Apple.  Also WebRTC, again potentially high impact because that tends to be Internet facing.  So there was an arbitrary code execution vulnerability there.



LEO:  And for "arbitrary" read "malicious"; right?



STEVE:  Oh, yeah, yeah.  We provide the code that we're going to stuff down your throat whether you like it or not.



LEO:  "Arbitrary" sounds so harmless.



STEVE:  Exactly.



LEO:  But it isn't completely arbitrary.  It's mostly whatever the bad guy wants to execute.



STEVE:  Exactly.



LEO:  Yeah.



STEVE:  Exactly.  So as we know, iOS has historically been less prone to reverse engineering attacks than Windows.  Yet a lot of these seem not good.  So I don't know why it is, but my systems update lazily.  It'll be like a week will go by, and then something will begin to say, yeah, you know, we'd like to reboot your iPad or your phone or something.  And I'll go, oh.  Anyway, so this time I went looking, and I was asked if I wanted to download an update.  And I said yes, thank you.  So I would just suggest to our listeners, I mean, again, probably targeted attacks.  As far as we know, well, we don't know whether they are in the wild or not.  They weren't disclosed as zero-days, so we can presume they're not.  But it would be good to update.



And consequently, or as a consequence of there being a lot of code overlap, an increasing amount of code overlap, and a common code base between iOS and macOS, the macOS update that was Mojave 10.14.3, which was also known as, for High Sierra, they simply called it the "Security Update 2019-001" and the same thing for Sierra, most of those things were also affecting macOS.  



LEO:  That's a good point.  All of us - you're right.  Of late you get updates on both iOS and macOS at the same time.



STEVE:  Yeah, and...



LEO:  That's a unified code base.  That's interesting, yeah.



STEVE:  Right.  And so the good news is...



LEO:  WebKit, of course, WebKit's the same on both, yeah.



STEVE:  Yes.  And the good news is, when you fix it on one, you fix it on the other.  The bad news is you've got vulnerabilities on both.  So that's not so great.



LEO:  Interesting observation, yeah.



STEVE:  Yeah.  There were some additional, well, so first of all, all of the same ones were present in the macOS:  Bluetooth, FaceTime, WebRTC, SQLite, the IOKit, and those affecting the kernel.  And then additionally, specific only to macOS, Sierra, High Sierra, there was a remote code execution vulnerability in the Intel graphics driver and a privilege elevation issue in the OS's hypervisor.  And then all three macOS versions that were patched were affected by an out-of-bounds flaw in the Quartz Core that could allow an attacker to read unrestricted memory.  So update now and update often.



And then of course we should mention while we're here that this FaceTime bug which has been much in the news, I mean, the headlines were all screaming with their hair on fire.  As I understand it - you guys talked about it on MacBreak just before this podcast - it would be possible for someone to call someone, to use FaceTime to call someone else, then add themselves to the FaceTime group, and as a consequence of a bug in the indexing of the identities in the group management, somehow the recipient of the call, the FaceTime running in the recipient's phone would misunderstand the addition of a redundant group membership to be answering.  And as a consequence, without the recipient of the FaceTime call answering, the call would be answered at their end, thus enabling the microphone and camera.  Which is a mistake.



So as I said, I expect we will have that fixed within the next few days, I mean, because, again, it got a lot of attention in the press.  I had some friends say, hey, you going to be talking about that on the podcast today?  It was like, yeah, we will mention that.  So not much more to say about it.  Were there any specific mitigations, Leo, that Rene and...



LEO:  Well, they've already done it.  Apple's turned off the Group FaceTime feature.  So you can't...



STEVE:  Okay.



LEO:  Yeah, the mitigation is done.



STEVE:  Okay.



LEO:  And, yeah, you could turn off FaceTime if you were really concerned.  But I think because Apple's turned off the server, I think you're all right.



STEVE:  Yeah, yeah, good.  So the group service died.



LEO:  That's right.



STEVE:  And as soon as they get the patch pushed out...



LEO:  That's exactly right.



STEVE:  ...then they will turn it back on again.  Cool.



LEO:  It's, you know, I mean, it's not a good bug.  It allows somebody to snoop on you.  But it's not a terrible bug.  You'll hear the ring.  They can only snoop on you for a while until it stops trying to get through.



STEVE:  And, you know, I have found, Leo, I don't know if there's been any detailed research on this, but the inside of someone's pocket...



LEO:  Not a good place.



STEVE:  It's very dark.  It's very dark in there.



LEO:  And muffled.



STEVE:  You may see, depending upon the knit of the fabric, you may see some little bits of light shining through.  But probably not, you know, not a bad bug.



LEO:  And if you've ever been butt dialed, you know that the contents of that audio isn't usually very useful.



STEVE:  No, a lot of sort of scratching around and rumbling and, yeah, exactly.



LEO:  I mean, it's a bad bug, but they're going to fix it.



STEVE:  It is, yeah.  So at the end of March 2018, a Swiss site, abuse.ch, initiated its most recent project which they call URLhaus, H-A-U-S.  I've got a link in the show notes for anyone who wants more detail.  And I sort of rewrote their statement to kind of bring it current.  It was written sort of in the past tense, and it was written in English probably by non-native English speakers, so their English is way better than my Swiss.



They wrote:  "At the end of March 2018, abuse.ch initiated its most recent project called 'URLhaus.'  The goal of URLhaus was to collect and share URLs that are being used for distributing malware."  And wait till you hear how many.  "The project was a huge success.  With the help of 265" - and Leo, I had to force myself not to write 256 because my fingers, they just automatically...



LEO:  There's a groove in your brain.



STEVE:  So I don't even, yeah, I was like, okay, wait a minute.



LEO:  2^8.  It's obvious.



STEVE:  Yeah.  Not 256, "...265 security researchers spread across the globe, URLhaus was able to coordinate the takedown of almost 100,000 malware distribution sites..."



LEO:  Holy cow.



STEVE:  "...in 10 months.  During that time, these 265 researchers identified and submitted" - get this - "in average 300 malware sites to URLhaus each day in order to help others to protect their network and users from malware campaigns.  Working with the security community, URLhaus managed to get the attention of many hosting providers, helping them to identify and remediate compromised websites hosted in their network."



They say:  "This was not a simple task, specially for large hosting providers who have tens of thousands of customers, and consequently a great many hijacked websites within their network that are being abused by cybercriminals to distribute malware.  Nevertheless," they write, "URLhaus in average counts between 4,000 and 5,000 active malware distribution sites every day, which is," they say, "way too much.  The following chart shows the number of active malware distribution sites tracked since the launch of URLhaus."  And it's sort of sad.  The blue line indicates the number of abuse reports sent out to the corresponding hosting providers and network owners.



So what's interesting here is, first of all, for those who are listening and cannot see, the graph spans June 12 of 2018 through January 15 of this year.  And it shows in red the active malware sites creeping upwards from at the beginning around 2,500, up to a max of it just barely crosses 7,000.  Now, that's not because that many new sites were created.  That's the awareness of them that was growing during the project as the 265 globally distributed researchers were feeding 200 to 300 new URL reports per day into URLhaus, who was aggregating them and consolidating them and counting them.  So the awareness of these sites kept growing.



Meanwhile, there was an ongoing background effort shown by the blue line to thwart this.  And what's interesting is there is, looks like maybe in September, on September 2nd, maybe between the 2nd and the 11th somewhere, that's where the graph is marked, there's a big blue spike up to 3,000 where 3,000 malicious sites were reported during a small period of time.  And that sort of seemed to tip the scales.  And we then see over the course of September, October, November, the number of sites dwindling, and then another little uptick.  And then there's some more reports, and they get pushed down again.



So the point is, this graph shows a real cat-and-mouse ongoing battle between malicious sites being created and then being taken down, then being found, then being reported to URLhaus.  Then URLhaus reporting them out to the hosting providers, saying, hey, you probably are not aware of this, but this domain name has got bad stuff on it.  You should take it down.  It's hurting people.



So then they also have, and I have it copied in the show notes, a table which they say:  "The table below shows the top malware hosting networks" - now, again, this is not the network's fault, necessarily, although you could draw your own conclusions - "hosting active malware content, counting online malware distribution sites only as of January 20th, 2019."  Okay, so that's very recent.  They say:  "As you can easily spot, two out of three of the top malware hosting sites are hosted either in the U.S. or China."



And what was interesting, and a little sad, is the length of time it takes for reports to be acted upon.  The worst were Chinese sites.  What they said is what is also an eye-catcher on this table is the takedown time of malware sites hosted in China.  The three top Chinese malware hosting networks have an average abuse desk reaction time of more than a month.  So from time that it's reported, four weeks go by before any action is taken.  Which is unfortunate.



Although in general it's not good, if I look here for the shortest one, there was actually one of the Chinese networks was three days, 11 hours, and 50 minutes.  The worst was one month, 23 days.  Okay, so that's way over a month, almost two months.  And they had 163 malware URLs.  The next biggest was 256 malware URLs.  That was a Chinese host that reacted after one month and nine days.  On the other hand, the number one hosting site provider was Digital Ocean in the U.S.  And they had 307 malware URLs, so more than any other provider.  And their reaction time was six days, 12 hours, and 56 minutes.  So certainly...



LEO:  We should mention, by the way, they're a sponsor, as you know.



STEVE:  Okay.



LEO:  And one of the reasons people use them is it's so easy to spin up a site; right?



STEVE:  Right.



LEO:  It's a simple thing to do.



STEVE:  Well, yes, exactly.  And also I was just going to say that these guys have to be responsible because they don't want to take down, they shouldn't take down a site based on a report without verifying it.  So otherwise you've got script kiddies maliciously reporting good sites that they don't like as being malicious, and getting them booted for no good reason.



LEO:  Very good point, yeah.



STEVE:  So when you have a huge number of sites, there's a lot of remediation work and burden that it has that goes along with it.  So anyway, so they went on to talk about what malware was found there.  And the number one malware by a long shot was something called Emotet, which is a very capable and increasingly flexible trojan which is sort of multipurpose.  It gets in, and then it's polymorphic.  It changes shape.  It's very hard to deal with.  And of course the bad guys are constantly churning out new domains to host this stuff and then spew out links in social networks and on download sites and in ads and wherever they can to get people to click on them to download the malware and then go from there.  So, boy, unfortunately that's the world that we live in today.  Crazy.



Chrome will be playing catch-up to IE and Firefox when it comes to mitigating drive-by downloads from iframes.  Web browser iframes, and we've talked about them, have always been frightening from a security standpoint.  We often talk about the classic tradeoff between security and flexibility.  Nothing could be a better example of that than the iframe.  iFrame, as we know, is short for inline frame.  It allows the designer of a web page to set aside a rectangular region, a frame, whose contents will be filled in by the result of an iframe URL fetch.



So the origin web page specifies the URL.  Then the browser goes to fetch it and to render it sort of as a mini web page unto itself.  And they are, iframes are what have enabled the entire web browser advertising industry, since they conveniently allow web pages to monetize themselves by agreeing to set aside space, these physical frames, which will be filled in by advertising aggregators and for which the originating websites then are paid, based upon the number of times those iframe URLs are pulled and displayed.



The danger is that, unless controlled and restricted, what are essentially mini web pages are full browser citizens capable of loading anything they choose and, as we've often talked about, running JavaScript, whatever JavaScript they may wish.  So way back in Internet time, on October 6th of 2015, the Chromium bugs list contained the observation - and I've got the link for anyone who's interested.  This is the Chromium bugs list.



"IE and Firefox do not allow download from a sandboxed iframe."  And then the posting included with this sample shows a simple little, I mean, like, minimal HTML.  You know, it shows a <!DOCTYPE>, then <html> opened, the <head> open, the </head> closed, the <body> tag opened, and then <iframe sandbox src> and then a URL, where the action is equal download and then close the </iframe>, close the </body>, close the </html>.  Basically just an iframe.  And back then, IE and Firefox were already not downloading content from a sandbox tagged iframe.  That was flagged as a bug in Chromium on October 6th of 2015, and it has remained outstanding ever since.



The good news is, although Chrome still has not, that does appear finally to be changing.  Various tech news outlets are reporting that Google's developers of Chrome have finally started working on adding drive-by download protection to Chromium.  The new feature is already active in the current Chrome Canary edition build and is scheduled to land in the stable version with Chrome 73 sometime in March or April.  Analysis has shown that when downloads are triggered in a web page's iframe element, hidden in its code, those downloads are almost always malicious.  Yeah, no kidding.  I mean, like, why are you downloading a file in an iframe?  That really should be restricted.  And this is the number one way that malvertising is still crawling into people's machines.



So I guess it was out of an abundance of, well, who knows.  Maybe Google is using this extensively in their own web apps.  It wouldn't surprise me.  Certainly there are use cases for it.  But the typical use, unfortunately, has been malicious.  So the Chromium developers stated:  "We plan to prevent downloads in sandboxed iframes that lack a user gesture, and this restriction could be lifted via an 'allow-downloads-without-user- activation' keyword, if present in the sandbox attribute list."



So basically the idea would be they're going to finally block by default, but allow permission if the designer of the web page intends for the iframe, the sandboxed iframe, to allow un-user-initiated downloads.  So that solution makes sense since you can imagine there might be instances where a web page might wish to allow content within an iframe to have download privileges.  So looks like we will be getting that at long last in Chrome, which is all for the better.



Now, this is where anyone who is responsible for a Cisco WAN VPN router, an RV320 or an RV325, pauses the podcast, takes those routers offline, and updates their firmware.  You can come back to the podcast; but you are absolutely, please, you are absolutely permitted to go unplug your router.  And if you're streaming the podcast, well, then, yes, you'll get disconnected. But believe me, you'll be glad.



Two days after Cisco released patches, last Wednesday, security researcher David Davidson published proof of concept exploit demo code on GitHub.  I have the link to his publication in the show notes.  His GitHub posting was titled "CVE-2019-1652 and -1653 Exploits for Dumping Cisco RV320 Configurations and Debugging Data and Remote Root Exploit!"  So attacks against these routers began shortly after David's code went public.  Okay.  So first of all, the two vulnerabilities are horrendous.  This 1653 allows a remote attacker to get sensitive device configuration details without a password.  How convenient.  1652 allows a remote attacker to inject and run admin commands on the device without a password.



David's GitHub posting could not have been more seductive and damaging.  I mean, he, like, left nothing to the imagination.  He called it "Cisco RV320 Dump."  He cited the CVEs and said "Exploits for Dumping Cisco RV320 Configurations and getting RCE."  In other words, remote code execution.  And he said:  "Implementations of the" - and he quoted the two CVEs - "exploits disclosed by Red Team Pentesting."



He says:  "I only tested these on an RV320; but according to the Cisco advisory, the RV325 is also vulnerable.  The following Shodan queries appear to find them.  If you're curious about how many are out there, there seems to be quite a few."  Then he gives four links to the Shodan queries to find the routers.  And so he's using ssl:RV320, so that's going to be port 443 of course.  So RV320 and RV325.  And then also port 161, that's the SNMP port.  So that RV325 and 320.



He says:  "The vulnerabilities allow for the following:  Dumping in plaintext the configuration file, including hashing for the web UI.  Dumping encrypted diagnostic/debug files, including config, and the /etc and /var directories.  Decrypting the encrypted diagnostic/debug files."  And he says, "Yes, you get /etc/shadow.  And post-authentication remote command injection as root via the exposed web UI."  And then he says:  "As an aside, the default creds are cisco:cisco."  In other words, username and password, cisco:cisco.



So Troy Mursch at Bad Packets Report did some white hat scanning and put up some more details last Saturday, the day after David's disclosure.  He posted on January 26:  "Over 9,000 Cisco RV320/RV325 routers are vulnerable to CVE-2019-1653."  He wrote:  "On Friday, January 25, 2019, our honeypots detected opportunistic scanning activity from multiple hosts targeting Cisco Small Business RV320 and RV325 routers.  A vulnerability exists in these routers that allow remote unauthenticated information disclosure via 1653, leading to remote code execution via 1652."  Yeah.



"Using data provided by BinaryEdge, we've scanned 15,309 unique IPv4 hosts and determined 9,657 Cisco RV [and we know the number] routers are vulnerable to 1653.  That's the information disclosure.  6,249 out of the 9,852 Cisco RV320 routers scanned are vulnerable."  And he said in parens:  "(1,650 are not vulnerable, and 1,955 did not respond to our scans).  Of the RV320, 16,247 are vulnerable, RV320 routers are vulnerable.  3,410 out of 5,457 RV325 routers scanned are vulnerable."  So this is a disaster.  They are sitting there, wide open.  Cisco released patches two days before they got scanned, the scanning began.  So we know thousands of them, if not almost all of them, are going to be subject to attack and compromise.  They are very popular among enterprise and ISP providers.



So as I said, if you have any RV320 or 325 routers within your purview, you're responsible for them, just go unplug them.  Update them and get them plugged in again.  I hope that the owners of those were on a mailing list, took this very seriously if they received email from Cisco, and didn't say, okay, yeah, we'll get around to that next week because it took two days.  And again, this is the world we live in now.



Okay.  Some miscellany.  Last Sunday's first presentation of the completed and finished SQRL system went well.  I had previously given presentations of SQRL at DigiCert's security summit and then to Stina and her crypto colleagues at Yubico.  Each of those points in time caught SQRL where it was then.  Today, as I've been saying recently, it's finished.  So the meet-up of what is known as LETHAL, the L.A. Ethical Hackers and Leets, was my first opportunity to publicly present the entire system from soup to nuts.



There is a video that was made, and it was sort of a best effort.  We turned the room lighting down so that everyone in the room could see the screen, but that left me pretty much in the dark.  And I think the battery died a couple times.  I mean, it was just an amateur video.  I have a link to it through Filemail.  I shared it over in the newsgroup this morning, and I got a couple comments back saying, yeah, you know, you were great, but I guess there are two big audio dropouts and so forth.



So anyway, I'm sure I will eventually speak in front of a group that records video all the time, and we will end up with a good presentation.  This one was - it's not what I want to do with you, Leo, and with Jason and with Mike Elgan.  I want to do a user-facing presentation, like what does the user see, and then also answer all of your "but what about" questions.



What I did with this group, because they were a bunch of techies, was to do - it was a technical presentation.  But to that end, during the presentation I kept referring to GRC's online documentation, but also needing to continually apologize that those pages are now more than five years old, and as a consequence much has changed since then.  So my own next top priority, like what I'm going to be doing tonight, is to begin the process of rereading those, revising them, and synchronizing with the way it actually turned out so that those are caught up, and so that from now on, when I'm referring people, like oh, yeah, all the documentation for this is online, it actually will be, instead of being, yeah, well, you know, the way I thought it was going to be five years ago because it's radically improved and tightened up since then.



And in the meantime, our wonderful XenForo developer sent me yesterday his version 1.0.0 of the SQRL integration for XenForo using the new SQRL service provider API.  It is a formal XenForo add-on.  XenForo is the web forum software I chose which has a mature add-on architecture.  And so, for example, this would allow anyone who drops this onto their XenForo forums to add SQRL authentication, just like it would take 10 minutes.  You would need an SSP API server because, you know, the SQRL Service Provider.  Right now I'm the only one that exists, and mine is for Windows.  Probably runs 32-bit Apache, too, because I wrote it as an ISAPI, you know, the IIS API, which IIS and Apache both support - although mine is 32 bits, and IIS allows you to mix bitness, 32- or 64-bit, but Apache doesn't.  You have to have a 32-bit Apache to run 32-bit modules.  And of course I wrote mine in assembly language.  We've got a guy in the newsgroup who is in the process of recoding it in Open Portable C.  So as soon as we have that, then we'll have something that we can compile under multiple platforms to really make adding SQRL very, very simple.  So every day we get closer.



LEO:  Woohoo.



STEVE:  Tonight I will drop SQRL into our forums, and we will begin the process with the guys in the newsgroups checking it out.  And then I hope to very soon be able to tell our podcast listeners about it; and, Leo, to schedule my visit to your studios to have a professional recording of how SQRL works.



LEO:  I promise our battery will not die.



STEVE:  And you can actually see me.  Oh.



LEO:  And there'll be light on your face.



STEVE:  So, okay.  This caught me by surprise.  And Joey, his name is Joey Kelley.  He's a listener.  He said:  "Hello, Steve and the rest of the GRC team."  He said:  "I purchased SpinRite some time ago and have had few occasions, thankfully, to use it."  He says:  "Most of my customers," he says, "I own a small consulting firm on the side, have been convinced of the need to back up their data; and, even when a hard drive fails, it is usually an inconvenience, not a total disaster.



"Earlier today, I tweeted you a quick picture entitled 'Dynastat Engaged!' showing SpinRite going to work on a drive that I am glad to say was successfully helped by your product.  Since the story behind this is interesting, I thought I would relay it to you."  And so, yeah, you just showed the picture on the video, Leo.  It just shows - and I went back and captured it, just so I could put his picture in.



He said:  "In the past couple of years I have become quite good friends with a couple and their family that run a store and lunch counter near where my parents live.  I swing in often, and I've tweaked their computers here and there as asked.  About two weeks ago they mentioned they had an old computer in the back that they would like to have the data from.  That old story, repeated countless times between you and Leo on Security Now!, began to play in my head, ending with that well remembered line:  '...and there is no backup.'



"Knowing that these people have had a lot of knocks in their lives, I thought I'd help them out and wound up taking the old computer home.  I set it up through a USB-to-IDE adapter and set SpinRite to going.  It found the typical one unrecoverable sector in the early sectors of the drive, did some recovery, and then finished.  However, it would still not come up in either Windows or Linux as a valid drive.  Figuring I had nothing to lose, I reran SpinRite on it with a Level 4 scan, and only four hours later the scan completed.  I was able to pull the drive up in both Linux and Windows, and was able to copy all of their data off the drive.  Success.



"Then something caught my eye, the name of one of the User folders.  Almost 15 years ago, these folks lost their 14-year-old son in an accident.  This was the family's computer at the time, and it contains photos that they have nowhere else of their son, which they now have back, thanks to SpinRite."  He said:  "Thank you for giving this family their memories back."



LEO:  Very nice.



STEVE:  So, Joey, thank you for sharing that.  His URL is JoeyFixesComputers.com.  And he said:  "You have my permission to use this email as you see fit."



LEO:  Oh.  That is a great story.  That must make you feel very good.



STEVE:  It very much does, yeah.  Yeah.  We have someone, Tony West, who's watching.  He tweeted:  "Watching Security Now! with @SGgrc for the first time.  Is that the biggest coffee cup ever, or a camera illusion?"



LEO:  Well, show us your coffee cup, Steve.  It's no illusion, my friend.



STEVE:  And of course, yes, hello.



LEO:  That's the 10-inch coffee cup.



STEVE:  That's right.  It's that the lens of course is so close.



LEO:  But wait a minute, though.  Pull it back because it's still as big as your head.



STEVE:  Yeah.



LEO:  If you're drinking coffee out of a cup as big as your head, it's a big cup.



STEVE:  Leo, it's a two-hour podcast.  What am I going to do?  I didn't want to run out.



LEO:  I love those.



STEVE:  So, browser extension security.  We have the persistent tradeoff between capability, flexibility, and security.  And it's been a theme of mine that I've noted to call this a "persistent" tradeoff, rather than "inevitable," because I think that using today's computer technology and today's computer architecture, we're pretty much stuck with trading off one for the other, you know, security versus freedom.  But I think that's only due to the way we're currently solving these problems.  And, okay.  Yours is bigger than mine, Leo.



LEO:  It's a TWiT mug.



STEVE:  Oh, my goodness.



LEO:  Sorry, I didn't mean to interrupt.



STEVE:  That's all right.  The only problem is you pour hot coffee in that, and it turns into cold coffee.



LEO:  Pretty quick.  Surface area.



STEVE:  So much volume to heat up, yeah.  Anyway, so I believe that it's only due to the way that we're currently solving these problems that we are in the dilemma we are.  I think it's inertia holding us back and that we will eventually have, I don't know what it's going to be, but hopefully we'll see it in our lifetime - I think we probably will - a rethinking of the way we do this because, basically, we're not doing anything differently than we were back in the mainframe days when machines with lights like that were blinking.  So but for now, here and today, we clearly do face a tradeoff.  And this tradeoff, which Google and Chrome, which of course is now the number one web browser worldwide, that's what the tradeoff is continually struggling with.



They're currently working to evolve the interface, essentially the API, offered to third-party browser extensions.  And that interface determines, obviously, what the hooks are that the extension has into the browser, what it's able to do.  We've been at what they call Manifest v2 because an extension contains a manifest where it declares what features and services it needs, and then the browser interacts with it based on its declaration of stuff that it wants to be able to do.  The Chromium team is currently headed toward Manifest v3.  But in this struggle, this tradeoff of security versus freedom, it's predictably ruffling some feathers.



ZDNet noted in their coverage, they had an article titled "Chrome API update will kill a bunch of other extensions, not just ad blockers.  Chrome extensions for antivirus products, parental control enforcement, and various privacy-enhancing services will also be affected."  And Bleeping Computer had two separate postings, noting that our podcast favorite, uBlock Origin, may die.  And I think we must have referred to that in the last couple of weeks, Leo, because that sounds familiar to me, that we were talking about uBlock Origin being endangered.



LEO:  Yeah, well, it doesn't work on Safari unless you go to the GitHub page and so forth.  And I guess this is maybe an issue there.  But I hope, and I presume, Gorhill is going to do something about that.  I mean, that's a big, big deal.



STEVE:  Yeah.  Well, and the other thing that may die is Tampermonkey, which I thought, what?  Apparently it's as popular as uBlock Origin.  Well, so both of Raymond Hill's, a.k.a. Gorhill, as you referred to him, extremely popular Chrome extensions, uBlock Origin and uMatrix, would, as he understands it, die.  If the changes happen...



LEO:  He can't fix it?



STEVE:  Unh-unh.



LEO:  Whoa.



STEVE:  Well, and we don't know how grumbly he's being.  But he said if the...



LEO:  Yeah, he's pretty grumbly.



STEVE:  Yes, if the changes happen as they're currently defined in the next version draft of this v3 Manifest.  So in the forthcoming v3, the Chrome developers have stated their intention to limit the blocking capabilities of something known as the webRequest API, which Raymond's extensions require.  The current proposal reads:  "In Manifest v3 we will strive to limit the blocking version of webRequest, potentially removing blocking options from most events, making them observational only.  Content blockers should instead use something known as 'declarativeNetRequest.'"



Raymond said that phasing out 'webRequest' API in favor of the 'declarativeNetRequest' API would mean the death of uBlock Origin, which is used by over 10 million users on Chrome.  He wrote to the bug tracking page where this Manifest V3 work is being discussed.  He said:  "If this (quite limited) declarativeNetRequest API ends up being the only way content blockers can accomplish their duty, this essentially means that the two content blockers I have maintained for years, uBlock Origin," and he says, parens, "(uBO) and uMatrix can no longer exist."  In his posting, he explained that his extensions are incompatible with the proposed declarativeNetRequest API.



LEO:  Oh.



STEVE:  Uh-huh.  Because it allows for only one specific filtering engine, whereas uBlock Origin and uMatrix rely on various filtering designs to do their job properly.  The proposed modification is more oriented toward more limited fixed static filtering capabilities such as those provided by Adblock Plus.  But the redesign would also limit the number of filters to 30,000, which while that sounds like a lot, is insufficient even for Adblock Plus.  Ray uses the example of the EasyList filters with rules for removing unwanted web content, which is currently larger than 30,000 entries and is not sufficient for, he says, a modern user's filtering needs.  The EasyList rule set is used by both Adblock Plus and uBlock Origin and is much larger than the limit imposed by this declarativeNetRequest.



So the good news is, and I'm so happy that we have this, I mean, there is a forum that exists where this stuff is being seen.  I mean, you know, there's interaction between the devs and the counter devs, those who are pushing back against.  And so, as we'll see in the second big topic here for the end of the podcast, there are security consequences to this.  But, and I get it that Google isn't, we know, removing things without cause.  But there is a tradeoff in our current model, the way things work today, of security and freedom.



So what about Tampermonkey?  This was the first I had encountered the term.  And I thought, what the heck is Tampermonkey?  Well, first of all, it is nothing like Honey Monkey, so disabuse yourself of any belief.  But I have heard of Greasemonkey, and it is like Greasemonkey.  So although I've been unaware of it forever, over 10 million users are making use of this.  And I don't know who they are, but they are pretty much super techies.  Okay.  So what all of these things are - Greasemonkey, Tampermonkey, there's also one called Violentmonkey, I don't know why, but anyway.  [Crosstalk].  It does.  So those are the three big things.  They're called userscripts managers.



LEO:  Yeah.  Greasemonkey's been around for years.



STEVE:  Yeah.  Yeah, yeah, yeah.  In fact, Greasemonkey predates Chrome.  It was available on Firefox in the old days.



LEO:  Right.



STEVE:  So what these all are, and it's like, okay, I thought uBlock Origin gave me all the control I needed.  No.  These things allow scripts to be added, injected, userscripts to be injected into websites, the pages of websites, in order to alter web browser behavior, to tweak it for whatever goal or benefit the user has.  Apparently Tampermonkey is currently now at the top of the heap, the most popular userscript manager.  Chrome supports Tampermonkey or Violentmonkey.  Apparently not Greasemonkey.  Firefox supports all three - Greasemonkey, Tampermonkey, Violentmonkey.  Safari does support Tampermonkey.  Microsoft Edge, Tampermonkey.  Opera, Tampermonkey or Violentmonkey.  Maxthon, only Violentmonkey.  Dolphin, only Tampermonkey.  And so on.  So anyway, Tampermonkey pretty much has it.



So the way Tampermonkey describes itself, it says:  "Tampermonkey makes it very easy to manage your userscripts" - I didn't know my userscripts needed management, but I'm learning - "and provides features like a clear overview over the running scripts" - which you would of course want if you wanted your userscripts to be managed - "a built-in editor, ZIP-based import and export, automatic update checks, and browser and cloud storage-based synchronization."  All the things you would want from a good Tampermonkey.



However, userscripts, they say - oh, no.  This is me talking because I did some browsing around, and I found a horrible downside.  Userscripts are very powerful.  And, of course, the script repositories have been overrun with malicious scripts hoping to get themselves injected into an unwitting user's browser.  So it should not be surprising that Google devs are wondering whether this whole thing is a good idea.  In his Google group's posting, Jan Biniok, Tampermonkey's creator, writes, he says:  "Hi, Chromium developers."  And "Hi, Devlin," who communicated with him via email.  He says:  "I'm the Tampermonkey developer, and I have not studied all the planned changes in detail yet.  But this is the one that worries me most."



And then he quotes, saying:  "Beginning with Manifest v3, we will disallow extensions from remotely hosted code.  This will require that" - which, okay, sounds good to me.  But Google says:  "This will require that all code executed by the extension be present in the extension's package uploaded to the web store.  Server communication, potentially changing extension behavior, will still be allowed.  This will help us better review the extensions uploaded and keep our users safe.  We will leverage a minimum required CSP [Content Security Policy] to help enforce this, though it will not be 100% unpreventable, and we will require policy and manual review enforcement, as well."



So, okay.  Just stepping back for a second, basically they're saying we're not going to - we're considering in v3 disallowing an extension's freedom to just reach out and download code and run it at will.  That's like, yeah.  Anyway, so then of course Jan, who is upset about this, says:  "While the text above might be interpreted in a way that an extension like Tampermonkey can continue to exist, I got the following explanation from Devlin in an email."



And Devlin writes, and apparently he's with Google because he says "we":  "Note that we will be limiting remotely hosted/arbitrary code execution" - there's that phrase again, Leo - "in all contexts.  The goal is that we should be able to perform an in-depth security review of an extension and be confident in what it does and whether it poses a security or privacy risk to users, which is possible through web page contexts, as well.  But let's move this conversation to another thread," says Devlin.



So then Jan says:  "I understand the need for security, but this means that v3 P1, in the way it's currently planned, will stop Tampermonkey from working entirely because arbitrary code execution is Tampermonkey's main functionality.  Every little userscript would then have to become an own extension."  Or he probably means its own extension.  "Anyone who wants to do that has to pay $5 to be able to publish an extension.  There are so many use cases" - well, and god, that would bury Google under random arbitrary extension storm - "many use cases for userscripts, so I hope that this planned change is reconsidered.  One possibility would be, for example, a new permission that relaxes this constraint and allows remote code execution again. All extensions with this permission could then be provided with a special warning and be examined more intensively.  What do you think?"



He finishes:  "I've been working on Tampermonkey since Chrome v4 or 5, and I could not live without it anymore."  And I'll just comment that somehow I have been living without it.  But I certainly understand, I mean, once upon a time I was living without JavaScript, too, with NoScript.  But those days are gone.



And to Jan's posting, somebody replied:  "Jan, this new remotely hosted code restriction also affects my project, a process automation/RPA system.  I asked the team about it at the Chrome Dev Summit last November, bringing up Tampermonkey as an example of a productive and popular use of remotely hosted code, arguing that a permission showing a big scary warning on install should be adequate.  Their response was that remote code is too big a threat vector.  Extensions can do too much harm, and even an extension that starts out benevolent might be later compromised.  They seem pretty committed to the decision."



He said:  "My solution is to output a browser extension for each customer, one containing all the automations/userscripts they use and want.  If you're interested in this kind of approach for Tampermonkey, I'd be happy to collaborate.  It avoids the new restriction, but does complicate the script update process, requiring the user to rebuild periodically.  I have some ideas for reducing rebuild frequency," blah blah blah blah blah.



So those are perfect examples of where, I mean, do we want a web extension to explicitly open our browser to remote code execution, which Google has seen hugely abused and wants to eliminate.  I don't know.



LEO:  Hmm.  You have to do what's secure.  I mean...



STEVE:  Yeah, yeah.  Google is, I mean, like, maybe stronger sandboxing so that you can't hurt yourself?  But as we'll see, even that doesn't solve the problem because it turns out there are very - there's a lot of power that extensions can use within the browser, which takes me to the final piece of this.



The paper was shortened EmPoWeb, short for Empowering Web Applications with Browser Extensions.  A French security researcher - boy, and there's no way I'm going to pronounce his name, Doliere Francis Some - took a long look at the security implications, I guess his team because he says "we," maybe, of the extreme powers which are currently given to browser extensions.



His abstract reads - I think it's a 19-page paper:  "Browser extensions are third-party programs, tightly integrated into browsers, where they execute with elevated privileges in order to provide users with additional functionalities.  Unlike web applications" - okay, and he uses that term throughout, so let's make sure we understand it.  Web applications are web pages.  They're JavaScript apps running on a page, like, you know, editing documents, spreadsheets, Google docs sorts of stuff.  That's a web app.



So "Unlike web apps, extensions are not subject to the Same Origin Policy and therefore can read and write user data on any web application.  They also have access to sensitive user information including browser history, bookmarks, credentials (cookies), and list of installed extensions.  They have access to a permanent storage in which they can store data as long as they're installed in the user's browser.  They can trigger the download of arbitrary files and save them on the user's device.



"For security reasons, browser extensions and web applications are executed in separate contexts."  Okay.  For security reasons.  So there was an attempt at isolation.  He says:  "Nonetheless, in all major browsers, extensions and web applications can interact by exchanging messages."  And I would argue they have to.  I mean, otherwise there's no point in having them.  "Through these communication channels, a web application can exploit extension privileged capabilities and thereby access and exfiltrate sensitive user information.



"In this work we analyzed the communication interfaces exposed to web applications by Chrome, Firefox, and Opera browser extensions.  As a result, we identified many extensions that web applications can exploit to access privileged capabilities.  Through extensions' APIs, web applications can bypass Same Origin Policy and access user data on any other web application, access user credentials (cookies), browsing history, bookmarks, list of installed extensions, extensions storage, and download and save arbitrary files in the user's device.



"Our results demonstrate that the communications between browser extensions and web applications pose serious security and privacy threats to browsers, web applications, and more importantly to users.  We discuss countermeasures and proposals, and believe that our study and in particular the tool we used to detect and exploit these threats can be used as part of extensions review process by browser vendors to help them identify and fix the aforementioned problems in extensions."



So I'm going to skip this - he provided a wonderfully detailed 19-page PDF.  I have the links here.  So I have just two last paragraphs.  They're long, but - well, they're not, they're medium-length - but that I snipped from toward the end of this after all of the underlying foundation is laid down.  But these essentially are what he found.



He says:  "We built a static analyzer and applied it to the message passing interfaces exposed by Google Chrome, Firefox, and Opera extensions to web apps.  When the tool found that a privileged extension capability could potentially be exploited by web applications, the extension was flagged as suspicious.  By manually reviewing the code of flagged suspicious extensions, we found that 197 of them, mostly on Chrome, can be exploited by web applications" - meaning just arbitrary websites - "to access elevated browser features and APIs and sensitive user information."



Okay.  So just to pause for a minute, that means, if you have any of these 197 different extensions installed and visit a site that is aware of these, the site can leverage the extension behind your back to attack you.



Okay.  So they continue:  "The extensions we have found have vulnerabilities that can be exploited by web applications [i.e., web pages] to, one, break the privilege separation between extensions and web applications and execute arbitrary code in the extensions context," which of course is privileged.  "Two, bypass the Same Origin Policy and access user data on other applications," that is, other web pages.  "Three, read user cookies and use them to mount session hijacking attacks."  In other words, the return of Fire - what was that thing, Firesheep?  Basically, if you went to a malicious site, it could get your Google session cookie, your Amazon session cookie.  The idea that we're all being statically logged on is convenient for us, but it represents a liability if anything else can get those cookies.  And this allows that.



"Read user cookies and use them to mount session hijacking attacks.  Access data such as user browsing history, bookmarks, list of installed extensions that besides violating user privacy can be used for tracking purposes.  Five, store and retrieve data from extensions persistent storage for tracking; trigger the download of malicious software on the user's device whose execution can then damage user data."  So that's what they found.



In their conclusions they mentioned their disclosure to vendors, and they said:  "We have disclosed the list of extensions to Chrome, Firefox, and Opera.  All vendors acknowledge the issues.  Firefox has removed all the reported extensions.  Opera has also removed all the extensions but two, which can be exploited to trigger downloads.  The reason given by Opera is that the downloads can only be triggered from specific websites.  However, we made them observe that those websites include third-party scripts that can also trigger arbitrary downloads.  So discussion still continues with Opera on the two remaining extensions, in particular to ensure that users are aware of the downloads.  Chrome also acknowledged the problem in the reported extensions.  We are still discussing with them on potential actions to take, either remove or fix the extensions."



So a beautiful piece of research, and important.  So what this says is that there are nearly 200 extensions, most on Chrome, where the capabilities snuck past the extension reviewers.  And if a page wished to, it could leverage the presence of the extension in order to do some or all of these things, depending upon what the extension is doing.  So it is the case that we have an important tradeoff here between capability and security.  We've talked before about how incredibly useful some of our browser extensions are.  We don't want to lose them.



And Leo, I was about to mention when we were talking about the first portion of this that maybe Chrome could have like an expert user version, or expert user mode.  Maybe Chrome as the mainstream browser has to eliminate some of the flexibility if it cannot provide it securely.  But we could still have a browser like Firefox that would have a different user profile, that would allow some of these things moving forward.



I don't know how this is going to sort out.  It would be nice if we could have all of this, have this kind of capability and power and security.  The fact that only 197 extensions are problematic suggests that it's possible to do what these things do without them being exploitable.  So maybe that's the solution is just focus, you know, because god knows there's a bazillion extensions for Chrome.  And if these guys, out of that, only found 197 that were causing this kind of trouble, one would think those could be fixed.



So anyway, it does suggest that we've got a ways to go.  I don't know what v3 will mean.  I would hate to lose uBlock Origin over on Chrome.  I would hate for Firefox to follow because it's where I mostly depend upon uBlock Origin.  And, boy, the web is no fun without something like this to tame the pages to some degree.



LEO:  I think increasingly people are - this doesn't solve it on mobile, but at home are using router-based ad blocking, like the Pi-Hole or Eero or Plume.  Both of my routers have ad blocking in the router.



STEVE:  I don't know how you could do that over HTTPS. 



LEO:  Yeah, I guess you couldn't, could you, because you can't see into it, yeah.



STEVE:  Yeah, right.



LEO:  So that's not going to be a long-term solution.



STEVE:  Although they could do DNS blocking.  So if they refuse...



LEO:  Yeah, they block sites, yeah, the sources.



STEVE:  Exactly.



LEO:  And think that's mostly what they do.  So you can't go to ads.syndication.google or whatever.



STEVE:  The problem, of course, then is you get all these little broken things all over the page.



LEO:  Yeah, holes.  I know.  You get a lot of holes.  That's, yeah, that's how Pi-Hole works.



STEVE:  Okay.



LEO:  Are there browsers that - do you think at some point there'll be - I don't think Google's going to do an inexpert version of Chrome.  But at some point Firefox or some other browser might be the expert's browser.



STEVE:  Yes.  I mean, I think that.  I could see that being the way this evolves is that, you know, Firefox wants to hold onto a niche, and that would be a beautiful niche for them to have.



LEO:  Yeah.



STEVE:  And Lord knows, if there are 10 million people that have downloaded honky-tonk monkey, what was that thing called?



LEO:  You'd better not mess with me.



STEVE:  Tampermonkey?  I don't know who, but 10 million people are injecting scripts into other - okay, well, you know, you guys really have your propellers wound pretty tight, I guess.



LEO:  At this point, you know, it's really Chromium versus the rest of the world.  And so, you know, in fact one Microsoft developer said Firefox should just stop, just give up and join the rest of the world so there'll be one web standard, Chromium, now that Edge is going with Chromium.  I completely disagree.  And this is an opportunity for Firefox to find a niche.



STEVE:  And the Mozilla team is staying, I mean, at par.  They're keeping up in performance and speed, and they're staying with the standards.  They still have the best side tabs.  I can't, you know, I mean, there's nothing better than that.  So, yeah.  I'm glad they're hanging in there.



LEO:  Although, if people say it's less secure, that's not going to be good.



STEVE:  What's less secure?



LEO:  Well, because they support these extensions, that they're more vulnerable.  That wouldn't be good.



STEVE:  Yeah.



LEO:  So I don't know.



STEVE:  Everybody I know uses Chrome.  That's just what they use.



LEO:  I think you said this already, but I just want to make it clear.  There's no implication that Google's doing this because they don't want ad blockers.  This is a security thing.  



STEVE:  No, no.  In fact, Adblock Plus, because it's a simple monotonic filter, could continue to be used.  But apparently they're trying to limit the number of filter rules that can be imposed.  So if that happens, then we'll have to choose the most important 30,000, believe it or not, and maybe lose some of the obscure ones.  But no, this would allow ad blocking, just not the...



LEO:  Not Gorhill style.



STEVE:  Exactly, where it's doing all - it's like a multifactor filter where it's hooked in all over the place and doing all kinds of - allowing you to have a multimodal filter of different kinds of things.



LEO:  Yeah.  Well, this will be interesting to watch as it develops.  Steve, as always...



STEVE:  I'm just glad we have a dialogue.  As long as there's discussion, there's hope.



LEO:  Yeah, yeah.  And this is a proposal from Google.  This isn't written in stone yet.



STEVE:  Yup, yup, and they're discussing it.



LEO:  Yeah.  And they're hearing, I hope.



STEVE:  And we're done discussing it, Leo.



LEO:  We're done.  We're going to go home now.  Steve's home is GRC.com.  That's where you'll find SpinRite, the world's best hard drive maintenance and recovery utility, saving family photos all over the world.  You'll also find this podcast.  He's got audio and transcripts there at GRC.com, and all his freebies, like SQRL.  GRC.com.  You can tweet at him at @SGgrc.  In fact, he'll take DMs from anybody, crazy man.  So if you've got a tip or a question, do it there or at GRC.com/feedback.



You can get audio and video versions of the show at our website, TWiT.tv/sn for Security Now!.  Or subscribe on your favorite podcast application.  That's the best way to do it.  That way you'll get every minute.  And you can complete your set, have all 700 next week, 700 Security Now! episodes.



We do the show every Tuesday, 1:30 Pacific, 4:30 Eastern, 21:30 UTC.  Come by and watch it live at TWiT.tv/live, join us in the chatroom at irc.twit.tv, or download at your convenience.  We don't care how you listen, as long as you listen.  We'll see you next time on Security Now!.  Bye-bye, Steve.



LEO:  Thanks, Leo.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#700

DATE:		February 5, 2019

TITLE:		700 & Counting

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-700.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we discuss Chrome getting spell check for URLs; a bunch of Linux news with reasons to be sure you're patched up; some performance enhancements, updates, additions, and deletions from Chrome and Firefox; more Facebook nonsense; a bold move planned by the Japanese government; Ubiquiti routers again in trouble; a hopeful and welcome new initiative for the Chrome browser; a piece of errata; a quick SQRL update; and some follow-up thoughts about VPN connectivity.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Lots to talk about, including new systemd vulnerabilities.  Linux users, listen up.  We'll also talk a little bit about Chrome, a new feature giving us URL spell checking, and why TLS 1.0 and 1.1 are soon to hit the highway.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 700, recorded Tuesday, February 5th, 2019:  700 & Counting.



It's time for Security Now!, the show where we cover the latest developments in the world of security and privacy, help you understand how computing works, and have a little fun along the way with this guy right here, Steve Gibson.  He's the commander in chief of the good ship Security Now!.  Aye aye, sir.  What you pointing - that is not the logo you want.  Maybe this.



STEVE GIBSON:  No, no.



LEO:  You want an "L."



STEVE:  Yeah, but which hand do I use?  I can never get that right.



LEO:  I don't know.  Yeah, it's very important.  Sometimes it's an "L."  Sometimes it's backwards.  Hey, Steve.  You're no loser in my book.  



STEVE:  Leo.



LEO:  You're number one.



STEVE:  Well, thank you very much.  Great to be with you again for, as I was saying before the podcast to you, I was tempted to title this one "The 700 Club"; but I thought, no, that's been taken.  So let's just call this one "700 & Counting."



LEO:  The 700th episode.



STEVE:  Yup.



LEO:  Amazing.



STEVE:  And as is sometimes the case, there was no one particular crazy thing that stood out.  So we've got a bunch of stuff to talk about.  We've got Chrome getting spell checking for URLs.  That's what I call it.  That's not what they call it.  A bunch of Linux news with reasons to be sure, extra sure you're patched up to date.  And some performance enhancements, updates, additions, and deletions from Chrome and Firefox.  There's some more Facebook nonsense.  And I have a theory about Facebook nonsense we'll talk about.  Also a bold move planned by the Japanese government.  I've not been listening to the network, so I don't know if you guys have been talking about this on other podcasts, but something big is in the works.



LEO:  Yeah.  We talked about it on TWiT, yeah.  Thought it was very interesting.  I'm glad you're going to bring it up, yeah.



STEVE:  And also we've got Ubiquiti routers in trouble again, big trouble.  A hopeful and welcome new initiative for the Chrome browser.  A piece of errata, a quick SQRL update, and some follow-up thoughts about VPN connectivity.  So I think I can promise another great podcast for our listeners.



LEO:  Well, that ought to do her, I'll tell you what.  Sounds like fun ahead with Steve.  And of course our Picture of the Week is good.  Steve?



STEVE:  So we had two pictures this week.  The first one I ran across just when doing some research I encountered a notice and a notification from Firefox that we've talked about in the past, but I had never seen it.  I went to LinuxForums.org and got, what do they call it?  I think they call it a "wall hanger" or something where it hangs down from - I've seen the term. Anyway, it says:  "Have an account on this site?  More than 200,000 accounts from LinuxForums were compromised in 2018."



LEO:  Wow.  That's nice.



STEVE:  Check Firefox - yeah, isn't that nice?  So it's a proactive notification that where you're going has had a security compromise.  I think I remember when we talked about this, like within the last year.  So it's not like forever.  But it's like, while it's relevant.  And then you're able to click on "Check Firefox Monitor," provide your email address, and I think it uses Troy Hunt's, I'm sure now that I remember, it uses Troy Hunt's Have I Been Pwned site.  Troy provides an API that allows facilities like this to query his backend database.  So it checks for you, given the email address, to see whether your address is on the Have I Been Pwned database and, if so, warns you.  So just a very nice sort of closing-the-loop, proactive, hey, you know, in case you haven't been listening to Security Now! or you don't have an eidetic memory, we'll help you by saying, hey, remember when we talked about Linux forums being pwned?  Well, maybe you should check your email address.  So very cool.



LEO:  Actually, I just wanted to mention that there is now a Chrome extension to do exactly the same thing.  Did you know that this is Safer Internet Day?



STEVE:  Oh.  Well, what a happy coincidence.



LEO:  Yeah, Happy Safer Internet Day.



STEVE:  February 5th.



LEO:  Yeah.  So Chrome has added a password checkup extension.  You have to install it, which I did immediately.  And it will say, as mine does right now, none of the recent passwords you've entered were detected in a data breach.  So it's a little bit different.  It's not precise.  It's actually looking at...



STEVE:  Ah, nice.



LEO:  And they make a big point about how they're using technology that they developed with Stanford University with the help of cryptography researchers to keep your privacy safe, you know, that passwords aren't getting sent to Google, that kind of thing.  So I don't think they're using Have I Been Pwned, it sounds like.  They probably have their own database.



STEVE:  And is it a Google extension from...



LEO:  Yes,  Oh, yeah, yeah, it's from Google.  Yeah, yeah.



STEVE:  Oh, interesting, yeah.



LEO:  So you can do this in Google, as well.



STEVE:  Well, and of course what's cool is that Google, if you have your Chrome browser saving your sites' passwords for you, then it's got a local database of the password.  And sometimes your username will be your email address and so forth.  So it would be able...



LEO:  Now, I don't do that, of course, because I use LastPass.



STEVE:  Right.



LEO:  But apparently it's still monitoring as you enter them, or as they get entered.



STEVE:  Yeah, very nice.



LEO:  And they do cross-account protection and stuff.  So it's kind of cool, very cool, yeah.



STEVE:  Well, and so the second picture that we have - I gave the first picture the title "Firefox Warned Me."  And so I titled the second one, "And So Did Chrome."  Although this was a different one, which is our first story.



LEO:  I like this, though.



STEVE:  Yes.  Yes, yes, yes.  This shows that paypai.com has been entered into the URL bar of Chrome.  And there's a dropdown saying, "Did you mean to go to http://paypal.com?"  I'm not sure why it didn't do https://.



LEO:  Yeah, that's interesting.



STEVE:  That's interesting.  But so...



LEO:  And it's a link, so you could click it, and you could say, oh, whoops, yes.



STEVE:  Yup.  That was a typo.  I meant - exactly.  So you click, and you go to the right place.  Okay.  So a bit of terminology first.  Typo squatting, which is what this practice is of bad guys registering lookalike or typo, like P-A-Y-P-A-I dot com, and hoping that some percentage of people are going to type "I" rather than "L" and go there.  And so what they'll do is they'll set up a fake-looking PayPal site and say, oh, you haven't used this machine, or you haven't registered before on this machine or, you know, one way or the other they will spoof you into getting you to give them your PayPal credentials, believing that that's where you are.  And then you're in trouble.



So this so-called "typo squatting" is formerly known as an "IDN homograph attack," IDN being International Domain Name.  But typo squatting is a lot catchier.  Similarly, although I think that URL spell checking is clear, Google calls their forthcoming technology "navigation suggestions for lookalike URLs."  I don't think that has a catchy abbreviation, NSFLU.



LEO:  NSFLU.



STEVE:  NSFLU, yeah.  So it's under active experimentation with the Canary release, which is at 70 now, in Chrome.  And if all goes as planned, it will be appearing in mainstream release before long.  You know, they like to sort of roll these things out incrementally, which is a good thing.  Actually, we're going to be talking about Firefox here in a minute where they did something that they followed out that caused a whole bunch of unexpected problems.  So it makes sense to have, as Firefox does, a nightly build channel.



And in the case of Chrome they have their Canary release, which precedes the rollout to the public by some length of time, depending.  So if you're on the Canary circuit, you can enable this feature if you want to play with it.  Under chrome://flags, it's enable-lookalike-url-navigation-suggestions.  And of course you, I think - I went there, and it just said "default," which is a little unclear.  It'd be nice if it said whether that meant yes or no.  But anyway, you could set it to "enable," and then maybe you would see some of that.



Oh, and I also noted that the flag - because I'm not on the Canary circuit, but mine had that.  So I'm in the just-released stable Chrome 72.  It had the flag, but it doesn't appear to be wired up yet.  So if you are on the stable release, and you do go there, and you turn it on, and then you experimentally type in "paypai," you know, P-A-I dot com, and nothing happens, it's like, well, yeah.  Apparently the flag is there, but it's a do nothing.  It hasn't been connected yet.



So we'll get there.  And I think that's great.  I mean, we're beginning to see a number of these reactions to longstanding problems where the browser developers are saying, you know, we just can't bury our heads in the sand here.  Users are being tricked and hurt by clever bad guys leveraging common behaviors, which technically is not our, the browser's, responsibility.  If someone says I want P-A-Y-P-A-I dot com, darn it, well, we should take them there.  It's like, well, yes, but let's just make sure they didn't mean the much more common PayPal.  And the beauty of the position that of course Chrome is in is they know exactly how many people go to PayPal.com versus PayPai.com.  And they can say, oh, you know, the chances are very good that this person actually meant PayPal.  So I think this is a fabulous enhancement.



LEO:  I feel like I've seen that before, but maybe I haven't.



STEVE:  Well, and the other thing, too, is that I guess people still type in URLs manually, but that's sort of fallen by the wayside, too.  I mean, aren't we just mostly clicking on stuff? 



LEO:  Yeah, yeah, yeah.



STEVE:  You know, it's like, when was the last time you had to, like, I don't even trust myself to enter a URL.  Besides, they're not friendly anymore.  They've got, like, GUIDs build into them, where it's 326957.3-4729 - it's like, okay, I can't enter that.  Just give me a link to click.  In fact, we may see ultimately the URL become sort of more obscured.  And it's like, well, do you really want to enter this by hand?  Because, you know, that's only for sophisticated users.  It's like, okay.



So speaking of sophisticated users, it is definitely time to, as I said, catch up on your patch up for Linux.  We talked previously about a not yet really made clear vulnerability in a module of a number of desktop Linuxes called "systemd."  It's not in the core base Linux, but it's added to a number of the desktop environments in order to help sort of create the desktop, get all kinds of different widgets and things loaded and get the system started up.  A number of the more popular Linux desktop distros have adopted this.



Well, just last week a company called Capsule8, they're a U.S. cybersecurity company, published a working proof of concept which unfortunately weaponized two of the three vulnerabilities which were reported late last year by Qualys.  And we talked about it at the time.  There wasn't much information, and that was on purpose because we didn't want this to happen, what just has.  On the other hand, it's been a couple months, and hopefully people are keeping their Linux distros current.  If not, as I said, now would be a good time to catch up your patch up.



On the 9th, Qualys posted the following detailed report.  Their report was titled "System Down:  A systemd-journald exploit."  And they said:  "We discovered three vulnerabilities in systemd-journald."  So this is a logging component of the system daemon module.  There are three CVEs that have been assigned for 2018 ending in 16864, 16865, and 16866.  The first two are memory corruptions where the attacker can control allocs, and the third one is an information leak which results from an out-of-bounds read.  So in researching it, thanks to the audit trail that we have on Linux modules, Qualys was able to see that the first of these, the 16864, was originally introduced in April of 2013 in the systemd v203.  And it became exploitable in February of 2016 in systemd v230, or build, I guess, v230.



They said:  "We developed a proof of concept for 16864 that gains EIP" - that's the Extended IP, or the Extended Instruction Pointer - "control on a 386 machine," on an x86 architecture.  The second one, 685, "was introduced in December of 2011 with systemd v38 and became exploitable in April of 2013 with v201."  Then the final, the third one, which is 16866, "was introduced in June of 2015 with systemd v221 and was inadvertently fixed in August of 2018."



So Qualys says:  "We developed an exploit for 16865 and 16866 that obtains a local root shell in 10 minutes on an x86 architecture and 70 minutes on a 64-bit machine on average."  They said:  "We will publish our exploit in the near future."  So they said:  "To the best of our knowledge, all systemd-based Linux distributions are vulnerable, but SUSE Linux Enterprise 15, openSUSE Leap 15.0, and Fedora 28 and 29 are not exploitable because their user space is compiled with GCC's -fstack-clash-protection."  Okay.  So systemd is used by Arch Linux, CentOS, CoreOS, Debian.  Is it Mageia?  Yeah, looks like Mageia.



LEO:  Mangia mangia.  No, I don't know the word, no.



STEVE:  Mangia, yeah, no.  Okay.  Good.  Yeah, it's spelled M-A-G-E-I-A.  So maybe it's a not...



LEO:  Mageia.



STEVE:  Mageia, yeah.  Mint, Red Hat Enterprise, Solus, and Ubuntu.  So of course the big ones there are Debian and Ubuntu.  And I did have Fedora in my list, but at least 28 and 29 are not exploitable.  So the point is this is a module which, if it has  not been recently patched, is subject to a local vulnerability.  It's worth noting that this is almost certainly only of local concern.



The 18685 is vulnerability triggered.  I'm going to go into this a little bit because it's interesting how we fit these together.  This is another example of individual vulnerabilities not being a great cause of concern.  But then you mix them, you combine them in order to get what you're - the hacker does in order to get what they're looking for.  So 16865 is a vulnerability triggered by code in the systemd's logging software - that's the journald - that allocates temporary memory to contain a log entry without first checking that the request is of sensible size.  This means that you're able to allocate, basically ask it to log something that's megabytes in size.



And in fact that's how they found it.  They were actually - Qualys was actually doing some research in something else and had some reason to actually produce a log entry of that size, like a dump of some sort, and it crashed the system.  And they said, "Whoops, what crashed?" and that's of course where these things all begin.  So what this allows for is ready code execution.  But that's thwarted by the presence of ASLR, Address Space Layout Randomization, which of course as we know prevents you or dramatically improves or increases the difficulty of exploiting because it's randomizing where things are located in memory.



Fortunately or not, the second bug, 16866, allows specially formatted text sent to the system log to cause the same systemd to write out a message containing data from parts of memory that the user is not supposed to see.  So this is a classic information disclosure vulnerability which again would, okay, if it's something running in your own machine, is that really a big deal?  Well, that allows the attacker the information required to then bypass Address Space Layout Randomization and exploit the previous vulnerability to run code of their choosing to accomplish whatever malignant goal they may have.  So it's not clear that a remote attacker would be able to cause the log to be written or essentially to exploit this remotely.  But it's certainly the case that something running on your own machine could.



So the idea would be, if this weren't patched, and now what's happened is, last week, full weaponized proof-of-concept code exists.  There is also, I'm seeing just sort of in the air this year a growing focus on Linux attacks.  Linux has so far and sort of compared to Windows enjoyed relative obscurity in the hacker community.  That really does seem to be changing now, maybe because Linux is being adopted on an international scale.



As we were talking about last week, Leo, it's like, well, okay, why is it that Russia and China are using Windows?  That just seems nuts.  And of course, as we know, they're moving away from that to their own typically Linux-based platforms.  So maybe that's why we're beginning to see more of this.  But in any event...



LEO:  Some would say that one of the flaws is it's become more monolithic.  More and more applications or distros are using systemd, which is not, you know, that's a little controversial.



STEVE:  Yeah.



LEO:  I'll just look at my systemd, and it's v232.  These are fairly old versions they're talking about.  I think.  Oh, no, 230 is vulnerable.  Okay.



STEVE:  Yeah, yeah.



LEO:  And I'm on, unfortunately, the Debian stable version, and Debian doesn't quickly update these.  And so it doesn't look like there's an update available, so that's interesting.



STEVE:  Well, good, I'm glad you checked.  And for our listeners, again, I can't say for sure that somebody outside your system couldn't force logging.  I don't think they could obtain the results of the logging, which is what is necessary for the second, for the ASLR bypass.  But just if in general you're lazy about, I mean, deliberately lazy, as many of us are now on Windows, like I don't think I want that feature update yet because it hasn't been going so well for people.  You know, just if in general you're not in a hurry to change things, under the "If it's not broke, don't fix it" approach, which I certainly understand, being somebody who just recently left XP, then now would be a good time to say, well, let me just check to see if there's something new.  Or maybe give it a month.  But don't give it forever because this thing could end up biting you.



So Chrome 72, which we were talking about a second ago, that's the just-released stable channel version, is evolving.  And I thought that some of the changes they've made are interesting because they involve deprecations and removals of things.  So, for example, pages as of Chrome 72, the current Chrome, for me I had to go into Help About, and then that kind of gave it a little kick in the butt, and it updated me from 71 to 72, and then you do a relaunch, and there you are.  Pages, and this was a good thing, may no longer use the window.open to open a new page during a page unload, thank goodness.  And of course we know that there are misbehaving sites where you leave a page, and it springs up another page to say, wait a minute.  Like bad sites do that.



So they've just decided, okay.  The Chrome pop-up blocker already prohibited this, but it's now prohibited whether or not the pop-up blocker is enabled.  So yay for a usability improvement because, you know, if you're leaving somewhere, you're closing a page, just let me go.  I don't want one last "please don't go" or whatever nonsense.  Also in this 72, HTTP-based public key pinning - which is different than HSTS.  That's header-based public key pinning.  This is, well, okay, that's - I'm getting myself confused here, and I don't want to confuse people.



HTTP-based public key pinning is HPKP, which was intended to allow websites to send an HTTP header that pins one or more of the public keys present in the site's certificate chain.  But nobody ever felt comfortable with that because what that would mean would be that, if you failed to anticipate an upcoming certificate change, you would have been proactively saying "Only trust this certificate that we're sending you."  And we did talk about this at the time, I remember.  So what using that meant was you would have to be absolutely sure that you were replacing your certificate with a new one.  And then the problem would be, what if someone hadn't visited your site for a long time and only had a certificate that had then expired?  You would be providing them with a newer updated certificate which your earlier certificate and page delivery had said not to trust.



So the point of this is, because this was never really very well thought through, it never achieved much adoption.  So, I mean, this is one of the benefits of the kind of telemetry that Chrome is obtaining from its users, is it's able, you know, they're able to look at this and go, you know, this only ever got like not quite 2%, which actually is the number that I remember seeing about this.  So we're going to just get rid of it because no one's using it, no one is going to use it, and it just, you know, this was something we can get rid of.  So it's gone in 72.  And notice that this was something where the removal of it wasn't anything that anyone depended upon.  It was a belt-and-suspenders sort of thing.  So, okay, we're going to just trust the suspenders, and we'll go beltless.  And there are better solutions for doing this, too.



Also, we've talked about the fact that, over the long term, FTP is finally going to disappear from our web browsers.  I mean, back in the day with Netscape Navigator you did sometimes, it was sort of convenient that you could use your web browser to show you the contents of an FTP server and click on links and navigate around.  It turns out that, up until now, a web page could use an FTP link to render an image.  You could have an image tag that wasn't https://, it was ftp.  And it was like, what?  Who would use an FTP link to render images on a page?  But apparently, well, maybe nobody, but the point was the feature has always been in our browsers until now.



So Chrome is beginning the process we discussed recently of deprecating FTP.  You'll still be able to use it to browse a directory and click on links to download things manually.  But the browser page will no longer use FTP to pull its own content, like images and so forth.  If that breaks anybody's page, well, okay.  It's time to move your content.  Well, first of all, it's nonsecured.  It's nonsecurable.  There is SFTP and FTPS which are secure versions, but that's not what we're talking about here.  So just in general it's going away; and, you know, fine.  I'm all for something like this that is this old and is not being really used in our current ecosystem to be removed.



And speaking of deprecation, TLS 1.0 and 1.1 are not long for the world.  1.0 is even older than this podcast, believe it or not, Leo, nearly 20 years old.  



LEO:  Nothing's older than this podcast.



STEVE:  And because 1.0 and 1.1 can make use of MD5, okay, and SHA-1, both which are no longer considered sufficiently strong, it's really time to retire these.  They also are both able to use RC4 and CBC ciphers.  Well, CBC being an encryption mode.  As we know, RC4, I loved it for its simplicity and that it was as strong as it was.  But it suffered from implementation weaknesses because its pseudorandom stream generator needed more warm-up time than it was being given in practice, so people said, okay, we don't trust this just because.  And the CBC, cipher block chaining, its construction is flawed, which made it vulnerable to some attack.



So anyway, the point is there were modes of 1.0 and 1.1 TLS that were just getting old and considered vulnerable and time to move away.  So they are further deprecated in Chrome 72.  And I looked, trying to figure out what exactly that meant in this case.  I mean, we know in the case of, as I was just saying, FTP, pages would no longer render FTP-provided assets.  Maybe it shows something on the URL bar that's like, stop doing this.  I don't know.  Because it's not actually being killed completely until Chrome 81, which is due about a year from now, in early 2020.  So they're doing the right thing.  They're giving people time to move away.



So presumably, if you go to a 1.0 or 1.1 site under Chrome 72, something will happen.  I don't know what.  Apparently it still works.  Maybe it slaps you or something.  I don't know.  Maybe one of our listeners will find a server.  Even GRC is at TLS 1.2 now.  So you can't test it with me.  It'd be interesting to know what happens with this deprecation of 1.0 and 1.1.  Presumably it's something that the user sees that causes them to be worried, that then puts pressure on the site to update their servers in the next year sometime.  So, good.



And part of this was Chrome's deprecation policy that I thought was interesting.  They said:  "To keep the platform healthy, we sometimes remove APIs from the web platform which have run their course."  And they said:  "There can be many reasons why we would remove an API, such as:  They are superseded by newer APIs.  They are updated to reflect changes to specifications to bring alignment and consistency with other browsers.  They are early experiments that never came to fruition in other browsers and thus can increase the burden of support for web developers."



They said:  "Some of these changes will have an effect on a very small number of sites.  To mitigate issues ahead of time, we try to give developers advance notice so that they can make the required changes to keep their sites running.  Chrome currently has a process for deprecations and removals of APIs, essentially announcing on the blink-dev mailing list.  Set warnings and give time scales in the Chrome DevTools Console" - ooh, I'll bet that's where the TLS 1.0 and 1.1 deprecation stuff shows, it probably is a dev console warning - "when usage is detected on the page."  And then:  "Wait, monitor, and then remove the feature as usage drops."



So, bravo.  I'm glad that the number one browser in the industry is making these moves and essentially creating some coverage for other browsers that want to follow along and also keep things clean.



Facebook, Leo. 



LEO:  Say no more.  Say no more.



STEVE:  Oh, boy.  They got in trouble again.  In fact, maybe you guys talked about it because it was relative to iOS things in this case.



LEO:  Oh, yeah.  We talked about it.  We talked about it on TWiT.  We talked about it on MacBreak Weekly.  We talked about it.



STEVE:  So I'm not of the camp that believes this is particularly sinister.  I know a lot of people run around and lawsuits are generated and so forth.  I just think it's ungoverned and irresponsible. 



LEO:  That's all.  That's all.



STEVE:  I suspect - yeah.



LEO:  They're like teenagers.



STEVE:  Exactly.  I suspect it's what happens under conditions of explosive growth, which they have had historically, and lack of adult supervision.  When you tell a horde of recently degreed 20-something coders to just do stuff and we'll keep whatever works, this is what you get is a lot of experiments that are sometimes not wise in retrospect.  So in today's installment of "What has Facebook wrought now..."



LEO:  Or just, "Now what?"



STEVE:  Now what?  We have TechCrunch's report.  Of course it was covered by a lot of people, and TechCrunch headlined theirs:  "Facebook Pays Teens to Install VPN That Spies on Them."  And so the short version of this is it has been discovered that Facebook was paying as much as - and I don't know what set the price, maybe how active they were or how busy they were or where they were going; but, I mean, 20 bucks a month is not nothing - to install this app, which as part of its installation requires you to install Facebook's root certificate.



So as we know, what this allows is a full man-in-the-middle interception of all the traffic that your device, typically your smartphone, is transacting, allowing full visibility into everything that you're doing for monitoring purposes, in return for which you're apparently being paid somehow.  And maybe you - I'm sure probably you have more details from the consumer end.



LEO:  Yeah, gift cards.  They were sending them gift cards, yeah.



STEVE:  Ah, okay.



LEO:  Twenty bucks a month.



STEVE:  So ages 13 to 35, apparently, in this coverage, since 2013.  And again, the concern was, okay, wait a minute.  This is not okay.  It is a privacy breach.  It's a root certificate installed on the phone.  Oh, and also, and this was the thing that upset Apple so much, is that Facebook had this enterprise, was participating in this enterprise developer program, which is a means for allowing a company to develop iOS apps for its own internal use that allows them to be signed, and thus honored by the enterprise's employees, but not the public at large.  Public at large is supposed to go through the iTunes store in order to download these things.



Well, Facebook was doing an end run around the iTunes store and misusing its enterprise developer program in order to make these apps available to end users without going through iTunes and the iTunes app store, and thus subject to Apple's scrutiny and verification.  So Apple's not happy.  Facebook said that they had removed this from iTunes availability.  Then it turns out that Apple had actually booted them from the store.  It's a mess.  And if you have anything more to add, Leo, I'm all ears.  



LEO:  No.  Just they're still offering, both Google and Facebook still offer their snooper apps on Android, in case you want to run them.



STEVE:  Oh, that's right, I forgot.  Still there.  Still there, available for use on Android.



LEO:  Well, it's not against the rules on Android.



STEVE:  Yeah.  TechCrunch reported they asked a German researcher, Will Strafach, and he was quoted saying:  "The fairly technical-sounding 'Install Our Root Certificate' step is appalling," he said.  "This hands Facebook continuous access to the most sensitive data about you, and most users are going to be unable to reasonably consent to this, regardless of any agreement they sign..."



LEO:  Yeah, that's the issue is do you know what you're getting?  Yeah.  And teenagers - so it was aimed, the Facebook product was aimed at 18 to 35 year olds, 5% of whom were teenagers.  And Facebook says, "But we got a signed consent form from their parents."  But that's the point is do the teenagers or even their parents really understand what they're agreeing to?  Probably not.



STEVE:  Right, right.



LEO:  They're just, you know, Facebook says something like, you know, we'd like to watch what you do online so we can make our product better.



STEVE:  Yeah, exactly.



LEO:  How bad could that be?  And I don't think Facebook's nefarious.  I mean, that's what Strafach was saying, you know, is that, well, you shouldn't give anybody root access.  But, I mean, honestly, I don't think they're using it for a man-in-the-middle attack.  Well, they are, literally.  But...



STEVE:  Well, in fact, that's why I was careful to say "man-in-the-middle interception" because...



LEO:  Yeah, there you go, not "attack."  Yes, yes, yes.



STEVE:  Right.



LEO:  That's right, yeah.  And they say, "We told everybody.  We told them we were going to do that."



STEVE:  You know?  Well, and in fact back when I discovered that Ad-Aware stuff in my machine and created OptOut, arguably the industry's first spyware removal tool, the uproar was unbelievable.  And the guys at Ad-Aware said, wait a minute, you know?  It's in the fine print.  That's like, that doesn't, you know, sorry, but you know nobody reads that.  You can't.



LEO:  Right.  Yeah.  Yeah, it's a little disingenuous to say, oh  everybody knows what we were doing.  We said that we were doing it.  It's okay.  You chose it.



STEVE:  In a few weeks the Japanese government is going to try its hand at the practice that's come to be known as "credential stuffing" across its own country's massive install base of IoT devices.



LEO:  Wow.



STEVE:  And you've got to know that in Japan they're IoT happy.  Anything it can find publicly from enterprise network level down to end-user routers whose owners never change their default passwords.  NHK is Japan's national public broadcasting organization, which reported that the government had approved this first-of-its-kind venture several weeks ago.  So mid-February, staffers from the National Institute of Information and Communications Technology, NICT, will take many previously successful usernames and passwords and use them to attempt, like official use them to attempt to break into as many as 200 million randomly selected, publicly accessible IoT devices located across Japan - routers, webcams, DVRs, IoT...



LEO:  That's, like, twice the population of Japan, by the way.  It's two per man, woman, and child.



STEVE:  Well, when you consider that electric toothbrushes are online in Japan, everything.  So then the owners of the breached devices will be told by the Japanese government to please fix their devices.



LEO:  Please.



STEVE:  Bolster their cybersecurity.  Okay.  We'll see how well that stage goes.  So the intent behind this interesting white hat move is to reduce the viable attack surface that's currently available to attackers prior to next summer's approaching Tokyo Olympics and Paralympics, which occurs in 2020, so summer after next.  Sophos noted that some systems did go down around the time of the opening ceremony for the Winter Olympics in Pyeongchang, South Korea last year.  So sort of worth being maybe a little preemptive.  So although the immediate goal is to tighten up Japanese citizens' Internet-facing security before the Olympics, the result will almost certainly be improved security overall, I mean, given everything we know about what happens when you scan the public Internet.  You find stuff.



So the NICT has reported that IoT devices were at the heart of a large number, more than half, 54%, of the cyberattacks that it detected in 2017.  And of course that number's only growing.  So I've talked about several times, way back in the early days of Code Red and Nimda, that I participated in some discussions with the U.S. federal government.  I was on a conference call that was particularly memorable with the head of the DOJ.  And a bunch of us security types were begging to be allowed to write a sanitizing worm that would find the vulnerable devices and fix them for their owners.  It would have been easy for us to do.  But we were told in no uncertain terms at the time that doing so would be illegal and would open us to the full prosecution and wrath of the U.S. federal government.  So it was like, uh, okay.  We'd like to help but we don't want to be put in prison.  So you know, Leo, since then a lot has changed, and things seem a lot less black-and-white than they were then.



LEO:  Mm-hmm.



STEVE:  We now have, you know, we're post-Snowden and post-WikiLeaks.  We've got leaks about the CIA and apparently about the NSA, which suggests that no one here is a Boy Scout.  And it's increasingly seeming that, because of the size of this threat, that tying the hands of white hat hackers while the black hats are allowed to run free and to cripple our cyberinfrastructure is seeming less and less correct in practice.



I don't think we've talked about it, but in the news recently has been the observation that apparently foreign actors are able to cripple our electric grid.  And there was something else.  Some other major aspect of our cyberinfrastructure has been reported in the news.  And it's just like, okay, are we actually  completely unable to do anything with the fact that we are hosting this kind of potential trouble?  And it sort of seems counterintuitive.  So if this Japanese experiment bears fruit, and you can bet that all governments are watching closely to see how it goes, it seems to me it could go a long way toward opening some doors and maybe changing some minds about the feasibility of being proactive.



And it could be done incrementally, also.  For example, individual ISPs could be chartered with the ability to inspect the publicly available ports of their own customers, whom they know because they have a customer relationship with them, and then to deal with them one on one to resolve any vulnerabilities that are detected.  And that of course would create, if that were allowed, that would create a market for a big hardware security vendor to produce carrier-grade network vulnerability scanning, which would scan into an ISP's address space, looking for and logging and then allowing vulnerabilities to be handled.  It just sort of seems like we're going to end up there sooner or later.  And sooner's better than later.



LEO:  Yeah, yeah.  Wow.



STEVE:  Yeah.  Firefox 65 is where we are now, recently.  And I've not had any problems because I'm not using Avast, AVG, Bitdefender, ESET, or Kaspersky.  But it turns out people who were, were having problems.



LEO:  Oh, really.



STEVE:  Yeah.



LEO:  That's pretty much everybody who uses antiviruses.



STEVE:  Yeah.  Well, in order to maintain their relevance, all of those AV tools had begun the practice of installing a root certificate of their own in the user's machine.  Because, after all, the web has gone HTTPS.  And if they're not able to scan into the files that are being downloaded, the content that is being brought into browsers, how do they maintain relevance?



So what happened was Firefox 65 began to roll out, and people began getting notices that their connections were not reliable or the sites they were visiting could not be trusted.  It turns out that Mozilla was implementing a man-in-the-middle attack detector.  And the Mozilla people also - and apparently none of their early testers were using Avast or AVG or Bitdefender or ESET or Kaspersky, so they didn't have any problems.  But as soon as it began going out to a wider audience, this began to happen.



And what's interesting is that we've long known, we've talked about this on the podcast, that Mozilla, due to its sort of odd origination with Netscape and their own security suite, Firefox, for example, on Windows, does not use the Windows built-in security API.  They bring their own Netscape security package with them, and it contains their own root store.  So it turned out that the certificates that were being stuck into the Windows root store were not also being trusted by Firefox that has its own root store.  And so there was this disconnect present which Firefox 65 then began to detect.  The moment these alarm bells began to sound, as 65 was rolling out, Mozilla halted the release of 65.  Also, AVG and Avast both issued patches in order to back off of their scanning of Firefox so that they would no longer be filtering that and triggering these events.



So basically everyone scurried around and stepped away from what was essentially a collision of sort of best attempts and good intentions in order to solve this problem.  So what I learned, which I never knew before and was an interesting tip, was it's possible to ask Firefox to use the Windows trust store.  If you go to, in Firefox, the standard "about:config," as we know, when you put in "about:config" you get this massive list of an incredible number of settings.  But so then you need to use the search in order to whittle this down.  If you just put in the phrase "enterprise," like the starship, what you will find is one item:  security.enterprise_roots.enabled.  That will normally be set to its default of false.  If you double-click on it, that flips it over to true, and that causes Firefox to then use the built-in Windows security root, rather than the one that it comes prepackaged with.  And then AV tools, which are installing root certs in the Windows store, will also be trusted by Firefox and solve this problem.



So on the 'Net the coverage of this has had people disabling, web scanning, uninstalling their AVs and so forth.  But I know that people who are using a third-party AV are doing so because they want to.  And people who are using Firefox are doing so because they want to.  Switching over to the Windows root store allows you to keep everything on and running and not have to back off of the AV that you want to use, still be able to filter HTTPS and not have any spurious warnings.  And Firefox has said that they're going to be continuing to move in this direction.  I don't know yet what they're planning for 66.  This is 65 that I've just been talking about.  66 says that there will be some sort of MITM, some sort of man-in-the-middle warning.  They are wanting to alert people when there are non-authentic certificates being used.



And here's the problem is that malware does this.  There is malware which, as part of its installation, is installing its own root into the Windows store in order to intercept and there do bad things with your traffic.  And the problem is that AV wants to do exactly the same thing for a benign purpose.  So I don't know how you solve this problem.  The other thing is you can't pre-warn Firefox, for example, with the formal certificate for the AV tool because in order to prevent exploitation, the installation of the antivirus has to generate a unique certificate just for that installation.



We've talked about this before.  For example, Lenovo has made the mistake of having one certificate which they use globally.  Well, that's a problem because the Lenovo system had the private key corresponding to the public key, which they stuck in the root store.  So that allowed anybody that got a hold of the private key, and it was easy because it was right there in the Lenovo app that was running on your system, was able to produce certificates that all Lenovo installations would trust.  So that's why the only way to do this safely for an AV tool is for them to mint their own certificate pair just for that installation so that only that particular installation of the AV tool has the right to sign certificates for that computer, and it's not globally trusted.



So for that reason, it's not possible to tell Firefox that, oh, don't worry, trust anything that, for example, Avast or AVG does.  So this is sort of a collision of - a collision that it's not clear how we're going to solve because I guess the anti - I just can't see how that could work.  Maybe the AV tool could install a root, and then it signs a certificate per installation, and Firefox trusts a certificate on the chain.  Anyway, I haven't thought it all through.  But it does, for the time being, switching to the Windows root store solves the problem.  Firefox will be happy.  You can use Avast and AVG and the others.  And it's not clear how moving forward we solve the problem of trusting some behavior, but not the other, without causing false positives that would scare users.  Certainly everybody's heart's in the right place, but there's just sort of a fundamental collision of technology.



Ubiquiti routers are again in trouble.  Jim Troutman, who's the co-founder of an Internet exchange point NNENIX, which is the Northern New England Neutral Internet Exchange, he tweeted last Tuesday morning, when we were doing the previous podcast, he said:  "Heads up.  Ubiquiti network devices are being remotely exploited via port 10001 discovery service."  So it's 10001 discovery service.  "It results in loss of device management, also being used as a weak UDP DDoS amplification attack."



He wrote:  "56 bytes in, 206 bytes out."  And he followed that tweet up saying that "Attackers are sending small packets of 56 bytes to port 10001 on Ubiquiti devices, which are reflecting and relaying the packets to a target's IP address amplified to a size of 206 bytes," which gives them an amplification factor of 3.67.  So not a huge amplification factor.  But still, if nothing else, it allows an attacker to obscure their IP address by spoofing the source IP and bouncing packets off Ubiquiti devices.



And it turns out it's all Ubiquiti devices.  I saw the list of the devices that were impacted, and it just looks like this is a standard feature of all Ubiquiti firmware is to be deliberately discoverable with an unauthenticated UDP packet on port 10001.  And of course we know that this is a - we've talked about UDP reflection attacks in the past.  This discovery service is deliberately designed to be lightweight, so it uses UDP.



And as we know, UDP is lightweight specifically because it does no connection setup.  There's no handshaking back and forth with ACK and SYN, SYN/ACK, and ACK packets.  It's just a simple UDP query that generates a reply.  Well, that means that there's no way to verify the sender's source IP.  The destination packet is sent blindly back to the source IP, which allows an attacker who's able to spoof their source IP and send traffic out of their network to bounce traffic off of those devices.



So the guys at Rapid7 picked up on this tweet and did some quick sleuthing to figure out what was going on.  So what they found was a little bit daunting.  They said that they - so they used a 4-byte packet and suggested that this meant an amplification attack of between 30 to 35 times.  But I think they may not have been factoring in the per-packet UDP overhead.  So of course you're not actually able to send 4 bytes.  You've got to wrap that in a UDP header and then the IP header, which all expands the size so that the effect of amplification is much lower than that because you need to wrap it in those things in order for it to go anywhere.



Anyway, what they found was that their scan of the Internet discovered - get this - 498,624 unique IPs answering with port 10001 on UDP open.  Of those, almost all of them, 487,021 unique IPs were confirmed to be answering this discovery protocol.  And of those, again, almost all of them, 486,388 unique physical devices based on the MAC address tuples which were returned in this discovery protocol, were found in the responses.  So we have just shy of half a million confirmed Ubiquiti devices wide open and ready to reflect and amplify, albeit weakly, like almost by a factor of four, a bandwidth flooding attack.



And also, as we have seen before with Ubiquiti devices, most of them are located in Brazil.  There's the NanoStation, which had 172,563; the AirGrid was next largest with 131,575.  Then it dropped to the LiteBeam with 43,673, the PowerBeam with 40,092, the NanoBeam with 21,360, and the NanoBridge with 20,440.  And then it continued dropping, diminishing across the entire Ubiquiti family.  And what was of a little somewhat more concern was that this discovery protocol was actually returning a wealth of information.  What Rapid7 found was that a number of devices had, in addition to just being probed, they'd been hacked:  9,146 of them returned the device name HACKED-ROUTER-HELP-SOS-HAD-DUPE-PASSWORD.



LEO:  [Laughing]



STEVE:  Uh-huh.  Almost 4,000 said HACKED-ROUTER-HELP-SOS-WAS-MFWORM-INFECTED.



LEO:  So who's putting these titles in?  The hackers?



STEVE:  Yeah, yeah.



LEO:  Just to be, I mean, doesn't that give them away?



STEVE:  Or maybe good guys.



LEO:  Yeah, maybe like the Japanese government.  I don't know.



STEVE:  Yeah, that one said HELP-SOS-WAS-MFWORM-INFECTED, meaning we disinfected you, 4,000 routers.  And now we just thought we'd leave a little breadcrumb behind in case you check your device name and realize, oh, wait.  1628 say HACKED-ROUTER-HELP-SOS-DEFAULT PASSWORD.  Whoops.  1,168 HACKED-ROUTER-HELP-SOS-VULN-EDB-39701.  So apparently a vulnerability number.



LEO:  This sounds like an automated worm doing this; right? 



STEVE:  Yeah, it sounds like maybe somebody did turn something loose and fixed these things.  And a little over a thousand said HACKED-ROUTER-HELP-SOS-HAD-DEFAULT-PASSWORD.  Slightly different than DEFAULT-PASSWORD.  So but all of them start off saying HACKED-ROUTER-HELP-SOS.  So probably one person who was going around finding and fixing these things and changing the device name in the process to say, you might want to give this a little attention.  So anyway, this led Rapid7 to conclude that the attackers had also identified additional vulnerabilities, although actually we're now thinking.  And I think you're right, Leo, these are not hackers.  This is a white hat who was fixing these things and leaving some evidence behind.



So if anyone is interested, this would be a good time.  Ubiquiti has said that they are looking into updating their firmware with a fix for this.



LEO:  Yeah, I'd look into it, too, if I were you.



STEVE:  Yeah.



LEO:  Yeah, let's look into that.



STEVE:  There is a command line, I've got it in the show notes here; and a link to it, also in the show notes.  You can, if you get into your Ubiquiti management console, you use the command "configure" that puts it into configuration mode.  So then the prompt changes to having a "#."  Then you say "set service ubnt-discover disable."  And, like, yes, do that.



LEO:  Yeah.



STEVE:  Then you say "commit," C-O-M-M-I-T, which then writes it into the configuration.  And then you want to back out of configure mode.  There's no reason for this to be on.  You are painting a flag on yourself.  You've saying, "Hey," you know, "I got a Ubiquiti router."  And a probe of that 10001 port will tell the attacker all about your device.  So that's not something you want to be advertising.  That's just not good.



So to all listeners, I mean, I have recommended the Ubiquiti EdgeRouter X, which was a stunning little powerhouse for $49 back in the day.  I still think it's an amazing device because what was unique about it was that the ports on it, I think it was a five-port device, every one of them was a separate interface, rather than being a switch, which allowed you go configure it and set it up for really good interport isolation to create very well secured domains using a $49 device.  I mean, it's an amazing little tool.



Unfortunately, they left this discover service on, on all of them.  So the takeaway is, by all means, update your firmware now, and again if they don't have a fix for this yet.  But in the meantime, disable this Ubiquiti discover service.  You don't want to be sending - oh, in fact, no, shoot, it's not.  I was going to say you could use ShieldsUP! to check you, except that that's UDP, and ShieldsUP! is a TCP scanner with the exception of the UPnP service, that port 1900 that I deliberately created for that purpose.  And I just don't have time right now to add a UDP test for 10001.  So anyway, you know who you are.  If you have a Ubiquiti router, I would take some time to turn that service off because you don't want to be easily discoverable.



LEO:  And just to reiterate, updating the firmware will not fix at this point because they haven't updated the firmware.  So you've got to turn it off.



STEVE:  Yes, yes, you want to turn it off.  Just in general.  I mean, you know, we've talked about being stealth.  It's just a good thing.  And here's something that's not just responding to a ping, it's saying here's all this information about me.



LEO:  Here I am.  What would you like to know?



STEVE:  Please, please discover me.  It's like, uh, no.



LEO:  We discovered you.



STEVE:  Bad idea.  So, okay.  Before our final break I want to talk about a very encouraging development on the Chrome side.  Google is working on something that they're calling Never-Slow Mode for faster browsing.  Bleeping Computer put me onto this interesting experiment one of the Chrome developers is working on.  He calls it Never-Slow Mode.  And the idea is that if, while a web page is loading, something is taking too long, or something is too large, it will be abandoned and ignored in favor of getting the page loaded.  I've got a link to his description in the - it's under Review for the Chromium project.  And so I'm going to share what he posted.



He said:  "Never-Slow Mode."  He says:  "Prototype - Do Not Commit," meaning I'm not suggesting that we push this out in any channel right now.  But he said it adds the enable-features=NeverSlowMode flag to enforce per-interaction budgets designed to keep the main thread clean.  Now, the main thread is that thing, it's the main thread which downloads the content.  And he says:  "Currently blocks large scripts, sets budgets for certain resource types (scripts, fonts, CSS [style sheets], images), turns off document.write, clobbers sync XHR" - that's the AJAX queries - "enables client-hints pervasively, and buffers resources without Content-Length set."



Now, that means if the resource - normally a resource that you're requesting returns a Content-Length tag, or a Content-Length header so the requestor can immediately see the size of this.  So he says it buffers resources without the Content-Length set, meaning it will get them, but it's going to kind of hold them off to the side until it can see how large they are, rather than holding up the page.



He says:  "Budgets are reset on interaction," meaning so this is a non-interaction, click to load the page.  As soon as you click or tap or scroll or do anything interactive, then the budget's reset.  So he says:  "Long script tasks greater than 200ms," so that's a fifth of a second, "pause all page execution until the next interaction."  So, like, for example, that would instantly stall anything trying to mine cryptocurrency on your page.  It would just shut down.



So he says these caps, as in caps, budget caps, do not apply to workers, meaning worker threads; and size caps are lifted for resources loaded from Service Worker Cache Storage, meaning things that are happening asynchronously off on the side, they're allowed to continue.  This is just like the main "show me the page," like the experiential thread.  So he says current caps, all values are wire, for example, transfer/compressed size.  So this is the actual over-the-wire compressed size, not the expanded size.  Which is to say the size that you feel because it's taking time to get into your machine.  So he has the per-image maximum size set to a megabyte.  Or is that a megabit?  I don't know.  M-I-B, lowercase I?  Anyway, good question.  I didn't think to wonder before.  Could be a megabit or a megabyte.  



LEO:  It's capital B.  I've never seen it written with the M-I like that.



STEVE:  Yeah, it's kind of odd.



LEO:  That must be some sort of European thing.



STEVE:  Could be.  So but what's interesting is the total image size, well, we'll call it a byte, a megabyte.  Total image budget, 2 megabytes.  So there's both a per and a total.  Per-style sheet max size, 100K.  Total style sheet budget, 200K.  Per-script maximum size 50K.  Total script budget 500K.  Per-font max size, 100K.  Total font budget is 100K.  Total connection limit is 10.  Long-task limit, as I mentioned, is 200ms.



So then he said, under TODO he said:  "<iframe> depth is not yet limited."  Oh, that's interesting, so nested iframes.  He said:  "And font-loading is not yet greedy.  Feature-policy header to trigger on per-page basis is not implemented.  No UI is implemented to inform users that a page is slow."  So, okay.  So I'm not suggesting that this is going to be like an immediate win.  And I would agree with anybody who thought that it seemed fraught with peril.



But I really do like the underlying incentive behind this because today there is virtually zero pushback against sloppy coding, lazy image sizing, massive code libraries being downloaded just so that a single function can be accessed.  I've seen that so many times, and it's just insane.  Some sloppy web coder wants a particular function in some library.  The library is massive, and it's all downloaded so one function can be called because the guy's too lazy to code something simple themselves.  It's just, ugh.  Anyway, that's what's happened to the web now.  And these guys don't care because they're busy, and no one is making them care.  So they just let the page's consumer pay in bandwidth and time.



So the idea that the number one web browser in the world might start actively pushing back and is experimenting with this against needlessly slow page loads, I mean, again, this could be fixed.  Images could be given more care, and compressed.  Code could be looked at to say, wait a minute, why is it so big?  Anyway, I just think - I say bravo to this.  I hope that the experiment is - we know that Google cares.  Google has done things like this about being a little bit police-y about caring about the page load time and the experience that their users have.  So I just wanted to sort of put this on everyone's radar.  We'll see where this goes.  It would be nice, I think, if it had some future.



LEO:  Steven "Tiberius" Gibson.



STEVE:  If we say that often enough, my Wikipedia page is going to get changed.



LEO:  Well, careful what you say there.  Go ahead, Steve.



STEVE:  So Firefox is finally catching up with Chrome, Opera, and Microsoft Edge in a nice way.  It turns out that web browser extensions often track their users.  They employ tracking scripts like Google Analytics, or redirects through servers that are used to track a user's browsing and search behavior.  Since this is not the behavior that users typically want when they are switching into incognito mode, Chrome, Opera, and Edge have already stopped running browser extensions by default when the user switches into their whatever the particular browser calls it, their please don't remember anything I do while I'm in here mode.



The good news is that Firefox will similarly begin blocking extensions.  Users will be notified that extensions are not running when they switch into incognito.  And they'll have the ability to selectively reenable those, perhaps that they need, like LastPass or SQRL, that they trust and wish to be able to continue using while they're in that mode.  This new feature has landed in Firefox's nightly builds.  So those on the nightly, if anyone's doing Firefox Nightly, you can see it.  And it was expected to move into production builds once any bugs had been worked out.  But this one is noncontroversial.  It's like, yeah.  We should be doing that, too.  So Firefox is going to get that.



Speaking of getting that, Linux is getting, the Linux kernel is getting a few more options.  It turns out the Spectre and Meltdown mitigations have been more expensive than Intel said.  Who would have ever thought that, Leo?



LEO:  What did they say, just out of curiosity?  Yeah, it cost us a couple of bucks.



STEVE:  No, no.  Remember, no, expensive in terms of performance.



LEO:  Oh, oh, yeah.



STEVE:  Remember, it's like, oh, well, you know, you're going to have to turn off - half of the chip is spent in predicting stuff.  But don't worry, turn that off, and it won't hurt you that much.



LEO:  Not at all.  Not at all.



STEVE:  Yeah, yeah.  Wait, wait, wait.  You're saying that you engineered half of the chip just to make this faster, but it doesn't really make it any faster, so you don't mind if we turn it off?  Anyway, yeah.  ZDNet wrote, they started their coverage saying:  "Believe it or not, the mitigations for the Spectre class of CPU vulnerabilities are now some of the biggest enemies of sysadmins."  And of course this follows what we've been saying for some time, that not only did Intel understandably significantly minimize the performance hit that disabling these powerful enhancements would cause, but there's never been an attack found in the wild.  So I'm going to paraphrase what ZD covered for brevity.



They said:  "Despite being security-focused patches, these mitigations are known to introduce huge performance hits to Linux systems.  A recent benchmark showed that one of the many Spectre mitigations, the one named Single Thread Indirect Branch Predictors (STIBP), introduced" - get this, Leo - "a 30% performance dip for PHP servers, which caused system administrators to reconsider applying some of these patches.  Despite being more than a year old, the Meltdown and Spectre vulnerabilities have remained" - as we know - "a purely theoretical threat.  No malware strain or threat actor has ever used" - as far as we know - "any in a real-world attack."  It's all theoretical for the last year plus.  "Consequently, during the past year, system and network admins have called on the Linux project for options to disable these protections.



"Many argued that the threat is theoretical and could easily be mitigated with proper perimeter defenses in some scenarios.  Even Linus Torvalds has called for a slowdown in the deployment of some performance-hitting Spectre mitigations.  The Linux kernel team has reacted favorably toward these requests and has been slowly adding controls to disable some of the more problematic mitigations.



"For example, since Linux Kernel 4.15, administrators can disable the kernel's built-in mitigations for the Spectre v2 vulnerability (CVE-2017-5715) with the 'nospectre_v2' kernel command line parameter.  Since Linux 4.17, admins have been empowered to disable all mitigations for Spectre v4 (CVE-2018-3639) with the 'nospec_store_bypass_disable' command line parameter.  Similarly, a way to disable mitigations for Spectre v1 has been added in the Linux Kernel 4.19, with the addition of the 'nospectre_v1' parameter.  These three parameters were added even though the kernel already had existing 'spectre_v2' and 'spec_store_bypass_disable' options for months, which allowed system admins to control the complexity level of the Spectre-class mitigations, which also included an 'off' mode."



But sysadmins wanted some way, of course, those are kernel build things; right?  So it's like, wait a minute, are these really off?  So sysadmins wanted some way to absolutely, positively guarantee that Spectre mitigations would not somehow kick in at all, ever, no matter what.  So now the most recent kernel releases have these three newer parameters which can be used.



"The latest effort to have these mitigations turned off and stay off is the addition of the PR_SPEC_DISABLE_NOEXEC control bit to the Linux kernel.  This bit prevents child processes from starting in a state where the protections for Spectre v4" - that's the one that's really expensive in performance - "are still activated, despite being deactivated in the parent process."  So experts argue that some processes, and I'm among those who argue - that some processes don't need Spectre protections, and the performance impact far outweighs the security impact, especially in closed systems where malicious code cannot be introduced, such as graphics rendering farms, off-the-grid supercomputers, or other strictly confined systems where no third-party code is ever run.



And I think that's exactly right.  As we've noted before here, the only real even theoretically practical threat is shared hosting systems where potentially malicious code might be running in an adjacent virtual machine.  Things like Linux-based routers and many other server scenarios don't have that kind of exposure to possibly hostile code.  And we've argued, and I do, that even an end user system, if you've got something in your machine which is using Spectre to try to create a breach, then it's dumb because it's already in your machine.  So, I mean, there are much easier ways to gain a foothold than something as really unlikely to be feasibly exploited as Spectre.  So I think this makes a lot of sense, and I was glad to see this.



I have a piece of errata that I wanted to share relating to last week's Cisco Small Business, the RV320 and 325 Dual Gigabit WAN VPN Routers, where I told everybody, hit pause right now if you have one of these because Cisco, remember, left default logins in this router.  A listener, Bryan, wrote:  "Just finished SN-699, where Steve tells everyone to pause the podcast, unplug your RV320, update it, then come back when that's done."



Anyway, he says:  "The TLDR is, according to Cisco, Vulnerable Products says this vulnerability affects Cisco Small Business RV320 and RV325 Routers running firmware releases 1.4.2.15 through 1.4.2.19.  Products confirmed not vulnerable:  'Only products listed in the Vulnerable Products section of this advisory are known to be affected by this vulnerability.'"  So he says that's from the security advisory for the command injection vulnerability.  The information disclosure vulnerability only affects 1.4.2.15 through 1.4.2.17, so a subset.



He said:  "I have family ties to a local business that's running an RV320, so as soon as I got home this evening I contacted them to begin planning for an ASAP update.  Once I got logged into their router, saw their firmware version, and read Cisco's security advisory notes, I relaxed a bit.  They are running an older firmware version, not one that's affected."



LEO:  Oh, that's interesting.  It's one of the ones in the middle.



STEVE:  Yes, exactly.  He said:  "For what it's worth, firmware version 1.4.2.15 appears to be the first 1.4.x release, which dropped 15-Sept-2017."  Okay.  So that means that since then things have been vulnerable.  And he said:  "And version 1.4.2.19 was released 29-Apr-2018.  So this has been hanging out there for about 16 months.  Not great, but much better than five-plus years."



He says:  "That makes sense to me.  Steve mentioned on the podcast that these were very popular enterprise and SMB firewalls, but also mentioned that the number of exploitable devices found via scans was somewhere in the neighborhood of 9,000.  That number seemed low to me," he wrote, "for such an ostensibly popular firewall.  But if, apparently, only the last three firmware releases before the newest, just-released 1.4.2.20 are subject to these attacks, that winnows down the field considerably.  For what it's worth, just in case anyone else starting running around with their hair on fire about this one."



So Bryan, thank you very much for the clarification.  And I imagine by now any of our listeners who may have been affected probably updated to the latest, and that's probably good, too.  So nice to know that if you had not been keeping yourself current for a couple years...



LEO:  And really not been keeping - you're good, yeah.



STEVE:  Yeah, it's probably a little bit better, yeah.  But I wanted to share a couple heartwarming, not about SpinRite this time, about SQRL.  Oh, first of all, I've been avoiding using the name of our illustrious XenForo assistant coder since I didn't know whether or not he wanted to remain anonymous.  But I've been so impressed with his work that I asked Rasmus if he would mind that I let the world know that his name is Rasmus Vind.  He has a site, HiveWorkshop.com, which is a XenForo-driven forum.  And he's as in love with XenForo as forum software as I am.  So it was nice to see because he is a serious web developer who clearly knows his way around PHP, JavaScript, HTML, CSS, and all the contemporary web tools of the day.



And I'm so glad that he happened upon a question that I had posted over in the XenForo developer site back in May when I was just, you know, I had the forums up, and I thought, okay, I cannot have SQRL forums where you can't use SQRL to log in.  That's just, you know, I can't have that.  So he of course raised his hand and said, "Hey, I know XenForo development. Can I help?"  And of course what I did then was, so that he didn't have to implement SQRL on the server side in PHP, I instead created the API which makes adding SQRL support to anything pretty easy.  But you still have to know the environment where you are.  And since he's a PHP XenForo guy, I said by all mean, be my guest.  So Rasmus, an official shout-out and thank you at HiveWorkshop.com.  I found a posting - oh, and there it is, yeah.



LEO:  And so it has SQRL on it now?



STEVE:  Not yet.  His doesn't.



LEO:  Yours does.



STEVE:  But mine does.  And so, for example, Archimedes, someone named Archimedes posted on Thursday at 6:48 p.m., he said:  "I've been lurking around the newsgroup for some time now, watching the development of SQRL with great anticipation.  Seeing the news today that the forums were SQRL-ready, I figured I'd go ahead and take the plunge.  What an incredibly seamless experience."  He wrote:  "Downloaded Steve's reference client and created an identity in less than five minutes," he said, "and that's only because I read everything slowly."



He said:  "I purposefully didn't even install SQRL," meaning you don't have to install it, you can just run it.  He said:  "And yet I was able to easily get going with the forums in Chrome.  Logout and login is a snap.  Incredible work, fantastic result.  As a professional developer in an American corporate world, I really wish I had more time to help actively.  But I'll definitely cheer on from the sidelines.  Archimedes."



BrianOfLondon wrote.  He posted:  "All done.  There's probably an orphan account somewhere called brianoflondon_s that you can delete if you want.  I've now associated this account with my SQRL ID which I'm using on my MacBook via Wine and on iPhone via Jeff's client."  That's Jeff Arthur in the U.K.  He said:  "Everything is working."  Then he said:  "WordPress...  As soon as that plug-in is ready, boy do I want to try it out on various WordPress sites."



And then Jason L. said:  "After I got locked out of my previous account (Jason), due to losing SQRL association and stupidly not creating a password or entering an email address, I decided it would probably be easier just to create a new account.  This time I used a password and email address.  I then associated this account with my SQRL ID.  It works.  I have logged in several times using SQRL.  I have even been able to use Jeff Arthur's iOS client to have my computer log into to the site."  Meaning using his phone.  He said:  "I think that is the coolest thing.  I can't wait to demonstrate this to my friends and family on their computers.  I won't even touch their computers.  I will just have them type the URL of these forums.  They will think it's some sort of magic trick, or I hacked them, LOL."



He said:  "I am assuming once SQRL is stable here and there's no danger of losing associations again, there will be a way to remove password and/or email address since the whole point of SQRL is that websites will not have secrets to keep safe.  I realize that sites will probably have email addresses to store, but those aren't really secret, and odds are they have already been stolen anyway."  He said:  "I know there's a feature in the client to only use SQRL to login, but figured that feature is not functional yet, and I am not ready to do that until I know it is safe to do so."



So anyway, just a note to our patient listeners that we are just about there.  What Jason's referring to is that Rasmus and I had a miscommunication.  He was using the API in a form where the identity associations were temporary because he wasn't informing the API that he needed to make it permanent.  And every night at midnight I have a sweeper that goes through and cleans up any usage of SQRL that didn't result in a permanent use of it on the website where it's being used.  And as a consequence, people had initially created their SQRL associations to accounts they already had, and SQRL no longer worked the next day.  So anyway, that was just, whoops.  It was lack of my clear documentation, actually.  So that's now clearly documented, and that's been fixed.



And then of course also people were having fun with the fact that you don't need to tell the forum anything about yourself.  You'd have to have a name so that, if you post something, there's a handle there.  But not even an email address if you would choose not to have notifications, not to receive notifications from the forum.  So anyway, we're getting very close.  I have some documentation to catch up on.  And then we will get this thing officially released.  And the various add-on guys are also working on stuff.  Jeff is working on iOS.  The Android client is running.  And we have, I mentioned, an extension for Chrome.  Actually he's developing it under Firefox, and it also runs under Chrome since they have a common extension API.  So I am very excited to be actually getting back to SpinRite soon.



Which brings me to a SpinRite question from Patrick McAuley in - and I don't know how to pronounce this, Leo.  G-U-E-L-P-H?



LEO:  Guelph; right?



STEVE:  Guelph?



LEO:  In Ontario.  Guelph, yeah.  We have an actual Ontarian in the studio, and he's saying it's Guelph.



STEVE:  Yes, Guelph, Ontario.  So he just said...



LEO:  Clark says yup.



STEVE:  Guelph, Guelph, not a typo.  Anyway, simple question.  He asks:  "Will SpinRite work on an external USB-connected drive?"  He said:  "Any PC setup drive changes needed?"  And the answer is it will work, and no changes are needed.  In the early days of USB on motherboards, it was a little questionable because not all BIOSes supported USB drives.  Now USB support is in the BIOS.  And so the only trick is you want to have the drive connected when you start SpinRite up, meaning when you boot the system, so the BIOS can see the drive and then include it in its list of drives for SpinRite to run on.  That's the only requirement.  And so long as you do that you are good to go.



And I did see actually a question I referred to at the top of the show about some follow-up on OpenVPN, which actually ties into one of the sponsors of the TWiT network and my comment about why would anybody not run the OpenVPN client for Android, that we were talking about a couple weeks ago.  It was two weeks ago.  Matt in Providence, and I assume that's Rhode Island, he was looking at what he called a stealth VPN to defeat VPN-blocking WiFi.  And so I assume he's talking about WiFi that blocks IPSEC, which is not uncommon; whereas SSL-based VPNs are able to get through that typical blocking.  So, and as a matter of fact our sponsor, a sponsor on the TWiT network, NordVPN, offers that, but I'll get there in a second. 



So he said:  "Please add a show topic for this."  He says:  "I have a need for VPN obfuscation/stealth VPN when using certain public WiFi networks.  In the name of security," he says, "forget the terms of service."  Presumably the network says you shouldn't use a VPN on here.  He says:  "I found a service named TorGuard for around $5 per month."  He said:  "I'm also interested to know whether you can host your own VPN server on AWS that can act like a stealth VPN, or if running OpenWRT VPN on my home Linksys WRT router can act as a stealth VPN.  By the way, I'm a proud licensee of SpinRite, and it fixed a boot issue for me on a computer where I wasn't running it preemptively to prevent problems.  Great product.  Thanks."



Okay.  So I already gave the disclaimer that NordVPN is a sponsor. 



LEO:  No, they're not, but okay.  ExpressVPN is our sponsor.



STEVE:  You told me two weeks ago they were.  Were they?



LEO:  ExpressVPN.  They might have been, yeah, but not now.  ExpressVPN is.



STEVE:  Okay.



LEO:  Don't get confused.  It's important.



STEVE:  Sorry.  I'm glad to be straightened out.  So we were talking about OpenVPN.  Typically these VPN services are using OpenVPN or offer it as an option.  So you are able to configure your use of an OpenVPN client for use with whatever service you like.  And I got completely thrown off by this confusion of providers.  I'm sorry, Leo.



LEO:  I believe Express does stealth, as well, just so you know.



STEVE:  Yes, yeah, exactly.  But I did want to mention that the OpenWRT is only an OpenVPN client.  As far as I could tell, it will not do OpenVPN serving.  So you probably still do need - you need something that can be an OpenVPN server.  For example, like pfSense, you could run that, and that would do it.  Or just use a service which, as we know, are not very expensive and will allow you to connect wherever you hope to be.  So I wanted to provide a little bit of clarification to Matt in Providence for that.  And that's our show.



LEO:  Well, thank you very much.  And for further information, TWiT.tv/sponsors is a list of all the people who pay good money to be on this show.  No, and there's nothing wrong with NordVPN.  I think at one point they were briefly a sponsor.



STEVE:  I will check that next time.



LEO:  But our current VPN sponsor is the number one VPN, ExpressVPN.  And, yeah, that's exactly the point.  I mean, if you don't mind appearing to be coming from your house, perfectly fine, which I don't think you do.  But if you'd like to appear somewhere else...



STEVE:  Like in the cloud, baby.



LEO:  In the cloud, or somewhere not on prem, or not even in your country, then you might need some help from a friend.  Steve Gibson is a good friend.  Thank goodness we've got him for 299 more shows, and that's all.



STEVE:  I think that'll be enough, but okay.



LEO:  You think.  We'll both be a little older and a little grayer by then.  Maybe, what's that, five years?



STEVE:  But no wiser.  I think we've topped out on wisdom, Leo.



LEO:  I think wisdom's done.  I think it's down - I don't know about you.  I can't speak for you.  You seem to be getting smarter.  I'm definitely going downhill.  If you go to GRC.com, that's Steve's site, the Gibson Research Corporation.  He doesn't sell giant coffee mugs, although I'm telling you, you'd find a market.  You'd find a market for head-sized coffee cups, screen-sized.  Geez, Louise.  Now I want coffee, inexplicably.



You will find lots of great stuff:  SQRL; SpinRite, which is his bread and butter, the world's finest hard drive recovery and maintenance utility; and this show, fueled by caffeine, heavily fueled by caffeine.  He has an audio version, and he has really nicely done transcripts.  Elaine Farris, our transcriber, writes it all down so you can read along as you listen, which for a lot of people is very helpful.  GRC.com.  We have audio and video, oddly enough, at our site, TWiT.tv/sn.



STEVE:  Leo.  Yes, sorry.



LEO:  No one knows.  No one knows.  But if you want to see Steve's giant mug, there's no better place to do it.



STEVE:  The mug or the mug.



LEO:  Be a good product, Steve's Giant Mug.  All you have to do is go to your favorite client and subscribe, and that way you'll get every copy automatically, the minute it's available, audio or video.  Or go to TWiT.tv/sn for Security Now!  Steve, have a great week.  See you next time on Security Now!.



STEVE:  Thank you, my friend, for 701.



LEO:  Wow.



STEVE:  Bye.  



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#701

DATE:		February 12, 2019

TITLE:		Adiantum

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-701.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm 



DESCRIPTION:  This week we look at Apple's most recent v12.1.4 iOS update and the two zero-day vulnerabilities it closed, as well as examine the very worrisome new Android image display vulnerability.  We dive into an interesting "reverse RDP" attack, look at the new LibreOffice and OpenOffice vulnerability, and consider Microsoft's research into the primary source of software vulnerabilities.  Mary Jo gets an early peek at enterprise pricing for extending Windows 7 support.  China and Russia continue their work to take control of their countries' Internets.  Firefox resumes rollout of its AV-warning Release 65.  We offer up a few more SQRL anecdotes, share a bit of listener feedback, then see how Google does the ChaCha with their new "Adiantum" ultra-high-performance cryptographic cipher.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Coming up we'll look at the iOS zero-days and the patch to fix them; an issue with RDP that Microsoft says it's not a high priority to fix that; and issues with OpenOffice, LibreOffice, and Russia's plans to disconnect from the Internet.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 701, recorded Tuesday, February 12th, 2019:  Adiantum.



It's time for Security Now!, the show where we cover your security and privacy online with the commander in chief of the privacy brigade, Mr. Steven Gibson.



STEVE GIBSON:  The privacy - every week it changes, Leo.  I never know what you're going to come up with, but it's always fun.



LEO:  You should see the uniform.  It's all lead foil.  It's fabulous.  Fabulous.  



STEVE:  Yeah, yeah.  A little heavy to wear if you get the thick version of that.



LEO:  I was reading that tinfoil actually conducts and probably is not ideal for a hat.



STEVE:  Well, as long as you didn't have any open seams it'd be essentially a Faraday cage, so nothing could get in.  But if it's just a little cap on top, that's just not going to do the job.  That's not going to happen.



LEO:  That's why I said the uniforms, the entire uniform.



STEVE:  So this episode is titled "Adiantum," which is a new term that Google has coined.  And it allows me to use the phrase "Google does the ChaCha," for reasons we will be getting to.  Some very, very cool technology.  And as we know, oftentimes among the news of the week nothing really stands out.  This time, last Thursday Google announced something which is significant for our industry.  So Adiantum is its name.



But first we're going to look at Apple's most recent 12.1.4 iOS update and the two zero-day vulnerabilities it closed, in addition to fixing the FaceTime problem.  Also a very worrisome new Android image display vulnerability involving PNG files, which make Stagefright probably look like nothing by comparison.  This is not good.  We're also going to do a bit of a dive into the research into a reverse RDP attack.  We've talked about the Remote Desktop Protocol and how bad it is for people to leave their RDP servers exposed to the public Internet.  Well, it turns out you need to trust the server you're connecting to every bit as much.  So that was some really interesting research that Check Point did.



Also there's a problem with LibreOffice and OpenOffice, which one of the two has been fixed.  But until it is, it's raising all kinds of alarm bells.  I wanted to share quickly Mary Jo's early peek at enterprise pricing for extended Windows 7 support which you guys talked about last week on This Week in Windows.  Also China and Russia are both continuing their work to take control of their own countries' Internet.  We'll touch on what that's about.  We've got Firefox's resumption of its AV warning, well, remember that they had a problem we talked about last week because it was causing such problems for AV.  They've resumed releasing Firefox 65 as a consequence of some changes.  I have a few more SQRL anecdotes to share, a bit of listener feedback, and then we're going to see how Google does the ChaCha with their new Adiantum ultra-high-performance cryptographic cipher mode.



LEO:  Oh, sounds cool.



STEVE:  So lots of fun stuff to deal with today.



LEO:  Awesome.  Steve?



STEVE:  Actually, come to think of it, I could have done a different picture.  I should have done the picture of the algorithm for this new cipher, Adiantum.



LEO:  Is it pretty?



STEVE:  Yeah, it is.  And it is in the show notes later on, so we will get to it.



LEO:  We'll get to it, all right.



STEVE:  But anyway, I just liked this.  This was this beautiful piece of work, this ridiculously inexpensive PDP-11/70 kit.  Someone tweeted...



LEO:  This is Oscar's kit?



STEVE:  Yeah, this is Oscar's kit.  And look at the, I mean, he just does just a stunning job.  I mean, that's just the...



LEO:  That's the insides.



STEVE:  Yes, that's the PC board that no one will ever see, which is just, you know, gold-plated and a beautiful legend mask and spaces for all the lights and switches and things.  And he's got light.  You can see up at the top of the picture our light masks that help to align the LEDs so that they line up properly, you don't have to fudge them manually.  And they also provide inter-LED isolation so the light from one doesn't bleed into the other.



LEO:  He's Swiss, isn't he.



STEVE:  Yes.



LEO:  You can kind of tell.



STEVE:  Yeah, yeah.  Anyway, just a beautiful piece of work.  So I wanted to just sort of share that since someone had sent that.  And he did the injection molding to make those funky switch toggles.  The DEC switch handles are this weird triangle with skirts, and they don't exist in the world.  And so he made them so that they fit on top of these beautiful Alcoswitch toggles.  And, I mean, it's just a work of art.



LEO:  What's the address?  Or maybe we shouldn't send people there anymore; right?



STEVE:  We probably should.  I think if you put in, what was it, guaranteed...



LEO:  Obsolescence guaranteed.



STEVE:  There it is, "obsolescence guaranteed" and then "PDP-11," maybe.  Or just "obsolescence guaranteed."  But look at it, I mean, it's just - and so the whole thing is - he did an injection-molded bezel so that you have the - and silk-screened front panel.  I mean, it's only about two or three inches deep because it's based on the Raspberry Pi that is emulating the PDP-11.  And so really all this is, is it's a physical I/O device.  It's switches and lights to emulate the PDP-11.  And he sat down with a friend, videotaping as they, like, toggled switches slowly on an actual PDP-11 to get every light correct.  So it's just, again, yes, his perfectionism is something I completely relate to.  And that's just a beautiful piece of work.  And it's $250 for this amazing kit that does not take a rocket scientist to build.  So I just wanted to show it again.  It's just a beautiful piece of work.



LEO:  I did get on the list, and I got an email from him a little while ago saying fill out the form.  And I've done that, and I'm just waiting for more information.



STEVE:  Yeah, I mean, he's just a hobbyist who's nuts.



LEO:  Yeah, yeah, that's awesome.



STEVE:  You say, well, this turned into a little bit of a career, more than he expected.  It's like, okay, as long as you want to do it.



So I didn't have a single one of my iOS devices prompt me to go get 12.1.4, yet on every one of them, when I went looking, it's like, oh, we have a firmware update for you.  So you may want to get it because, aside from fixing the unattended FaceTime - did I say "Facebook" before?  Probably - FaceTime eavesdropping bug that we talked about either a week or two ago, the one where somebody could add themselves to a group while calling you that would cause your phone to think that the person you're calling had answered so that you'd get this zero notification activation of your microphone and camera.  So Apple responded by quickly shutting down the group servers that took that feature away from FaceTime.



Anyway, 12.1.4 is the fix that we expected to be immediately forthcoming.  But in addition to that, there are three additional flaws which were fixed, two of which are being actively exploited in the wild as zero-day exploits.  Google's Project Zero researchers found those two zero-days, which they privately disclosed to Apple.  And the third one Apple independently discovered and fixed.  The two from Google, there's a memory corruption issue that could allow a malicious application to gain elevated privileges on the vulnerable device, and a different memory corruption issue that could allow malicious application to execute arbitrary code with kernel privileges.



So as always, we never get any further information.  Apple's not saying anything.  Google's not saying anything.  It's just, okay, let's get this fixed.  But that's sort of demonstrates that, as much focus as Apple puts on securing these things, we're still finding problems.



And then the third problem, which is also fixed, was discovered by Apple themselves.  It's a different flaw in FaceTime involving live photos, which they have now foreclosed.  So probably worth updating.  Again, as I said, none of my devices said, hey, we've got an update.  I don't know, like, some of them downloaded them.  Some of them needed to download this update.  Maybe they just wait for you to go looking or to see the little red badge on the settings icon and then go, oh, there's something I should do.  I don't know.  But eventually... 



LEO:  They roll it out bit by bit.  That's the problem.



STEVE:  Yeah.



LEO:  And so there are people vulnerable who - like Lisa this morning said, oh, there's an update, did you know?  And I said, well, yeah, kind of, but...



STEVE:  Exactly, yeah.  And you're right, Leo, the problem is there are just so many of these devices out there that it's going to take a while.  Oh, and let's not forget macOS.  As a consequence of this increasingly shared codebase, it is vulnerable to all of these problems.  So macOS, you want to go to Mojave 10.14.3, which fixes all three of those iOS vulnerabilities which macOS also had.  So update all your Apple stuff.  And a little bit later I'm going to talk about this continuing problem that we have with security and why we just never seem to get this right.



Google announced some things they fixed in their February 2019 updates for Android.  And there were a trio of surprisingly worrisome Android PNG display bugs, image display, which means this is in the PNG interpreter which is invoked when anything on your Android device needs to render a PNG image, which says, what, the web.  Any web page, any ad on a web page, a picture in an email that you receive and open, a PNG received through instant messaging.  So, I mean, it is a big vulnerability because it can execute arbitrary code.



Google, in explaining what they had fixed, now that they have fixed it, said the most severe of these issues is a critical vulnerability in framework that could allow a remote attacker using a specially crafted PNG file to execute arbitrary code within the context of a privileged process, meaning that this thing that's doing the interpretation is privileged.  It's down in the kernel, not up where the user is.  So this is a service being provided by the OS which means, if you have a problem there, the code that gets loose is getting loose with the privileges of the process that was running it, which is the kernel.



So they said:  "The severity assessment" - which was highest.  "The severity assessment is based on the effect that exploiting the vulnerability would possibly have on an affected device, assuming the platform and service mitigations are turned off for development purposes or if successfully bypassed."  They said:  "We have had no reports" - this is important - "no reports of active customer exploitation or abuse of these newly reported issues."  They said:  "Refer to the Android and Google Play Protect mitigations section for details on the Android security platform protections and Google Play Protect, which improve the security of the Android platform."



Under Framework they said:  "The most severe vulnerability in this section could enable a remote attacker using a specially crafted PNG file to execute arbitrary code within the context of a privileged process."



The Hacker News site framed it a bit less clinically, and probably more realistically.  To paraphrase what they wrote, they said:  "Using an Android device?  Beware.  You need to be more cautious when opening an image on your smartphone downloaded from anywhere on the Internet or received through messaging or email apps.  Yes, just viewing an innocuous-looking image" - and of course they showed one of a kitten - "could hack your Android smartphone thanks to three newly discovered critical vulnerabilities that affect millions of devices running recent versions of Google's mobile operating system, ranging from Android 7.0 Nougat to its current Android 9.0 Pie.  Although Google engineers have not revealed any technical details, the updates mention fixing a heap buffer overflow flaw, and also errors in SkPngCodec, and bugs in some components that render PNG images."



So we know that Google, as we talked about last year, Google is working diligently to improve the Android update ecosystem that is working to require manufacturers to be responding to Android patches and to push them within some length of time.  But what has already shipped has already shipped.  And most of Google's efforts are necessarily forward looking.  You know, they can't retroactively amend the contracts that they've had people sign about what they've already done.  So that means that the bad guys now know that there are three vulnerabilities, one of which at least is very bad, allowing arbitrary code execution, and PNG rendering image flaws in Android since v7, which was released on August 22nd of 2016.



So 2.5 years ago this flaw was introduced.  Every version since and every phone sold since has it, until it's updated with these February patches which have just been released and announced by Google.  And of course they made them available to partners, but we also know that doesn't mean a lot at the moment.  So every phone sold in the interim for the last 2.5 years, until and unless patched, will be vulnerable to these flaws.



Those who have been listening to this podcast since the summer of 2015, that is, the year before that, will recall the Android Stagefright exploit which caused quite a bit of activity on both sides of the law, given that this problem only needs to display a PNG of a specially crafted image, and that these fixes are probably reverse-engineerable by the bad guys since Android has the mixed blessing of being way more open than Apple is with their iOS platform.  I have a feeling we haven't seen the end of this.  So it may well be that a month from now we are talking about people receiving just an image on their phone and having their phone commandeered.  So we'll see.



What it means for our listeners is, if you are using Android, hopefully you have a phone from a manufacturer who will make those updates available, maybe directly from Google, maybe through a responsible manufacturer.  Maybe you can go get them.  Whatever it is, it would be good to get them because I will be surprised if the bad guys don't jump on this.  The problem is too many Android devices are either never going to get patched or are going to create a large window until they are patched.  And the idea of just showing somebody a picture is, well, I mean, is really bad.



We recently covered the news of exposed remote desktop servers.  Microsoft's RDP, Remote Desktop Protocol, is what typically Windows servers expose, which allows people with an RDP client to connect in order to get a remote desktop.  And I guess apparently, based on behavior, either people believe it's safe to leave an exposed RDP server unmonitored to the Internet for anyone who wants to, I mean, it's what, port 3389?  Shodan, just do a search, you find all these port 3389s, happily accepting connections.  Hey, I'm RDP.  Come guess my username and password.  You can have a desktop.  Oh, my lord.  Anyway, that's the world as it is now.



And, you know, the new term is "credential stuffing," which is the term that's now in vogue for brute force automated guessing of logon credentials.  We used the term "credential stuffing" last week when we talked about the Japanese government, who was going to be scanning hundreds of thousands of their own citizens' IPs performing credential stuffing attacks on anything they can find, typically IoT things, and see whether they can get in, in which case they're going to work with those people to tighten their security.  So, I mean, I use RDP because I'm a Windows guy.  I've got Windows servers.  You won't find a port 3389 open anywhere because people would be loony, just it's crazy to have a server like that exposed.  But Check Point Research became curious, not about the security of RDP servers, which we already know is frightening, but about the security of the client connected to an RDP server.



The RDP protocol is extremely complex and was developed in the dark as opposed to in the light of day, being as it is a proprietary protocol of Microsoft's.  So of course this hasn't prevented its reverse engineering by others, and several third-party and open source clones exist, client clones exist of RDP clients.  But as we know, a "complex protocol," if you put that in quotes, should give any security-aware person pause since we know that complexity is the enemy of security.  No one should be surprised to learn that the RDP clients are rife with exploitable flaws.



Check Point wrote in their disclosure, they said as an overview:  "Used by thousands of IT professionals and security researchers worldwide, the Remote Desktop Protocol (RDP) is usually considered a safe and trustworthy application to connect to remote computers.  Whether it is used to help those working remotely or to work in a safe VM environment, RDP clients are an invaluable tool.  However, Check Point Research recently discovered multiple critical vulnerabilities in the commonly used Remote Desktop Protocol that would allow a malicious actor to reverse the usual direction of communication and infect the IT professional or security researcher's computer."



In other words, if the RDP server you connect to with your RDP client has been maliciously modified, it can attack your client, your machine from which you are connecting with a vulnerable RDP client - and it turns out they all are - in order to hurt you.  They found 16 major vulnerabilities, and a total of 25 security vulnerabilities overall.



They looked at three clients.  The de facto default Microsoft built-in RDP client is mstsc, which is Microsoft Terminal Services Client is their name.  But there are two others that are popular and mature open source clients.  There's one called FreeRDP, which is the most popular RDP client, on GitHub.  And there's rdesktop, which is an older open source RDP client, which happens, as an interesting twist, to come by default in Kali Linux distros, which is interesting because Kali Linux is what many red teams use as their security platform.  And should they use rdesktop to work to penetrate a remote machine, it could penetrate them instead.



Okay.  So Check Point wrote:  "As is usually the case, we decided to start looking for vulnerabilities in the open source clients.  It seems it will only make sense to start reverse engineering Microsoft's client after we will have" - their English is a little broken, but I'm going to read what they wrote - "after we will have a firm understanding of the protocol.  In addition, if we find common vulnerabilities in the two open source clients, we could check if they also apply to Microsoft's client.  In a recon check, it looked like rdesktop is smaller than FreeRDP, has fewer lines of code, and so we selected it as our first target."



I should explain.  I'm going to go into some detail here because I think our listeners will find the strategy that they adopted and the way they talk about what they found to be interesting, sort of as an inside look into how an organization like Check Point, that is very mature security research, how they tackle something like this.  So first they went for the open protocol, or rather the open source, because that was examinable.  And then they went for the smaller of the two just because there was less to examine, the argument being, if they find problems there, then that would give them a starting point for looking at the closed source Microsoft implementation.



So they said:  "We decided to perform an old-fashioned manual code audit instead of using any fuzzing technique.  The main reasons for this decision were the overhead of writing a dedicated fuzzer for the complex RDP protocol, together with the fact that using AFL" - that's the American Fuzzy Lop is one of the often-used fuzzers - "using AFL for a protocol with several compression and encryption layers didn't look like a good idea."  In other words, fuzzing, as we've talked about before, is the process of just throwing stuff at an API, just giving it a bunch of stuff and see if the system crashes.



LEO:  It's like system events, though, always is like clicks and mouse moves and stuff like that?  Or is it...



STEVE:  Yeah, well...



LEO:  It's mostly system events.



STEVE:  Yeah.  So when you're actually connecting to the server, there is, in the case of RDP, a deep API.



LEO:  Oh, so you have all sorts of garbage over the API.



STEVE:  Yes.  And the problem is so much of it is going to be that - so the idea is the API is so highly structured that meaningful exploits are probably not going to be random.  There are so many ways that the API would reject fuzz that your likelihood of finding something is really low.  That would require that they write a custom fuzzer that is protocol-aware in order not to just be rejected out of hand.  And so they thought, okay, we're not going to write custom fuzzing code.  Let's just take a look at the code.  So they just looked, they just started reading the source.



They first started, as I mentioned, with rdesktop v1.8.3.  They said:  "After a short period, it looked like the decision to manually search for vulnerabilities paid off.  We found several vulnerable patterns" - and I'll explain that in a second - "in the code, making it easier to 'feel' the code and pinpoint the location of possible vulnerabilities."  And this is sort of, it's like when you read code you can get a feel for the personality of the person who wrote it.



I mean, if you looked at my assembler, you would absolutely get a sense for me.  I mean, code is that expressive, where you can see, like, oh, okay, Steve, he's weird.  He'll push some arguments onto the stack because he's going to later need to use them when he calls something.  And rather than popping them off the stack just to then push them on the stack as arguments to the call, he'll prepush them a ways ahead to have them there and then not go through popping and then repushing because he understands what's actually happening in the processor.  And so you'll look at that and go, you know, but I'm careful to comment that because it's like, okay, someone looking at my code will go, wait, why is he pushing these over here?  And so I always comment, as much for myself as for someone else, it's like, these are being pushed because these are arguments that'll be used down lower a ways.  And so I might as well just have them there waiting for that function.



Anyway, the point is code really can reflect the personality, for lack of a better term, of the person writing it.  And what they saw was they saw some habits that this coder had that were bad.  They were bad coding habits.  So not just obscure, which is what you might call mine, but actually dangerous.  So what that did was that allowed them to then look for instances of the use of this bad habit and see if they were located in vulnerable places.  And they found 11 vulnerabilities with a major security impact and 19 vulnerabilities overall in the library, just after getting a sense for the habit of the coder where they saw that, oh, he's doing some things that aren't really that safe, and that caused some problems.



They also noted as an aside that the other end of this rdesktop connection was an xrdp server, which is also open sourced and based on that same rdesktop code, which led them to believe that, well, in fact they did some additional recon.  And based on their findings, it appears that similar vulnerabilities can be found in RDP, as well.  And that's a concern because that is a server as opposed to a client, which might be exposed and now is believed to have some problems.



So anyway, I'm not going to go into detail because I just kind of wanted to give our listeners a sense for the way they operated with this.  In their disclosure, and I've got a link in the show notes - yup, I do, just checked.  In their disclosure, they've got screenshots and details for anyone who's really curious about the nature of the programmer's personality that they found and what it meant.  But to sum it up, they characterized what they found as leading up to, as they put it, massive heap-based buffer overflows.  They wrote:  "By chaining together these two vulnerabilities" - they were sort of related - "each pair found in three different logical communications channels," they wrote, "we now have three remote code execution vulnerabilities" they found in the client.



And then dealing with a different instance of a vulnerability, they wrote, and this one had been given a CVE of 2018-8795, remote code execution, they said:  "Another classic vulnerability is an integer overflow when processing the received bitmap for screen content updates."  Remember that this is remote desktop, so the server is sending the client chunks of its screen, like regions of screen that have changed and need to be updated.  So the server is saying, here's a rectangular bitmap region of the screen with this width, this height and this pixel depth.



So what they noted was, although the bitmap width and height parameters are only 16 bits each, by multiplying them together with the bits per pixel, the pixel depth, it's possible to trigger an integer overflow, meaning that 16-bit value times a 16-bit value times the bits per pixel depth can overflow 32 bits, which is the integer value that was being used here, and lead to badness.  They said:  "Later on, the bitmap decompression will process our input and break on any decompression error, giving us a controllable heap-based buffer overflow."  And they said:  "This calculation can be found in several places throughout the code of rdesktop, so we marked it as a potential vulnerability to check for in FreeRDP," that is, the other open source RDP client.



So then of FreeRDP they wrote - and they tested 2.0.0 RC3, so Release Candidate 3.  They said:  "After finding multiple vulnerabilities in rdesktop, we approached FreeRDP with some trepidation.  Perhaps only rdesktop had vulnerabilities when implementing RDP."  They said:  "We still can't be sure that every implementation of the protocol will be vulnerable.  And indeed, at first glance," they wrote, "the code seemed much better.  There were minimum size checks" - oh, and technically that's what was missing from that code pattern that they kept seeing in the way rdesktop was coded.  They weren't checking for minimal - they weren't doing a sanity check, as I call it in my code, for minimum size.  So "...before parsing the data from the received packet, which was the primary feature missing from rdesktop," they said, "and the code feels more mature."



They wrote:  "It's going to be a challenge.  However, after a deeper examination, we started to find cracks in the code, and eventually we found critical vulnerabilities in this client, as well."  And I should note that here's another perfect example of the fact that, just because the code is open and can be audited doesn't automatically mean it's more secure.  Nobody ever apparently looked before.  So there it is, and it's open, and it's free, and it's full of bugs.  And we've talked about this back and forth, is closed better than open and so forth.  And it's like, yes, open source has the potential to be verifiably secure, but somebody has to go verify it.  Just the fact that it's there doesn't mean that it's any better than something that somebody wrote and didn't publish open.



So they said they found critical vulnerabilities in this client, as well.  They found five vulnerabilities with major security impact and six vulnerabilities overall in the library.  Then some additional sleuthing discovered that the RDP client NeutrinoRDP, which is a fork of an older version of FreeRDP, therefore probably also suffers from the same vulnerabilities.  It exists and probably bears scrutiny, if anyone is using it.



They said:  "At the end of our research we developed a proof-of-concept exploit for this client," which they were able to demonstrate.  And then also there's something they called this "same integer overflow."  They said:  "As we saw earlier in rdesktop, calculating the dimensions of a received bitmap update is susceptible to integer overflows.  And indeed, FreeRDP shares the same vulnerability."  So there was some, you know, even if neither project inherited code from the other, both implementations failed to check for, basically believed the server they were connecting to and were not being defensive about the data coming from the server.  And basically these clients, as we often see, are interpreters.  And so these interpreters, these RDP client interpreters were just taking on faith the safety of what the RDP server was providing and were very vulnerable to that.



So what about Microsoft?  The original RDP client from which these were reverse engineered - since as far as I know Microsoft never published, formally published the protocol.  And of course Microsoft's original RDP client is the one that's built into all of our Windows machines, and it's the one which Windows users will be using.  If you are on Mac or on Linux, then you're probably using one of these other clients if you're connecting with remote desktop to a Windows server.



So these guys wrote:  "We started by testing our proof of concepts" - the ones they had developed, the proof of concept exploits they developed for the other two - "for the vulnerabilities in the open source clients.  Unfortunately" - well, unless you're a Windows user.  They said unfortunately for the success of those proof of concepts.  But they wrote:  "Unfortunately, all of them caused the client to close itself cleanly, without a crash."  So the client just said, okay, I don't know what I've connected to, but this doesn't seem like a valid Windows server.  I'm going to go away now.  So just it shut down without crashing.



Then they said:  "Having no more excuses, we opened IDA" - which of course we've discussed recently also, that's IDA, the Interactive Disassembler.  And they said:  "We started to track the flow of messages.  Soon enough, we realized that Microsoft's implementation is much better than the implementations we tested previously."  They said:  "Actually, it seems like Microsoft's code is better by several orders of magnitude, as it contains several optimization layers for efficient network streaming of the received video; robust input checks; robust decompression checks to guarantee that no byte will be written past the destination buffer; and additional supported clipboard features.  Needless to say, there were checks for integer overflows when processing bitmap updates."



So Microsoft's client is written to protect the system using it to connect to a remote foreign server.  However, Check Point researchers did find some troubling flaws in clipboard sharing.  They go into great detail in their posting, and I didn't want to drag us through that because there isn't much, I mean, there is a lot of technical detail, but nothing of much interest.  But Remote Desktop Protocol supports clipboard sharing.  And it turns out that it's possible for a malicious RDP server with Microsoft's current RDP client to use classic path traversal attacks to place files anywhere on the user's system, limited only by the user's current privileges.



So the RDP client is running in the context of the current user, so that's good.  It's not running with any elevated privileges.  But Microsoft's client is susceptible to a path traversal attack so that, if the server you're connecting to were modified, or the protocol intercepted or somehow manipulated, files could be placed anywhere that you have permission to do so.  And you have permission to put files in your startup folder, which would mean that they would be executed next time you log in or start the system.  So that's not good.



They also discovered that the RDP server, if it were not well behaved, that is, if it were malicious, could dynamically monitor the client's clipboard because there's clipboard sharing which is enabled by default; and that anything that the user even transiently places on their clipboard while you have RDP open, a connection open, the server gets a notification and the contents of the clipboard on the fly.  So, for example, if you just pasted a complex admin password, that goes to the server, even if you don't do anything with it explicitly over the connection, and even if it's only briefly present.



So then in their posting they outlined their disclosure timeline because they did full responsible disclosure for this.  And the short version is the FreeRDP and the rdesktop supporters immediately responded, quickly fixed the problems, asked that Check Point verify the fixes and give them the green light.  They did fix the problems.  Check Point gave them the green light.  And there are now new versions of both FreeRDP and rdesktop.



Microsoft responded:  "Thank you for your submission.  We determined your finding is valid but does not meet our bar for servicing."  And I put in here parenthetically, "(In other words, we have MUCH bigger problems over here.)"  Anyway, they said, so, yeah, we agree, that could happen.  We're not going to fix it.



LEO:  You'd have to log into a malicious RDP server, though.



STEVE:  Correct, correct.



LEO:  So that's probably what they're thinking is, well, it's on you, then, buddy.



STEVE:  Well, and how likely is it, really.  I mean, a Windows user is probably always going to be logging into a Windows RDP server, not something else.  So seems very unlikely that that would be the case.  So out of an abundance of caution, they do note that the clipboard sharing can be disabled.  It's a simple configuration checkbox over on your client, whether you want to share your clipboard with the server.  And so if you have occasion or any reason to mistrust the server you are connecting to, you could disable clipboard sharing.



That's the only problem that they found in the Microsoft RDP client.  And for what it's worth, if you are a Linux or Mac user, it's worth updating.  This all just happened.  So do get the later versions of rdesktop or FreeRDP, if you use those.  Again, in any event, this is a reverse attack against you by a malicious server you connect to.  Interesting from a technical standpoint.  Interesting from a let's take a look at how defects are born.  But not nearly as big a concern, for example, as all the Android devices for the last 2.5 years that can be compromised when displaying a PNG image that they receive from anywhere.  That's one to worry about.



So any users of LibreOffice and Apache's OpenOffice should update.  There was a classic path traversal vulnerability in those.  It's funny how that's always bit us.  The idea of ../../.., it's sort of a clever hack; but, boy, has it caused problems historically.  A security researcher, Alex Infuhr, has discovered a severe remote code execution vulnerability in both of these open source office suites that can be triggered just by opening a maliciously crafted ODT.  That's the OpenDocument Text file format.  The attack relies on exploiting, as I mentioned, a directory traversal flaw, which has a CVE-2018-16858.  It automatically executes a specific Python library bundled within the software.



Both of these office suites bundle their own Python interpreters so you don't have to have one separately installed.  It's there.  And by mixing a white-colored link with an onmouseover event and turning the whole page into a link, he's able to get this thing to trigger.  So all it requires is that a user open one of these maliciously crafted documents, and bad guys' code runs on your system.



He wrote in his disclosure that the Python file named "pydoc.py" that comes included with LibreOffice's own Python interpreter accepts arbitrary commands in one of its parameters and executes them through the system's command line or console, thus essentially creating a pipeline that allows the attacker supplied command to execute, even though it's in a document.  That pydoc.py has a security problem with it, I mean, like fundamentally.  He provided a proof-of-concept video demonstration which shows how he's able to trick that event, the onmouseover event, into calling a specific function within the Python file, which then executed the payload that he had provided through a Windows command line, and showed no indication, no warning dialog of any kind to the user.  And he didn't really have great success.



We've talked before about the problems that researchers have.  Some researchers have even been grumbly about how difficult it has been to report problems.  There was a little bit of that.  He said:  "At first I reported it via the LibreOffice Bugzilla system.  Apparently for security issues it's better," he said, "to send an email to officesecurity@lists.freedesktop.org," he said, "but I didn't know that."  In fact, we've talked about the initiative for there to be some sort of consistent reporting path independent of organization, some way for people who discover vulnerabilities to know who to talk to, especially if you're not doing this all the time.  It takes some digging around.



He said, anyway:  "So my Bugzilla report got closed, but I convinced them to have another look.  The bug was picked up and moved to a thread via officesecurity@lists.freedesktop.org, which is where it should have gone."  He said:  "The issue was verified and fixed quite quickly."  So that was on the LibreOffice side.  OpenOffice confirmed via email that they recognize there's a problem, but OpenOffice does not allow the passing of parameters.  So his proof of concept doesn't work as it was developed for LibreOffice.  But the path traversal is still there and can be abused to execute Python script from another location on a local file system.



So at this point I would opt for LibreOffice, if I had a choice, because those guys jumped on it and actually fixed the problem.  So no real big message there beside the fact that, if you are using LibreOffice, you want to just check for an update which is waiting, depending upon how long it's been.



Also I picked up an interesting little tidbit from a BlueHat security conference which occurred in Israel last week.  Microsoft security engineer Matt Miller said that, over the past 12 years, not changing very much, around 70% of all Microsoft patches were fixes for memory safety bugs.  I guess it shouldn't really surprise us.  Leo, you've got the graph that Mike provided in his slides from his presentation on the screen, basically showing kind of a, you know, looks like one of those tanks with oil and water in it, where the oil doesn't mix with the water, and it sort of has a wave going back and forth.  It's just sort of a wavy blue line separating the light blue from the dark blue, with a dotted red line right at 70%.  And it's maybe a little more than average over 70, but it's kind of around 70, dipped down below a little bit for a while in 2012.



But basically the point is, over a decade and two years, 12 years, we've not seen much change.  It's always been around 70.  And despite everyone's efforts at improving things, address space layout randomization and doing everything we can, I guess I would argue that it would have gotten a lot worse if we hadn't fought back with technology to work against these memory-related problems.  But it's pretty much been a standoff at 70%.  Which is sort of interesting.  So that would be all memory-related things.  Now, arguably it's a big bucket, a catchall.  There's buffer overflow.  There's memory-related race conditions, page faults, null pointers, stack exhaustions, heap exhaustions and heap corruptions, use after free vulnerabilities, or double free vulnerabilities, where you free something and then free it again, and that causes another upset.  So that is arguably a bunch of stuff.  But again, it's been hovering at 70%.



Anyway, the reason Matt explained for this high percentage is because Windows, he says, and I agree, has been written mostly in C and C++, which he describes as two memory unsafe programming languages that allow developers fine-grained control over the memory addresses where their code can be executed and where data can be stored.  Just a simple mistake in the developer's management of memory can lead to exploitable memory errors that attackers can exploit to obtain remote code execution or the elevation of their privilege within the system.



And, I mean, we know that's the case.  Anyone who's looked at C realizes, I mean, you can put anything you want to into a pointer.  And if you dereference that pointer, you're accessing your memory space.  So, I mean, there's no abstraction.  There's nothing between you and the bare metal.  Which is, I mean, that's why C was written.  We've talked about this a number of times through the years is Kernighan and Ritchie developed C to be like, I mean, they wrote the first Unix in assembler.  And then they said, okay, let's do a minimal abstraction on top of assembly language in order to give us some convenience.



So C is a very small language.  It adds just a small layer on top.  And then it uses libraries in order to further expand the language.  So again, programmers like it because they want the power that comes with it.  Unfortunately, with that power comes responsibility.  And we seem to be having a problem with that.



So as a consequence, all of this makes memory management and safety errors today's biggest attack surface for hackers.  And these errors are what attackers are capitalizing upon, thus the reason that what Microsoft patches are memory-related problems.  His presentation, Mike Miller's presentation asserted that use after free and heap corruption vulnerabilities remain the preferred go-to bugs for attackers searching for exploitable vulnerable code behavior.  And so there's no big surprise there.



And I've been mentioning recently that it's somewhat surprising to see how deeply entrenched existing computing technology actually is, meaning how reluctant we appear to be to really change anything fundamental, despite the fact that fundamental change is what we clearly need.  My own current focus, as everybody knows, has been on usernames and passwords to log into computers.  But we're largely using the same technology today that, I mean, we've added password managers in order to deal with the problems of the fact that websites are unable to keep our secrets.  We're using the same technology today that mainframe terminals used in the early 1970s, which is now approaching 50 years ago.  That hasn't changed.  Which is mindboggling.



But similarly, coding has always not appreciably changed throughout all that time.  Yeah, there have been, and we hear about various dynamic programming languages from time to time.  There have been academic experimental memory safe languages, where the allocation is done for you.  You're protected from yourself.  Your variables are garbage-collected after you so you don't have to worry about the whole allocation problem.  But for many different reasons, they never gain foothold as a mainstream core implementation language.



And if the language itself were to be safe, the problem is that they're still running on old-school hardware architectures and being compiled, or more likely interpreted, because these fancy languages tend to be interpreted, they're being interpreted by buggy interpreters running on old-school hardware.  So it never really gets us anywhere.  And then of course there's programmer  hubris, which is never to be underestimated in scope or depth.  No programmer ever thinks they're going to make a mistake.  And when they inevitably do, it's considered a mistake.  Whoops.  And it's treated as though it won't ever happen again.  But of course it will and it does, over and over and over.



If this is going to change at some point, we need to rethink the whole thing.  And I don't know where that rethink comes from.  Maybe from some ivory tower somewhere.  But I also wonder, will  it really ever change at this point, since it's been so long, Leo, that we've had what we've had.  I mean, there was even a Forth processor, a Forth chip that was, you know, ran Forth.  And that sort of just never went anywhere.  



LEO:  I think it's probably easier to mess things up with Forth than almost anything, to be honest.



STEVE:  Yes.  Yes.



LEO:  You know, it's a stack-based language.  You can put anything on the stack, pop it off the stack and, you know.



STEVE:  Well, and as we've often said, it's a write-only language because you are basically the compiler, and it is really difficult to read Forth after the fact.  But I wonder really if there's any motivation in this direction, any true motivation.  People say they want security, but they really don't insist upon it.  They complain when something is insecure, but users cannot see security.  They don't understand what it really is.  It's not a feature.  It's not an obvious benefit.  It's a complete intangible.



And of course we've used the analogy before how we smugly lock the front doors of our homes that have easily broken glass windows.  So it's like, okay.  A home would be much more secure if it were a bunker with no windows.  But nobody wants to make that tradeoff, even though, yes, it would be much more secure.  So the presumption is, well, it's secure enough.  Right, until someone breaks in.  And it's well understood that Apple's iOS-based devices are significantly more secure than Android.  They're not perfect.



LEO:  Swift does a lot of memory management and garbage collection protection.



STEVE:  Yes.  Yes.  On the other hand, most Android devices are much less expensive, so that's what most people buy.  I mean, I know people with Android.  And it's like, it's good enough, and it was cheaper.  So even though Apple has problems, too, people buy Android.  So I guess not only don't and won't people actually pay more for security, but this industry still doesn't actually have true security to sell them.  So maybe we are going to need more than three digits for this podcast, Leo.



LEO:  There's got to be a - good, I hope so.  There's got to be a way, though.  I mean, NASA designed software that can't be patched.



STEVE:  Oh, but at such cost.  At such cost.



LEO:  Maybe that's the answer is it's too expensive, yeah.



STEVE:  And that's the problem.  Yes.  Using today's technology, today's computing technology, we can produce provably secure software.  But no one has time.  It's just too expensive.  So we don't bother.  And speaking of updates, Mary Jo on This Week in Windows last week shared with you and Paul some news from Microsoft.  We know that Windows 7 is now on extended...



LEO:  Life support.



STEVE:  ...security, yes, yes, life support.  Until this time, until February of 2020, so one year from now.  We've talked about the idea that - because it only just happened, despite all the crazy pressure Microsoft has put on the world to go to Windows 10, it only just happened that Windows 10 passed Windows 7 in usage.  And nobody who has 7, mostly enterprises, wants to leave.  They're like, everything's working.  And besides, Windows 10 has a bad rap, or rep, or both, when it comes to privacy.  I mean, Leo, have you installed Windows 10 on a new machine?



LEO:  Dozens of times, yeah.



STEVE:  It looks like an arcade game.  I mean, stuff is flipping up and down and jumping around, and you're being asked if you want candy corn dropped on you or I don't even know what this is.  It's just insane.  I just look at this, and I think, you've got to be kidding me.  Anyway, so it's no surprise that enterprise people want nothing to do with this nightmare.  So anyway, Mary Jo shared last week, and I have a picture of what she posted over on ZDNet in the show notes.  Windows Enterprise versions starting next year will be able to pay $25 per seat for continued updates for the year.  Well, actually, wait a minute, January 20 through January 21.  I don't understand that.



LEO:  This doesn't take effect till January of 2020, till next year.  The extended updates expire 2020.



STEVE:  Oh, 2020, right, 2020.  Okay.  So next year $25.  The year after, $50 per seat.



LEO:  Doubles every year.



STEVE:  Yes.  And the year after, $100 per seat.  That's the Enterprise, which is the cheaper of the two.  If you are on Windows 7 Pro, it's $50 the first year, $100 the second year, and $200 the third year.  So the idea is, yes, and this is not for end users.  End users are going to get left in the cold.  You know me.  Actually, I mean, I have Windows 10 all over the place now.  And I have arranged to strip the crap out of it.  There's a bunch of things you can do in PowerShell that just removes all of this, whatever the heck that crap is that they have in there now, the new apps that nobody wants.  I heard Paul a couple weeks ago just saying to you, "It's all junk.  Have you looked at those Windows Store apps?  They're just junk."  It's like, yeah, but I guess they're secure junk, so okay.  Anyway, I'm making peace with Windows 10.  



LEO:  This is making peace?  I can't wait to hear when you make war.  Okay, yeah.



STEVE:  Anyway, the point is, for what it's worth to our people who are in enterprises, Microsoft is going to start pushing hard to, if you want to keep security updates, and if you want to stay on Windows 7, they're going to start pushing hard, starting next year, to move people off.  I don't know what enterprises will do.  Maybe they'll just say, fine, we'll pay.  Maybe they'll give up and move to 10.  Or maybe, you know, there is a version of 10, I think I heard Mary Jo talking about it, or I read about it - no, she did refer to it.  There's a version of 10 that is available to enterprises that doesn't have any of this nonsense in it.  End users can't get it.  We can't get it.  But there is one that Microsoft makes available that is tamed.



And so maybe that'll get made more widely.  I mean, nobody wants Xbox junk that you can't uninstall in your Windows 10, if you want nothing to do with Xbox.  It's just loony tunes.  And then ads for games and, ugh.  Anyway, so now we know what it's going to cost.  We can't get it, enterprises can, and it's going to cost a bit.



Okay.  Whew.  Maybe I've had too much caffeine.  China's cybersecurity law actually went into effect last November.  And we didn't talk about it much.  But in light of Japan's forthcoming test, their own essentially pen test of their own network's IoT devices coming up here soon, I just wanted to note that - and we'll be talking about Russia here next, in a second. But China has, not surprisingly, given themselves some expansive powers.  So I wanted to follow up on our discussion of Japan.



This newly enacted law allows China's Ministry of Public Security, the MPS, which is the same agency which maintains China's Great Firewall and runs its national facial recognition system and the surveillance camera network, has been since November empowered to conduct in-person or remote inspections of the network security defenses taken by companies operating in China, check for prohibited content banned inside China's border, log security response plans during onsite inspections, copy any user information found on inspected systems during onsite or remote inspections, perform penetration tests to check for vulnerabilities, perform remote inspections without informing companies, share any collected data with other state agencies, and has the right to have two members of the People's Armed Police, the PAP, present during on-site inspection to enforce procedures.



So again, as we know, it's not a democracy.  But I'm feeling like, as a consequence of this, as a consequence of what Japan is going to do, they're going to end up with much more secure network infrastructures, that is, public infrastructures, than we, the U.S., who so far refuse to allow white hat or government agencies to do the same.  And I don't think this is going to stand.  I bet you that we're going to see Australia follow suit, given the penchant that Australia has seen.  And then probably the U.K., and then I wouldn't be surprised if the U.S. does.  I just think, nice as it is to say, okay, nobody gets to do this, to only allow the bad guys to do it, look at the preponderance of evidence this last year of podcasts that we've been talking about.  It was nice once, but it's just no longer practical.



And I said we were going to talk about Russia.  This is going to be really interesting to see.  Russian authorities and major Russian Internet providers are planning to disconnect the country from the Internet as part of a planned experiment.  The ultimate goal is for Russian authorities to implement a web traffic filtering system like China's Great Firewall and to also arrange to have a fully working, self-contained, country-wide Intranet - essentially that's what you would call it if it's no longer global - in case the country needs or chooses to disconnect.  And you could imagine hearing in a Russian accent:  "Why do we need the rest of world?"



So the initial experiment, which is intended to occur before, but hopefully not on, April 1st, will serve to provide insight and feedback and possible modifications to this law which is proposed to allow this to happen, which has been introduced in the Russian Parliament last December.  The first draft of the law mandated that Russian Internet providers should be able to ensure the independence of the Russian Internet space, which they refer to as "Runet," in case of foreign aggression, to disconnect the country from the rest of the Internet.  Additionally, Russian networking firms would have to install "technical means" to reroute all Russian Internet traffic to exchange points approved or managed by, Leo, our favorite organization, Roskomnadzor.



LEO:  The FCC.



STEVE:  Yes.



LEO:  Should we do this in the U.S.?  What do you think?  I don't think this is such a bad idea.



STEVE:  It's not such a bad idea.



LEO:  I mean, obviously in the U.S. you'd only do it in case of war.  I mean, you wouldn't do it just because you didn't like what you were hearing from the outside world.



STEVE:  Right.  Well, yes.  And in fact I wouldn't be surprised if we don't already have...



LEO:  A kill switch.



STEVE:  Cutoff switches on all cables coming, all transatlantic cables coming into the U.S. and satellite stuff. 



LEO:  That's really interesting.



STEVE:  We probably do.  The problem is it would cause chaos.  Our system would collapse if it didn't have access to - because there's just the assumption of all these resources located globally.  To their credit, what Russia is doing is they're being upfront about it.  They're saying, you know, we're going to start running experiments, a series of drills.



LEO:  Let's just see what would happen.



STEVE:  Yes, exactly.



LEO:  What would happen?



STEVE:  Let's just pull the switch.



LEO:  Might be okay.  We'll see.



STEVE:  Let's pull the plug and, yeah.  So anyway, Roskomnadzor will inspect the traffic to block prohibited content.  And that's really interesting, too, because, okay, they're saying they're wanting to experiment with routing everything through Roskomnadzor, but everything is TLS now.  So that suggests everybody...



LEO:  You can only turn it off or on.  You can't really...



STEVE:  Yeah.  You can't look in it unless you get the Russian root cert that they're going to force everybody in Russia to have.



LEO:  Oh, boy.  Well, they might do that. 



STEVE:  I think China is going to do it.  And anybody, any state that insists on being able to inspect the traffic, the contemporary traffic of their citizenry has to put a root cert on every machine and then perform fake cert creation and proxying at the border.  That's the only way to do it is to perform a state-sanctioned man-in-the-middle inspection of traffic.



LEO:  Wow.



STEVE:  Anyway, the Russian government has been working on this project for years.  And in 2017 Russian officials said they plan to route 95% of all Internet traffic locally by 2020.  That is, meaning through Roskomnadzor.  



LEO:  I like how you say that.



STEVE:  Buckle your seatbelts.  To pull this off, they'll still need their own DNS.  So authorities have built in-country backup of the entire domain name system.  It was first tested in 2014, and then again last year.  It will be a major component of Russkinet - wait, no, Runet.



LEO:  Oh, I like Russkinet.  That's good.



STEVE:  Russkinet.



LEO:  Russkinet.



STEVE:  Russkinet - when ISPs plan to pull the plug and disconnect the country from the rest of the world.  So we're going to, ooh, sometime before long, next month or maybe, I guess, oh, no, before the first of April.  So we're going to be talking about what happened when Russia pulled the plug on the Internet.  Woohoo.



LEO:  Wow.  Wow.  Very interesting.



STEVE:  Yeah.



LEO:  Yeah, I mean, if you use 1.1.1.1 in the U.S., and we cut off our Internet connection, your DNS would go down.



STEVE:  Yeah.



LEO:  Right?



STEVE:  Yeah.



LEO:  So we're so interconnected at this point, you can't...



STEVE:  Yeah, but I really, you know...



LEO:  And you need a kill switch because...



STEVE:  I'm sure there's no chance right now, given what we know about the Internet, that there isn't one that can isolate the country.  And the problem is...



LEO:  It would isolate the country.



STEVE:  Everything will break.



LEO:  That's the problem.



STEVE:  Yes.  Everything will break.



LEO:  Very interesting.



STEVE:  So I just wanted to mention also that we talked about Firefox's suspension of Release 65, which was warning its users who had Avast and AVG AV and also some others.  All the vendors have now issued an update, the AV vendors, to suspend their scanning of traffic for users of Firefox.  So Mozilla has resumed rolling out 65.  So for what it's worth, if Firefox users want AV scanning, you'll need to switch to a different browser, probably use Chrome.  Or just rely, as you and I do, Leo, on Defender, that does very good, it's rated now, it's ranked right up there with the other guys, very good antivirus.  And it does so because it's native to the OS.  It doesn't cause Firefox a problem.  So yay.



LEO:  Steven?



STEVE:  So I'm going to share three anecdotes and then talk a little bit about why I'm not doing more.  Domsch is his handle in the SQRL forums.  And he said:  "Just wanted to chime in.  I created my first SQRL ID months back without having a use for it.  Of course I lost the restore codes.  So after listening to SN-700" - meaning last week's episode - "on my commute yesterday and today, I set up a new identity created on my Android phone with the awesome Android app, signed onto the forums in seconds, and five minutes later signed onto the forums on my work PC without any problems."



He wrote, "This is awesome.  The biggest thing for me, it's independent of a password manager."  Which is not to say we don't love password managers, I'm glued to mine, but we all need them right now.  His point is, and he writes:  "The password manager extension is one of the big things keeping me with Firefox.  I'd really like to switch to Falkon, but without my passwords that's a no-go."



He says:  "So here is me hoping SQRL will find wide adoption on all platforms so I can finally leave my password manager behind."  He wrote:  "Thanks, Steve, for creating this awesome authentication method, and especially for not going with the 'early bird, alpha, preorder, green light' approach that's so common nowadays, but instead doing it right and delivering an actually 100% working product."



Then CormaP said, and I just picked these comments up in the forums, he says:  "I installed the Android SQRL client a week or two ago and have just used it to register here for the first time.  I foolishly forgot my password for my identity."  And I think, okay.  He says:  "So I reset the password using the rescue code.  Painless.  Logged into the forums using SQRL.  Painless.  Set up two-factor using the LastPass authenticator.  Painless."



He said:  "All good so far.  My question is, can I avoid having to reenter my password when logging into a site on my phone?  Or can I just use the phone's fingerprint sensor?  Or is it simply because I have only logged in twice?  Am I missing the point/doing something wrong?  Thanks."  He said Cosma, so maybe it is Cosma.  Or maybe I just, no, maybe - anyway, he spelled it differently.



And to answer his question, first of all, he used the features that I built into the whole system to allow people to do their own password recovery because exactly what he had happen is going to happen.  And with SQRL, because it is rigorously two-party, there's no one to go crying to if you forget the one password you only ever need.  The good news is you only need one ever for all your sites that use SQRL, so you're much less likely to forget it.  But it could happen.  Or you could reenter it twice, and then forget - you could change it and forget what you changed it to.



So anyway, this solves all those problems, and it worked for him.  I'm asking Jeff Arthur, who's the author of the iOS version, to be less rigorous with unlocking SQRL on the iPhone under the assumption that, if you've unlocked your phone with your fingerprint or your face, then you've already just - in fact, you have to do it per authentication.  So you are right then reauthenticating yourself to the SQRL app.



Right now he's been following the protocol I use on my Windows client where it's a little finickier about making you reenter your whole password because in its default mode Windows doesn't have biometrics built in, whereas the newer iOS phones, as we know, do, and even tablets.  And Android, the most recent Android Pie has a usefully secure fingerprint authentication also.  So to answer his question, yes, when this settles down, you will be able to only use a contemporaneous reauthentication in order to log on.



And lastly, KenRS said, and he was writing to Jeff in the forums because all of the authors of the clients have their own development spaces there.  He said:  "Jeff, just trying your client on this forum, and it's great to see it working.  The identity import using the printed QR code worked perfectly.  Once set up, I scanned the SQRL QR code appearing on my laptop screen with my iPad and was instantly logged in on the laptop."  And he says:  "That part does seem like magic."  He says:  "Logging in 'on-device' on the iPad also worked.  The only thing that still is rough is signing in using the fingerprint authentication on the iPad.  Instead of logging in instantly, I'm left with Safari searching for localhost.  Canceling that leaves me logged in.  Regards, Ken."  And, yes, that's something that Jeff will be fixing.



So anyway, we're getting close.  I'm sharing these anecdotes while a great deal of work is underway behind the scenes, as we sort of are in countdown.  It's become clear to the authors of the other SQRL clients that this bread is just about baked and that SQRL really does work and is actually going to happen; at least, I mean, like we'll have multiple clients and multiple servers and a working website where people can go and experiment with this.



So right now work is proceeding at an increased pace as we approach the finish line.  I'm working to flesh out the SQRL forums content so there's material there for newcomers.  At the same time, I need to update SQRL's documentation, the online documentation, so that those who want to evaluate its underlying technology and operation will be able to do so because those docs are five years old now, and they are only barely reflective of the way things actually turned out.



So I guess my point is that right now we're sort of getting a trickle of people, and those who really know it already, who have been in the newsgroup, have moved over and are experimenting.  It's sort of just the right level of activity that we can see problems and work to fix them.  While you've been doing the sponsor breaks, Leo, I've been researching why I'm getting a very occasional server error 503 from the FastCGI module of IIS because it's running PHP.  And now I think I know why, thanks to you talking to our listeners about Sophos.



LEO:  He uses those breaks intelligently, ladies and gentlemen.



STEVE:  Yeah, that's right.  When you see me doing this when you come back to the camera, there's a reason for it.



LEO:  I love it.  You waste no time.  I love it.



STEVE:  So anyway, I mean, this is - I'm getting very excited and also very nervous.  You can imagine, you know, this has been a huge effort.  But, I mean, it really all is working.  And it is very exciting.  Oh, there's also a web extension running both under Firefox and Chrome.  So you don't even need to have - and that'll give us Mac and Linux.  And there are people running my Windows app under Wine on a Mac.  And so anyway, lots happening.  I know I'm teasing people, but it's getting close.  And right now I have enough happening that, if there was a flood of people, it would kind of overwhelm us.  So we're just working out the details because you know me, I'm a "measure it twice or maybe 10 times and then cut once accurately" person.  So we're getting there.



LEO:  Good. How exciting.



STEVE:  I'm very excited.  So three things about pfSense real quick.  Dave, who tweeted from @darkmatter_0x00, he said:  "Hi, Steve.  Great podcast.  Quick question.  A few episodes ago" - oh, and we just heard a yabba-dabba-doo, so thank you.  Someone just purchased a copy of SpinRite, which I didn't talk about in this podcast.  But, I mean, it absolutely is the reason SQRL exists, because our listeners and the public and those needing to keep their hard drives in shape and to recover from problems have supported this effort as a consequence of purchasing something which is hopefully useful to them immediately and which I have promised will be useful moving forward in the future, and which I will get back to as soon as this SQRL project is behind me.



He said:  "Quick question.  A few episodes ago you mentioned a small firewall VPN device/router that can be bought.  What was the name of it?  Only has a few ports and ran pfSense.  Maybe I'm wrong about the 'pf' part.  Thanks."  And of course that was the SG-1100.  And think of Steve Gibson, although there's no affiliation, as in SG - what does it stand for?  Probably secure gateway?  Anyway, SG-1100 [Security Gateway].  If you google that, it ought to take you to Netsense, I think it is, or Netgate.  They're the people.



So that's what it is.  And yes, it is three ports, so a WAN port and two on the LAN side, which allows you to do network isolation.  It does run pfSense, which is absolutely the drop dead manager that you want.  pfSense has just really, well, first of all, it runs everything and has a web browser-based interface that allows you to do stuff.  For example, I'm pulling off all kinds of networking magic, thanks to having the flexibility that it offers.



Bldg, which looks like Building 35, tweets @Bldg35.  He said:  "Hi.  Just wanted to say thanks for planting the idea in my head regarding pfSense.  I've got my SG-1100 online now.  Reconfigured my ASUS RT-AC5300 to AP [Access Point] /AiMesh mode."  And actually that's exactly what I did with mine.  He said:  "I was surprised that I didn't lose any WiFi features, such as guest node isolation, et cetera."



He said:  "It didn't take too long to get an OpenVPN server running in pfSense for remote access.  Also, for some reason, Internet access feels noticeably faster.  For instance, my 10 Ring security cameras [wow] now come up on my phone much faster in live view and recorded videos.  Not sure why that is, but I'll take it."  And for one thing, it is five times faster, the SG-1100, five times faster than the predecessor, SG-1000.  So it's one thing to have gig ports.  It's another thing for them to be saturatable.  And this little SG-1100 is great for 150 bucks.  It's able to do that.



And, finally, Andy Blak, B-L-A-K, who tweeted under the same name:  "Hey, Steve.  Recently you mentioned the Netgate pfSense products on Security Now!.  I purchased one after the show for my home network, though I'm having trouble configuring it.  Do you have any recommendations for who could assist with this?  Netgate charges $900+ minimum for their support."  He says:  "I called them."  He says:  "Just looking for a more reasonable option.  Many thanks.  Andy."



And the answer is yes.  The Google is your friend.  The upside of pfSense is that, I mean, it can do anything.  The downside is that it can do anything.  It's way more comprehensive and therefore configurable and complete than any router.  But with that comes some configuration overhead.  The good news is pfSense is the software you need to focus on, and there is a ton of how-to stuff on the Internet.  So just google any question you have:  pfSense; OpenVPN, how to set up OpenVPN; pfSense, you know, how to brew my espresso stronger.  Any question you could possibly ask, pfSense probably does it, and you'll find somebody who can help you.



So don't be shy.  And, yikes, that is expensive support.  In fact, I got a different email I didn't mention, someone who said that they couldn't get the firmware from Netgate.  They wanted a support contract.  And it's like, well, that's ridiculous.  The pfSense firmware is freely available for the SG-1100 on the Internet.  Once again, Google is your friend.



Okay.  What is Adiantum?  I guess the question is, how can crucially important full-disk encryption be offered on inexpensive commodity hardware which lacks any encryption acceleration?  That's the question Google set themselves.  Because they have Android, they want to offer full-disk encryption, but we know what that does if you run it on a platform without some assistance for AES.  And, you know, think about this for a moment.  Many of us now take for granted the fact that our devices at rest are fully encrypted, meaning that the mass storage that our devices carry internally cannot simply be read out as a file system, either by good guys or bad guys who want to know what's in there.



I mean, again, I feel like we now just take that for granted on our mobile devices.  But anybody running Android who doesn't have full disk encryption enabled has their file system storage as an unencrypted file system.  And so anybody who gets a hold of it can just get it.  I mean, it's just all there.  So it's crucial that it be encrypted and decrypted on the fly.  But as we've mentioned before, the reality for users of low-end Android smartphones who, initially wanting full security, activate their drive's whole disk encryption, only to suffer such a reduction in performance that they're effectively forced to back out and just use their systems without that kind of encryption.



The trouble is caused by our chosen, the industry's chosen best-of-class cipher, Rijndael, which was chosen, as we know, to be the AES, the Advanced Encryption Standard, which while it's wonderfully secure, comes at the significant cost of per-byte encryption performance.  And that overhead, that performance hit is bad enough that, back in 2010, when Intel introduced their new core, their Intel core processor family with Westmere, that new silicon and all silicon since has incorporated six new instructions known as the AES-NI, which is for AES New Instructions.  Because I guess all the cooler names were already taken.  They specifically speed up the use of the AES cipher on Intel architectures.



And it's not that AES cannot be done without those instructions.  It can, of course.  But each of those instructions is specific to an aspect of what the AES cipher algorithm needs so that each is able to replace a great many more standard instructions to get the same job done much more quickly.  The point being, yes, AES is super secure, but not very quickly without some help.  And also, as we've discussed before, the reason encrypting the entire drive is so sensitive to the performance of its encipherment is that bulk enciphering must happen continuously on the fly for every byte of data read and written to the drive.  That cipher is stuck in-line and cannot be bypassed.



So happily, the bright lads at Google decided to take aim at this problem to see how they might improve upon the situation.  Since it's not simply to speed up the Rijndael - it's not possible to simply speed up the Rijndael cipher without hardware, they knew that they would need to replace Rijndael with something else.  This is feasible in any closed system where the cipher is only used for internal purposes.  For example, when transacting over TLS, before the ChaCha20 cipher got added to TLS, which has happened recently, there was no choice other than to use AES, since its use there is for an external standard where you want to connect to a server remotely over a cipher that you both have.  Maybe the other server doesn't have anything other than AES, so you've got to use it.



But internally, deep inside an Android device, Google is 100% free to choose anything that works.  And what works is this cipher I mentioned, ChaCha.  There's a chart, big chart in the show notes showing the performance of the bulk encryption mode known as XTS, which is what typically AES-256 cipher uses.  It's the mode used for bulk-encrypting mass storage, versus Adiantum, which uses the ChaCha cipher, although in its reduced 12-round strength, rather than its normal 20-round strength because 12 is strong enough, and they're going for speed here.  The more rounds you have, the slower it is.



Anyway, this bar chart shows about a five-factor performance improvement, which is in fact what Adiantum delivers with ChaCha versus AES-256 with equivalent security margin.  And I smiled when I saw ChaCha because it is the brainchild of the same person who designed the 25519 elliptic curve cipher that is entirely responsible for enabling SQRL's solutions.  That person is Daniel J. Bernstein, Dan Bernstein.



So Google's online security blog last Thursday, February 7th, was entitled "Introducing Adiantum:  Encryption for the Next Billion Users."  I guess they have high hopes for the use of Android.  But really, I mean, they've nailed it.  And I can't say any better than they have.  I'm going to quickly share what they wrote.



They said:  "Today, Android offers storage encryption using the Advanced Encryption Standard.  Most new Android devices have hardware support for AES via the ARMv8 Cryptography Extensions.  However, Android runs on a wide range of devices.  This includes not only the latest flagship and mid-range phones, but also entry-level Android Go phones sold primarily in developing countries, along with smart watches and TVs.  In order to offer low-cost options, device manufacturers sometimes use low-end processors such as the ARM Cortex-A7, which does not have hardware support for AES.  On these devices, AES is so slow that it would result in a poor user experience.  Apps would take much longer to launch, and the device would generally feel much slower."  And remember that speed and power consumption also go hand in hand.  You'd really like to run the chip slower and cooler and thus use less battery, if you could.



"So while storage encryption," they write, "has been required for most devices since Android 6.0 in 2015, devices with poor AES performance," and they say 50 Mbps and below.  And maybe that's megabytes.  There's that MiB again.  That's got to be megabytes [mebibyte] - "50 million bytes and below are exempt."  So that is to say, to make sure everyone heard that, even with Android 6.0, which requires hardware encryption, the AES hit is so onerous that, if devices have lower performance, they are exempt from Google's own requirement.



They said:  "We've been working to change this because we believe that encryption is for everyone.  In HTTPS encryption," they say, "this is a solved problem.  The ChaCha20 stream cipher is much faster than AES when hardware acceleration is unavailable, while also being extremely secure.  It is fast because it exclusively relies on operations that all CPUs natively support:  additions, rotations, and XORs.  For this reason, in 2014 Google selected ChaCha20 along with the Poly1305 authenticator, which is also fast in software, for a new TLS cipher suite to secure HTTPS Internet connections.  ChaCha20-Poly1305 has been standardized as RFC 7539, and it greatly improves HTTPS performance on devices that lack AES instructions."



So just to pause for a second, that means that light bulbs and low-end IoT devices that are performance challenged, as long as the server that they're connecting to offers this ChaCha20-Poly1305 mode of encipherment, which is now part of the TLS cipher suite spec, will be able to negotiate that with the server and end up with a much higher performance, yet still equally secure protocol for moving bulk data across an encrypted connection.  So that's all for the good.



Google said:  "However, disk and file encryption present a special challenge."  Oh, note that we're talking about a stream cipher in ChaCha20.  ChaCha20 generates, given a key, generates a stream of pseudorandom bits which then is XORed with the plaintext to create the ciphertext.  As opposed to AES, which is a block cipher, which produces blocks of eight bytes, thus 128 bits at a time - wait, blocks of 16 bytes, thus 128-bit blocks, which are then used in various modes to encrypt the plaintext into ciphertext, typically CBC or some other block chaining mode, to create an inter-block dependency.  We've talked about all this in podcasts past.  But ChaCha20 is a stream cipher.  Anyway, I wanted to make sure people understood the distinction.



They said:  "However, disk and file encryption present a special challenge.  Data on storage devices is organized into sectors which today are typically 4096 bytes.  When the filesystem makes a request" - and actually they're noting, they're kind of mixing sectors and clusters.  You know, clusters are typically clusters of sectors.  Sectors still tend to be logically 512 bytes.  But we now have clusters of eight bytes.  So, yes, clusters of eight sectors, which end up with allocation units of 4096 bytes.  So Google says:  "When the file system makes a request to the device to read or write a" - they said "sector," but they really meant a cluster - "the encryption layer intercepts that request and converts between plaintext and ciphertext.  This means that we must convert between a 4096-byte plaintext and a 4096-byte ciphertext.  But to use RFC 7539" - that's the streaming cipher used for TLS - "the ciphertext must be slightly larger than the plaintext.  A little space is needed for the cryptographic nonce and for the message integrity information."



In other words, and we've talked about this all before, in these normal block chaining modes you have an initialization vector, a nonce that is at the front end, doesn't need to be secret, but does need to be unique.  And then you also have an authentication tag added to the end which is how you verify that nothing has changed in the stream.  Both of those things slightly increase the length of what you're transmitting.  But of course, since it's a communication stream, that doesn't matter.  It's a tiny little bit added in front and at the end.  No one cares.  But you can't think about it.  If it's a hard drive, they're like fixed chunks of space.  There's nowhere to stick extra stuff.  They're blocks.



And so you need to take, and this is the point Google is making, need to take a 4096 block and go [sound], that's the technical term, and turn it into a 4096 ciphertext and vice versa.  You can't ask for a little bit extra space in the beginning and the end.  There's none because you've got the block ahead and the block behind.  So Google says:  "There are software techniques for finding places to store this extra information" - that would be a mess.  They're talking about adding metadata to the filesystem.  But they said:  "But they reduce efficiency and can impose significant complexity on a filesystem design.



"Where AES is used, the conventional solution for disk encryption is to use the XTS or CBC-ESSIV modes of operation, which are length-preserving.  Currently, Android supports AES-128-CBC-ESSIV for full-disk encryption and AES-256-XTS for file-based encryption.  However, when AES performance is insufficient, there is no widely accepted alternative that has sufficient performance on lower end ARM processors.



"To solve this problem, we have designed a new encryption mode called Adiantum, A-D-I-A-N-T-U-M.  Adiantum allows us to use the ChaCha stream cipher" - which is very fast on commodity hardware without acceleration - "in a length-preserving mode, by adapting ideas from AES-based proposals for length-preserving encryption such as HCTR and HCH.  On ARM Cortex-A7, Adiantum encryption and decryption on 4096-byte sectors is about 10.6 cycles per byte,  making it around 5X faster than AES-256-XTS."



They said:  "Unlike modes such as XTS or CBC-ESSIV, Adiantum is a true wide-block mode.  Changing any bit anywhere in the plaintext will unrecognizably change all of the ciphertext, and vice versa.  It works by first hashing almost the entire plaintext using a keyed hash based on Poly1305 and another very fast keyed hashing function called NH.  We also hash a value called the 'tweak' which is used to ensure that different sectors are encrypted differently."



Well, in fact the tweak in practice is typically the cluster number.  It's called a "tweakable block cipher," and each block, the number of the block, the offset of the block in the filesystem is the tweak factor.  Which means that, if you were to encrypt the same data in two different places on the drive, it would look completely different and no one could tell it was actually the same.  You want to prevent even that slight leakage of information.  So this is known as a tweakable block cipher, where the tweak adds - you can think of it as like another key to the keyed cipher.



They say:  "This hash is then used to generate a nonce for the ChaCha encryption.  After encryption we hash again so that we have the same strength in the decryption direction as the encryption direction.  This is arranged in a configuration known as a Feistel network, so that we can decrypt what we've encrypted.  A single AES-256 invocation on a 16-byte block is also required, but for 4096 inputs this is not performance critical."



So in the show notes I have a picture of the algorithm showing how it all works.  I won't go into it because that was a description of it just now.  And they note that:  "Cryptographic primitives like ChaCha are organized in 'rounds,' with each round increasing our confidence in security at a cost in speed. To make disk encryption fast enough on the widest range of devices, we've opted to use the 12-round variant of ChaCha  rather than the more widely used 20-round variant."  For example, the communications protocol, NTLS uses ChaCha20.



"Each round vastly increases the difficulty of attack.  The seven-round variant of ChaCha was broken in 2008 and, though many papers have improved upon this attack, no attack on eight rounds is known today."  And we've got eight, nine, 10, 11, 12.  So a five-round pad with each round, as they said, vastly increasing the difficulty.  This ratio of rounds used to rounds broken today is actually better for ChaCha12 than it is currently for AES-256.  So higher security margins than AES right now.



They said:  "Even though Adiantum is very new, we are in a position to have high confidence in its security.  In our paper, we prove that it has good security properties, under the assumption that ChaCha12 and AES-256 are secure.  This is standard practice in cryptography.  From 'primitives' like ChaCha and AES, we build 'constructions' like XTS, GCM, or Adiantum.  Very often we can offer strong arguments, but not a proof that the primitives are secure; while we can prove that, if the primitives are secure, the constructions we build from them must be, too.  We don't have to make that assumption about NH or Poly1305 hashes.  These are proven to have the cryptographic property that they rely on.



And, finally, Adiantum, as I was mentioning I think before we began recording, Leo, is named after the genus of the maidenhair fern, which in the Victorian language of flowers, floriography, represents sincerity and discretion.



LEO:  They're quite beautiful, actually.



STEVE:  And Adiantum is a fern which water runs off of, interestingly.



LEO:  I think they wanted to have it be adamantium, but they didn't know how to spell it.  Which is the metal in Wolverine's fingers.



STEVE:  Right, right.  



LEO:  That would have been better.



STEVE:  It would have been, yeah.  Maybe it was taken.  I mean, like maybe it's...



LEO:  Well, it's also trademarked, probably, so maybe they couldn't get adamantium.



STEVE:  Yeah.  And maybe they were trying to be close, but not, you know - no, we were talking about the fern.



LEO:  It's barely pronounceable, to be honest with you.  It's not...



STEVE:  I know.  You have to really struggle to, you know, I was rehearsing before the podcast, Adiantum, Adiantum.  



LEO:  Adiantum.  Adiantum.  Well, there you go.



STEVE:  Yes.  Yup.  So we have a new, a very cool new high performance, low processor requirement cipher mode, appropriate for block ciphers.  And I take my hat off to Google.  That's going to be super useful.



LEO:  They solved a problem they had, and quite elegantly.



STEVE:  Yup, yup.



LEO:  Friends, we've come to the end of this fine show, but not the end of the conversation.



STEVE:  No.



LEO:  You can go right to GRC.com, that's Steve's website.  Not only get copies of the show and transcripts, you can get SpinRite, the world's best hard drive maintenance and recovery utility.  You can join the SQRL groups and participate there.  There's lots of other great free stuff there.  GRC.com.  We have audio and video of the show at our site, too, of course:  TWiT.tv/sn.  We do Security Now! on Tuesdays, 1:30 Pacific, 4:30 Eastern, 21:30 UTC.  If you want to watch it, just go to TWiT.tv/live.  You can watch or listen live.  And if you do that, join us in the chatroom at irc.twit.tv.  That's where the other people watching live hang out.



But as usual, on-demand versions are the best way to do it.  And again, TWiT.tv/sn.  You can listen.  You don't have to miss your jazz radio show or anything like that.  You can listen at your leisure, at your convenience, and even at high speed, as some people do.  I don't blame them.



STEVE:  Okay.



LEO:  All of our shows sound - it's funny.  I had somebody come in yesterday or Sunday who was listening to TWiT.  He said, "You sound so weird because I listen at double speed almost all the time."



STEVE:  Ah.



LEO:  "You sound a little drunk."  This is what we normally sound like.  Don't forget our TWiT survey, TWiT.to/survey19.  We do this every year to get a better idea of what you're interested in, who you are.  It helps us sell advertising.  We are not collecting personal information of any kind.  Don't worry.  But it just gives us a general idea of the audience.  So TWiT.to/survey19.  Thanks, Steve.  Have a great week.



STEVE:  Thank you, my friend.  Talk to you next week for Episode 702.



LEO:  [Whistling] Bye.  



STEVE:  Bye.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#702

DATE:		February 19, 2019

TITLE:		Authenticity on the Internet

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-702.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm 



DESCRIPTION:  This week we catch up with last week's doozy of a Patch Tuesday for both Microsoft and Adobe.  We also examine an interesting twist coming to Windows 7 and Server 2008 security updates, eight mining apps pulled from the Windows Store, another positive security initiative from Google, electric scooters being hacked, more chipping away at Tor's privacy guarantees, a year and a half after Equifax and where's the data?, the beginnings of GDPR-like legislation for the U.S., and some closing-the-loop feedback from our terrific listeners.  Then we take a look at an extremely concerning new and emerging threat for the Internet.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  He's got the most scary AI story ever, he says.  We'll talk about that.  Plus Microsoft's Patch Tuesday was last week.  Why was it so big?  And why you might want to be more careful the next time you ride a rent-a-scooter.  Turns out they're pretty hackable.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 702, recorded Tuesday, February 19th, 2019:  Authenticity on the Internet.



It's time for Security Now!, the show where we cover your security and privacy online with this guy right here, our commander in chief, Mr. Steve Gibson of the Gibson Research Corporation.  Hello, Steve.



STEVE GIBSON:  Yo, Leo.  Great to be with you for Episode 702.



LEO:  Yow.



STEVE:  We have a bunch of stuff to talk about.  And I was just saying to you before we began recording that one of the things we'll talk about is the beginnings of GDPR-like legislation heading into the U.S. as a consequence of a GAO report that was commissioned by Congress two years ago.  And I had that as the topic of the show.  I think it was "GDPR for USofA" was what I was going to title this.



And then I ran across a blog post from the OpenAI group in Northern California.  OpenAI is an Elon Musk-founded organization.  And it chilled me to the bone so that I thought, okay, stop the presses.  I renamed this podcast "Authenticity on the Internet" because - and the good news is they understand how scary this is, too.  As I was reading through what I will be sharing with our listeners, I was thinking, oh my god, oh my god, oh my god.  And it's like no, no, no, no, no.



And then, as I got down toward the end, they addressed what they recognized they had created and what it would mean if it got loose.  So much so that the various tech press picked up on that and said, "Something so scary they're not even letting it go."  The problem is it doesn't matter.  I mean, it's going to change the world, and not for the better.  But we'll get to that in a minute.



We're going to catch up with last week's doozy of a Patch Tuesday, which was a doozy for both Microsoft and Adobe.  We are going to examine an interesting twist coming to Windows 7 and Server 2008 - which is, you know, Server 2008 R2 is the equivalent matched version with Windows 7 - and their security updates.  Mary Jo wrote about it in ZDNet, and I haven't quite figured out one aspect of it that we'll be finding out about in a couple months.



Eight mining apps were pulled from the Windows Store.  Another really interesting positive security initiative from Google.  Electric scooters being hacked.  Who would have guessed?  I mean, like, of course.  And wait till you hear the response from the company.  More chipping away at Tor's privacy guarantees through some new research.  We're a year and a half downstream from Equifax's big breach.  Where did the data go?  There's been no sign of it.  So we'll talk about that.  And as I mentioned, the beginnings of some GDPR-like legislation which of course the EU famously has, and now it looks like we may be getting something like that, and sort of apropos of where we are in time.



We've got some closing-the-loop feedback from our listeners.  And then we've got to talk about this sort of astonishing work that the OpenAI project did.  And we have a great Picture of the Week and some fun things to talk about relative to old-school computing.  So lots of stuff.



LEO:  Busy, busy, busy.



STEVE:  Yeah.



LEO:  Yeah.  And we had it "Authentication on the Internet."  It's "Authenticity on the Internet" is our topic of the day.  And I've modified that lower third to reflect that.



STEVE:  So we should talk about the little unit you have behind you.



LEO:  Oh, Oscar Vermeulen is awesome.  This all started with Steve and his blinking lights right over his left shoulder there.  Those are - what's that, PDP-8; right?



STEVE:  PDP-8, which was my first computer.  That's what I encountered when I was in high school.  One of the teachers in the math resource center said, "Hey, Steve, I think there's a company over in San Carlos that you might want to check out."  And this was, I mean, this was when Bill Gates was at a school...



LEO:  He was in high school.



STEVE:  ...with more money than mine.  Yeah, we were both in high school.  He had a computer in his school.  We had no computers in our school.  This was 1971.  But so it was just beginning to happen.  Yet this company, Technica Education Corporation, they were in the business of beginning to put little timesharing systems in schools, in elementary and high schools, and beginning to teach programming.  So they had PDP-8s and also little HP mini computers.  Anyway, it was where I first encountered an actual computer and learned assembly language.  And as all our listeners know, I just stuck with that because, if it's not broke...



LEO:  It worked pretty good, yeah.



STEVE:  It worked really well.  So, yeah, so the machines behind me are PDP-8s, which is a 12-bit machine.  And DEC sold - Digital Equipment Corporation - sold a bunch of them, I mean, like really.  It was a very popular solution because they were inexpensive.  And at the time, you needed things to run laboratory equipment and collect data and do little databases.  And everybody was kind of a programmer because there was no readymade software, really.  It was just, you know, "Here's your computer.  Good luck."  And it came with manuals.  Like here's the instruction set.  And so it was, you know, a very different era back then.  But that thing, even though there was an operating system, OS/8, created for it, it came with 4K - 4K - of core memory.



LEO:  Because it was expensive back then.



STEVE:  Oh, my goodness, yes.



LEO:  You didn't put in unnecessary memory.  It costs a lot.



STEVE:  No, 4K.  Well, and 12 bits can only access 4K.



LEO:  Oh.



STEVE:  Remember that 10 bits is 1,000, well, 10 bits is 1,024; 12 bits is 4,096.  That's all you can access is 4K.  Now, that was such a problem that they did sort of the equivalent of what Microsoft, and you remember the EMS deal where you could add some additional - they had a banking system, as in memory banks, which allowed you to have an additional three bits for instructions, an additional three bits for data.  So you could have your instruction - so the instruction pointer was still only 12 bits, so 4K.  But it sort of extended in a bank an additional three bits that brought you all the way up, Leo, to 32K.



LEO:  Whoa.



STEVE:  So, oh my god, what could you ever - you're never going to use all that much memory.



LEO:  Well, not if you keep writing in assembly language.  You probably wouldn't.



STEVE:  Well, and you could do like real work with four or 8K of memory.  Anyway, so the problem was it really was a little underpowered.  The machine that put DEC on the map - they sold a ton of PDP-8s.  But it was the PDP-11.  That was a 16-bit machine.  And these are all, you know, we're all used to hex now, which is to say grouping our binary bits in groups of four bits.  So you have zero through nine and then A through F.



Well, these machines were all octal.  We hadn't figured out yet that octal was wrong.  And so everything was zero to seven and grouping in sets of three.  So that worked well for a 12-bit machine because you just had four groups of three.  It doesn't work so well for a 16-bit machine because you could do five groups of three, but that's only 15 bits.  So you end up with like the first, the top one being either a zero or a one.  But still it was an octal-based machine. That's the PDP-11, which is what you and I both have.  Here's mine, and you've got yours.



LEO:  Here's mine, yeah.  And this is thanks to a crazy Swiss, Oscar Vermeulen; right?



STEVE:  Yup, yup.



LEO:  He's the guy who did your replicas that are behind you, and now he's moved on.



STEVE:  Right.  So as we were sort of talking about this when we were talking about it before, it's actually based on a Raspberry Pi, which gives it more memory than any PDP-11 ever dreamed of having.  So the console is like an I/O peripheral.  I mean, it's got the switch banks and the blinking lights.  And so back then, back in the day, in order to actually load it with software - there, you can see a picture with the various lights lit up on the console.



Back then, these machines had core.  There were later 11s that had RAM in them, dynamic memory.  But the original ones had core memory.  So you'd flip the switches up and down to select a memory location, and then change them again, and then press or actually raise the deposit switch, which would write those bits into that address in memory.  And it would increment the address pointer, the address counter, so that then the next deposit would automatically go into the succeeding address.



And you would put, like, maybe 10 instructions is what you needed in order to create the tiniest program, which was known as the "bootstrap loader," which would read from a paper tape on the KSR-33 or 35.  And so you would start the program.  Then you'd put the tape in, and it would just go [sound effects] to, like, suck this tape in.  And basically all it was doing was reading from the paper tape and storing much more into memory than you could manually toggle in.  So the so-called "bootstrap" was just the barest smallest program that you would put in.



And many of the actual computers at the time, actually most of us memorized the bootstrap because we were having to toggle it in all the time.  Many of the actual machines you would see had them either handwritten or typed on a little piece of paper on the front panel for people that were a little lame and needed a crib sheet in order to remember the instructions, in order to key them in.  But so that little program would then bring in the next bigger program.



And in fact there was also sometimes a three-stage because that second program was still not very sophisticated.  So you'd read that in, and then that would bring in the paper tape loader, and then you'd run that in order to load the operating system.  And it could take two hours to load the operating system, literally feeding in massive rolls or fanfold tape in order to get this thing loaded.  The good news is, is it was nonvolatile memory.  So as long as the system didn't crash, or no one tripped over the cord while it was running, you could generally keep the OS loaded for weeks at a time before you'd have to go and load something again.  So anyway, the point is that we have little emulations.  Basically it's an emulated PDP-11.  And as I mentioned before, Oscar recorded what - did you plug it in?



LEO:  Yeah, and I turned it on.  And, look, I've got lights.



STEVE:  Oh, and they're moving.



LEO:  And by the way, my key works.



STEVE:  Nice.



LEO:  Yeah.  I can turn it off again.  Or maybe it only works once.  Anyway, that's pretty cool.  Yeah, they're moving lights.



STEVE:  Ah, very cool. 



LEO:  Yeah, I don't even have to program in some blinking lights.  He says:  "Think of it as either a very expensive front panel for a Raspberry Pi or your very own mini" - it's running an emulator; right?  So it's actually running a PDP-11, as if it were a PDP-11.  You can log into it by SSH or...



STEVE:  Yeah, there is a whole, I mean, well, and it was on the PDP-11 that Unix was born.  That's, I mean, Unix first ran on a PDP-11.  It was written first in assembly language.  And then of course they thought, okay, this is crazy.  We need a higher level language.  And so that's where C was then, again, the first C compiler was written in PDP-11 assembly language to compile C.  And then they wrote it in itself, and it compiled itself somehow.



LEO:  I'm just going to leave it right here.  This is all I needed to do.  That's perfect.



STEVE:  Yes.



LEO:  It's perfect.



STEVE:  Very nice.



LEO:  It's got some demo running.  It's awesome.



STEVE:  Very nice.



LEO:  Really nice.  And thank you, Oscar, for sending these along.  And once again, if you are interested, Oscar has made already over a thousand of these, which is kind of mindboggling.  I know a lot of our listeners...



STEVE:  Leo, $249.



LEO:  It's very affordable. I mean, that's great.



STEVE:  It's crazy.



LEO:  If you've got a Raspberry Pi, Raspberry Pi not included, but they're 35 bucks.  You've got them lying around.  I did.



STEVE:  Right, right.



LEO:  It's a simple thing.  Obsolescence.  I would say, if you googled "obsolescence guaranteed," or even "PiDP-11" because it's a Pi DP, you'd probably find it right away.  If he had not built it at Wix, he would be easier to find, but okay.  So there you go.



STEVE:  And just so our listeners know, he is now heading toward a PDP-10.



LEO:  Yes.



STEVE:  Which was the granddaddy of the DEC machines.  That was a 36-bit machine, and it used kind of weird yellow and green coloring.  Whereas this is sort of two-tone, like burgundy and red.



LEO:  You can see the influence of the LCARS interface on the Star Trek stuff.



STEVE:  Yeah.



LEO:  I mean, this is awesome.  So I have the source code for Unix.  Could I just enter it all in?



STEVE:  It's probably already in there.  I mean, it might be - I don't know whether it's running Unix or running - but there is for the Raspberry Pi all of the DEC software.  So it's a full PDP-11 emulator.  And you could run, right in PDP-11, assembly language, or they've got compilers and Fortran and C and everything.  So watch out.  It is just sort of a blast from the past.



LEO:  There's a whole manual which he sent.  And both, let's see, you can hook it up - oh, there's a PiDP-11 Google Group.  He says there's a few of the old-timers in there, some of the designers of the PDP-11.  He did a lot of software archeology, he calls it, to get this working.  And yes, it comes running a PDP emulator on the Raspberry Pi.  So that's pretty sweet.  And I love - this is fine.  The demo program...



STEVE:  It's done.



LEO:  It's done.  I don't need to enter in any blinking lights.  I got the blinking lights.  I'm going to leave it right there in front of you.



STEVE:  So when you saw the Picture of the Week, you busted up because...



LEO:  Oh, I had to laugh.



STEVE:  And I mentioned this before on the podcast.  It's one of my most enduring pet peeves are people who show charts where, I mean, the whole reason you have a chart is to show proportionality, to give a sense for, if a line is going up, then the amount it goes up as it moves horizontally matters.  I mean, that's why you're doing it.  So it's about proportionality.  Well, if you don't make it very clear where, well, the way the vertical axis is anchored, then the scale of its change has no meaning.



LEO:  Sometimes people do that in a nefarious way to make the scale of change look greater.



STEVE:  Yes.  Yes, exactly.  In fact, that's the reason that it's annoying is you'll see someone that shows, like, okay, rate at which national debt is increasing, and it's like, whoa.  You go, holy crap.  But then it's like, wait a minute.  It started, like the Y scale is already really high.  So it's basically the amount of change that occurred over time.  It's like, well, yeah, okay, fine.  Whereas, if you zero base it, it looks like a much flatter line.



LEO:  Well, whoever did this illustration apparently had a poll; right?  Is it misleading to truncate the Y axis?  In other words, not to have it zero based? And the results are in.  Notice, by the way, that this graph starts at 98%.



STEVE:  Right.



LEO:  And so there was 1% no, and 99% yes.



STEVE:  Correct.  Exactly.  So in a properly scaled graph that was based at zero, it would almost be no red slice at all that you could find.  It'd be 1% of the entire height, rather than 50%, as this chart shows.  Anyway, someone sent this...



LEO:  That's just great.  I just love that.  I love that.



STEVE:  Someone sent it to me some time ago, and it was in my bag of things to show on the podcast, so I wanted to get around to it.



So looking back as we have been at previous Second Tuesdays of the Month patches, by any measure, last week's Patch Tuesday was a doozy.  And it turns out "doozy" is a word.  I was glad to know that because I thought, there's just...



LEO:  Playing Scrabble, is that how you found it?



STEVE:  Well, I had to look.  I looked it up because my spellcheck said, "What?  Are you drunk?"



LEO:  There's no "doozy."



STEVE:  Wikipedia or wherever it was says, oh, yeah, doozy.  So I'm now having to train all my various spellcheckers.  It's like, let me just type "doozy" without bothering me.  So these Patch Tuesdays seem to be getting larger.  Remember, Leo, those quaint old days a few years ago when a big Patch Tuesday was like maybe 15?  We'd be like, wow, 15 things.  Okay, yeah.  Not recently.  Last Tuesday Microsoft's February patches resolved 74 CVEs and also had three advisories, so a total of 77 problems.  And they encompassed IE, Exchange Server, the ChakraCore, Windows itself, Office, Microsoft Office Services and Web Apps, Azure, Team Foundation Services and the .NET framework.  And of those 74 CVEs, 20 of them are rated critical, with the other 54 rated as important.  And then we've got those three advisories as moderate.



Twenty-one of them were sourced by Trend Micro's Zero-Day Initiative.  Four of the bugs are publicly known, and one is under attack at the time that it was patched.  So it was good to get done.  One of the more interesting things was CVE-2019-0626, which was a Windows DHCP Server Remote Code Execution Vulnerability.  Most of us have DHCP servers looking inwards on our networks.  Although apparently Microsoft's DHCP server has some public exposure.  It wasn't exactly clear to me, although there was some commentary on the 'Net suggesting that this was a wormable exploit.  Well, to be wormable, really it needs to be outward facing, that is, bound.  It has to be a DHCP server that has some presence publicly, although I wasn't able to nail that down one way or the other.



Anyway, the point is that this is one where certainly if you're an enterprise, you know, our DHCP servers are on our - we all have them on our routers looking inward, providing dynamic host configuration protocol, DHCP, to the various machines and IoT stuff and our smartphones and everything we have inside of our LANs.  An enterprise will be using a DHCP server typically also for all of its network.  And even if it's only an inward facing problem, depending upon the size of your company, we know that you can have a disgruntled employee or somebody who's just feeling mischievous and wants to see whether his corporate DHCP server has been patched.



Anyway, the point is this last Tuesday round was huge and not something that an enterprise wants to sit on for long.  And of course for the rest of us there were 20 remote code execution problems that, to varying degrees, affect us.  So definitely not one you, I mean, the problem is we've seen these patches recently causing problems for people.  So the tendency is not to jump on them.  But now we're also seeing "in the wild" exploits happening.  So it's sort of a toss-up.



And Adobe, not to be left behind, weighed in with its own list of, well, they didn't quite match Microsoft, but they came close:  71 bugs.  On the other hand, 44 of those 71 were rated critical, so more than twice as many critical problems for Adobe as Microsoft had.  And of course it's across the spectrum - Acrobat Reader, Flash, ColdFusion, and Creative Cloud.



So once again, just make sure, if you're using Adobe stuff, and if for some reason you're still stuck using Flash because of corporate policy - which generally is the reason that's still being done.  There are corporations that have Flash-based internal things for whom they've lost the source, or the programmers went away or whatever.  You do want to make sure that you're being kept current.  And really you want to try not to have Flash, if possible, available to your browser.  Maybe you just have an internal standalone Flash app that requires that you have Flash in your system.  So anyway, the point is, if you're doing Adobe stuff, it would be a good idea to make sure you are up to date there, as well as for Windows.



Last Friday, February 15th, Microsoft notified the world of an important forthcoming change.  I have the link to their notice, which I read carefully, and it still left me with some confusion.  The title was "2019 SHA-2 Code Signing Support Requirement for Windows and WSUS," which is Windows Server Update Services.  So I'll share what they said, and then we'll talk about it.  They said:  "To protect your security, Windows operating system updates are dual-signed using both the SHA-1 and SHA-2 hash algorithms to authenticate that updates come directly from Microsoft and were not tampered with during delivery."  That we all know.



"Due to weaknesses in the SHA-1 algorithm and to align to industry standards, Microsoft will only sign Windows updates using the more secure SHA-2 algorithm exclusively."  And so here's where it hits half of us:  "Customers running legacy," okay, yeah, legacy - "OS versions" - meaning in Microsoft's consideration - "Windows 7 and Windows Server 2008 R2" - in the case of Windows 7 it's SP1, Service Pack 1, which was the last one that they did for Windows 7; and for Windows Server 2008 R2 it's SP1 or SP2 - "will be required to have SHA-2 code signing support installed on their devices by July" - of this year, by July of 2019.  Microsoft said:  "Any devices without SHA-2 support will not be offered Windows updates after July 2019."



So then they said:  "To help prepare you for this change" - Microsoft writing this said - "we will release support for SHA-2 signing in 2019," meaning before this time.  They said:  "Some older versions of Windows Server Update Services will also receive SHA-2 support to properly deliver SHA-2 signed updates." Then they said:  "Refer to the Product Updates section for the migration timeline.  Starting in early 2019, the migration process to SHA-2 support will occur in stages, and support will be delivered in" - now here's the key - "in standalone updates."



Okay.  Today we're in February; right?  So they said March 12th, 2019, which is the next Patch Tuesday, which is March 12th is March's Patch Tuesday, they said:  "For Windows 7 SP1, Windows Server 2008 R2 SP1," they said, "Standalone updates that introduce SHA-1 code sign support will be released as security updates."  So I read that as saying that they're going to push this out in the normal security update Patch Tuesday.  So we don't have to do anything.



Then they said April 9, 2019, so that was March, so April, which is the - April 9 because April's Patch Tuesday, the second Tuesday of April, they said, for Windows Server 2008 R2 SP2, they said standalone updates that introduce SHA-2 code sign support will be released as security updates.  Okay.  So that's in March and April, successively.  So for most of us, next month, March, for Windows 7, those of us who are still using Windows 7.



So then we have April, which is where servers get updated, Server 2008 R2 servers get updated with SP2.  Then we have May and June and July to make sure we're all up to speed because the August 2019 updates will start being single-signed using SHA-2 only.  So at the moment all of our Windows 7 systems are only aware of code signing using SHA-1. The point is, ending after July, so beginning in August, all of our updates are only going to be signed using SHA-2.



So what's weird is that also in there they talk about Windows 10.  And they said July 16, 2019 for Windows 10 - and that's 1507, 1607, and 1703 - they said Windows 10 updates signatures change from dual-signed to SHA-2 only.  "No customer action is expected for this milestone."  Well, they didn't say no customer action was expected for the other OSes.



So anyway, we'll certainly talk about it next month.  And the question is, will it be checked for us automatically?  Are we all going to get it?  The good news is we'll have plenty of time to make sure we got it because I fully intend to keep using Windows 7 until forever, hopefully, although as I said, I've kind of made peace with 10.  If you strip all the nonsense out of it, you can pretty much live with it.



But anyway, so I just wanted to sort of cover this.  Mary Jo talked about it.  I imagine she'll - she wrote about it last week.  I imagine she'll talk about it on Windows Weekly tomorrow.  Maybe she'll know whether this is just being all done for us, or if we have to go, like, getting it for some reason.  I don't know, when they say it will be released as a security update, that sort of sounds like, okay, fine.  Well, like everything else.  So just give it to me.  But maybe it won't be checked by default, like Silverlight, where it's like no, thank you, please don't give me that.



So we'll find out next month.  We'll certainly talk about it because, as we know, Windows 10 just crossed over the 50% mark.  So the majority, and certainly all of enterprise, lots of enterprise, are still hanging back on Windows 7 and trying to decide how much they're willing to pay for continued service of updates from Microsoft.



I got sort of a smile out of the fact that there's been much  talk made of the security of apps in the Windows Store.  And of course Paul, I sort of also grinned when he was talking about how much junk is there.  It's just like, oh, come on, really?  It's just nothing is there.  Symantec found eight apps on Microsoft's app store that mine Monero without the user's knowledge.  Symantec wrote that:  "On January 17th we discovered several potentially unwanted applications" - and I got a kick out of this acronym.  PUA is the acronym we're using now, P-U-A.



LEO:  What is it?



STEVE:  A Potentially Unwanted Application.



LEO:  Oh, PUA, yeah.  A PUA.



STEVE:  A PUA, yeah.  You don't want of those PUAs.  It's too bad there wasn't some way to make it PU.  That would have been better.  On the Microsoft Store that - so what they're doing is of course, as we know, Symantec wrote:  "Surreptitiously use the victim's CPU power to mine cryptocurrency."  And they said:  "We reported these apps to Microsoft, and they subsequently removed them from the store."



Now, despite the fact that there were eight apps, it turns out they were all from one source.  So they figured, okay, we'll just kind of spread this around eight apps, so maybe somebody will find one, but they won't find the others.  Symantec wrote:  "The apps - which included those for computer and battery optimization tutorial, Internet search, web browsers, and video viewing and download - came from 'three developers':  DigiDream, 1clean, and Findoo."  In total...



LEO:  You got some DUA from Findoo.  And PUA.  Findoo put PUA on your computer.



STEVE:  PUA from Findoo, that's right.  It says:  "In total, we discovered eight apps from these developers that shared the same risky behavior."  Uh-huh, because they're actually all one entity.  "After further investigation," they wrote, "we believe that all these apps were likely developed by the same person or group."  The one thing I found interesting here was they found a new way, well, new in this instance, for getting malicious JavaScript into these things.  These are running as - what was the term?  I wrote it here somewhere.  Oh, yeah.  The eight apps fall under the category of progressive web apps which are installed as a Windows 10 app running independently from the browser in a standalone that's the wwahost.exe process.  So basically it's a JavaScript web app, but doesn't run with all the browser window dressing.  It just looks like it's a freestanding thing.



Anyway, the point is that Symantec said:  "As soon as the apps are downloaded and launched, they fetch a coin-mining" - so they don't include coin mining in the app, which is a little clever, so they could be scanned and checked and so forth.  And then, oh, yeah, look.  They look fine.  Let them on.  They fetch a coin-mining JavaScript library by triggering something known as the Google Tag Manager (GTM), which is from Google's domain.  The Google Tag Manager is a tag management system created by Google to manage JavaScript and HTML tags used for tracking and analytics on websites.  So it's not malicious.  It's not bad.  It's just basically sort of a form of CDN.



So these apps use GTM, the Google Tag Manager, to obtain the JavaScript from Google, where essentially it's being hosted.  The mining script gets activated and of course begins using the majority of the computer's CPU cycles to mine Monero for these bad guys.  Although the apps appear to provide privacy policies, just as if to look complete, there's no mention of coin - yes.  Like, hey, we have a privacy policy.



LEO:  Can we just copy something from the Internet and put it in there?  Yeah.



STEVE:  Exactly.  We got this from something else.  There's no mention of coin mining, no surprise, not that anyone read the privacy policy anyway.  They ought to really say, "and we're going to mine cryptocurrency..."



LEO:  They should because nobody reads them.



STEVE:  Yeah, exactly.



LEO:  Then you're off the hook.



STEVE:  Yeah, exactly.  Anyway, the apps were published between April and December of 2018, most toward the end of the year.  So one or I guess some of them had been around for a while.  And  Symantec wrote that even though the apps were on the App Store for a relatively short period of time, a significant number of users apparently downloaded them.



They said:  "Although we can't get exact download or installation count, we can see that there were almost 1,900 ratings posted for these apps.  However," as they note, "app ratings can be fraudulently inflated, so it is difficult to know how many users really downloaded them.  When each app is launched, the domain Fast-search.tk, which is the domain for the Fast-search Lite app which is hardcoded into each app's manifest file, is silently visited in the background and triggers Google Tag Manager with the key GTM-PRFLJPX," whatever that is, "which is shared across all eight apps."  Thus the common link that ties them together and lets Symantec believe that these were all coming from the same place.



So Symantec, just to be clear, explained that "GTM is a legitimate tool that allows developers to inject JavaScript dynamically into their applications.  However, GTM," Symantec observes, "can be abused to conceal malicious or risky behavior since the link to the JavaScript stored in GTM is https://www.googletagmanager.com/gtm.js?id=," and then that GTM ID, which was that GTM-PRFLJPX, the point being that it's being hosted from Google.  So trust us.  Anyway, yeah, no.  So just another clever way for bad guys to get code loaded into their apps after it's been downloaded and waiting, essentially deferring the actual load of the payload until it's actually run.



And as we were talking about recently, this is one of the behaviors that Google will be deliberately blocking in Android apps because it's just too dangerous to allow the app to defer loading most of itself until after it's already passed through Google's scrutiny.  What does it mean, then, to check the app for its behavior, if you're allowing the app to load more of itself or change itself after the fact.



Anyway, Symantec informed Microsoft and Google about these apps' behaviors, and Microsoft removed them from the store, and Google pulled the mining script from the Tag Manager.  So anybody who already had it will no longer have it running on their machines because it won't be able to any longer load its mining code.  And as we said, there is incentive to create these things because it is generating so much money for the bad guys.  They're just - it's not hurting people except that it's like, gee, I'm running my battery optimizer, and now my machine is running really slow.  Yeah.  Exactly.



So the next major release of Chrome and others through 2019 will be offering an experimental new lockdown technology which Google has dubbed "Trusted Types."  This is aimed at developers rather than end users.  End users, we get the benefit indirectly of, if this succeeds as Google expects, solving or potentially eliminating the by far number one biggest problem with client-side cross-site scripting vulnerabilities.



Our browser ecosystem, the way this has evolved, it's become incredibly complex over time.  It's based on a textual, loosely typed and interpreted authoring environment which results in code which on one hand easily does what developers want, but unfortunately will also obligingly do what developers never intended.  In this environment where precoded modules are being sucked down from other sources with abandon in order for various functions to be just looped into existing code the developers use, developers are rapidly gluing together complex functions built up from code they've never seen.



So this makes it incredibly difficult to program defensively.  And the result is what we've got today, which is extreme vulnerability to very subtle and very difficult to spot cross-site scripting vulnerabilities.  And as I said, they're very difficult to find for developers, even in the unfortunately rare event that developers are looking for them.  And unfortunately, it's often the bad guys who are doing the looking and finding.  So without getting too far down in the weeds of the details, the problem that Google is fixing, or proposing a fix for, arises because elements which make up web pages are part of the so-called DOM, the D-O-M, the Document Object Model, which is a formal description of a web page's structure, which over time sort of has been reverse engineered.  We ended up just like sort of with an ad hoc, here's a web page, and then we said, okay, let's formalize this.



So it's been formalized and carefully designed.  And now we have a rigid, well-defined model for documents.  And the specifications for the sources for the objects that populate the document object model are strings.  And it turns out that all too often the composition of these strings, and by that we mean like URLs, HTML URLs, the composition of these strings can be subject to malicious manipulation, with the result being that foreign content from some malicious source can be injected into the innocent page's document object model and then made to execute in the context of this model.



So again, the problem is that simple strings can be used as the source specifiers of these objects.  And as Google terms it in their security blog posting about this, they said they are insecure by default.  So yes, they're easy to use, but also they are the number one source today of web-based attacks.



So what Google's developers are proposing is the addition of a new and optional argument to an existing browser header.  We already have something called CSP for our browsers, Content Security Policy, where the page author is able to put constraints on the things that the page is allowed to do.  The browser receives this as a CSP, a Content Security Policy header.  And it's up to the browser to enforce what the web server has said it wants enforced.  So this is adding a new feature known as Trusted Types.



So a web server that wanted to solve the problem of its pages being abused would add a content security policy of Trusted Types.  And if that's present, then all of those places where a string could have been used and easily misused, will refuse to accept a standard string, just a random string as their argument.  Today they accept anything you give them.  If you say no, we want to enforce Trusted Types on this page, then they will require an explicitly specified policy, basically a template and policy to be created for that object's specification.



So essentially, I mean, it is tightening down on this kind of freewheeling, anything goes approach that we've had, which has unfortunately resulted in a huge amount of abuse.  Like this cross-site scripting is the number one problem we have on the web today.  Google says:  "In practice, modern web applications need only a small number of these policies."  They wrote in their explanation of this:  "The rule of thumb is to create a policy where the client-side code produces HTML or URLs - in script loaders, in HTML templating libraries, or HTML sanitizers."  The point being that most of the time you don't need to dynamically produce those.  Where you do, if you enforce Trusted Types, then you'll need to explicitly control what those things are able to do.



And they said:  "All the numerous dependencies that do not interact with the document object model do not need the policies."  And they said:  "Trusted Types assures that they can't be the cause of the cross-site scripting."  So with the next release, I think it's 73 of Chrome, this will begin to appear.  And 73, 74, 75, and 76 are the releases Google has slated for this year, where they're going to begin to roll this out and allow developers to explore this.  They also have - they've come up with some JavaScript code that non-Chrome browsers can use in order to essentially retrofit this functionality in so that developers can experiment with it in places other than Chrome.



And the hope is, first of all, that there's no gotchas that have not been foreseen; that developers will appreciate the leverage that this provides, that is, essentially developers would like to have it, but the browsers don't offer it at the moment, and the servers are not suggesting that it be enforced.  So all these things sort of have to happen.  Then of course finally, if this proves out, then this would move into standards mode and ultimately get adopted by Firefox and hopefully, well, actually, if Edge is going to be adopting the Chromium engine, as we believe it will be - well, we know it's going to be - then before long that'll be able to be used by Windows 10 users with Edge on Windows, as well.



So again, who would have thought that a company that began as a search engine - of course Google has since expanded to be much more.  But I just, you know, we're covering a lot of things that Google is doing to make the web a safer and more secure environment for us.  And I just - I take my hat off to them.  I'm glad that there's somebody who's being as proactive as they are.  So yay.



And Leo, I think you told me how to pronounce this name.  It's Xiaomi, X-I-A-O-M-I.  Xiaomi I think is how we decided that should be produced.  Who would have imagined that Xiaomi's electric scooters would be vulnerable to remote hijacking.  Well, or on the other hand, who would have thought they wouldn't be, given that we have such a problem securing things these days.



Last Tuesday researcher Rani Idan, who is with Zimperium - Zimperium we've spoken of often.  They're the somewhat controversial zero-day exploit promoter and reseller, reselling being the controversial part of this.  They are offering a lot of money, and we were wondering who's buying these zero-days.  Anyway, they disclosed a vulnerability which is present in the Xiaomi...



LEO:  Xiaomi.



STEVE:  Oh, Xiaomi, Xiaomi.  Thanks.  Xiaomi.  The Xiaomi M365 electric scooter.  If you happen to have a Xiaomi M365 electric scooter, be careful.  Until it's patched, and now it's widely known publicly, the demo that the Zimperium guy showed only showed them stopping the scooter because that's the least horrific thing that could be done is that it stopped when it's not moving at a stoplight.  And, oh, look, you try to go, and it doesn't.  The guy had to pick his scooter up and carry it across the street.  You really would not want to be on one of these that accelerates to maximum speed and refuses to stop, which is something that can happen.  So the story is...



LEO:  Oh, no.  It's a scooter, so you can hop off.



STEVE:  Well, yeah, but still, I mean, it's going to catch you by surprise.  Do you know how fast they go?  I don't have any idea.



LEO:  Not that fast. 



STEVE:  At full speed.  Okay.



LEO:  I mean, unless you can break through the speed limiters, which you might be able to do.  I don't think they're - I don't know how they work, but...



STEVE:  Yeah.  So this scooter interface app is running on the phone.  So somebody has a phone which they authenticate to with a password to the scooter's app.  What the Zimperium guy found was that the phone app has no authentication protocol to the scooter.  So there's, like, it's wide open.  The scooter itself has no authentication.  So anybody who reverse engineers the protocol which the scooter uses to talk to its app, and apparently it's wide open, also it works at a hundred-meter distance, which suggests it's WiFi and not Bluetooth.



So the point is that a bad guy, knowing that these scooters have no authentication, and I don't think there's - oh, yeah, Zimperium did create proof-of-concept code, which they used in a video that they produced to cause this scooter to stop.  Anyway, there is the ability to completely control acceleration and any other features that the scooter offers through this phone.  Unfortunately, Xiaomi said that they'd been made aware of the findings, and said that it was a known issue internally, and blamed it on third-party products.  So no patches available.  It's not known whether any one is forthcoming.



So just a little heads-up, if you are using one of these M365 scooters.  I mean, the chances are you yourself are going to be targeted seems remote.  And as you said, Leo, you can hop off.  But still, it would be startling to suddenly have your scooter accelerate and ignore its controls, which apparently you can do through this software.  And now the word is out.  Oh, and the software that these guys came up with scans for available scooters to take over.  I don't know how it...  



LEO:  I'd be more concerned if it were, like, Jump or one of the, you know, Scooter, Scoot, or Skip, or Lime, one of those big scooter rental companies because those are all over some cities.  And it is an issue.



STEVE:  Well, and apparently the article did mention, although I didn't have it in the notes, that they rent these scooters.



LEO:  Oh, okay.



STEVE:  That is, so these scooters are being used.



LEO:  Oh, Lime uses these scooters.  Oh, never mind.  Then it is a huge issue.  I take it all back.  Lime is all over the place, those Lime scooters.  And if Lime uses these, then what that means is, well...



STEVE:  It's bad.



LEO:  Lime could fix it.  But that explains it because most scooters are not controlled by phones.  That seems like an unusual way to control a scooter.



STEVE:  Right.



LEO:  Okay.



STEVE:  Right.  It'd be a handgrip, right, that you use to control a scooter, yeah.



LEO:  Yeah, yeah.  Why would you control it with a phone?



STEVE:  For whatever reason...



LEO:  But on these rental scooters you need Android or, I mean, smartphones to connect to it and unlock it.



STEVE:  Right, right, right.



LEO:  Which means it has an interface, and obviously that's what they've worked around.  So, yeah, I would guess this is in fact rental scooters.  And that's a lot more serious.  You get a lot of people don't know what the hell they're doing on them.



STEVE:  Exactly.  And, I mean, I can imagine, first of all, you wouldn't be expecting this.  And if you're on it, and it suddenly takes off at speed, it's like, you know, you're going to be startled.  And, now hopping off, I don't know how fast they go, but to hop off, if you're moving, you've got to like hop off and run; right?  Otherwise you'd fall.



LEO:  Oh, yeah.  They don't go faster than 20 miles an hour, but that's still pretty fast.  Apparently Lime and Bird, which is the other big company, use these 365s.



STEVE:  Oh, boy.



LEO:  On the other hand, that means Lime and Bird have heavily customized them and probably could fix this themselves in the interface, I would think.



STEVE:  I hope they...



LEO:  To hear there's no authentication, that seems really dopey.



STEVE:  Isn't that bizarre?  And that's what Zimperium found.  



LEO:  At least XOR the stuff or something.  Or something.



STEVE:  Yeah.  Like whoops.



LEO:  Whoops.



STEVE:  Okay.  So some interesting research, chipping away at Tor.  We love Tor.  Talked about it in the beginning.  Very, very clever technology.  If anyone isn't - if we have a new listener who's not up to speed on Tor, we did a whole podcast on it, The Onion Router, TOR, was what that originally stood for.  For some reason they decided they didn't want to be an acronym or an abbreviation anymore.  So they said no, no, no, we're just Tor.  We're not The Onion Router.  It's like, okay.



The ultimate vulnerability to Internet anonymity, as we've talked about before, is the fact of there being any traffic flow between endpoints.  We can encrypt the traffic so we cannot read it.  We can encrypt the flow's metadata so that we cannot learn anything about what the content is, other than perhaps its size, of the data being moved.  And we can introduce camouflage packet padding and short-term aggregation of packets to clump them together to hide their individual presence.  There's like all these things that we can do.  But eventually, the same packet needs to come out of this cloud of obfuscation that went in, so that any entity that can see enough of the cloud's perimeter can get some sense for which endpoints are communicating.  So they may not know what is being said, but the existence of a flow is a form of metadata that reveals something.



So in this new - and we've talked about how, in the worst case, looking at the flow in and out of the Tor cloud, even though packets go in, and they jump around a lot inside, and no node can see what the other node is doing and so forth, I mean, it's beautifully designed. It comes out the other end eventually, after all of the wrappers of encryption have successively been removed from this packet.  And if somebody suspects that two endpoints are communicating, then it's much easier to confirm a suspicion than it is to use Tor to generate the belief of endpoints connecting because it is a big, large, active network now.  Still, confirmation is easier than coming in cold and trying to figure out who's talking to who.



We have another piece of this that's been done.  In their paper titled "Peel the Onion:  Recognition of Android Apps Behind the Tor Network," four Italian researchers at the Sapienza University in Rome have chipped away a bit more at the protections offered by Tor.  What we have now they have coined the term "application deanonymization attacks."



They wrote:  "In this work we show that Tor is vulnerable to application deanonymization attacks on Android devices" - and they just used Android, iOS would be the same - "through network traffic analysis.  For this purpose, we describe a general methodology for performing an attack that allows us to deanonymize the apps running on a target smartphone using Tor which is the victim of the attack.



"Then we discuss a proof of concept implementing the methodology that shows how the attack can be performed in practice and allows us to assess the deanonymization accuracy that it's possible to achieve.  While attacks against Tor anonymity have already gained considerable attention in the context of website fingerprinting in desktop environments, to the best of our knowledge this is the first work that highlights Tor vulnerability to apps deanonymization attacks on Android devices.  In our experiments we achieved an accuracy of 97%."



Okay.  So their 15-page paper goes into every detail of what they did - lots of pretty graphs - what they observed and what they found.  The bottom line is, not surprisingly, mobile applications tend to be quite chatty.  They assume and use the connectivity that they have freely.  And in the process they give their identity away, even when the traffic is protected and encapsulated and wrapped up by multiple layers of onion.



Again, the fact of traffic occurring creates a fingerprint.  So even with a Tor-enabled smartphone, what these guys demonstrated is that it is possible, after some training, and given some universe of applications whose behavior becomes known to figure out what a person is doing on their smart phone simply by passively sniffing the WiFi traffic or cell traffic that the phone is using.  So again, you know, it's not a decryption compromise.  It's not a huge deal.  And by itself I would argue that it isn't.



But I think it's an interesting finding, and it succeeds in breaking one of the privacy assumptions that a TOR user might have, which is that it's not possible for someone listening to their communications to know what they're doing.  They don't know what they're saying.  But they can identify, these guys demonstrate with a high degree of accuracy within a given set of applications, what it is that the user is doing.  So we sort of come back to where we began, talking about how the Internet works and packet routing, noting that none of the Internet was designed with security and even privacy in mind.  Back then, the fact that it even worked was amazing.  And we were later able to retrofit privacy on top of it by encrypting the communications between endpoints.



But the inherent nature of the whole packet-switching approach to the Internet and the way traffic routes really makes complete privacy difficult.  We can hide the content, I mean, robustly hide what we're seeing.  But the fact that communication is occurring, they're just - without, I mean, even when you go to all of the trouble that Tor has gone to, there are still ways to chip away at that.



LEO:  This isn't what Tor is for.  Tor is to anonymize you so it's not clear where the traffic's coming from.



STEVE:  Ah, that's a very good point.



LEO:  It doesn't in any way encrypt the traffic.  If you wanted to...



STEVE:  Well, no, it absolutely encrypts it.  And that's the four layers or more of onion.  But oh, I see, it's like a VPN.



LEO:  Use a VPN if you want to encrypt.  If you use Tor plus a VPN, I don't think this technique would work.  Of course they don't know where the traffic's coming from, but they can see the apps that you're using because apps have a certain signature.



STEVE:  Correct.



LEO:  That doesn't seem - that seems not why you're using Tor.  Now, they're right when they say "privacy assumptions a Tor user might have" because that's...



STEVE:  Correct.



LEO:  That may be a mistake in the Tor user; right?



STEVE:  Exactly.  Exactly.



LEO:  Okay.



STEVE:  Although Tor does encrypt in the same way that a VPN does.



LEO:  It does.  You don't need a VPN with Tor?



STEVE:  Well, correct, because the client - what happens is the client decides what nodes they're going to move through, so it picks like four Tor nodes and gets the public keys from each of the nodes.  Then it successively encrypts in the reverse order that it's going to transit, the packet's going to transit.



LEO:  Oh, right, of course, yes.  It wraps it and then unwraps it.



STEVE:  Right, right, right, right.  And so in that sense it's very VPN-esque, the idea being, though, that by getting lost among the Tor nodes, it's not easy to backtrack and for someone to see where you're connected to.



LEO:  Right.  And of course, like VPN, it's not encrypted once it emerges from the Tor exit node.



STEVE:  Exactly.  Exactly.



LEO:  Okay.  Yeah, of course it's encrypted.  What am I thinking?  That's the technology, yeah.



STEVE:  Right.  So CNBC, it occurred to somebody there to do some little digging around and wonder, almost a year and a half after Equifax, whatever happened to that data?  It's been just a month shy, 17 months, of a year and a half since the 2017 Equifax data breach, which we later found to have compromised the data of nearly 148, I think it was 147.9 million individuals, which is nearly every adult in the U.S., with more than 45% of the U.S. population directly affected by the incident.



Anyway, an investigative report by CNBC found that, somewhat surprisingly, none of the data has turned up on the dark web.  According to CNBC's "threat hunter" sources, they talked to a bunch of people, it's increasingly looking like it was, as they called it, a "spy job," meaning carried out by a nation-state, not criminals who are aimed at ID theft or short-term financial gain.



Threatpost covered the CNBC research, and they asked our well-known friend Troy Hunt of HaveIBeenPwned fame.  Troy said:  "Frankly, I think the bullet point under the headline about it being state-sponsored explains a lot."  He said:  "Actors at that level aren't looking to cash data in for a few bitcoin," he said, "and it wouldn't surprise me in the least if that data never sees the light of day."  He said:  "Just think about how many incidents must be out there already that we may never know about simply because those responsible have no reason to advertise it."



And I think that actually, you know, that's a very salient point.  We know of breaches when we see them turn up and then can reverse engineer where they came from.  Sometimes corporations proactively learn of a breach and then disclose it because they have due diligence.  But we also wonder how many corporations think, ooh, shoot, yikes, somebody was in here, but as far as we know nothing got out.  Well, they may not know because they're not logging, or they don't have any incidence management or who knows what.



Anyway, I just thought it was fun.  We've talked about Equifax a lot.  And it could easily be that some nation state just wanted to collect this data and then use it selectively in targeted attacks for identity theft against specific individuals, rather than, as Troy said, just selling it for a few hundred or thousand dollars on the dark web.  It could potentially be worth a lot more to a nation-state.



So two years ago the U.S. Congress's House Energy and Commerce Committee requested that the GAO, the U.S. Government Accountability Office, prepare a report about Internet privacy.  The report ended up just happening, last week it was finished, titled "Internet Privacy:  Additional Federal Authority Could Enhance Consumer Protection and Provide Flexibility."  I read the entire thing, just to see if anything really jumped out at me that I should share.



The first hearing on the report is scheduled for a week from today, which puts it on the 26th of February.  So there will be a hearing in Congress's House Energy and Commerce Committee.  And essentially the GAO is strongly recommending that the Federal Trade Commission, the FTC, be put in charge of overseeing the enforcement of Internet privacy under some legislation that they're also proposing.  The FTC already technically has responsibility, but it has lacked sufficient legislation and essentially sufficient enforcement power to allow it to do much.  For example, in the entire history of the Internet, the FTC has been involved in only 101 Internet privacy-related cases, despite wide privacy abuse being reported by users and of course the media.



The GAO in their report argues that this new proposed legislation should give the Federal Trade Commission much more teeth in dealing with privacy abuse.  They talk about in the report, of course, the Facebook/Cambridge Analytica relationship, the dangers to user privacy due to lack of regulation and oversight in many different fronts.  They talked about the ever-growing Internet of Things sector, where devices collect massive amounts of information without the user's knowledge; automakers collecting data from smart car owners; the lack of federal oversight over companies that collect and resell user information, the lack of protections for mobile users against secret data collection practices.



So I think we're just at the beginning of the process.  This report will go to this committee.  There'll be some discussion.  This will probably take a while.  We don't know how quickly the U.S. Congress will act.  But in the current climate it does feel like the world is ready, or the U.S. is ready, for something that is much more substantial.  The GAO report does interview and talk about "stakeholders," as they put them, on both sides of this.  Of course the argument is, oh, this is going to stifle innovation.  This is going to keep us from doing the things that we need to do, the argument being, well, okay, we'll take that into advisement and into consideration.  I'm sure there will be lots of lobbying back and forth.



What I do hope is that whatever it is we get is clear and is not just some murky, weakened legislation that just allows lots of lawsuits to get filed so that everything gets up and just gets tangled up in court and in appeals and ultimately burdening the Supreme Court as a consequence of legislation not being made very clear.



The GAO ended up saying:  "Congress should consider developing comprehensive legislation on Internet privacy that would enhance consumer protections and provide flexibility to address a rapidly evolving Internet environment.  Issues that should be considered include which agency or agencies should oversee Internet privacy; which authorities an agency or agencies should have to oversee Internet privacy, including notice-and-comment rulemaking authority and first-time violation civil penalty authority; and how to balance consumers' need for Internet privacy with industry's ability to provide services and innovate."  So no surprises there.  But we've seen what the EU has done with the GDPR, and I wouldn't be surprised if this is the beginning of something similar for the U.S.  I think we need it.



So I was talking about this version of Windows 10 that was available to enterprises.  And I got a note from a Todd Fillingim in Mississippi.  The subject was "Windows 10 Long-Term Service Channel," and he said "(Branch)."  He said:  "Steve, I've been listening to SN since Episode 1..."



LEO:  Yay.



STEVE:  Yes, "...but have never submitted feedback.  Love the show, and thank you for doing it for so long.  In SN-701," so last week, "you mentioned a version of Win10 that has everything stripped out of it, available to enterprise customers."  In fact, that's why I'm bringing it up because we know that enterprises are being reluctant to make this move.  So this is an option I want to make sure our enterprise listeners are aware of.



He says:  "You mentioned a version of Win10 that has everything stripped out of it, available to enterprise customers.  I work for a small utility company, and we've just recently started testing this version for use on our control room operator workstations."  So you could imagine they want a solid and stable version of Windows.  "Microsoft used to call this version the Long Term Service Branch (LTSB), but recently changed it to Long Term Service Channel (LTSC).  It is a version of Win10 that has none of the extra features of the regular version, and every build has a commitment from Microsoft for 10 years of security patches and support."



He says:  "Interestingly, everything you read on this version from Microsoft tries to talk you out of using this version.  They REALLY," he has in all caps, "don't want anyone using this for desktop use."



LEO:  How funny.



STEVE:  And there's a link to it.  So he says:  "Thanks again for the podcast."  And of course it's not available to end users anyway, so it's going to be for enterprise deployment.  But depending upon how you feel about having Candy Crush on your employees' machines, enterprise customers, this may just be the thing for you.



Owen LeGare in Davis, California.  Ah.  Many people picked upon this, my confusion over MiB for megabytes.  We've talked about it a couple times.  I've never really paid that much attention to whether it's a thousand or 1024.  I mean, as a programmer I do.  I know exactly what I'm talking about.  And I sort of thought I remembered the convention once, like in KB, if the "B" was capital, it was bytes, and if it was lowercase it was bits.  But of course this is different.  This is, is it a binary or a decimal thousand.



LEO:  And you wouldn't, by the way, need this at all.  It's always going to be binary, except that stupid hard drive manufacturers decided to inflate their capacity.



STEVE:  To inflate, exactly.



LEO:  And say it in decimal.



STEVE:  Exactly.



LEO:  And that's so annoying that we have to even say KiB or MiB.



STEVE:  I know.  So just, you know, M-O-U-S - anyway.  So Owen said - and everybody, thank you for your email and your tweets.  I just chose this one.  So the point is, he says, he uses his flash drive as an example, where it was done exactly as we were saying, where it's 8.13GB or 7.57GiB because you're using 1024 versus a thousand.  Anyway, thank you for the clarification.



LEO:  I think we said that on the show.  We clarified.



STEVE:  Yeah, we did.  I just didn't want - because I wanted everybody to know that we'd gotten the message.



LEO:  I'm trying to remember if there was a - I believe there was a consumer action against using these decimals because at a certain point they started to indicate decimal, or as he did, this is decimal.  This is binary.  I think that somebody - I remember there was a lawsuit or there was some consumer action that said you can't be using these inflated numbers without saying.



STEVE:  Right.  I got an interesting note from someone calling himself Dr-Mosfet on Internet isolationism.  He said:  "During the last few podcasts you've discussed various forms of Internet isolationism."  He said:  "If Elon Musk's Starlink becomes a reality, it could shake things up in both good and bad ways."  And I just wanted to just point that out.  That's an interesting note is that, if you can get the Internet via satellite, then it does, I mean, then it's in the air.  And it makes it much more difficult.  I guess totalitarian regimes could shoot the satellites down or refuse to have them hovering over them.  I don't know how that would work in real life.



But I just wanted to note that I've just been entirely focused on wired gateways and borders and routers and so forth.  But radio works.  Radio Free Europe back in the day was a way of communicating in a way that was difficult to prevent.  So this sort of, you know, the Internet via satellite is an alternative.  And then, finally...



LEO:  Yeah, and I suppose anybody could get access anywhere in the world, as long as it's up in the air.  Maybe that's Elon's secret plan.  I wouldn't be surprised.



STEVE:  I wouldn't either, although this other plan of his has got me a little worried.  But we'll be talking about that momentarily.  I know that you talked about DuckDuckGo in Safari, Leo.  I don't think, I mean, I know you mentioned it everywhere else.  I don't think we talked about it on this podcast.  So Walter Pereira in Brazil, he said:  "Hi, Steve.  I'm a more than decade-long user of SpinRite."  He says:  "The best piece of software I have, period.  Not once failed me.  As far as I remember, the only software I own with that record.  Thanks so much."  He said:  "But I'm writing to suggest a very small correction on what was affirmed" - and this was, you know, when there was a mention made...



LEO:  You didn't say it, though; right?



STEVE:  No, I didn't.



LEO:  So you don't need to run corrections for me. 



STEVE:  Well, but our listeners just wanted to make sure...



LEO:  Oh, god, I know.  I got a million letters.  Don't worry.



STEVE:  Exactly.  Yeah.  So it is possible to set up DuckDuckGo as your search provider for Safari.  You're not stuck with Google.



LEO:  No.  But nobody does.  That was the real problem, which is Google made $9 billion last year by being default, yeah.



STEVE:  Exactly.  Okay.  So this was last Thursday, which it was a blog posting by a company, OpenAI.com, which summarizes the recent work of six AI researchers at this company that Elon Musk founded.  And I did note - I didn't have a chance to track it down, ran short of time.  But apparently he's distancing himself from this.  And I don't know what that means, like why he's not happy with...



LEO:  He was not the only founder.  A bunch of people raised money - Reid Hoffman, Elon, Peter Thiel - to do safe AI.  They were very concerned about what everybody else was doing.  So they created OpenAI to create safe AI.  And I think half the mission of OpenAI is to warn people about AI.



STEVE:  Well, yeah.  And in fact, what they created they decided they couldn't have open because - as a consequence.



LEO:  Well, okay.  So, yeah, I agree, and I can't wait to hear what you talk about here.  But remember they're trying to stop certain kinds of AI.  And since they never released this, this all could be speculative.



STEVE:  Well, okay.  So I guess we should have seen this coming.  For quite some time, as we know, we've had bots roaming the Internet, talking about Google, before then Alta Vista.  And those bots were roaming the Internet, indexing its pages, so in some sense reading them, but just to index them.  And we've also had bots warring with each other.  And then of course eight years ago we sat silent, witnessing the stunning performance and success of IBM's Watson on "Jeopardy!."  It was somewhat intimidating since I couldn't have answered most of those questions, and this thing just, like, knew.



LEO:  Yeah, but Steve, if you had access to Wikipedia instantaneously, you could have answered all those questions, too.  I don't know why people were so surprised at the ability to answer.  To me the amazing thing was the ability to understand.



STEVE:  Well, that's what I was just going to say, to understand the questions, yeah.



LEO:  Because if I typed in the - Wolfram Alpha will do it.  Google will do it.  It's understanding and giving it back in speech.  It was a little bit of a parlor trick, if you ask me.



STEVE:  Yeah.  Anyway, it impressed me.  So of course it's one thing for a stylish black cube to sit there, and of course now it's doing cancer diagnosis and other expert system AI-ish things.



LEO:  Failing at, I should point out.



STEVE:  Oh, is it?



LEO:  Yeah.



STEVE:  Oh, okay.



LEO:  Watson has proven to be not nearly as good at some of the things it's been tasked with as they were hoping.



STEVE:  Well, and of course we know that, much like security problems, they only ever get better.  They never get worse.  So now what we have is text-generating AI bots which could endlessly roam the Internet, reading what they find, and pouring out nonhuman generated content under the guise of being human.  Which, okay, for quite a while we've been living with photos being photoshopped, I mean, so much so that that's now a term, "photoshopped."  And of course more recently we've been growing aware of the possibility that someone's clearly recognizable voice might be saying something that they never uttered.



So this spoofing is entering the mainstream.  But in the jargon that we use on this podcast, we would say that those attacks were targeted, you know, photoshopping a specific image or targeting someone's voice.  So while that's disturbing, their reach is inherently limited.  But that's not the case if text-generating AI bots are let loose on the Internet.



Okay.  So this blog posting was titled, modestly, "Better Language Models and Their Implications."  And in their descriptive blurb - and I think I have a link, yeah, I have a link at the top of this for anyone who's interested in more details.  In their descriptive blurb they said:  "We've trained a large-scale unsupervised language model which generates coherent paragraphs of text, achieves state-of-the-art performance on many language modeling benchmarks, and performs rudimentary reading comprehension, machine translation, question answering, and summarization  all without task-specific training."  Meaning just without knowing a priori anything specific.



Okay.  So I want to share a sample of what this thing puts out.  But first I'm going to share a little bit of sort of their context so we get a sense for what they did.  They wrote:  "Our model, called GPT-2" - which is the successor to GPT, so this is GPT-2 - "was trained simply to predict the next word in 40GB of Internet text."  They said:  "Due to our concerns about malicious applications of the technology, we are not releasing the training model.  As an experiment in responsible disclosure, we are instead releasing a much smaller model for researchers to experiment with, as well as a technical paper."  And I'll just note it doesn't matter.  I mean, if it can be done, it will be done.  And this demonstrates it can be done.



They wrote:  "GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset of eight million web pages.  GPT-2 is trained with a simple objective:  predict the next word, given all of the previous words within some text.  The diversity of the dataset causes" - so if any of us, I know, Leo, you have toyed with, as I have, Markov chains.



LEO:  Yeah.  In fact, it's surprisingly easy to do.



STEVE:  Yes.  An incredibly large, highly trained Markov chain where you have probabilities of going from this node to the next one and so forth.  So this is sort of that kind of thing, where it can be used to create a predictive model.  So they said:  



"The diversity of the dataset causes this simple goal" - that is, predict the next word, but with a huge model.  "The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains.  GPT-2 is a direct scale-up of GPT" - so basically they said, wow, this is working.  Let's make it bigger, see what happens.  They said:  "...with more than 10X the parameters and trained on more than 10X the amount of data.



"GPT-2 displays a broad set of capabilities, including the ability to generate conditional synthetic text samples of unprecedented quality, where we prime the model with an input" - and we're going to demonstrate that in a minute - "prime the model with an input and have it generate a lengthy continuation.  In addition, GPT-2 outperforms other language models trained on specific domains like Wikipedia, the news, or books, without needing to use these domain-specific training datasets.  On language tasks like question answering, reading comprehension, summarization, and translation, GPT-2 begins to learn these tasks from the raw text, using no task-specific training data."  That is, just pour the text in.  "While scores on these downstream tasks are far from state-of-the-art, they suggest that the tasks can benefit from unsupervised techniques, given sufficient unlabeled data and computation."



Okay.  So they said, finally:  "GPT-2 generates synthetic text samples in response to the model being primed with an arbitrary input.  The model is chameleon-like.  It adapts to the style and content of the conditioning text.  This allows the user to generate realistic and coherent continuations about a topic of their choosing, as seen by the following sample."  Okay.  So here is a sample of this machine.



LEO:  Can I just ask one thing?



STEVE:  Yeah.



LEO:  Did they do this in front of people?  Did they, like, say, okay, come in here, we're going to run this, and we're going to let you watch it, it'll be generated, and these people, whoever they are, verify that it was actually generated by this software?  I mean, they haven't released the software.



STEVE:  No.



LEO:  How sure are we that this actually happened?  So you're trusting OpenAI, but OpenAI's mission, sworn mission is to warn people about the dangers of AI.



STEVE:  Well, they've done that for me, at least.  So, okay.



LEO:  I just don't - I don't know.  I'm not sure I believe it.  But go ahead.



STEVE:  You know me.  I'm gullible.  I believe this.  So here's what was given to the machine:  "In a shocking finding, scientists discovered a herd of unicorns living in a remote, previously unexplored valley in the Andes Mountains.  Even more surprising to the researchers was the fact that the unicorns spoke perfect English."  Okay.  So that was given.



LEO:  That was a human.  That was a human wrote that.



STEVE:  A human wrote that as input to this system that has been generated.  It then produced - and they did say it took 10 tries.  So there was a button where they said, "Try again."  "Try again."  Okay.  But this is what it produced.  And again, remember, these things only get better.



LEO:  Now, is this the one that learned everything from Reddit?  I think the content that it absorbed was Reddit pages, I think I remember.



STEVE:  There was something about Reddit.  Anyway, so again, okay.



LEO:  Which might help you understand what you're about to hear. 



STEVE:  So they put in:  "In a shocking finding, scientists discovered a herd of unicorns living in a remote, previously unexplored valley in the Andes Mountains.  Even more surprising to the researchers was the fact that the unicorns spoke perfect English."  That was put in.



This machine then produced:  "The scientists named the population, after their distinctive horn, Ovid's Unicorn.  These four-horned, silver-white unicorns were previously unknown to science.  Now, after almost two centuries, the mystery of what sparked this odd phenomenon is finally solved.



"Dr. Jorge Perez, an evolutionary biologist from the University of La Paz, and several companions were exploring the Andes Mountains when they found a small valley with no other animals or humans.  Perez noticed that the valley had what appeared to be a natural fountain, surrounded by two peaks of rock and silver snow.



"Perez and the others then ventured further into the valley.  'By the time we reached the top of one peak, the water looked blue, with some crystals on top,' said Perez.  Perez and his friends were astonished to see the unicorn herd.  These creatures could be seen from the air without having to move too much to see them.  They were so close they could touch their horns.  While examining these bizarre creatures, the scientists discovered that the creatures also spoke some fairly regular English.  Perez stated:  'We can see, for example, that they have a common language, something like a dialect or dialectic.'



"Dr. Perez believes that the unicorns may have originated in Argentina, where the animals were believed to be descendants of a lost race of people who lived there before the arrival of humans in those parts of South America.  While their origins are still unclear, some believe that perhaps the creatures were created when a human and a unicorn met each other in a time before human civilization.  According to Perez, 'In South America, such incidents seem to be quite common.'



"However, Perez also pointed out it is likely that the only way of knowing for sure if unicorns are indeed descendants of a lost alien race is through DNA.  'But they seem to be able to communicate in English quite well, which I believe is a sign of evolution, or at least a change in social organization,' said the scientist."  Okay, now, Leo, I am stunned that some software can produce something as coherent as that.



LEO:  We don't see all the versions it produced that they didn't give you.



STEVE:  That's true.  Again, be skeptical.  Just wait a year.  As I said,  these things only get better.  So they wrote, and I'm glad they appreciate this, they said:  "As the above samples show, our model is capable of generating samples from a variety of prompts that feel close to human quality and show coherence over a page or more of text."  They said of policy implications:  "Large, general language models could have significant societal impacts, and also have many long-term applications.  We can anticipate how systems like GPT-2 could be used to create AI writing assistants, more capable dialogue agents, unsupervised translation between languages, better speech recognition systems.



"We can also imagine the application of these models for malicious purposes, including the following, or other applications we can't yet anticipate:  generate misleading news articles, impersonate others online, automate the production of abusive or faked content to post on social media, automate the production of spam and phishing content."



They wrote:  "These findings, combined with earlier results on synthetic imagery, audio, and video, imply that technologies are reducing the cost of generating fake content and waging disinformation campaigns.  The public at large will need to become more skeptical of text they find online, just as the 'deep fakes' phenomenon calls for more skepticism about images."



They wrote:  "Today, malicious actors  some of which are political in nature  have already begun to target the shared online commons, using things like 'robotic tools, fake accounts, and dedicated teams to troll individuals with hateful commentary or smears that make them afraid to speak, or difficult to be heard or believed.'  We should consider how research into the new generation of synthetic images, videos, audio, and text may further combine to unlock new, as-yet-unanticipated capabilities for these actors, and should seek to create better technical and non-technical countermeasures.  Furthermore, the underlying technical innovations inherent to these systems are core to fundamental artificial intelligence research, so it is not possible to control research in these domains without slowing down the progress of AI as a whole."



So anyway, when I saw where we are with something that a machine can produce, and what this does is, I mean, we have a system with the Internet where there is an assumption we have today that what we're reading, the postings we're reading, are from people.  And I think that will soon no longer be the case.  Bots have demonstrated themselves to be fabulously, I mean, astonishingly capable of indexing content on the Internet.  No one can imagine life without a comprehensive Internet index, which we have entirely thanks to the fact that bots, I mean, like the Internet is their domain.  It's where they live.  And we're transacting, I'm transacting constantly with textual content that I have known is generated by other people.  That is entirely, I think, endangered.  And that represents a sea change in the way we think of the content on the Internet.



LEO:  Okay.  You obviously didn't see the article from two years ago about the Washington Post's robot reporter that did 850 sports articles without anybody noticing they were auto-generated.



STEVE:  Wow.



LEO:  That's because it's in a domain, very specific domain.



STEVE:  Yes, yes.



LEO:  So this is a technology that the Washington Post calls "Heliograf."  And the thing is, it's very easy to write sports reports because it's really all very formulaic.  Interestingly, just to clarify, the way they got the data for this robot, the OpenAI robot, is they did go to Reddit, but they used outbound links from Reddit, so to a variety of sources.  But they used Reddit to qualify the sources because it had to have a score of three karma or better.  So they said, well, that means humans picked this as a source of good content.  But it also explains why this OpenAI AI was very good if you asked it questions about Miley Cyrus, Brexit, or "Lord of the Rings."



STEVE:  Although it got her age wrong, I think.  I thought it was - I think something that it was referring to as seven years old.  And so...



LEO:  I'd be more nervous about it being used to generate, almost to use signal jamming to generate a lot...



STEVE:  Leo, it's going to.  That's my point is that, when we have...



LEO:  Not that it's good content, particularly, but just that it's just content.



STEVE:  Yes, flood.



LEO:  Flooding.



STEVE:  A flood of junk, I mean, where it's just - it cannot be - it can no longer be discriminated.  This is awful.



LEO:  Yeah.



STEVE:  This is awful.



LEO:  It's just the Internet.  And you're right, it's coming soon to a common thread near you.  On the other hand, there's plenty of humans that can generate endless amounts of crap all by themselves.



STEVE:  No, that's just it.  Automation.  I mean, it is volume, unfortunately.  It's a flood.



LEO:  What a world.  What a world.  But that's what we cover here on Security Now!.



STEVE:  Oh, Leo.  Oh, lord.



LEO:  I think this is a great topic to talk about in general, authenticity on the Internet.  I think it's a great topic because it isn't just text.  It's as you said, deep fakes.  It's photos.  We've already crossed that line with photos.  Any photo can be...



STEVE:  Yup.



LEO:  And videos, too, actually.  We know videos and photos can easily be humbugged.  And I think the general populace kind of understands that now.  It used to be "photographic evidence" was deemed, well, that's proof positive.  I think people now know, oh, that can be retouched.  So much retouching's been done on magazine covers for 50 years...



STEVE:  Well, Leo, the term "photoshopped," it just it means...



LEO:  Comes from that, yeah.



STEVE:  Yeah, that a picture was faked.  It was edited.



LEO:  I remember how discouraged I was when I found out they can essentially do the same thing in movies.  They can take a movie star who has some wrinkle or mole or acne and literally clean it up.  And they use automated tools that do it for every frame.  They do it on a few frames, and then for every frame.



STEVE:  Wow.  Wow.



LEO:  And Alex Lindsay told me this has been widely used for years.



STEVE:  Well, and we've been talking also about digital actors coming where...  



LEO:  Yeah, that's obviously - yeah.  Did you see "Alita" yet?  One of the characters is a fully digital actor.



STEVE:  I did unfortunately see "Alita."



LEO:  Who talked you into that?



STEVE:  I just, no, I wanted to see what Cameron was up to, and Lorrie and I suffered through it.  The technology was good; but, oh boy, yeah.



LEO:  So she was a human wearing a suit during all of the scenes and had two cameras on her face that were painted with dots.



STEVE:  That was the "Avatar" technology that Cameron developed for "Avatar."



LEO:  Right.  Nothing's real.  Except you and maybe me.



STEVE:  We're staying real.



LEO:  The jury's still out on me, though, I've got to say.



STEVE:  Keeping it real.



LEO:  We do Security Now! every Tuesday, 1:30 Pacific, 4:30 Eastern.



STEVE:  Whatever day it is.



LEO:  I don't know what day it is.  You kidding?  Are you kidding me?  I have no idea.  That would be 21:30 UTC Tuesdays, Tuesdays, Tuesdays.  Please stop by, say hi.  You can watch the live stream of the show as we produce it, TWiT.tv/live, or listen.  We have audio, two different audio streams you can listen to.  And if you do that, you might want to join us in the chatroom.  That's where everybody else is who's watching and listening live:  irc.twit.tv.



Steve puts a whole bunch of nice show notes together, transcriptions of the show, and audio, as well, on his website, GRC.com.  In fact, when you get to GRC, you might as well pick up a copy of SpinRite, the world's best hard drive and recovery and maintenance utility.  That's the only thing Steve charges for.  Everything else on that site, so much great stuff.  Find out how SQRL's going.  Learn about passwords.  There's health information there.  There's lots of good stuff.  GRC.com. 



@SGgrc is Steve's Twitter handle.  You can DM him there, or leave a message at GRC.com/feedback.  We have audio and video of the show at our website, TWiT.tv/sn.  And of course every podcast application in the world, including Spotify, has a copy of Security Now!.  And if you subscribe, you'll get it the minute it's available of a Tuesday evening.  And that's my spiel.



Oh, don't forget.  We want to make sure we get everybody, listeners of every show to answer questions on our TWiT survey.  I don't want any one show to be overrepresented.  So we know a lot of you listen to Security Now!.  We do this survey once a year.  I know, if you listen to the show, you are privacy nuts.  We do not collect personal information.  We don't ask for your email address.  It's on Survey Monkey.



Survey Monkey will record your IP address to keep you from answering the survey more than once.  I think you can figure out how to get around that.  But that's the - and we don't get that information, nor do we want it, because really what we're looking for is an aggregate of all listeners.  But I do want to make sure that everybody from Security Now! weighs in because you're probably a little bit of a different group than some of the other shows:  TWiT.to/survey19.  We do this once a year, and it helps us both sell advertising, but also plan our programming for the future.



STEVE:  And you're saying that our listeners' propellers are wound a little tight, Leo?  Is that what you're saying?



LEO:  No.  In fact, I'm thrilled they are.  I'm thrilled they are.  It's great.  And it's one of the reasons - nobody who watches or listens to any of our shows is unaware of the privacy implications.  And that's why we need to do the survey, because we don't collect information about you in any way, at any time.  And so the survey is the one time once a year we get to find out in aggregate what your interests are and so forth.  So thank you for doing that.  I appreciate it.  Not an obligation.  Don't worry about it.  No salesman will call.



Thank you, Steve.  Have a great week.  We'll see you next time on Security Now!.



STEVE:  Thanks, buddy.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#703

DATE:		February 26, 2019

TITLE:		Out in the Wild

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-703.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we discuss a number of ongoing out-in-the-wild attacks, along with a bunch of other news.  We have another early-warned Drupal vulnerability that has immediately come under attack in the wild, and a 19-year-old flaw in an obscure decompress for the "ACE" archive format, which until a few days ago WinRAR was supporting to its detriment.  Microsoft reveals an abuse of HTTP/2 protocol which is DoSing its IIS servers.  Mozilla faces a dilemma about a wannabe Certificate Authority, and they also send a worried letter to Australia.  Microsoft's Edge browser is revealed to be secretly whitelisting 58 web domains which are allowed to bypass its "Click-to-Run" permission for FLASH.  ICANN renews its plea for the Internet to adopt DNSSEC, NVIDIA releases a handful of critical driver updates for Windows, and Apple increases the intelligence of its Intelligent Tracking Prevention.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  The show "Out in the Wild" talks about a bunch of new vulnerabilities out in the wild, including a terrible Drupal flaw.  He'll also talk about how Apple's doing a great job with privacy in Safari, and a shocking file hidden away inside your Windows 10 that doesn't do what you think it should do.  It's coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 703, recorded Tuesday, February 26th, 2019:  Out in the Wild.



It's time for Security Now!, yes, indeed, that show you look forward to all week long, with Security Now!'s head honcho, the man in charge, Steven Gibson.  Hello, Steve.



STEVE GIBSON:  Yo, Leo.  Great to be with you once again.  Episode 703 and counting down to 999.  No, just kidding.  So there was no huge single piece of news that happened in the last week, but there are a number of ongoing attacks in the wild.  So I titled this week's podcast "Out in the Wild" to just sort of say, well, yes, unfortunately, this is what's happening right now.  And for some interesting reasons.  It's always now a consequence of a disclosure of a vulnerability that is then jumped on immediately by the bad guys.



So we're going to discuss, as I mentioned, a number of these ongoing, out in the wild attacks.  And we have a bunch of other news.  We've got another early warning Drupal vulnerability that - it's one of the sources of these problems that immediately came under attack in the wild.  Believe it or not, a 19-year-old flaw in an obscure decompression DLL for the ACE archive format.  And Leo, I don't think I've ever used ACE.



LEO:  I remember ACE, though.  That's pretty old.



STEVE:  You do?  Yeah, yeah, yeah.  And until a few days ago, WinRAR - which is my standard archiving tool.  I'll talk about why a little bit later.  But it was supporting this ACE archive format to its detriment.  It no longer does.  But we definitely need, if we've got a lot of people here who are also WinRAR users, as I am, they need to update.  And we'll talk about that.



We also have Microsoft's revelation of an abuse of the HTTP/2 protocol which is being used to DDoS, well, actually not DDoS, just DoS.  A single connection can bring down an IIS server.  Mozilla is facing a dilemma about a wannabe certificate authority that it doesn't really know what to do about.  And they separately sent a worried letter to Australia, sort of complaining about, well, not even sort of, clearly complaining about the recently enacted legislation about privacy.  Microsoft's Edge browser was revealed to be secretly whitelisting 58 web domains which were allowed to bypass its click-to-run permission for Flash.



ICANN has renewed its plea for the Internet to adopt DNSSEC.  And NVIDIA released a handful of critical driver updates, mostly for Windows, although Linux, because there's some overlap in drivers, gets updates too, although the problems for Linux are not nearly as bad as they are for Windows.  And Apple in the beta of 12.2 of iOS, and also the Safari for the next macOS, is increasing the intelligence of its Intelligent Tracking Prevention.  So lots of stuff to talk about.



And earlier this week, actually it wasn't early this week, it was late last week, but previously, I had run across - a friend sent me a link to a tweet that just blew my mind on Twitter.  So it's apropos because we're going to be talking about cookie things and Intelligent Tracking Prevention with Apple.  And this was a WebKit guy commenting about something that they have seen which just I couldn't believe.  So a fun Picture of the Week, and then lots of great news.



LEO:  Nice.  Shall we do the Picture of the Week?



STEVE:  Well, sure.  This was a tweet from one of the Apple engineers on the WebKit team.  And I just had to shake my head.  We'll be talking about this at the end of the podcast.  But he tweeted - this is Don Marti.



LEO:  Oh, yeah.  I know Don Marti, yeah.



STEVE:  "We've investigated reports of news site subscribers getting spuriously logged out, and found that trackers were adding so many cookies that the news site's legitimate login cookie got pushed out."  That's just...



LEO:  Wow.  I didn't even know that could happen.



STEVE:  ...so wrong.  Yeah.  There is, I mean, but it's Ks.  I mean, it's thousands of characters, but there is a limit on the size of the cookie header.  And if you have enough trackers, I mean, and I'm seeing the number 70.  We'll be talking about there was an instance of 70, seven zero, seven times 10, 70 tracking cookies that just - they were hogging all of the available cookie space with lots of data, which is ridiculous also.  A cookie doesn't have to be big, it just has to be unique.  So it could be, you know, 12 characters.  But no.  We're just going to not - we don't care.  We're a tracker.  We don't care.  And as a consequence, people were like, wait a minute.  I thought - why do I have to log in again?  Turns out too many trackers just pushed the legitimate cookie off the end.  Oh, it's just - it's so wrong.



LEO:  Wow.



STEVE:  Anyway, Apple has improved the intelligence of their Intelligent Tracking Protection, and we'll be talking about that at the end of the podcast.



In the meantime, once again, Drupal has trouble.  I'll start with the takeaway, like the most important thing I could say about this to any of our listeners who are responsible for Drupal sites is you want to be absolutely sure that you get their email.  This time they released the news of a forthcoming critical release only the day before.  So the announcement was on February 19th, which was, what was that, last Tuesday.  Yeah, it was a week ago.  Highly critical, 20/25, critical release.  They said there will be a security release of 8.5.x and 8.6.x on February 20th, so the next day, between 1:00 p.m. and 5:00 p.m., they said America/New York, and then they gave the UTC, 18:00 to 22:00.  To see this in your local time zone, refer to the Drupal Core Calendar.  The risk on this is currently rated at 20/25, highly critical.



So, okay.  So the point is that, because of the fact that this is a PHP-based content management system, it is ridiculously easy for bad guys to reverse engineer any change that they make.  But I'm kind of getting ahead of myself.  So first of all, not all configurations are affected.  That was their statement.  And they said:  "Reserve time on February 20th" - meaning the next day from the date of this release - "during the release window to determine whether your sites are affected and in need of an immediate update.  Mitigation information will be included in the advisory."  And unfortunately the mitigation information was wrong.



And then they said:  "Contributed module security updates may also be required.  If you're running Drupal 7, no update is required, but you may need to update contributed modules if you are using an affected module."  They said:  "We are unable to provide the list of those modules at this time."  Oh, and they wanted to make it clear, because they figured this would generate a lot of concern, they said:  "Neither the security team nor any other party is able to release any more information about this vulnerability until the announcement is made."



So in other words, don't bug us.  We're making everybody wait because, again, unfortunately the way the industry is developing is that we are now seeing attacks pouncing on updates and turning them into, well, attackers pouncing on updates and then turning them into viable attacks with increased speed because they recognize there is some window of opportunity.  And the window, depending upon the environment, it could be open for years, as we have seen, or it could be hours, minutes, days.



Oh, I was confused by my own notes because I said hopefully if you're running Drupal 8.5x or 8.6x, this is old news to you, meaning to our listeners, because you are on the mailing list, you did get the notice of this, and you made some time on Wednesday of last week, which was the 20th, in order to respond.  Two days after Drupal's disclosure, two different sites published proof-of-concept code.  And the day after that, using one of those proof of concepts as its foundation, attacks against Drupal sites began.



Yesterday, Monday, Imperva Security summed up their observations for the preceding two days where they watched attacks against their customers whose networks they were monitoring as part of their service.  They said:  "Latest Drupal Remote Code Execution" - and I should mention, yes, it is a remote code execution flaw - "Used by Cryptocurrency Miners and Other Attackers."  Imperva wrote:  "Another remote code execution vulnerability has been revealed in Drupal," they wrote, "the popular open source web content management system.  One exploit, still working at the time of this writing, has been used in dozens of unsuccessful attacks," they write, against their customers, with an unknown number of attacks, some doubtless successful, against other websites.



So there are two vulnerabilities, and we've talked about this  before.  It was some data in the queries that were not properly sanitized, which allow bad guys to essentially inject commands through the open and exposed Drupal interface.  Unfortunately, what Drupal said was that - and these are REST modules, which use HTTP protocol.  So HTTP, we often talk about GET and POST, and there's also PATCH and PUT and DELETE and a few other less used verbs.  Well, Drupal said that, if you disabled PUT, PATCH, and POST, then you were fine.  It turns out that the GET request was able to perform this so that Drupal's mitigation advice was incorrect in this case.



So what the bad guys do is, all they have to do, and in fact from one of the reverse engineering sites we know this is what they actually did, was they "diffed," as the jargon is.  They looked at the difference between the pre- and post-patched source, immediately spotted what it was that was changed, and said, oh, and then went about designing an exploit.  Which is far easier because, unfortunately, we have, as I said, a PHP source base release, rather than, for example, in the case of a Microsoft patch, it's some DLL, and it's necessary to go in and have a much higher level of reverse engineering skills in order to figure out what it was that got changed.



The compiler that produces the DLL may also tend to rearrange things if it's set up to do so or chooses to or if it's got optimization turned on.  So it can help to obscure what's going on.  It's just much less easy to do that when you're patching source code in PHP, which is ASCII.  It's just text.  And so you can run some software that finds the differences between the various files and easily design an exploit around that.  So that's what these guys did.  They diffed 8.6.9 and 8.6.10, and then designed an exploit.



So Drupal has built a very nice system, but it does suffer from having, first of all, a large exposed attack surface, I mean, it sort of has to by design.  That's what it's for.  And it's written in PHP, which means that changes to it are easy to spot.  So anyway, as I said, the takeaway for our listeners is, if you are responsible for Drupal, be very, very sure that you are subscribed to their email list, and that you take these notices to heart.  And they're now, I mean, I remember the last advisory from them, what was it, maybe three or four months ago, Leo?  They gave the industry several weeks of notice.  They said, you know, at some point in the future, and they told us when.  But there was a larger window.  I don't know...



LEO:  This is a zero-day.  That's the problem; right?  It's already been in the wild.



STEVE:  Well, it actually took three days from their...



LEO:  Oh, it's a three-day.



STEVE:  Yeah.  So I'm a little curious to know why they rushed this out.  Maybe they were not sure that they had containment of the nature of the problem and so they thought, okay, we just can't sit on this for too long.  And so what that says is that this is not something where you want to be lazy about even checking your inbox if you might be getting incoming mail from Drupal, if you have the ability to flag email for your emergency attention by the email's source.  I would put Drupal on that list of mail you want to see because right now they're only giving, I mean, if this is an example, they're giving people 24 hours to get ready.  And it's not like, oh, yeah, we'll get around to updating this soon.  I mean, you really need to do it quickly.  So there's the first of our out-in-the-wild problems.



LEO:  And we'll be updating.  We are Drupal, and I love Drupal, but we have professional maintainers who are keeping an eye on it.  Any open source project's going to have this problem, PHP or not.  If you can read the source of the patch and compare it to the previous code, you'll be able to see it immediately.  The only reason Microsoft doesn't is because they're not open.  It's a binary blob; right?



STEVE:  Right, right.  Well, in fact it's a little different because the deliverable is PHP.  I mean, it would be...



LEO:  Yeah, but you could do - I think you do obfuscated PHP.  I'm not sure, but I think, just like with JavaScript, you know, you can obfuscate it.  But it doesn't matter because the source, it's open, and so the source is going to be somewhere, I think.  So, yeah, I think any patch in open source has this problem.



STEVE:  So WinRAR is my favorite archiving tool.  With a little bit of tweaking, it's able to produce significantly smaller compressed archives than, for example, ZIP or many other tools.  Yesterday, meaning Monday, what, the 25th, security researchers at 360 Threat Intelligence Center detected an in-the-wild malspamming email campaign which is distributing a malicious RAR archive file that exploits what was only a few days old, a newly discovered - you know, the press is a little confused about this.  This is one of these deals where you can't simplify it or it becomes inaccurate because technically it isn't a WinRAR vulnerability, although that's how it's being painted, because WinRAR supports a large array of archive formats.



This ACE format was developed 20 years ago.  It hit its peak popularity in 1999 to 2001.  It was developed by a guy named Marcel Lemke and later bought by e-merge.  It always remained proprietary.  And back then there was some advertising or ad-supported form of it, sort of like, you know, and I remember I got a copy of WinZip once that was also adware.  And I thought, okay, these guys have gone further than I want to go with them.  WinRAR has been well maintained over the years, but it supports many different formats:  CAB, ARJ, LZH, TAR, GZ, TAR.GZ, BZ2, TAR.BZ2, UUE, I remember that one.  Even Java Archives, JAR files.  And it also can open up ISO images, CD images, and allow you to see inside them, as well as 7-Zip and other formats.  And it supports this ACE.  The support for ACE came from a DLL.



And WinRAR is not alone in being an archiver that supports ACE.  There are some others that do.  WinRAR, however, is the most popular, and it's got about 500 million users.  So a half a billion users of WinRAR have this installed on their machines.  So it represents a rather large attack surface for bad guys.  Unfortunately, WinRAR, or I guess maybe cleverly, WinRAR looks at the content of the file rather than the file extension.  So you can be a victim if you open a file that says .RAR, thinking it's safe, when in fact it's a .ACE format.  WinRAR will look at it, say oh, that's actually a .ACE format.  And what happens is it invokes this separate DLL.



The point I was making was that, because this was a proprietary compression archive format, nobody had access to the source for this.  And in some of the coverage of this, the WinRAR developers said, yeah, I don't have source for this DLL, so we're just going to remove it.  So that's the upshot of this is that the capability of decompressing ACE archives as of a few days ago has been removed from WinRAR.



So the takeaway for our listeners is, if you, like me, are a user of WinRAR, go update.  I'm not sure whether WinRAR has an update check.  Mine didn't give me any notification.  I went to check, found that there was a new version, and that the website no longer mentions ACE among the archive formats that it supports.  It boasts of everything else, but not that one.



So what happened was Check Point were the ones who found a problem.  And we've also been talking a lot recently about these path traversal problems.  Well, here was another one.  They found an absolute path traversal bug in this unace.dll.  That is, nothing could produce ACE archives, but if you encountered one somewhere - and I don't know.  I never have.  But Leo, you at least remembered them.  I didn't even remember that there was a .ACE.



LEO:  I'm an old-timer.  Oh, wait a minute, so are you.  Never mind.



STEVE:  Yeah.  Even older than you.  So it turns out that all they could do was open them.  Nobody was able to produce them, and nobody cared because, you know, who cares if you can create an ACE archive.  Nobody else can open them.  So anyway, somehow they found this path traversal bug which allowed someone to design an arbitrary code execution exploit which would plant an executable file wherever in the user's system they wanted - for example, in their startup folder, such that next time they logged in, this file that had been maliciously unarchived thanks to WinRAR's support of the ACE format would get executed, and then you could be in trouble.



So we are now at - WinRAR is at 5.70 beta 2.  The news coverage, which was fresh, it was only a day old, mentioned beta 1.  So it looks like there was another change that was made for some reason since then.  I now have 5.70.  I went to look beforehand and afterhand.  And sure enough, beforehand there was an unace.dll.  That's gone after updating to the new version of WinRAR.  So what's happening is there is a spam campaign that is, you know, it's like click here to, what, get the news of the sweepstakes that you just won or something.  And what it's actually doing is, thanks to this bug, it's able to plant an executable file somewhere that will get run next time the user logs in.  You don't want to get bitten by that.  So if you're a WinRAR user, be sure to update.  And of course you should never be clicking links that are offering you sweepstakes winning notifications either way.



Gal Goldshtein of F5 Networks found and reported a vulnerability which affects Microsoft's recent implementation of the HTTP/2 web protocol.  Our listeners know that back in the beginning of the podcast we were at /1.0, and then we went to 1.1, where we've been for quite a while.  Now the world is at 2, all IIS servers running Windows Server 2016, Windows Server versions 1709 and 1803, as well as all Windows 10 versions 1607, 1703, 1709, and 1803.  And you know, Leo, as I was putting this together I was thinking, isn't it nice how Microsoft has executed on their "We're only going to have one version of Windows now."



LEO:  Well, it's the same version.  Just updates.



STEVE:  What a disaster.  That's right.



LEO:  Can't just sit still.  Nobody would want that; right?



STEVE:  Well, but it's necessary now to enumerate this trail of debris that you've left behind.  So anyway, I'm obviously a little biased about this.



LEO:  They've got to polish their turd, my friend.  Somebody's got to.



STEVE:  That's right.  It turns out that by setting up a maliciously crafted HTTP/2 connection, it's possible for any remote attacker with a single connection to bring one of those many IIS servers to its knees, producing what they called, kind of euphemistically, "IIS resource exhaustion."  And it's the CPU that gets exhausted.  It's able to pin the processor at 100% so that it's just essentially stuck in an infinite loop, and nothing else happens.  The server locks up.  No other connections are answered.  And it maintains that state until the connection times out.  The default connection timeout is 120 seconds, two minutes.  However, as soon as the server comes back to life, the bad guy simply initiates another connection, similarly, and brings it right back down again.  So it's possible, with no bandwidth whatsoever, to pull an IIS server down.



Microsoft wrote:  "The HTTP/2 specification allows clients to specify any number of settings frames with any number of settings parameters.  In some situations, excessive settings can cause services to become unstable and may result in a temporary CPU usage spike until the connection timeout is reached" - and I don't mean to sound so gleeful about this - "until the connection timeout is reached and the connection is closed."



The default IIS connection timeout is two minutes.  This makes overlapping connections - oh, this is me speaking now because I did a little bit of research.  I thought, well, how long is a connection, because I thought it was a couple minutes.  The default IIS connection, as I mentioned before, two minutes, allowing a chain of connections in order to keep the machine offline.



In their advisory, Microsoft states that there are no known mitigations or workarounds for the vulnerability.  So essentially it says, you know, the spec says any number of settings frames, with any number of settings.  So probably someone puts in something like two billion, a large positive 32-bit number or something, and it just, you know, it's something that Microsoft's system was not prepared to deal with, and so it collapses it.



They have updates.  There are 14 editions of Windows which are enumerated in their update list, which are covered by a total of four patch editions among them.  So if you are - first of all, technically this affects Windows 10 because Windows 10 has IIS in it.  But no home user typically has a publicly exposed web server.  We're all behind NAT routers and happy.  But certainly Server 2016, and if you are using Windows Server 1709 or 1803, then you do have a publicly exposed presence.  If you're curious why your server's been going offline recently, maybe you already know.  Anyway, there is a patch for this.  So you want to choose the proper one of the four patches and install that.  And then that should get you back up and going again.



So who do we trust to be in our certificate root store?  We've talked about this...



LEO:  Hong Kong Post Office.



STEVE:  We've talked about this quite a lot.  A company called - and here's the first problem - DarkMatter, based in the UAE, the United Arab Emirates...



LEO:  Oh, no, no, no, no.



STEVE:  I know, I know.  Kind of like, okay, if you want to be trusted, you're going to say, hey, let's call ourselves DarkMatter.  It's like, okay.



LEO:  Well, it gets worse.



STEVE:  It does.  They're petitioning Mozilla to include their certificate authority root cert in Firefox.  The problem is, DarkMatter has been known to sell surveillance and hacking services to oppressive regimes throughout the Middle East.  And last month a report by Reuters further described DarkMatter's involvement in helping the Saudi government spy on dissidents.  Reuters said - their coverage was titled "Project Raven:  Inside the UAE's secret hacking team of American mercenaries.  Ex-NSA operatives reveal how they helped spy on targets for the Arab monarchy - dissidents, rival leaders, and journalists."  Okay, and I won't go into any more detail.  For anyone who's interested, it's a fascinating report that Reuters put together, and I have a link in the show notes.



So as we know, I'm a Firefox user, and I don't want any CA root cert from a company who chose to name itself DarkMatter anywhere near my machine.  No thank you.  The only possible reason to be carrying such a certificate is if I were going to be visiting a website whose TLS certificate was purchased from DarkMatter.  So that's a chance I'm happy to take.  You know?  I don't want that cert in my machine.



And it's true that certificate mis-signing is increasingly difficult to pull off in today's world with the degree of welcome certificate issuance oversight that we now have in our industry.  But the benefit, at least to me, and to probably our listeners who are not maybe in the UAE and might visit a website that got their cert from DarkMatter - and, by the way, that's apparently zero right now so it's like, okay, why start? - the benefit seems marginal at best, compared to the risk.



So Mozilla, not surprisingly, is under pressure by the Electronic Frontier Foundation, also Amnesty International and the Intercept, to decline DarkMatter's request.  But DarkMatter, which does have the ability to issue certificates because it is trusted by another well-placed certificate authority, QuoVadis, claims that it has never abused its TLS certificate issuance power to do anything bad.  So there's no technical basis...



LEO:  Of course not.



STEVE:  Oh, yeah.  Oh, not us.



LEO:  No, we never did anything like that.



STEVE:  So there's no technical basis for Mozilla treating it with less trust than other CAs that have applied for this same privilege in the past.  And DarkMatter wishes, apparently, I mean, technically they can issue certs, but their certs are trusted only because QuoVadis has signed their cert, so they're an intermediate.  They wish to move from sort of a second-class CA up to first-class status.  And why not?  Of course they want that.



So concerns are further heightened because Mozilla's list of trusted root certificates is also used by some Linux distros.  That is, that's where the Linux distros get their root store.  So there are fears that, once approved and added into Mozilla's certificate store, DarkMatter would then be able to issue TLS certificates to intercept Internet traffic without triggering any errors or warnings on Linux systems, which are often deployed in datacenters and cloud service providers.



So the point is it kind of - it sort of perniciously creeps out from just being browser authentication to cloud service providers authentication.  So Mozilla has a dilemma.  But there are many who are not the least bit ambivalent.  The EFF's Cooper Quintin said - oh, Mozilla opened a Google Group discussion to sort of air this publicly and get opinions.  And, oh, did they.  The EFF's Cooper Quintin said:  "Given DarkMatter's business interest in intercepting TLS communications, adding them to the trusted root list seems like a very bad idea."  He wrote:  "I would go so far as revoking their intermediate certificate, as well, based on these revelations."



Quintin expanded on his fears in a post on the EFF blog, reminding Mozilla that it went through a similar issue 20 years ago, back in 2009, with CN-NIC.  That's the Chinese government's official certificate authority.  Back in 2009, Mozilla approved CN-NIC as a trusted root CA in Firefox.  Then, six years later, that CA was caught mis-issuing certificates for Google domains back in 2015, which we covered at the time, which allowed threat actors to intercept traffic meant for Google sites - which got CN-NIC banned from most certificate stores.



And the outcry against this CA addition is overwhelming in its support for not doing this.  Mozilla publicly posted that:  "Mozilla's Root Store Policy grants us the discretion to take actions based on the risk to people who use our products.  Despite the lack of direct evidence of mis-issuance by DarkMatter, this may be a time" - you think? - "when we should use our discretion to act in the interest of individuals who rely on our root store."  And to that I say, "Amen."



So, you know, if they feel like for some reason they absolutely have to do this, then I would vote for an option, maybe call it "Allow Sketchy CAs" and have it off by default, or have it produce a warning.  And then, if you're absolutely sure that you want to do this, well, we do know about this root cert which is disabled by default.  And if you're sure you want to turn it on, then okay.  Anyway, it sounds like saner heads are prevailing here, and DarkMatter is going to have to go do something else.  I mean, or keep signing with - and maybe the reason they're not happy developing a business around being an intermediate is that they do feel themselves endangered.  It's like, well, sorry, but it's about trust.  And if they're in the business of intercepting TLS communications, then you don't get to be in the root store.



Meanwhile, Mozilla worries that its employees could be subject to Australia's legislation.  And I guess this was a close reading of this legislation that we've talked about before.  Sophos, their headline, caught my attention and initially puzzled me because their headline was "Mozilla fears encryption law could turn its employees into insider threats."



So last Friday, on the 22nd of February, Mozilla wrote to the Committee Secretary of the Australian Parliamentary Joint Committee on Intelligence and Security.  And they said:  "Regarding Comments for Parliamentary Joint Committee on Intelligence and Security (PJCIS) Review of the Telecommunication and Other Legislation Amendment."



And so Mozilla said:  "Thank you for the opportunity to provide comment as part of your review of Telecommunication and Other Legislation Amendment (TOLA)."  Mozilla said:  "This legislation grants sweeping and dangerous new powers to Australian law enforcement and intelligence agencies and, thanks to the foreign assistance provisions, extends these powers to foreign authorities, as well.  In doing so, this legislation raises grave concerns for the security of Internet users and infrastructure in Australia and abroad, and fails to place appropriate limits on government surveillance.  Given the serious threats to security and privacy posed by this Act, we welcome the Committee's review of this legislation and urge you to move swiftly to ameliorate its harms."



They said:  "Mozilla's mission is to ensure the Internet is a global public resource, open and accessible to all.  Our flagship product is Firefox, which is an openly developed and open source web browser used by hundreds of millions of people worldwide."  I'm one.  They said:  "The Firefox code base is also used for the Tor browser, which allows anonymous browsing.  In addition to protecting the security of our products, Mozilla has influenced core security protocols used in the Internet and backed the adoption of HTTPS, which encrypts website connections to enable more private and secure browsing.  In addition, we have advocated to judges and policymakers in many countries on the importance of transparent and robust government processes to handle security vulnerabilities and surveillance requests.



"As we noted in our submission to this Committee when this legislation was initially under consideration:  'Any measure that allows a government to dictate the design of Internet systems represents a significant risk to the security, stability, and trust of those systems.  Mozilla believes that TCNs or any similar device would significantly weaken the security of the Internet.'"  They said:  "We do not believe that this law should have been passed in the first place; and we believe the best possible path is to repeal this legislation in its entirety and begin afresh with a proper, public consultation."  And then they go on to say that they know that's unlikely to happen, blah blah blah.  They give eight points that they think really need attention.  I'll just focus on the first because that was what created the headline that caught my attention.



They said:  "1.  Clarify that Australian authorities cannot target an employee of a Designated Communications Provider."  And they said:  "Due to ambiguous language in TOLA" - the T-O-L-A that I talked about before, that's this telecommunications and other stuff - "one could interpret the law to allow Australian authorities to target employees of a Designated Communications Provider rather than serving an order on the DCP itself through its General Counsel or otherwise designated official for process.  It's easy to imagine how Australian authorities could abuse their powers and the penalties of this law to coerce an employee of a DCP [Designated Communications Provider] to compromise security of the systems and products they develop or maintain."



And of course we've talked about this back in the Snowden days, the idea that individuals within a corporation could have been subjugated one way or another.  And Mozilla's concerned that this is essentially codifying this in legislation that could allow this to happen.  They said:  "In order to ensure due process, appropriate diligence, and full compliance where appropriate with orders issued under this legislation, we strongly believe that Australian authorities should only serve an order on the Designated Communications Provider (DCP) itself.  Serving an order on an individual employee rather than a DCP would fail to allow a DCP to avail itself fully of the protections afforded under this legislation in regards to consultations, assessments, and legal challenges.



"Further, this potential would force DCPs to treat Australia-based employees as potential insider threats, introducing another vector for compromise that could undermine trust in critical products, incentivizing companies to move critical roles to other localities.  Parliament recognized the wisdom of this limitation in regards to Contracted Service Providers, but not DCPs."



So anyway, Mozilla is point out that, hopefully as an omission that could be corrected, where there was a carve-out for contracted service providers, the legislation needs the same thing for Australian-located employees of non-Australian communications providers like Mozilla because they don't want their Australia-based employees to be subject to the legislation as written.  So anyway, they finished in bold italics, saying:  "We recommend the Committee:  Add a clarification in the Section [and it happens to be] 317B definition of Designated Communications Provider to specify that this term 'does not include a person who performs such services in their capacity as an employee, agent, or vendor of the provider.'"



So anyway, this legislation is still obviously in some flux, and they're entertaining comments, and the industry is responding.  So I salute Mozilla and thank them for adding this clarification, which we need.  Or at least they feel we need.



Okay.  This is a real headshaker, Leo.  It turns out that Ivan Fratric - who we've talked about before, he's at Google's Project Zero - he was curious about a file located in the Windows\system32 directory whose name was edgehtmlpluginpolicy.bin.  So the file was not named in any way to obscure its intent:  edgehtmlpluginpolicy.bin.  But what was obscured was its contents.  For reasons that really could only have been to hide what it was doing, the file contains SHA-256 hashes of domains which are allowed to bypass Edge's click2play Flash blocker.



LEO:  Hmm.



STEVE:  Yeah.  And you should scroll down in the show notes because I have a list of them.  And some of them are a little distressing.  So what we now know as a consequence of Google's Project Zero work is that Microsoft's Edge web browser comes with a deliberately obscured whitelist, specifically, which, now, it's been whittled down as a consequence of Project Zero's disclosure.  Now it only allows Facebook to circumvent the request for user consent with its...



LEO:  But what's StupidVideos going to do?  Or ontvtime.ru?  What are they supposed to do now?



STEVE:  I know.  Gee, they're going to have to ask for permission before they run Flash content in your Edge browser. Oh, boohoo.  Yeah.  I liked dilidili.wang.  Oh, my goodness.



LEO:  I don't even want to know what that is.



STEVE:  Okay.  So back on November 26th, Ivan posted:  "In Microsoft Windows there is a file, C:\Windows\system32\edgehtmlpluginpolicy.bin, that contains the default whitelist of domains that can bypass Flash click2play and load Flash content."  And, I mean, we all know what, I mean, like, how bad this is.  Like Flash can't die soon enough, and for some reason we're postponing its death, what is it, till next year, 2020?  Or is it 2022?  I think it might be 2022.  It's like die now.  Anyway, "to load Flash content without getting user confirmation in Microsoft Edge."



So today's updated version of the previously secret Edge whitelist only allows Facebook to bypass Flash's click2play policy.  So www.facebook.com and apps.facebook.com are the two Facebook domains still allowed to run Flash content without getting users' okay.  So technically, I mean, it was called a "bug report," but it certainly wasn't a bug.  Ivan also highlighted the security implications of even having a Flash autorun whitelist bundled with a web browser.



LEO:  I don't know why anybody would be worried if dilidili.wang could get through, or totaljerkface.com.



STEVE:  Oh, goodness.



LEO:  It's very odd.  I mean, I can understand, you know what, games.aarp.org or Facebook.  I can understand that because they want to make it easy, and those are safe.  And Vudu, maybe.  But honestly.



STEVE:  I know.



LEO:  Some of these it doesn't make...



STEVE:  NSEIndia.com?  It's like, okay.



LEO:  India.com is probably the government; right?  Maybe not.  I don't know who owns it.



STEVE:  Wasu.cn, so there's a Chinese site.  I'd like to know.



LEO:  Microsoft's in there.



STEVE:  What's WGT.com?



LEO:  I don't know.  See, I think a lot of these are kind of, okay, we know these are safe.



STEVE:  Deezer.  Deezer.  At least it wasn't Geezer.



LEO:  It's a music service, actually.



STEVE:  Okay, it wasn't Geezer.



LEO:  Deezer's a music service.  I think some of these it's like, well, you know, we can whitelist these because they're established.  But totaljerkface.com or dilidili.wang?  I mean,  what are those?



STEVE:  Yeah.  And, I mean, I guess my argument is, if you're using Edge, and one of the selling features, not that anyone bought it, but one of the stuffed-down-your-throat features was that it had click2play protection for Flash content.  And users are like, I mean, security-conscious users are like, yes, I want that.  And then there's a - oh, and notice, Leo, this - okay.  So the other thing is that they hid this.  This wasn't ever documented.  It wasn't stated.  And in order for Ivan to determine the list, he had to reverse, he had to crack these SHA-256 hashes of these domain names in order to figure out what it was they were.



Anyway, but it's kind of worse because he says, as I was saying, he highlighted the security implications of having a Flash autorun whitelist bundled with a web browser, especially given the number of Flash security patches issued by Adobe nearly every month.  "A whitelist," he wrote and explained, "is insecure for a number of reasons."



As we mentioned last week, by far the most common and most prevalent problem on today's web are cross-site scripting vulnerabilities.  With this sort of domain name-based whitelist, a cross-site scripting vulnerability on any of the whitelisted sites would allow a bypass by a malicious site of the click2play policy.  And moreover, there are currently publicly known and unpatched instances of cross-site scripting vulnerabilities on at least some of the whitelisted domains.  So bad guys who knew of a Flash vulnerability and who knew of this Edge whitelist could employ a cross-site scripting vulnerability to essentially attack a user who was visiting a site not on the whitelist.



So anyway, as I mentioned above, the big issue reported by Ivan was partially addressed by Microsoft just two weeks ago.  In this month's Patch Tuesday they trimmed the whitelist down to just Facebook.  Oh, and they were also - I also forgot to mention that it also wasn't limited to HTTPS.  So many of these domains did not support HTTPS at all, so you could only go to them without encryption, which allowed man-in-the-middle attacks to easily bypass this click2play policy that Edge was enforcing.  I mean, it was a mess.  So he reversed the hashes, found the 58 domains which were whitelisted.  The good news is, as of a couple weeks ago, they are no longer all present.  So dilidili.wang, which may or may not have supported HTTPS connections, won't be able to do anyone...



LEO:  Nice job, Microsoft.  Nice job.



STEVE:  Nice job.  That's right.  Super secure.  So anyway, Tuesday...



LEO:  What's N/A mean?  Does that mean we don't know what it is, or it doesn't mean anything, or...



STEVE:  Maybe he wasn't able to reverse that one.  Yeah, maybe that's like a mystery.  And how about OK.ru?  It's like, okay.



LEO:  Yeah, that's not okay.  That is not okay.



STEVE:  So Ivan tweeted, he said:  "The default Flash whitelist in Edge," and he has a link, he said, "really surprised me.  So many sites for which I'm completely baffled as to why they're there.  Like a site of a hairdresser in Spain."  And then he has a link.  "I wonder how the list was formed, and if Microsoft Security knew about it."  Anyway, so, yeah.  Good news is, thanks to last Tuesday's update, that list has been whittled down to just Facebook.



ICANN, of course, I love ICANN, I-C-A-N-N, the Internet Corporation for Assigned Names and Numbers, last Friday put out a press release kind of begging for full DNSSEC deployment.  The press release was titled "ICANN Calls for Full DNSSEC Deployment, Promotes Community Collaboration to Protect the Internet."  And I'll share what they said.  It's not very long.  They said February 22nd, 2019, Los Angeles:  "The Internet Corporation for Assigned Names and Numbers believes that there is an ongoing and significant risk to key parts of the Domain Name System infrastructure.  In the context of increasing reports" - and we've covered this, like, maybe about a month ago - "of malicious activity targeting the DNS infrastructure, ICANN is calling for full deployment of the Domain Name System Security Extensions (DNSSEC) across all unsecured domain names."  And I confess I'm guilty.  "The organization also reaffirms its commitment to engage in collaborative efforts to ensure the security, stability, and resiliency of the Internet's global identifier systems.



"As one of many entities engaged in the decentralized management of the Internet, ICANN is specifically responsible for coordinating the top level of the DNS to ensure its stable and secure operation and universal resolvability.  On the 15th of February, in response to reports of attacks against key parts of the DNS infrastructure, ICANN offered a checklist of recommended security precautions for members of the domain name industry, registries, registrars, resellers, and related others to proactively take to protect their systems, their customers' systems, and information reachable via the DNS.



"Public reports indicate that there is a pattern of multifaceted attacks utilizing different methodologies.  Some of the attacks target the DNS, in which unauthorized changes to the delegation structure of domain names are made, replacing the addresses of intended servers with addresses of machines controlled by the attackers."  In fact, we did go into this in detail about two weeks ago.



"This particular type of attack, which targets DNS, only works when DNSSEC is not in use.  DNSSEC," they write, "is a technology developed to protect against such changes by digitally signing data to assure its validity.  Although DNSSEC cannot solve all forms of attack against the DNS, when it is used, unauthorized modification to DNS information can be detected, and users are blocked from being misdirected."



Anyway, they go on.  But we all get the gist of this.  I've talked about DNSSEC for years, and about the benefit that we would have as an industry if and when we finally produce signed DNS.  What it would give us is an incredibly powerful distributed ability to look up information which we still to this day, despite the fact that DNS has been around for a long time, don't have.  So I just - I thought it was good that they're sort of reminding everyone and saying, look, here's a checklist of things to do.  Please make this happen.  And it reminds me that I need to get GRC's DNS zones signed also.  There's just no excuse for not.  I don't remember now what support Hover has for that.  I'll have to take a look at that and see what they do.



LEO:  You know, we register at Hover, but we use DNSimple for our DNS, and they seem to be very complete for kind of industrial-grade DNS.  My guess is that Hover doesn't.  Maybe we haven't covered this, but at some time we ought to.  The DNS Flag Day from ISC, are you familiar with that?  So February 1st, I guess some sites who weren't supporting DNS extensions would stop working on the Internet.  So was it the ISC created DNS Flag Day to let people know.



And but then they did another thing, which was they flagged - they said, look, this site will continue to work.  It's DNSflagday.net.  This site will continue to work, but it's not fully compliant.  And one of the things they said is, if sites aren't fully compliant, won't be able to implement DNSSEC in the long run, I think, was the idea.  I'm looking at this again.



STEVE:  It must just be like ancient BIND DNS servers that do not support...



LEO:  No, no, no, a lot of them don't.  So it's extended DNS, which wasn't part of the protocol.



STEVE:  Right, EDNS.



LEO:  EDNS.



STEVE:  Right.



LEO:  And as you know, it's one of the reasons that you can get DNS amplification attacks, because you can use those extended fields to flood.



STEVE:  Yeah.



LEO:  And Hover does not support the full extended DNS stuff.  And I asked them because Greg Ferro was all over them for it.  So I talked to their guy, and he said, yeah, we've looked at it, of course.  We certainly could implement it.  We decided not to for a number of technical reasons.  One is that it would break DNS for some of our customers.  Then you always want to err on the side of being functional.



STEVE:  Yeah. 



LEO:  And he said most people don't, you know, if you're doing, like TWiT, if you're doing big pulse, lots of traffic DNS, you wouldn't use Hover as your DNS provider anyway.  You'd use, as we do, DNSimple.  You'd use a bigger provider.  It'd be like using your ISP for Amazon.com.  You just wouldn't do that.  So he said we're basically a retail DNS provider.  We provide it as a service for our clients who just want simple websites and whatever.  But we don't really think it's something - we're reluctant to implement it right away.  And I think that, and I may be wrong on this, and I'm looking through the DNS Flag Day site, but I think that the EDNS is a prerequisite for Secure DNS, but I may be wrong on that.  Maybe you know more.



STEVE:  It probably is.  And I think, as I recall now, I mean, I'm still running - I'm running, I think it's FreeBSD version 4.  And BIND.



LEO:  You're using BIND; right?



STEVE:  I'm using BIND, like version 2.



LEO:  Yeah, no, no.  So you're not.  So you're not, yeah.



STEVE:  So I have a newer Unix machine which I just haven't brought online yet.  And that one would be using - I don't think I was using even BIND.  There's a much nicer DNS.  Shoot, I can't remember the name of it now.  But I've got it all set up and configured.  I just haven't deployed it.



LEO:  Well, let me run - you want me to run GRC.com through this tester?



STEVE:  No. 



LEO:  You think it might not be compliant?  No, you know what?  All okay.  Your domain is perfectly ready.  You do not need to worry about DNS Flag Day.  Your DNS administrator is doing a good job.  Send them a sincere thanks.  Are you your - I'm my own DNS administrator.



STEVE:  I am.  Yeah, in fact, so what I do is I run the master DNS, and then I've got a pair of Level 3 servers that are my public-facing...



LEO:  That's probably what they're hitting; right?



STEVE:  Yeah.  Oh, yeah, exactly, yes.  So they're quite happy with me.



LEO:  They say BIND pre 9.14 does not.  Not resolver does.  PowerDNS Recursor 4.2 and earlier and Unbound 1.9 and earlier.  Anyway, Hover was very aware of it.  And they said, you know, given what we do in our business, we're really a registrar, not a DNS provider.  We don't want to break people's DNS.



STEVE:  Yeah.  One of the problems is - now I remember what's going on is that I have to do some rejiggering because GRC has pseudo DNS servers for its domain.  Maybe that would - I have to see whether that would affect us or not.  Because, for example, when the DNS Benchmark checks to see if there's a new version of it - and I did update DNS Benchmark after many years to add 1.1.1.1 and 9.9.9.9 - I actually do it with a DNS query because it's super easy and lightweight.  And so the DNS Benchmark issues a DNS query which receives a response which can change.  And so the whole deal with signing your zone is you can't have your DNS changing because it's, you know, you're basically - you have to have the zone signed as it is and then have the signature verifiable.  So I may have to do some changing.  But I wonder if maybe I could sign it without the variable.  Anyway, I have to look into it because it is, you know, I should practice what I'm preaching, and I'm certainly loving the idea that we'll get DNSSEC widely deployed someday.  That would be great.



NVIDIA has released some updates to their drivers, which if you are especially a Windows user, it's worth doing.  I don't know if NVIDIA drivers will flag their own need to update.  It turns out I am using NVIDIA-based display adapters.  And I went looking, and my drivers were not affected, which is to say, mine were dated 2016, and there's nothing newer available.  But NVIDIA has released, I think it's patches for their drivers covering eight flaws, which can lead to code execution, although not remote code execution because obviously it's a video driver.  I mean, there's no public exposure.  But code execution, escalation of privileges, denial of service, or information disclosure on both Windows and Linux machines, far more critical for Windows than for Linux, but still worth doing.



The exploitation of these problems which have been patched does require local system access.  They're not remotely exploitable.  But bad guys that have some means of running code on a machine could take advantage of these problems, if not patched, to execute code and elevate their privileges to allow them, you know, install a rootkit and so forth.  The CVSS rating system is a 10-point scale, with five of the eight vulnerabilities for the Windows drivers receiving an 8.8 out of 10.  So they're bad.  NVIDIA's own documentation, they said, for example, there's a CVE-2019-5665 from their page, and I've got a link to the downloads in the show notes.



They wrote:  "NVIDIA Windows GPU Display driver contains a vulnerability in the 3D vision component in which the stereo service software, when opening a file, does not check for hard links.  This behavior may lead to code execution, denial of service, or escalation of privileges."



Another example:  "The GPU Driver contains a vulnerability in the kernel mode layer create context command in which the product uses untrusted input when calculating or using an array index.  But the product does not validate or incorrectly validates the index to ensure the index references a valid position within the array, which may lead to denial of service or escalation of privileges."  And there are even some, oh, here's one:  "Contains a vulnerability in the kernel mode layer handler in which the application dereferences a pointer that it expects to be valid, but is NULL, which may lead to code execution, denial of service, or escalation of privileges."



Anyway, as I said, I'm not sure that NVIDIA drivers proactively notify their users if they are in need of update.  But our listeners, who are certainly security aware, if you are NVIDIA-based, I would go checking.  It's just www.nvidia.com/download/index.aspx.  Check to see if you have updates available to your drivers because it sounds like you don't want to have something there that bad guys are going to know about quickly and be able to reverse engineer and then leverage to take advantage of the vulnerability. 



Let's see if there's anything else that I had to say about that.  Oh, yeah.  Mine have not been updated since 2016.  Nothing new was available.  But I think our listeners should check.



LEO:  Steve Gibson continues. 



STEVE:  So Apple has increased the intelligence of their already intelligent, now it's more intelligent, Leo...



LEO:  Super intelligent.



STEVE:  It's super intelligent.  Well, I wouldn't go that far.  But I am impressed by how much they're really focusing on this.  God, how many hours have we spent on this podcast talking about tracking.  This is the Intelligent Tracking Prevention, ITP, which now goes to - they're calling it version 2.1.  The beta release of iOS 12.2 and Safari 12.1 on macOS, which will be High Sierra and Mojave, they include this updated version of Intelligent Tracking Protection.  One of these things is so obscure, I thought, okay, wait.  And I had to read this several times to get a hold of it.



So their stated goal is to further reduce trackers' ability to establish user identities across sites.  Of course that's what tracking is; right?  To know that somebody who was at Facebook, then went to Amazon, and then went to, you know, did something at Google, and then went over to dilidili.wang or, you know, so forth.



LEO:  Don't track me, man.



STEVE:  So previous versions of this Intelligent Tracking Protection allowed domains that were classified with tracking capabilities to store what Apple called "partitioned cookies."  And those were cookies keyed off of the top site.  I'll explain partitioning in a second.  Then they said:  "As of ITP 2.1, which is the more intelligent tracking protection, partition cookies are no longer supported, and third parties classified with cross-site tracking capabilities now have to use the storage access API as opposed to the standard cookie API."  So here's how Apple explains what they call "verified partition cache."



They said:  "WebKit implemented partitioned caches more than five years ago."  They said:  "A partitioned cache means cache entries for third-party resources are double-keyed to their origin and the first-party eTLD+1" - top level domain and then down one, so like Apple.com.  So dotcom is the top level; Apple.com is one step down.  They said:  "This prohibits cross-site trackers from using the cache to track users."  Okay, now let me stop for a second.



So the problem that we have is that the good news is Apple has been alone in the industry, and I've saluted them always, for by default, and here again this is the tyranny of the default, by default blocking third-party cookies.  When I did that cookie forensics work years ago, I was compiling cookie - the visitors to GRC, compiling their browsers' cookie handling by browser.  And all of the browsers were like up near 100% except Apple alone, with a very, very low percentage in third-party cookies  enabled because it was off by default.  Meaning that by default almost all of the Apple visitors or Safari users who visited GRC had third-party cookies disabled.  So very nice.



However, by getting up to various tricks with JavaScript, as we've talked about before, it's possible to circumvent the third-party protection by essentially establishing a first-party presence using JavaScript and iframes and other tricks.  So what partitioning does is very clever.  It's something that Apple alone has been doing which tags the cookie that's being issued in a first-party context to the parent first-party context.  That is, like, the actual domain on the URL of the page where a first-party cookie is being issued where it's different from the page's primary domain.  That's what they mentioned when they say it's double-keyed to the origin of the first party and to the domain of the page.



So essentially it allows a cookie to be set by somebody, for example, an advertisement which is being hosted and then getting around third-party protection, yet it no longer - so it allows it to be set, but it's only going to be retrieved when that domain is being visited from the original domain where it was issued, thus preventing tracking.



Anyway, they say, continuing with Apple's description:  "Even so, our research has shown that trackers, in order to keep their practices alive under ITP" - that's Apple's previous Intelligent Tracking Protection - "have resorted to partitioned cache abuse.  Therefore, we have developed the verified partitioned cache."  Which is what makes the new one more intelligent.



They said:  "When a partitioned cache entry is created for a domain that's classified by the Intelligent Tracking Protection as having cross-site tracking capabilities, the entry gets flagged for verification.  After seven days, if there's a cache hit for such a flagged entry, WebKit will act as if it has never seen this resource and load it again.  The new response is then compared to the cached response."  So basically WebKit is faking it out, saying, huh, don't know about you, when in fact it does.  But that's only in the case if it's greater than seven days.  It'll compare the cached response to the new response.



"If they match in the ways we care about," Apple wrote, "for privacy reasons, the verification flag is cleared, and the cache entry is from that point considered legitimate.  However, if the new response does not match the cache entry, the old cache entry is discarded, and the new one is created," meaning the tracking is blocked because it won't be returning the one that it had from before.  "The new one is created with the verification flag set, and the verification process starts all over."  And if everybody's just completely gone cross-eyed listening to this description, I don't blame you.  As I said, I had to read it several times in order to say, what what what what?  And so this is the extent to which Apple has engineered tracking protection.  So again, I tip my hat to them.  I'm glad they're on our side and that they're doing this.



The upshot of this is that third-party cookies are blocked by default.  Also another thing that they're doing is that session cookies for domains not visited for 30 days are deleted.  So you can still have the "I want to remain logged in."  But if you don't go back to a domain where you said you wanted to be logged in, like Google or Apple or Facebook or something you're doing enough, if you don't go back within 30 days, it will prune that.  And this is Apple trying to get control of this massive cookie abuse that we talked about with our Picture of the Week where there are just so many ridiculously large cookies now that it's hard for them to be used for the actual valid purpose for which they were intended.



And the other subtle thing is cookies can be set in two ways.  The normal way of setting a cookie is to receive it as a set cookie header in a response from the web server.  That's how it's traditionally been done.  And you want those to both be marked as secure, meaning the cookie can only be read over HTTPS, and you want it to be also flagged as HTTP only, meaning only with the HTTP protocol, not scripting.  The reason that's important is that you don't want your scripting on the page to be able to see cookies which may have sensitive content in them, that is, to allow session hijacking if the cookie's value were readable by script.  But that's one way of setting the cookie.



The other way is there is a document.cookie property that the Document Object Model that we talked about last week, the DOM, maintains.  So script is able to set cookies.  Now this more Intelligent Tracking Protection is separating cookies received from a web server, which are first-class cookies, from those that are set by script on the page.  Cookies that are set by browser script are removed after seven days.  So again, that's one of the ways, for example, that tracking is being done.  And Apple, in seriously looking at where all these cookies are coming from, is working to pare this down.  And so cookies set by script only get to last a week, and then they are removed.



So again, hats off to Apple for continuing to come up with ways, basically, they're struggling not to break anything that we want to have work while deliberately breaking all of the clever workarounds that are the extent to which trackers are going.  And think about that.  Think about the extent to which tracking is being, like, just held onto.  It suggests that it is really valuable for advertising services and others to maintain a grip on users and profile where they go, the fact that they're willing to go to such degrees.



So again, Apple is alone in the industry, I mean, even in disabling something as simple as disabling third-party cookies by default.  There was a point at which IE, I don't remember which IE it was, a while ago Microsoft said, yeah, we're going to turn off third-party cookies.  There was such an uproar from the industry that they ended up never shipping a browser that had third-party cookies disabled, even though Apple always has.  Safari has that.



LEO:  Says something about market share more than anything else, frankly.



STEVE:  Yeah, probably does, yeah.  And Leo, that's our show.



LEO:  Wait a minute.  You've got to plug SpinRite, the world's finest hard drive recovery and maintenance utility.



STEVE:  Well, yes.



LEO:  Please buy it, folks.



STEVE:  I did want to thank our listeners.  I saw, after I mentioned that the support that this podcast's listeners have provided me lo these five years while I have been working on SQRL, I saw an effect in SpinRite's sales from that.



LEO:  Good.  Thank you.



STEVE:  So I have been conscious of that ever since mentioning that.  I really appreciate the help.  It, like, keeps the ship afloat.  And I actually did have a person, Van Zeck, posted in the SQRL Forum.  His subject was "Wow!" exclamation point.  He said:  "The magic is real," he said, "and in my hands."  And the good news is it will soon be in everyone's hands.



LEO:  That's cool.



STEVE:  He said:  "Steve, I just used Jeff's" - that's Jeff Arthur, the guy who's done the iOS client.  He said:  "I just used Jeff's iOS client to log into the SQRL Forums for the first time.  As I posted in Jeff's area, fantastic!" exclamation point.  He said:  "I have read about and watched others use the magic, but it was a real rush to have the magic right in my own hands.  Thanks for persevering with SQRL and showing how it is possible to potentially eliminate the biggest hassle and risk in Internet life - passwords."  He said:  "I have been lurking around SQRL for all five years, and it is truly exciting to see things come to fruition.  Van."



LEO:  Nice.



STEVE:  So Van, thank you for sharing your reaction.  And again, to our listeners for your patience, those who are waiting for the next version of SpinRite, I can't wait to get back to it.  And we're getting close.  Where I am now is Rasmus is actually adding some additional features to SQRL's support for XenForo, which is our forum software.  I'm building, finishing up on the static content for new SQRL users who will go and wonder what's going on and how they get started and so forth.  And that'll be separate from all of the dynamic forum content.  As soon as that's in place, then I start working on catching the specification stuff up to speed.  But basically we're there.  So it won't be long.



LEO:  Nice.  Yeah, when you actually - I think that's going to be the key to getting it out there is people using it.  Because as soon as you see it, you go, oh.  It's a little hard to understand, maybe.  But when you use it, it's like, oh.



STEVE:  Oh, Leo.



LEO:  Wait a minute, what happened?



STEVE:  That's just it.  I mean, it is - and I think, you know, I realize, I mean, I respect your skepticism and the skepticism of others who are like, okay, what chance does some guy have of getting some random protocol established?  Well, first of all, it's more than a guy now.  There's a community.  But the experience is, first of all, it is so secure, and the experience is so friction-free, that when you use it, you're like, wait, what just happened?  And, like, could this possibly be secure?  And then it's like, okay, why don't we have this everywhere?



LEO:  Just to be clear, my skepticism is not the technology.  I completely believe the technology and understand that it works.



STEVE:  Right.



LEO:  And I would be thrilled to no end if the world would adopt it.  I just - I worry that it's kind of an uphill battle, given you've got FIDO and Google and Microsoft and everybody in the world; you know.



STEVE:  It is.  I recognize that.  But how could I not...  



LEO:  You're got to do it; right?



STEVE:  ...offer it if it's possible?



LEO:  Yeah.



STEVE:  So...



LEO:  No, absolutely.  I mean, if Tesla had said, eh, this Edison guy's got it all, I'm just going to stop, he's got it under control...



STEVE:  Oh, my god, we'd have DC, and nothing would work.  We would.  Edison was DC.  Tesla was AC.



LEO:  Yeah, yeah.



STEVE:  And AC is so much better.  I mean, you could have transformers.  You can't transform DC.



LEO:  You see?  You see?  And they thought he was crazy.  In fact, he was basically persecuted to death.



STEVE:  Yeah.



LEO:  So I'm not - I don't think you're going to get persecuted to death for SQRL.



STEVE:  I'm not worried.



LEO:  But if it happens, I'll be the first to send you a card.



STEVE:  Gee, thanks.



LEO:  No, I love you, Steve, and I'm so glad you're here every Tuesday, 1:30 Pacific, 4:30 Eastern, 21:30 UTC to do Security Now!.  You can watch live at TWiT.tv/live.  You can watch or listen live.  You can join the chatroom if you're doing that at irc.twit.tv.  Of course Steve has on-demand audio that you can get anytime at your convenience.  That's at his website, GRC.com.  And he also has transcripts there, which is really nice.  Not computer-generated, real human written transcripts.



STEVE:  Yeah, boy, she got snowed in, too.  Elaine's been going through some...



LEO:  She's in the High Desert?  Where is she?



STEVE:  Yeah, wherever she is, I mean, I remember snow on a cactus.  So that would be a clue.  Whew.



LEO:  Wow.



STEVE:  Yeah.



LEO:  It sounds like the High Desert.  Wow.  If you're at GRC.com, yes, pick up a copy of SpinRite, the world's best hard drive recovery and maintenance utility.  And then, once you've paid your fare, you can visit all the rest of the site - well, you can do it anyway - free.  There's all sorts of good stuff there, including all the deets on SQRL.



We have audio and video at our website, TWiT.tv/sn.  Hey, if you're over there, you might take the survey.  I want to make sure that a lot of Security Now! listeners respond to the survey because we want to get your unique point of view, get the nerdy point of view.  Yes, I'm looking at you, Dave Redekop.  Go to TWiT.to/survey19.  It's our yearly attempt to get to know you a little bit better.  But don't worry.  No salesman will call.  We're not tracking you.  We're not interested in any personally individually identifiable data.  It's the aggregate that makes the difference.  Thank you, though.  If you don't have to do it, but if you do, thank you in advance at TWiT.to/survey19.



And I think we can wrap this sucker up for the day.  Don't forget to subscribe to the show.  You'll get it every Tuesday when it's available.  Thank you, Steve.



STEVE:  See you in March, my friend.



LEO:  What?  You taking a one-week vacation?  What?



STEVE:  No, next podcast.



LEO:  I know, yeah.



STEVE:  Yeah, it's like, whoa, March already?  Wow.  Yeah, a one-week vacation, exactly.  Seven days.



LEO:  See you in seven days.  Thanks, Steve.



STEVE:  Bye.



LEO:  Take care.  Bye-bye.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#704

DATE:		March 5, 2019

TITLE:		Careers in Bug Hunting

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-704.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we look at a newly available improvement in Spectre mitigation performance being rolled out by Microsoft and who can try it right now, Adobe's ColdFusion emergency and patch, more problems with AV and self-signed certs, a Docker vulnerability being exploited in the wild, the end of Coinhive, a new major Wireshark release, a nifty web browser website screenshot hack, continuing troubles with the over-privileged Thunderbolt interface, bot-based credential stuffing attacks, some SQRL, miscellany, SpinRite, and listener feedback.  Then we examine the increasing feasibility of making a sustainable career out of hunting for software bugs.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here with the latest on a Docker exploit.  Yes, there is a weird one.  ColdFusion, why you might want to stop using that.  And Spectre, another Spectre mitigation.  But do you really need to protect yourself against Spectre?  And one 19-year-old Argentine who's become a millionaire finding bugs.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 704, recorded Tuesday, March 5th, 2019:  Careers in Bug Hunting.



It's time for Security Now!, the show where we cover your security and privacy with the guy in charge, Mr. Steve Gibson, GRC.com, Gibson Research Corporation.  Hello, Steve.



STEVE GIBSON:  Yo, Leo.



LEO:  How's the research going?



STEVE:  The research is coming along just fine.



LEO:  I just thought about this.  What are you researching at the Gibson Research Corporation?



STEVE:  Well, so the original - okay, had a little too much coffee this morning.  The preceding company was Gibson Labs.  And of course then I guess it would be like, well, okay, what are you mixing up in your test tubes and beakers in your laboratory?



LEO:  Well, the Tech Guy website is the Tech Guy Labs.  I like the image, anyway, yeah.



STEVE:  Yeah, I do, too.  And what happened was there was some concern that the name was encumbered as a consequence of the way I wound down Gibson Labs where Atari kind of bought it because they wanted the Light Pen technology that I developed.



LEO:  Oh, I didn't know that.  Oh.



STEVE:  For the Apple II.  And then Atari was collapsing because the home computing market back then and that first attempt at videogames was kind of collapsing.  So they reneged on the deal, and I got it back.  And then Koala, that had the KoalaPad, I had become good friends with Jeff Heimbuck, who was the VP of Marketing at Atari toward the end.  And he'd gone over to become the president of Koala.  And so I said, "Hey, Jeff.  I got the Light Pen back.  Want it?"  And he said, "What do you mean, you got it back?"  He says, "They're never going to give that up."  I said, "Oh, they did.  They didn't really have a choice."



So anyway, it was contractual, and attorneys and contracts and everything.  So when I wanted to start up again, the best advice from my attorney was, you may want to stay away from Gibson Labs because, if you did something really amazing, someone might come back and say, "Hey, that's ours."  And I said, "Oh, okay.  I don't want that to happen."  So I did, "Let me think, hmm, hmm.  Oh, research.  Gibson Research."



LEO:  That's like Labs.



STEVE:  Yeah, it's kind of, you know.  And I've been researching stuff.



LEO:  That's a good name.  I never - it's funny because I never really even thought about it.  And I just said it, and I thought, hmm.



STEVE:  Yeah.  And GRC, it's funny, too, because when we wanted to get a domain name, I remember telling the guy who was going to go do that for me, his name was Millard.  Willard or Millard?  No, it was Millard.  Millard Ellingsworth III.



LEO:  Oh, my.  Oh, my.  That's how long ago that was.



STEVE:  I said, "Millard, go get us a domain name."  And he came back after, like, in two days or something because nothing was quick back then, I mean it was like ponies and camels and stuff.  He said, okay, well, we can't have "Gibson."  I said, "Ooh."



LEO:  You wanted Gibson.com.



STEVE:  No, that was gone already.  It was like guitars or refrigerators or something.



LEO:  Oh, yeah, yeah, yeah, right, yeah.



STEVE:  And he said, "But how about GRC?"  And I said, "Oh, I like that."



LEO:  Three letters is even better.



STEVE:  Oh, my god.  Now, I mean, I get offers all the time, not because they want me, they just want three letters.



LEO:  Three letters, yeah.



STEVE:  And we registered GRC, we were about six months behind another famous domain, Microsoft.com.



LEO:  Wow.



STEVE:  So, I mean, that was, you know, this was when it was all just beginning to happen back then.  And of course Bill was going to do MSN to compete with CompuServe and The Source.  And we're all going to get our modems, and we're going to stick the phones in there and dial into a big something.



LEO:  [Modem sounds]



STEVE:  Yeah, that didn't quite happen the way he thought.



LEO:  Don't you wish back then you'd thought to register a whole lot of domains?  I mean, I wish I'd gotten Leo.com back then.  It would have been nice.



STEVE:  Actually, it was very difficult.  Back then they were all very picky about, well, Leo, that doesn't sound like a company name.  Dot com is for companies.  Edu was for schools.  And org is for nonprofit.  We need to see your certification.



LEO:  Oh, yeah, I remember that, yeah.



STEVE:  It was very different, you know.  And so now it's like,  you want dot email?  Fine.  It's like, okay, whatever.  So, yeah.  And I feel the same way.  I've often thought about, you know, I mean, I've never been a squatter.  That annoys me.  There have been people who've, like, offered me very tasty domains.  And I've said, "You want to give it to me, I'll take it.  You want to charge me 25,000, I just won't just because that's wrong."



And there are people who have come to me asking for domains that I've been keeping alive for no reason that I've never used, and I've said, "Yeah, you're right, I'm not going to use it.  Here you go."  And they're like, "What?  You're giving it to me?"  I said yeah, you know, because it's wrong to charge.  That's just not right.  So anyway, this week, speaking of things kind of evolving, there was some news that kind of thought, okay, we need to share this with our listeners because, well...



LEO:  Of course it is.



STEVE:  It's generally true of this podcast every week, come to think of it.  But this was career related.  And that's my point.  I knew there was one there somewhere.  I've really had too much coffee.



LEO:  That's not possible, Steve.  That's not possible.  You can't have too much coffee.



STEVE:  So "Careers in Bug Hunting" is the title of this Episode 704 of Security Now! because Santiago, I want to say Lopez, I don't have it in front of me, whoever he is, he made a million dollars.



LEO:  And he's only 19.



STEVE:  Yes.  He is a teenager.



LEO:  Self-taught.



STEVE:  And he's got a little Mini Cooper, and now he's got a Peugeot.  He's buying cars.  He apparently likes to swim.



LEO:  I'm relieved.  Because when I saw the title, I thought this was out of storm - what is that troopers movie where they had "Go to space, hunt bugs"?  Heinlein, remember that?  The Heinlein story.



STEVE:  "Starship Troopers."



LEO:  "Starship Troopers," thank you, thank you.



STEVE:  Anyway, so that's what we're going to talk about.  We're going to wrap up by talking a little bit about and share - HackerOne is now the number one legitimate, unlike Zimperium.  And I guess it's going to be a bidding war; right?  If you've got a really tasty zero-day, Zimperium may be offering more.  If you're having to support yourself, I wouldn't say that it's wrong to sell something to Zimperium.  But hopefully HackerOne, who is not turning around and selling them, like to governments, would be an alternative.  Anyway, the point is that... 



LEO:  I have lots of questions about this, and we'll talk, of course.  But I really - I need to understand this a lot better.



STEVE:  So we're also going to take a look at a newly available improvement in Spectre mitigation performance, which is just today, actually, being rolled out by Microsoft.  And there are four people who qualify because it turns out the newer Intel chips can't do this, but the older ones, Broadwell and older, can.  There's some registry tweaks.  If you're up to 1809 - and I heard you mention after MacBreak Weekly that your system was just now being offered 1809.



LEO:  Yeah, my Microsoft, yeah, yeah.



STEVE:  That was the October 2018 release.  And I've got mine holding off because I can.  It's like, okay, I don't really want to take any arrows in the back.  But anyway, the point is that the Spectre mitigation slowed things down, especially for older  chips.  Those older chips can now get a reprieve, maybe.  We also have an Adobe ColdFusion emergency and patch.



LEO:  Uh-oh.



STEVE:  Which, again, I hope anybody who's ColdFusion based, that's the platform that another - Adobe's things refuse to die, and I think Adobe should be worried about why everyone wants them to.  But that's another matter.



LEO:  Does Adobe own ColdFusion now?



STEVE:  Yeah.  Adobe has ColdFusion.  And again, just die.  But it won't.  But it should, just like everything else, Shockwave and Flash.  I would worry if I were Adobe.  And maybe this is a stock tip.  I don't own any stock, so I can say this.  If you want something to sell short, you know, the company where everyone wishes everything that they published would just die already.  Anyway, I really do think I've got to lay off the caffeine.



LEO:  No, this is going to be a fun show, everybody.  Fasten your seatbelts.  It's going to be a bumpy ride.



STEVE:  We also have more problems with AV and self-signed certs.  A Docker vulnerability being exploited in the wild.  The end, believe it or not, at the end of this week, Coinhive is shuttering themselves.  We also have a major new Wireshark release, a nifty web browser screenshot website hack courtesy of Bleeping Computer, continuing troubles with the ever-overprivileged Thunderbolt interface, bot-based credential stuffing attacks, some little updates on SQRL, some miscellany, a bit of SpinRite feedback, some listener feedback, and then, breathlessly, we will be examining the increasing feasibility of making a sustainable and ecologically beneficial sustainable career out of hunting for software bugs.  So yes, no sign of - I don't think the bugs are going to end, and certainly this podcast never will, either.



LEO:  This very episode may never end, at this rate.



STEVE:  Single episode, just tuck yourself in.  So our Picture of the Week ties into a story that we'll be getting to.  But I just sort of liked it because nothing else was grabbing me for the week.  And this is showing the result of a Shodan search for either of the two ports that Docker uses which are never supposed to be publicly exposed.  Docker uses, as do now many systems, an internal network-based API.  So the idea is that the Docker daemon binds itself to the localhost IP, 127.0.0.1, on two particular ports which are well-known ports for it, so that processes in the system are able to open a connection within the system.



So it's a nice, I mean, it turns out that this IP stack forms a very convenient Inter Process Communications system, IPC, for one process talking to another.  And so the idea is that it's just meant for processes within the system.  But because it is networking, if it is misconfigured, and I have to think some misguided people are doing this on purpose, like they think, oh, it'd be really nice to...



LEO:  To be able to log in.



STEVE:  ...make our Docker API available to people in China.  Like, oh...



LEO:  What could possibly go wrong?



STEVE:  What could possibly go wrong with that?  So this shows a map of the world on the left and shows that, of the 3,951 exposures apparently of Docker - and that number should be zero, right, it should not be 3,951 - 929 are in the U.S., followed by 680 in China, 240 in Singapore, 225 in Ireland, 224 in Germany, 214 in France, 212 in Canada, and so on down the line.  So for whatever reason, and we'll be covering this in a few minutes, there are lots of opportunities for people to hook onto Docker.  And, okay, I won't step on the punch line because we could probably at this point guess what has set up shop in those Docker containers.



Anyway, in the meantime, Microsoft has just rolled out on March 1st, which is last Friday, an update which is available to Windows users.  Now, I had intended to go see if it was going to give it to me automatically, but it's only for the October 2018 update 1809 of Windows 10.  I'm still back on 1803 because I'm not in a hurry to discover if there's anything that goes wrong when I update myself.  But Leo, you could because your system was just updating itself an hour ago to 1809.



LEO:  Yeah.



STEVE:  So what would be interesting would be to see if you got KB4482887.  That was just made available on Friday, March 1st.  What it does is it allows - oh, and maybe if it's smart it probably knows you didn't qualify because Skylake and later - wait, no, Skylake and newer - is that earlier?  Sooner?  I don't know.  Anyway...



LEO:  That's both the same thing.  Newer or later, same thing.



STEVE:  More recent.



LEO:  Yeah.



STEVE:  Okay.  Oh, yeah, instead of earlier, later.  Okay.  More recent.



LEO:  Have some more coffee.  I think you've entered a space-time warp.  Alex Gumpel, if you're listening, check to see if we've got 448287 on the 1809 update.  I'd be curious.  It's not a Skylake, I mean, it's later than Skylake, so it wouldn't - more recent.



STEVE:  Earlier?  Newer?



LEO:  Yeah.



STEVE:  Okay, good.



LEO:  It wouldn't probably - I think it is.  Actually, maybe not.



STEVE:  Yeah, it probably is.



LEO:  It's a couple years old.



STEVE:  Leo, everything you've got is newer than that.



LEO:  I would hope.



STEVE:  Anyway, so, I mean, remember I bought my current machine back when it was believed that Windows 7 was going to stop supporting Skylake.  And it was like, what?  They are going to do that?  So I immediately bought, no, I guess it was going to stop supporting after Skylake.  Then they thought better of that  because many other people with more clout than I were a little annoyed by that fact.



Anyway, remember that Google invented something called Retpoline, which is just difficult to say, Retpoline.  It's a contraction of "return trampoline."  A trampoline is like the word sounds.  A trampoline is something you bounce off of; right?  So a return trampoline is a technique of bouncing off a return instruction.  Google came up with this as a lighter weight solution to the second variant, the Variant 2 of Spectre.  And the problem with it was Microsoft's reaction, or, well, Intel's solution was just to shut down a speculative execution benefit, which is what the Variant 2 of Spectre was leveraging.  And just like throwing the switch, it's like, okay, we're turning that off.



Well, it caused a significant performance impact.  So what Google realized was, you know, rather than just - there's only like some instances where this is really a problem.  And if you modify your code in those particular places, then you can just kind of like make little micropatches all over, but leave it on the rest of the time where it can't actually be leveraged against you.  So as Microsoft explains in their update, they said Retpoline - so they're incorporating this invention of Google.  Thank you very much, Google.



"Retpoline works by replacing all indirect call or jumps in kernel-mode binaries with an indirect branch sequence that has safe speculation behavior."  They wrote:  "This proves to be much faster than running all of kernel mode code with branch speculation restricted."  And this is their IBRS set to 1, which is their indirect branch restriction on speculation.  "However," they write, "this construct is only safe to use on processors where the RETURN instruction does not speculate based on the contents of the indirect branch predictor."



And this is where everyone's head explodes, of course, because it's like, what?  But not Google's.  Google said what, okay, and then we'll figure out what that means.  So Microsoft did, Google did, everybody, you know, said okay.  So unfortunately, Intel's newer processors, thus the newness of them, do that.  They speculate based on the contents of the indirect branch predictor, which means that Retpoline cannot be used on them.



LEO:  Is that good or bad?  I've kind of lost track at this point.



STEVE:  I know, I know.



LEO:  So the newer processors are more vulnerable.



STEVE:  Yeah, well, they're more newer.



LEO:  Okay. 



STEVE:  So they're fancy. 



LEO:  I grant you that.



STEVE:  They're fancy.  And their RETURN instructions, being fancier, do speculate based on the contents of the indirect branch predictor.  Therefore we can't rely on Retpoline to work for them.  Processors where we can are all AMD, which don't do that, as well as Intel processors from Broadwell and - okay, I'm going to get this right - earlier, meaning older and before, and up until and including.



LEO:  No, pre-Broadwell, not Broadwell.  Just like it was...



STEVE:  No, no, Broadwell and pre.



LEO:  Oh, okay.  So inclusive.  Okay.



STEVE:  Yes.  And so that means that Skylake came after Broadwell.



LEO:  I got it.  That's right.  Yeah, and by the way, that Surface Studio is Skylake.  So that would mean it's...



STEVE:  Yes, that you're not going to get this.



LEO:  This Retpoline patch.



STEVE:  Retpoline cannot be used on your system because...



LEO:  I feel like an idiot, but is Retpoline good?  Or is Retpoline bad?



STEVE:  Retpoline is a fix for Variant 2 of Spectre.



LEO:  Okay.  But do I not need Retpoline because I have a Skylake processor?



STEVE:  Oh, you wish you could have it, but you can't.



LEO:  Oh.  So in other words, it's bad to have the newer processor because the Retpoline fix doesn't work.



STEVE:  Yes.  Although it's kind of good to have it be newer because it's not slowed down as much by Variant 2 of Spectre.



LEO:  Ah.  Because the Retpoline has side effects, bad negative side effects.



STEVE:  Basically we thought we were going to finally stop talking about this.  Like we were going to limit this to 2018.  But no.  Here it is in 2019 still, and it's just as mind...



LEO:  Bending.



STEVE:  ...boggling as ever.



LEO:  Yeah.



STEVE:  So here's the takeaway.  If any of our listeners have - if their ears are not bleeding.  If you happen to have Windows 10 with the 1809 October 2018 update, and they're not going to ever do this, they've said sorry, we're not going to go back in time any further because we're all exhausted.  So if you have October 2018 update 1809 of Windows 10, and a Broadwell or older chip, which got slowed down by the 2018 panic over Spectre - and remember, we're, like, no one ever actually used this to attack anybody in the first place; right?



LEO:  But they could.



STEVE:  Well, yeah, but on an end user's machine, you know, if you've got something in your machine attacking itself, then you've already got problems.  So Retpoline and, I mean, willy-nilly, who cares?  So the only problem was in cloud environments where you might have a deliberately shared hosting environment where malicious code was running in one VM, and this was being used to try to leak secrets out of an adjoining VM, which is not anything that your typical end user has anyway.



But essentially what'll happen is, if you are that person, Broadwell or older chip, where you would have seen things slow down, if you didn't, for example, use my InSpectre utility to turn this nonsense off because it's just not a problem in the first place, in time Microsoft is going to roll out for those people a performance improvement.  Which you could apply today, on March 5th, if you follow some registry changes in the link to the update that I have in the show notes.  



LEO:  Oh.



STEVE:  And because that probably has winnowed down our entire applicable audience to five, I'm not going to go through that now.  I'm just going to say that's there.



LEO:  Is it in InSpectre?  Did you put all this in InSpectre?



STEVE:  Yeah.  InSpectre has always had this.



LEO:  It knows all about it, okay.



STEVE:  So, yeah, you could turn this stuff off, and then your  machine runs fast and fine.  And again, we've always said we will certainly let everyone know if they should ever actually need to turn these mitigations on.  As far as anyone knows, this has never actually been used to, I mean, this is probably the biggest example of the sky is falling, security concern without any basis for believing that an individual needs to do anything.  But boy, was it a great source of material for 2018.



LEO:  Oh, man.



STEVE:  Yeah.  So today Microsoft updated their posting, saying, "While the phased rollout is in progress," that is, they're going to tippy-toe this out, this Retpoline deployment, sort of cautiously.  They said:  "While the phased rollout is in progress, customers who would like to manually enable Retpoline on their machines" - those with Windows 10, October 2018, 1809, with a Broadwell or older chip - "can do so," they wrote, "with the following registry configuration updates."  And again, link in the show notes for the five of you out of the who knows how many listeners we have who are still at this point, like, oh, yeah, that's me.  Go get it.



Meanwhile, Adobe ColdFusion gets an emergency patch.  Last Friday, March 1st, Adobe released an emergency patch for their Java-based ColdFusion website development platform to close a vulnerability that was being actively exploited in the wild to execute arbitrary code.  So yes, emergency.  So hopefully, again, if you're using ColdFusion, you are current with your email update notification list, and this is already old news to you, because this was a zero-day that they became aware of.  The vulnerability allowed an attacker to bypass restrictions for uploading files.  So to take advantage of it, the website had to be configured to accept executable uploads.  So, okay.  So that immediately, hopefully, disqualifies...



LEO:  That's a problem right there.



STEVE:  Yeah.  Now, on the other hand, there are places where you could imagine you could be allowing executable uploads for some reason where they would be sequestered and then could not be executed.  The flaw allows an HTTP request to execute that uploaded file.  Whoopsie.



LEO:  Wow.  Wow.



STEVE:  So not good.  Really not good.  All previous ColdFusion versions on all platforms are vulnerable to this flaw.  It's CVE-2019-7816.  I've got a link to their security advisory, which you just had on the screen a second ago.  Adobe's summary said:  "Adobe has released security updates for ColdFusion versions 2018, 2016 and 11 dot anything.  These updates resolve a critical vulnerability that could lead to arbitrary code execution in the context of the running ColdFusion service.  Adobe is aware of a report that CVE-2019-7816 has been exploited in the wild."



It turns out that an independent consultant named Charlie Arehart discovered this when he found it being used against one of his clients.



LEO:  Ooh, bad.



STEVE:  Yeah.  After figuring out what was going on, Charlie reported the flow to Adobe, along with a proposed solution.  To their credit, and doubtless due to the bug's extreme severity affecting all appropriately-configured ColdFusion-based websites ever, Adobe had the fix ready within just a few days.  So bravo for them getting on this immediately.



Bleeping Computer interviewed Charlie, who they quoted saying:  "Getting folks to implement this fix is of critical importance."  Oh, and Charlie did not disclose any additional details of the attacks since he didn't wish to help any attackers.  However, he did tell Bleeping Computer that he believes that a skilled attacker will be able to connect the dots from Adobe's security bulletin and find a way to exploit the glitch.  So knowing that the key is finding a site that will accept an executable upload, there is now a way then to generate an HTTP query of some sort which will execute that.  And so ColdFusion has been around for a long time.  It's one of those things that refuses to die.  Like everything else that Adobe - except, you know, PDFs.  We like those.  But everything else, no.



LEO:  This was also a problem with PHP is if you weren't careful about your directory permissions, you can execute PHP code in a directory, arbitrary code, and boom.



STEVE:  Yup.  Yup.  So for an interim mitigation, Adobe wrote:  "Note:  This attack requires the ability to upload executable code to a web-accessible directory, and then execute that code via an HTTP request. Restricting requests to directories where uploaded files are stored will mitigate this attack."  Okay.  Just update ColdFusion.  Again, so if something prevents you from doing that, then oh, my goodness, yes, by all means don't allow directories where files are uploaded to be web accessible by any means.  You certainly shouldn't.  I mean, that's Web Security 101.



So again, ColdFusion 2018 update 2 and earlier; 2016 update 9 and earlier; and ColdFusion 11 update 17 and earlier.  Basically, that is, all of ColdFusion is susceptible.  So hopefully anybody, as current security really requires, anybody who is doing things with web-based systems needs to make sure that, you know, we talked about this with Drupal last week.  Make sure that your email address that they have on file for you is correct and that alarms go off when they send you a security bulletin because right now the exploit window is - we are seeing how quickly bad guys jump on these things.



We have another instance of a self-signed certificate problem emerging and a collision with another AV.  In this case it's Kaspersky.  More than a month ago, since early February, or rather for more than a month since early February, Chrome users, okay, so people using the Chrome browser, which we know is most people in the world now, who are also using Kaspersky's AV in its default mode of performing secure connection filtering, which is of course what all of these AV systems that are offering this are doing now because otherwise they can't see anything coming and going from your system, for more than a month users have been getting and complaining about mysterious pop-ups from Kaspersky.



I have a link to the Chromium bug report and a picture of the pop-up.  It says Kaspersky, this was both the free and the paid version, says "Cannot guarantee authenticity of the domain to which encrypted connection is established."  The application shown is Google Chrome.  A URL is bizarre-looking, it's a GUID, one of these hyphenated hex things which is long.  The reason is given as a self-signed certificate.  So any version of Chrome.  In this case it was Windows.  And the third-party software involved is Kaspersky.  So says the Chromium bug report.



The bug report reads:  "There's been a sudden increase in device discovery reports.  Reviewing the reports indicated that it's common on all Windows platforms.  And reviewing the logs show a commonality of cast channel" - meaning Chromecast channel - "authentication errors, which can often be attributed to antivirus or security software."  Then they said:  "In a similar timeframe, some discussions appeared on Kaspersky's online forums."  And there are two links that are given in the Chromium report, "chrome-self-signed-certificate-cannot-guarantee" and then also "self-signed-certificate-issue-on-google-docs."



The person producing this bug report says:  "I was able to reproduce the issue with Kaspersky Free, and confirmed with some external users using Kaspersky Total Security," which is the subscription-based system.  To reproduce it:  "Have Kaspersky software, either free or Total, installed and running on a Windows machine."  Now, here is what's interesting.  "Have a Chromecast device connected anywhere to the same network as the computer.  Then open Chrome."



So he says:  "Immediately when Chrome is opened on a network with a Chromecast device, a pop-up dialog appears from Kaspersky stating 'cannot guarantee authenticity of the domain to which encrypted content is established.'"  Okay, you know, remember the user hasn't done anything at this point.  They've just fired up Chrome to get ready to do something, and suddenly they're being shown this thing saying we're not happy.



"Even after clicking Continue on the dialog, the Chromecast devices do not appear in the Cast dialog."  So Kaspersky is apparently not allowing this to happen or has already dismissed these devices.  "Disabling 'Scan Encrypted Communications' in the Kaspersky network settings," which of course is the thing which is allowing Kaspersky to intercept and filter with its own certificate any attempted TLS connections, the bug report says, disabling the scan-encrypted connections, then allows device discovery to work, and the Kaspersky error dialog does not appear.



So what has been figured out, looking at this, is that when the Chrome browser is launched, it sends out a broadcast to the Ethernet broadcast on the local network, querying for any available and listening Chromecast devices.  And it turns out that Chromecast devices may be present even when they are unknown to the network's user.  For example, many recent smart TVs now include Chromecast built in, so that they're able to receive casts just as part of the service that the smart TV offers.  Chromecast has a device discovery service which listens for anybody asking for it and accepts connections on TCP port 8009.  And it will establish a TLS connection to a client connecting to it using its self-signed cert.



So when Chrome is started up, it sends out a "are there any Chromecast devices listening out there" on the local broadcast to the LAN.  And any powered up and online Chromecast devices will hear the call and reply.  Then the Chrome browser attempts to bring up a TLS connection to that responding device's IP at port 8009.  In the TLS handshake, the Chromecast device sends a self-signed cert in order to encrypt the communications, and Kaspersky freaks out if it is monitoring all connections, not even to Chrome, but to the PC on which Kaspersky is installed.



So that's what's been happening.  Kaspersky's complained that it's not easy to distinguish this event from everything else going on.  But they have said that within a week or two they'll have a fix for it shortly.  My guess, because there's really no downside to doing so, is that they will simply allow connections to be made from the user's computer to other devices on the same LAN, on the same network subnet, because that's going to be safe, and to make this port 8009 an exception and basically whitelist a self-signed cert warning on port 8009 when it's on the local net.  There's no reason not to.  You're not opening yourself to any security vulnerability because it's your own LAN.  And you're establishing a TLS connection.



The fact that it's a self-signed cert is not a problem.  I'm sure that Chromecast is doing the responsible thing, which many vendors, unfortunately, have not.  We've talked about this, that is to say that Chromecast is creating a cert on the fly with a unique private key.  That way there is no globally known private key that can be leveraged against people who have decided that they want to trust this self-signed cert.



LEO:  Honestly, we see this crap all the time.  This is just why you shouldn't use an AV.



STEVE:  Yes.  I know.



LEO:  It's dumb, overprotective security.



STEVE:  I know.  I completely agree.



LEO:  If you're going to use an AV, I think we all agree you shouldn't be using Kaspersky.



STEVE:  Yeah, that's a little bit of a stretch.



LEO:  In Soviet Union, antivirus infects you.



STEVE:  None of our listeners at this point would think that I actually need any additional caffeine.



LEO:  But would you like some anyway?  Go for it, Steve.



STEVE:  But my vocal cords have dried out, so...



LEO:  All right.  Time to hydrate.



STEVE:  So Docker containers are having another problem.  We've talked about these before, and unfortunately they're back.  But we should back up a bit and talk about Dockers a bit, and I don't mean the pants, since we have never discussed them in any detail.  And they're becoming increasingly popular and are therefore becoming an increasingly lucrative target for attack on the Internet.  And this is again another one of these issues that should not be publicly exposed.  We showed the picture of the nearly 4,000 Docker ports that Shodan was indexing as being publicly available. 



Okay.  So we all understand the concept of a virtual machine since they've been around for a long time.  A Docker moves the encapsulation boundary over to the other side of the OS, is a way to think about this.  In a virtual machine environment we have the so-called "hypervisor."  It takes advantage of the amazingly complex hardware features of processors to create isolated abstractions of the hardware processor itself.  Thus the term "virtual machines," you know, virtual CPUs.  And once you have an abstraction of a processor, a virtual machine, then you boot an operating system onto that virtual machine to create an instance of a system which can then run that operating system's client software.



Okay.  But think for a moment about how expensive this is in a cloud computing environment.  Say that a given hardware system wants to run six separate tasks.  Taking the virtual machine approach, the system's RAM is divided up into six partitions.  An operating system instance is booted into each one.  And then each one is given a task to run.  The flexibility this offers is that the hardware could be - could be - simultaneously running six different operating systems, each running their own task.



But the reality in today's computing environment is that more stuff in the cloud is Unix or Linux based.  So you're not actually running a totally heterogeneous OS environment.  It tends to be homogeneous.  So booting six redundant copies of the same Linux VM on a cloud computing hardware instance is very wasteful.  And remember back when we were talking about Rowhammer attacks, how we learned that practical VM environments worked very hard to consolidate identical regions of memory.



So if you had six copies of Linux VMs, each running the same OS, a great deal of the memory that they're using is the same because it's the same OS.  So, you know, same drivers, same kernel, same a lot.  And so the virtualization hardware allows those duplicate regions of memory to be consolidated so that each VM sees its own memory map and isn't aware that it's actually sharing that physical memory among other VM instances.  The point is that there's been a lot of work, difficult work, which in this case exposed Rowhammer vulnerabilities as a consequence, but a lot of work to get back the lack of efficiency of running all of these separate VM instances.



Okay.  So under this original VM-based cloud computing model, the encapsulation was the VM.  What Docker does is it moves this encapsulation boundary onto the other side of the OS, whereas the VM model the encapsulation boundary was between virtual machine hardware and the VM's OSes.  Docker places the boundary above the OS at the OS service level.  So whereas a VM contains an operating system, a Docker container does not.  The Docker container runs on or above an operating system to which has been added a Docker interface API.



So the Docker container doesn't have the OS in it.  It encapsulates all of the various library and service dependencies and requirements of whatever the task or process is that the Docker is intended to perform.  So this is like way more efficient in a cloud computing environment.  The use of Dockers are becoming increasingly popular since they offer a much more efficient sharing of a hardware instance's resources.  And the cloud computer has a single highly tuned instance now of an OS running, which is exposing a Docker API that allows it to host and run many independent instances of the so-called "containers."  A container is sort of the equivalent of a VM in Docker land, a Docker container.



So the Docker container resident run-time module, that is, this thing which is running on typically a Unix or a Linux, is known as "runc," short for "run container."  Runc is an open source command line utility designed to spawn and run containers and, at the moment, is used as the default runtime for containers with Docker.  There's one called "containerd," as in "daemon";  Podman; Kubernetes that we've talked about in the past; and LXC, which is the Linux, sort of the Linux execution container offering.



Okay.  So we have a CVE-2019-5736.  It reads:  "Runc through 1.0 release candidate 6, as used in all Docker before 18.09.2 and other products" - so runc is the problem - "allows attackers to overwrite the host runc binary" - whoops - "and consequently obtain host root access by leveraging the ability to execute a command as root within one of the Docker containers" if a new container with an attacker-controlled image is mounted, or an existing container to which the attacker previously had write access, that can be attached with Docker exec.  And they write that this occurs because of file-descriptor mishandling relating to /proc/self/exe.



So what this creates is a Docker breakout security flaw which has been discovered in that runc container runtime, which allows malicious containers with minimal user interaction to overwrite the host "c" runtime binary to gain root level code execution on the host machine.  So the maintainer of runc is a senior software engineer at SUSE Linux in Germany.  And I won't go through his posting.  But he posted details about the flaw and updates with a seven-day window before releasing a proof of concept.  They were under pressure to produce a proof of concept because many people who were applying the update felt very strongly about the need to verify that the patch had done what was expected.  So a proof of update was produced a week later, creating a relatively small window before the bad guys would have a running proof of concept that they could use against other Docker instances which were exposed.



Which brings us to what the guys at Imperva found, which was taking a look at Docker's public exposures on the Internet.  All the big vendors responded immediately.  Amazon runs Dockers.  Google does.  Docker themselves do.  But of course it is something that Unix and Linux instances are able to run.  So of course there are a gazillion of those out on the Internet.  And as we started off saying at the top of the podcast, nearly 4,000, or actually in some cases a little more than 4,000, were reachable and were believed to represent exposed Docker instances.



As I also mentioned earlier, it is supposed to only be bound to the localhost interface.  I can't explain how thousands of these could be exposed publicly except maybe somebody wanted to have a Dockers instance that would be only on the LAN, and then their firewall was misconfigured so it got out.  But you could also bind the server only to the local network, rather than just the localhost, but apparently that wasn't done.  I guess the problem is it's just a numbers game.  If you have enough instances of anything globally, you're going to find some which are misconfigured.



So the guys at Imperva first did a Shodan search to see what was available.  Then they dug deeper and connected to the IPs that appeared to be advertising Docker to see what version of Docker was running.  And they did find thousands of them still vulnerable.  Then they dug even deeper and looked inside.  They found that out of the nearly 4,000 IPs that were apparently exposed by the Shodan search engine, about 400 of them were still responding as Docker.  The presumption is that other bad things may have crawled inside and closed the door behind them so they were no longer publicly exposed.  But 10 percent of them were still exposed.  And they said that on these unpatched Docker servers that had remained accessible, they found, not surprisingly, Docker images of cryptominers, as well as legitimate services and production environments.



So where possible, bad guys, given four weeks to - I think it was on February 11th this was first made publicly exposed, publicly disclosed.  So it didn't even take a month before immediately cryptominers were set up on these machines, presumably, I mean, they may well be servers, which have some strong hardware.  And so there is some hope, no doubt, in the minds of the bad guys that, if you can get a cryptominer mining Monero cryptocurrency on a strong big iron server platform, you stand to make some money.  So Imperva summed it up yesterday in their disclosure, so that was just Monday the 4th, saying hundreds of vulnerable Docker hosts exploited by cryptocurrency miners.



Which brings us to sort of an unexpected but interesting story.  We've been talking about Coinhive off and on for I think it's been a couple years.  Coinhive, of course, is the company deeply in the gray.  They pretend to be a well-intended service.  The idea was that people who wanted to make money from visitors to their websites, rather than putting ads on the site, would host the Coinhive JavaScript in their web pages so that visitors to their website would download a link to the Coinhive JavaScript, which would then download from Coinhive the mining software, which while the visitor was moving around this cryptocurrency based or cryptocurrency financed website would spend a little time participating in a mining pool in order to generate some money in return for the short-term use of the visitor's processing resources.



Okay.  If all of that was the way it worked, that would make sense, especially if a notice is put up on the web page saying, hey, we notice you're blocking ads.  Would you mind clicking here, give us your permission to run a benign cryptocurrency miner on your web page while you're visiting our site.  You say, okay, yeah, fine.  Seems like a reasonable tradeoff.  You say yes.  That happens.  Of course what happened was bad guys said, oh, I'm going to create a Coinhive account, and I'm going to inject this Coinhive mining script everywhere I possibly can.



So as a consequence we discovered, we talked about last year how there were routers, MikroTik routers were injecting Coinhive script into unsecured pages of the browsers behind the router every chance they could get, and all kinds of similar things.  There were ads that were carrying Coinhive, so the ad was pinning someone's CPU while they were visiting a page, and that was causing a problem.  Then of course there was pushback from the browsers, who then started trying to detect whether cryptocurrency mining was occurring on the browser, and on and on and on.



Okay.  Yesterday's blog post titled "Discontinuation of Coinhive":  "Some of you might have anticipated this," reads the blog.  "Some of you will be surprised.  The decision has been made.  We will discontinue our service on March 8, 2019."  That's in three days.  Today's the 5th.  "It has been a blast working on this project over the past 18 months; but to be completely honest, it isn't economically viable anymore.  The drop in hash rate - over 50% - after the last Monero hard fork hit us hard."  Oh, and also it's worth noting there's another fork slated for the 11th, next Monday, which will further drop the rate.  So that no doubt factored into their thinking.



They also said:  "So did the 'crash' [in quotes] of the cryptocurrency market, with the value of XMR [Monero currency] depreciating over 85% within a year.  This and the announced hard fork and algorithm update of the Monero network" - oh, yeah, they do mention it - "on March 9" - oh, it'll be on Saturday, March 9th - "has lead us to the conclusion that we need to discontinue Coinhive.  Thus, mining will not be operable anymore after March 8, 2019.  Your dashboards will still be accessible until April 30" - so the rest of March and all of April - "so you will be able to initiate your payouts if your balance is above the minimum payout threshold.  Thank you all for the great time we had together."



So out with Coinhive and in, I imagine, with whatever takes its place.  This will certainly put a kink in the cryptojacking enterprise, but I doubt this will be the end of it altogether.  Recall that Coinhive had monthly revenue themselves, back in the heyday, of approximately a quarter million dollars a month at one point.  And in terms of I guess what you would call "market reach," they enjoyed a 62% share of all websites using a JavaScript cryptocurrency miner.  Which suggests there are other cryptocurrency miners.  And if they can't use Coinhive, the bad guys will just switch to whatever they can use.



So maybe this is a sign of the fact that the cryptocurrency mining phenomenon was supported by that balloon that we had in cryptocurrency valuation, and that the expansion of mining, which allows this continual hard forking and revision of the algorithm of cryptocurrencies, in fact the algorithm change is further fighting back against ASIC mining in the case of Monero.



So it may just be that it's really no longer viable; that we're not going to be suffering this continual concern over mining injection into web browsers because it just no longer pays, that there are other ways for these bad guys to make more money than injecting cryptocurrency mining into the browsers of unwitting users.  Despite the fact that at one point 200,000 MikroTik routers had been commandeered and used for cryptojacking campaigns in several different waves.  So I imagine we'll see.  I will certainly cover what happens moving forward.



LEO:  Who do you think created Coinhive?  You think that was just some graduate student in his dorm room?



STEVE:  Yeah.



LEO:  That's the impression I get now.



STEVE:  Yeah.  I really do think that it was - I think it was well intended.  It was probably always misdirected because there was never a way to prevent its abuse.  And he was saying, set up an account with me, and you can have your visitors to your site mine Coinhive to generate some revenue for you.  That's kind of a cool model.  It was like, oh, okay.  The problem is bad guys.  There was no way to police it.  There was no way to keep bad guys from injecting that same script, creating an account at Coinhive and then just spraying this script everywhere they could in order to get nonpermitted cryptocurrency mining happening.  So I think it was kind of a good idea.



It would be interesting to know how much money the guy is, like, pocketing as he shuts things down and steps away.  But it was just probably always a bad idea, only because there's just no way to do it, or at least he didn't have a way to do it in a way that did not allow it to be abused.  I still think it's interesting.  I mean, the idea of generating revenue for a site, I mean, I feel badly for Wikipedia.  I donate every year when Jimmy Wales comes knocking, and he does.  But then I want to support Wikipedia, but would it be a bad thing if my processor was used to generate some revenue for them while I'm looking at a Wikipedia page?  That's a tradeoff I could make, rather than have ads on.  I would really not like to have ads on Wikipedia.  So, but the Wikipedia pitch every year is this is expensive for us to keep this stood up and maintained.



So it still strikes me as an interesting model, the idea that, because of the design of the cryptocurrency, it is still feasible for a CPU to mine, and it hasn't been, like as is the case in bitcoin, that's just gone.  That's just all custom hardware now.  They designed an algorithm with Monero which is hostile to that kind of scaling, ASIC scaling.  And the idea of generating revenue while you're visiting a site with your permission, that really does seem interesting.



LEO:  Yeah, yeah.



STEVE:  I think the way to do it maybe would be to install something on your computer so that you can mine efficiently,  and then have a web standard where the browser is able to get your permission and then engage the miner while you are there, and basically the CPU work that you're doing goes to benefit the mining pool that has been assigned to the website that you're visiting, that sort of thing.  So anyway, I can imagine some ways of it being done in a user-supported way that makes sense.  This really was kind of a hack.  And unfortunately it didn't really pay off in the long term.  And boy, was it abused.



LEO:  Yeah.  That's the real tragedy.



STEVE:  Yeah.  So we have a new version, I wanted to notify our listeners, of Wireshark.  Wireshark has been around since the late 1990s.  Back then it was named Ethereal.  And boy, have I gotten my mileage out of this thing.  It is, for those who don't know, Wireshark is the go-to utility for capturing and analyzing packetized network traffic.  I've been using it, as I mentioned, for years.  And things like ShieldsUP! and, boy, especially the very complex DNS spoofability test service would have been far more difficult for me to get built were it not for Wireshark.  I absolutely used it to look at the way things were happening "on the wire," as they say.



It was originally named Ethereal, and then it got renamed in 2006 to sidestep some trademark issues because Ethereal was not available.  It always relied upon an old-timer, a venerable packet capture driver known as WinPcap, which has also been around forever.  And ShieldsUP!, GRC's service that we were talking about before, originally used the same driver.  I used WinPcap in the beginning, until I later wrote my own kernel driver so that I could do more custom work down in Ring-0, back when I really needed per-packet efficiency.



So the big change, which was announced last Thursday with the release of Wireshark v3.0.0, is that the WinPcap driver has finally been abandoned in favor of a new driver called Npcap.  Npcap is an NDIS v6 filter shim driver, meaning that it inserts itself neatly into the network stack in such a way that it's able to watch and inject network traffic without needing to worry about any adapter-specific details since those are handled down in the lower layers of the stack.



This Npcap driver is EV-signed so that the latest Win10 systems will allow it to slip in between their network layers.  Version 3 also for the first time can capture localhost loopback traffic.  As I was talking about anything that binds to 127.0.0.1, normally Wireshark has bound to network interfaces, and so you could only see traffic coming and going in and out of your machine.  For me, that's been an inconvenience.  So it's very cool that the new Wireshark will allow you to monitor the things going on that I was talking about before in the case of Docker, internal communications using the network protocol, but which never leave your machine.  Wireshark 3 can now do that.  So that's going to be very handy.



And because we have a lot of communications which is now encrypted, one of the other cool features of Wireshark, it's had this for a while, is if you give it the server's private key, it can decrypt TLS communications from its packet capture passively.  And we've sort of talked about how that's possible.  Remember that we talked about how the NSA is sucking up all of this network traffic which they cannot read.  But if in the future the private key, even after it's expired, becomes available to them, they can go back and use the server's private key to decrypt past captured network traffic.  This uses that same approach in order to allow probably a custom rolled server private key.  You have to be very, very careful with your private key that it doesn't get loose.



But being able to bring up a service with a self-signed certificate that you've told the client service to trust, and then give the private key of that self-signed cert to Wireshark so that you can then see into the TLS traffic that is being transacted, that's just super handy.  So anyway, I just - oh, and also in WiFi.  In the past you had to have typically an AirPcap hardware WiFi dongle in order to sniff radio traffic, especially promiscuously sniff all the traffic that it was able to see.  Not so anymore.  This Npcap that the position of the Npcap driver in the stack means that it's able to capture 802.11 WiFi traffic out of the air without needing any special hardware.  So I just wanted to make sure to put it on everyone's map that Wireshark is now at v3.0 and with lots of feature improvements, including it's able to decode many more network protocols than it was before.  So definitely very cool.



And this was a neat trick.  I'm not sure that I have a use for it, but it's another thing that I wanted to sort of add to our listeners' bag of tricks.  This is from Bleeping Computer's founder Lawrence Abrams.  He discovered that it was possible to use either Chrome or Firefox in a headless fashion, that is, no UI shown, to use the browser to render the image of a remote website's page by URL and save the page rendering to an image file, all with never launching the browser itself, which is kind of interesting.



In the show notes I've got a link to Bleeping Computer's post about this, or you could just go to BleepingComputer.com and read down through the chronologically posted items.  Chrome and Firefox can take screenshots of sites from the command line.  Anyway, basically the idea is you open up a command window, and you need to give it the path to Chrome, wherever Chrome.exe is located, then --headless --screenshot=, and then the path to the image.png, whatever you want to call it, and then the URL.



And when you launch this command, nothing appears to happen.  It takes a minute or two or however long, I mean, hopefully not that long, and the cursor drops down and gives you a command prompt again.  If you go look, and everything worked right, you'll find an image file which, if you then open it, is a picture of that URL as it would have been rendered by Chrome.  There are some other additions to the command line possible.  You can specify the window size in width and height.  You can tell it you don't want scroll bars to be shown and a few other things.  And the same thing can be done with Firefox.



So anyway, I don't know specifically how this might be useful, but I just thought it was a real cool hack.  Larry reported that it was quicker and easier to do this with Firefox than with Chrome, but both could do it.  And it does provide you with a means - you can imagine maybe a periodic script to take a picture of a web page and then check to, like, check it for changes or who knows what.  So anyway, just a very cool little hack that I thought was worth sharing.



Oh, boy.  We've talked about the danger of DMA, Direct Memory Access, enabled interfaces.  The first one we encountered was the venerable Firewire.  And it was with some surprise that it was like, we learned that Firewire, cute and small as the form factor was, it's a nice little plug, it's a high-speed serial interface.  And there is a sort of a meta command language that runs over that serial interface that allowed Firewire to directly transfer blocks of memory into and out of the machine.  The idea was, whoever designed this was like, oh, wouldn't this be nifty if the Firewire peripheral, whatever it was that we plugged in, typically a streaming video device, a camera, who knows what it was back in the day.  Probably an optical disk writer or something.  If it could autonomously suck data out of the OS and/or send data back in, how fun.



Well, yes, except that we have talked about for years on this podcast that, if not protected, it opens up a system to, not a remote, but a very potent local attack.  So it turns out that Firewire is known to have this problem.  We have talked about how a very powerful successor to that, known as Thunderbolt, has the same capabilities.  And it turns out that, even though there has been work done on mitigating these problems, somehow no one ever got around to actually turning them on, believe it or not, with the single exception of macOS.  So bravo for macOS.



I have a link to a PDF in the show notes, and I will just share briefly this abstract of the research.  I won't go into it any more deeply than that.  But they wrote:  "Direct Memory Access attacks have been known for many years."  And indeed they have been, which makes it kind of a quandary how we're still so vulnerable to them.  They wrote:  "DMA-enabled I/O peripherals have complete access to the state of a computer and can fully compromise it, including reading and writing all of system memory.  With the popularity of Thunderbolt 3 over USB Type-C and smart internal devices, opportunities for these attacks to be performed casually with only seconds of physical access to a computer have greatly broadened.



"In response, commodity hardware and operating system (OS) vendors have incorporated support for Input-Output Memory Management Units (IOMMUs), which impose memory protection on DMA and are widely believed to protect against DMA attacks.  In this research, they say, "we investigate the state of the art in IOMMU protection across OSes using a novel I/O-security research platform, and find that current protections fall far short when placed within a functional network peripheral that uses its complex interactions with the OS for ill intent.



"We describe vulnerabilities in macOS, FreeBSD, and Linux, which notionally utilize IOMMUs to protect against DMA attacks.  Windows uses the IOMMU only in limited cases, and it remains vulnerable.  Using Thunderclap" - which is what they call their research and their device - "an open source FPGA research platform we built, we explore new classes of OS vulnerability arising from inadequate use of the IOMMU."  In other words, all of our platforms now have it; and it is disabled, believe it or not, almost all the time.



They said:  "The complex vulnerability space for IOMMU-exposed shared memory available to DMA-enabled peripherals allows attackers to extract private data (sniffing cleartext VPN traffic) and hijack kernel control flow (launching a root shell) in seconds using devices such as USB-C projectors and power adapters."  In other words, the mythical evil power adapter, 100% feasible.  "We have now worked closely with OS vendors to remedy these vulnerability classes, and they have now shipped substantial feature improvements and mitigations as a result of our work."



And I have in the show notes a table from their research showing Windows 7, 8.1, Windows 10 Home and Pro, 10 Enterprise, two versions of Enterprise, macOS 10.10 and so forth, Linux, Ubuntu, Fedora, Red Hat, Linux, FreeBSD, PC-BSD.  The sobering thing is there is a "can use" IOMMU.  It is not available for Windows 7, not available for Windows 8, not available for Windows 10 Home Pro.  Only available for the others.  So Windows 10 Enterprise has it, macOS, the Linuxes Ubuntu/Fedora/Red Hat, FreeBSD, PC-BSD.  But it is only enabled by default on one OS, and that's Mac.  In other words, even where it is available for use, it is not enabled.  And then the table goes into the details of their research further.



But what this says is that right now today, if you have typically a laptop was where the vulnerability would be, with Thunderbolt 3 available to an I/O connector, probably a laptop, as I said, with USB Type C connector, someone could plug something into that machine while it's running and steal its secrets in a few seconds.  And what this research demonstrates convincingly.  So their mitigation recommendation is mine.  If you do not know that you need Thunderbolt 3, for example, probably Alex Lindsay, who's doing crazy video stuff...



LEO:  He needs it, yeah.



STEVE:  Needs it and knows he needs it and is using it.  The rest of us, eh.  I have no need for it.  There's nothing I'm doing that is, you know, that I'm attaching a Thunderbolt 3 peripheral to.  Yes, USB 3, for sure.  That's different than Thunderbolt 3.  The point is you can almost always disable Thunderbolt in the BIOS.



LEO:  I'm confused because I guess - oh, but Windows 10 does, well, see, I use Thunderbolt 3 on Windows 10 Home, I thought, but maybe not.  Or Pro.  I'm sure I do.



STEVE:  Well, yes.  And so what they're talking about is that it is vulnerable.  So the IOMMU is the gatekeeper, essentially, for Thunderbolt.  But it allows...



LEO:  But it says Windows 10 does not support IOMMU.  Windows 10 Home or Pro.



STEVE:  Correct.  So Home or Pro...



LEO:  Can't support Thunderbolt 3?



STEVE:  No, has Thunderbolt 3 and is vulnerable.



LEO:  Oh.



STEVE:  The IOMMU is what provides...



LEO:  Oh, it's like Retpoline.  It's that thing all over again.  It's a double-negative product, okay.



STEVE:  Yes, exactly.  So the IOMMU is the gatekeeper that allows Thunderbolt to be used safely.



LEO:  So on macOS when I use Thunderbolt it is safe, then.



STEVE:  Yes.  And only macOS.  Only on macOS.  Yup, exactly.



LEO:  Okay.  So don't use Thunderbolt 3 on Windows 10.



STEVE:  Well, yes, exactly.  If you don't know you need it, and the only one we know who does is Alex Lindsay...



LEO:  And he doesn't use, I promise you, doesn't use Windows 10.



STEVE:  Right.  Oh, that's a good point.  So he's already clear because he's on macOS.  So I would say just reboot into your BIOS.  Or maybe you're on a mountaintop somewhere, or in a cave.



LEO:  Somebody would need physical access; right?  DMA requires physical access.



STEVE:  Exactly.  It is a physical access vulnerability.  So let's also remind ourselves that nobody has ever been attacked ever in the history of man by Spectre and Meltdown, as far as we know, despite all the ink that it has been given.  Similarly, if there's nobody that's going to come along and stick something in  your port that might be malicious, you don't have anything to worry about, either.  But if you might be subject to a targeted attack, or even just a brush-by attack in an airport, where you're using a laptop, or if you're a high-value target, hopefully your enterprise management has already disabled Thunderbolt 3.  It should not be enabled on your device unless you know you need it.



LEO:  Wow.  I had no idea that it was that risky.



STEVE:  Yeah, it is.  It is a port into your operating machine's memory that would allow something to suck it out and take your keys, your BitLocker keys, your encryption keys, anything that is statically available in RAM at that instant.  Or plant things.  It's able to inject something into your system instantly.  Yeah, so remove it.  Disable it unless you know you need it.  It's one of those things where it's like, oh, look, Thunderbolt 3, isn't that wonderful.  Yeah.  And when you buy something that needs it, then turn it on.  Until then, no.  Leave it off.  Turn it off.



LEO:  Wow.  Good advice.  Okay.  All right.  Ready to wrap things up with Steve Gibson.



STEVE:  Yeah.  So we are seeing a lot of bot-based credential stuffing, "credential stuffing" being the new jargon that we've talked about now a couple times.  Once upon a time we had what is now the quaint image of a hacker in his basement, repetitively trying to log into some target victim's account by guessing their password; right?  Typing in candidate after candidate attempt, one at a time, over and over, until, hah, what do you know, I'm in.  That evolved into an automated brute-force attack against someone, first running through dictionaries of commonly used passwords and keyboard keystroke walks and eventually getting down to trying every possible password.



Then we had website-based database breaches where hundreds of thousands of usernames and hopefully hashed passwords were disclosed.  The bad guys would then use high-speed ASIC-based hashing rigs to reverse the hashes in bulk back to their original textual input for use in impersonating the people whose passwords were unlucky enough to be reversible.  And now today we have the latest evolution of the so-called credential stuffing attacks, where fleets of bots, increasingly composed of code loaded into compromised consumer routers, are fanning out across the Internet, not only to replicate themselves, but to launch patient and widely distributed username and password guessing attempts against Internet-facing websites.



Akamai issued a report.  I won't go into it in great detail, but there were some summaries of their stats were interesting.  I do have a link to the PDF in the show notes.  And they covered three different issues.  This story is the first of those three.  They said:  "All three of our stories in this issue of the State of the Internet Security Report are about things most organizations are not examining."  They said:  "Whether the cause is that organizations don't perceive some issues as important to their environment, if they don't have tooling to monitor these issues, or if the resources to monitor this traffic are not available," they say, "this traffic is often being overlooked.



"Although organizations examine the traffic generated by botnets, without specialized tools that traffic is often treated the same as any other type of network activity.  There are very few places where this is more dangerous than in the retail sector," they say, "where botnet creators and retail defenders are playing a multidimensional game, with real money on the line."  Akamai wrote:  "Our team looked at All-In-One (AIO) bots and considered them in the context of the billions of credential abuse attempts," they say, "that we see on a monthly basis."



They said, okay, so here's some numbers.  Between May 1st and December 31st there were 10 million - no, sorry, whoa, I got my zeroes off - 10 billion with a "b," 10,000,588,772 credential stuffing attempts in the retail industry detected on Akamai's network.  In other words, they're looking at their traffic.  Their network is widely distributed and ubiquitous.  So they detected more than 10 billion username and password guessing attempts against retail industry partners over their network.



They said when that's expanded to all other customer industries, so that is just 10 billion on the retail industry, when they expand it to all other customer industries, Akamai detected, okay, just shy of - I'll keep the numbers short because no one cares about all these digits - barely shy of 28 billion.  It was 27 billion, 985 million blah blah blah credential abuse attempts over that eight-month period.  So that works out to more than 115 million attempts to compromise or log into user accounts every day, 115 million a day.



They say:  "The reason for these attempts is not complex.  The malicious actors responsible for them are looking for data such as personal information, account balances, and assets; or they're looking for opportunities to cash in on the online retail market that's expected to hit 4.88 trillion, online retail market at 4.88 trillion by 2021."



The credential stuffing attempts, as Akamai refers to them, logged by Akamai are automated, thanks to bots.  Bots can represent up to 60% - six zero, we've talked about this number before - 60% of overall web traffic are not people clicking links and looking at web pages, but are now automated thingies.  But less than half of them are actually declared as bots, which is often the case.  A bot only, for example, Googlebot and Bingbot I often see cruising around my servers.  Well, they're declaring themselves as bots.  But many times a bot wants to look like a user clicking a link on a Chrome or Firefox or IE browser, so they pretend to be users clicking links on browsers.



LEO:  Is this for click fraud, a lot of it? 



STEVE:  Yeah, yeah.  And just because you wouldn't expect Googlebot to be logging in as a user.  So you could very easily prevent a self-declared bot from successfully logging in with a username and password.  But whereas you'd want to present a non-bot appearance if you're trying to use this credential stuffing attack.  So anyway, and they said that not only are less than half of them declare themselves as bots, many bots are not malicious.  They're for good purposes.  They're like checking to see if a web page has been updated.  Maybe they're looking at prices on other websites.  Who knows what they're doing?



Anyway, Akamai said:  "For criminals, credential stuffing attacks are a numbers game.  They're counting on the fact that people recycle their passwords across different accounts.  When this happens, a compromised set of credentials from one website quickly translates into dozens of others."  The point being that essentially they've become sophisticated enough to take advantage of everything that they see on a successful attack immediately gets relayed across the industry, looking for other places the same credential can be used to log in.  So again, it's one of those, they're taking advantage of every hint and clue that is available.



Oh, and Akamai said:  "Consider the 116 million accounts compromised during the LinkedIn data breach.  Using this list of email address and password combinations, criminals targeted dozens of other websites in hopes that people were using their LinkedIn credentials elsewhere.  These credential stuffing attempts led to several secondary account takeovers.  This is why" - and this of course directly speaks to what you were saying about LastPass.  "This is why security professionals stress the use of password managers," writes Akamai, "as well as the use of long and unique password strings for each website."  And of course that has also created a dependence on password managers because I can't log in anywhere.  I don't know any of my passwords anymore.



LEO:  Yeah, yeah.



STEVE:  So they said:  "The battle against credential stuffing isn't an easy one to fight.  When asked, 71% of the respondents to an Akamai survey conducted by Ponemon Institute said that" - so 71% of respondents said - "that preventing credential stuffing attacks is difficult because fixes that prevent such action might diminish the web experience for legitimate users."  In other words...



LEO:  Well, we know they do.  Look at those silly, god, it drives me nuts now, the new ones, because you're basically - you know what you're doing.  You're teaching the Google autonomous vehicle unit how to recognize cars and storefronts and crosswalks.



STEVE:  That's right.



LEO:  And it pisses me off.  They're using, I mean, my cycles for their benefit.  But I guess they have to with that many bots out there.



STEVE:  They said:  "On average, organizations report experiencing 12.7 credential stuffing attempts each month."



LEO:  Wow.  So now I understand why, I mean, I couldn't figure out why do these sites care if I'm a bot or not?  But now I know, yeah.



STEVE:  Yeah, yeah.  With each attempt, each of those 12.7 on average credential stuffing attempts targeting 1,272 accounts.  So they said:  "The reflexive action to just block the bots responsible for these attempts outright makes sense at first," Leo, as you said.  "But such a move," writes Akamai, "might cause serious harm to the business if legitimate customers are impacted."



They said:  "The same survey revealed 32% of respondents lacked visibility into credential stuffing attacks," meaning they don't even know it's happening.  "And 30% said they were unable to detect and mitigate them.  When asked if their organization had sufficient solutions and technologies for containing or preventing credential stuffing attacks, 70% of those responding said their organization was lacking when it came to such defenses."  So this is something that they're just kind of ignoring and hoping for the best and hoping that their customers secure themselves against these sorts of attacks, sort of saying, well, you know, we're not really looking at that too much.



Okay.  Posted over in the SQRL Forum, CosmaP said:  "Hi.  My phone committed suicide yesterday."  And he has a frowny face.  He said:  "Fortunately, my provider, EE in the UK, was on the ball, and I received a replacement today."  He said:  "Great customer services, a win from EE."  He said:  "Long story short, reinstalled," he said, "well, I am still reinstalling all the apps and came to the SQRL app.  Installed the app."  And he has an Android phone.  He said:  "No problem.  Imported my identity."  He means his SQRL identity.  "No problem.  Entered the rescue code, no problem.  Signed into the forums using my SQRL password, no problem.  All smooth as silk, and I am back operational."  He says:  "It helped that I have all the required info in one place."  He says:  "Job's a good'un.  Regards, Cosma."



LEO:  What info would he need to keep to reinstall his SQRL account?  Is there a QR code or...



STEVE:  Yes.  When you create your identity, you print a page, hopefully.  You can store it as a file, but it's better if you just print it out because we know that paper is one medium with great longevity.  And that's all you need.  That is your one, hopefully for your entire life, identity for SQRL.  And so when you want to bring up another device, or in his case his phone croaked, so he got a new phone.  He just let the Android client see that piece of paper.  And then you can export your identity with or without your password.  And it's slightly safer to export it without your password because then you need the so-called rescue code, which is much higher - because it's 100% entropy, it's not something any user creates.  Users, as we know, cannot come up with anything random.



So he apparently just chose for maximum security to export his identity to paper without his password, which is best for like archival storage.  But that meant in order to use it he had to use the so-called rescue code in order to decrypt it for his phone.  And what we're going to do, Leo, when we get together, we're going to have a SQRL party in your studio, and we're just going to turn a bunch of cameras on and let them run and do all of this.  You, Lisa, Jason, Mike Elgin if he's around, I hope he will be, and I will just go through all this and do it all candid and cover all the what-ifs and everything else.  I think it's going to be perfect.



Oh, and I got an interesting observation from someone, I guess he's a listener.  Oh, yeah.  His name is Jeff Root.  And he wrote:  "SQRL's friction is its lack of friction."  He said:  "I've had the SQRL client on my Android phone for a while, but nothing to use it with.  After watching SN-703, I decided to try out the Forum login.  And it was anti-climactic.  You've been teaching us for years," he says, "that convenience and security are opposites.  The entire security community has been in agreement on that.  But SQRL, by being incredibly convenient and easy, appears insecure because of that.  And so the biggest impediment to SQRL adoption may be that SQRL has zero friction."



Well, I thought that was kind of a fun observation.  Actually, I think this is why it's going to succeed.  I mean, when you experience it, you really do have a thought of, like, and this is secure?  I mean, because it's so easy.  I mean, and, see, I think that's the key is that so far the way we've responded to the problem with usernames and passwords is by just putting people through more hoops.  I mean, I'm now constantly having to go to my one-time authenticator and look up the six digits in order to log into this or that or the other.  And if it's near the end of its expiration, then I kind of have to wait for a new code to be emitted so that I'll know that I'm able to copy those digits over into the form that's waiting.  And sometimes I'm having to switch pages.  It's a pain on an iPad because of having to switch back and forth between apps and go through all this.



So far, our solution has been adding factors and making this a much bigger problem.  So Jeff's point is we've taught everybody that, if you really want security, you've got to do more things.  Well, what we're going to see with SQRL is, because security was designed into it from the beginning, the security is a given.  But its ease of use, I think that's what is going to sell it to users because users don't care about security.  I mean, they're, like, grumbling about having to have different passwords now for all their different websites and having to have, sorry LastPass, a password manager.



LastPass is never going to go away.  Usernames and passwords are never going to go away.  But I wouldn't be surprised if LastPass thought, hmm, maybe we ought to build SQRL into LastPass because that could certainly be done, too, and then we'd have the best of all worlds.



Anyway, we will be talking about SQRL both on a TWiT Live Special and certainly more moving forward.  But I do think, when you see it, Leo, when you have that experience, it's like, wait.  This could be the way we log in everywhere someday?  Uh-huh.



LEO:  You know, it's not two-factor.  You don't need a password at all.



STEVE:  No. 



LEO:  It's your password.



STEVE:  Well, it's not even - you don't even need a username.  See, that's the thing.  All these other things you need to identify yourself.  SQRL identifies you and confirms your identity.  It's a zero-factor.



LEO:  That's nice, yeah.



STEVE:  Yeah, I mean, it is.  Oh, it's freaky.  Also I did want to make a note.  The developer of the Android client asked for some help with his user experience, the so-called UX.  So if we have any user experience experts who would like to volunteer some time and, for example, would like the Android client which Daniel is working on to work the way they think it should, he would be happy to have the client work the way an expert thinks it should.  But we need the expert to tell him.  I have a SQRL feedback form, GRC.com/sqrl/feedback.htm.  Drop me a note.  I will get back to you with a way to contact him.



The way things are going, we have about, I think I looked, it was like 657 people now members in the SQRL Forum, which is just right.  I mean, I'm not inviting everyone there yet because a flood of new users would not help anybody.  The clients are moving forward.  I'm working now, I've got the introduction Q&A, the user Q&A finished.  I'm now working on the what-if.  What if this?  What if that?  But what if this, and what if that, and what if that?  So that's all getting fleshed out.



So I'm still putting the user-facing content together, the goal being that, once everybody does get invited there and come and play, it won't be a disaster.  But for that to be the case, most of this has to be self-serve.  So I'm working on all the self-serve stuff.  But the clients are still - Jeff's iOS client and Daniel's Android client, lots of people are using them, but they're still not feature complete.  And I'm delighted that Daniel is saying, hey, I would welcome assistance with the user experience side.  So if we have any people who have experience developing UI stuff, GRC.com/sqrl/feedback.htm.  Drop me a note, and I'll put you in touch with Daniel.  And everybody would be very thankful for having a better client experience.



I love this little bit of miscellany.  Remember I used the term, I said, whoa, that was a doozy.  And I didn't know if D-O-O-S-E-Y was like, was that really a word?  Matt in London knew where the word came from.  He sent me a note saying "Duesy as in Duesenberg."  He said:  "Hey, Steve.  I heard that a Duesy [D-U-E-S-Y] is named after the Duesenberg car that was so expensive that no one could afford one.  Hence slang for a magnificent failure."  And I thought that was a kick.  Where is that thing?  Wow, that's a Duesy.  Well, turns out it's like, yeah, from the Duesenberg.



And I got an interesting note, Ralph in New York City.  The subject was "SpinRite still working after all these years."  And he didn't mean on one drive.  He said:  "I have a LAN with two WiFi high-def security cameras on the 2.4GHz band.  They are recording to a USB 3 120GB SSD plugged into the router.  Both cameras frequently stream together, but many times only one or the other would record a file, and there were random freezes on many of the files during playback.  I was suspecting a bandwidth issue until I ran a Level 4" - that's the deep pass - "pass of SpinRite on the SSD."  And this was really interesting.  "Watching the real-time screen, I could see random pauses, retries on reads and writes.  After SpinRite completed, both cameras happily record at the same time.  This won't be a surprise, but SpinRite REALLY" - he has in all caps - "works."



And so there's something to think about.  The real-time screen on SpinRite actually shows you the data.  It just flickers on the screen.  But you can easily - and you actually can recognize, like, oh, look, there's my name.  It's like showing you the actual raw data in that drive that SpinRite is working on.  But on a drive which is not responding very well, there will be visible pauses while the drive and SpinRite negotiate this data that it seems to be having a problem with.  And so he was seeing that on an SSD.  And again, running SpinRite on it fixed the problem.  This is why I've talked about SpinRite beyond the 6.x series.  It's clear to me that it has plenty of life left in it because, even if drives stop spinning, although the name will be a problem, there's still going to be a use for it.



Two bits of closing-the-loop feedback.  Fresher in the U.K. sent me a note:  "Where the 'f,' he said, is Jeff's iOS SQRL client?  Mentioned in the last Security Now!, where is it?  It's not in the app store, so what are we supposed to do?  You didn't make it U.S. only, I hope."  Well, first of all, no, because Jeff Arthur is in the U.K. also, Fresher.  But this is to my point.  People who want it enough are able to find it.  And people have wanted it enough.  But at this point we have enough people, I mean, a flood would not be - it wouldn't be good for anybody.  We have enough people working with these things and participating and helping to flesh out content.  So Fresher, if you want it, and you're a clever person, you could find it.  But it is not available in the app store.  So enough said.



Neil Taneja in Chandler, Arizona, asked about drive mounting and unmounting.  He said:  "You mentioned a few Security Nows ago about how you mount and unmount drives automatically for your backups to protect them.  How are you doing that?"



So in Windows, fire up a command prompt and give the command "mountvol," M-O-U-N-T-V-O-L.  What you will see is a list of mounted volumes by drive letter that we're all used to seeing, C: and D: and Z:, whatever, and then a series of \\?\Volume, then open curly brace, and then one of those GUIDs, and then closed curly brace.  That's the volume ID by which Windows identifies the volume.  And the volume has been mounted when it's been associated with a C:\, a drive letter.  Mountvol also, if you do mountvol/? you get a list of things, and you are able to create an association, that is to say, mount one of those and delete the mounting, which is to say unmount one with a /d command.  So, Neil, that should give you everything you need to build that into a little script and allow drives to appear and disappear from your system in order to keep the drive offline when it's not in use.



And lastly, to our listeners who are - or maybe, you know, well, yeah, we know we have listeners of all ages.  We have people who say, hey, how do I get started in security?  I want to consider a career.  What do I do?  Or maybe you've got some free time.  Maybe you're living with your folks, or you're in high school, and you're thinking it'd be fun to see if you could earn some extra cash on the side.  It is truly possible to have a career, if you're good, as a bug hunter, and getting bug bounties.



It was a picture that was covered in HackerOne's posting.  We have a picture of 19-year-old Santiago Lopez who has just crossed the $1 million mark from purely - $1 million U.S.  He is the first bug bounty hunter millionaire, just from finding and reporting security vulnerabilities through HackerOne's bug bounty program.  I noted at the top of the show that we don't know if maybe he's made actually more money than that because he's sold some really tasty ones to Zimperium.  Who knows?  He's got an interesting Twitter feed I would commend people to go poke at.  I've got a link in the show notes also.



And last Friday, March 1st, the BBC ran an interview with Santiago titled "How One Teenager Is Making Millions by Hacking Legally."  Their little summary said:  "This is 19-year-old Santiago Lopez from Argentina."  I have a problem pronouncing that.



LEO:  Argentina.



STEVE:  Argentina.  Gee, yeah.  I've got one more syllable in it.  Argentina, thank you, Leo.  "He's the first millionaire bug bounty hacker, which means he gets paid" - this is the BBC talking.  Of course we all know what they are - "gets paid to find glitches in the software of some of the world's biggest companies.  Mr. Lopez made his money on the world's biggest ethical hacking platform, HackerOne.  BBC News's Joe Tidy has been to see how he spends the money."  Thus the BBC story that I think our listeners may find interesting.  And I have a link, as I said, to that in the show notes.



And also HackerOne did a report published on February 1st, so a little over a month ago.  They said:  "Today the HackerOne community hit $45 million in bounty payouts.  Join us as we celebrate the hackers who are making the Internet a safer place every single day.  The party is going to last the whole way to a history-making $50 million in bounty payouts."  And in this posting that I have, or their PDF that I have a link to, they started off by defining "hacker."  So they show "hacker" and how it's pronounced phonetically, declaring it to be a noun.  And for their definition, which I really like, they said:  "One who enjoys the intellectual challenge of creatively overcoming limitations."  And I think that's a great definition of a hacker, "one who enjoys the intellectual challenge of creatively overcoming limitations."



And again, I would commend this report to our listeners who might be interested.  They have a lot of bios and details about the hackers.  I'll share just the top of it.  They said:  "Welcome to the age of the hacker.  Hackers are heroes.  They are in it for the good, and there is more opportunity than ever before.  We share some of their stories and celebrate their impact in this, the third annual Hacker Report.  The Hacker Report details the more than 300,000 individuals that represent our hacker community today.  It highlights where hackers live, what motivates them, what their favorite hacking targets and tools are, where they learn, why they collaborate, and much more.



"In 2018 alone, hackers earned more than $19 million in bounties, almost the entire amount awarded in the years prior combined."  So it's on a ramp.  "And while the most successful find it very lucrative, it's about so much more than money.  Many are finding career-building opportunities through bug bounties, with companies hiring from within the hacker community at a faster clip than ever before."  And that's a point I've made.  There are some very beautiful pieces of work where I've thought to myself, boy, you know, if I were in the hiring business still, I'd ask this person for a job.  They are good.  They say:  "Companies are utilizing bug bounty reports and hacker engagement as an enhanced resume of proven skills that will impact company goals and security efforts from day one.



"The generosity and camaraderie of hackers continues to impress, with more emphasis than ever before on education, collaboration, and giving back.  As hacking grows in popularity, training continues to be a focus.  With more than 600 hackers registering to join the ranks on any given day, in-depth training modules such as Hacker 101 Capture the Flag challenges are in demand."  They say:  "This past year we saw incredible individual performances such as hackers earning $100K for one vulnerability, and the first hacker [Santiago] passing the $1 million milestone.  We also saw unmatched collaboration, like hackers acting as teams to report over 250 valid customer vulnerabilities."



They say:  "Hackers represent a global force for good, coming together to help address the growing security needs of our increasingly interconnected society.  The community welcomes all who enjoy the intellectual challenge to creatively overcome limitations.  Their reasons for hacking may vary, but the results are consistently impressing the growing ranks of organizations embracing hackers through hacker-powered security" - I like that - "hacker-powered security, leaving us all a lot safer than before."



And they did note that top earners, top hacker earners can make up to 40 times the median annual wage of a software engineer in their home country respectively.  So I think what we're seeing is we're seeing a sea change here, hacker-powered security.  And, you know, if you've got "skillz" with a "z," I think you ought to consider it.



LEO:  Where does HackerOne gets its money from?  Do private enterprises hire them?



STEVE:  Yes.  So bounties are posted by, like, GE and Tesla and others to say we are formally inviting people to try to find problems in our products.  And, if found, we will pay.



LEO:  Yeah, but why go through HackerOne?  Why not just go directly to GE?  Does HackerOne keep a cut?



STEVE:  No, well, they do keep a small cut.  But they're a clearinghouse.  And so they've got the hackers registered with them, and they're able to then post opportunities of, you know, here are things that GE would be interested in having you attempt to find vulnerabilities in, and here are the payouts from GE on various classes of vulnerabilities that you find.



LEO:  Got it.  And they keep 5% or something for that service.



STEVE:  Exactly.  Exactly.  And so they're like the clearinghouse, and they put the hackers together with those offering bounties to have, basically, hacker-powered security,  have their security tested and improved.  I just think it's a win.



LEO:  It's a brilliant business idea, actually, on their part.



STEVE:  Totally see, yes, totally see that for the right kind of guy, who's eclectic and doesn't want to work for the man and has faith and confidence in their own skills, it's something you could start part-time while you're in high school or going through college, see if you've got what it takes.  And just like this podcast that is not running out of material, lord knows, we're at the two-hour and nine-minute mark at this point, so it is very clear the world is not going to run out of bugs ever.



LEO:  Very nice.  And that concludes the thrilling, gripping edition of this week's Security Now!.



STEVE:  Indeed it does.



LEO:  I would like you to spend some time on a future show on how to learn how to do this kind of thing.  We've talked about one of the techniques people use, fuzzing, but there must be other kind of standardized techniques.  I think it would be interesting, I mean, obviously you can't teach us in a two-hour show how to become a white hat hacker.  But maybe some resources and places to look to learn that skill.



STEVE:  Will do.



LEO:  Yeah.  I think it'd be very interesting.  You'll find Steve at GRC.com.  Lots of great stuff there, including SpinRite, the world's finest hard drive recovery and maintenance utility.  Get yourself a copy.  If you've got a hard drive, you need SpinRite.  You can also get this show...



STEVE:  Or even an SSD.



LEO:  I think I'm going to call them all hard drives.



STEVE:  Okay.



LEO:  Right?



STEVE:  Mass storage.  Mass storage.



LEO:  Mass storage.  I mean, a hard drive just means not a soft drive.  I don't know what that means.  It means a hard place you store your stuff.



STEVE:  Yeah.  Okay, good, yeah.



LEO:  Yeah.  It doesn't imply spinning, or does it?  Hard drive.  I think probably it's a good idea to distinguish that from an SSD.



STEVE:  Well, we are seeing that SpinRite is valuable on SSDs.  We keep getting reports, so yeah.



LEO:  You'll also find great free stuff.  He gives away a lot of stuff at GRC.com.  All you need to know about SQRL.



STEVE:  Everything else is given away.



LEO:  Clues to the secret hunt for the iOS app.  All of that.  I don't know if there's clues there or not.  He also has this show, and he has audio of it, and he has a very nicely written transcription of every word so you can read along as you listen.  It's useful for searching, as well.  GRC.com.



Our website, TWiT.tv, has the show as well, audio and video, TWiT.tv/sn.  You'll find it on YouTube, too.  It's everywhere.  In fact, you could subscribe with your favorite podcast client, and that way you'd be guaranteed to get the episode, every episode, the minute it's available.  We will come to you.  We will deliver, hand-deliver, or CacheFly will, hand-deliver a copy to your door.



Steve, thanks so much.  Have a great week, and we will see you next time on Security Now!.



STEVE:  Thanks, buddy.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#705

DATE:		March 12, 2019

TITLE:		Spoiler

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-705.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we look at the zero-day exploit bidding war that's underway, the NSA's release of Ghidra, Firefox's addition of privacy enhancements which were first developed for the Tor version of Firefox, a pair of zero-days that were biting people in the wild, news of a worrisome breach at Citrix, the risk of claiming to be an unhackable aftermarket car alarm, a new and interesting "windows developers chatting with users" idea at Microsoft, a semi-solution to Windows updates crashing systems, detailed news of the Marriott/Starwood breach, a bit of miscellany from Elaine, a SpinRite question answered, and then we finish with SPOILER - the latest research exploiting yet another new and different consequence of speculation on Intel machines.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here with some interesting topics, including why would somebody pay half a million dollars for a hypervisor bug, a step-by-step discussion of how the Marriott hack happened, and a new feature in Firefox to prevent letterboxing.  What?  Yes, it's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 705, recorded Tuesday, March 12, 2019:  Spoiler.



It's time for Security Now!, the show where we cover your security, your privacy, talk a little bit about how the Internet works, how computers work with this guy right here who knows about everything, understands it all, and even is able to communicate it, a rare combination of abilities, Mr. Steve Gibson.  Hello, Steve.



STEVE GIBSON:  Actually, you know, I was seriously thinking about getting a T-shirt printed:  The more I learn, the less I know.



LEO:  Yeah, well, that's the truth, isn't it. 



STEVE:  Because that really is the case.  The arrogance of youth who sort of think, oh, I've got this all figured out.  And then when you really start digging in you think, whoa, this is a lot bigger than I thought it was.



LEO:  Well, see, that's because you're intelligent.  You're not subject to the Dunning-Kruger effect.



STEVE:  What?



LEO:  The Dunning-Kruger effect is a well-known problem of cognitive bias in psychology in which people of low ability have mistakenly assumed that they're brilliant; they're smart.



STEVE:  Oh, well, yeah.  And after all, really, how would you know?



LEO:  Right.  And it turns out the dumber you are, the smarter you think you are.  And the corollary would be the smarter you are, the more you realize I don't know anything.



STEVE:  I have no clue about what's going on.



LEO:  And that's kind of, I think, a form of wisdom, really.  As you get older, you realize, yeah, no.



STEVE:  Well, and I've always said that one of my favorite phrases is "I don't know."  I mean, it really came to light in the early days.  I was working with an engineer who worked for me on v3 of SpinRite.  And this is the first time I sort of had an engineering group, like other people to test things and to interact with.  And his name was Jim.  A neat guy.  He was a GATE child, so I'm sure his mother thought he was just brilliant.



LEO:  Gifted and Talented, or something like that, Education.



STEVE:  Yeah, GATE, exactly.  And so he kept insisting, when there was, like, something in SpinRite didn't work right, he insisted on guessing.  He would just jump ahead and just guess what it was.  Which is not my approach.  And because I was working with him, the contrast of our approaches was very clear to me.  And I said, "Well, Jim, maybe.  But now we have a problem."  And he said, "What?"  And I said, "Well, you have a stake in the outcome now."



LEO:  Ah, yes.



STEVE:  "So we all have egos.  We're all human.  Now you're going to work to prove yourself right."



LEO:  That's right.



STEVE:  My interest is to find the problem, and I don't care.  I mean, I'm not leaping ahead.  I don't need to guess and then plant a flag on it.  I just want to go get rid of it.  And it happened over and over and over.  And I'm not sure I ever worked him out of that problem.  But it did, it highlighted for me, as these things often do, that my nature is to sort of say, hmm, okay, let's go find what's wrong.  And also I would say to him, "Well, that might be.  But it might not be."  And the point being that the beginning of the search for knowledge is to say I don't have any.



LEO:  Yeah, I don't know.



STEVE:  I don't know.  And then that's where you want to start.



LEO:  We should teach kids that in school, that it's okay not to know; that it's good not to know.



STEVE:  Yeah, well.  It's probably just genetic and hormonal, and at this point...



LEO:  Yeah, it probably is.  As we know, none of this is rational; right.



STEVE:  And you also sort of have to be able to afford not to know.



LEO:  Yes.  That's a luxury, too. 



STEVE:  And there is a - exactly, exactly.  So this is Episode 705 for March 12th.  This is Patch Tuesday.  And as always happens, I'm so, like, every second for the last eight hours, four hours this morning, four hours yesterday, has been in assembling this podcast for our listeners, that I ran out of time, didn't have any chance to go digging.



So next week I will - in fact, there's one real question because there's a zero-day in a 32-bit version of Windows 7 which is being exploited in the wild, or was being when it was being leveraged by another zero-day in Chrome, which Google immediately killed last week.  But I'm curious to know whether Microsoft also killed it in the 32-bit version of Windows 7, and I don't know.  But I'll find out.



"Spoiler" is the title of this week's podcast, and it's supposed to be in all caps.  But because it's like kind of an abbreviation, except the research paper titled "SPOILER" in all caps, then says, colon, and then this name, somewhere I read that they just got - they were so tired by the time they got through with the research they didn't have the oomph left to reverse engineer a name for the paper that created Spoiler as an abbreviation.  So they just said, okay, we're going to call it Spoiler, and it doesn't stand for anything.  But it is interesting.  We're going to wrap up the podcast as we typically do by talking about that in some depth.



But we're first going to look at a zero-day exploit bidding war that is now underway.  We have the NSA's release last week of Ghidra that we predicted.  That was at the RSA conference that the NSA formally released Ghidra, that is, their interactive reverse engineering tool.  So we'll talk about that.  We have Firefox's addition of privacy enhancements which were first developed for the Tor version of Firefox.  This, as I was mentioning, a pair of zero-days that have been biting people in the wild, one in Win7 (32) and the other combined in Google's Chrome browser.



News of a worrisome breach at Citrix, and sort of their CSIO really underplayed it, I think, compared to some later news that came out.  The risk of claiming to be an unhackable aftermarket car alarm.  You never really want to say "We're unhackable."  It's like, oh, that's just dangling the bait in front of the penetration testers.  And oh, boy, did we find out otherwise.



We have a new and interesting "Windows developers chatting with end users" idea from Microsoft.  A semi-solution to Windows Updates crashing systems.  Detailed news finally of the Marriott/Starwood breach.  A bit of miscellany from Elaine.  A SpinRite question answered.  And then we're going to finish with Spoiler, the latest research exploiting, get a load of this, here we are a year downstream, yet another new and different consequence of speculation on Intel machines.



LEO:  Oh, no.  Oh, no.



STEVE:  Yes.  It turns out there's another thing Intel did to get more performance that is speculative, and it allows a dramatic leveraging of Rowhammer.



LEO:  Hmm.



STEVE:  So, yeah, so it's sort of the - and that's part of the title of their paper that we'll be getting to.  And we have, nobody knows yet, the Picture of the Week.



LEO:  What are you laughing at?



STEVE:  Now I'm wishing that I had titled this podcast "Vague, but Exciting."



LEO:  Okay.  Okay.



STEVE:  Because that was the comment that Tim Berners-Lee's boss wrote at the top of his document which proposed the creation of something which grew into, 30 years ago...



LEO:  That's hysterical.



STEVE:  ...the World Wide Web.



LEO:  Oh, my god.  Vague, but exciting.



STEVE:  Vague, but exciting.  So our Picture of the Week is a picture of the document.  And you can see in the upper right-hand corner it says March 1989.  So that was 30 years ago.  And what is not clear, and I'm not sure how much time Tim's boss spent staring at this diagram, but this is a self-referential diagram that is a diagram about this document because you can see where, in the lower right it says "CERN, DD division, OC group, RA section, Tim Berners-Lee."  So there's a hierarchy.  And there's an arrow, "wrote This document," which then describes - it's pointing to "Hypertext," which includes "Hypermedia" pointing down, and "includes Linked information for example."  And anyway, so it's...



LEO:  Hyper Card's actually in this document, which is cool.



STEVE:  Yeah, yeah.  And in fact there was some commentary that suggested that Hyper Card, had it been network-aware, which it wasn't, could have swayed things in a very different direction because, I mean, Hyper Card might have had a very different future had it been network-aware.  So anyway, this thing is under the heading "Information Management:  A Proposal."  The abstract - and again, I don't disagree with the boss who said "Vague."



LEO:  Vague, but exciting.



STEVE:  Yeah.  The abstract says:  "This proposal concerns the management of general information about accelerators."  And we're talking about linear or circular, I mean, like atomic accelerators.  I can't think of the right word.



LEO:  Particle accelerators.



STEVE:  Particle, thank you, particle accelerators, "...and experiments at CERN.  It discusses the problems of loss of information about complex evolving systems and derives a solution based on a distributed hypertext system."  That's all.  I mean, so there's the abstract.  And so you can imagine the boss saying, "Huh?  That's kind of vague, but I'm excited."  Anyway, so the point is that it was 30 years ago that Tim wrote this letter or produced this proposal which foretold - and actually, I mean, from that, over the course of sort of some evolution.  I have a link to it below the picture, if anyone is interested.  Anyway, I just wanted to commemorate this...



LEO:  Yeah, it's a big deal.



STEVE:  ...this 30-year-ago history.  And so begat what we now know as the World Wide Web.  And I'm thankful.  I mean, it's interesting.  He talks about the loss of information.  And we do have, although there is some link fade where you go after something where the domain got lost and something that we were hoping to find is gone, although we do have the web archive.  And I found myself several times actually in the past week using the web archive to find some information, which it had grabbed following links and archiving of domains, that had since been disappeared.  So it is nice to have that.  But it also is the case that, boy, just anything you want to know.  I just make so much use of the Internet for, you know...



LEO:  Oh, my, yes.



STEVE:  ...expanding my knowledge.  Okay.  So last week's topic, as we know, was "Careers in Bug Hunting."  While we were delivering that podcast last Tuesday, and although we were primarily talking about HackerOne, I mentioned Zerodium as an admitted alternative cash-out source if someone found a particularly tasty and important zero-day flaw, "important" meaning in something where it really matters, like in a named mainstream product, that would allow somebody exploiting the flaw, we won't characterize them, to gain some information that they're trying to get.



As it happens, while we were delivering that podcast, Zerodium was upping the ante.  They tweeted:  "We're paying up to $500,000 for zero-day exploits targeting VMware ESXi," and they said in parens "(vSphere) or Microsoft Hyper-V, and allowing Guest-to-Host escapes."  In other words, VM escapes to get to the underlying OS.  They said:  "The exploits must work with default configs, be reliable, and lead to full access to the host.  Contact us," and then there was a link in the tweet.  And they signed off "Zerodium (@Zerodium) March 5, 2019."  Half a million dollars for something that someone discovers in Hyper-V or in VMware.



So it turns out Zerodium does, and has in the past, done what they call "acquisition raids," offering limited time increased incentives for classes of flaws they have a particular interest in obtaining, either I presume to balance out their exploit portfolio - and of course, as we know, these flaws are being found and patched on an ongoing basis.  So there is some churn in their portfolio.



And, boy, I would love to be on the inside of the contract that they have with a purchaser of one of these to get a sense just for what it looks like.  Clearly they're paying a lot for it, and they're not wanting it to be disclosed.  It is in the interest of the purchaser of the flaw who wants to exploit it not to disclose it so that they can use it against presumably in targeted attacks.



This is nothing you're going to spray all over the Internet because it'll be found immediately by all of the different security services that are looking for anything like this.  So, and presumably the purchaser, if Zerodium is paying half a million, we don't know how large the other side market is, you know, how many people are purchasing these.  But they're going to be paying a pretty penny.  We do know that it's typically governments and intelligent services of governments that are paying.



LEO:  So that's how this economically works with both Zerodium and HackerOne is that they're a marketplace.  They're reselling it on to somebody else.



STEVE:  No.  HackerOne is not a reseller.



LEO:  Zerodium is.



STEVE:  Yes.  HackerOne, for example, would be contracted by GE to be like the middleman to handle all the mechanics and payment and terms and conditions.  So the idea would be, GE being a responsible provider of software recognizes that the best way to get their bugs found is to offer a bounty.  So there, HackerOne is a contracted middleman.  But Zerodium is very different.  Zerodium, you know, they're selling them to the dark side, arguably.



LEO:  Got it.  Got it.



STEVE:  Anyway, so they do these periodic what they call "acquisition raids" to increase the incentive, either to balance out their portfolio, maybe as I said some of the things in their portfolio are killed because they're patched, or perhaps they have a customer who is making a particular request.  And so, again, they're standing between the security side, us, the public, and the back side.  And we don't know who is, you know, what the terms and conditions or anything about the interaction on the other side of this.



So maybe they have a customer who says, okay, we really have a need for a Hyper-V zero-day.  So we're willing to give you a quarter million if you can get one to us.  And so Zerodium turns around and says, ah, well, we can resell that several times over.  So let's bump the bounty for this to half a million dollars, knowing that, I mean, half a million dollars.  That would be some significant incentive.  It's not like, oh, yeah, we'll pay you 10 grand if you stumble over a browser escape or something.  It's like, okay.  So you could imagine that's going to have an effect.



In this acquisition raid, they bumped the payout for a high-quality solid and stable hypervisor exploit from what it had been, 200 grand, to $500,000.  ZDNet in their coverage asked Zerodium's CEO what the deal was.  He replied by email, saying:  "Our new payout for hypervisors will last for a couple of months, and then we'll decide if we reduce it or keep it high, depending on the number of acquisitions we make."  And that kind of gave me a chill.  It's like, whoa, the number of acquisitions, suggesting not one, but oh, look, we got five new zero-days.



Anyway, Zerodium has previously held such acquisition raids for zero-days in iOS, in instant messaging apps, in the Tor browser, a.k.a. in the Firefox version of the Tor browser, in Linux, in Adobe Flash Player, in routers and USB thumb drives.  And, okay, so the fact that they've done it for the Tor browser, you know, that sort of gives you a clue about who's on the other side buying them because they're not going to be bumping the bounty  unless there's some reason to believe that they've got a buyer or buyers of these specific things.



So what's really going on here, believe it or not, is it's becoming a bidding war.  And Zerodium was being outbid.  Last summer, we talked about this at the time,  Microsoft made the decision, and I salute them for this, to buy up its own flaws when they upped the ante for Hyper-V bugs to a quarter of a million dollars.  So Microsoft has been and is officially on the record saying we will purchase a bug in Hyper-V for a quarter million dollars.  Well, if that's the case, first of all, that outbids Zerodium and other exploit buyers who are only offering - only, only - $200,000.  And whereas selling a powerful zero-day to Zerodium would leave me feeling a bit compromised, I would feel 100% great selling Microsoft one of their own bugs.  That would be wonderful for a quarter million dollars.  Right?



LEO:  Yeah, it's a win-win.  Everybody wins, including Microsoft Windows users.



STEVE:  Yes, yes.  And you're just selling Microsoft their own bug.  Here you go.  I'll be happy to take a quarter million dollars for telling you about a problem I found in your stuff.



LEO:  Right.  That's ethical.



STEVE:  That's cool.  That's 100% cool.  But Zerodium's CEO told ZDNet:  "Microsoft's bounty for Hyper-V exploits is very attractive for researchers.  However," he said, "VMware is not paying anything to zero-day hunters.  We have decided to fill this gap," as he put it.



LEO:  Oh, they're such nice guys.



STEVE:  Isn't that nice?  We're going to fill that gap because nobody's paying anything for those problems in VMware.  He says:  "...and we've been paying $200,000 for such exploits.  And," he says, "we've acquired many of them so far."  He said:  "However, we've recently observed an increase in demand from customers, and we have decided to increase the bounty to $500,000 to outbid vendors and all existing buyers."



LEO:  Wow.



STEVE:  So specifically saying to outbid Microsoft, to double what Microsoft has been offering, Microsoft being an existing buyer of problems in their own product.



So Zerodium claims that the company's hungry customers are government and law enforcement agencies, and these do seem like government-level purchase prices.  So again, we don't know how big the market is on the dark side.  But there obviously is one if they're willing to pay half a million dollars for a Hyper-V or VMware ESXi escape to the host.  And the idea that we'll see how many we get, then we'll decide if we want to keep them coming at half a million dollars each, or drop it back down to be, you know, I mean, doesn't make any sense.  I mean, if they were offering it for a quarter million, and Microsoft was offering it for a quarter million, I'm selling it to Microsoft because selling it to Zerodium gives me a creepy feeling.  I mean, Zerodium gives me a creepy feeling.



But again, you know, I mentioned them last week because if we had somebody who decided to make a career out of finding problems, and they did find a problem that was significant, I couldn't fault a bug hunter for selling it to the highest bidder.  It's not fair, really; but if it's your bread and butter, I mean, if it's not something you just stumble upon, again, half a million dollars is half a million dollars.  So, wow.  Anyway, we are now having a bidding war is the bottom line between companies, well, at least in the case of Microsoft, who is saying, yeah, we'd rather buy them ourselves than, I mean, look.



From Microsoft's standpoint, look at it.  Zerodium was offering 200,000 until now.  Microsoft said, okay, we'll see you and up you 50,000.  So Microsoft was at a quarter million.  Being willing to outbid Zerodium for flaws in their own software, feeling that that's better than allowing Zerodium to purchase them for 200,000 and resell them who knows how many times to who knows who.  Because there's no reason to believe that these things aren't going offshore.  So Microsoft, I think, did the right thing.  Is Microsoft going to outbid Zerodium?  I don't know.  Anyway, interesting to see what a weird market has developed around flaws.  And, boy, there's money in them there flaws.



Last week, as predicted and planned and as we talked about several months ago, the NSA did release Ghidra v9.  That was last Wednesday during the RSA conference.  This is, I'll remind our listeners, their free, powerful, and mature interactive multiplatform reverse engineering tool.  The official site - you should bring this up on the screen, Leo.  It's neat.  It's that second link there.  G-H-I-D-R-A hyphen S-R-E dot org.



LEO:  Lisa talked to the NSA at RSA and actually met one of the developers of Ghidra.  They were all excited about Ghidra at the RSA.



STEVE:  Yeah.  After our mention of it a couple months ago, I received all sorts of terrific feedback about Ghidra's back story and proper pronunciation because I was clueless.  I think I was saying "g-hydra" or something.  Anyway, several people said, "Uh, Steve, it's Ghidra."  Anyway, there's sort of an IDA, if you stretch, in the middle, and that's for Interactive Dis-Assembler.  So that's where this word came from.



LEO:  Oh, oh.



STEVE:  Yeah.



LEO:  I get it.  But it's also Godzilla's buddy.



STEVE:  Yes, exactly.  Anyway, Ghidra has been used internally at the NSA and other similarly closely aligned government agencies, like the CIA we talked about a couple months ago, for more than 10 years, during which time it's been evolving and developing continuously.  It will doubtless prove to be extremely useful for anyone researching the operation and security of closed source software, and of course including for reverse engineering malware.  The NSA explained that their general plan was to release Ghidra to enable security researchers to get up to speed and get used to working with it, get this, before applying for positions at the NSA.



LEO:  It's a recruitment tool.



STEVE:  Uh-huh. 



LEO:  Wow.



STEVE:  Yeah, isn't that interesting?  Or other government intelligence agencies with which the NSA has previously shared Ghidra privately.



So as we explained when we first noted this coming release, Ghidra is a free alternative to IDA Pro - IDA, I-D-A, Interactive Dis-Assembler - which is a similar reverse engineering tool that's only available under a very expensive multi-thousand dollar, I mean, like several thousand dollars, priced license.  So by being offered for free, and soon to be open source, they're still in the process of getting it ready for open source release.  There is a link.  You can find a Ghidra at GitHub, but it points you over to the site that you went to and brought up on the screen.  So by being offered for free, of course, most experts expect Ghidra to snap up a big portion of the reverse engineering tools and market share immediately, especially since the reviews have been almost entirely positive.



On GitHub the NSA has this to say.  They wrote:  "Ghidra Software Reverse Engineering Framework."  And that's what the SRE is, Software Reverse Engineering.  They said:  "Ghidra is a software reverse engineering framework created and maintained by the National Security Agency Research Directorate.  This framework includes a suite of fully featured high-end software analysis tools that enable users to analyze compiled code on a variety of platforms including Windows, macOS, and Linux.  Capabilities include disassembly, reassembly, decompilation, graphing, and scripting, along with hundreds of other features.  Ghidra supports a wide variety of process instruction" - they actually meant processor, they wrote process - "processor instruction sets."  And my god, I mean, I saw the 6502 is there.  So if you've got an Apple II or a Commodore 64...



LEO:  Oh, finally I can disassemble Chopper Command.



STEVE:  Exactly, or Dig Dug or...



LEO:  Super Breakout.



STEVE:  Lode Runner.



LEO:  Load Runner, ooh, I would love to disassemble that, yeah.



STEVE:  Anyway, "...and executable formats and be run in both user-interactive and automated modes."  So literally it will emulate the processor, and you can run the code in Ghidra.  "Users may also develop their own Ghidra plug-in components and/or scripts using Java or Python."  They wrote:  "In support of NSA's cybersecurity mission, Ghidra was built to solve scaling and teaming problems on complex SRE [Software Reverse Engineering] efforts, and to provide a customizable and extensible SRE research platform.  NSA has applied Ghidra SRE capabilities to a variety of problems" - oh, don't you know it - "that involve analyzing malicious code and generating deep insights for SRE analysts who seek a better understanding of potential vulnerabilities in networks and systems.



"This repository" - meaning at GitHub - "is a placeholder for the full open source release.  Be assured efforts are underway to make the software available here.  In the meantime, enjoy using Ghidra on your SRE efforts" - I mean, I am so tempted to play, but I can't - "developing your own scripts and plug-ins..."



LEO:  Steve.  SpinRite 6, Steve.



STEVE:  I know, I know, "...and perusing the over one million lines of Java and Sleigh code released within the initial public release.  The release can be downloaded from our project home page.  Please consider taking a look at our contributor guide to see how you can participate in this open source project when it becomes available.  If you are interested in projects" - here it is.  "If you are interested in projects like this and would like to develop this and other cybersecurity tools for NSA to help protect our nation and its allies, consider applying for a career with us."



So anyway, Leo, this is going to be big.  I mean, I'm sure that IDA Pro has been cracked, and you can get cracks for keys and so forth on the 'Net.  There's just no way that a tool with that kind of profile that is being sold for that kind of money hasn't been cracked by the crackers and available.  But having an arguably superior tool, as I said a couple months ago, this announcement was not good news for the folks at Hex-Rays that have been arguably extorting the market because they had a captive market for years.  That's over.



"Installing Ghidra is as simple as unpacking a zip archive.  The only requirement is a version of the JDK [Java Development Kit] 11 or later, which is needed to run the app's GUI.  The tool's official docs have the following to say about installation."  Well, or quasi installation.  They said:  "Ghidra does not use a traditional installer.  Instead, the Ghidra distribution file is simply extracted in place to the filesystem.  This approach has advantages and disadvantages.  On the upside, administrative privilege is not required to install Ghidra for personal use.  Also, because installing Ghidra does not update any OS configurations such as the registry on Windows, removing Ghidra is as simple as deleting the Ghidra installation directory."  So you just dump the files out of the zip, and you're ready to go.



They said:  "Besides an installation guide, Ghidra's docs also come with classes and exercises for beginners, intermediates, and advanced levels that will help" - I mean, the NSA is really going for this.  Although this was probably all developed for partners like other people at the CIA and other law enforcement, where they wanted to do reverse engineering, but they were like, huh?  Okay, we unpacked it.  Now what do we do?  So the point is there's a lot of support here to help people get it going.



And they said:  "This will help users get used to the tool's GUI, which is very different from any similar tools," i.e., IDA Pro.  So this is not good news for the Hex-Rays folks, but it was inevitable.  And, boy, I mean, this is a strong offering.  And Leo, I think this is going to be significant.  The idea where you have a tool like this, I mean, I really think this is enabling of, for example, this is the perfect follow-on to last week's subject of a career as a bug hunter.  You cannot bug hunt unless you have a way of looking at code.  And here it is, free for the download, a potent, world-class tool for allowing someone to start looking into code and developing an understanding of what it's doing and how it works.  I just think this is very cool.



LEO:  Now they just have to release a fuzzing tool, and we'll have the complete set.



STEVE:  Exactly.



LEO:  We can do all the hacking, all the reverse engineering, bug hunting, yeah.



STEVE:  Wow.  Wow.  Interestingly, Firefox is adding something to their mainstream browser which the Tor version of Firefox has added a while ago.  That's an anti-fingerprinting technique called "letterboxing."  And as I was reading about this, I was thinking, really?  So I suppose that I underestimate how determined online advertisers and profilers are to track us.  And I guess I'm not that worried.  But I certainly get it, that knowing all the places that a user goes can reveal a great deal of information about them.



So I suppose I'm glad that my browsing habits are likely to put anyone tracking me to sleep because I'm not very interesting from a "where do I go with my browser" standpoint.  But I also do get it that the idea of unseen tracking and profiling can deeply offend the sensibilities of many users.  They are, after all, not being asked and not being remunerated in any fashion for indirectly providing the history of their use of the Internet to unknown and unseen third parties who are apparently making money from the collection of the data of, to whatever degree that can be determined, who they are and where they go and what they do.  So my underestimation of the determination of the trackers is revealed by the fact that Firefox thinks this is necessary.



Okay.  So the Tor Project uses a version of Firefox, as I mentioned.  The Tor folks have gone to, and there I can understand it, the extreme lengths to protect the privacy which their users very clearly want.  We know that JavaScript running on a web page, and specifically in advertisements where JavaScript runs, is able to query for the size of the browser's display window.  This is super useful for making responsive websites, which dynamically alter their appearance to deliver a good experience, regardless of viewport size.



This is the way state-of-the-art sites are able to reconfigure themselves.  The script that loads the page or that loads on the page immediately looks to see where am I?  Am I on a desktop?  Am I on a mobile platform?  What's the shape of the window?  How big am I?  And then builds a web page, essentially, on the fly in order to suit the environment that it's in.  So that's script that needs to know about its viewport.  So definitely important.  But the instantaneous size of the browser's window, not surprisingly, can also be a signal that leaks identifying and de-anonymizing information to web page scripts.



And of course this is especially true when a user has their browser's window at some intermediate height.  I actually sometimes do.  If I've got a sufficiently large screen, I'm not running edge to edge or top to bottom.  It may be sort of floating a little bit.  Which means that JavaScript running on any frame for any reason on any tab of that browser will see a much more unique and arbitrary height and width.  And in fact I think the script can also get the coordinate of the upper left corner, which would give it additional information.



Anyway, the point is that that could provide substantial disambiguating information to anyone who was tracking with serious intent.  As we've discussed in the past from time to time, the mainstream Firefox browser has been backporting various enhancements for privacy, which were initially developed over on the Tor side, back into the mainstream Firefox.  And this will happen again in two months, in May, with the release of number 67 of Firefox.  It will inherit this so-called "letterboxing."  Letterboxing deliberately masks the browser window's true dimensions by rounding the window width and height down to even multiples of 200 pixels in width and 100 pixels in height, respectively, during window resizing, which generates a much less unique window dimension for its users.



And of course to do this they're having to jump through some hoops.  So during dragging, gray space will be added at the top, bottom, left, or right of the current page as needed to fill in the gaps.  And if that sounds like something you don't want, and I'm certainly not as concerned as perhaps I should be, I don't really want this.  The good news is that this new behavior will be a capability only and will be disabled by default.  Once we get to release 67 in two months, you'll have to set privacy.resistfingerprinting to "true" in Firefox's about:config page in order to enable this letterboxing, which gives tracking scripts one less exact measure to track.



I wanted to share what Mozilla wrote on their Bugzilla page about this.  They said:  "Window dimensions are a big source of fingerprintable entropy on the web."  And I don't doubt that a bit.  "Maximized windows reveal available screen width and height, excluding toolbars; and full-screen windows reveal screen width and height.  Non-maximized windows can allow a strong correlation between two tabs" - now, that's something that had not occurred to me before, but that's also true - "in the same window."  Then they said:  "While bug 1330882 takes care of new window creation, it would be ideal to protect windows continuously, even if users resize or maximize their window or enter full screen."



They wrote:  "For Tor Browser we experimented with an approach to dynamically round the viewport," that is, the content rectangle dimensions, "to multiples of 200 by 100. In our prototype, when a user drags the corner of a window, the viewport 'snaps' to the largest contained multiple of 200 by 100, leaving a temporary margin of empty gray space in the window chrome.  Then, when the user stops resizing, the margin of the window shrinks to nothing so that the outer chrome tightly encloses the viewport again."



So what you would see would be you'd be smoothly dragging the lower right corner of the window, or I guess any corner of the window, and the contents would be sort of holding back.  Say that you were increasing the size.  The contents would be holding back until it had the ability to grow by a multiple of 200 in width or 100 in height, and then it would snap to that next size.  And in the interim the margin would get filled with gray.  So again, this is where I'm kind of thinking, whoa, really?  Are you guys that worried about this?  Apparently.  Then, when you let go, the outer chrome would snap back to tightly enclose the inner viewport so you wouldn't be left with this gray outstanding.



So anyway, that's what they're planning to do.  And it'll be available to anyone who is serious about wanting to reduce the amount of fingerprinting available to anyone who is watching them very closely in Firefox.  As I said, I'm not that concerned about it myself, but I wanted to bring it up.  And I'm sure I'll mention it briefly in two months when 67 lands.



LEO:  You saw the thing Mozilla also did.  They're really cranking it out.  The new Firefox Send?



STEVE:  No.



LEO:  Oh, yeah, this is a cool feature.  Private, encrypted file sharing through - actually, you don't even have to use Firefox.  You go to send.firefox.com.  If you're just somebody off the street, you can send up to a gigabyte.  If you sign into your Firefox account, 2.5 gigabytes.  It gives you an anonymous URL which you can click, HTTPS downloads.  It's a pretty nice feature for people who need to send files.  Now, I know you had a system that sent much larger files.



STEVE:  Filemail that I had discovered.



LEO:  But this is right in the browser.



STEVE:  But still, 2.5 gigs is great.



LEO:  Ain't bad, yeah.  And I think most people who use Firefox have an account.



STEVE:  And I have a Firefox, I mean, I am a Firefox user, and I do have an account because I used it to synchronize my tabs between locations.  That's interesting.  So I upload a file to them, they give me a link which I then send to somebody else.



LEO:  That's right.



STEVE:  Do you know how long the file stays available?



LEO:  It's temporary.  Let me see if it says.  They do say it's temporary, with a link that automatically expires.  Let me upload something and see how long.  It's drag-and-drop.  Oh, you can expire it after one download to 100 downloads, or one day to seven days.  And you can password protect it, as well.



STEVE:  Nice.  Filemail is seven days.  Ah.  And you have to pay Filemail in order to add password protection.



LEO:  No, that's all for free.



STEVE:  I'm glad you brought that up, and I'm glad our listeners all heard you.



LEO:  It's a great, I think a very useful service.  And, you know, 2.5GB is a lot, especially since you can break it up.



STEVE:  Yeah.



LEO:  Yeah.  Steve?



STEVE:  So send.firefox.com is just - it's not a button in the browser.  It's a site.



LEO:  I would guess a service for Mozilla, yeah.



STEVE:  Yeah, exactly.  And so, for example, for me, bing, I mean, 2.5GB, that's plenty.  And I like the fact that I can encrypt with a password.  They say:  "Simple private file sharing.  Firefox Send lets you share files with end-to-end encryption, and a link that automatically expires."  Now, wait.  End-to-end encryption.  So that would...



LEO:  So they can't see it; right?



STEVE:  Right.  But I'm thinking that means the recipient has to be able to decrypt.



LEO:  Oh, that's interesting.



STEVE:  So the link that you - yeah, the link you get must be to send.firefox.com.



LEO:  It is.



STEVE:  So that you - okay.  So that then that means - so you send someone that link.  They click the link.  It brings them to send.firefox.com, which then allows them to - that allows your browser to decrypt the blob that comes down.  Anyway, they say "...encryption and a link that automatically expires so you can keep what you share private and make sure your stuff doesn't stay online forever."



LEO:  Which is, of course, what you want.



STEVE:  Yeah.  And so they're sort of selling the idea of expiration, which actually is a practical matter for them.  They don't want to have all these 2GB blobs sitting around, stuck forever.



LEO:  Right.



STEVE:  Which is, you know, it made sense for Filemail, too.  But I do like the idea that you can password-protect something that is sensitive and that it's encrypting it in your browser and then decrypting it in the recipient's browser.  So I'm sure there's some browser-side JavaScript, probably in WebAssembly, that's doing encryption on the fly.  That's just really cool.



LEO:  Yeah, they're really - this is, they say, part of their manifesto.  This is what they're doing.  I guess this has been around as a test, but they're making it official, even say they'll have an Android app.



STEVE:  Oh, wait.  So you give it a password.  That would mean that you'd have to send the recipient the link and the password, which then they put into their browser in order to perform the decryption.



LEO:  Right, symmetric encryption.



STEVE:  Yeah.  Nice.



LEO:  So, yeah, pretty cool.  That's a nice service.  These guys, this is really - I don't know if it's a nonprofit.  They make a lot of money in that Google search bar there.  But they really are certainly altruistic in their efforts.  I think it's really good.



STEVE:  I'm very pleased with what they're doing, yeah.  It's still my browser of choice because it's got integrated tabs.  And Google just seems to refuse to do side tabs for some unknown reason.



LEO:  You're a tab guy.



STEVE:  Oh, my goodness, I couldn't survive without them.  Oh, and by the way, in case anyone's listening - oh, shoot.  What's it called?  I should have prepared.  The tab tool that I use uses CSS to customize the tabs.  And as a consequence of a recent evolution of the Firefox browser, Tree Style Tab.  And I saw somebody somewhere said that they weren't happy with it because their tabs were too large.  And like me, I want my little - I want itty-bitty skinny tabs so that I can stack a whole bunch of them.  And it turns out it is CSS style sheet customizable with lots of sample CSS styles already pre-canned.  So you can do all kinds of things.  So for what it's worth, I really like that free style or Tree Style Tab.



Okay.  So Chrome and Windows 7(32) were suffering an in-the-wild zero-day.  Week before last, on Wednesday, February 27th, and coming to light a week later, so last week, last Wednesday, security researcher Clement Lecigne, who is with Google's Threat Analysis Group, discovered and reported a high severity vulnerability in Chrome that could allow remote attackers to execute arbitrary code and take control of the computers that the end users were using.  And Google has warned that this zero-day remote code execution vulnerability is being actively exploited in the wild to attack Chrome users.



The vulnerability was assigned a CVE of 2019-5786 and affects the web browsing software for all major operating systems, that is to say that the Chrome vulnerability is present in all major operating systems - Windows, macOS, and Linux.  And until a majority of Chrome's users have been auto-updated, the Chrome security team is deliberately keeping all technical details to themselves.  They've only stated that the trouble is a use-after-free vulnerability in the FileReader component of the Chrome browser, which leads to remote code execution attacks.



And we've talked about this kind of vulnerability before.  A use-after-free refers to memory that was temporarily, often briefly allocated by the operating system when the browser had something it needed to do.  You can imagine that FileReader would be that kind of thing.  After that memory is no longer needed, it is released.  It's freed, that is, the use-after-free.  So it's freed back to the operating system.  But due to a coding error in Chrome, essentially an accessible pointer to that memory persisted, and it could be used by a sufficiently clever attacker to execute code of the attacker's choosing on the user's PC.



And so Google security said:  "Access to bug details and links may be kept restricted until a majority of users are updated with a fix."  And as we know, that won't take long because Chrome is constantly updating itself.  I always have, since I do run Chrome, it's got stuff running on my machine 24/7, essentially, checking to see if there's anything new and updating itself autonomously.  So I'm sure this wouldn't take long.  And this is, again, an example of, sad as it is, the fact that one way or another today's software that has an attack surface, and as we know nothing has a larger attack surface than a web browser, today's software that has an attack surface has to be able to update itself within short notice.



Anyway, they said:  "We will also retain restrictions if the bug exists in a third-party library that other projects similarly depend upon, but haven't yet fixed."  So they're going to look to see how pervasive this is and make sure that it's not - if it's not just constrained within their browser, they may not tell us for a while, despite the fact that Chrome has long since eradicated the problem.  Anyway, so this FileReader API is a standard JavaScript API that's been designed to allow web applications to read the contents of files or raw data buffers stored on a user's computer using either "file" or "blob," as they're called, objects to specify the file or data to read. 



That was Wednesday of last week.  The next day, on Thursday, we further learned that the active in-the-wild attack was actually leveraging a pair of zero days, the one in Chrome being the first, and a previously unknown zero-day existing, as far as we know, only in Windows 7 (32), but not affecting Windows 10.  And notice that it was observed on the 32-bit versions of Windows 7.  We don't know for sure that it doesn't affect other versions.  And this is what I was saying I also don't know because Microsoft has known of it for two weeks.  Well, that's not a long time in Microsoft's world to prepare a patch, depending upon what it is, for a Patch Tuesday that was today.



So they only had apparently two weeks' notice because that's all Google Research was able to give them.  But it was actively exploited in the wild, so it would have had priority with them.  It's a local privilege escalation in the Windows win32k.sys kernel driver that can be used as a security sandbox escape.  So of course that's why it's of use.  If you have a browser vulnerability, you may still, despite the fact that you've got a vulnerability, you may be constrained by the browser's own sandbox that prevents you from doing anything.



So you also need a sandbox escape, thus the second vulnerability in 32-bit versions of Windows 7.  The vulnerability is known as a "null pointer dereference."  In Win32k there's a function, MN get pointer to item from index, under certain special circumstances that the bad guys were able to leverage in order to pull off an exploit.  So Google said:  "We strongly believe this vulnerability may only be exploitable on Windows 7 due to recent exploit mitigations added in newer versions of Windows."  And they said:  "To date we've only observed active exploitation against Windows 7 32-bit systems."



So then they finally said:  "Pursuant to Google's vulnerability disclosure policy, when we discovered the vulnerability we reported it to Microsoft.  Today, in compliance with our policy, we are publicly disclosing its existence because it is a serious vulnerability in Windows that we know was being actively exploited in targeted attacks."  Ah, so targeted attacks, not spray, or not widespread.



And they said:  "The unpatched Windows vulnerability can still be used to elevate privileges or combined with another browser vulnerability, that is, a theoretical browser vulnerability, to evade security sandboxes.  Microsoft have told us they are working on a fix."  So we don't know, again, if we made it into this Patch Tuesday.  I should know next week.  And hopefully, you know, Microsoft tends to be better, well, like way better than Apple, for example, in telling us what things they have fixed after they have been.  So we may be able to know one way or the other.



I mentioned at the top of the show that Citrix was not having a good year.  Last Wednesday Citrix learned - and this is a little obscure because they actually apparently learned earlier than this, but I'll explain.  The news coverage said that Citrix learned from the FBI that their own corporate Citrix network had been infiltrated.  When I read the blog posting by Stan Black, who's their Chief Security and Information Officer, now known as a CSIO, it didn't seem like that much of a deal.  And again, I can understand him wanting to sort of downplay this.



His posting said:  "On March 6, 2019, the FBI contacted Citrix to advise they had reason to believe that international cybercriminals gained access to the internal Citrix network.  Citrix," he writes, "has taken action to contain this incident.  We commenced a forensic investigation, engaged a leading cybersecurity firm to assist, took actions to secure our internal network, and continue to cooperate with the FBI.



"Citrix is moving as quickly as possible, with the understanding that these investigations are complex, dynamic, and require time to conduct properly.  In investigations of cyber incidents, the details matter, and we are committed to communicating appropriately when we have what we believe is credible and actionable information.  While our investigation is ongoing, based on what we know to date, it appears that the hackers may have accessed and downloaded business documents."  Okay, now, that was the line that caught me up short when I learned what actually apparently happened.  "It appears that the hackers may have accessed and downloaded business documents."  Okay.  



"The specific documents that may have been accessed, however, are currently unknown.  At this time, there is no indication that the security of any Citrix product or service was compromised.  While not confirming," he says, "the FBI has advised that the hackers likely used a tactic known as 'password spraying,' a technique that exploits weak passwords."  And of course we've been talking about this recently.  "Credential stuffing" is actually what it's called.  "Once they gained a foothold with limited access, they worked to circumvent additional layers of security."  Right.  "Citrix deeply regrets the impact this incident may have on affected customers."  And then I thought, what?  What?  Wait.



LEO:  Hmm?  But it didn't affect Citrix's products or services.



STEVE:  That's right.  "Citrix is committed to updating customers with more information as the investigation proceeds, and to continuing to work with the relevant law enforcement authorities."  So I was a bit curious about that second-to-last sentence, "Citrix deeply regrets the impact this incident may have on affected customers."  Okay.  Because of course as I said it appears that the hackers may have accessed and downloaded business documents.  Anyway, I assumed that meant Citrix business documents; right?  So that was Wednesday.



LEO:  Ohhhhh.



STEVE:  Two days later, last Friday, NBC News posted the story with a somewhat different take than Citrix's own CSIO.  NBC's headline and lead-in read:  "Iranian-backed hackers stole data from major U.S. government contractor.  The hackers are believed to have penetrated the software giant Citrix years ago and have remained inside the company's computer network ever since."  Whoo.  Ouch.



NBC said:  "Iranian-backed hackers have stolen vast" - vast tracts of land, no - "...vast amounts of data from a major software company that handles sensitive computer projects for the White House communications agency, the U.S. military, the FBI, and many American corporations, a cybersecurity firm told NBC News.  Citrix Systems, Inc., came under attack twice, once in December and again Monday, according to Resecurity, which notified the firm and law enforcement authorities.



"Employing brute-force attacks that guess passwords, the assault was carried out by the Iranian-linked hacking group known as Iridium, which was also behind recent cyberattacks against numerous government agencies, oil and gas companies and other targets, Charles Yoo, Resecurity's president, said."  And here it comes.  "The hackers extracted" - are you sitting down, Leo? - "at least 6TB of data and possibly up to 10TB..."



LEO:  That's a big document.



STEVE:  "That's 10 [trillion, trillion, 10 trillion] terabytes in the assault on Citrix, Yoo said."  That's Y-O-O.  "The attackers gained access to Citrix through several compromised employee accounts," he said, "so it's a pretty deep intrusion, with multiple employee compromises and remote access to internal resources."



So between 6 to 10TB of something was exfiltrated from Citrix.  This sort of sounds, though, as thought it might be their customers' business documents.  And, yeah, Citrix would likely regret the impact that this incident would have on their customers.  The security company, of course, in question was Resecurity.  And in their own reporting of this - I've got a link to their blog posting in the show notes - they indicated that Citrix - and this is what's a little troubling - was first notified of the presence of an Iridium APT [Advanced Persistent Threat] last December.



Resecurity wrote:  "Friday, December 28, 2018 at 10:25 AM.  Resecurity has reached out to Citrix and shared early warning notification about targeted attack and data breach.  Based on the timing and further dynamics, the attack was planned and organized specifically during Christmas period."



So what's odd is that Citrix's CSIO, who presumably would have been the recipient of Resecurity's note back on December 28th, stated in his blog that on March 26th, 2019 the FBI contacted Citrix to advise they has reason to believe that international cybercriminals gained access to the internal Citrix network.  Yet Resecurity makes it very clear that they notified Citrix of this three months earlier.  So one wonders whether Citrix just ignored the first report.  But in any event, a multiyear targeted 6 to 10TB data exfiltration makes this a major breach of a major cybersecurity services company.  Yikes.



Okay.  And why you never want to loudly proclaim that "Our security cannot be hacked."  Pen Test Partners' blog posting is titled:  "Gone in six seconds?  Exploiting car alarms."  The two most popular aftermarket car alarm systems in the world open their owners to hacking.  Who would have guessed?



Pen Test Partners' blog begins:  "Key relay attacks against keyless entry vehicles are well known."  And of course we've had, you and I, Leo, have covered key relay attacks extensively in previous years on this podcast.  Their blog says:  "Many third-party car alarm vendors market themselves as solutions to this."  In other words, oh, key relay attacks are bad.  You need to install this alarm system to be secure.



"We," they write, "have shown that fitting these alarms can make your vehicle even less secure."  No surprise.  "These alarms can expose you to hijack, maybe allow your engine to be stopped while driving, and it may even be possible to steal vehicles as a result."



Okay.  So the car alarm systems are those by Pandora and Viper, and Viper is known as Clifford in the U.K.  And I bought, shortly after getting the very first version of SpinRite up and going, I bought a Jeep Cherokee, and I equipped it with a Clifford alarm system.  That was at the time the alarm that you wanted, digital encoded and all that.  So I had a Clifford alarm.



LEO:  Notice you don't hear car alarms go off much anymore?



STEVE:  No, thank goodness.



LEO:  Used to hear them all the time.  Remember?



STEVE:  Oh, they were so annoying, especially those ones that went through, like, 25 different...



LEO:  [Mimicking car alarm]  So annoying.



STEVE:  I know.  Awful.  Anyway, they have been found to be vulnerable, that is, these car alarm systems by Pandora and Viper have been found to be vulnerable to remote exploitation, enabling attackers to hijack the vehicles they're installed on and spy on their owners.  The exploitable software flaws are found in the smartphone apps used to control - because of course we're going to give you an app for that - to control the alarm systems developed by Pandora and Viper.  So get a load of this.  The smartphone app has been downloaded more than three million times.



Pen Test Partners, who poked at the smartphone app, said that:  "The vulnerabilities are relatively straightforward, insecure, direct object references in the API."  They said that:  "Simply by tampering with parameters, one can update the email address registered to the account without authentication, send a password reset to the now-modified attacker's email address, and take over the account."  So in other words, it's apparently trivial to change the email address, then ask for a password reset, which then goes to the changed email address, which then allows you to take the account away from its owner.  And then you have access.



Having access, playing with these parameters, you are now able to locate the car in real-time, disable its alarm, unlock it, enable or disable the vehicle immobilizer.  In some cases you're able to kill the engine while driving.  One of the two alarm brands allows drivers to be snooped on through a microphone in the car.  And depending upon the alarm, it may be possible to steal the vehicles outright.  And the flaws they observed in the car alarm APIs exposed huge amounts of personally identifiable information.



They also noted that it's not necessary even to purchase an alarm.  That is, you don't have to own an alarm system.  You're able to create a trial test and demo account online and then take over.  From that demo account, it's possible to access any genuine account - it just sounds like it's horrible, like one of those put the email address as a parameter in the URL kind of things, I mean, just unconscionable security - and retrieve the user's details.



So in their blog - I'm going to quote some things just because it's amazing.  Under "Killing car engines to order," they wrote, "This part is crazy.  We discovered we could kill the engine on the Viper-equipped car" - that's the Clifford, known as Clifford in the U.K. - "whilst it was in motion.  Promotional videos from Pandora indicate this is possible, too, though it doesn't appear to be working on our car."  So they had a Pandora alarm-equipped car.  "The intention is to halt a stolen vehicle.  Except, using the account takeover vulnerability in the mobile app, one could kill the engine of any car fitted with these alarms.



"Audio snooping on drivers."  They said:  "Yes, really.  The Pandora alarm has the ability to make SOS calls.  A microphone is fitted in order to enable this.  The microphone can be accessed and enabled remotely owing to the authorization flaw in the API.  Therefore, all cars and drivers with the alarm fitted can be silently listened to.  Millions of drivers snooped on."



And then under, they said:  "CAN control."  And of course that caught my attention because of course we know that the CAN bus we've talked about extensively on the podcast.  They said:  "CAN control.  OMG!  Both the Viper and Pandora have the ability to send custom CAN messages."  They said:  "This is where things get a bit scary.  In recent years car alarms have the ability to interface directly with the CAN.  This is necessary given the level of complexity," they write, "of modern vehicles.  This also helps to reduce alarm wiring and installation time."  Yeah, just plug it into the bus.



"Higher-end alarms can automatically detect the vehicle type they are being fitted to and customize their command set to the CAN messaging being used.  This speeds up installation significantly."  How convenient.  "However," they write, "when the alarm doesn't recognize the vehicle, or it isn't automatically supported, the installer will need to program the alarm manually.  As far as we can determine," they write, "alarm programming has to be done locally using a laptop app or, more interestingly, a mobile phone using Bluetooth.  While there appear to be methods to program over the air from the API, they aren't documented fully, and we haven't been able to fully reverse engineer them yet.  We are still working on this area, but each requires a different vehicle with an alarm fitted to prove it.



"But," they write, "Start/stop functionality is already enabled.  After analyzing the firmware, manuals, and related changelogs, we found some scary functionality.  It is vehicle specific, so we've been unable to test it all.  If you own one of these  vehicles with the relevant Pandora alarm, we would love to know if it works.  Obviously, take great care; and we don't advise doing this on public roads.  Mazda 6, Range Rover Sport, Kia Quoris, Toyota Fortuner" - I guess they meant Fortuner - "Mitsubishi Pajero, Toyota Prius 50, and the RAV4, these all appear to have undocumented functionality present in the alarm API to remotely adjust cruise control speed.  Some workarounds for stop/start functionality also require a false brake pedal message to be sent, simulating the driver pressing the pedal and starting the vehicle."



So anyway, we covered the CAN bus and the power that it has in detail.  As we know, and we talked about it at the time, there's an explicit and deliberate firewall which separates the infotainment side.  There's essentially multiple CAN buses now with a careful firewall that allows controlled bridging of the two buses that separate the infotainment side from the critical vehicle operations side.  It's clear that these extremely insecure alarm systems are being attached to the critical vehicle operations side, meaning on the dangerous side of the firewall, in order to give them the control they require.



So essentially, anyone adding this aftermarket alarm system is bypassing the CAN bus firewall and attaching a highly insecure and obviously very poorly designed system to the inner guts of their car.  Fearing that the bad guys might already know of these vulnerabilities and be exploiting them in the wild, this Pen Test Partners outfit gave both companies a very short seven days to fix this most obvious security problem.  Both Pandora and Viper, to their credit, responded and patched them immediately.



Pen Test Partners wrote:  "Pandora's U.K. representative responded in about 48 hours and had their Moscow-based headquarters take action quickly.  The insecure direct object references were fixed overnight, and we confirmed that the following morning.  Viper responded even faster, but took a little longer to fix the vulnerability.  That one is also confirmed fixed."



They also wrote:  "It's important to note that we did not carry out a full test of the APIs.  Doing that would have required further authorization which we don't have.  We have no idea if there are other vulnerabilities in the API."  In other words, they found the big horrific flaw in the alarm systems and notified the vendors.  What they did not do was perform a full in-depth security audit.  And, boy, that really is wanting.  That should be done.



It's scary to imagine that, I mean, if this first obvious problem was found so easily, and these alarms are being directly plugged in over on the engine side of the CAN bus and can generate custom CAN messages, this is really ripe for exploit.  And I saw the numbers, and I didn't put it in the show notes.  Tens of millions of these alarm systems exist and have been installed.  So a big target of opportunity.



Okay.  So everybody who's listening to the podcast, I want you to raise your hand if you use the Alt+Tab Windows key combination.



LEO:  I use it on Windows, Mac, and Linux.



STEVE:  Yay.  You're a power user, Leo.



LEO:  What?



STEVE:  Well, definitely some of us use it.  But I'm sure it's a minority of Windows users.  I use it often to quickly ping-pong between - typically between a pair of apps.  Sometimes I'll use it to find an app that's deeper down in the MRU, the Most Recently Used stack.  But I mostly use it to toggle between a pair which I'm interacting with.  So Alt+Tab is my go-to or my "go back to the previous app" jump.



LEO:  Right, right.



STEVE:  And so, for example, when I'm working on this week's podcast, I use an outliner called ThoughtManager Desktop, which is no longer on the market.



LEO:  That's a vintage program.



STEVE:  Yes.  It was on the Palm Pilot.  It was an outliner for the Palm Pilot, and it had a Windows add-on component which was the desktop version.  And you could export the things you did on the Palm Pilot to the outliner.  But believe it or not, of all the outliners available for Windows, it is the simplest and cleanest, and it's where I do all of my - like all of my brainstorming and project organization is in ThoughtManager Desktop.



And so anyway, so I'll have it, and then I'll have Firefox.  And as I'm pulling notes together and grabbing URLs and text from various web sources, I'm copying, and then Alt+Tab bounces me to ThoughtManager.  Then Ctrl-V pastes it, and then Alt+Tab brings me back to the browser.  And so I'm just - I'm bouncing back and forth all the time.



Anyway, my point here is that BleepingComputer had an intriguing piece about a new practice that Microsoft is exploring.  Believe it or not, they've having and inviting five- to 10-minute conversations by appointment with Windows 10 users about their usage of specific Windows features.  Lawrence Abrams, who covered this, wrote:  "Microsoft has started to display notifications in the Windows 10 Action Center asking users to have a phone call with Microsoft developers and provide direct feedback about" - in this instance, there was a different one in February, he says, but in this instance - "about the ALT+TAB feature in Windows."



He says:  "While using a Windows 10 Insider Build today, I was shown a Feedback Hub notification stating that:  'Microsoft wants to hear your opinions.  To set up a phone call with Windows engineers, go to...'" - and then there's a link.  Microsoft apparently has a domain www.aka.ms, which I thought was kind of cute, you know, also known as dot ms, and then /alttab.  He says:  "This link then redirects to a web page at ux.microsoft.com/?AltTab."  He says:  "It's not known if this is only being shown to Windows Insiders users at this time.



"When users visit this link they'll be shown a Microsoft User Research page stating that a Windows 10 product team is looking to 'understand our customer needs' and would like to have an anonymous five- to 10-minute phone call with the user.  In this particular case, the phone call will be with Microsoft engineers to discuss how users use the ALT+TAB feature to switch between apps.  Microsoft states they're performing these calls in order to get a better understanding of how a feature is being used while they're in development."



And so they said:  "Your feedback is important to us.  As we develop new software and services, it's critical that we get feedback from customers who use our products.  Hearing from you early in the development cycle helps us make changes to our products and test them before release.  Early customer interactions ensure we hit the mark with features.  The time you spend with us today can improve our products for users around the world."



"According to the website," Lawrence says, "Windows engineers will be available on [yesterday] 3/11/2019 between 11:15 a.m. and 1:00 p.m. Pacific time, and [today] 3/12/2019 between 9:30 a.m. and 11:30 a.m. Pacific time, to schedule a call.  The page goes on to say that users can expect a five-to 10-minute call, but it could last longer if there is more to discuss.  They also state that the calls will not be recorded, are anonymous, and the content of the call will not be stored."



And then Lawrence wrote:  "When researching this notification, I ran into a Reddit thread where a user received a similar notification over the weekend.  Jen Gentleman, a Community Manager and Software Engineer at Microsoft, stated that these notifications are a new program being piloted that gives engineers the ability to talk to users in real time about features they're working on.  So," he says, "this is not the first time that Microsoft has asked Windows 10 users to call engineers.  In February, Microsoft also asked users to contact them regarding the Windows Update feature."



So anyway, I just thought that was fascinating that, okay, we're going to expose our cloistered and closeted Windows 10 engineers to actual end users.  And I don't know if you get this notice if you don't use Alt+Tab?  I mean, or...



LEO:  What about Alt+F4?  I think you should definitely talk to those people.



STEVE:  Oh, the close button, yes.



LEO:  Yeah, the most obscure shortcut that people would use all the time if they only knew it.



STEVE:  Yup.



LEO:  Wow.  Geez.



STEVE:  But isn't that interesting?  It's like, okay.  Now, of course, I would say, just leave it alone.  What, you know, why...



LEO:  Stop messing with it, yeah.



STEVE:  Stop messing with it.  But no.



LEO:  No.



STEVE:  Yeah.



LEO:  Wow.



STEVE:  So in other Win10 news, aside from Alt+Tab and who uses it, self-uninstalling updates is a new...



LEO:  Oh, lord.



STEVE:  I know.



LEO:  You shouldn't really need that, but okay.



STEVE:  I know.  When you can't get it right, you can at least determine that Windows didn't start up and then back it out.  Windows 10 will soon acquire the ability to autonomously uninstall updates that cause problems for it.  Can you say "We are losing control of the boat?"  According to a support document published yesterday, Windows 10 will begin automatically uninstalling automatically installed Windows updates that cause startup failures due to some incompatibility with something.  You know, it's because it's just becoming too fragmented, and they're constantly changing it.



Updates will be automatically removed when Windows detects that it has recovered from a startup failure after all the other "automatic recovery attempts have been unsuccessful," whatever that means.  And furthermore, to prevent an immediate repeat, because of course Windows will go, oh, I have an update missing, Windows will blacklist those from installing for the following 30 days.



So Microsoft wrote:  "To ensure that your device can start up" - which is handy, you know.



LEO:  Yeah.  We would want it to start up, yeah.  Good thinking, yeah.



STEVE:  "To ensure that your device can start up and continue running" - oh, yes, after starting up you would like it to continue running as expected.



LEO:  Indeed.  Indeed.



STEVE:  This is a new benefit, Leo, of Windows.  It will continue running.  "Windows will also prevent problematic updates" - those pesky problematic updates which we've had so much of lately - "from installing automatically for the next 30 days.  This will give Microsoft and our partners the opportunity to investigate the failure and fix the issues.  After 30 days, Windows will try again to install the updates."  Wow.



LEO:  This is actually really good because this does happen.



STEVE:  Yes.  Yeah, no, it does happen, and it is good.  But it's sad that it's necessary.



LEO:  It's weird, yeah.



STEVE:  Which both you and I understand.  "Users who still want to install them" - and who would that be? - "because they believe the removed Windows updates weren't the cause behind startup failures" - so that's some kind of power user - "can still do so by manually downloading them from the Windows Update Catalog or, in the case of device drivers, using the system's Device Manager to go get them."



So, yes, this feels like a useful and practical and unfortunately necessary CYA move for helping to deal with this past run of troubles Windows 10 has been experiencing after updates messed up the systems.  And of course it's why I have to have my updates held off for a month because on this Windows 10 system I'm talking to you over, Leo, I don't want it to collapse on me on a podcast day.  So anyway, there's that.



We finally have really good information about the Marriott Hotel's Starwood breach.  The CEO, in some testimony in front of the Senate Committee on Homeland Security and Governmental Affairs, the Permanent Subcommittee on Investigations, Arne Sorenson, who's the CEO, apologized in his official statement and testimony to the company's customers, but also shot down some rumors that apparently were circulating that China was behind the attack.



So in his prepared statement, Sorenson said that Marriott learned something might be wrong for the first time on September 8th of 2018.  So that was, what, late summer, early fall of last year, when they were contacted.  They first learned when they were contacted by Accenture, the IT company who's responsible for managing the Starwood guest reservation database.  And of course as we know and we talked about at the time, Marriott had acquired the Starwood Hotels properties two years previously in September of 2016.  Work was underway to migrate Starwood's customers over to Marriott's own guest reservation system, but at the time the Starwood system was still separate from the rest of the Marriott network.



So on that September 8th, Accenture told Marriott's IT staff that a database monitoring system known as IBM Guardium had detected an anomaly on the Starwood guest reservation database the day before, on September 7th.  Sorenson said - I guess it's Guardium.  I have it twice here, so that must be correct:  "The Guardium alert was triggered" - and I really was impressed by this - "by a query from an administrator's account to return the count of rows from a table in the database."  It's like, well, that's very cool that it thought that was suspicious.



"Such queries," he said in his testimony, "are considered dangerous because the software that runs on top of a database doesn't usually need to make such a query."  Again, I'm impressed.  "This meant that a human operator was making this type of very specific query by hand."  Sorenson continued:  "As part of our investigation into the alert, we learned that the individual whose credentials were used had not made the query."  At that point the Marriott staff realized they were dealing with a probable breach.



LEO:  This is good.  This is what's supposed to happen.



STEVE:  Exactly, although they didn't know if it was something big or just the beginning of a hack that could be very easily contained before the attackers accessed any user data.  So it brought in a third-party forensic investigator, or third-party forensic investigators, on September 10th, so pretty much quickly, to help its IT staff look into a possible breach.  Within a week, the forensic firm's investigation uncovered malware on the Starwood IT systems.



Sorenson testified that:  "The investigators uncovered a Remote Access Trojan (RAT)," he writes in his testimony, "a form of malware that allows an attacker to covertly access, surveil, and even gain control over a computer."  He says:  "I was notified of the ongoing investigation that day, and our Board was notified the following day."



Significant forensic work was required to reveal the full scope of the attack.  And despite the RAT's presence on the Starwood IT system, at that point there was still no evidence that unauthorized parties had actually accessed customer data from Starwood's guest reservation database.  But the investigation continued.  One month later, in October, the forensic firm discovered Mimikatz, a well-known penetration testing tool which searches a device's memory for usernames and passwords in RAM.



LEO:  Wow.



STEVE:  Uh-huh.  The tool was most likely used to help attackers acquire passwords for other Starwood systems to help them move laterally to other parts of the IT network.  Still, however, investigators found no direct evidence that attackers had accessed customer data.  They knew they'd had a bad infiltration, but they didn't have actual proof.  A month after that, the hack turned from probably bad to definitely bad when, in November, investigators discovered that the hackers had been active on Starwood's IT network since July of 2014.  This meant that a long-term persistent attack had been underway for more than two years without ever being detected.  And this meant that the forensics firm had to dig through years of logs.  Still, there was no actual evidence of attackers accessing customer data.



In mid-November, that evidence was finally found.  Sorenson's statement reads:  "On November 13th, our investigators discovered evidence that two compressed, encrypted files had been deleted from a device that they were examining.  The files were encrypted, and the actual content was unknown.  There was also evidence to suggest that those two files had potentially been removed from the Starwood network.  Six days later, on November 19th, 2018, investigators were able to decrypt the files and found that one contained an export of a table from the Starwood Guest Reservation Database containing guest data, while the other contained an export of a table holding passport information."



The hotel chain then notified authorities and went public with its data breach disclosure on November 30th, revealing a breach that impacted around 500 million customers.  According to Sorenson's testimony and an update on the Starwood breach notification website, the latest stats surrounding the Marriott breach, which have been updated several times as more was learned, are 383 million guest records were involved; 18.5 million encrypted passport numbers; 5.25 million unencrypted passport numbers, 663,000 from the U.S.; 9.1 million encrypted payment card numbers; 385,000 card numbers that were still valid at the time of the breach.



Sorenson stated that investigative efforts have yet to uncover evidence to suggest that attackers gained access to the encryption key used to encrypt the 9.1 million payment card numbers.  But that also presumes that they were probably able to get the keys required to decrypt the other content.  So it's likely that some Starwood employee somewhere fell victim to a phishing attack which allowed someone or something malicious to get into, well, something and someone malicious to get into their machine.  That might have been a Remote Access Trojan that phoned home to report its success and then gave some remote attacker visibility into that machine.



From there the attacker likely perused the connected network and may have moved laterally to other machines.  By downloading and installing and running Mimikatz in various machines, vestigial credentials could then have been recovered from RAM and used to further penetrate the organization's network, servers, and services.  So now we know basically the full forensics breakdown of what happened with the Starwood properties.



LEO:  That's actually fascinating.  You realize how hard it is to catch this stuff.



STEVE:  Yes.



LEO:  And now we know, we're pretty sure at least, the intelligence community seems to be pretty sure that these were Chinese state hackers on the Marriott one, that they were trying to kind of follow spies and Chinese nationals overseas in their reservations and so forth.  Because it never went in the dark web and all of that.



STEVE:  Right.



LEO:  So these were, one presumes, very, very skilled hackers.



STEVE:  Well, and delighted to, well, yes.  And so the property was probably - it was a targeted attack, probably phishing email, which could have easily been designed to seem very authentic.  All it takes is one employee somewhere...



LEO:  That's all it takes, yeah.



STEVE:  ...to let it in, and then it gets a foothold and then begins, you know, then looks at the things that that user's machine's connected to, moves itself over into that machine, brings Mimikatz down to scan RAM, finds the login credentials of people who have previously logged into that machine.  That allows the bad guys then to log into that machine.  And as they say, that's all she wrote.



LEO:  Yeah.  Wow.



STEVE:  But, yeah, really, really cool to have a forensics readout of this.



LEO:  Yeah.  Fascinating.



STEVE:  And notice that we wouldn't have this if it weren't for Congress saying, you know...



LEO:  Tell us, yeah.



STEVE:  Getting a public account from a CEO, how did this happen?  Because no one wants to talk about this in this kind of details.



LEO:  And yet, yeah, and I understand why, I mean, obscurity's security and all that.



STEVE:  Yeah, it's not in their PR interest.



LEO:  Hackers know what they're doing, you know.  And I'm sure they only left Mimikatz behind because, I don't know, why did they?



STEVE:  Didn't matter.  Didn't matter at that point.  They got what they needed.  And, I mean, they were still there, Leo.  They were there when this was discovered.  So remember that it was somebody...



LEO:  Well, they're going to stay there till they get kicked out; right?



STEVE:  Yeah.  So again, it was somebody who, in real time,  asked for how many rows in a table that tripped the IBM monitoring software that said, whoa, wait a minute, people don't ask for that.  Or, I mean, sorry, automated software doesn't ask for that.  Who wants to know?  And that's what started this, I mean, that tripped the alarm that told them that somebody was there.



LEO:  Wasn't that fascinating?  Yeah.



STEVE:  So they were active at the time.



LEO:  That's good security software, I think.



STEVE:  I'm impressed by that, yes.  That's impressive.  And I just got a kick out of this, so I wanted to add this to the show.  Elaine sent back, in our communication that we have every week:  "According to Merriam-Webster."



LEO:  Oh, boy.  I don't like it when emails start like that.



STEVE:  Thank you, Elaine.  "While it's often maintained that the word 'doozy' derives from the Duesenberg in the name of the famed Duesenberg Motor Company, this is impossible on chronological grounds.  'Doozy' was first recorded, in the form D-O-Z-Y, in eastern Ohio in 1916, four years before the Duesenberg Motor Company began to manufacture passenger cars.  The related adjective 'doozy,' D-O-O-Z-Y, meaning stylish or splendid, is attested considerably earlier, in 1903.  So where did 'doozy' come from?"  Entomologists?  Etymologists.



LEO:  Etymologists, yes.



STEVE:  "Etymologists believe that it's an altered form of the word 'daisy,' which was used especially in the late 1800s as a slang term for someone or something considered the best."  Ooh, you're a daisy.  It's a daisy.



LEO:  Interesting.  It's a daisy.



STEVE:  So it turned into "It's a doozy."  Okay.  I got a note from Jamie in Sydney, Australia, who said:  "SpinRiting my iPod," he said, "working on one spot for hours and hours and hours."  He said:  "Steve, a bit of a story.  There is a question in there somewhere, I promise."  He says:  "I've had a copy of SpinRite for ages.  I purchased it many years ago, soon after having discovered the Security Now! podcast."  He says:  "At the time, I went back and started from number 1, so I've heard them all."



He wrote:  "I've used SpinRite quite a few times around the house.  I've even had to re-download it once or twice over the years as it is a very small exe that is very easy to lose."  So, yes, you can misplace your SpinRite, and our system allows people who retain their purchase information to update their copy, to re-download it anytime they want.



So anyway, he said:  "Recently I inherited an iPod."  He says:  "That's me justifying a short iPod Classic phase I went through where I picked up a few on eBay."  He says:  "I didn't realize at the time, but the hard drive had an issue.  I mean, how are you to know, really?  Oh, well, I figured I'd fix it with SpinRite.  Didn't seem too much of an issue."  Then he has a link to an article, "Running SpinRite 6 on macOS."  He said:  "Following this guide, I managed to get SpinRite running in VirtualBox, connecting to the iPod in Disk Mode.  Very cool that we can do that at all."



He said:  "I kicked off a Level 2 scan.  As it progressed, it came up with an estimate of 33 minutes to complete.  Things were chugging along, so I left it running and went on with some other stuff and sort of forgot about it.  A couple hours later it's still running.  SpinRite is fine, and I can navigate around the different UI screens, so the VM is still going, but we're still sitting on 14%.  The time estimate has not moved."



And he says:  "If I listen" - and here it is - "to the iPod, I can hear the drive in what seems like a loop.  It's doing the same thing over and over again, chuck chunk chunk chunk squiggle berrrrrp squeak, over and over and over and over.  I guess this is the drive trying and trying to read one troublesome spot."



"And so onto the question:  When connecting to an iPod like this, or any USB drive, do all the special data recovery techniques you've mentioned over the years, coming at the spot from all angles, et cetera, still work?  On the website you say 'It contains and deploys an extensive arsenal of data recovery techniques and technologies designed to pull a drive's data back from oblivion.'  Do you still have the same low level control of the drive as you would with a direct IDE connection?  Are you able to do everything to a drive connected in this way as you can to something directly connected to a physical PC via IDE/SATA?



"By the time you get this, and if it were to air on Security Now!" - oh, it is - "I may well have an answer of my own.  I plan to just leave it running, at least overnight, maybe the weekend.  Thanks for any answer, via Security Now! or not.  Happy to share on Security Now! if you're running low on SpinRite stories.  Long-time listener and enjoying every episode.  Thanks.  Jamie.  P.S.:  Could we bring back a few of the 'deep dive' style episodes where you go deep and tell us all about something?"



So anyway, to answer your question, Jamie, it is still possible to get better recovery if we have a direct connection.  But many people, in fact many iPods have been recovered by SpinRite, and we've talked about that in the past.  So anything is better than nothing.  And you've got nothing.  What you've got is clearly a drive that is very troubled.



And I remind people, and this is something of a value judgment, you know, some people, when SpinRite brings a drive back from the dead, they're like, oh, everything's great again.  And it's like, uh, yeah, good.  But you had a problem, and so you may have a problem again in the future.  So it's not, I mean, it's great that you were able to get everything back.  In some cases it makes sense to keep using the drive.  But what I have long said is, in a competition between SpinRite and the drive, where SpinRite wants it to stay alive, and the drive is trying to die, ultimately the drive will win that competition.



LEO:  You can't save a drive from itself.  Everybody knows that.



STEVE:  If it is absolutely determined to stop being a drive and start being a doorstop, it will become a doorstop.  So yes, SpinRite can help.  It has helped lots of iPods in the past.  Doing what you did, Jamie, is better than nothing.  On my list, on my development plan, I'm first going to do the direct connect BIOS bypass AHCI connection for direct attachment of all of our IDE, AHCI, and SATA drives to motherboards.



But then my plan is - and I'm going to get that out immediately.  And my plan is probably with .1,  well, definitely with .1, probably do native Mac at the time because that would help a lot of people, too, who have drives in Macs.  But then I absolutely want to do the serial interface.  And so I plan to then do the same direct to the controller connection for probably I'll call it SpinRite 6.2 and release that again, do a series of consecutive releases.  That's the plan.  So that's where we stand.  But yes, definitely.  And as for the deep dive, I don't know how we were ever able to afford those, Leo, in terms of the podcast.



LEO:  We can only do it when there's no news.



STEVE:  Yeah.  And boy, that's just - give us a break, would you, world?  And then I'd be happy to do a deep dive about something.  But, boy, just no time.



Okay.  So finally we're going to get to the topic of the show here at two hours in.  It's called SPOILER.  They have it in all caps.  And it stands, well, I want to say it stands for, but it really doesn't, because it's "Speculative Load Hazards Boost Rowhammer and Cache Attacks."  So Spoiler is not an abbreviation or an acronym or something.  And I read somewhere that they just didn't bother trying.  So the story is researchers from Worcester Polytechnic Institute in...



LEO:  Worcester.  Worcester.



STEVE:  Worcester?



LEO:  Yeah, well, Worcester.  But yeah, but they call it "Woosta."  Worcester Polytechnic, yeah.



STEVE:  Oh, in Massachusetts.  I get it.



LEO:  It's like Worcestershire.  It's "Woosta."



STEVE:  Okay.  Worcester Polytechnic Institute in Worcester, Massachusetts.



LEO:  That's it.  Now you got it.



STEVE:  All right, and the University of Luebeck in Luebeck, Germany, named their work SPOILER.  And I saw a comment somewhere that they were too tired afterward to even bother trying to reverse engineer that in order to make it into some clever abbreviation.  So SPOILER is Speculative Load Hazards Boost Rowhammer and Cache Attacks.  And so it's got buzzwords in it, but it doesn't stand for anything.



Okay.  Now, their abstract, the abstract of their paper, is going to make everyone's eyes cross, I guarantee it.  And it's going to stop the propellers of anyone whose propeller is spinning on their beanies.  But that's also kind of the point I want to make with this.  I want to show how the subtle and nuanced attacks such as Rowhammer and speculative execution evolve over time, and how far out into the weeds the academic security research community can and does go.



So for the simplified version, the abstract of their paper says:  "Modern microarchitectures incorporate" - and I'm going to stop a few times to clarify.  But "Modern microarchitectures incorporate optimization techniques such as speculative loads and store forwarding to improve the memory bottleneck."  Okay, in other words, "speculative loads" means they're like reading ahead.  They're like, as we know, fetching from DRAM, which is slow, in order to try to get content from memory that they believe they may be needing.  So that's speculative.



"The processor executes the load speculatively before the stores, and forwards the data of a preceding store to the load, if there is a potential dependency."  Okay, now, that means they've gone ahead and fetched something, but ahead of the logic storing something back into memory.  So if where they're storing it back happens to be what they prefetched, then it's whoops, that would have changed the contents that they fetched if they had been waiting to fetch it, so that's a dependency.  So that's known as store forwarding.



They said:  "This enhances performance since the load does not have to wait for preceding stores to complete.  However, the dependency prediction relies on partial address information, which may lead to false dependencies and stall hazards."  They said:  "In this work we are the first" - meaning the first researchers - "to show that the dependency resolution logic" - I mean, we're so far out in the weeds - "the dependency resolution logic that serves the speculative load can be exploited to gain information about the physical page mappings."  Okay?  Now that's of course the logical, the physical mapping.  And that bears relevance because Rowhammer needs to know that the software is able to pound - it's trying to pound on physical memory, yet it's got to work through the page mappings, which has always been a problem for Rowhammer.



"Microarchitectural side-channel attacks such as Rowhammer and cache attacks rely on the reverse engineering of the virtual-to-physical address mapping.  We propose the SPOILER attack, which exploits this leakage to speed up this reverse engineering by a factor of 256.  Then we show how this can improve" one of the particular Rowhammer attacks known as "the Prime+Probe attack by a factor of 4096" which is really dramatic, I mean, it's not like it doubles it, it's 4,000 times faster "speed up of the eviction set search, even from sandboxed environments like JavaScript.



"Finally, we improve the Rowhammer attack by showing how SPOILER helps to conduct DRAM row conflicts deterministically with up to 100% chance, and by demonstrating a double-sided Rowhammer attack" - remember, that's where you hammer on both sides of your target, both rows on either side of your target row - "with normal user's privilege.  The later is due to the possibility of detecting contiguous memory pages using SPOILER leakage."  Whew.



Now, I actually had more here in the show notes, but I'm not going to dig into it because, I mean, it's - we're all exhausted at this point.  Basically what has happened is this is yet another very clever attack on a different aspect of the Intel architecture speculation.  We've talked endlessly through 2018 about branch prediction speculation and jump prediction speculation where there was explicit hardware present to prefetch and go down both paths of a branch and how it's possible to train the Intel speculation to expect to go down one path and then leak information when you do a context switch because you're using the same hardware as you were using as another process or VM, for example, was using.



Well, it turns out these guys recognized there was something else Intel had done to improve performance.  And that's in this prefetching system where it turns out that Intel has a system which is responsible for this fetch ahead which is different than any of the speculation we've been talking about.  And it turns out it can be tricked in a way which is new and unique.  None of the existing speculation changes that we've made to microcode have any effect.  They state that they don't expect to see for at least five years any change because it appeared in the very first core, the Intel core, the first-generation Intel core microarchitecture.  It is fundamental to a much greater degree than these other speculative tweaks that Intel has made.  It is like really deep in.



Microcode tweaks won't fix it.  The only thing they could do would be to turn it off completely, and it would just crash performance if they did that.  And what they did was they found this other aspect of crucial performance speculation which has been present in every core microarchitecture, and they figured out how to leverage it to create a far more potent Rowhammer attack by a factor of 4,000 than we've ever seen before.  It is potent enough to work in browsers through JavaScript, which is a concern for even the Rowhammer mitigations where timers have been deliberately made fuzzy in order to thwart the attack.  That fuzziness, the level of fuzziness needed was a function of how difficult the existing research had shown Rowhammer would be to work through an already uncertain environment like JavaScript, or to a lesser degree WebAssembly.  These guys have got it working in JavaScript and in WebAssembly in a web browser.  And so we are once again in trouble.



And Rowhammer, remember, is not something we ever really mitigated against.  Well, there was mitigation in the browsers.  There was talk about increasing the refresh speed so that the cells would tend less to be flipped by noise created by adjacent hammering on the rows.  And then the possibility of next-generation DRAM being smarter about refreshing areas that were under read preferentially so this kind of problem - essentially make hammering attacks obsolete in the hardware.



But, you know, it takes years for that kind of mitigation to get into the channel and then into systems.  And many systems are still using old-school inexpensive RAM that is very prone to Rowhammer-style bit flipping.  And now every version of Intel core architecture, from core one, is subject to this kind of attack.  So again, the famous words of Bruce Schneier echo:  "Attacks never get worse.  They only ever get better."  And here's another example of that.



They did disclose their findings to the Intel Product Security Incident Response Team back at the beginning of December 2018 and probably ruined their holiday because there is just nothing they can do about this.  This is really - oh, but no impact on ARM and AMD.  This is explicitly an Intel memory system.  It's called a "memory order buffer" is what they are exploiting.  And so neither ARM nor AMD employ this particular form of speculation.  But there are an awful lot of Intel core processors out there.  And these guys don't expect them to be fixable.  Yeah, yeah.



LEO:  Well, fortunately, again, no exploits in the wild.



STEVE:  Spoiler, indeed.  No.



LEO:  Good work by the researchers once again.



STEVE:  Yeah.  I would argue, you know, it's speculation we have never seen exploited.  But we have seen Rowhammer exploited.  Rowhammer works.



LEO:  Right, right.



STEVE:  And so what this does is this leverages speculation to dramatically improve the efficiency and performance of Rowhammer.  So this is, you know, it's like...



LEO:  Interesting.  They're inching closer.



STEVE:  ...where we combine problems and end up with something much worse than either one alone, yeah.



LEO:  Very interesting.  As always, the show is fascinating, and it's every week.  And we've got, what is that, about 294 episodes left.  So make sure you tell your friends.  Security Now!, you can find a copy of it at Steve's website, GRC.com.  He's also got transcripts from the dictionary-loving Elaine Farris.  And you can get those at GRC.com.  While you're there you can also pick up a copy of SpinRite, the world's best hard drive maintenance and recovery utility.  What else can you do there? Oh, my gosh, there's a limitless amount of fun.  Just explore around.  Everything else is freely available, including SQRL and more.



If you want video, we've got it, along with audio, at our website, TWiT.tv/sn.  And if you want to watch us live, we do it 1:30 Pacific, 4:30 Eastern.  That is now 20:30 UTC because we sprang forward, 20:30 UTC at TWiT.tv/live.  Audio and video is there.  Join us in the chatroom if you do that because they're watching live, too, irc.twit.tv.  And on-demand audio and video from TWiT.tv/sn.  Or best thing to do, subscribe in your favorite podcast application.  That way you'll get it automatically.  Complete the set.  Get all 999.



Steve, have a great night, and I'll see you next week on Security Now!.



STEVE:  I did want to mention we got a very nice response to my call for Android UX experts.



LEO:  Oh, good.  Oh, for SQRL.



STEVE:  Like seven or eight people have responded and said, hey, I can - oh, yeah, I said SpinRite, I meant SQRL, SQRL UX experts.  So thank you very much, everybody.  And they are online, and they're taking a look to see about enhancing the Android SQRL app.  And I'm continuing to work on that, getting it ready for the world.



LEO:  The world needs it.



STEVE:  Okay, my friend.



LEO:  Have a great night.



STEVE:  Bye.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#706

DATE:		March 19, 2019

TITLE:		Open Source eVoting

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-706.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we look back at last week's March Patch Madness.  We have an answer about the Win7 SHA-256 Windows Update Update; big news regarding the many attacks leveraging the recently discovered WinRAR vulnerability; what happens when Apple, Google, and GoDaddy all drop a bit; an update on a big recent jump in Mirai Botnet capability; some worrisome news about compromised Counter Strike gaming servers; some welcome privacy enhancements coming in the next Android Q; a pair of very odd web browser extensions for Chrome and Firefox from Microsoft; a bit of follow-up on last week's Spoiler topic; some closing-the-loop feedback from our terrific listeners; and an early look at a VERY exciting and encouraging project to create an entirely open eVoting system.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  An update on the WinRAR virus.  It's out in the wild.  Be careful.  Update your WinRAR.  We'll also talk about a funny little pop-up Steve got when he was trying to read up on WinRAR on the McAfee website.  And then a proposal from DARPA for a secure eVoting system.  This one might really work.  Steve's got the deets, coming up next.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 706, recorded Tuesday, March 19th, 2019:  Open Source eVoting.



It's time for Security Now!, the show where we cover your privacy and your security and how computers work and all sorts of stuff with this guy right here giving us the Vulcan salute, Mr. Steve Gibson.  Hi, Steve.



STEVE GIBSON:  Yeah, so we decided the thumb is out for the Vulcan salute.



LEO:  Very important.



STEVE:  It's not thumb in, it is thumb out.



LEO:  It's very important.



STEVE:  So I look like a crazy person.



LEO:  Not to us.



STEVE:  I've tried to minimize the caffeine, so we'll see how the podcast goes today, Leo.  I think maybe a little less nutty.



LEO:  Were you hyper last week?  I don't even remember.



STEVE:  Yeah, I over caffeinated, and it was like, whoa.  So of the news that we'll talk about this week, one topic stood out because it's something that we've talked about, we've encountered, sort of every summer after Def Con, after the hackers just chew apart the voting machines that are actually in use throughout the United States despite all reason.  DARPA, the, well, the source of the Internet because it was the Defense Advanced Research Projects Agency, DARPA, that was behind the original funding of the early experiments to see whether packet switching was something that could actually be made to work.  And that, of course, grew into a global network.



Well, they have let a $10 million contract for the development of open source eVoting, thus the title of today's podcast.  And what we will be discussing as we wrap up this couple hours is some of the details which are very exciting, because they are known, of what is in the process of being developed now to create an open source eVoting platform which we will then hope states demand that companies like Diebold - oh, wait, Diebold.  We decided it was Diebold...



LEO:  Diebold, yeah.



STEVE:  ...then produce to that spec.  But we've got a whole bunch to talk about.  We have...



LEO:  Before you get onto that, I just want to mention last night, yesterday, we did an interview with a guy named Mark Richardson, a photographer.  And look at this.



STEVE:  I saw a picture of the IMP, yes.



LEO:  The IMP.  And I thought of you when you mentioned DARPA, this was the Interface Message Processor that was the first unit on the DARPANET.  This is what was sending messages back and forth.



STEVE:  Yup.  I've had my hands on one of those, actually.  There was one at Stanford University's AI lab when I was there in the early '70s.  And I remember standing there next to it, looking at it and thinking...  



LEO:  History, boy.



STEVE:  ...what the heck is this?



LEO:  That was the first node on the Internet.  This was actually IMP Number 10, apparently.



STEVE:  Wow.



LEO:  Yeah, isn't that cool?  I just thought you'd enjoy that.



STEVE:  Yeah, very cool.



LEO:  Yeah, there's a lot of good pictures in this book.  He took them all at the Computer History Museum.



STEVE:  I love that kind of book.  It's like nostalgia.



LEO:  Well, you'd recognize that, wouldn't you?



STEVE:  There it is, baby.  That's my PDP-8/E, yes.



LEO:  You've got a few of them behind you, as a matter of fact.  Nice.



STEVE:  Very cool.



LEO:  What else?



STEVE:  So we're going to look back at least week's March Patch Madness.  We have an answer about that Win7 SHA-256 Windows Update Update, where remember that right now Windows updates are being cosigned, but that's being dropped after June, so starting with July.  So it's necessary for us, for Windows 7 users, to be sure that we're able to deal with non-cosigned updates.  We have an answer as a consequence of last week.  Big news regarding the many attacks which have surfaced to leverage the recently discovered WinRAR vulnerability that we just talked about.  Turns out it's no longer a theoretical problem.



We're going to look at what happens when Apple, Google, and GoDaddy all drop a bit.  An update on a big recent jump in the Mirai botnet capability.  Some worrisome news about compromised Counter Strike gaming servers and the surprisingly large percentage of them that were infected and infecting the players of Counter Strike.  Some welcome privacy announcements coming in the next Android Q.  And Leo, you need to brainstorm with me what tasty treat begins with the letter Q.  I got nothing.



LEO:  It's tough.  We've been worried about - there's been this Q crisis.  We've been thinking about this for several generations now.  And all we can come up with is quince jam, which is not any good.



STEVE:  No.  I don't think I want any.



LEO:  No, thank you.



STEVE:  So anyway, we have also a pair of very odd web browser extensions, one for Chrome, the other for Firefox, from Microsoft.  Which really kind of sets me off on a, well, as our listeners will see, a bit of a quandary and then a rant.



LEO:  Oh.



STEVE:  Because it's just like, what?  You've got to be kidding me.



LEO:  Oh, I can't wait to hear this, yeah.



STEVE:  And we have a bit of follow-up on last week's Spoiler topic, some closing-the-loop feedback from our terrific listeners, and then we're going to take, as I started off saying, an early look at what is a very exciting and encouraging project to create an entirely open eVoting system platform with secure hardware from DARPA, secure software that will be open and audited.  But I don't want to give it all away.  So I'm going to just bite my tongue at this point and just say, hold on till the end of the podcast.



LEO:  Steve?



STEVE:  So our Picture of the Week is something that confronted me when I was doing some research for the podcast.  You can see behind the dark overlay that it's the McAfee website.  And I was going there to look at their coverage of the WinRAR absolute path traversal vulnerability that we talked about.  And instead I get this dark cover and a pop-up saying - this is from McAfee; right?  That used to be a security company.  "Your browser is blocking some features of this website.  Please follow the instructions at" - and then I have http, notice no "s," http://support.heateor.com/browser-blocking-social-features/ -  "to unblock these."  That is, to stop my browser from doing what I intended it to do for the sake of security.  So thank you, McAfee.  I just grabbed a screenshot of this.  I thought, okay, that's the Picture of the Week.



LEO:  It was probably like the Facebook Like button or something, or social sharing.



STEVE:  Yes.  Yeah.



LEO:  Come on.



STEVE:  Leave me alone, yeah.



LEO:  That's not okay.



STEVE:  Wow.



LEO:  Wow.



STEVE:  Yes.  Your browser is blocking some features of this website.  Yeah, apparently something I don't want.  And so thank goodness it's still my browser.  Well, yeah, it is, actually.  Okay.  So we had March Patch Madness last Tuesday.  Last week we talked about the Google Chrome exploit which was leveraging a pair of zero-day vulnerabilities.  Remember one in Chrome and another in Windows.  Combined, they were being used against Windows 7 users.  Microsoft had only been informed of the problem the week before, and I wondered aloud last week whether this would give them time, or maybe it was the week before, to get it fixed for March.  Well, they did, and it is.  So that's good.



But it turns out there was a companion zero-day.  Remember the news was, oh, this had only been seen being exploited in the wild for Win7.  Turns out there was a probably identical vulnerability that, due to some code changes between 7 and 8, and of course we don't really care about 8, as I heard you guys, I guess it was talking on Windows Weekly recently, saying does anyone even know anybody who has Windows 8?  I don't, you know, we just sort of - that was an embarrassment that Microsoft, well, and Vista also.  And now everybody's on 10, of course.



So in total, Microsoft addressed 64 now CVE-listed security vulnerabilities across a bunch of their products, of course, in Windows OSes and other products.  Seventeen of those 64 were critical, 45 rated as important, one moderate, and one low in severity.  They spanned the OSes, IE, Edge, Office, SharePoint and the Chakra Core, Skype for Business, and Visual Studio NuGet.  And I didn't even take the time to figure out what that was.  That's something that's new to me.  Four of the important security vulnerabilities had been disclosed publicly, but none were known to be exploited in the wild.



But in addition to the one zero-day involved in that actively exploited in-the-wild Chrome attack, which Google immediately shut down, thanks to the fact that they've got on-the-fly constant patching of Chrome, Microsoft also patched that other zero-day that I mentioned which was, it turns out, it was also under attack.  Both were elevation of privilege flaws residing in the win32k.sys driver.  Whereas that first zero-day was only affecting Win7 systems, the second zero-day was also an elevation of privilege vulnerability, was also being exploited in the wild, but not as a consequence of leveraging Chrome.  And it affected all Windows and Server versions after Windows 7, which is why I think since they were so closely related it was probably the same problem expressed differently due to some code changes after Windows 7.



It's also worth noting that last Thursday the 14th, the Chinese 360 Core Security group decided to publish a working proof of concept for this Win7 vulnerability.  Their release justified this, I think, well, semi-justified it by saying:  "Considering that some users are still using Windows 7" - well, yeah, half of us, or just now it's flipped from just barely more using Windows 7 to just barely more using Windows 10, like, what, a couple months ago.  And of course that'll change once the security updates stop a year from last month, so it'll be February 2020 that real pressure begins to be put on people to finally pry them away from Windows 7.



Anyway, they said:  "Considering that some users are still using Windows 7, this vulnerability, combined with Chrome's RCE, has been used for real APT," they wrote, Advanced Persistent Threat - "attacks.  So this zero-day is very likely to be exploited to perform large-scale attacks and pose a real threat," and as I mentioned, except that it was fixed in Chrome even before its announcement.  So it's like, okay.



They said:  "Therefore, 360 Core Security Technique Center constructed the proof of concept and reproduced the vulnerability triggering process so that security vendors can reference to increase the corresponding protection measures."  Okay.  I mean, what we're really seeing is we're seeing a problem increasingly with these proof of concepts being published before there has been a chance for their mitigations or their fixes to happen.



And so this was on Thursday, two days after Windows Patch Tuesday update that fixed the problem.  I would argue that's not time enough to allow people, I mean, many people put off rebooting.  I mean, it's funny, some of these updates say "May require a reboot."  But it may require you to stop using your system for an hour.  Well, okay, maybe not.  But still, certainly a reboot.  So we know that there are instances, I encounter them in my daily life, of people who just say, no, no, no, not now.  No, no, no, not now.  And they keep pushing it off.



So the point is I don't think two days works.  I would say two weeks probably would be responsible.  And there's been dialogue now in the security community about how long someone should responsibly wait before pushing out a proof of concept.  What's really happening is they're wanting to draw people to their site by producing it.  And the strong argument is it's of more use to bad guys than it is to good people at this point.  So, and again, like why does a security vendor need to worry about this if Microsoft has fixed it in their patches?  It's a non-problem like all the other non-problems that Microsoft has patched for the last three decades.  So it's like, okay.



Anyway, that second flaw was detected and responsibly reported to Microsoft by security researchers at Kaspersky Labs, who in a blog post coincident with the patch's release revealed that the flaw - without giving any details, they were responsible - was being actively exploited in targeted attacks which we did not know before by several threat actors including one that we've spoken of before, FruityArmor, okay, and SandCat.



And ZDNet in their coverage had an interesting note about the FruityArmor/SandCat connection.  ZDNet wrote:  "The November zero-day" - so speaking of a previous one - "was also abused by SandCat, a new group on the APT" - again, Advanced Persistent Threat - "scene about which Kaspersky has few details, such as its use of the March and November zero-days, the Chainshot exploit, and the FinFisher/FinSpy hacking framework.  What all this tells security experts is that there's at least some type of connection between these two threat actors, FruityArmor and SandCat.  They are either managed by the same intelligence service, or" - and this is what kind of gave me a chill - "they're buying zero-days from the same exploit vendor."



LEO:  Ooh.



STEVE:  And of course we've been - yeah - we've been talking about exploit vendors recently.  And I thought, wouldn't that  be creepy if we're seeing separate entities now having essentially the same exploits in the same timeframe because in - and again, in targeted attacks, again, that's what you would expect if you're buying zero-days from an exploit vendor.  You're not doing opportunistic spray spam campaigns.  You're going after specific individuals or organizations with an individual to target attacks, and thus to minimize the exposure of this because, once it becomes well known, it's no longer of any use.



LEO:  Although, I mean, it's possible, isn't it, that Zerodium or somebody like that sells - I bet you for a price you can get exclusive access to a flaw, and for a lower price you can get shared access; right?



STEVE:  Right, right.  I would bet that's true.  It's sort of like...



LEO:  Yeah.  But I hope that, what is it, FruityCat?  FruityArmor and SandCat?



STEVE:  FruityArmor, yeah.



LEO:  They could be the same guys, too; right?  I mean...



STEVE:  They absolutely could, yes.  And also some of the naming of these is somewhat obscure.  I mean, they're not like declaring themselves.  Normally a disassembly of their code finds...



LEO:  We are FruityArmor, and we are here to investigate.



STEVE:  Exactly.



LEO:  Wow, it's so funny.  It's so funny.



STEVE:  Exactly.  So as for the overall March Madness patching, as usually, nearly all of the critical rated vulnerabilities do lead to remote code execution attacks and primarily impacted various versions of Windows 10 and Server editions.  Most of the flaws reside in the Chakra Scripting Engine, VBScript Engine, DHCP Client, and IE.  In other words, notice those are all things that have Internet-facing attack surfaces.  So that is now where we need to be paying the most attention.  While some of the only important vulnerabilities can also lead to remote code execution attacks, others allow elevation of privilege, information disclosure, and denial of service attacks.



Okay.  Part 2 of this is what about this SHA-2 Windows Update signing update?  I also confirmed last Tuesday that my Win7 machine did acquire with last Tuesday's updates the ability now to verify SHA-2, which is SHA-256.  Technically we had SHA-1 that was just a single, sometimes called SHA-1, a single hash, and I'm thinking it's what, 160 bits.  SHA-2 is actually a second-generation of SHA hashes, one of them being SHA-256, so that gives you a 256-bit hash output, substantially bigger than SHA-1's 160-bit hash output.  So thus better.



Anyway, so the point was that Microsoft announced, and we covered several months ago, that this was going to be happening.  But what wasn't clear, no matter how closely you read the disclosure from Microsoft, because they were like warning people, and there was some sense of you may have to go get this.  So if that was the case, I wanted to make sure everybody who was still using Windows 7 did because updates would just unceremoniously stop after June.  So the answer is yes.  My system obtained the update.  And that's 4474419.



And so if you just look, if a Windows 7 user is curious, you can just look in your history, your update history, and a ways back, you'll probably see a bunch of daily, since last Tuesday, you'll see like a bunch of daily Windows Defender updates, assuming that you're doing that.  And then you'll see the Patch Tuesday batch, and you'll find 4474419, which says that, or means that your system now is able to - it's been updated, not to understand SHA-256 hash.  It had that already in order to be using any of the SHA-256 signed TLS certs that are now on the Internet.  We've had that for a long time.  In fact, it has had it for a long time.  Windows XP was actually getting that back with Service Pack 3, so Windows 7 has had that, but not for Windows Update updates.  Now it does.  So we know that moving forward we will be able to get them.



And, you know, I was worried that, if someone hadn't updated, then they might sort of get in a Catch-22 situation after June.  But you'd still always be able to get the double-signed updates from like March and April and May and June, which would include this 4474419, and then you'd be able to continue moving forward.  So I'm sure that even, for example, new systems set up with Windows 7 after June will be able to get themselves brought current until Microsoft decides they don't want to do this anymore, starting with next February.



Okay.  So I talked at some length a couple podcasts ago about this WinRAR vulnerability that was discovered by Check Point Research.  What they found, to recap, is that - and this was about a month ago - was that an old DLL for which the source had been lost was able to decompress ACE archives.  Okay.  So WinRAR knows about a whole bunch of different archive formats.  And so just it was a bullet point in the features list, is yeah, we can open ACE archives.  No one can make them.  But if there are any lying around the Internet somewhere, WinRAR could open them.



It was a theoretical concern when it was announced.  So this was not a zero-day.  This was not discovered being exploited.  I don't think it was.  Now I'm questioning myself.  But I don't think so.  What they found was - they called it an "absolute path traversal" vulnerability, which allowed an ACE archive to be created that would allow an executable to escape from the directory where it was being expanded or decompressed or de-archived to, which is typically down, like under the My Documents or My Downloads or whatever folder, which is not going to - so if there was an executable there, it wouldn't automatically be run.



The other trick was that WinRAR determines archive type by inspecting it, not by believing the file extension.  So someone might think, what the heck's an ACE, and not download it.  But if it said .rar, and if somebody had installed WinRAR, they probably understood what a .rar was, that it was a high-compression, typically high-quality archive.  So at the time this was announced, RAR Lab immediately dropped support with v5.70 that was released on February 27th.  By the time I reported this on the podcast, I had updated and was able to confirm that not only, yes, had ACE support disappeared, but the two ACE-related files had been proactively deleted from the subdirectory where WinRAR was installed to prevent any mischief of it.



Well, okay.  So get this.  The problem is WinRAR doesn't have any form of auto update mechanism.  And I really don't fault an archiving tool for not auto updating, especially one that is two decades old.  It's 19 years old.  Except now we wish it had one because what this means after 500 million users have downloaded WinRAR through the years is that all of them are vulnerable to this, and almost none of them are going to get updated because most users, 500 million, we wish we had that many people listening to this podcast, but we don't.  Which means unless they somehow, I mean, it's just not going to happen that any of these versions of WinRAR which were downloaded and installed over this period of time are going to be updated.  Yes, some fraction, but not significant.  And the bad guys know that.  The bad guys know that that's the case.



So what's happened is we have now seen, as reported by McAfee, hundreds, more than a hundred different exploits trying to target this vulnerability to install malware in people's machines.  In one recent example, there was a bootlegged copy of Ariana Grande's hit album, "Thank U, Next," that had the filename Ariana Grande...



LEO:  Aptly named.



STEVE:  Huh?



LEO:  Very well named.  Thank you.  Next.



STEVE:  Yes, exactly.  Exactly.  It was "Ariana_Grande-thank_u,_next(2019)_[320]," presumably meaning it was a high-quality compression of her album, dot rar.  And if you were to decompress that, you would end up with a trojan, a remote-access trojan installed in your startup folder so that the next time you restarted Windows it would run without any UAC prompting, no user interaction, and install itself into your system.



So this has gone from a theoretical problem as a consequence of the fact that the bad guys also, as I, do not expect this thing to essentially ever get fixed.  So it will take the malware, the AV tools - and I have not looked.  What I do know is that at the time of this Ariana Grande album being released, the it as malware, that is a malformed RAR archive, was detected by only 11 AV, and 73 AV products failed to alert the users of anything being amiss.  So hopefully this will get preempted by Windows Defender and other AV tools stepping up and starting to filter this because otherwise this is going to be bad for users.



And there is a flavor of ransomware, as well.  These 360 Threat Intelligence Center guys spotted in the wild one of these WinRAR archives which was called "vk_4221345.rar" which had a compressed picture of a girl, only it looked like the decompression had failed so that it was cut off halfway.  Maybe that's to induce the person to open the archive to hopefully get the whole picture or others or who knows what.  I think it was Chinese, and so it was difficult to understand exactly what was going on.



Anyway, it installs a ransomware malware payload which they named JNEC.a.  Written in .NET, it asks for 0.05 bitcoins.  And with bitcoin now generally hovering around $4,000 U.S., that's about $200 U.S. ransom payment they're asking for.  So anyway, I mean, this is what we're seeing.  A new vulnerability comes to light, and we are now, I mean, there's just - there's an industry, essentially, in place, ready to turn that thing into exploits of one form or another.  So what a world we're in.



Okay.  This is really interesting.  GoDaddy, Apple, and Google  have technically mis-issued more than two million TLS certificates.  And this sort of falls under the heading of one thing leads to another.  So remember our recent discussion about that sketchy wannabe UAE-based certificate authority who decided to name themselves DarkMatter.  And it's like, okay, we're going to be a trusted certificate authority called DarkMatter.  And of course they have been accused in the past of using their technology, their man-in-the-middle middlebox technology on behalf of repressive regimes to spy on people.  And now they're appealing to Mozilla to have their CA signing public key added to Mozilla's root certificate store so that we'll trust their certificates without question.  What could possibly go wrong?



Well, as it turns out, as a consequence of the discussion in mozilla.dev.security.policy group about DarkMatter's controversial application to become a fully fledged cert-issuing CA, people were poking into DarkMatter's existing countersigned certs and happened to discover that the company's supposedly 64-bit serial numbers which were being embedded in its certificates were actually, Leo, coming up one bit short - 63 bits.



LEO:  Oh.



STEVE:  But then engineers at other major organizations, and by that I mean GoDaddy, Apple, and Google, who were reading the thread, realized that their own certificates - whoopsie.



LEO:  But what's the impact of being a bit short?



STEVE:  And that's where we're headed next.



LEO:  Okay.



STEVE:  So they realized their own certificates were similarly affected.  So as I said, one thing leads to another.  So what's behind, first of all, what's behind this broadly made mistake?  And then we'll talk about what does it mean.



As it turns out, this is a consequence of everyone using a not technically RFC fully compliant default setting in a commonly used open source certificate serial number generator.  It is actually an open source CA.  It's known as EJBCA.  And Leo, if you google EJBCA, you'll see, like, it's a big deal.  GoDaddy, Apple, Google, and apparently many others, as a consequence of them using this without like looking at all the default settings, are now facing the revocation and reissuance of more than two million certificates.  GoDaddy alone estimated that they had issued 1.8 million certs which had 63-bit serial numbers.



So, okay.  In my opinion, this is ridiculous.  This is a tempest in a teapot.  But, you know, standards are standards.  Rules are rules.  I went digging into RFC 5280, which is the RFC standard.  The title is "Internet X.509 Public Key Infrastructure Certificate and Certificate Revocation List Profile."  It says in Appendix B, ASN.1 Notes:  "CAs MUST [all caps MUST] force the serial number to be a non-negative integer."  Well, uh-huh.  A negative serial number would be a little screwy; right?



And then it says:  "That is, the sign bit in the DER encoding of the integer value MUST [all caps again] be zero.  This can be done by adding a leading leftmost 00," you know, null, all-zero, they use the term "octet," which is the fancy word for byte, "if necessary."  So they're saying stick a double-zero, you know, two hex zeroes, eight bits of zero, on the front, if you must.  That way you're sure you're going to get all eight of your leading zero bits, or eight of your leading bits are zero.  So you've got lots of margin.



Anyway, the Appendix B goes on:  "This removes a potential ambiguity in mapping between a string of octets [a string of bytes] and an integer value.  As noted in Section 4.1.2.2, serial numbers can be expected to contain long integers.  Certificate users MUST [all caps] be able to handle serial number values up to 20 octets" - that is, 20 bytes, so what's that, 160 bits - "in length.  Conforming CAs MUST NOT use serial number values longer than 20 octets."  So they said you could have serial numbers up to but not longer than, 20.



Okay.  So who is this EJBCA that got everybody in trouble?  They call themselves "The Open Source CA."  Their own site says it's "a PKI [Public Key Infrastructure] Certificate Authority software built using Java (JEE) technology.  Robust, flexible, high performance, scalable, platform independent, and component based, EJBCA can be used standalone or integrated with other applications."  Unfortunately, they didn't happen to mention in there that they're not RFC compliant.  Whoops.  Okay.



But they go on to say:  "Extremely scalable and flexible, EJBCA is suitable to build a complete PKI infrastructure for any large" - I mean, Google, GoDaddy, Apple, so yes - "any large enterprise or organization.  If you only want to issue a few single certificates for testing, there are probably other options that will get you started quicker.  But if you want a serious Certificate Authority to manage your Public Key Infrastructure" - and cause you to reissue millions of certificates, no, it doesn't say that - "we recommend EJBCA."  And Wikipedia goes on to talk about them.  I mean, so they're a well-known group.



Okay.  Unfortunately, until recently, this very nice-looking and capable open source CA system was configured to generate eight octet serial numbers.  Eight octets is eight bytes, which is 64 bits.  Serial number fields are defined, and I do not know why, as "signed integers."  This must be some nutty committee decision, since a negative serial number makes no sense.  But that's what it is.  It's signed.



So let's take a moment to remember binary number format.  The industry standard two's complement math, which is what all of our computers use, that's what these little blinky lights behind me are doing is two's complement math.  The number zero is represented as all binary bits being off.  All binary bits are zero.  If you start to increment that binary value, the first bit, the lowest order bit, the rightmost bit when they're all stretched out linearly, the rightmost bit turns to a one.  You increment it again, it goes back to zero, and the bit to its left turns into a one.  You increment again, then they're both ones.  And again, they both go to zero, and the third bit turns on.  And so on.



So what if we decrement?  We start - so go back, reset our register to all zeroes.  Now we decrement.  What happens is all the bits go to one.  So minus one in two's complement math is all binary one bits on.  If you decrement again, then the least significant one goes to zero.  Decrement it again, it goes back to one, and the one to its immediate left goes to zero.  Decrement again, both lower order bits are zero, and so on.  So if you think about it, what this sort of means is, and actually does mean, that the highest order bit, that leftmost bit is the sign bit.  If it's zero, then you have a positive binary number.  And if it's one, then that binary string of bits in two's complement representation is a negative number.



So the RFC says we need to generate for a certificate a high-entropy, that is, like 64 bits of entropy, serial number.  And it also says it must be positive.  So doing so is simple.  You ask your entropy source for some number of bytes.  You get them.  And then you turn off, you make sure the highest significant bit, the leftmost bit, you just force it to zero.  So it was normally going to be a one or a zero with 50/50 probability, right, because all the bits in all of the bits you get are going to be 50/50 chance of being a one or a zero.  So you just make sure, you just clear, you set to zero the most significant bit.  Now you're guaranteed that the result is positive, is a big random positive number.



Unfortunately, it no longer has 64 bits of entropy; right?  It used to.  But that's because we weren't sure if that last bit was going to be a one or a zero.  We're now - we made it zero.  Well, that just killed off a bit of entropy.  Now we only have 63 bits of entropy, and we have fallen out of spec.  And that's what this EJBCA CA was doing to all of the certificates that it generated.



So that's not good.  Okay.  But what does it really mean in terms of security?  And this is why I think this is all a bunch of nonsense.  It's technically not in compliance.  I guess that's enough.  But on the other hand, all of those mis-issued certs are going to be expiring themselves after two or three years.  And revocation doesn't work.  We know that.  We've covered that extensively.  So how is one bit less of entropy in certificate serial numbers a big cause of concern?  Is it?



So what is the concern with a certificate serial number?  The concern is that serial numbers are sometimes used to pin certificates.  That is, you say, okay, I want to pin the cert, and you use its serial number.  You record it and you make sure, like, when you get that serial number from a remote server, you check the serial number, having memorized it earlier, to make sure it hasn't changed.  I mean, I guess that's, I mean, certainly that is being done.  To me, it makes much more sense to use the much longer thumbprint hash, but people often, you know, which is included in the certificate, so you could check it.



But the serial number is there.  It's part of the spec.  And people may want to rely on it.  So what we're concerned with is the chance that another randomly generated certificate's serial number might have the same serial number as the one we have pinned.  That is, there could be a serial number collision.



So with the classic "birthday paradox," that doesn't apply here, as we'll see.  With the birthday paradox we would be asking, given a pool of certificates of a certain size, and each certificate containing some entropy of a certain size, what's the likelihood of any pair, any two certificates, having the same serial number in that entire set?  That's the birthday paradox.  And what's surprising, we've talked about this in the past, is how quickly that probability falls as the size of the pool increases.  And this happens, of course, because every certificate serial number is being compared with every other certificate, kind of all at once.



But that's not the concern here.  In our case, we're just worried that one other certificate serial number might have the same serial number as the one certificate we have pinned that we care about.  So that makes the math simple.  The math on the birthday paradox is 1.2 times the square root of the size of the pool is an estimate.  But that doesn't apply here.  The CAB spec, the CA Browser spec wants that two-cert serial number collision probability to be no greater than one in 2^64.



So I did the math.  That's one chance in, and the number is 18446744073709551616.  Naming large numbers, we've talked about large number naming in the past.  We know there's million, then billion, then trillion, then it goes to quadrillion, then to quintillion.  That's where we are here.  So with the full 64 bits, the chance of collision with another randomly generated certificate serial number is one in 18.446 quintillion.  That's what the spec wants.  When we lose a bit by forcing the first bit to be zero, we of course cut the total universe of possible serial numbers in half, thus doubling the collision probability from that one in 18.446 quintillion down to only one in 9.223 quintillion.



And so I just cannot get very worked up about that, especially given the fact that these certs, with their one-bit-shy serial numbers, will all be expiring themselves in two or three years.  And notice that, if the serial number you are comparing with is longer, then it can't possibly collide with one of these 63-bit serial numbers.  So if your serial number that you have pinned is 256 bits, well, it's going to be different.  No possibility of collision.  So it's like, okay.



But despite the fact, rules are rules, specs are specs, and apparently Apple, GoDaddy, Google, and who knows who else are all now in a froth, feeling like - in fact, it's supposed to be done within a few days.  I think seven days is what the requirements call for if this sort of problem occurs.  GoDaddy just shrugged and said,  "We can't.  I mean, we physically, logically, logistically can't reissue 1.8 million certificates in seven days.  It's going to take us at least a month."



LEO:  And by the way, I believe our TWiT certificate is a GoDaddy certificate.  I'm not sure.  Most of them are DigiCert.



STEVE:  It just doesn't matter.  It just doesn't matter.



LEO:  No, it's not insecure.



STEVE:  Right.  It's not about security, it's about a serial number that, first of all, isn't used.  Nothing actually uses it...



LEO:  Oh, it's crazy.



STEVE:  ...for the transaction or for TLS or anything.  It's only there, I mean, it's just a bit of entropy to give the certificate a unique token, a minimum 64-bit token.  It turns out, whoops, because it has to be positive, and because this lame bunch of committee people said, oh, it's signed, so make it positive, and so the way you do that is you make sure it has a leading zero, and this is the result.  I also got a kick out of the fact that the newsfeed over at EJBCA, the top of the feed currently notes, says:  "EJBCA can be configured to generate certificate serial numbers."  And it says, parens, "(positive integers from 4 to 20 octets)."



LEO:  Oh, man.



STEVE:  And then they said configurable - this was posted on March 13th.  "Configurable serial number entropy default value raised to 20 octets."  So too little, too late in the case of many millions of certificates.  Anyway, this should just kind of be noted and then, like, okay, everybody ought to get a pass on this.  But I guess that's not the way this is being handled.



LEO:  Wow.



STEVE:  So Leo.



LEO:  Yes.



STEVE:  I have bad news for you.



LEO:  Oh, no.



STEVE:  Your serial number is 654AB5B273E66A34.



LEO:  Yes.



STEVE:  Sixty-three bits.



LEO:  [Exclamation].



STEVE:  My serial number.



LEO:  Yeah?  Yeah?



STEVE:  07A7894CFA6A7FEFEB90996F59D9F402.  128 bits.  Actually 127 bits.



LEO:  How come yours is so long?



STEVE:  I got mine from DigiCert.



LEO:  Ah.  We should have done DigiCert.



STEVE:  That's right.



LEO:  By the way, the ransomware has shut down one of the world's largest producers of aluminum.  Norsk Hydro of Norway, malware hit computers in the U.S. on Monday night.  By Tuesday morning it had spread to other parts of the company.  It's in 40 different countries.  Some of the plants, this is one of the biggest aluminum producers in the world, plants have been stopped or disrupted because of ransomware.



STEVE:  Yeah.



LEO:  Unbelievable.  It's LockerGoga.  I don't know if you've heard of that one.  LockerGoga ransomware.



STEVE:  Wow.



LEO:  So it's still happening.  It's incredible.  Well, so you got all the bits.



STEVE:  Yeah, so...



LEO:  But it doesn't mean we're insecure or anything; right?



STEVE:  No.



LEO:  There's not going to be a collision.



STEVE:  No.  Well, it means that your certificate is going to be revoked, baby.



LEO:  Yeah.



STEVE:  I mean, someone's going to come...  



LEO:  Well, I thought you said GoDaddy was going to just give up.



STEVE:  Well, I don't know what they're going to do.  But, I mean, you've got one of the bad ones.



LEO:  We got one of the bad ones.



STEVE:  Yeah.  Now, notice that the first byte is 65.  So a 65 hex is 0110.  So if it were eight or greater, well, it can't be because then the most significant bit would be one, which would be a negative serial number.  



LEO:  Yes, right.



STEVE:  But what's dumb is that, well, at least Firefox is showing me the serial numbers in hex.  So it doesn't - it's not signed.  It's not a signed integer.



LEO:  It's not signed.  It doesn't matter.



STEVE:  It's not an integer, exactly.  So this is all like a display issue, not anything more.  But again, this is another example of DigiCert doing it right, and their serial numbers are double the minimum size and half the maximum.  So they're not going to break anything at the high end, and they're absolutely compliant at the low end.  So, but it does mean you've got one of the noncompliant certificates.  So presumably, like all of the certificates GoDaddy has been issuing are like this.  Because you've probably had yours for a while.  This is not like this just happened yesterday.



LEO:  We renewed like in the last six months.



STEVE:  Okay.



LEO:  And normally we use DigiCert.  And Russell said - Russell.  "GoDaddy's a lot cheaper.  Why don't you use GoDaddy?"  Russell.  And I said, oh, you know, it's expensive because we have a wildcard cert for TWiT.  You know, saved me some money.  So I was cheap.  And you see?  You see what you - you see, you get  one fewer bit.  You save money, or in your case I get 64 fewer bits.  



STEVE:  Yeah.  And unfortunately, that's one too few.



LEO:  I like big bits.  I cannot lie.



STEVE:  Sometimes you get what you paid for.



LEO:  Yeah.  One bit short.



STEVE:  My company comes through again.



LEO:  Yeah, I really - honestly, I love DigiCert.  And my own personal certs are all with DigiCert.  But I cheaped out and lost a TWiT bit.



STEVE:  So speaking of - this is sort of apropos, actually, this ransomware attack.  The Mirai botnet is alive and well and more scary and capable than ever.  We'll remember that it is considered an IoT malware.  It broke the DDoS attack size record in 2016 when it was used to attack Brian Krebs, forcing him off the 'Net.  Also the French web host OVH was attacked.  And then, most famously, the DNS provider DynDNS was forced off the 'Net.  And of course because we all depend upon DNS to varying degrees, that caused a ripple effect, and all kinds of other sites disappeared as their DNS expired, and it couldn't get refreshed from their assigned DNS provider that was in this case DynDNS.



So Mirai has been updated to target a new crop of devices including two which are often found inside enterprise networks where, as we know, bandwidth is often more plentiful than it is on consumer IoT networks.  Mirai now knows how to infect webcams, routers, DVRs, as well as many other Internet-connected devices which typically ship with default credentials, as we know, and typically they're running never updated, and thus woefully outdated, versions of Linux.



Yesterday morning Palo Alto Networks Unit 42 - I love it that they call themselves Unit 42.  They posted news of a new Mirai titled "New Mirai Variant Targets Enterprise Wireless Presentation and Display Systems."  So I've edited this down a bit for length and clarity.  But they basically wrote:  "The Mirai variant that Unit 42 discovered is notable for targeting different embedded devices like routers, network storage devices, network video recorders, and IP cameras, using numerous exploits against them.  Specifically, Unit 42 found this new variant targeting wePresent WiPG-1000 Wireless Presentation systems and LG's Supersign TVs."



They wrote:  "Both these devices are intended for use by businesses.  This development indicates to us a potential shift to using Mirai to target enterprises."  Attack code exploiting a wePresent command-injection vulnerability was published in 2017, while a remote code execution exploit for the LG Supersign TVs has been available since last September.  After being packaged into this new Mirai variant, the exploits become much more likely than previously to actively be used to compromise their vulnerable devices.



And this is not the first time Mirai has been aimed at enterprise networks.  Last September Palo Alto Networks reported that Mirai was found targeting the same Apache Struts vulnerability that hackers exploited to breach Equifax.  So in addition to this newer targeting, this new Mirai variant incorporates 11 new exploits in its multi-exploit kit, and has four sets of new credentials used in brute-forcing device sign-on.  So Mirai is still alive and well.  It had 16 previously seen exploits.  It added 11.



So it's gotten way more competent, and it is still out there scanning the Internet, looking for new victims and, unfortunately, finding them.  It uses an HTTP flood to do DDoS attacks, and it is a worm.  So once it gets a beachhead, it then starts scanning for other available devices, both inside and outside of its network.  So it doesn't look like the Internet is going to be getting rid of Mirai anytime soon.  And we're just going to be stuck with this thing, and it's going to be causing more havoc.



This is odd, Leo, and I wonder if - I guess it's really not on Paul's radar.  But I think of him because of gaming.  Counter Strike has been striking its players.  The malicious network has now been taken down, but there's a useful lesson, I think, to be learned in its aftermath.  The servers in question were malicious Counter Strike 1.6 servers which were being used in zero-day attacks to infect game players with malware.  The Russian AV firm Dr. Web found that 39%, that's 1,951, 39% of all Counter Strike 1.6 servers were malicious and were actively trying to infect their users with malware.



Dr. Web wrote in their report, which I'll summarize, they said:  "Introduction:  The game Counter Strike 1.6 was released by Valve Corporation back in 2000."  So it's been around for quite a while.  They said:  "Despite its rather considerable age, it still has a large fan base.  The number of players using official CS 1.6 clients reaches an average of 20,000 people playing online."  Okay.  So it's not 200,000 or two million, but 20,000.



They said:  "While the overall number of game servers registered on Steam exceeds 5,000, selling, renting, and promoting game servers is now deemed an actual business, and these services can be purchased on various websites.  For example, raising a server's rank for a week costs 200 rubles."  And I did the conversion.  That's $3.11 currently.



LEO:  It's cheap.



STEVE:  Yeah.  And they say, "Which is not much.  But a rather large number of buyers makes this strategy a rather successful business model.  Many owners of popular game servers also raise money from players by selling various privileges such as protection against bans, access to weapons, et cetera.  Some server owners advertise themselves independently, while others purchase server promotion services from contractor.  So, yeah, there's a commercial ecosystem there.  Having paid for a service, customers often remain oblivious as to how exactly their servers are advertised.



"As it turned out, the developer named 'Belonard' resorted to illegal means of promotion.  His server infected the devices of players, that is, players' PCs, with a trojan, and used their accounts to promote other game servers.  The owner of the malicious server used the vulnerabilities of the game client and a newly written trojan as a technical foundation for their business.  The trojan infects players' devices and downloads malware to secure the trojan in the system and distribute it to devices of other players.  For that, they exploit remote code execution vulnerabilities, two of which have been found in the official game client and four in the pirated one.



"Once set up in the system, the Belonard trojan replaces the list of available game servers in the games client and creates proxies on the infected computer to spread the trojan.  As a rule, proxy servers show a lower ping, so other players will see them at the top of the list.  By selecting one of them, a player gets redirected to a malicious server where their computer becomes infected with trojan.belonard.  Using this pattern, the developer of the trojan managed to create a botnet that makes up a considerable part of the CS, the Counter Strike 1.6 game servers.



"According to our analysis, out of the some 5,000 servers available from the official Steam client, 1,951" - so just shy of 2,000 - "were created by the Belonard trojan.  This is 39% [as I said] of all game servers.  A network of this scale allowed the trojan's developer to promote other servers for money, adding them to lists of available servers in infected game clients.  We previously reported," they wrote, "a similar incident with Counter Strike 1.6 where a trojan could infect a player's device via a malicious server.  However, a user then had to approve the download of malicious files, while this time a trojan attacks devices unnoticed by the users."



Dr. Web, this outfit, have informed Valve about these vulnerabilities and other vulnerabilities of the game.  But as of now, there is no data on when the vulnerabilities will be fixed.  Trojan Belonard consists of 11 components, so it's not, I mean, it was an effort to put this together, and operates under different scenarios depending on the game client.  So it's also multi-homed.  If the official client is used, the trojan infects the device using an RCE (Remote Code Execution) vulnerability exploited by the malicious server, and then establishes within the system.  A clean, pirated client is infected the same way.  If a user downloads an infected client from the website of the owner of the malicious server, the trojan's persistence in the system is ensured after the first launch of the game.  Wow.



So there's really nothing that I can suggest that a user might do to protect themselves from this threat.  Even the official Valve client has two known and exploited by this trojan vulnerabilities.  So perhaps stick to well-known and trusted game servers.  What, a little over 60% of them are not part of this trojan server network, so you'd be safe there.  Maybe use a throwaway PC, if that's your platform.  Just because you're sort of in a high - unfortunately, gaming in a gaming server network can be high risk.  So, you know, don't do this on your main system where all your banking is being stored.  And I guess the best takeaway is to maintain an awareness that this kind of thing is going on and just be a little bit more cautious and suspicious than you would otherwise be.



Next release of Android is Android Q.  And as we said, Leo, I mean, I thought for a while.  I can't think of a tasty treat.  You know, we've had Marshmallow, and was R Raisin?  Or was that something else?



LEO:  What was R?  Rocky Road?  I don't remember.  But Q, there's one thing, there's a French dessert, the Quenelle.  But, see, this is the problem.  There's no obvious kind of English-language dessert.  I don't know what they're going to do.



STEVE:  Did we start with A?  Or did we jump?  I don't remember.



LEO:  There were apparently A's, B's.  But Cupcake was the first public release, C.



STEVE:  Oh, okay.



LEO:  And I used to be able to rattle them all off.



STEVE:  Was it Donut for D?  Eclair?



LEO:  Eclair, Donut, yeah.  Cupcake, Donut, Eclair, Froyo.



STEVE:  Oh, that's right, yeah.



LEO:  Remember?  Yeah.  G, what was G?  Gingerbread.



STEVE:  Ah, right.



LEO:  Now, see, you've got me started.  I'm going to start, I'm going to have to finish.  Honeycomb.  "I" I don't remember.  I'm sure Jason Howell could do them all.  Ice Cream Sandwich.



STEVE:  Well, I will be excited to see what they come up with for Q because, I mean, they're on a roll.



LEO:  They should just skip Q and go to Rocky Road.  Honestly, that's an obvious R, so...



STEVE:  Yeah, yeah.



LEO:  I don't know.  We've been wondering about this ever since P.



STEVE:  Is Zagnut a...



LEO:  Zagnut, yeah.  Only once did they use brands.



STEVE:  Oh, right, right, right, instead of it being [crosstalk].



LEO:  And that was kind of a mistake, I think.



STEVE:  Yeah, yeah.



LEO:  So I don't know what they're going to do.  KitKat, that was K.



STEVE:  Well, what we do - oh, yeah, right, of course.



LEO:  Oreo is a brand, actually, yeah.



STEVE:  Yes, yeah, yup.  What we do know is that Android Q, whatever it ends up being called, will finally be delivering really robust Mac address randomization.  Also it'll have new location permission pop-up, which I actually have on the second page of this story.  I grabbed a snapshot of it.  And it will be preventing clipboard sniffing, which has been a big privacy concern.



The beta of Q was first released last week, promising a bunch of welcome privacy improvements.  As for access to clipboard data, Android apps all used to have access, for example, as Windows apps today do.  But Android apps will no longer be able to access the Android operating systems clipboard data, the shared clipboard, unless they're in focus, running in the foreground, onscreen.  The exception which had to be made is the default input method editor, i.e., the system keyboard.  It does have access to the clipboard all the time.  But that really reduces the clipboard attack surface dramatically.



Also, Android Q will have Mac address randomization on by default.  Google introduced Mac address randomization in Android 6.0, but devices broadcast a random Mac address only when the smartphone would initiate a background WiFi or Bluetooth scan.  Android Q devices will now transmit a randomized Mac address by default at all times and for all communications.  And as far as I know, this bests iOS as a privacy feature.



Last time I looked, and last time we talked about it, iOS was broadcasting a random Mac address when it wasn't associated with a WiFi access point, but it reverted to its real fixed and unchangeable factory set Mac address when it was actually associating with a WiFi access point, presumably so that the WiFi access point could determine which iPhone or iOS device it was.  But of course that's a privacy breach because you don't know who's reading those Mac addresses from the access point that you have opportunistically associated with.



So bravo to, I mean, and there's no reason it can't be a random Mac address unless maybe you would want to pin the Mac address to give privileges on - but, again, that's not really secure because Mac addresses can be spoofed.  So since they can be spoofed, let's just always spoof them and throw, I mean, maybe I'm wrong, and iOS fixed this, and Rene is, like, saying "Steve, Steve," you know, I don't know.  I'm sure I'll find out.



LEO:  I feel like it is something iOS does, but I don't off the...



STEVE:  Well, they went halfway.  I remember when we talked about this they were doing random until you associated, and then they used their real Mac address.  Android Q is never - doesn't have a real Mac address.  It's just going to make one up, which is cool.



LEO:  Here's an article from The Register from last year.  Thomas Claburn, who's pretty good.  "Mac randomization, a massive failure that leaves iPhones and Android mobiles open to tracking."



STEVE:  Yeah, yeah.  And it has been noted that it isn't a huge win, but it's better than nothing.  And I just say, you know, why not do it?  They're also removing in Q easy access to network data.  There was the /proc/net function that just...



LEO:  "U.S. Naval Academy researchers report they were able to track 100% of devices using randomization, regardless of manufacturer, by exploiting a previously unknown flaw in the way wireless chipsets handle low-level control frames."  So it just doesn't work.



STEVE:  Yeah, thanks anyway, yes.  So anyway, the access to /proc/net is now being restricted.  It represented very low-hanging fruit used by some data harvesters to access information about the device's network state, and it's been removed.  And similarly, easy access to device details is being curtailed.  Starting with Q, Google will require app developers to request a special permission before they can access what Google calls non-resettable device identifiers such as the IMEI and the serial number.  So much as the app has to ask for permission for this or that sort of granular feature, apps will need to be asking for this kind of device identification that is not user resettable, starting with Q.  So it's like, hey, this all sounds good.



And, finally, more location, or more control over location data.  Android Q will receive a new permissions pop-up, kind of the thing that we're used to seeing from iOS devices, asking about location data.  Beginning with Q, users will be able to give apps access to location data all the time, or only when the app is in focus, you know, in the foreground.  And of course one of the features that I like about iOS is after some length of time it'll come back and say, hey, you know, this app still has location data access.  Do you want it to keep it?  Which I think is really a nifty feature.  It's like, oh, yeah, forgot to turn that off.  And then, you know, you have the opportunity to.  So bravo on the Android front for being a little more privacy-forward.



Okay, Leo.  Here's one that really surprised me.  I first encountered the headline, I thought, wow, how cool.  This really is a new Microsoft.  The headline, which is pretty much repeated  by all the tech press, is along the lines of "Microsoft releases Application Guard extensions for Chrome and Firefox."  And it's like, what?



So the coverage across the tech press begins with things like "Microsoft has released browser extensions, one for Chrome and another for Firefox, which port the Windows Defender Application Guard technology from Edge to Chrome and Firefox."  And I'm like, wow.  So the articles typically say:  "The extensions only work for Chrome and Firefox running on current Windows Insider builds [okay], but are expected to work with the upcoming Windows 10 stable release, 19H1, scheduled for release later this spring."  So, yeah, Windows 10 will get this.



"The Windows Defender Application Guard technology is a relatively new Windows Defender security feature that until now has only been available to Edge users."  Right.  So I'm thinking, wow, like Microsoft is going to fix Chrome and Firefox browsers to protect their users on Windows 10 from malicious web content.  How amazing is that?  But then, reading into this a bit further, you encounter:  "When using Chrome or Firefox, administrators can establish a list of trusted websites and local resources that the user can access within those browsers."  Wait, what?



"But when a user of Chrome or Firefox attempts to visit any URL not on the trusted list, Windows Defender Application Guard comes into effect by launching a sandboxed session of Edge - within a Hyper-V-enabled container - where the untrusted website will be loaded into a safe environment within the Edge browser and isolated from the rest of the underlying operating system."  I had to read that several times to be sure I wasn't missing something.



Microsoft's own blog posting last Friday, March 15th, 2019, 2:02 p.m., was titled:  "Announcing Windows 10 Insider Preview Build 18358."  And it has a number of sections.  Scrolling down you get to the section about this titled "Windows Defender Application Guard as browser extensions in Google Chrome and Mozilla Firefox."  And then, again, no wonder the press was confused.  They said:  "To extend our container technology to other browsers and provide customers with a comprehensive solution to isolate potential browser-based attacks, we have designed and developed Windows Defender Application Guard extensions for Google Chrome and Mozilla Firefox.  This way, any potential attack won't be able to reach and grab the user's data, or plant malware on any local operating system.



"Here's how it works," says Microsoft.  "The extensions for Google Chrome and Mozilla Firefox automatically redirect untrusted navigations to Windows Defender Application Guard for Microsoft Edge.  The extension relies on a native application that we've built to support the communication between the browser and the device's Application Guard settings."



Okay, now in other words - I'm breaking from Microsoft's announcement.  In other words, Windows Defender Application Guard knows that the world is a scary place.  And should you attempt to venture out there with Chrome or Firefox, this nifty new web browser extension will jump in to protect you from your wayward wanderings, taking you instead into Microsoft's proprietary Hyper-V VM where, from the safety of their Edge browser, you can peer out into the gloom which is the Internet.  How very thoughtful of Microsoft.  It's like, okay.  What?



Then they said:  "When users navigate to a site, the extension checks the URL against a list of trusted sites defined by enterprise administrators.  If the site is determined to be untrusted, the user is redirected to an isolated Microsoft web session."  Oh, Leo, I tried to get us a screenshot of it, but it was small and blurry, and I couldn't find a full-size one.  When it first comes up it says, "Why am I here?"



LEO:  Wow.  If it can answer that, I want it.



STEVE:  It's like, what happened?  I was using Firefox or Chrome, and now I'm in Edge.



LEO:  Now you're in Edge.  This is not nice.



STEVE:  Oh, my goodness.



LEO:  I mean, maybe their motivations are pure.  But I'm not using Edge for a reason.



STEVE:  Right, exactly.  And, wow.  And then they said:  "In the isolated Microsoft Edge session, the user can freely navigate to any site that has not been explicitly defined as trusted by their organization without any risk to the rest of system."  Oh, and then, with our upcoming, but not quite ready yet, dynamic switching capability, if the user tries to go to a trusted site while in an isolated Microsoft Edge session, the user is taken back to the default browser.  So eventually they'll put you back to where you came from, Firefox or Chrome, if you happen to go back to a URL that we've decided you should trust.  How thoughtful.  Unbelievable.  You know, if they would just spend their time fixing Windows bugs.  Why not just fix Windows instead of, like, all of these shenanigans?



LEO:  Have Mozilla or Google responded?



STEVE:  No, but I looked.  The extensions are real.



LEO:  Yeah, they're on the store, I mean, they could knock them off if they wanted to.



STEVE:  Yeah, it's a Google Chrome extension.  Oh, but the reviews are pretty funny because I read a couple.  One guy said, "Okay, this installs a button that takes you to Edge in protected mode.  I guess that's good.  Except it's monitoring in the meantime everywhere you go to see if it's good or not, and it's accumulating data which you are unable to disable.  So no, thank you."  Why would anybody put this, install this in their Firefox or Chrome?



LEO:  Well, because you might get the impression it's somehow safer and sandboxing you and stuff like that.



STEVE:  Well, yeah, because you don't get to use...



LEO:  Is it?



STEVE:  Yeah, because it won't let you use Firefox or Chrome.



LEO:  That's better.  Only from Microsoft's point of view, but okay.



STEVE:  Wow.  Wow.  So just a real quick follow-up on our discussion of the Spoiler vulnerability.  AMD stepped up and confirmed that, as far as they understand it, their architecture is not subject to this Spoiler-based amplification of the strength of attacks like Rowhammer.



They said:  "We are aware of the report of a new security exploit called Spoiler which can gain access to partial address information during load operations.  We believe that our products are not susceptible to this issue because of our unique processor architecture.  The Spoiler exploit can gain access to partial address information above address bit 11 during load operations.  We believe that our products are not susceptible to this issue because AMD processors do not use partial address matches above address bit 11 when resolving load conflicts."



So essentially that reduces the issue that Intel has on every one of their core processors from Core 1 on, to the essence of what Spoiler is about.  So I did want to just confirm, for anybody using AMD stuff that, indeed, just not going to be a concern there.



Lawrence in Philadelphia, his subject was "Making Windows 10 Usable."  He said:  "Steve, I've heard you mention a few times that, in making peace with Windows 10, you've done a bunch of things to strip out the junk and make it into a usable operating system."  And I just will say, boy, you install a new version of Windows 10 Pro, it's unbelievable.  It's truly unbelievable.  Anyway.



He says:  "I would love some guidance on how to make it actually functional.  Would you share some instructions on the show or provide some sort of checklist of what steps need to be taken?  I dream of some tool like your Never10 program that simply fixes everything with one click.  But I suspect that this is not so easy, and I know you're tied up with new versions of SpinRite and SQRL.  Thanks for all your hard work and dedication."



So I just wanted to mention this because I've received many versions of this, since I've several times mentioned this idea of having wrestled 10 down to the ground.  Maybe that's a good name, Wrestle10.  Anyway.  Or Tame10.  Anyway, Never10 is at three million downloads now.  So it's been a great success.  And I don't know why anybody's downloading it now, I mean, it's still downloaded, I think just because it's there.  I'm certainly not going to do it anytime soon.  I could foresee a point in time where, if I've got a version of SpinRite that's out for testing, and I'm waiting to get feedback from a group of people who are playing with it to see what they think, it wouldn't take that long.  But I'm not going to do it until then.



And really I didn't intend to do Never10, except that there were a couple not-well-written attempts at it; and I thought, okay, I just, you know, this is too important not to do.  And since Windows 10 is apparently going to be with us forever, as it continues to morph, and Microsoft shows no sign of de-cartoonizing it, then I think something like that has to happen.  And I just noted, while I was checking to see what the download count was for Never10, and I noticed that still in first place is GRC's DNS Benchmark.  It's at 4.7 million downloads and getting more than 2,000 a day.  I mean, tell you, I mean, people love benchmarks.  You give people a good benchmark, they just, you know, especially one like this, that allows people to see how stuff is working, and they want their systems to go faster.  So that was a win.



Oh.  And a SpinRite user wrote, at the end of a longer note about something else, he says:  "Please don't put off native USB support to later versions."  So I wanted to explain that.  It's not that I'm going to put it off to later versions.  My intention is to, in every way I can, make up for the fact that it's taken me this long to get back, or will have taken me this long when I do, to get back to SpinRite.  So my plan is, because, for example, I mean, there was a test version I had that people were using, I mean, it wasn't functional, it was just code.  But people were playing with it when I stopped work on it in order to get SQRL done.



And so my plan is, as I have working code, it will be possible for all owners of SpinRite 6 to use their access to the SpinRite delivery, the SpinRite product delivery system to obtain whatever I have at the time.  Which essentially means stuff that won't hurt you, that I'm sure won't hurt you, but I haven't finished yet with.  But works.  But does stuff.  So, and I will at some point declare benchmarks.  And so the point one benchmark will be SpinRite screamingly fast, running on, like, someone said he had a 14TB drive.  And it's like, whoa.  Okay, well, no, so that would take 28 hours as I benchmarked it last with this next version of SpinRite that is the bare metal 32MB buffer AHCI hardware interfacing screaming blizzard version.  I want to put a point one on that and make it official.



But my point is that I'm doing that because that's the most important thing for me to do.  USB support will then immediately start to happen, but I can only do one thing at a time.  And so my intention is to be a bit of a production line and put point two out, which will be USB, and then point three, which will be whatever else it needs.  And so the idea is I'm not going to, rather than releasing nothing until I am done with 6, I'm going to serialize them and even make pre-point release beta code available to people who want to play with it.  And I know our listeners.  I get feedback from them all the time.  I know people want it, like, the first moment there's something that they can use.  So I'm going to accommodate that.  So that's the plan.



Open source eVoting:  This was a really nice lengthy article in Motherboard, and so I've got the link at the top of my coverage of this for anyone who wants to dig in deeper.  But I'm going to encapsulate this from having edited and excerpted from Motherboard's much longer coverage.  The headline on Motherboard read:  "DARPA Is Building a $10 Million, Open Source, Secure Voting System."  Okay.  Well, there's a lot of problems with that headline.  But I'll explain where they got the headline.  I mean, the people who do the headline skim the article, I guess, and come up with something that people will want to read.



The system will be fully open source and designed with newly developed secure hardware to make the system not only impervious to certain kinds of hacking, but also allow voters - and this stuff is so cool, I'll get to the details of this in a minute - to verify that their votes were recorded accurately.  We're going to see some really cool new crypto involved, as you'll see.



"For years," Motherboard writes, "security professionals and election integrity activists have been pushing voting machine vendors to build more secure and verifiable election systems."  And I would say open.  It's just, it's nuts that Diebold could possibly sell to anyone a box under the "just trust us."  I mean, okay, what's that, JTU?  That is the exact reverse of TNO.  Just Trust Us versus Trust No One.  That's just - how did that happen?  I don't know how that happened.  But it's the world we're in right now.



So, Motherboard says, "so the voters and candidates can be assured election outcomes have not been manipulated.  Now," Motherboard writes, "thanks to a new $10 million contract" - so that's where this $10 million number came from - "DARPA has launched to design and build a secure voting system that hopes to be impervious to hacking."  And I would argue this has the chance to happen, due to the way they're doing this.  "This first of its kind system will be designed by an Oregon-based firm called Galois" - G-A-L-O-I-S, which is a math term used in crypto - "a longtime government contractor with experience in designing secure and verifiable systems.  The system will use fully open source voting software instead of the closed proprietary software currently used in the vast majority of voting machines, which no one outside of voting machine testing labs can examine."



And we have seen what a piss-poor job they've done because voting machines, when they are available, and of course the reaction as we've talked about it of the voting machine companies is to buy them all off of eBay so that they can't be found for hacker competitions during Def Con.  So it's like, no, don't look, we're not going to let you look at our machines.  And whenever anybody has, they've cut into them like Swiss cheese.



So:  "More importantly, these next-generation machines will be built on open source hardware made from secure designs and techniques developed over the last year as part of a special program at DARPA.  The voting system will also be designed to create fully verifiable and transparent results" - that is, output - "that the voters don't have to blindly trust that the machines and election officials delivered correct results."  Get to more of that in a second.



"But DARPA and Galois won't be asking people to blindly trust that their voting systems are secure, as voting machine vendors currently do.  Instead, they'll be publishing source code for the software online and bringing prototypes of the systems to the Def Con Voting Village this summer and next, so that hackers and researchers will be able to freely examine the systems themselves and conduct penetration tests to gauge their security."  So they will have working systems and the source code to read through and try to find problems.  "They'll also be working with a number of" - "they" meaning DARPA and Galois - "working with a number of university teams over the next year to have them examine the systems in formal test environments."



Linton Salmon is the program director for DARPA's Microsystems Technology Office which is overseeing the project.  In a phone call he told Motherboard:  "Def Con is great, but hackers there will not give us as much technical detail as we want about problems they find in the systems.  Universities will give us all the information, but we don't have as many people or as high visibility when we do it with universities."  So they're going to use both is his point.



The systems Galois designs won't be available themselves directly for sale, but the prototypes it creates will be available for existing voting machine vendors or others to freely adopt and customize without costly licensing fees or the millions of dollars it would take to research and develop a secure system from scratch.  So they're creating exactly what we want, a secure hardware and software open standard which then existing voting machine manufacturers will be able to adopt for free to turn into commercial machines.  And then states will be able to say to Diebold, we'll buy your machine as long as our purchasers can verify, and you certify, it is compliant with the gold standard, which will then be available and testable and verifiable.



So that's where we're going to be going.  And it's perfect.  Linton said:  "We will not have a voting machine that we can deploy.  That's not what we do.  We will show a methodology that could be used by others to build a voting system that's completely secure."



Joe Kiniry [K-I-N-I-R-Y] is the principal scientist at Galois, who's leading the project at his company.  He said that Galois will design two basic voting machine types.  So here's some juicy details.  The first will be a ballot-marking device that uses a touchscreen for voters to make their selections.  The system won't tabulate votes.  Instead, it will print out a paper ballot marked with the voter's choices, so voters can review them before depositing them into an optical scan machine that tabulates the votes.  Galois will bring this system to Def Con this year.



Many current ballot-marking systems on the market today have been criticized by security professionals because they print bar codes on the ballot that the scanner can read instead of the human-readable portion voters review.  Someone could subvert the bar code to say one thing, while the human-readable portion says something else.  Kiniry said they're aiming to design their system without barcodes.  So the point being, what the scanner scans and tabulates from is human readable so that there's no question.  The optical scan system - okay.  So that's the thing that creates the thing to be scanned.



Part two, the optical-scan system, will print a receipt with a cryptographic representation of the voter's choices.  After the election, the cryptographic values for all ballots will be published on a website, where voters can individually verify that their ballot and votes are among those present and counted.  Kiniry said:  "That receipt will not permit you to prove anything about how you voted, but it permits you to prove that the system accurately captured your intent and that your vote is in the final tally."



Members of the public will be able to use the cryptographic values to independently tally the votes to verify the election results so that tabulating the votes isn't a closed process solely in the hands of election officials.  In other words, everything gets made public and published.  Kiniry said:  "Any organization interested in verifying the election results that hires a competent software engineer can write their own tabulator.  We fully expect that Common Cause, League of Women Voters, and the political parties will all have their own tabulators and verifiers."  The second system Galois plans to build is the optical scan system that reads paper ballots marked by voters by hand.  They'll bring that system to Def Con next year.



So there is a bunch of, I mean, all of that sounds right.  Some people clearly thought about this.  They're like, how to be completely open, how to allow an individual voter the satisfaction of knowing that they're using an academically scrutinized open system where after the fact they're able to use a website to cryptographically verify that their individual vote was captured and is part of the final tally that this tabulator system uses.  I think they nailed it.



And so it looks like a couple years from because they're going to have this first system at this summer and then the paper ballot tabulator next summer.  I mean, I hope that the voting machine vendors read the handwriting on the walls, recognizing that this is out of their hands.  This, you know, they had their day.  And then to immediately get up to speed on this new technology so they can be first to market, early to market, with the system as it gets finalized because this is a beautiful solution.  Bravo.



LEO:  Nice, yeah, awesome.



STEVE:  Yeah.  Very, very cool.



LEO:  And I guess one more little story that's one of those object lessons.  Did you see this?



STEVE:  Yes.



LEO:  MySpace.



STEVE:  How big was their loss?



LEO:  They lost everything that had been uploaded to MySpace from the years 2003 to 2015 because the hard drive failed, and they didn't have a backup, in short.  Significant.



STEVE:  It really is out in space now.



LEO:  Yeah.  MySpace is in space.



STEVE:  Wow.



LEO:  It's just it's hard to believe, since best practices are well known, and frankly most of our listeners perform it every day at home, that MySpace could have lost it all.  But they did.  And I hope that if you uploaded your music to MySpace, you kept a copy for yourself.



STEVE:  I have Drobos on multiple sites and their dynamic real-time change file synchronization running.  I mean, it's like, nothing could - I mean, it's like this - it's not hard.



LEO:  It's not hard.



STEVE:  It's not expensive, no.



LEO:  It's not hard.



STEVE:  It just takes doing it.  Whoa.  Yeah, I caught the headline.  I didn't have a chance to dig in.  So, wow.



LEO:  Unbelievable.  I'm not sure how much data it was.  I seem to remember it was something like 48TB.  But I'm not - I don't really know for sure.  But, doh.



STEVE:  You pressed what button?



LEO:  They said for a long time they've been saying, uh, we've been working on it.  But eventually they admitted we just - we lost it.  It's all gone.  It's gone.



STEVE:  And what does that mean for them as a service?  I mean, that means they have the last four years.



LEO:  Yeah.



STEVE:  The most recent is probably what makes the most - is the most valuable.  But yikes.  If nothing else, it's an embarrassment.  And you've got to wonder what that means about their IT.



LEO:  If I could show you this email, I would.  It says - it's from the Data Privacy Officer at MySpace.  "Hello, Austin.  Yes, this is true.  Due to a server migration, files were corrupted and unable to be transferred over to our updated site.  There is no way to recover the lost data.  Thanks, MySpace."  Oh, I shouldn't laugh.  That's just kind of the end of the line for MySpace.  But, you know, today MySpace, tomorrow Facebook or Google or anywhere.  Your data is your data.  Don't let anybody else keep track of it.



STEVE:  Yup.



LEO:  You, Steve Gibson, have done many, many versions of this show backed up in many, many places.  It shall never be lost.  It shall continue into the heat death of the Sun.  And then after that we can't make any promises.  All the shows, they are at Security Now!'s website, GRC.com.  That's Steve's website, the Gibson Research Corporation.  He has audio, and he has transcriptions.  So if you really wanted to be safe, what you would do is download all 706 episodes...



STEVE:  Oh, people do.



LEO:  ...and print out the transcriptions.



STEVE:  Yup.



LEO:  Put them in a vault.



STEVE:  Yup.



LEO:  No, we've got - I think that's the beauty of this is that there are many people have every show, so we don't have to worry about that.  But it is a good place to get it.  While you're there, pick up a copy of SpinRite, the world's best hard drive maintenance and recovery utility, and read up on all the other stuff Steve's up to.  He is a polymath, and there is stuff in every possible subject, fascinating reading.  It's one of those websites you start and then eight hours later you realize, oh.



STEVE:  Where did the day go?



LEO:  Where did the day go?



STEVE:  What did I get done?



LEO:  What happened here?  GRC.com.  We have all 706 episodes, actually 709, I don't know what happened, but there was a mistake at the factory.  Patrick's telling me there's 709.  Don't tell Steve that.  He'll end this three episodes earlier.  We have seven, maybe 600.  I don't know.  We have a few.  Couple of dozen, anyway, at TWiT.tv/sn.



STEVE:  With a short serial number.



LEO:  Yes, with a number.  Zero through 12.



STEVE:  Yes, with a short serial number.  A 63-bit serial number.



LEO:  Yeah, that's right.  Not insecure, just not numbered properly.  Yes, that's true.  Maybe go to GRC.com.  He's got a 128-bit serial number.  And you know how important that is.  You can watch us do the show live, 1:30 Pacific, 4:30 Eastern, 20:30 UTC, every Tuesday at TWiT.tv/live.  Watch or listen.  We have live audio and video streams.  If you do that, irc.twit.tv is the place to chat along with Steve.  And, let's see, I guess that's about it.  Nothing more to say except thanks for joining us.  Subscribe to the show.  Get every episode.  And we'll see you next time on Security Now!.



STEVE:  Thanks, Leo.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#707

DATE:		March 26, 2019

TITLE:		Tesla, Pwned

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-707.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week on Security Now! we have the return of "Clippy," Microsoft's much-loathed dancing paperclip; operation "ShadowHammer," which reports say compromised ASUS (but did it?); the ransomware attack on Norsk Hydro aluminum; the surprise renaming of Windows Defender; a severe bug revealed in the most popular PDF-generating PHP library; an early look at Microsoft's forthcoming Chromium-based web browser; hope for preventing caller ID spoofing; a needed update for users of PuTTY; Mozilla's decision to conditionally rely upon Windows' root store; Microsoft to offer virtual Windows 7 and 10 desktops through Azure; details of the Windows 7 End of Life warning dialog; then a bit of Sci-Fi, SQRL and SpinRite news, followed by our look at the results of the much anticipated Mid-March Vancouver Pwn2Own competition - one of the results of which our episode title gives away!



SHOW TEASE:  It's time for Security Now!.  Birthday boy Steve Gibson is here.  He is - we're going to call him "Commodore 64" today because it's his 64th birthday, and he's got a lot to talk about including the malware, the big Norwegian aluminum company.  You won't believe the sign they put in the door.  And a play-by-play of Pwn2Own, all three days, coming up on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 707, recorded Tuesday, March 26th, 2019:  Tesla, Pwned.



It's time for Security Now!, the show where we cover your security and privacy online with the man and the plan, Panama, Steve Gibson.  Actually, ever wear a Panama hat?  You'd look good in a...



STEVE GIBSON:  No.



LEO:  You'd look great in a Panama hat.



STEVE:  Mine is sort of the French beret.  I'm sure you remember me in that.



LEO:  You look good in a beret.  I do remember that.



STEVE:  I used to wear that all the time.



LEO:  That was your hat.



STEVE:  I just kind of fell out of the habit somehow.



LEO:  It was no brim.  You need one with a brim.



STEVE:  Well, no.  That's true, it does not keep the rain out of your eyes nearly as well as a big Panama hat.



LEO:  So let me add security, privacy, and hat couture.



STEVE:  That's true.



LEO:  What's coming up this week?



STEVE:  So we have, for Episode 707, the title of the podcast is "Tesla, Pwned."



LEO:  Oh, yeah.  I saw that.



STEVE:  But it wasn't a bad pwnage.  It was about as mild a pwnage as it could be.  On the other hand, they did drive off with a Tesla Model 3.



LEO:  Yeah, yeah.



STEVE:  So they're pretty happy.  This, of course, was last week's three-day Pwn2Own.  And it's called Pwn2Own because, if you pwn it, you own it.  And they pwned it, and they're now driving it.



LEO:  And a little bit of cash also.  This is Fluoroacetate.  They're at it again.



STEVE:  They are.  And what impressed me the most, I have to say, is that the prize money that they won, relative to what Zerodium would have paid, because one of the things they did would have qualified for the half a million dollar recently increased - and we talked about this a couple weeks ago - zero-day in VMware, where they were able to execute code on the host outside of the VM after just browsing to a web page, which is just like, oh, you know, that's like - that's the golden goose from Zerodium's standpoint.  But they didn't sell it to Zerodium.



LEO:  Good.



STEVE:  I'm very impressed.



LEO:  It's a proof-of-concept for Pwn2Own, really, that shows it's a benefit.



STEVE:  Yeah.  So the problem is normally our show notes are 14 pages, and we run out of time.  We have 20 pages today.  I mean, there is a lot to talk about.



LEO:  I'll shut up.



STEVE:  We've got the return - and, okay.  Now, arguably there's a little bit of padding here because I've had some fun, too.  I could not pass up the fact that we have the return of Clippy, Microsoft's much-loathed dancing paperclip, which actually is our Picture of the Week that came to me when Hawaii had their problem with that false alert.  We also have Operation ShadowHammer, which reports say compromised ASUS.  But I may be the only one in the industry who's a little skeptical.  We'll talk about that.



We have the ransomware attack that you brought up, it was occurring during last week's podcast against Norsk Hydro aluminum, which took down across 40 different plants located globally.  It was a ransomware attack.  We have the surprise renaming of Windows Defender for a purpose, a severe bug revealed in a most popular PDF-generating PHP library, an early look at Microsoft's forthcoming Chromium-based Edge browser, hope for preventing caller ID spoofing, a need to update for users of PuTTY, which is a very popular freeware SSH client.



We've got Mozilla's decision to conditionally rely on Windows root store rather than their own.  Microsoft will be offering virtual Windows 7 and 10 desktops through Azure and what that means.  Also I heard you and Mary Jo and Paul talking about this last week, sort of some wondering about what the Windows 7 end of life, end of service life, end of update life warning dialog would look like.  We now know.



I've got a tiny bit of sci-fi, some SQRL and SpinRite news, and then we're going to take a look at the much-anticipated mid-March Vancouver Pwn2Own competition, three days.  And of course the title of our podcast gives away what happened during the third automotive pwning day.  But I think another great podcast for our listeners.



LEO:  And let me know if you need the sportscaster voice because I'm ready.



STEVE:  Howard Cosell.



LEO:  I could do the play-by-play if you should need it.



STEVE:  Okay.



LEO:  Clippy's back.  I missed little Clippy.



STEVE:  Well, okay.  I was going to say you're alone in that, but I don't think you are.



LEO:  He was cute.  I just don't want him helping me with grocery lists, you know.



STEVE:  Yes.  Anyway, the Picture of the Week was sent to me back when Hawaii had that bogus ballistic missile alert.  And anyway, so this is the Clippy from yesteryear.  It was introduced with Office 97, I think it was, and it was taken out of commission with Office XP.



LEO:  Whew.



STEVE:  Yeah.  Anyway, much maligned and - anyway.  So anyway, our Picture of the Week is Clippy saying, "It looks like you're sending out a ballistic missile alert."



LEO:  Would you like some help?



STEVE:  Would you like some help?  And that's what it used to do.  It used to be, like, watching what you were doing.



LEO:  Yeah.  Annoying.



STEVE:  And it would jump in and, oh, my god, so annoying.  And, I mean, it was just, I don't know, I guess it wasn't right for the time.  But it turns out Clippy is coming back.  Well, at least for some places.  In a blog posting made on April 11th, 2001 - okay, so back 17 years ago, no, 18 years ago nearly.  Microsoft titled it "Farewell Clippy:  What's Happening to the Infamous Office Assistant."  And their title said "in Office XP."  But what they meant was "with Office XP," that is to say, it was introduced in Office 97, and they are saying farewell to it.



So what they wrote was:  "Whether you love him or hate him" - and actually the vote was very heavily weighted toward the latter - "say farewell to Clippy automatically popping up on your screen.  Clippy is a little paperclip with the soulful eyes and the Groucho eyebrows," they wrote, "the electronic ham who politely offers hints for using Microsoft Office software."



Okay, and I love the way they spun this.  They said:  "But after four years onscreen, Clippy will lose his starring role when Microsoft Office XP debuts on May 1st.  Clippy, the Office Assistant introduced in Office 97, has been demoted in Office XP."  And I did enjoy this pun:  "The wiry little assistant..."



LEO:  Because he's made out of a paper clip, yeah.



STEVE:  Uh-huh, "... is turned off by default" - I didn't even know it was still there, so I'm glad it was gone.



LEO:  You could turn it on?  Who would ever want to do that?



STEVE:  Exactly, "...turned off by default in Office XP.  But diehard supporters can turn Clippy back on if they miss him."



LEO:  Aw.



STEVE:  And here's the spin.  Lisa Gurry, a Microsoft product manager, explained:  "Office XP is so easy to use..."



LEO:  Oh, please.  The spin machine.



STEVE:  I know, "...that Clippy is no longer necessary..."



LEO:  No, it's easy.



STEVE : "...or useful."  That's right.  We've finally figured out how to make our UI work, so we don't need a paperclip to come springing out and helping you.  Anyway, she said:  "With new features like smart tags" - whatever those are - "and task panes" - whatever those are - "Office XP enables people to get more out of the product than ever before."



LEO:  Thank god.



STEVE:  Oh, whew.  "These new simplicity and ease-of-use improvements really make Clippy obsolete," she said.  And then, finally:  "He's quite down in the dumps," Gurry joked.  "He has even started his own campaign to try to get his old job back or find a new one."



Now, surprisingly, that was then.  A report in USA Today, well, USA Today, not today, back in 2002 stated that Microsoft banked on its customers' contempt - this is actually - USA Today said this back then.  "Microsoft banked on its customers' contempt of Clippy to promote Office XP."



LEO:  There's a selling point.  No more Clippy.



STEVE:  That's right.  No more of that - anyway.



LEO:  That's funny.



STEVE:  "On Thursday," they wrote, "On Thursday, Microsoft is scheduled to unveil the last installment in a nontraditional advertising campaign that aims to sell the newest version of Office, called XP, by encouraging customers' hatred of Clippy."  Unbelievable.



So here we are now, today, finally today, 18 years later, and wouldn't you know it, Clippy's lobbying to return to the limelight appears to be paying off.  Clippy is about to make a not-long-awaited comeback for Microsoft's Teams app.  The effort is open source and on GitHub, so the animations are all publicly available.  And I have to confess, Leo, that Clippy has become such a meme from the past that, had I him available to embellish the occasional iMessage on my iOS device, that might be kind of funny.  Fun.  I mean, and I put down here at the bottom a snap of one of them.  We have the beer-drinking Clippy because it's 2019 now, so we can do that.



LEO:  They let him drink beer?



STEVE:  And he's also got a coffee mug.  There's one with a coffee mug.  There's one where he's holding like a Starbucks-style paper with the little heat guard slip-on dealy-do.  Anyway, there's a bunch of them.  They're animated, and I'm sure that someone is going to grab them off of GitHub and sprinkle them around.  So I've never been much of a big fan of the emojis and things, but if we had this little bank of animated paperclips, almost because it's a dated meme I think it would be kind of fun.  So I'll bet it happens.



LEO:  Nice.



STEVE:  Okay.  So now here's - this is really odd.  It's called Operation ShadowHammer.  And first I'm going to share Kaspersky's post about the incident.  Then I'll explain what puzzles me so much about this.  So Kaspersky wrote:  "Earlier today" - and this just happened - "Motherboard published a story by Kim Zetter on Operation ShadowHammer."  And I should mention that Motherboard story is based on Kaspersky's research, so they're sort of self-referential here.  By Kim Zetter on Operation ShadowHammer, "a newly discovered supply chain attack that leveraged ASUS Live Update software."



And Motherboard's headline read:  "Hackers Hijacked ASUS Software Updates to Install Backdoors on Thousands of Computers."  And their subtitle says:  "The Taiwan-based tech giant ASUS is believed to have pushed the malware to hundreds of thousands of customers through its trusted automatic software update tool after attackers compromised the company's server and used it to push the malware to machines."  And for anyone who's interested, I have the link to the whole Motherboard article in the show notes.



So Kaspersky says:  "While the investigation is still in progress, and full results and technical paper will be published during SAS 2019 conference in Singapore" - which I think is only like 12 days away, so a week and a half we'll know more.  They said:  "We would like to share some important details" - this is Kaspersky speaking - "about the attack.  In January 2019" - so two months ago, beginning of this year, they write - "we discovered a sophisticated supply chain attack involving the ASUS Live Update Utility.  The attack took place" - okay, and not really the utility.  That's the thing that reaches back to ASUS, right, to check for any updates.



Anyway, "The attack took place between" - get this - "June and November."  Now, not meaning a single event of attack, but meaning for five months this was ongoing.  So the attack took place for the span between June and November 2018.  "And according to our telemetry, it affected a large number of users.  ASUS Live Update," they write, "is a utility that is pre-installed on most ASUS computers and is used to automatically update certain components such as BIOS, UEFI, drivers, and applications."  And of course we've talked about this a lot.  Lenovo has this.  Famously Microsoft invented this.  I remember how much at the time it was like, wait a minute, you're going to update my computer without my involvement?  That was a thing once.  Now it's just like, okay, please bring it on.



"According to Gartner, ASUS," writes Kaspersky, "is the world's fifth largest PC vendor by 2017 unit sales.  This makes it an extremely attractive target for APT [Advanced Persistent Threat] groups that might want to take advantage of their user base."  Okay, but let me tell you why this doesn't track.  We'll get there.  Kaspersky says:  "Based on our statistics, over 57,000 Kaspersky users have downloaded and installed the backdoored version of ASUS Live Update at some point in time."



LEO:  Oh, so they know this because Kaspersky saw it.



STEVE:  Yes.  Their own Kaspersky instrumentation on those users' machines.  They said:  "We are not able to calculate the total count of affected users based only on our data; however, we estimate that the real scale of the problem is much bigger" - of course it would be because they don't have their stuff in every ASUS machine - "and is possibly affecting over a million ASUS users worldwide.  The goal of the attack was to surgically target an unknown pool of users" - and, okay, listen carefully to this.



LEO:  Guess who that might be.  Okay.



STEVE:  Well, "which were identified by their network adapters' MAC addresses."  Which is really screwball.



LEO:  That's by manufacturer, then; right?



STEVE:  Well, we know it's ASUS because ASUS is the infection channel.  But they're selecting targets based on their MAC address.  Okay.  So anyway, Kaspersky says...



LEO:  All a MAC address tells you is who made that device.



STEVE:  Well, no.  The MAC address is 48 bits.



LEO:  No, there's extra stuff, but the first part of it is manufacturer; right?



STEVE:  Correct.  Right.  So Kaspersky says:  "To achieve this, the attackers had hardcoded a list of MAC addresses in the trojanized samples, and this list was used to identify the actual intended targets of this massive operation."



LEO:  Hmm.



STEVE:  I know.  It gets weird, Leo.  "We were able to extract more than 600 unique MAC addresses from over 200 samples" - which Kaspersky got from their own customers - "used in this attack."  They said:  "Of course, there might be other samples out there with different MAC addresses in their list.  We believe this" - and this doesn't make sense to me, but we'll get there in a second.  "We believe this to be a very sophisticated supply chain attack, which matches or even surpasses the ShadowPad and CCleaner incidents in complexity and technique.  The reason that it stayed undetected for so long is partly due to the fact that the trojanized updaters were signed with legitimate ASUS certificates, AsusTek Computer Inc."  And Leo, not once but twice.  We'll get there, too, in a second.



"The malicious updaters were hosted on the official liveupdate01s.asus.com and liveupdate01.asus.com ASUS update servers.  Although precise attribution," they say, "is not available at the moment, certain evidence we have collected allows us to link this behavior to the ShadowPad incident from 2017.  The actor behind the ShadowPad incident has been publicly identified by Microsoft in court documents as BARIUM.  BARIUM is an APT actor known to be using the Winnti backdoor.  Recently, our colleagues from ESET wrote about another supply chain attack in which BARIUM was also involved, that we believe is connected to this case, as well.



"A victim distribution by country for the compromised ASUS Live Updater looks as follows."  And I've got a picture of the graph in the show notes just because Kaspersky provided it.  But remember this is their view into victims, and it's going to be massively skewed by their customer base.  And they acknowledge that.  They said:  "It should be noted that the numbers are also highly influenced by the distribution of Kaspersky users around the world."



LEO:  Mostly in Russia.



STEVE:  Yes.  "In principle, the distribution of victims should match the distribution of ASUS users around the world."  They said:  "We've also created a tool which can be run to determine if your computer has been one of the surgically selected targets of this attack.  To check this, it compares MAC addresses of all adapters to a list of predefined values hardcoded in the malware, and alerts if a match was found."



And I have a link in the show notes for any ASUS computer user among our listeners who is listening to this thinking, ugh.  It's https://kas.pr/shadowhammer.  That downloads a 50k ShadowHammerCheck.zip, which then checks against the hardcoded list.  They say you may also check MAC addresses online.  And there it's https://shadowhammer.kaspersky.com.  "If you discover that you have been targeted by this operation, please email us at" - and then they have their email address, shadowhammer@kaspersky.com.



So things feel fishy to me about this.  First of all, ASUS official servers were being used to supply the initial malware.  And the malware was signed by legitimate ASUS certificates.  And though Kaspersky's brief summary didn't mention it, other coverage noted that ASUS was being uncooperative in the extreme about this, denying that anything had happened at all, 100% stonewalling.  What I find so puzzling and curious is that the malware delivered by ASUS's own servers from ASUS and signed by ASUS used the victim's MAC addresses to identify individual specific ASUS machines.  And what's most troubling is that no one but ASUS, the manufacturer of those machines, would reliably know what MAC addresses specific machines have.



As we know, MAC addresses, while not highly secret, neither are they widespread.  A machine's MAC address is often printed on the label outside the box, and on the label underneath the machine, for example, in the case of a laptop, or on the label on the machine.  But the MAC address is inherently highly local because it's not transmitted over the Internet.  The MAC address, as we have often described, provides local Ethernet network hardware addressing for use within a single Ethernet subnet.



So, for example, any IP, Internet Packet router, serves as an intelligent link between separate Ethernet networks, with a different network on each of its interfaces.  Unless a router is bridging two networks at the Ethernet layer, the MAC address from one network is removed, and its contained IP packet is routed to another interface where it is reencapsulated with an Ethernet packet for that other network containing its source and destination MAC addresses.  So my point is how would some random external malicious agency obtain the physical hardware Ethernet MAC addresses that are only useful for ASUS customers because that's the source of this infection, across a large collection of specific ASUS machines.



Okay.  So if we're brainstorming, one possibility is that these were wireless laptops.  We've been talking about MAC addresses recently, and MAC address spoofing on WiFi.  So if they were wireless laptops, they would have been promiscuously broadcasting their MAC address more or less constantly to every WiFi access point within range.  And as we know, a MAC address is a 48-bit value composed of two halves, a 24-bit registered manufacturer number, and a 24-bit serial number within that manufacturer.



LEO:  So they had the full qualified MAC address, not just the first half.



STEVE:  Right.



LEO:  So they were specifically targeting machines.



STEVE:  A machine.  A machine, yes.



LEO:  Well, that is interesting.



STEVE:  I know.  So the fact of them being ASUS laptops would have been evident from their MAC addresses.  So there's, I mean, if you were stretching, some possibility that the machine's MAC addresses of specific individuals could have somehow been gathered over time.  But it stretches credulity.  If they were wired desktop machines, it's difficult to come up with any theory to explain how some random remote third party could obtain those machines' Ethernet MAC addresses.  And if some agency was close enough to a wired machine to obtain its MAC address, it probably has physical proximity anyway, so it wouldn't need to go this weird circuitous route to get its malware into this ASUS machine.



Occam's Razor suggests that when confronted with a lack of definitive evidence, the simplest explanation is likely to be the best.  And distressing as this is, this suggests that the entire thing was likely a covert and deliberate campaign, if not on the part of all of ASUS, then an insider action within ASUS.  Only ASUS has the certs to sign their update downloads.  From Motherboard's reporting, Motherboard said:  "The attackers used two different ASUS digital certificates to sign their malware.  The first expired in mid-2018, so the attackers then switched to a second newer legitimate ASUS certificate to resign their malware after this."  So what sort of security are we to believe exists at ASUS if they were not a willing or begrudging collaborator?



LEO:  Let me provide a scenario.



STEVE:  Okay.  But let me finish.  One more second.



LEO:  Finish, yeah, yeah.



STEVE:  So the attackers first signed their malware with ASUS's protected, guarded, super-secret code-signing certificate.  And those attackers placed that ASUS-signed malware onto both of ASUS's software update servers, where it stays undetected for five months.  But later, as that first certificate nears expiration, the "attackers," in quotes, obtain ASUS updated newly freshened code-signing certificate, resign the malware with that updated cert, and replace the soon-to-be-expired malware on both of ASUS's software downloaded servers with freshly signed new malware.  That's what we're to believe.  And ASUS had no knowledge of any of this.



And so the least that seems feasible is that a well-placed person on the inside arranged for all of this except for the MAC addresses.  That would be a very different region within this very large company because the MAC addresses would probably be in sales records for those machines, which indicate who owns which machines with which MAC addresses.  So let me just finish quickly.  The fact that the follow-up malicious backdoor payload was later sourced from elsewhere gives ASUS some plausible deniability, and Kaspersky indicated that attribution was unavailable at the moment, plus it's very easy to plan a bit of misdirection which would have been in ASUS's interests.



So finally Motherboard wrote:  "Motherboard sent ASUS a list of the claims made by Kaspersky in three separate emails last Thursday, but has not heard back from the company," as of yesterday.  So three separate emails, ASUS doesn't respond.  But Motherboard wrote:  "But the U.S.-based security firm Symantec confirmed the Kaspersky findings on Friday after being asked by Motherboard to see if any of its customers had also received the malicious download.  Symantec is still investigating the matter, but said in a phone call that at least 13,000 computers belonging to Symantec's customers were infected with the malicious software update from ASUS last year."



Liam O'Murchu, director of development for the Security Technology and Response Group at Symantec, was quoted by Motherboard, saying:  "We saw the updates come down from the Live Update ASUS server.  They were trojanized or malicious updates, and they were signed by ASUS."  So I think that's all my coverage.  So I'm just, for our listeners' sake, for five months late last year, ASUS was delivering a malicious download which, if you were one of 600 selected people by the MAC address of your machine, that machine then reached out to a trojan supply server to download additional active malware into your machine.



LEO:  So of the thousands of people who were infected, only those 600 got anything malicious.



STEVE:  Correct.  Yes.



LEO:  Oh, that's interesting.  Sounds targeted.



STEVE:  Yes.  They were infected with - yes.



LEO:  So here's the scenario.  Let me offer a scenario and see if this makes sense.  ASUS has this built in.  This is a standard updating procedure.  They have all the MAC addresses.



STEVE:  Yes.  Yes.



LEO:  They have this built in.  Presumably they sign the software when they deliver it.  It's an update package.  If a bad actor got into ASUS's system and replaced the update package with a malicious package, which then got signed and sent on as if it was a regular update package, all of this would fit.  Except for that one little bit, which is, in order to target 600 machines you'd have to have 600 MAC addresses.



STEVE:  Yes.  And remember that the malware was updated when its first certificate was nearing expiration.



LEO:  So the bad guy's in there.  I mean, we know people, you know, bad guys sit in networks.



STEVE:  Okay.



LEO:  So let's say that ASUS, by the way, a Taiwanese company, not a Mainland China company, but let's say a bad actor from North Korea or some nation-state had access to the ASUS network, got in there, was able to put the malware in there.



STEVE:  Yup.



LEO:  Is it conceivable, I mean, when you hear that something's targeting 600 machines, that sounds like a nation-state going after individuals.  It's not a mass attack; right?



STEVE:  Correct.  Correct.



LEO:  It's a targeted attack.



STEVE:  Because they're estimating in five months a million people, a million ASUS customers checked in, got this...



LEO:  Because it's part of the normal ASUS update process.



STEVE:  Exactly.



LEO:  Yeah.  So couldn't a bad guy who had access to ASUS's network perpetrate this - I mean, it sounds like especially a nation-state bad actor - perpetrate something like this?



STEVE:  Yeah.



LEO:  And I could see why ASUS would be very slow to respond because it looks really bad.



STEVE:  They're, like, going holy crap, what?



LEO:  Yeah.  They're, I mean, the first thing, if I'm the CISO at ASUS, I'm going, guys, let's find this intruder.  Let's figure this out.



STEVE:  And we like ASUS.  I mean, they're a great company.



LEO:  Oh, they're very good.



STEVE:  They make beautiful hardware.



LEO:  I just want to make sure that it doesn't mean necessarily that ASUS is malicious.



STEVE:  Corporate, right.



LEO:  Somebody inside might be bad.  Or in my opinion, I mean, look at all the companies that malicious nation-state hackers have gotten into.



STEVE:  True.



LEO:  And just sit there.



STEVE:  An Advanced Persistent Threat where that person has really deep access to, I mean, like, again, it seems to me that the database where customer to MAC address sales records are is different from the software download/update stuff.



LEO:  Right, should be, yeah.  Were the MAC addresses sequential or just random?  And do we know anything about those 600?



STEVE:  No.  In fact, in 12 days Kaspersky's - that thing is Kaspersky's own SAS.  It's the Security Analyst Summit in Singapore where they're going to present a paper on this.  So in two weeks we should have some more information from them.



LEO:  Very interesting.  I mean, it could have been us.  Could have been the NSA.



STEVE:  Yeah.  But again, it's weird because it's limited to ASUS customers.  I mean, no non-ASUS customer is going to...



LEO:  Well, yeah.  You start with ASUS.  But you get whoever you're in; right?



STEVE:  But there are 600 of them that were of interest to somebody.



LEO:  What if you noticed that, I don't know, the Israeli Embassy had just purchased a large number of ASUS computers.



STEVE:  Yeah?



LEO:  I don't know.  I think that we need to know more, obviously.



STEVE:  And that's a good point, too, because it certainly could be that there is a - if this were targeted, and we don't know targeted by whom, but if - for example, ASUS is a major brand.  There's probably many enterprises who have standardized on ASUS hardware.  That's what they buy.  And so if you know that, like, all of your employees are using ASUS laptops...



LEO:  Exactly,



STEVE:  ...and you can somehow get a list of who's using which laptop by MAC address, then...



LEO:  Or just target the whole organization; you know?



STEVE:  Yeah.



LEO:  I mean, maybe, who knows, it could be Lenovo doing this.  We want to make ASUS customers unhappy.



STEVE:  Yeah, that'll do it.  So speaking of making people unhappy, Leo, we have the Norsk Hydro ransomware attack.  I have a picture in the show notes that someone took of the notice scotch-taped to the door of one of the Norsk Hydro plants.  And it's dated, I think it's 3/19.  So it says:  "Warning:  Cyber Attack Against the Hydro Network.  Please do not connect any devices to the Hydro Network.  Do not turn on any devices connected to the Hydro Network.  Please disconnect any device (Phone/Tablet etc.) from the Hydro Network.  Await new update."  And then it was signed "Security."  And then there's some note handwritten in probably Norwegian next to the one that's in English.  So, and you brought this breaking news to us during last week's podcast.  It was just happening.



LEO:  It's funny, we saw almost identical - something in the door a couple of years ago during a ransomware attack.  Was it Maersk?  I can't remember who it was.  I think it was Maersk, the shipping line.  Same kind of thing in the door.  Don't connect to our network.



STEVE:  Yeah.



LEO:  Wow.



STEVE:  You'll get hurt.



LEO:  Yeah.



STEVE:  So paraphrasing from Ars - everybody had pretty much the same coverage.  Paraphrasing from Ars Technica's coverage, they said:  "One of the world's biggest producers of aluminum has been hit by a serious ransomware attack that shut down its worldwide network, stopped or disrupted plants, and sent IT workers scrambling to return operations to normal.  Norsk Hydro of Norway said the malware first hit computers in the United States on Monday night.  By Tuesday morning, the infection had spread to other parts of the company, which operates in 40 countries, on every continent.



"Company officials responded by isolating plants to prevent further spreading.  Some plants were temporarily stopped, while others, which had to be kept running continuously, were switched to manual mode where possible.  The company's 35,000 employees were instructed to keep computers turned off, but were allowed to use phones and tablets to check email," maybe using WiFi and not using their network.  Or maybe they figured out...



LEO:  LTE, not WiFi.



STEVE:  Yeah, not going to affect them.  Chief Financial Officer Eivind Kallevik said during a press conference Tuesday:  "Let me be clear.  The situation for Norsk Hydro is quite severe.  The entire worldwide network is down, affecting our production as well as our office operations.  We are working hard to contain and solve this situation and to ensure the safety and security of our employees.  Our main priority now is to ensure safe operations and limit the operational and financial impact."



According to Kevin Beaumont, who's an oft-quoted security guy and, Ars said, "tweeting in his capacity as an independent researcher and citing local media reports, the ransomware that infected Norsk Hydro is known as" - and this has been confirmed - "LockerGoga [G-O-G-A]."  He said:  "LockerGoga doesn't rely on the use of network traffic or on domain name system or command-and-control servers, which all allow ransomware to bypass many network defenses."



An independent research group calling itself MalwareHunterTeam pointed to a LockerGoga sample uploaded to VirusTotal from Norway on Tuesday morning.  At the time the malware was first scanned, it was detected by only 17 of the 67 biggest AV products, although detections increased once awareness of the Norsk Hydro infection grew.  The malware had also once been digitally signed by security company Sectigo [S-E-C-T-I-G-O].  So the malware had been digitally signed by the security company Sectigo, but the certificate was revoked at an unknown time.



In the statement, Sectigo Senior Fellow Tim Callan wrote:  "As a policy, Sectigo revokes certificates used in malware attacks and does not issue certificates..."



LEO:  Oh, that's a relief.



STEVE:  "...to known malware" - it's like, oh, thank you.  You know?  But wait, Leo.  It gets better - "to known malware purveyors."  He said:  "We encourage security researchers to report instances of malware employing Sectigo certificates at signedmalwarealert@sectigo.com."  Okay.  Now, when I first read this, I thought to myself, who the heck is Sectigo?  Leo?  Guess who?  Our old friends, Comodo.



LEO:  Oh, lord.



STEVE:  Now operating under a shiny new name.



LEO:  Yes.  The old one got a little tarnished, yeah.



STEVE:  They so thoroughly ruined their previous name.



LEO:  Oh.



STEVE:  So horse of a different color.  Company by a different name.  But yes, Comodo issued the certificate that signed the LockerGoga malware.  So a text file that the attackers included in the malware, it's a longer file with a bunch of nonsense.  But it starts out saying there was a significant flaw in the security system of your company.  "You should be thankful that the flaw was exploited by serious people and not some rookies.  They would have damaged all your data by mistake or for fun.  Your files are encrypted with the strongest military algorithms, RSA-4096 and AES-256.  Without our special decoder ring" - no, not ring, just decoder.  "Without our special decoder, it is impossible to restore that data.  Attempts to restore your data with third-party software such as Photorec, RannohDecryptor, et cetera, will lead to irreversible destruction of your data."



LEO:  Oh, god.



STEVE:  Okay.  So the Norsk Hydro CFO said the majority of the company's plants were operating normally, but that the network shutdown prevented plants from receiving future orders from customers.  He said the losses at the moment were "minimal," but he conceded they would grow over time if automated systems aren't restored.  Kallevik - that's the CFO - was unable to provide any timetable for how long it would take to disinfect the network.  He said company IT teams are working to remove the ransomware - and actually I heard in some separate reporting that Microsoft, a team from Microsoft had flown over to help with that.



He said:  "Company IT teams are working to remove the ransomware from infected systems.  Once that's done, the teams plan to restore lost data using company backup systems," which he described as "good."  Asked by a reporter if the company would rule out paying the demanded ransom, the CFO said:  "The main strategy is to use backup."



Lawrence Abrams at BleepingComputer, who is of course everyone's go-to site for ransomware details, added, he said:  "It should be noted that while this ransomware has had high-profile targets, it is not the most active one out there targeting companies and has not seen wide distribution."  He said:  "Furthermore, it's very noisy as it consumes a lot of CPU, causes Windows Explorer to crash repeatedly, and borks the system," he wrote, "enough while encrypting that you can't run normal programs."  In other words, it's not very stealthful while it's doing its deed.  He says:  "Unless it's launched on an idle machine, it would have a good chance of being spotted."



So anyway, what I had heard in subsequent reporting is that they have removed it, and they are restoring from backup.  So no mega payout.  Oh, and there was no fixed price given, either.  These guys, in their note, they instructed the infected company to contact them and strike up a dialog, and that the amount requested would be a function of how long it took to reach out and contact them.



LEO:  Oh, yeah.  Don't delay.  Call today.



STEVE:  Call today.  That's right.  So anyway, that's the background on that attack.  And we're going to talk about Microsoft renaming Windows Defender.  Microsoft has renamed Windows Defender Advanced Threat Protection, you know, APT, to the more generic Microsoft Defender Advanced Threat Protection.  Why?  Because they're offering it for the Mac.  It was a post last Thursday on...



LEO:  Oh, that's so interesting.  Wow.



STEVE:  Yeah, it really is.  A Microsoft blog posting was titled:  "Announcing Microsoft Defender ATP for Mac."  They said:  "Today we're announcing our advances in cross-platform, next-generation protection and endpoint detection" - oh, and I should mention that Linux is coming - "endpoint detection and response coverage with a new Microsoft solution for Mac.  Core components of our unified endpoint security platform, including the new Threat & Vulnerability Management also announced today, will now be available for Mac devices.



"We've been working closely with industry partners to enable Windows Defender Advanced Threat Protection customers to protect their non-Windows devices while keeping a centralized [they called it] 'single pane of glass,'" meaning everything monitored in a single location.  "Now we're going a step further by adding our own solution to the options, starting with a limited preview today.  As we bring our unified security solution to other platforms, we're also updating our name to reflect the breadth of this expanded coverage:  Microsoft Defender ATP.



"There are two key parts for cross-platform support for Microsoft Defender ATP on Mac."  They said:  "A new user interface on Mac clients called Microsoft Defender ATP.  The user interface brings a similar experience" - meaning like look and feel - "to what customers have today on Windows 10 devices."  And then:  "Reporting for Mac devices on Microsoft Defender ATP portal."  And then they said:  "Microsoft Defender ATP can be installed on devices running macOS Mojave, High Sierra, or Sierra which you want to manage and protect."



And then they said:  "In a limited preview, this app provides next-generation antimalware protection and allows end users to review and perform configuration of their protection, including," you know, and then they had a list of all the standard AV things - quick scan, full scan, deep scan, quarantining, blah blah blah.



And then they said:  "Users will also be able to configure advanced settings:  Disabling or enabling real-time protection, cloud-delivered protection, and automatic sample submission; adding exclusions for files and paths; managing notifications when threats are found; manually checking for security intelligence updates."  And they said:  "Microsoft AutoUpdate service is also installed, which ensures that the app is kept up to date and properly connected to the cloud."



So Leo, I am completely out of the loop on Mac AV.  Are there multiple vendors offering Mac AV?



LEO:  Oh, yeah.  It's just [crosstalk] the same.



STEVE:  So it's the same as on the PC?



LEO:  Yeah, I mean, you know, I think you probably agree with me, I'm not a recommender of AV in general.



STEVE:  You know I agree that, I mean, I've got my little fort with the flag on it looking at me from the tray.  So I've got Microsoft's integrated Defender in my system where it's not bothering me.



LEO:  Well, and it comes with Windows 10.  It's kind of, because it's operating that way, you're not...



STEVE:  Yeah.



LEO:  But that's now installing a third-party standalone AV on Mac, which I wouldn't - I don't think I would recommend.  Apple does have, not an antivirus, but has some pretty sophisticated security stuff on there, including malware scanning and Gatekeeper.  So I don't think you need it.



STEVE:  Yeah, I mean, and I agree.  And I wonder if - do you know if Microsoft's portal stuff integrates with Apple's so that you could stay native and still get this single pane of glass thing?  Or do you think they're just, like, separate?  I don't know.



LEO:  I have no idea what the implementation specifics are.  But this isn't surprising.  Remember Microsoft put out that Chrome and Firefox plug-in.  You talked about it last week.



STEVE:  Yeah.



LEO:  So basically there's an Edge sandbox inside Chrome and Firefox.



STEVE:  An Edge takeaway.  Oh, you're about to go on the Internet.  Let's switch you over to Edge.



LEO:  But in their defense, their sandbox technology's on Edge, it's based on Edge, so that's why they do that.  But I think this is the new Microsoft.  They don't care that much about Windows.  Windows is not the crown jewels by any means of the company anymore.  And so why not put a Microsoft everywhere?  It is bizarre.  I would never have thought that we'd be looking at antiviruses for Macintosh from Microsoft. 



STEVE:  No, it is.  And I wonder, I mean, homegrown?  Or maybe I wonder if they acquired somebody that was already there?



LEO:  Oh, that's a good question.  I mean, Defender's based on Giant antivirus, remember that, way back when they bought it.  But by now it's so different than Giant originally was that it's a unique product.



STEVE:  But that's generally how Microsoft acquires a big new technology, you know, something that's really alien to what they already have is they just, you know, they acquire it because money is not a problem at Microsoft.



LEO:  Right.  So, yeah, I'd love to know more about this.  And for now, I don't know about you, but I wouldn't recommend it.



STEVE:  No.  I just wanted to let our listeners know.  And maybe there's an enterprise need for...



LEO:  That's probably it; right?



STEVE:  Yeah, because that is where, you know, the Azure Cloud rigmarole.  Okay.  So for our listeners who may be responsible for a website based on PHP, which generates PDFs on the fly, a severe security bug was found six months ago in the most popular PHP library for creating PDF files.  The three most popular libraries used by web servers to create PDF files for like invoicing, purchase receipts or whatever on the fly are TCPDF - which I think is a clever name - TCPDF, MPDF, and FPDF.  And as we know, well, I'm sorry.  And now we know, after the very responsible disclosure by a security researcher who waited, not six hours, not six days, not six weeks, but six months after the flaw was disclosed privately so that it could be fixed, that now today we know that a serious remote code execution flaw exists in the one of those three which is the most popular, which is TCPDF.



The vulnerability is a variation of another researcher's discovery which was first found by a guy named Sam Thomas, a researcher at Secarma who in a series of experiences last summer showcased a deserialization bug - and we've talked about this, and I'll remind our listeners about that in a second - affecting PHP apps over the summer of 2018.  He released a research paper detailing PHP serialization attacks against WordPress and Typo 3 CMS [Content Management System] platforms, but also the TCPDF library, which is embedded in the Contao CMS.



Then in a blog post just this past weekend, an Italian researcher who goes by the online handle Polict, P-O-L-I-C-T, revealed a new PHP serialization flaw impacting TCPDF, like in the same way as the one discovered by Thomas last summer.  Polict says the vulnerability he found can be exploited two ways.  The first is on websites that allow user input to be part of the PDF generation process, such as for example when adding a name or an email address or other details which would then be bound into the resulting PDF.



The second would be on websites that contained cross-site scripting vulnerabilities where an attacker is able to plant malicious code inside the HTML source code that will then be fed to the TCPDF library to convert it into a PDF.  One way or another, the requirement is to supply deliberately malformed data to the TCPDF library, which causes that library to call the PHP servers, something known as the "phar," phar:// stream wrapper, which later then abuses the PHP deserialization process to run code on the underlying server.



So what the upshot of all this is, it is a very potent remote code execution vulnerability.  He notes that it's a complex attack requiring advanced PHP coding skills to exploit.  And as we have discussed previously, deserialization attacks are difficult to uncover and are the bane of many programming languages.  Ruby, Java, .NET, and PHP all have these problems.  Recall that when we serialize something, we convert a complex structured data into a linear, thus a serialized byte stream.  I mean, that's like for storage.  In order to store a complex data structure, it's serialized into a blob.  Then when we deserialize in order to read it back and reconstruct it, we're reversing the process to convert that byte stream back into the original complex data structure.



But all too often, coders who write the deserializer, well, they're typically the same people who wrote the serializer.  So they inherently assume that the byte stream they are being fed back is the one that their serializing code generated, that it was created by a well-meaning serializer.  But of course a deserializer is necessarily an interpreter.  It reads the incoming byte stream and interprets its meaning in order to reconstruct this complex data structure.  And as we have often noted on the podcast, interpreters are inherently fraught with problems.



So Polict said that he reported the vulnerability, and it was given a 2018-CVE-17057, to the TCPDF library author last August, so a long time ago.  The TCPDF team then released an updated TCPDF 6.2.20 a month later, in September, to address the issue.  Unfortunately, when the TCPDF team did that, they accidentally reintroduced the earlier vulnerability reported by Sam Thomas while attempted to patch the one reported by Polict.  Like I said, interpreters are finicky beasts.



So finally both issues were resolved in v6.2.22.  Polict published the details about this vulnerability last week after waiting six full months after the patch's release, due to the bug's severity, the fact that it is remotely exploitable.  It runs the attacker's code on the attacked server, and it affects any PHP-based website that uses TCPDF to render its PDFs on the fly, which are a lot.  So he waited six months to allow updates to actually get out to the endpoint.



In their coverage of this, ZDNet noted that the TCPDF library is one of today's most popular PHP libraries and has been used, they wrote, "all over the place - in standalone websites, in content management systems, CMS plug-ins, CMS themes, enterprise Intranets, CRMs, HRMs, invoicing solutions, many PDF-centered web apps, and others.  And," ZDNet says, "patching isn't as easy as it sounds.  In some cases this means replacing a file and editing a build instruction, but in other places it might require rewriting large swaths of code."



So anyway, and we know that PDF libraries are sort of that unsexy back end that are exactly the sorts of things that tend to be overlooked by web developers and website maintainers.  So the takeaway for our listeners is, if you're responsible for any website that does generate on-the-fly PDFs, make sure that, if you're using TCPDF, that it is at v6.2.22 or later.  Because now that this is public, bad guys are going to know, I mean, we know what the pattern is here now.  Something like this that's juicy is revealed publicly, the bad guys roll up their sleeves, and they start looking for opportunities to exploit.  So you want to make sure that in the, I mean, ideally in the intervening six months, just in the natural course, that library would have been updated because you've had plenty of time.  If not, then don't delay.



News is from Bleeping Computer that Microsoft's leaked Edge browser based on Chromium is looking really good.  Bleeping Computer reported that over this past weekend a leaked build for the Chromium-based Edge browser has been released that is providing users with their first look at the upcoming browser from Microsoft.



I think it was Lawrence who said:  "If you are currently using Chrome, the reports indicate that this Edge preview browser feels, performs, and basically offers, not surprisingly, the same feature set."  Oh, yeah, it was Lawrence.  He writes that:  "Microsoft has been quiet regarding their upcoming Microsoft Edge Insider browser, but a slow trickle of leaks has provided quite a bit of information.  With this leaked build, users get their first look at the upcoming Edge browser, which from all reports feels like it has the best chance of putting a dent in Google Chrome's market share.



"Users who have tested the leaked build have also stated that the browser performs really well when browsing the web, and that it is more than ready for public preview.  Microsoft has modified the layout of the browser to make it feel more like a Microsoft app.  For example," he says, "the Settings pages have a left-hand navigation panel similar to other Windows 10 apps."  I like the way the Control Panel looks because I saw a picture of it.  "Microsoft also included their own services into the browser," not surprisingly.  "For example, Google Safe Browsing has been removed in favor of Microsoft SmartScreen.



"In addition to setting up a dedicated Microsoft Extension store, Edge" - and this is really interesting - "also allows users to enable the installation of extensions from Chrome's web store.  While Microsoft states that these extensions are unverified as a warning, it provides an enormous pool of extensions for users to install right off the bat, right from release."  So this feels significant.  We've got Google with currently the largest browser share, and we've got then the largest browser built into the largest desktop and laptop share, effectively merging into one.



So, I mean, this will only be Windows 10.  But still it'll automatically - we know what Microsoft will do.  As soon as this thing is ready, all Edge will be converted to this, and a month later everybody running Windows 10, which is a little over half of the Windows install base now, will be essentially merged with everybody who is - whether you're running Windows 10 or Google's Chrome, and everybody not running Windows 10 who is using Google Chrome, effectively all using the same browser.  So that's a lot of weight for that browser.  And it makes me glad that we have a SQRL plug-in running on Firefox and Chrome because that'll soon also then be running on Edge.  And that'll be cool.



Leo, I titled this next piece "From our pained abbreviation department" because, boy, this one is a stretch.



LEO:  Yeah, yeah.  I know what you're going to talk about, too.



STEVE:  They really wanted to call this SHAKEN/STIR.



LEO:  Little Bond reference, yeah.



STEVE:  Yeah, exactly.  SHAKEN, S-H-A-K-E-N.  And, oh, this one was a reach.  Signature-based Handling (so we've got the S and the H) of Asserted (now we have the A) Information (okay, we're going to forget about the I) Using (forget about the U) toKENS.



LEO:  Oh, boy.



STEVE:  And we're going to get the K-E-N to finish the SHAKEN.  So, ouch.  And then of course STIR, that comes from Secure Telephone Identity Revisited.  So it's like, okay, you guys.  The military, I think, has the best acronym people.  I don't know where, how that happens.



LEO:  Well, Congress is pretty good, too.  They come up with some wild acronyms.



STEVE:  Yeah.  Anyway, it's...



LEO:  They're mostly retronyms, you know.



STEVE:  Yeah.  Horrible as those abbreviations are, taken together, SHAKEN and STIR do deliver a protocol for authenticating phone calls with the help of cryptographic certificates.  The U.S. Federal Communications Commission has been pushing for SHAKEN/STIR's adoption and has imposed an end of 2019 hard deadline for networks to implement the protocol.  I have a link to the FCC.gov/call-authentication.  And I had to cut out some of the self-serving Ajit Pai nonsense from it.  But what I did keep reads - it was titled "Combating Spoofed Robocalls with Caller ID Authentication."  And notice it's spoofed robocalls, not robocalls.



So this reads:  "FCC Chairman Ajit Pai:  'American consumers are sick and tired of unwanted robocalls."  Amen.  "This consumer among them," he says of himself.  "Caller ID authentication will be a significant step towards ending the scourge of spoofed robocalls.  It's time for carriers to implement robust caller ID authentication."



And then in the same announcement:  "How Will Caller ID Authentication Help Consumers?  Caller ID authentication is a new system aimed at combating illegal caller ID spoofing."  Okay, I didn't know it was illegal, but apparently it is, but everyone does it.  "Such a system is critical to protecting Americans from scam spoofed robocalls and would erode the ability of callers to illegally spoof a caller ID, which scam artists use to trick Americans into answering their phones when they shouldn't."  Oh, okay.  Well, I'm not going to pick this apart.  I'll keep reading.



"Additionally, consumers and law enforcement alike could more readily identify the source of illegal robocalls and reduce their impact."  That's true.  "Industry stakeholders are working" - and I didn't know this, and I'm glad for this - "to implement caller ID authentication, which is sometimes [unfortunately] called SHAKEN/STIR.  Once implemented, it should greatly help the accuracy of caller ID information and should provide consumers with helpful information for determining which calls are authenticated.



"SHAKEN/STIR is a framework of interconnected standards.  SHAKEN/STIR are acronyms for Signature-based Handling of Asserted Information Using toKENs and the Secure Telephone Identity Revisited standards.  This means," they write, "that calls traveling through interconnected phone networks would have their caller ID 'signed' as legitimate by originating carriers and validated by other carriers before reaching consumers."  Okay, now, of course, this is what we've had for HTTP ever since - what was SSL?



LEO:  Secure Sockets Layer?  What?



STEVE:  The original Netscape browser.  Netscape 4 or something, at least.  So, yes, we've had, I mean, all they're talking about is that the originator of a call sign the caller ID.  Thank you.  And that the signature be verified by the receiving network.  Okay.  Doesn't seem like rocket science.  They're going to do it, which is good news.



"In November of 2018, Chairman Pai" - this is, again, their announcement - "sent letters to voice providers asking those that apparently had not yet established concrete plans to protect their customers using the SHAKEN/STIR standards to do so without delay.  In February 2019, Chairman Pai welcomed many carriers' commitment to meeting his timeline for implementation, called on others to catch up, and made clear that the FCC would consider regulatory intervention if necessary."



So the news is last Wednesday, almost a week ago, AT&T and Comcast announced that they had successfully tested what they believe to be the first SHAKEN/STIR-authenticated call between two different telecom networks.



LEO:  And of course the content was "Watson, come here, I need you."



STEVE:  So apparently, work on the SHAKEN/STIR protocol has been underway for a while, and telecom operators have used it internally, but only for calls originated and terminated within their own networks.  So they've been getting ready to reach out and touch someone, and they finally did.



LEO:  I'm so glad you said that.



STEVE:  So once broadly adopted, incoming calls not signed, and that's of course I guess still a possibility, could simply be dropped as a subscriber option.  And spoofed caller ID for signed calls would become impossible.  So this will not, by itself, stop robocalling; but it will certainly chill the callers, who then significantly know that law enforcement, if they were to use signed caller ID, they can't be spoofed anymore.  So anyway, I just want, you know, the robocalling is a problem.  The spoofing of caller ID is a problem because you can't block - I think we've talked about this recently, in fact.  You can block a robocall, but then it just comes back on a different number because they just make them up.



LEO:  Right, right, right.



STEVE:  So there's no benefit at all to doing that.  And so anyway, progress.  And then maybe at least it'll give us initially some more control.  And then we just need legislation, if that ever happens.  So we'll see.



I wanted to quickly note to all we users of PuTTY, P-u-T-T-Y, and of course that's an old play.  The original teletype was known as the TTY, pronounced "titty."



LEO:  Really.



STEVE:  Yes.



LEO:  I never heard that.



STEVE:  Absolutely.



LEO:  Maybe that was your group.



STEVE:  Us old-timers, it would be, "What happened to the titty interface?"



LEO:  No, no, no, no.  TTY.  No, no, no, no.



STEVE:  I'm sorry, TTY, that's what it's called.



LEO:  I always said TTY.



STEVE:  All of us old-timers.



LEO:  That's, see, you're two years older than me, so that's why.



STEVE:  That's right.  So it is a very popular SSH and Telnet client for securely with SSH connecting to remote systems over a network.  They have just announced and released 0.71.  And I don't know what's - why they're not at 1.0.  It's like, get off, I mean, just make it 1.0.  I mean, it's been around for a long time.  Anyway, for whatever reason it's at 0.71.  This update patches eight high-severity flaws which affect - they're very much like we were talking about the RDP exploit where, if you remote desktopped, to make a verb of it, if you remote desktopped to a Windows server, if it was malicious it could, because your RDP client assumes a benign server, like a friendly protocol, it could get you.  Well, that's what these eight high-severity flaws are.  So it's unlikely that you're going to PuTTY to a remote server.  But should you do so with any version of PuTTY before 0.71...



LEO:  I believe that's PUTitty.  Right?  I might be wrong there.  Sorry.



STEVE:  I really don't want to say...



LEO:  No, it's not [crosstalk].



STEVE:  I mean, so everybody should update.  I've got a copy, and I updated when I ran across this news.  



LEO:  Surely there's a better terminal for Windows by now.  This thing has been decades - it's decades old.



STEVE:  It is very old.  There is - I'm looking at the link to it right here.  I also use - I can't see it on my desktop.  It's here somewhere.  Anyway, there are other SSH.  But it's very popular.  And it's, you know...



LEO:  Oh, yeah.  I installed it for years, along with Cygwin.



STEVE:  Yup.



LEO:  But seems like there must be - maybe, of course, now you can put Bash on there with LTS, so...



STEVE:  Yeah, yeah.



LEO:  Or WSL, rather, yeah.



STEVE:  So Firefox, I wanted to also note quickly, they're going to conditionally use - they're experimenting with conditionally using the Windows cert store to avoid the AV SSL scanning issues that they have been subjected to recently.  When we were exploring the recent mess that was caused by Firefox Update, where the AV vendors who had added their own AV HTTP TLS interception roots into the Windows root store had not done so for Firefox.  And at the time we noted that Firefox did have an option, which was disabled by default, to import the Windows root store.  That was security.enterprise_roots.enabled, which is normally false, but you can set it to true.  And if you do, all of those problems immediately vanish because you're using the Windows root store.



Now Mozilla is considering automatically flipping that preference proactively in detectable situations to avoid future problems with third-party AV.  I have a link to their Bugzilla posting, and they called it "Retention/Engagement impact of enabling the Enterprise roots feature in presence of an AV."  And the bug says:  "Several AVs recently broke HTTPS with their HTTPS scanning features that require their certificates to be added to our certificate store."  And we discussed Avast, but there's also Bitdefender had a Mozilla bug filed for it, and Kaspersky also.



They said:  "The security team confirmed that having the preference security.enterprise_roots.enabled set to true would have fixed all of these issues without known regressions; and we want to validate that, in the presence of an AV, enabling this preference would have a positive impact on retention" - I guess that means Firefox user retention, like they're not going to lose people to Chrome or Edge because of a problem that Firefox is causing - "and engagement."  And they said, parens:  "(We cannot detect a change in certificate error page displays through telemetry since telemetry is sent over HTTPS," this person writes, "that breaks in these instances.)"



So then they said, and this is what I thought was interesting:  "Description of the impacted population."  First bullet point:  "Win 10 and Win 8 release users."  And they said, parens:  "(The API allowing detection of an AV registered with the system was only available since Window 8)."  And then also:  "An AV is registered with the system and is not Windows Defender.  This information is available on the telemetry under 'sec.antivirus.'"  So the point being, from Windows 8 on, there is a means of determining whether a non-native, that is to say, not what we used to call Windows Defender, which is now Microsoft Defender, when it's not that, but it's a third party, there is a way for Mozilla to detect that.



So in those instances, what they're exploring is proactively flipping this so that, if you are on Windows 8 or 10, if you have a third-party AV, not the built-in Microsoft AV, and you are using Firefox, you will not have a problem.  So anyway, just good news for people who want to use Firefox and a third-party AV on Windows 10 without the hassle of needing to, well, actually, you can just go manually flip that switch if you're a listener to the podcast.  Everybody else will have it done automatically.



Also a quick note that last Thursday Microsoft announced that their new Windows Virtual Desktop product was now available for public preview.  The technology, this Windows Virtual Desktop product, which is perversely named Remote Desktop - which seems like a name collision to me.  But maybe they're going to amalgamate the existing Remote Desktop with what is called Windows Virtual Desktop into a single thing called Remote Desktop.  So there is no collision because there won't be two different things.  Anyway, it is a technology that allows enterprise to move their desktops and applications to Azure for hosting on Windows 10 or Windows 7 Virtual Machine operating systems that are always secured with the latest updates.



What's interesting is that this will continue to get, if you choose to use Windows 7, it will automatically be getting updates through 2023.  I will remind our listeners that an Azure trial subscription is available for free for a year.  And apparently this looks like a move by Microsoft to come up with a compromise with users, enterprise users, where there are some things they want to do that they have a hard time moving from Windows 7 to Windows 10.



So what Microsoft is billing this as is as a way where desktops can be migrated to Windows 10, while at the same time those applications that are stubborn and need still to stay on Windows 7 can be accessed through the cloud on this Azure Windows Virtual Desktop that will just become Remote Desktop.  And it will be part of your Azure subscription.  You create a tenant, an Azure tenant, and then you publish desktops and applications to it, and then you're able to use it just like the application was running natively and locally on your desktop.  So anyway, and that gets updates, as does the extended service of Windows 7, through 2023.  So just another benefit.



And, finally, as I said, Leo, you and Mary Jo and Paul were talking about, were wondering out loud last Wednesday about what would Microsoft do about bugging people about Windows 7 end of support date.  We now know, thanks to coverage by Lawrence at Bleeping Computer.  The update is KB4493132.  The good news is so far it is not selected in Windows Update for installation by default.  So seems unlikely that Microsoft is not at some point going to flip that on, but we'll see.  It doesn't identify itself as providing end-of-life notifications.  It states that it is an update to "resolve issues in Windows."  Yeah.  Like you're still using Windows 7.



LEO:  No, we need some more money from you, that's the issue.



STEVE:  That's right.  So we now know what the dialog looks like.  It's a big dialog, and it reads:  "After 10 years, support for Windows 7 is nearing the end.  January 14th, 2020 is the last day Microsoft will offer security updates and technical support for computers running Windows 7.  We know change can be difficult."  Yeah, and of course it would be a lot less difficult if you didn't make Windows such a - but anyway.  "We know change can be difficult.  That's why we're reaching out early to help you back up your files and prepare for what's next."  And then the good news is, in fine print, itty-bitty down in the far lower left, there is a checkbox that says "Do not remind me again."  Which of course you can turn on, and then I guess you close the box.  It doesn't say okay.



LEO:  Yeah, there's no okay button, yeah.



STEVE:  It just, yeah, there's a close.  So we'll hope that works.  Lawrence has torn this thing into bits in his posting.  I've got a link in the show notes.  I mean, so he's figured out what it is that it downloads, what's the CAB file.  It puts two entries into Windows Scheduler.  Where it runs.  It looks like it's got multilingual HTML assets so they can change the pretty picture and wording anytime they want to.



But the good news is it isn't currently being installed by default.  It's hard for me to believe Microsoft will not turn that on once they're sure it's not causing problems, or once they get feedback from people who have turned it on, who always turn on all the optional updates.  So it's one of the optional updates which is not, you know, that's optional.  Eventually it seems certain that they're going to feel they need to warn people that the end is nigh.  So we'll see what happens with that.  But at least we know what it looks like.  And they are saying stop bothering me about this in the future.



I just wanted to remind our listeners of the excellent three season series "The Expanse."  It first aired on Syfy Channel.  It's now available from Amazon Prime.  And Lorrie and her son were out of town together traveling for several days last week.  And so I thought, okay, this is my chance to watch the final season.



LEO:  Lorrie doesn't like sci-fi.



STEVE:  No, no.  She actually really, really, really does like sci-fi.  But she's not big on violence, and...



LEO:  She didn't watch the first two with you, either.  So she's kind of behind.



STEVE:  Precisely.  So there's really no - it didn't make sense to try to catch her all up.  Oh, my god, is it good.  It's just - it is.  I remember when it was on Syfy thinking, where did this come from?  How is this possible?  Because Syfy Channel, much as I love them, they just produce crap, I mean, just horrible, barely watchable stuff.  Now "Galactica," that was an exception, too.  But I don't know how this was produced, but this was just excellent.  And so it's three seasons.  I don't remember how many episodes the previous two had, but this one had 13.  And it is top quality.  I mean, it demonstrates that our computer-generated graphics has really come a long way because you can kind of sense that some of it is CG, but not rubbery looking.  I mean, it sells it.



And, but I mean, like I'm watching as they were under acceleration, and somebody was trying to get to somebody else, and they were having to roll the ship.  And the physics was perfectly done of this guy hanging onto this bar hand-over-hand while loose tools were flying from one side of the room to the other, and they were having to duck them as the ship maneuvered.  I was just - I was stunned.  I thought, these guys have some serious science advisors.  Anyway, now, the only downside is it's a little bit into politics.  There's Mars and the U.N. and the Belters are sort of the three political factions.  And so if you're really intolerant of any, like, political back story or political machinations, it might turn you off a little bit.  Or you can just kind of ignore that part and just watch the fun because - anyway, it was great.



So I just wanted to put it on people's radar.  If you're an Amazon Prime person, it's free for you.  And it's probably 39 hours of good - and it was an hour-long episode.  It wasn't like 45 minutes where it used to be for TV and they cut out the commercials.  So I recommend it.  If the politics doesn't turn you off, because there is that.  And I did want to remind, or I just wanted to mention that the next book in the never-ending 75-book series...



LEO:  Ryk Brown is crazy.



STEVE:  Ryk Brown.



LEO:  He's nuts.  You didn't even have to tell me.



STEVE:  R-Y-K, Ryk Brown.  It's "The Frontiers Saga."  I am in love.



LEO:  I don't think there's a 75-book series anywhere else.  That's it.  That's got to be the one.



STEVE:  No, no.  And you know, Leo, when I was writing the show notes I thought, how would I describe this?  And I said, well, we all - we're familiar with comfort food.  This is comfort reading.  I mean, it's not, you know, earthshaking, world-changing, bleeding-edge sci-fi.  It's just a bunch of people who you get to know really well.  The writing is excellent.  The plot twists and stuff, I mean, there's an arc in place.  It's just so pleasant.  And so, yes, I am now on book number 26.



LEO:  Twenty-six, not 76.



STEVE:  No, he's going to write - so what he's laid out is five 15-book arcs.



LEO:  Seventy-five.



STEVE:  Seventy-five.  And so the first one was 15.



LEO:  That's insane.



STEVE:  I'm now on book 11 of the second 15, so I'm on book 26.  And it is so good.  It is just - it is so good.  I was reading something else last week, again because Lorrie was out of town, so I had a little more spare time in the evenings.  And so I thought - I continued to read what I was reading, and I thought, where is that next book?  And I checked, and it had been released.  It's like, ah.  And so I immediately switched to it, and I'm sadly about a third of the way through.  It's just - I just wanted to say "The Frontiers Saga," if you just like comfort sci-fi, you know, it's just great.



Over on the SQRL side of the world, everything is moving forward apace.  I'm working to get the static content of the web forums finished.  Then I get to take my Windows client to final.  A couple people, I mean, this is all working out well.  I know it's taking a long time.  But, for example, now that more people are using it, the observation was made that users are used to, when they change their password, it's like everything is cloud-synced; like if you change your password with LastPass, then LastPass, you know, like it's changed everywhere.  Or if you change your password with a website, then of course when you log in somewhere else to that site, it's the changed password.



Well, SQRL is different, of course, because the slogan I've come up with, the best way of thinking of this, is that SQRL logs into websites for you.  That is, so it offloads all of the mess.  It logs into websites for you.  But you have to remind SQRL or prove to SQRL that you're you and not somebody else using your SQRL identity.  Anyway, the point being that if you change your SQRL password, that is, the password you use, the only one you need, the one password for SQRL - and yes, it can be your face or your fingerprint, and people are loving that - that doesn't automatically change your password on a different device, like on your Android or your iOS device, where you also have your SQRL identity.



So the point is I'm going to add a little reminder in the wizard for changing your password, just to remind people that, to prevent confusion, they should make the same change they have just made here on other places where they have and use SQRL.  So those little final touch-ups.  So anyway, I have to do that.  There's a bunch of development code still in the client that gets removed.  And so I'm about to take it to 1.0 because it hasn't been yet.  Then I need to update the online documentation of the spec because there's lots of pressure now that we've got now someone working on a pure JavaScript implementation.  We've got the Android client, a lot of work on that, and the iOS client coming along fast, and the web extension.  Someone's doing something for React that'll be available for NPM.  And so lots of pieces coming together.



The point is that I did see - we were discussing, there'd been some discussion in the SQRL forums about remaining logged into a site, which is not in any way related to SQRL because SQRL's role is just to authenticate you to the site.  And then of course the site maintains a cookie is how your browser session stays connected.  But what happens is it's so easy to sign in, almost fun to sign in using SQRL, that it does arguably sort of shift the balance away from necessarily remaining signed in everywhere because it's so easy now to reassert one's identity.



Anyway, so day before yesterday I saw a post.  It was followed up yesterday, and I thought I would just share these.  So day before yesterday at 1:00 p.m. someone calling himself "Gristle," he said:  "I am loving logging in with SQRL so much that I actually don't want the cookie to persist longer than an hour, or maybe even minutes. It's just so fun and simple to use SQRL."



And then I think he came back and saw his own post, and yesterday he quoted himself, saying:  "I'm not joking.  I find myself logging out and in and out and in, just for fun.  It's crazy, I know, but it's just so cool.  Reminds me of the first time I tried unlocking a phone with my fingerprint.  I kept locking and unlocking it, unbelievable that there is so much crypto behind such a simple gesture."  So anyway, I will - we're not far away from getting this thing off the ground.  I mean, it's no longer just me pushing forward on the client and the protocol.  All of the other pieces that we need to create a functioning ecosystem are, I mean, the Android and the iOS client both work.  People are using them like crazy now.  So we're getting there.



Oh, and I talked to Ralf, I can't pronounce his last name, it's  Wondratschek or something.  I'm sorry, Ralf, for mangling your name.  But he was the person who wrote that Android client a long time ago, and some people were getting confused about it because it's still on the Google Play Store.  I asked him if he would just make a mention that this was no longer current for the SQRL protocol as it exists today.  And so he said he wanted to keep it there for his rsum; but, yes, he would make a change, and he did.  So thank you, Ralf, for that.



I'm wondering if I want to skip talking about SpinRite because of where we are with time.



LEO:  Oh, we've got all the time in the world, unless you've got a date.



STEVE:  No.  Okay.  So...



LEO:  That's right, Lorrie's out of town.  Go ahead.



STEVE:  This is some good techie stuff that I thought our listeners would find interesting.  It came from a reply that I wrote.  And as I was writing, I thought, okay, I'm going to make this a little more general reply that we can use.  So one of our customers said:  "I have a few questions about SpinRite."  He said:  "Those sectors you read with surface scan errors, do you try to overwrite them so that sector mapping by S.M.A.R.T. takes place?"  He says:  "Recall the remapping will only take place when the data is overwritten, which in a file system may never happen until the disk is completely, sector by sector, reformatted."  He says:  "Imagine a file that is read often, but never rewritten, and resides on one or more sectors with surface scan errors."



Okay.  Well, that's full of misunderstandings.  But that prompted me, when my tech support guy forwarded it to me, it prompted me to explain.  And I said:  "If you think about this a bit, you'll see how this can work."  I said:  "Surface defects do not manifest when sectors are written because writing to the disk is an entirely blind process.  The drive gets no information about the readability of a freshly written sector.  That only occurs when a subsequent attempt is made to read back that sector.  Because a read-after-write would be prohibitively slow, all modern drives incorporate Error Correction Code (ECC) technology which creates a safety margin to defend against the possibility that, due to some problem which may have occurred during writing, or a surface anomaly that interferes with reading, the data that was written to a sector cannot be read back without the aid of algorithmic correction."



And I wrote:  "ECC operates by appending additional carefully designed redundant data to the end of the sector, based upon the sector's intended content.  This extra data allows the location of a read-back problem to be identified along with its length and its error mask.  From this data the drive is able to determine, within limits, what data was originally written to the sector.  Those error recovery limits are the number of separate errors occurring within a single sector, and each error's bit-run length."



I wrote:  "On today's drives, data densities have grown so high that some level of background error correction is occurring more or less continuously.  It's only when the extent of correction required for read-back recovery begins to approach the drive's inherent limits on recovery that the drive gets worried and decides to remove that worrisome sector from service."



And then he wrote:  "If you do overwrite them and force remapping, how do you stop a disk from being rendered useless because the max limit (say 100) of remapping has taken place due to your reading all sectors and forcing all those remappings?"



And so I wrote:  "A sector can be misread due to its own aging or transient vibration or physical shock which forced the head slightly off track when the sector was last written or when it is now read.  To determine whether the error was transient and not caused by an actual defect, SpinRite first writes inverted data, then reads it back.  It then rewrites the original data and reads that back.  If any of those three reads - the original read, the inverted read, and the reinverted read - result in sufficiently threatening data read-back errors, the drive will take that sector out of service and remap the sector."



And then he asked:  "When a sector is found with a surface scan error, are you able to tell me what NTFS (Windows in my case) file is using that bad sector?"



And I wrote:  "Since a bad sector is replaced with a good sector at the drive's physical interface level, this falls beneath the operating system's file system.  In essence, all surface errors and remappings are transparent to the OS and the file system, so nothing needs to be done there.  Before this autonomous drive-level sector-replacement technology existed universally as it does now, SpinRite did this itself.  All early versions of SpinRite managed surface defects for the drive.  SpinRite would determine which cluster the defective sector occupied.



"It would then determine what role that cluster was playing in the file system, if any.  If the cluster was in use by a directory or a file, it would obtain the closest nearby free cluster, copy the bad clusters data into the new cluster after recovering that data, and relink the replacement cluster into the system's file system.  It would then mark the defective sector as bad in the low-level format, and mark the defective cluster containing that sector as bad in the system's cluster management tables.



"All of that technology still exists in today's SpinRite for possible use if it should encounter the need.  But SpinRite is able to interact with all modern drives to perform these data relocations underneath the file system.  Other than on floppy disks, SpinRite's high-level defect management is never needed today."  So there's a little kind of interesting background on what SpinRite does to keep people safe and to manage to help to work with drives to help them manage the health of their sector pool and how they relocate and replace sectors when they are truly damaged.  And how, because we don't immediately trigger on a single problem, how we separate transient errors from true physical surface defect errors.



You know, remember, our long-term podcast listeners remember how you can shout at a drive, and suddenly it will slow down the rate at which it's able to transfer data.  Well, that shouting is vibration, which forces the heads off track just enough to cause them to misread what's there.  And so the surface, the platter has to spin around again and try to do a retry.  And so, I mean, it really is the case that drives are very vibration-sensitive.  So one tip is make sure that your drives are not, well, aside from being shouted at, are operating in an environment where they are protected from undue vibration.  They really do try to protect themselves.



So last Wednesday, Thursday, Friday was Trend Micro's organized ZDI, their zero-day initiative, with support from Microsoft, Tesla and VMware:  the Pwn2Own competition.  The first day resulted in four successful hacks and one partial win, with the contestants earning a total of $240,000 U.S. in cash awards, plus because it's the "own" part of the Pwn2Own, the laptops which were used to demonstrate their research.  So in their traditional blow-by-blow style, Trend Micro described the first day as follows.



They said:  "The contest started with the team of Fluoroacetate" - who always dominates this competition - "Amat Cama and Richard Zhu, targeting the Apple Safari web browser.  They successfully exploited the browser and escaped the sandbox by using an integer overflow in the browser and a heap overflow to escape the sandbox.  The attempt nearly took the entire allowed time because they used a brute force technique during the sandbox escape.  The code would fail, then try again until it succeeded.  The demonstration earned them $55,000 and five points toward Master of Pwn.



"The Fluoroacetate duo returned targeting Oracle VirtualBox in the virtualization category.  Although their first attempt failed, the second attempt successfully used an integer underflow and a race condition to escalate from the virtual client to pop calc at medium integrity.  It wasn't the race condition that caused their first failed attempt.  Their memory leak was working, but their code execution failed.  Everything aligned on the second attempt, which earned them $35,000 and three more Master of Pwn points.



"Next up, Pwn2Own newcomer anhdaden of STAR Labs also targeted Oracle VirtualBox.  He also used an integer underflow to escalate from the virtual client to execute his code on the hypervisor at medium integrity.  Interestingly, he used a unique integer underflow different than the previously demonstrated underflow.  His first foray into Pwn2Own netted him $35,000 and three Master of Pwn points.  This also marks the first Vietnamese winner at Pwn2Own.  We hope," they wrote, "to see more of him in the future.



"In their final entry for Day One, the Fluoroacetate duo targeted the VMware Workstation.  They leveraged a race condition leading to an out-of-bounds write to go from the virtual client to executing code on the underlying host operating system.  They earned $70,000 USD and seven additional Master of Pwn points.  This brings their Day One total to $160,000 and 15 Master of Pwn points."



And I'll note that this is a full VMware escape, executing code on the host OS for which, as we recently noted, Zerodium has upped their bounty to half a million dollars.  So it seems clear that these guys are the good guys who are interested in increasing security, rather than using their talent to indirectly attack others, which of course is what happens if you sell your zero-day exploit to Zerodium.



"The final entry in Day One saw the phoenhex & qwerty team targeting Apple Safari with a kernel elevation.  They demonstrated a complete system compromise.  By browsing to their website, they triggered a JIT [Just In Time] bug, followed by a heap out-of-bounds read, used twice; then pivoted from root to kernel via a Time-of-Check-Time-of-Use (TOCTOU) bug.  Unfortunately, it was only a partial win since Apple already knew of one of the bugs used in the demo.  Still, they earned themselves $45,000 and four points toward Master of Pwn.



"On Day Two the day began with Fluoroacetate duo back again, targeting the Mozilla Firefox web browser.  They leveraged a just-in-time bug in the browser, then used an out-of-bounds write in the Windows kernel to effectively take over the system.  They were able to execute code at system level just by using Firefox to visit their specially crafted website.  The effort earned them another $50,000 and five more points toward Master of Pwn.



"The prolific duo returned with perhaps their greatest challenge of the competition.  Starting from within a VMware Workstation client, they opened Microsoft Edge and browsed to their specially created web page.  That's all it took to go from a browser in a virtual machine client to executing code on the underlying hypervisor."  And I'll just stop for a moment to say, do we realize how much that exploit would have meant to Zerodium?  Zerodium would have moved heaven and earth to obtain that hack.  I mean, you visit a page, and you're running code on the hosting hypervisor.  Unbelievable.



Anyway, ZDI Trend Micro continues:  "They started with a type confusion bug in the Microsoft Edge browser, then used a race condition in the Windows kernel, followed by an out-of-bounds write in VMware Workstation," so a serious exploit chain.  "The masterfully crafted exploit chain earned them $130,000" - so no small potatoes there, either - "and 13 Master of Pwn points.  They now have a commanding lead with 33 points total.  In the two days of the competition, they racked up a total of $340,000 as a result of their phenomenal work.  Tomorrow, they will attempt to cap their week off with a successful demonstration in the automotive category."



Then the blow-by-blow continues:  "The third attempt of the day had Niklas Baumstark target the Mozilla Firefox web browser.  He used a just-in-time bug in the browser, followed by a logic bug, to escape the sandbox.  In a real-world scenario, an attacker could use this to run their code on a target system at the level of the logged-on user.  The successful demonstration earned him $40,000 and four Master of Pwn points.



"The final attempt for Day Two had Arthur Gerkis of Exodus Intelligence targeting Microsoft Edge.  The newcomer to Pwn2Own  wasted no time by using a double free bug in the renderer, followed by a logic bug to bypass the sandbox.  His debut entry earned him $50,000 and five points toward Master of Pwn."



And they write:  "That brings Day Two to a close.  We awarded $270,000 for nine unique bugs today, which brings the Day Two total to $510,000."  They said:  "Join us tomorrow as we debut the automotive category with the two final entries of Pwn2Own Vancouver 2019."



So Day Three, the automotive category.  Two months ago, mid-January, writing for Forbes magazine, Thomas Brewster wrote:  "Think you can hack a Tesla?  Now's your chance.  And you could win more than $900,000 in the process.  For the first time ever," Thomas wrote, "Pwn2Own, perhaps the world's best-known competition for ethical hackers, will have a Tesla Model 3 opened up for participants to break.  Prizes range from $35,000 to $250,000.  The more difficult the hack, the greater the prize.  The lowest prize," he writes, "will go to an as-yet-unspecified attack on the car's infotainment system.  The top $250,000 reward will go to the first person or team who can break any of the three critical Tesla internals:  the Gateway, the Autopilot, or the VCSEC.



"The Gateway," he writes, "acts as the central hub for controlling data flowing around the Tesla.  Taking control of that system would give a hacker power over many of the car's functions.  Manipulating Tesla's Autopilot could lead to all-too-obvious problems.  Imagine if the hacker simply shut Autopilot down without the driver noticing.  The VCSEC is the part of a Tesla responsible for a variety of security functions, including the alarm.  Again, it's not hard to guess just what a hacker could do if they commandeered that part of the car."  And he concludes:  "Another $100,000 is on offer for the first to hack the doors by breaking the key fob or mobile app unlock tech.  Starting the car without owning the legitimate key will also land a lucky hacker" - although I would say no luck is involved - "$100,000."



So what happened on Day Three?  ZDI writes:  "The day began not with a bang, but with a whimper, as the Team KunnaPwn withdrew their entry from the automotive category.  Although they did not demonstrate any of their research at this contest, we hope they submit some of their research to our program in the future."



And I found through other research, this was supposed to be the day when Team KunnaPwn demonstrated a hack of the Tesla Model 3's VCSEC security component.  So that was the expected or possible quarter million dollar hack.  But they withdrew from the competition.  And again, VCSEC is an abbreviation for Vehicle Controller Secondary, and as we know is responsible for security functions such as the alarm.



However, Fluoroacetate was next up.  ZDI wrote:  "When their scheduled time arrived, the dynamic Fluoroacetate duo of Richard Zhu and Amat Cama thrilled the assembled crowd as they entered the vehicle.  After a few minutes of setup, and with many cameras rolling, they successfully demonstrated their research on the Tesla Model 3, which is a Chromium-based web browser.  They used a just-in-time bug in the renderer to display their message on the Tesla's infotainment system and earned $35,000."  Of course, this is Pwn2Own, so they also got to drive away in the car.



So they wrapped it up saying:  "Overall, the three days of Pwn2Own Vancouver 2019 have been a great success.  We have," they wrote, "awarded a total of $545,000 for 19 unique bugs in Apple Safari; Microsoft Edge and Windows; VMware Workstation; Mozilla Firefox; and, in its inaugural year, the Tesla infotainment system.  And it should come as no surprise that the Fluoroacetate team of Richard Zhu and Amat Cama have been crowned the Master of Pwn for 2019.  Their amazing research earned them $375,000 over the contest and resulted in 36 Master of Pwn points.  They dominated Pwn2Own Tokyo and have carried that wave through to the spring.  We can't wait to see what's next for this pair of talented researchers."



So that's the story.  And hats off to everybody involved.  This is a great way to incentivize researchers to ethically hack and to fix problems without exposing end users to them because of course there is a "full disclosure" room at the Pwn2Own conference where everything is disclosed in return for the award money.  And then the manufacturer, the affected products get the benefit of getting patches before they are made public.  So yay to everybody involved.



LEO:  Yeah, especially Fluoroacetate.  They made some big bucks.



STEVE:  Boy, these guys, they've got skillz, S-K-I-L-L-Z.



LEO:  Yeah.  Do you figure this is like a full-time job for them?  All year long they search for these exploits, and it all culminates in March in Vancouver?



STEVE:  Yeah.  I mean, as we said, what was it, SPOILER was last week, and I think week before was hacking as a career.



LEO:  Yeah.



STEVE:  I mean, you can, if you're good, you can support yourself now.  We're in an industry where there is money, either responsibly or, I would argue, irresponsibly disclosed; there's money if you can find problems.  And, boy, the target richness of the environment is not decreasing.



LEO:  Yeah.



STEVE:  It is growing exponentially.



LEO:  I remember they won a lot of money in Tokyo, too, for Pwn2Own Tokyo.



STEVE:  Yup, exactly.  So not even annual, it's biannual.



LEO:  Yeah.  It's an interesting - I wonder if - it must be a certain brain type and certain skill set that just makes you well suited for this kind of stuff.  Because probably you would enjoy it, but most people would just go nuts staring at hex dumps and fuzzing readouts and, I mean, it'd be horrible.



STEVE:  I could do it.  I really - I prefer creating.



LEO:  Yeah, and reverse engineering and, yeah.



STEVE:  I love finding my own mistakes.  I just...



LEO:  Debugging is fun.  I agree.



STEVE:  I love debugging, yes.



LEO:  It's fun if you're not tearing your hair out.  If when you find the bug, let's put it this way, when you find the bug it's fun.  It can be less fun if you can't find it.  That's really frustrating, when you say, no, this should work, this should work.  What's going on?  I don't understand it.



STEVE:  Yeah, one of the things that I created that I mentioned before was an API for servers, which is what Rasmus used to create the SQRL login for our XenForo forums.  And you don't have to know anything about SQRL.  The problem is, of course, I code in MASM, and so I implemented the first one in assembly language.  We have a guy who's in the process of - I've given him my source, and so he's using that as a template.  And I also have a full spec which is online, and so he's using that.  Anyway, Paul is rewriting it in what will then be Open C, so it will be completely cross-platform, and anyone will be able to host a service API on their server in order to easily, trivially, add SQRL support to a server.



The point is that he's found several bugs in my code, and I've received a couple of pieces of email saying, Steve, if this happened here, wouldn't that just not return a response to the user?  And it was like, ooh, you're right.  So, I mean, I love having someone carefully rereading my own code because, as we know, it's very difficult to see your own mistakes.  But he's found a couple.  So I've been really grateful for that side effect also, even though it's not clear to me anyone will ever use mine because it's MASM.  But at least it's fixed.



LEO:  Hey, we offer a handy-dandy assembly language API for those of you.



STEVE:  That's right.



LEO:  Well, Steve, it's been fun.  And I don't mind 20 pages, 25, 30, it's always great.  Security Now!.



STEVE:  We just had a lot to - I threw a bunch of stuff out because there just wasn't time to get to it all.



LEO:  Yeah, some days there's a lot to say.



STEVE:  That's right.  Thank you, industry, for never giving us a boring podcast.



LEO:  You'll find copies of this show at Steve's site, GRC.com, along with transcriptions.  So if you like to read along while you listen, they're all there.  Along with SpinRite, the world's finest hard drive recovery and maintenance utility.  And SpinRite.  Did I say SpinRite?  I did say SpinRite.



STEVE:  Yeah.



LEO:  And SQRL, that's the other thing.  And a lot of other things.  In fact, it's a black hole.  You'll go there, and you won't come out for hours.



STEVE:  That's a black hole, right.



LEO:  Read everything you can.  It's fun.  GRC.com.  We have audio and video on our site, TWiT.tv/sn.  It's also on YouTube.  It's everywhere.  And, you know, the best thing to do would be get a podcast program, there are so many good ones out there, and subscribe.  That way you'll get every episode.  And if you want to go back in time, every one of the 707 episodes are stored at TWiT.tv/sn, so you can get them bit by bit.  And there are a number of scripts out there, I have a few on my blog, PowerShell scripts and so forth, that will suck the entire set to your server.  From our server to yours.  Thank you, Steve.  Have a great evening, and I'll see you next time on Security Now!.  Bye-bye.



STEVE:  Thanks, Leo.  Bye.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#708

DATE:		April 2, 2019

TITLE:		Android Security

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-708.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we are primarily going to share Google's well-deserved, self-congratulatory, but also very honest update on the status of Android Security at its 10th birthday.  But before that we're going to share some of the continuing news of the WinRAR vulnerability, some really interesting data on Russian GPS hacking, Android's April Fools' Day patches, Tesla autopilot spoofing, some follow-up on the ASUS "ShadowHammer" attack and the targeted MAC addresses, the final release of the Windows 10 (last) October 2018 update, a VMware update, a SQRL question, two bits of listener feedback, and a SpinRite development question.  Then we take a look at the state of Android 10 years in.



SHOW TEASE:  It was 10 years ago Google came out with the first Android phone.  Ten years later, they've come out with a security report.  Steve Gibson talks about the state of Android security.  And I'll just give you a little hint, a little spoiler alert.  He's pretty impressed.  Coming up next on Security Now!. 



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 708, recorded April 2nd, 2019:  Android Security.



It's time for Security Now!.  Yay.  The moment you wait for all week long.



STEVE GIBSON:  Woohoo!



LEO:  Woohoo!  Steve Gibson, he's been slaving over a hot keyboard, apparently making some funny faces, too.



STEVE:  Steve, Steve the reindeer.



LEO:  Do you work - I think you work all week preparing this show.  I know it's a lot of work.



STEVE:  I start yesterday, and I work - and Lorrie and I have this whole routine where, like, Monday is just set aside, and there's no dawdling Tuesday morning.  It's like I get right to it.  So, yeah, I do put a lot of time in.  But I think that's, I mean, I don't know how to do this, what we're doing now, any differently.  I want to look at all of the news of the week.



Today's big topic, when I started out I didn't realize that I was going to be as impressed as I was.  But last week Google, for like the 10th birthday, the 10th anniversary of Android, did, I think, a well-deserved, somewhat self-congratulatory, but also very honest update on the status, today's status of Android security.  And there was so much detail and so much meat in this - I think it was a 22-page document - that I thought, okay.  Our listeners know I'm pretty tough on Google and Android when I think something really needs to get fixed.  But I came away feeling differently.  I'm impressed by what they have tackled that is not a job I would ever want, you know, putting an open source powerful platform running on capable hardware in the hands of non-computer-savvy consumers who just want it to work in the face of a proactively extremely hostile environment.  I mean, we know, because we cover it all the time, how much pressure there is to get into and to subvert people's devices.  And looking at the details changed my opinion enough that I thought, okay, I have to share this.



So our main topic, which we will wrap up with, is Android security here on, you know, basically 10 years in.  Where are we?  But there was also a lot of other interesting news.  There was continuing news about the WinRAR vulnerability, including email that I just received this morning from WinRAR, which means that for the first time they got proactive.  Also some really interesting research that shows the extent of Russia's hacking of global positioning systems and what that means.  We did have an Android update yesterday on April Fools' Day, but it's no laughing matter.  It fixed a couple remote code execution vulnerabilities we'll touch on.



Also there was a very - a disturbingly mis-headlined report about Tesla autopilot spoofing.  It got picked up because it was hot, unfortunately, saying that researchers at Tencent had arranged to spoof Teslas into driving into oncoming traffic, which is not at all what they did, but that's what the headlines said.  So we have to fix that.  Also we have some really interesting follow-up on last week's discussion of ASUS's ShadowHammer attack and those targeted MAC addresses.  We've got the final release of last October's 2018 Windows 10 Update; a VMware update that followed from the Pwn2Own exploits that we covered.  We've got a SQRL question; two bits of listener feedback; a SpinRite R&D question.  And then we're going to spend a lot of time, because I think it deserves it, talking about what Google has achieved with 10 years of really concerted effort.



LEO:  I'm relieved because when I saw the title I thought, oh, crap.  Because I like Android.  I use Android.  In fact, while I have an iPhone X, I've been lately really in love with the brand new Galaxy S10 Plus.  It's just a great phone.  And I would be kind of sad if I thought I couldn't use it securely.



STEVE:  Well, and as we'll see, because there's a lot - they really did a statistical breakdown.  Staying with the major suppliers - if not Google themselves, then one of the other major partners - you are in a much better position because it still is the case, you know, not only are they being more responsible - we'll talk about that.  There's, again, lots of numbers in this report.  And I've, like, whittled it way down.  So I've just kind of - I skipped big chunks of okay, well, we don't need to talk about that.  But also their attention, the big partners' attention to detail and their commitment to pushing the patches out that Google produces.  So, I mean, it's very clear, even with something like an archiving piece of software, which we now wish had auto update capabilities, in the world we are in, stuff has to take care of itself.



LEO:  Yeah.  Yeah.



STEVE:  And so unfortunately, the nature of Google's Android ecosystem is there are many fringe partners who are like, oh, yes, we'll be happy to take that.  And then, you know...



LEO:  I like that, "fringe partners," yeah, yeah, mm-hmm.



STEVE:  Anyway, so, yeah, we've got a lot of fun stuff to talk about today.



LEO:  Good.  LastPass.com/twit.  I'm sorry, I get overexcited when I start talking about - I could go on and on and on.



STEVE:  Well, and I fully accept that passwords are never, and I really do mean never, they're never going to go away.  Look at all - we still have FTP.



LEO:  Yeah, yeah.



STEVE:  These things just don't die.



LEO:  No.  Tell me.



STEVE:  They'll become increasingly fringy.



LEO:  Right.



STEVE:  But still there will always be a place.  And I don't know any of my passwords anymore because they're all gibberish.



LEO:  Right.  They should be gibberish.



STEVE:  Like they're supposed to be, yeah.



LEO:  Like they're supposed to be, yeah.  And you know what, SQRL's going to help a lot.  We're going to get SQRL out there.  And the more sites that use SQRL, the better.  Would SQRL work on a mobile app, too?  Or does it have to be a website?



STEVE:  Oh, no.  We have iOS and Android apps that are, like, running right now and working great.



LEO:  But they work with websites.  I mean, if I had an app that needed a password, could I use SQRL to authenticate with the app?



STEVE:  Ah, yes, yes.  You are able to do that.



LEO:  Oh, nice.



STEVE:  Yes.



LEO:  Can't wait till you finish, Steve.  Hint, hint.



STEVE:  So, and I'll talk about where I am at the moment.  When we get there, I'll touch on that.  Our Picture of the Week, you asked me right off the bat, is this our Picture of the Week?  Because it's, like...



LEO:  It's not funny.



STEVE:  Exactly.  It's not funny.  Actually, Leo, on the Sophos website there is the most adorable kitten.  And I was so tempted to just put up this little kitten.  But I thought, okay, Gibson, you know, how much have you been drinking?  So I thought, no, I really can't do that.  But  I don't know what it was, it was just the most adorable thing.



LEO:  Now I'm going to have to go.  You're going to have to see it.



STEVE:  Oh, lord.  Anyway, so the reason this graph is significant is - so first of all, we will be talking later in the podcast about PHA.  I'll be using the acronym PHA because it's so much easier than saying Potentially Harmful Applications.  But that's like Google's term, their grab bag term for any crap of any sort you don't want on your Android device, PHAs, Potentially Harmful Applications.



What's, well, first of all what's interesting is that they acknowledge that there has been an increase in PHAs, even in the Google Play Store, 0.05% up to 0.08%, that was in 2016; 0.08 in 2017 and holding at that.  However, they also redefined, they added a large class of ad-click PHAs into that definition.  So they've revised the definition to include what they felt was a major harmful category, an unwanted application, which are these fake clicking things.  And so that's the reason for the increase.



But the big point here is that the outside of the Google Play Store, that is, in the wilds, where people tend to want to sideload things that are not available, the rate of potentially harmful applications is much higher.  What is it, like maybe 12, 13% higher.  I mean times higher.  So it's 0.74 as opposed to 0.05%.



LEO:  Although, you know, you tell me, oh, go to a third-party store, I'd think it's 5% or 10%.  I'm in some ways relieved it's less than 1%, anyway.



STEVE:  Yeah.  Yeah, yeah.  And although there are some other scary statistics that we will be getting to, like just in terms of, you know, it's still - certainly it's a "downloader beware" environment.  But you can be much less worried if you just get things from Google Play.  So, I mean, and the good news is I think most...



LEO:  There's no reason not to, really.



STEVE:  Exactly.  You know, everything you want will be there.  And Leo, even the SQRL app is there.



LEO:  Yay.



STEVE:  So, okay.  So this WinRAR vulnerability has turned out to be as big a problem as we originally expected it to be, only because it has such breadth of install base at half a billion downloads over time.



LEO:  Wow.



STEVE:  It has always been vulnerable.  It is still vulnerable.  There is no proactive update mechanism for it, which means it isn't going to fix itself.  And so right now the bad guys are in this environment where they're in a big panic when some new vulnerability on a mainstream platform comes out because they know their window of exploitability is short.  It's going to get fixed soon.  The machines are going to update themselves with little user interaction.  And then any investment that they make in creating an exploit for it has a very short time horizon before it just won't work anymore.  Not so with this.



So this has generated an outsized response from the hacker community, I mean, the malicious, I don't mean to use - I always try to say the attacker or the malicious hacker because we know not all hackers are bad.  But FireEye Security, who we haven't talked about for a long time, but they're still out there kicking, they did a summary of the things they had seen which I thought just was worth sharing with our listeners.  The title of their blog posting from their threat research group was "WinRAR Zero-Day Abused in Multiple Campaigns."  And I'm unsure this really counts as a zero-day any longer.  I mean, yeah, on Day Zero it did.  But then there was Day One, and then there was Day Two, and now we're like on Day, what, 20 or something.  But the point is I wouldn't say it's a zero-day any longer.  It did surprise us when it happened, but now everybody knows.



So they said:  "WinRAR, an over 20-year-old file archival utility used by over 500 million users worldwide, recently acknowledged a longstanding vulnerability in its code base.  A recently published path traversal zero-day vulnerability, disclosed in CVE-2018-20250 by Check Point Research, enables attackers to specify arbitrary destinations" - and actually that's true.  We've only been talking about putting it in the user's start folder.  But they actually are arbitrary, depending upon the permissions of the user at the time, which potentially allows further exploitation.



Anyway, "...arbitrary destinations during the file extraction of ACE formatted files, regardless of user input.  Attackers can easily achieve persistence and code execution" - which that's like the two golden things you want, persistence and code execution - "by creating malicious archives that extract files to sensitive locations, like the Windows Startup Start Menu folder.  While this vulnerability has been fixed in the latest version of WinRAR, WinRAR itself does not contain auto-update features, increasing the likelihood," they write, "that many existing users remain running out-of-date versions."  And really, I mean, who wouldn't, unless you were listening to this podcast, or you were following this, I mean, this is not going to make mainstream news.  WinRAR?  What?  So nobody's going to find out about this.



Then they said:  "FireEye has observed multiple campaigns leveraging this vulnerability, in addition to those already discussed by 360 Threat Intelligence Center.  Below, we will look into some campaigns we came across that used customized, interesting decoy documents with a variety of payloads including ones which we have not seen before, and ones that use off-the-shelf tools like PowerShell Empire," which is what they wrap up with.  Anyway, so there was Campaign 1, impersonating an educational accreditation council letter.  And they write:  "When the ACE file Scan_Letter_of_Approval.rar is extracted with vulnerable WinRAR versions lower than 5.70, it creates a file named winSrvHost.vbs in the Windows Startup folder without the user's consent.  The VBScript file is executed the next time Windows starts up.  To avoid user suspicion, the ACE contains a decoy document."



And notice it's an ACE file with a .rar extension.  That's what we're always seeing because someone's going to look at ACE and think, what the hey?  But as we know, WinRAR examines the header to determine the actual archive type independent of the Windows extension, the Windows file extension.  So as long as you have the 20-some things checked in WinRAR that it knows about, it will get control when you click on it, and it will open.  And certainly it's going to be in charge of a RAR file, even if it contains an ACE extension.



So anyway, they explain that this ACE file contains a decoy document, Letter of Approval.pdf, which purports to be from CSWE, the Council on Social Work Education, apparently copied directly from the CSWE website.  So someone just got the PDF, the legitimate PDF from the website, RARed it with a maliciously created winSrvHost.vbs file, and either targeted or spammed it or whatever.  And then they go into extensive detail I won't cover here about the way the VBS file operates, its access to a specific command-and-control server at 185.162.131.92 via HTTP requests, how the requests are handled, what information it pulls from the system to incorporate into the headers, and so forth.  You know, all of that is the stuff that these guys enjoy researching, but they're just an example of what one of these things does.  



It ends up installing a RAT, a Remote Access Trojan, known as NetWire, which is a well-known RAT, which supports a backdoor and commands.  It can receive the command "d," which will delete a file in the command parameters; "Pr" downloads a file from a URL and executes it; "Hw" retrieves hardware information; and "av" looks for antiviruses installed from within a predetermined list.  So just a relatively simple set of commands, but that allows the command-and-control server to see what AV you've got installed and then potentially alter its behavior based on what it knows or who it knows may be watching what happens next.  So again, this is something that they have found actively exploiting the current WinRAR vulnerability.



Second campaign was an attack on the Israeli military industry.  They said:  "Based on the email uploaded to VirusTotal" - which is where they found this - "the attacker sends a spoofed email to the victim with an ACE file named SysAid-Documentation.rar as an attachment."  They said:  "Based on the VirusTotal uploader and the email headers, we believe this is an attack on an Israeli military company.  The ACE file contains a collection of decoy files related to documentation for SysAid, a help desk service based in Israel.



"One of the files, Thumbs.db.lnk, is a valid Windows shell link file pointing to C:\Users\john\Desktop\100m.bat.  But the icon for this link" - and John is probably the name of the user who downloaded it, so it created this thing.  The icon for the link - or maybe it wouldn't even need to actually be able to resolve it because the icon for the link is remotely hosted on one of the command-and-control servers.  What's clever about this is that Windows will reach out to obtain the icon; and it will include, as we've often discussed, when you do file and printer sharing, it sends NTLM, NT LAN Manager hashes in order to assert the identity of the system that is asking.  So this is a way for this command-and-control server to steal the LanMan hashes.



Then, upon extraction, the WinRAR flaw causes a previously unknown payload that FireEye found and named SappyCache to be copied to the user's Startup folder with the filename ekrnview.exe.  And they explained the payload will be executed the next time Windows starts up, of course, because it's in the Startup folder.  SappyCache attempts to fetch the next stage payload using three different approaches.  And I'll quickly say it decrypts a file to a specific location and then tries to - the malware reads a file, decrypts it using RC4 to get a list of command-and-control URLs.  Otherwise it decrypts a resource from an executable if the first attempt was not successful, retrieving, again, a set of command-and-control URLs.



And then, if neither of those work, then it retrieves the command-and-control URLs using a payload from four different hard-coded URLs, which are determined at runtime.  It grabs the computer's name, the version of Windows being used, then enumerates all the processes running in the system and sends those to the command-and-control server so that it can see everything about what the user is running and then decide what it wants to return as the second-stage payload based on what it finds running.  Which is more what we're seeing now is that, with AV being so present, it may want to pick and choose its response based on what it knows about the system that the user is running at the time.  So clever, from that standpoint.



The third attack was targeting individuals in Ukraine which they were able to specifically determine from what they saw.  And the fourth campaign was a credential and credit card dump.  It actually used those as decoys.  So it looked like this is the classic "there's no honor among thieves."  So this would have been posted somewhere where other nefarious people would be looking to obtain credential dumps and credit card credential dumps.  Those were RARed with a malicious RAR that would then infect the user's machine with the QuasarRAT, the Quasar Remote Access Trojan.



So anyway, FireEye concludes their report and their analysis by saying:  "We have seen how various threat actors are abusing the recently disclosed WinRAR vulnerability using customized decoys and payloads, and by using different propagation techniques such as email and HTTP queries."  They said:  "Because of the huge WinRAR customer base, lack of auto-update feature, and ease of exploitation of this vulnerability, we believe that this will be used by more threat actors in the coming days."



They said, and this is what I found really interesting in their report, too:  "Traditional AV solutions will have a hard time providing proactive zero-day detection for unknown malware families."  I was assuming that AV would be able to look inside of RARs to detect the abuse of this path traversal attack.  But that may not be feasible, which I found interesting.



They said:  "It's also worth noting that this vulnerability allows the malicious ACE file to write a payload to any path if WinRAR has sufficient permissions.  So although the exploits we've seen so far choose to write the payload to the Startup folder" - which of course is always guaranteed to work, and so why look any further - they said "...a more involved threat actor could come up with a different path to achieve code execution so that any behavior-based rules looking for WinRAR writing to the Startup folder could be bypassed."  That's a good point.  You could watch the behavior of WinRAR and say, wait a minute, there's absolutely no reason that WinRAR should be writing to the Startup folder, and so block based on that behavior.  They said:  "Enterprises should consider blocking vulnerable WinRAR versions and mandate updating WinRAR to the latest version."



And then, as I said, this morning my inbox contained a letter from Julia D. Seymour at Win-Rar.com with the subject line:  "Update WinRAR and get Malwarebytes Premium FREE!"  All caps on FREE, exclamation point.  She said:  "Dear Customer," meaning me because I'm a registered WinRAR user, of course.  I want to support them.  "Greetings from WinRAR.  You may ask yourself why we are contacting you at this particular time.  We wouldn't usually contact our users individually, but these are extraordinary circumstances.  We have recently released the new version of WinRAR 5.70, following the discovery of a potential security vulnerability within the unacev2.dll.  For more information please check here," and she gives a link.



"Here at Win.Rar GmbH, we believe in full transparency, which is why we are contacting our users personally to explain and offer advice regarding their continued safe use of WinRAR.  We always recommend that users update to the latest version to remain risk-free and to have full access to all of the current improvements, additions, and bug fixes.  As a WinRAR license holder, we wanted to make sure that you are aware that you can upgrade to the latest version free of charge.  You will find the latest version of WinRAR 5.70 in your desired language here."  And then they give a link.



"In addition to that, we are offering you Malwarebytes Premium for free."  And there's an asterisk that we'll explain in a second.  They said:  "Download the most current version from here" - www.malwarebytes.com/mwb-download/thankyou - "and insert the license key by clicking the 'Activate License' button in the top menu bar of the software."  And she said:  "Here is your Malwarebytes registration key."  And I blanked it out of the show notes because it's not clear to me whether that was a per licensed user or whether it's blanket, and everyone got the same one.  But in any event, to respect them, maybe they have a deal, and I didn't want to just spew this around.  Also it's not lifetime.



So then she says, under WINRAR Upgrade:  "Upgrading is quick and easy.  No complicated uninstalling of the previous version is necessary."  Which I had verified and told our listeners about several weeks ago.  "Do not delete your existing WinRAR program folder.  Your registration information and WinRAR settings will be kept then.  Close all open WinRAR archives and exit WinRAR before installing.  Then you can just install the new version of WinRAR over your older installation by double-clicking on the EXE file you've downloaded.  Now you're ready to continue using the best compression tool around.  We would also recommend that you sign up to receive our newsletter," blah blah blah.



Anyway, the point is that the asterisk brings us down to a little, in very fine print at the bottom of the email:  "Using the provided license key, you will receive Malwarebytes Premium for free for a period of three months."  And I think that's entirely appropriate.  Their concern is that something may have crawled into your computer through your use of WinRAR, which you had no way of knowing to update.  If you were licensed and supporting them, and you had kept your email address from the time you licensed, you would have received this today.  And they're giving you for free, in return, a useful AV with which to scan your system for any damage that may have been caused by this.



So props to them.  I think they've acted as responsibly as they possibly could.  And certainly all of our listeners have been protected for weeks because we knew about this the day it happened.  So I think this probably closes the chapter.  Unless there's some massive widespread worm or exploit or something that happens that brings it back into the news, we've pretty much beaten this thing to death.



Russia has been messing with Global Positioning Systems.  It's a 66-page detailed and extremely compelling analysis of signals intelligence collected from fixed and in-orbit assets, as we will see in a second.  So I'm certainly not going to go through 66 pages.  But I will share the executive summary because that gives us a good overview and summary.  And I just sort of wanted to plant a little more detail than just saying, oh, yeah, there have been anecdotal reports of GPS being blockable or spoofable, but we don't have any details.  These guys do.



And for anyone who's interested, I can certainly recommend this PDF.  It was from Sophos, and the PDF is on their site.  So I'm not sure it's going to be available forever.  So if anyone is interested, you may want to go to the show notes.  Or maybe the - the paper is titled "Above Us Only Stars," with the subtitle "Exposing GPS Spoofing in Russia and Syria."  So presumably googling that expression would take you to this also.



So they said, in their executive summary:  "GPS and other Global Navigation Satellite Systems" - collectively GNSS, Global Navigation Satellite Systems - "are used in everything from cellular communication networks to basic consumer goods, high-end military systems, and stock trading inputs.  But these systems," they write, "are vulnerable.  By attacking positioning, navigational, and timing data through electronic warfare (EW) capabilities, state and non-state actors can cause significant damage to modern militaries, major economies, and everyday consumers alike.  With recent technological advances, the tools and methodologies for conducting this interference are now at a high risk for proliferation.  GNSS attacks are emerging as a viable, disruptive strategic threat.



"In this report, we present findings from a year-long investigation ending in November 2018 on an emerging subset of electronic warfare activity:  the ability to mimic, or 'spoof,' legitimate GNSS signals in order to manipulate PNT" - that's the timing data, the PNT data which is how positioning data gets spoofed.  They said:  "Using publicly available data and commercial technologies, we detect and analyze patterns of GNSS spoofing in the Russian Federation, Crimea, and Syria that demonstrate the Russian Federation is growing a comparative advantage in the targeted use and development of GNSS spoofing capabilities to achieve tactical and strategic objectives at home and abroad.  We profile different use cases of current Russian state activity to trace the activity back to basing locations and systems of use."



So there's four sections, which they summarize.  "In Section One," they say, "we examine GNSS spoofing events across the entire Russian Federation, its occupied territories, and overseas military facilities.  We identify 9,883 suspected instances across 10 locations that affected 1,311 civilian vessel navigation systems since February of 2016.  We demonstrate that these activities are much larger in scope, more diverse in geography, and longer in duration than any public reporting suggests to date. 



"In Section Two we examine the role of Russian GNSS spoofing for very important person (VIP) protection.  We find a close correlation between movements of the Russian head of state and GNSS spoofing events.  We believe the Russian Federal Protective Service (FSO) operates mobile systems to support this activity.  Through a review of Russian procurement data, we identify one possible mobile system, manufactured by a company closely connected to the FSO (Federal Protective Service).



"In Section Three we profile the use of Russian GNSS spoofing for strategic facilities protection.  We identify potential technology in use for facility protection in Moscow.  We also highlight spoofing activities taking place in proximity to protected facilities on the coast of Russia and Crimea in the Black Sea.  Through a line-of-sight analysis, we judge the most likely placement for a GNSS spoofing transmitter on the Black Sea to be at a multimillion dollar 'palace,' formerly owned by reported family members of senior FSO officers and previously reported to be built for President Putin.



"Finally, in Section Four, we expose the use of GPS spoofing in active Russian combat zones, particularly Syria, for airspace denial purposes.  This is a capability scarcely reported in the public domain.  Using data from a scientific sensor in the International Space Station, we are able to identify ongoing activity that poses significant threats to civilian airline GPS systems in the region.  We pinpoint the most likely location for the system to be the northwestern quadrant of Khmeimim..."



LEO:  I don't think that's how it's pronounced, but I might be wrong.



STEVE:  Well, K-H-M-E-I-M-I-M.



LEO:  Yes,  Khmeimim, right.



STEVE:  "...Khmeimim air base, and identify potential military grade electronic warfare systems in use through publicly available information."  And then they conclude their summary, saying:  "The Russian Federation has a comparative advantage in the targeted use and development of GNSS spoofing capabilities.  However, the low cost, commercial availability, and ease of deployment of these technologies will empower not only states, but also insurgents, terrorists, and criminals in a wide range of destabilizing state-sponsored and non-state illicit networks. GNSS spoofing activities endanger everything from global navigational safety to civilian finance, logistics, and communication systems."



So anyway, I just thought that was very interesting.  Again,  if anyone is interested, if you browse through this PDF, you will come away convinced.  They've got diagrams, charts, pictures, annotations, I mean, a huge body of clear evidence to back this up.



LEO:  Now, the Russians use their own version of GPS called GLONASS.



STEVE:  Correct, yes.  There are four different positioning systems operating worldwide.



LEO:  But they're also attacking GPS, as well.



STEVE:  Yes.



LEO:  Yeah, okay.



STEVE:  And China also has their own.



LEO:  Right.



STEVE:  So everyone has built their own because they don't trust anybody else.  So it's like, okay.



LEO:  Right.  Well, and originally the U.S. military didn't let ours be used by them, either.



STEVE:  Right.  And remember that ours also had its resolution blunted.



LEO:  It's fuzzed, yeah, yeah.



STEVE:  Yes, for consumer purposes, or civilian purposes, yeah.



LEO:  Yeah.  So, interesting.



STEVE:  Anyway, so this is going on.  I just kind of wanted to put this on our listeners' radar.  And what basically they're saying is that the collapsing cost of producing transmitters capable of confusing GPS and other positioning systems means that it's going to become more prevalent in the future.  So it was amazing, decades ago.  And it was sort of the province of high end.  That's not the case anymore.



So, as we were saying, yesterday was April 1st, infamous April Fools' Day.  But no one was fooling here.  Just wanted to note that Android users should update or look for updates from their provider because there were a pair of critical remote code execution vulnerabilities, and nine high-severity privilege elevation vulnerabilities, and also an information disclosure vulnerability, all patched.  They were, once again, the RCEs, Remote Code Execution problems, were in the much-troubled Media Framework, which of course has been a constant source of trouble because it is a massive interpreter, and we know how hard those are to get right.



So there were two vulnerabilities.  What were updated is versions 7.0, 7.1.1, 7.1.2, 8.0, 8.1, and 9.  So everything essentially from 7.0 on.  Depending upon where you get your Android, do, as it was just released yesterday, update yourself because, again, what we have seen is that a patch gets reverse engineered, and the bad guys jump on it.  And we know that the Media Framework is particular susceptible because essentially your Android mobile device is a wide-open maw, a funnel, looking for things, tweets and Snapchats and Twitter pictures and just everything coming into it.



And if there is a problem in the renderer of some type of content, then it's readily exploited.  And the bad guys are going to look at this, and they started yesterday, and they're going to try to get people who haven't updated.  So do so.  They did say of them that there were no reports of active customer exploitation or abuse of any of these newly reported issues.  So none of these are zero-days.  But we know that even one-days is now, these days, enough.  So worth getting fixed.



Okay.  Now, Leo, as a Tesla owner, this will be of interest to you. 



LEO:  Yes.



STEVE:  And I'm sure we have many Tesla owners.  The attention-grabbing headline which is very, very wrong was "Researchers Trick Tesla to Drive Into Oncoming Traffic."



LEO:  Now, that would not be a good thing.



STEVE:  Really.



LEO:  I would prefer not to, thank you.



STEVE:  In terms of ruining your day...



LEO:  Yeah, yeah, pretty high up on the list.



STEVE:  And unfortunately, in this case, the hack appears to have been easy to pull off, but not at all what the headlines have said.  There is a 40-page research paper published by researchers at Tencent Keen Security Lab.  Their paper was titled "Experimental Security Research of Tesla Autopilot."  And I hadn't - the pun of "autopilot" hadn't occurred to me, actually, Leo, until I began...



LEO:  Oh, auto.  I get it.



STEVE:  Isn't that wonderful?



LEO:  I never thought of that either.



STEVE:  Isn't that wonderful?



LEO:  I don't like the name because it implies it flies itself, and it doesn't.



STEVE:  No.



LEO:  So it's a bad name.



STEVE:  And I will argue, and our listeners may be a little more - even you may be a little more convinced of that by the end of this because they did find something which is worrisome. But anyway.  So their abstract reads - and I'll share it because they did three different things.



The abstract reads:  "Keen Security Lab has maintained the security research work on Tesla vehicle" - this is a Chinese outfit, by the way, so you'll see that their English is not quite ours, but still very legible, or intelligible - "on Tesla vehicle and shared our research results on Black Hat USA 2017 and 2018 in a row.  Based on the root privilege of the APE" - that's Tesla Autopilot ECU, software version 18.6.1.  And we should note it's now at 18.25 or something, so this is somewhat dated - "we did some further interesting research work on this module.  We analyzed the CAN" - and we know that that's the private bus that ties everything together.



They said:  "We analyzed the CAN messaging functions of APE and successfully got remote control of the steering system in a contact-less way.  We used an improved optimization algorithm to generate adversarial examples of the features," and then they said here, "auto wipers and lane recognition which make decisions purely based on camera data, and successfully achieved the adversarial example attack in the physical world."  I know what all those are, so I'll explain that in a second.  "In addition, we found a potential high-risk design weakness of the lane recognition when the vehicle is in Autosteer mode.



"The whole article is divided into four parts:  First, a brief introduction of Autopilot.  After that we will introduce how to send control commands from APE to control the steering system when the car is driving.  In the last two sections, we will introduce the implementation details of the auto wipers and lane recognition features, as well as our adversarial example attacking methods in the physical world.  In our research, we believe that we made three creative contributions:  One, we proved that we can remotely gain the root privilege of APE and control the steering system.  Two, we proved that we can disturb the auto wipers function by using adversarial examples in the physical world.  Three, we proved that we can mislead the Tesla car into the reverse lane, meaning oncoming lane, with minor changes on the road."



Okay.  So first of all, they did succeed in taking over steering.  And just for the fun of it, they used a Bluetooth-connected gamepad controller. 



LEO:  Oh, god.



STEVE:  To steer the Tesla.  So, yeah.  It turns out that for the second point, this auto wiper, the Tesla, whereas other cars like mine use a moisture sensor, essentially an inductive moisture sensor, to sense the presence of water on the other side of the glass windshield, Tesla, since they already have forward-looking cameras, they thought, well, let's, you know, we're looking out through the windshield.  We should be able to sense if there's drops of water there.  And so what's what Tesla does.  So they have an algorithmic auto wiper technology.  But it turns out that you can spoof it using deliberately created images at a distance.  And so they put up some poster boards with funny-looking stuff on it, and the windshield wipers went.  So it's like, okay.  So they were spoofing the AI algorithm after...



LEO:  Actually, I could use that because my windshield wipers never work properly.



STEVE:  Oh, well, yes.  And so maybe the algorithm needs a little more attention.



LEO:  Yeah, maybe, yeah.



STEVE:  Okay.  And so then the third thing they did, obviously most worrisome, was by reverse engineering and understanding the autopilot's lane recognition algorithm, they were able to induce a lane change to the left which, in a two-lane road scenario, would have been into oncoming traffic.



LEO:  Right.



STEVE:  And all that was necessary, perversely, was the strategic placement of three large dots on the road ahead of the car.  So just sort of like - it looks sort of like as if it was - I could see it would be interpreted maybe as the side of the road moving to the left; and the car said, oh, I need to change lanes, and so it went to the left in order not to hit an obstruction.



LEO:  Well, I mean, this is like Roadrunner and Wile E. Coyote, painting the lines on the road to go off the cliff.  If you're not paying attention, and you let the car follow the lines in the road, yeah, it's going to go off the cliff, of course.



STEVE:  Well, these were three small dots, though.  I mean...



LEO:  Well, that's what I want to see is, well, what did it look like?



STEVE:  Yeah.  So but here's the point.  And this is what unfortunately the people who didn't read the article, but just wrote the headline, or maybe who just wanted to say, ooh, we're getting a lot of clicks on this...



LEO:  I think the latter, yeah.



STEVE:  Yeah.  This does not mean, I have this in capital letters in my notes, NOT mean that a Tesla would actually turn into oncoming traffic, but rather that, in this carefully crafted and isolated instance, they were able to induce the car to redirect into the lane to the left.



LEO:  Yeah, yeah.



STEVE:  That's different than saying...



LEO:  Oh, yeah, turning into traffic.  



STEVE:  Yes.



LEO:  No.



STEVE:  For example, I would be shocked if the AI didn't have, it must have...



LEO:  Oh, if there's oncoming traffic, it's not going into that lane.



STEVE:  Yes, multiple other sensors that would - and input to expressly forbid it from actually turning into oncoming traffic.  So the headlines did Tesla a big disservice.  But there is an important message here, I think, nevertheless.  A car's autopilot really is an extremely complex interpreter of the car's sensorium.  I mean, it's an interpreter.  And how many times have we talked about how interpreters can be deliberately fooled by malicious actors who have access to the interpreter's internals?



That's exactly what happened here.  Within an isolated environment, with preset conditions, the Tesla's autopilot was fooled, in a limited test case, by something that would never have fooled a human driver.  So we would think, huh?  I wonder why there are three weird dots on the road.  Whereas the Tesla AI, seeing the same thing, in that version - and we don't know about today's AI, we know about the one they tested - apparently thinks, oh, time to change lanes to the left.  Absent any inhibitory input which it probably also would be looking for that would say, ooh, but except that there's this oncoming lane of traffic, so I'm going to ignore the three dots.



LEO:  Actually, I suspect the Tesla would do better in this environment than a lot of - so lane keeping is something a lot of cars have.  And because Tesla has all this other self-driving technology, including radar and other cameras, I bet you it's less likely to follow those dots than many other lane-keeping vehicles.  But no matter what, Tesla does a lot to make sure you're paying attention and have your hands on the wheel.



STEVE:  Yes.



LEO:  In fact, if you don't see the alerts that say keep your hands on the wheel after five minutes, it disables Autosteer and says you don't deserve it for the rest of the trip.  It gives you - it spanks you with a big red notice and says, "We're turning it off.  Sorry, buddy."



STEVE:  You have abused the privilege.



LEO:  And I don't need to be told that because honestly I don't trust it.



STEVE:  Unh-unh.  Good.  Because I really do, I do enjoy doing the podcast, Leo.  And I want to run out all three digits of the numbering system here.



LEO:  We will, I promise.  But you don't trust it.  You keep your hands on the wheel.  You keep your eye on the road.  You're paying attention, just in case, because it is just trying to figure out where the lane is.  It's going to make a mistake sometimes.  And it's happened to me in the three years I've had this Tesla, the Model X, more than once, that particularly on one curve that we go around on the way to San Francisco, that it's started to veer over into the guardrail.  And of course I'm expecting it now, so I pull it back.  But it's just that doesn't surprise me that it can be fooled by weird lines on the road.  That doesn't surprise me.



STEVE:  Yeah.  You know, I really - I come back to the Palm Pilot and how the brilliance of Jeff's design for that was that he asked the user to accommodate the alphabet a little bit.  And that just allowed the recognition to nail what the user was doing.  And the analogy here is I really think we ought to move to a mode where our - because this seems to be where we're headed with this kind of car technology.  Our roads ought to have some car assist stuff.  And I think it's foreseeable that it's going to happen where, for example, Tesla must have some informatics feedback where they know, in some database somewhere, that people are having to pull their car back onto the road at that turn because yours isn't the only Tesla, I'm sure, that tends to do that in that location.



LEO:  Yeah, yeah.



STEVE:  And so it would make sense for either it to be built into the car's database, a better awareness based on knowing where it is, which probably it's accumulating over time; or maybe let roads that are less traveled have some sort of way of providing that feedback to the car.



LEO:  Cadillac CT5 will not let you use Autosteer except on highways it has mapped. 



STEVE:  Ah, okay.



LEO:  I think for that reason.  I mean, everybody's trying to solve this.  Also the Cadillac, the way the Tesla works, it's the torque on the steering wheel.  It has a torque sensor.  So you have to kind of slightly move the steering wheel to let it know you're holding onto it.  Cadillacs use a capacitive sensor, so just your touch is enough.  And they have a camera watching your eyes.  And if you look away from the road, it rumbles your seat vigorously.  So really they - I think, look, two 737 MAXes crashed because apparently the auto stall feature that was supposed to pull the nose down did it incorrectly, and pulled it down into the ground.



STEVE:  Right.



LEO:  It's a very similar problem.  And pilots who didn't know enough to disable it, that's what happened.  So I think it's just - autopilot is always going to need, at least for a while, anyway, human intervention.



STEVE:  Well, and consider the lawsuit.  I mean, there's just no way these car companies are not needing to be able to say we took proactive measures for this to only be an assist function, not a "roll up in the back seat and take a nap while we drive you to work" feature.  So, yeah.



LEO:  Yeah.  A really interesting topic.



STEVE:  Yeah.  So, oh, this is a classic hack.  Of course we talked last week about ASUS's ShadowHammer MAC addresses, well, the ASUS ShadowHammer attack, how two of their download servers were infected with multiple versions of malware over a duration of five months, presumably by someone who got an advanced persistence presence in their system and was able to do this.  In reporting I did note that they were only laptops.  So that's significant because remember that one of my - as I was scratching my head, brainstorming, where could a list of MAC addresses have been resourced, one of them was from WiFi hotspots in a mobile scenario.



LEO:  That's right.  They see the MAC addresses, don't they.



STEVE:  Yes.  Mobile hotspots get the MAC addresses.  And the other interesting thing was it turns out there was a list, a further refined list of double MAC addresses where it was the LAN and the WiFi MAC address which was known.  So I don't know what that further tells us, but that would potentially...



LEO:  That's interesting.  That confirms that, I think, yeah.



STEVE:  One of my hypotheses is that they had seen them roaming.  They knew who they were.  And so they were going to come back and get them.  Okay.  So anyway, as I described last week, what Kaspersky did was they offered an online resource where people could put their MAC addresses in, and it would tell them whether they were of those 617, I think it was, addresses; or a downloadable tool.  If you didn't want to put your MAC address into Kaspersky's page, you could download a standalone EXE that contained them all.  Well, okay.  So get this.  For whatever reason, they chose not to publish their full list of MAC addresses; right?  It was submit it to us, and we'll tell you; or download this EXE.  Well, they obscured the MAC addresses by hashing them with a salted hash, a salted SHA-256 with a complex algorithm that merged the MAC addresses and the salt several times in the hash in order to make it, you know, they just made up their own hashing function, essentially.



Well, this apparently bothered some guys at an Australian security firm, Skylight Cyber.  They wrote:  "The question of who did this and why is intriguing, but not one we were trying to answer in this case.  First things first.  If information regarding targets exists, it should be made publicly available to the security community so we can better protect ourselves.  Kaspersky have released an online tool that allows you to check your MAC address against a database of victim MAC addresses, which is hidden.  Good on Kaspersky, on one hand.  But on the other hand" - And "good on," of course, they are Australian.  Good on Kaspersky.  "But on the other hand, this is highly inefficient and does not really serve the security community.



"So we thought it would be a good idea to extract the list and make it public so that every security practitioner would be able to bulk compare them" - that is, the whole list - "to known machines in their domain.  If you are interested in the list, it can be downloaded here or here for the extended list."  And I have a link to this page in the show notes where those "here" and "here" are links to the extended list.  And I also have that actually down below.



So these guys also felt that having a simple list of targeted victim MAC addresses would be far more useful for large enterprises with many hundreds of thousands of systems where the stakes were pretty high.  Because, after all, we're talking about the reliable installation of a trojan backdoor by unknown actors into specific laptops, when who knows whose.  You know, specific ASUS laptops.



So how do we solve this problem, that is, the problem that these guys faced?  Well, of course it's a variation of the classic brute force password cracking problem.  Although it's significantly simplified because in this case we know that every test MAC address is a 48-bit binary input to the cracking hash function.  And we know that half of it will be one of a handful of 24-bit vendor MAC prefixes within the 48-bit binary.  So it's like a password whose length we exactly know.  And in fact half of it is one of a subset of possible 24-bit chunks.



So the Skylight Cyber guys calculated that their own fastest - oh, first of all they reverse engineered the algorithm because there it was, sitting in an EXE.  They used IDA, the interactive disassembler.  It'll be fun when in the future we start hearing about them using the NSA's tool, but that'll take a while to proliferate through the ecosystem.  They figured out exactly what the hashing function was.  They then designed - they took Hashcat and tried to use it, but the function was custom.  So they customized and built a custom version of Hashcat to reverse the Kaspersky hash functions.  Their own fastest system running Hashcat, they figured, would require about 162.5 days to extract all of the MAC addresses from Kaspersky's offline checking tool.



So they thought, hmm.  Let's hire out.  They did the entire job in less than an hour by renting an Amazon AWS p3.16xlarge instance, which comes equipped with eight Nvidia V100 Tesla 16GB GPUs.  So they figured out the algorithm using a reverse disassembler, verified it using Hashcat, realized, okay, 162.5 days, too long.  So they simply reimplemented it under an Amazon AWS instance and broke down 583 of the total of 619 it was, MAC addresses in less than an hour.  There is a beautiful list that they linked to, and I have linked to in the show notes.  Just it's numerically sorted, right down the left-hand page, 583 lines, every MAC address that was being targeted.



So now we all know.  And they're guessing that the reason they're missing a few is that they did take advantage of every trick in the book, that is, to shorten the guessing time, they were only guessing the known handful of 24-bit addresses.  It would have taken substantially longer, but certainly not impossible, obviously.  And also at much more expense.  I don't know what the renting Amazon's AWS instance with the eight Nvidia V100 Tesla 16GB GPUs would have cost.  But running that thing so that it's smoking for any substantial period of time probably would have run up the credit card bill.  So anyway, 583 out of 619.  We now know what they are.  And just a tip of the hat for these guys pulling off what is kind of a cool hack.



I did want to mention, just come back to noting that we are officially, as of yesterday - oh, no, I'm sorry, last Thursday it was - Microsoft announced that they are now designating Windows 10 October - that is, last October - 2018 Update Build 1809 as ready for broad, rather than its previously targeted, deployment.  Their Windows as a Service evangelist John Wilcox said:  "Based on the data and the feedback we've received from consumers, OEMs, ISVs, partners, and commercial customers" - in other words, nobody said no - "Windows 10 version 1809 has transitioned to broad deployment.  With this, the Windows 10 release information page will now reflect Semi-Annual Channel (SAC) for version 1809."  He said:  "We will continue to communicate for future releases the transition from targeted to broad deployment status."



And then just a very quick note that I just got a kick out of the fact that, not coincidentally, VMware has released a bunch of fixes for their VMware software.  When I saw that, I fired up my copy of VMware.  I am on v14.1 point something.  And sure enough, it came up and said, oh, we've got updates for you.  Would you like them?  They're free.  And I said yes.  And so I am now on whatever it was on the other side of that.  And of course, as we know, the Pwn2Own guys did find some rather sobering problems with the currently most recent updates of VMware.  So the good news is that has been fixed.



Okay.  Three quick things, or four, I think.  Steve Watson in the U.K. asked of SQRL adoption.  Oh, actually today, April 2nd.  He wrote:  "Hi, Steve.  I've been listening to the Security Now! podcast for a few years now and think it's great.  Thank you for the continued effort you put into producing it.  Makes my journeys to work a lot less boring.  I recently set SQRL up on my phone and computer and think it's great.  I just wondered how you're going to get large companies to adopt this technology."



And so for what it's worth, A, it's free.  It's way better than anything else.  It is multiplatform.  All platforms are supported.  It's easy to deploy.  And the earliest response I received when I started talking about that to our listeners was from enterprise.  Enterprise has a, you know, they're a more or less closed ecosystem.  They really want a solution to allow essentially a single sign-on sort of thing for their employees.  So the idea that they could adopt this for their own enterprise websites, which they absolutely can, rather trivially, and then all their employees can use a zero-cost, extremely secure solution, that's compelling.  And so I expect that we're going to see enterprise adoption happen as, like, on the leading edge of this overall.



Seth G. in North Carolina, he said:  "'The Expanse,' oh, wow."  And actually his is representative of many pieces of feedback I got after just reminding our listeners about it, or mentioning that it was on Amazon prime.  Now, I don't know where Seth lives.  Well, we do know where he lives.  He's in North Carolina.  I don't know where he works, and I don't want to know, and I'm glad I don't know his last name because I don't want to get Seth in trouble.  He said:  "Steve, things have been slow at work, and I'm blitzing through 'The Expanse.'"



LEO:  Maybe he's a security guard.  It's okay.



STEVE:  That's right.  "I'm blitzing through 'The Expanse' on Amazon Prime."  He says:  "This is sci-fi at its best.  Thank you for doing us a public service and letting us all know about this series.  It makes me want to read the books, and I rarely get to read more than manual pages and whitepapers these days.  All the best for you as we go into the 700s on SN, and congrats on nearly finishing SQRL.  Hopefully we will see some more SpinRite soon, like you've been talking about on the podcast.  Kind regards, SG in North Carolina."



And I should mention of SQRL, where I am at this instant is I'm just, like tonight probably this may happen, I'm wrapping up the fixed content that the site needs, separate from all of the user interactive stuff which will give the site its life, so the how do I start, the user Q&A.  What I'm finishing up is the "what if" section.  It occurred to me some time ago, and I've said so on the podcast, that I realized at one point I now had an answer for every possible "what if" question anyone could ask.  But what if this?  But what if that?  But what if this?  But what if - anyway.  So I created a thread in the forum to solicit everybody's, no matter how hare-brained they are, you know, what if I die?  That's not so hare-brained.  We've talked about that problem on the podcast.  What if my parents die?  What if my dog eats my printed copy of my identity?  I mean, anyway.



So not all of them made the cut.  But I have 40, 40 different, like, every possible variation of what could go wrong.  And so what I did was I wrote up all the answers, then I posted it to an open community editable Google Doc, which they've now been attacking for about five days, while I wrote a translator to turn that into the funky Q&A expandable/contractable format that XenForo uses.  And I finished that.  So I will return to the Doc.  I will edit everyone's comments and edits down into a final version, or at least v1, then run it through my translator to convert it into this wacky BB code stuff, and then post it.  And at that point the fixed content for the forum is done.  And then I turn my focus on GRC's very old original web pages, bring them current.



And meanwhile, in the background, I should say that a lot of great work is happening on the three other main client platforms - iOS, Android, and the web extension.  So those are quickly getting - they're working, but they're not yet feature complete.  And it's confusing people who use them, like wait a minute, I created my identity here.  Why can't I export it?  It's like, oh, well, that feature's not yet there.  My Windows client, of course, had the advantage of coming first, and where all of this was prototyped.  So it's been feature complete for quite some time.  So anyway, we're getting very close.  And people are universally loving it, so that's fun to see, too.



Fabio Esquivel in Costa Rica, his subject was "Alternate PDF Reader."  He said:  "Hi, Steve.  I wonder what do you use or recommend as an alternate PDF reader, hopefully free, but otherwise affordable."  He says:  "I don't want to pay Adobe for its insecure and bloated software.  So I've been using Sumatra PDF for years now.  But it seems abandoned by the developers.  Last update was in August 2016, and I fear it's subject to recent vulnerabilities that go on unpatched.  Thanks for your suggestions."



Well, he's not going to like what my solution is, and we know I'm something of a dinosaur.  On the other hand, I'm not opening PDFs like in email ever.  I just don't.  So what I'm using is the very - and I also like using sort of more authentic PDF.  And I should also mention, just to be fair, our browsers now contain built-in PDF readers.  Whatever Google is using in Chrome to view PDFs, that's going to be state of the art.  That's going to be secure.  And Firefox opens PDFs for me, too.  And I don't know that I have installed a third-party reader.



But I'm using Acrobat, or Adobe Reader 9.5, which is old, but it's real Acrobat, so it never - there's never anything in a PDF that it doesn't know how to open.  And of course I do not have it associated with my browser.  I have JavaScript disabled in it.  So I've tightened it down so that it's not able to do the bad things that a default wide open Adobe Reader could do back then.



And many people don't know, but Adobe maintains an FTP server, speaking of FTP.  I put a link in the show notes.  But if anyone is listening and is curious, in your browser, since our browsers will still allow us to browse FTP, type ftp://ftp.adobe.com/pub/adobe/.  Now, that will take you to Adobe's product list.  And in there is Reader; and in there are all the platforms, Mac and Windows and so forth; and in there are all the versions.  And you can get 9.5, and it doesn't need a license.  It's freely available.  It's real Adobe.  And anything after that is get on the signup and be a subscription.  And I just don't roll that way.



So on one hand I would just use the reader that's built into your browser, which is certainly being maintained by both browser vendors, Chromium and Firefox.  And I assume Edge has the ability to read PDFs, too.  And so that's probably being maintained.  If you like to poke around on Adobe's FTP server, as I imagine some of our listeners may, it's there.



And, finally, SpinRite.  David W. Roscoe in North Andover, Massachusetts.  Subject is "Becoming a SpinRite Volunteer."  He said:  "Hello, Steve.  I'm the person who volunteered the testimonial about using SpinRite to repair my brother's desktop music studio which you read in SN-707."  He says:  "I thought I heard you say in a recent Security Now! episode that you were interested in acquiring more volunteers for testing the next versions of SpinRite.  If this is true, then I hereby declare myself available for becoming one of those volunteers.  If you're interested, please tell me what I need to know.  Thanks again for all you do.  David Roscoe."



So anyway, to David and all of our listeners, any SpinRite owner will be able to use their SpinRite serial number or their transaction number from their purchase, either way, to obtain the prerelease of SpinRite once I return to it and begin working on it.  That's where it was when I paused it - a rather long pause, yes, I know - to get SQRL done.  Even I've heard from people who were furious at me for doing that, until they experienced logon with SQRL.  And I've read several people who have recently said, "Okay, you were right.  This has to happen.  So I'm glad it has, and I still look forward to SpinRite."  Of course, I do, too.  I can't wait to get back to it.  I miss it.  It'll be really fun not to be doing something else finally.



So anyway, once we get there, I will explain how that works.  And as I have releases, incremental things that are working, I will certainly tell our listeners who have been very patient with me and very supportive for lo these many years, where they can play along with SpinRite as we go.  So we'll get there.  And it's looking like it's going to happen before long.



So Android security, 10 years in, since this is the 10th birthday.



LEO:  Hard to believe, yeah.



STEVE:  And really, hasn't it gone fast?  I mean, I didn't realize we were doing the podcast when it happened.



LEO:  I still have the first Android phone right here.  Well, I won't get it now.  But it's right back there in my museum of old crap.



STEVE:  Nice.



LEO:  Yeah.  It's a terrible phone.  But as you say, we've come a long way.



STEVE:  We have.  So, and really I want to talk about exactly that in detail, how we've come from a security standpoint.  In the show notes I've got a link to the Google Android Security 2018 Report Final, as they call it.  It's a PDF, 31-page report, which examines and shares the statistics of what they recognize - Ecosystem Data, the benefit of what they call Google Play Protect, the Android Platform Security, and then essentially the threats that are out there, the very aggressive PHA families, the Potentially Harmful Applications.



They write that:  "The Android security team's mission is to protect every one of the more than two billion Android users."  They said:  "We do this through massive investment and continuous improvement in our security and privacy technology and operations."  And I should explain I've cut this down, and I'm going to cut it down ever more as I go through this because we're only halfway through the show notes.  There was just - there was so much good stuff here, I had a hard time, like, oh, god, I have to put that in.  I have to put this in.



And so I said the report did share some interesting and impressive stats.  For example, the broadest statistic for measuring device hygiene is how frequently a full-device scan detects PHAs, Potentially Harmful Applications.  They said:  "Google Play Protect, Android's built-in defense mechanism, is very effective in keeping PHAs out of Google Play; but malicious apps can still be downloaded from other sources."  And of course this is apropos of the Picture of the Week because it demonstrates the benefit of sticking within Google Play.



They said:  "These apps endanger not only the device, but also threaten the sanctity of the Android environment.  This is why Google Play Protect scans all apps installed on a device regardless of the source."  Right?  So whether you get it from Google Play or outside.  "In 2018 only 0.08% of the devices that used Google Play exclusively for app downloads were affected by PHAs.  In contrast, devices that installed apps from outside of Google Play were affected by PHAs eight times more often."  And again, Leo, as you observed, eight times 0.08 is, you know, 0.64.  Still not, like, horrible, but eight times more often.



They said:  "Compared to the previous year, even those devices saw a 15% reduction in malware due to the vigilance of Google Play Protect."  So essentially, as we know, relatively late in the game, right, because here we are at 10 years, and this happened in 2018.  So within the last couple years they decided, okay, we're going to have to get even more proactive than we have been.  They did, and it has had a significant reduction effect.



They said:  "Android's security saw a strengthened application sandbox in 2018, along with hardened developer APIs" - which we've been talking about all along as these increases and improvements have occurred - "with features like Biometric Prompt and an updated target API level for apps.  The Android Security team continued their investment in hardware-backed security through discrete tamper-resistant secure elements that enable the use of industry-first security APIs, such as Protected Confirmation and Strongbox."  They also note that in 2018 they surpassed $3 million in reward program payouts, meaning that they're proactively soliciting developers and hackers who are able to find problems and then show, like turn them over for a reward.



They said:  "Our Android security rewards programs allow us to work with top researchers from around the world to improve the security of the Android ecosystem.  These programs offer some of the highest priced rewards in the industry.  Through a combination of platform improvements like Treble, new original equipment manufacturer agreements" - and we know about those, remember, they tightened up the OEM agreements to require a maximum duration of was it six or nine months?  It still seemed like a long duration, but at least it was there for the improvements that Google makes available to be pushed out to the consumer.  And, they said:  "...and partner programs such as Android Enterprise Recommended, the Android ecosystem," they say, "has made significant progress in releasing security updates.  In the fourth quarter of 2018," they write, "we had 84% more devices receiving a security update than the same quarter the prior year."



So that's - think about that.  In the fourth quarter of 2018, so just this last previous calendar quarter, we had 84% more devices receiving a security update than the same quarter the prior year.  Now, that's great.  That's nearly double.  But on the other hand there are two million devices out there, and we know many of them are not getting any updates.  So what they're not saying is the truth of the fact that many devices are woefully unupgraded.  And I don't think that's a problem that they can fix.  I mean, they want to provide an OS for, Leo, as I said, and you got a kick out of, "fringe partners."  And fringe partners are going to be fringe.  That's what they're going to do.  



LEO:  Fringy.



STEVE:  So they're going to be fringy, yes.  So as we've said, if you want to use Android, great.  Go with a mainstream partner.  Go with a partner who today, the day after Google's release - and, by the way, Google did provide the April Fools' Day updates a month prior to all their partners.  You should already have the update on your device.  If you do, you're with a partner that you know is keeping your device current.  If you don't, well, the supplier of your Android device has had them available for more than a month.  And there are two very potent remote code execution vulnerabilities in it that should be fixed.  So buyer beware.



Under the topic of platform security and privacy, they said:  "Improving Android's security with every major Android release and monthly security update is critical [yeah].  However, in order to be even more effective, we must work to continually increase security without putting a burden on our end users.  A layered security model is part of our fundamental design principle [good] and is a foundation of Android's architecture.  The Android platform controls how the operating system works and how apps interact with other apps, device hardware, and other services.  Supported by Google Play Protect" - which is like Windows Defender, it's in there, and it's looking at everything - "Android is protected around the clock," they say.



The following table lists some of these protections that are designed to provide better platform-level security.  And this is stuff we've talked about.  Encryption protects data from unwanted access.  Hardware-backed security strengthens key storage and cryptographic services and enables strong remote authentication.  Kernel self-protections protects the kernel against memory corruption vulnerabilities and other security flaws in kernel components and drivers.  Sandboxing keeps each app in a separate space, protecting its data and processing from other apps.  SELinux provides an auditable definition of and enforcement of security boundaries on all operating system and app components above the kernel.  User space hardening protects the operating system and apps against memory corruption vulnerabilities and other security flaws; includes address space layout randomization, data execution prevention, and control flow integrity.  Verified boot cryptographically verifies that the operating system starts in a known good state.



So what we just ticked off there, I mean, that is the state-of-the-art menu for what you need to do today in a hostile environment to protect the user.  And it's there.  It wasn't there 10 years ago.  It wasn't there five years ago.  But the point is today's Android platform is now state-of-the-art security, given that you maintain communication with your provider and they with Google, that being the last piece of this.



They said:  "With Android 9 we added a myriad of great security features.  We strengthened the application sandbox and hardened the developer APIs."  And we've talked about that as that has happened.  "We continued to invest in hardware-backed security via the trusted execution environment and on selected devices through discrete tamper-resistant hardware.  We also layered a set of privacy-preserving enhancements and adopted more anti-exploitation techniques so that bugs don't turn into exploitable vulnerabilities."



On the topic of security research competitions and zero-day vulnerabilities, they said the Android Security & Privacy team participated in a number of external vulnerability discovery and disclosure competitions, including Mobile Pwn2Own, which took place at PacSec conference in Tokyo.  We talked about that this summer.  And they said:  "At this event, researchers were rewarded for demonstrating working exploits against major mobile operating systems.  Exploits against Google Pixel devices were categorized in the top reward category, along with other devices such as the iPhone.  No exploits successfully compromised Google Pixel devices, and none of the exploits demonstrated against devices running Android utilized a security vulnerability in the Android operating system."  So that's significant.  They were there, they were dangling a big carrot, and nobody got it.



"Further," they write, "in 2018, no critical security vulnerabilities affecting the Android platform were publicly disclosed without a security update or mitigation available for Android devices."  So that says there were no zero-days on Android last year.  And that's significant, too.  Again, it matters that the devices are in the update loop.  But where they are, and that's the only thing Google has control over, this demonstrates nothing but success.



They have an Android update program.  They said:  "Google mitigates security vulnerabilities discovered through the Android Security Rewards program and additional engagements through regular Android security updates.  In 2018, we continued to work with Android device manufacturers, mobile network operators, and system-on-chip vendors to increase the number of devices receiving regular security updates.  Through our combined efforts, which include platform improvements, new OEM agreements" - meaning, yes, we're going to make you do this legally - "and partner programs such as Android One and Android Enterprise Recommended, we've made significant progress in releasing the latest security updates.



"In fact, in the fourth quarter" - oh, and here it is, they're repeating their stat - "we had 84% more devices receiving a security update than the same quarter the prior year."  On the other hand, they don't tell us what percentage of all two billion.  So, yeah.  Again, a hard problem to solve.  "As of December 2018, over 95% of deployed Google Pixel 3 and Pixel 3 XL devices were running a security update from the last 90 days."  So again, as of December 2018, so end of last year, over 95% of those two devices, the Pixel 3 and the Pixel 3 XL, were running a security update as recent as the last 90 days.  So they are staying current.



On system image scanning they said:  "In the Android Security 2017 Year in Review report" - meaning the previous report of this - "we announced that we had begun scanning for preinstalled PHAs" - again, we know what those are - "across many software builds for devices with Google services.  In 2018 we expanded this program and launched it as Build Test Suite (BTS) for partner OEMs.  BTS is similar to the Compatibility Test Suite (CTS).  OEMs submit their new or updated build images to BTS.  BTS then runs a series of tests that look for security issues on the system image.



"One of these security tests scans for preinstalled PHAs included in the system image.  If we find a PHA on the build, we work with the OEM partner to remediate and remove the PHA from the build before it can be offered to users.  During its first calendar year, BTS" - and that was last year, 2018 - "BTS prevented 242 builds with PHAs from entering the ecosystem.  Anytime BTS detects an issue, we work with our OEM partners to remediate and understand how the application was included in the build."  That is, how did something not wanted, potentially harmful applications, get into the build that this OEM vendor submitted to the BTS program for scanning?  They said:  "This teamwork has allowed us to identify and mitigate systemic threats to the ecosystem."  Like they had to explain why that's there.  And that fixed some policies that these guys had.



"This teamwork has allowed us to identify and mitigate systemic threats.  Through the BTS program we discovered, analyzed, and remediated PHA families such as Chamois and EagerFonts, which are described in detail in the PHA Families section.  In 2019 we are continuing our commitment to vetting approved Android devices for security issues."  So that's very cool.  They made it very simple for their partners to submit an image which they are intending to burn onto their Android devices, and 242 of those submissions have been found containing something unwanted.  So that's very cool.



Of PHAs, Potentially Harmful Applications, they said:  "Potentially Harmful Applications are apps that could put users, user data, or devices at risk.  Common PHA categories include trojans, spyware, and phishing apps.  In 2018 we started tracking click fraud as a PHA category.  Click fraud apps simulate clicks on advertisements without user consent."  User-wanted PHAs, which I guess exist:  "Some apps with attractive features also weaken Android's built-in security.  When users try to install these apps, Google Play Protect warns users about potential hazards so they can make informed decisions.  Our statistics separate these from classic malware PHAs.  For example, Google Play Protect warns users about apps that disable Android security features, such as SELinux, or root the device with disclosure and user consent.



"Google Play Protect discourages changes that lower Android's built-in security protections, but allows individuals to choose the risks they're willing to take with their devices.  A warning message is displayed to the user anytime a PHA installation is detected.  If they decide to ignore this warning and proceed with the installation, they will not receive further security warnings about the app.  Interrupting the Google user experience with constant warnings would make Google Play Protect more annoying than useful.  In 2018 user-wanted PHAs comprised 0.11% of app installations downloaded outside of Google Play."  And then they said Google Play doesn't allow any security-breaking apps, even if they are user-wanted.



They also have a category, mobile unwanted software.  And I'll just summarize, saying that, which they are looking at, things that grab your phone number, your primary email address, information about installed apps, any information about third-party accounts, without user consent, they consider that mobile unwanted software.  With this change the total number of installed attempts coming from what they call MUwS apps declined from 2.09% in 2017 to 0.75% in 2018.  So about to one-third of what it had been before.  They note that they are now considering click fraud apps as PHA culprits.  They talk about the threat landscape changing.



In 2018 there were two notable changes to the Android threat landscape:  an increase in preinstalled PHAs, meaning from vendors; and backdoored SDKs, software development kits.  They said of the preinstalled PHAs:  "Malicious actors increased their efforts to embed PHAs into the supply chain using two main entry points:  new devices sold with preinstalled PHAs, and over-the-air updates that bundle legitimate system updates with PHAs.  Neither entry point requires action from users, making them difficult to defend against."



There are three possible reasons for an increase in the number of preinstalled PHAs.  First, the developers of preinstalled PHAs only need to deceive the device manufacturer or another company in the supply chain instead of a large number of users, so it's easier to achieve large-scale distribution.  Even a less popular device model can compromise hundreds of thousands of users through one preinstalled harmful application.



The second instance is preinstalled PHAs can gain more privileged access to the device, so it's easier to execute malicious behavior that would usually be blocked by Android's security model.  At the same time, these additional privileges allow PHAs to defend against security tools or removal attempts by users.  So they enjoy sort of a more protected status.



And, finally, third, large families of PHAs used exploits to root devices, but they say this is increasingly more difficult due to Android's constantly improving security model which blocks privilege escalation exploits to achieve similar privileges and defense levels for regular apps.  Developers of these apps know that it is easier to compromise the supply chain of device manufacturers than to attack the Android platform security model.



So I do think it is interesting that the Android supply chain has come under really aggressive attack by entities, you know, bad actors that recognize, hey, if we can just convince someone to get this into their phone, then we score a major win.  They said:  "To combat the problem of preinstalled PHAs, the Android Security team launched a security program in 2017 as part of the Android device certification process.  We expanded the program in 2018, and now every new Android-certified device goes through the same app scanning process as apps on Google Play.  Additionally, our security scanner looks for other common privacy and security issues and denies device certification until device manufacturers fix these problems."



And the other source was backdoored SDKs, as we mentioned.  They say:  "Some SDKs appear legitimate, but include behaviors and functionality that the app developer may not have known about when they included the SDK.  This functionality may compromise user data, device integrity, or experience. It may also be used as a part of a larger initiative, such as committing click fraud, mining cryptocurrency, or app install attribution fraud.  Here are some approaches, they write, developers use to include malicious code in legitimate SDKs."



There's four:  Backdoored SDKs that otherwise have legitimate functionality; backdoored Android system code that injects the malicious code into every app on the device; modified Google apps with backdoor code injected; and, finally, modified SDKs rehosted with similar names to confuse legitimate developers into accidentally downloading them.  "Hundreds of apps," they say, "have been affected by backdoored code.  We have been working with impacted developers to educate them about this new threat and to publish updated versions of their apps without the backdoor code."  



And then they talk about the device and ecosystem hygiene.  They say:  "The broadest statistic for measuring device hygiene is how frequently a routine device scan detects PHAs."  This is recapping what we talked about at the beginning.  "Since we began to measure device hygiene in 2014, an average of less than 1% of devices have Potentially Harmful Applications installed at any point.  This trend remained steady in 2018."  And then they have a bunch of stats.



What's interesting is there is a big difference, we see a big difference by country.  I have a chart here in the show notes showing the potentially harmful application presence rate found in 2016, 2017, and 2018.  The steepest decline was in India, where they started in 2016 with the highest PHA rate, and now they're still, well, they're number two, with the U.S. being currently the highest, but everybody much lower.  They said:  "In India, which is by far the biggest Android market, the number of devices affected by PHAs has decreased each year.  In 2018, 0.65% of all Indian devices were affected by PHAs at any time, a 35% drop from the previous year."  And it was a very substantial drop from the year before also.



They said:  "As in previous years, the U.S.A. is the second biggest Android market."  So India number one; U.S.A. number two.  They said:  "In 2018 the number of impacted devices rose from 0.4% to 0.5% due to the introduction of click fraud as a PHA category."  So it didn't actually increase, as I mentioned.  They redefined PHA.  They said:  "However, compared to India, the U.S.A.'s context for PHAs is different and less severe.  Eight of the top 10 PHAs in the U.S.A. were wanted by users or don't significantly impact them directly.  Of these eight, four are power user tools for rooting devices or for circumventing other security settings, and four are click fraud apps."



So in other words, these are half of the things that Google considers potentially harmful are things that cutting-edge hackers chose to install because they want the benefits that they are introducing.  So overall, I think pretty good security.



I'm trying to see if there's anything else here that jumps out at me.  Oh, over versions, device hygiene by Android version.  Newer versions of Android are less affected by PHAs.  Android 8 and Android 9 have PHA rates that are 0.19 and 0.18 respectively.  They say:  "We attribute this trend to advancements in platform security.  In particular, newer Android versions are more resilient to privilege escalation attacks that previously allowed PHAs to gain persistence on devices and protect themselves against removal attempts.  On newer versions, GPP is effectively cleaning PHAs.  In conjunction with platform changes, GPP is preventing PHAs from protecting themselves from removal or being disabled."



And, let's see, anything else?  Outside of Google Play, backdoors is a problem.  In 2018 backdoors were the most prevalent PHA category outside of Google Play, where they made up, get this, 28% of all PHA installs.  So more than one quarter of outside of Google Play potentially harmful, and I would say in this case obviously harmful, were backdoors.  As the prevalence of trojans and hostile downloaders decreased in 2018, backdoors took the top spot.  So that's happened.  That's reversed.  Then they go through the PHA families.  And basically these are all the things that Google Play Protect, that's GPP,  which is their background scanning, protects for.



There's the Chamois, or I guess it's Chamois family, which is one of the most impactful PHA families.  SnowFox is an advertising SDK with two variants.  One variant steals OAuth tokens from a device; the other injects JavaScript for click fraud into WebViews with loaded apps.  



Cosiloon, C-O-S-I-L-O-O-N, is a family of hostile downloader PHAs that was preinstalled on uncertified Android devices.  Cosiloon apps are two-stage PHAs with the first stage preinstalled on the device.  There are two variants of it.



There's BreadSMS, a large PHA family that Google Play Protect began tracking at the beginning of 2017.  BreadSMS evolved rapidly in 2018, accumulating over 11 million installs, with approximately 98% of those occurring on Google Play.  In 2018, BreadSMS added cloaking and obfuscation techniques to evade detection.



Then there's View SDK.  View SDK is a monetization SDK that uses JavaScript to perform ad click fraud.  View SDK was originally discovered by Google Play Protect in December of 2017.  However, Google Play Protect didn't begin treating click fraud as a PHA category until March of 2018.  Affected apps drop a JAR file containing View SDK during execution.  View SDK then downloads JavaScript from a remote server and injects it into WebView, showing ads and triggering fake ad clicks without user intervention.



There's Triada.  Triada software was first documented by Kaspersky in 2016 as a rooting trojan.  In mid-2017, Dr.Web documented a preinstalled backdoor variant where the log function in the Android framework of the device's firmware was being backdoored.  Any application related to Triada can then communicate and perform commands using the backdoored logging method.  And actually we talked about that at the time.  I remember the backdoored log.  



There's CardinalFall, a large PHA family with an SDK that implements click fraud.  FlashingPuma is another large click fraud family discovered in 2018.  EagerFonts is an SDK embedded in the Fonts apps that comes preinstalled on some Android devices.  Idle Coconut is an SDK that developers include in their apps for monetization.  The apps include - I wonder who's being monetized.  The apps double as endpoints of a commercial VPN that routes traffic through affected Android devices.  SDKs use a WebSocket for communications with a command-and-control server and then connect to hosts that command-and-control commanded over normal sockets.  None of this behavior is disclosed, and the user's device unknowingly becomes used as a proxy network.  Thus Google flags these as trojans.  About 60% of the installs of Idle Coconut in 2018 came from Google Play, and approximately 40% were from sideloaded applications.



So after the 12.5 years that we've been doing this podcast, I would imagine that our listeners likely have a realistic and sober appreciation for the difficulty of the task that Google has willfully undertaken by shepherding something like Android.  It's a challenge I would never want to face.  They are placing, as I mentioned at the top of the show, a highly capable and powerful open source operating system running atop equally powerful hardware, sourced very indirectly in most cases through partners far and wide and largely out of their control.  They're putting these into the hands of inherently trusting, non-computer-savvy consumers; and, for the most part, in an openly hostile environment, against quite unfavorable odds.  And I would argue that it's working, that they are doing a very good job of this.  So to all of this I say bravo, Google, and thank you.



LEO:  Nice job.  As an Android user I am grateful.  



STEVE:  Yeah.



LEO:  Do you think it's comparable to OS security?



STEVE:  No.  But it's not a fair comparison because iPhones come from no other vendor, and Apple has absolute control over it.  They have a lockdown boot process.  I mean, they make mistakes, too.  Bad stuff gets into the iTunes store.  But the power of Apple producing every iPhone cannot be underestimated.  But it's also the reason they're much more expensive than a lower end Android device.  If you want a value, I would argue without question Android gives you a better value.



LEO:  Oh, well, there's no doubt about that.



STEVE:  Yeah.



LEO:  Although if you want secure Android, you're probably going to go with Samsung and their Knox implementation.



STEVE:  Correct.



LEO:  And then in most cases you're spending as much as you are for an iPhone.



STEVE:  Correct.  And I think that essentially is a perfect "you get what you pay for" in the security model because you really, really, really want it.  To get the most security, you just need to have a locked down device.  I mean, just think about the enumeration of all, I mean, all of those things I just went through?  Those are people somewhere out there desperately trying to get in.  I mean, when we began the podcast, this would have seemed like science fiction to us.  Oh, how cute, a Honey Monkey.  You know?  Now it's like, oh, my god, how do you even turn your device on?



LEO:  Yeah, yeah.  We've come a long way.



STEVE:  Yeah, we have.



LEO:  And we have a lot farther to go.  I mean, that's really the story of this show.



STEVE:  Well, that's why a couple weeks ago I suggested that hackers could, like, earn a living by finding problems.  You can.  You can pay your rent by, if you're good, and you find problems in products, everybody wants to pay so that the bad guys don't get them ahead of time.  And so it's a new income stream.  I think it's great.



LEO:  And the old adage is that security is not a state, it's a process.



STEVE:  Right.



LEO:  And that's the process we are engaged upon each Wednesday, I'm sorry, Tuesday here on Security Now!.



STEVE:  Yup.



LEO:  Thank you, Steve.  We do this show Tuesdays at about 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  If you want to watch or listen live, you can do that at TWiT.tv/live.  You can get on-demand versions of this show from Steve's site, GRC.com.  He's got audio.  He's also got something unique:  transcripts.  So if you like to read along while you listen, I know a lot of people do, that's a good place to go, GRC.com.



While you're there, of course pick up SpinRite, the world's best hard drive recovery and maintenance utility.  You could check up on the progress of SQRL, find out how it works.  There's lots of fun stuff, all of it free except for SpinRite.  So enjoy Steve's site, GRC.com.  On the Twitter he's @SGgrc.  He takes DMs there, but also you can leave a message at the website, GRC.com/feedback.



We have copies of the show, audio and video.  That's unique to us at TWiT.tv/sn for Security Now!.  Or you can subscribe in your favorite podcast application, and CacheFly will push it out to you the minute it's available late on Tuesday.  Thank you, Steve.



STEVE:  Thank you, my friend.



LEO:  Have a great weekend.  We'll see you next time on Security Now!.



STEVE:  Talk to you then.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#709

DATE:		April 9, 2019

TITLE:		URL "Ping" Tracking

HOSTS:	Steve Gibson & Jason Howell

SOURCE:	https://media.grc.com/sn/sn-709.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we discuss more news of Microsoft's Chromium-based Edge browser; the U.K. government's plan to legislate, police, and enforce online social media content; improvements to Windows 10's update management; news from the "spoofing biometrics" department; the worrisome state of Android mobile financial apps; an update on the NSA's Ghidra software reverse engineering tool suite; perhaps the dumbest thing Facebook has done yet (and by policy, not by mistake); an important change in Win10 1809 external storage caching policy; and a bit of miscellany and closing-the-loop feedback from our terrific listeners.  Then we're going to take a close look at another capitulation in the (virtually lost) battle against tracking our behavior on the Internet with URL "ping" tracking.



SHOW TEASE:  It's time for Security Now! with Steve Gibson.  This week Steve's going to talk about the next evolution of click-tracking in browsers.  There's the Online Harms White Paper.  Microsoft Edge browser is now official on Chromium.  There's the Galaxy S10's in-display fingerprint sensor - apparently it can be spoofed now - and so many more topics.  I'm Jason Howell.  I'm going to be here with Steve next on Security Now!.



JASON HOWELL:  This is Security Now! with Steve Gibson, Episode 709, recorded Tuesday, April 9th, 2019:  URL "Ping" Tracking.



It's time for Security Now!, the show where we talk about the latest security news happening this week.  And of course, well, I'm Jason Howell.  I don't sound anything like Leo.  That's because I'm not him.  He is enjoying himself on the beach, I think.  But joining me as always, every single week, Steve Gibson, the man about town when it comes to security.  How you doing, Steve?



STEVE GIBSON:  Jason, great to be with you this week while Leo is basking in the sun on one of the Hawaiian islands somewhere.



JASON:  I'm not jealous at all.  Nope, nope, not going to be jealous.



STEVE:  I've done Hawaii a few times.  I spent the second half of my honeymoon - the first half was in Napa, the second was in Hawaii.  And it's like, I'm not really big on the super humid tropical clime.  That's sort of not my thing.  I like the dry air of air conditioning.



JASON:  Okay.



STEVE:  So I'm sort of looking around because that's what I have right now.



JASON:  You're surrounded by the dry air produced by all the technology and gadgets that are whirring in the background.  I myself love a good tropical vacation.  I have not been in literally years.  This Thanksgiving we're going back to Hawaii.  So we're going to get 10 days in Hawaii.  And so I am very much looking forward to that.  I can appreciate the dry air thing, but I need some humidity in my life.  I need a break from all this dry air.



STEVE:  That's right.  So we're going to talk about, when I began to assemble the news of the week, nothing really jumped out at me.  But the more I looked into one of the topics, the more I thought, you know, this really does represent kind of an important drift, if nothing else, in the industry.  There is a little known feature in URL links that we're all clicking on all the time, when we click on ads or we click on links in articles or just anything on the 'Net, which has languished for about well, let's see, at least a decade.  And it was finally ratified in HTML5.



And the thing that popped this onto the news is that Google's Chrome browser, which we now know will be echoed, unless Microsoft makes changes, and it's unexpected, in their Edge browser, which is now Chromium based, it is removing the option to disable this.  It was always enabled.  It used to be disableable.  Opera and Edge as it is today all have this enabled.  Only Firefox and Brave don't.  But it is sort of the ultimate in tracking our actions, sort of like the final capitulation to any hope of resisting the idea of being tracked on the Internet.  It's like, no, that's the way the Internet is going to work, folks.



And so it's now supported officially, tracking is officially supported by the HTML protocol rather than being sort of a, you could argue, an unintended side effect, sort of in the way that third-party cookies - I've often talked about how they were never intended to be used to track us.  Cookies were meant to allow persistent sessions with the sites we were visiting, not to allow third parties where we are not visiting to track us across the Internet.  Similarly, the practice of redirecting our browser through redirect links was never intended to be used for tracking, but it has been subverted for that.



Well, this final move using an argument in URLs known as "ping," believe it or not, has no other purpose than for tracking.  That's why it's there.  And the fact that it is going to be always enabled, cannot be disabled - and Google is already using it.  I show, and we'll get to it toward the end of the podcast, that when you bring up Google Search, you get two different pages if you are doing it in Chrome or in Firefox because Google is already using this, which they know is in their own browser because they put it there, and they know Firefox has it off by default.  Anyway, I think that's going to be some interesting content for our listeners that they haven't come across.



But we've got much more than that to talk about.  We've got some more news about Microsoft's Chromium-based Edge browser.  We've got the U.K. government's plan to legislate, police, and enforce online social media content.  Improvements to Windows 10 update management.  News from the spoofing biometrics department.  The worrisome state of, in this case it was Android mobile financial apps, but I explained it's probably all apps, it's just that the research was done on Android because it was easier for them to do.  We've got an update on the NSA's Ghidra software reverse engineering tool suite.  Perhaps the dumbest thing Facebook has done yet.  And, you know...



JASON:  Lot of competition.



STEVE:  I'm sure in the same way that this network has This Week in Google, you could do This Week in Dumb Facebook Things.  But in this case it was done deliberately and by policy, not by anything they could claim as a mistake.  So we have to talk about that because it's unbelievable.  Also an important change in Windows 10 1809.  They just finally ratified as of last week, we talked about that, update to Windows 10.  They're changing in a significant way the default external storage caching policy.  And I know that our listeners will be interested.



We also have a bit of miscellany; some closing-the-loop feedback.  And then we're going to take a close look at this kind of sad capitulation is the best word I have for it, creeping trend toward making tracking official, rather than something that people could block if they were saying, I want to turn that off or please don't and so forth.  It's just the way the Internet's going to be.  So I think lots of fun stuff for our listeners.



JASON:  It's just the way the Internet's going to be, says Steve Gibson.  Basically what you're saying is it's time to put your hands in the air because tracking is here, and it's not going anywhere.  It probably hasn't, you know, it's been here for such a long time that this just seems to be more of a cemented way, cemented approach.



STEVE:  It's also sort of like - I liken it to JavaScript, how a couple years ago, using NoScript on Firefox, if you were kind of an old curmudgeon, and you didn't like the idea of websites you're just randomly visiting running code on your browser, you could turn scripting off and then selectively turn it on if you needed it.  Well, some time ago that just became impractical because websites, there were just like no websites left that would do anything without JavaScript.  Mine, okay, except that, you know, that doesn't really count.  So I don't have any script on my site except specific pages that you go to when you want - like the Password Haystacks page.  It uses script to do an actual job for you, rather than endless tracking.  And anyway, so yeah, we will get to all that.  Going to have lots of stuff to talk about.



JASON:  Absolutely, a ton of stuff to talk about.  All right, security news time.  Microsoft's Chromium Edge browser is now a thing.



STEVE:  Yes.  With rather surprising speed this has become available.  We covered the news last week of it having leaked, and there were kind of some sketchy places that you could download it.  And I know I heard Mary Jo say that she was running it.  What was interesting was it was running on Windows 7 as opposed to, as is everything now that Microsoft is doing, explicitly Windows 10 only.  So I have the link in the show notes.  Presumably you can google it also.  But it's microsoftedgeinsider.com.  I think that's probably all you need, www.microsoftedgeinsider with no punctuation there, dot com.  There's a download page.



They declare that - so this is just to remind our listeners, or if someone has skipped a couple podcasts.  Microsoft apparently threw in the towel - "capitulation" seems to be the word of the week on the podcast - just gave up all of the work that they had put into their Edge browser, which was arguably very good work, and said let's just use Chromium.  Let's use the open source browser which of course Google's project Chromium has famously created, which is the number one browser on the Internet, despite all efforts of Microsoft's to make it hard for people to make that change.  People want to use Chrome.



And I have to say, everywhere I go, I mean, I'm still holding onto Firefox.  I like Firefox.  It is arguably becoming - its differentiation from Chrome is increasing as Firefox continues to maintain more of an independent, non-ad supporting, non-tracking profile.  And mostly I just like the UI.  I mean, a browser is almost all about the user interface.  And Firefox allows integrated vertical tabs down the left-hand side that I cannot live without.  I mean, until I can't do that or until Chrome allows me to do that, and there's been some talk about it, I'm using Firefox because UI on a browser matters.



Anyway, so Microsoft gave up.  They're switching to Chromium.  The early reports from the leaked copy were all glowing.  I mean, it was, you know, basically it's Chrome with an Edge wrapper around it from Microsoft.  Well, it's gone official.  It for Windows 10 is now available.  And, interestingly, they intend to make it available officially for Windows 7, 8, 8.1, and macOS.  So, I mean, again, I'm not that excited because I'm happy with Firefox on my Win7.  But if somebody for some reason didn't want to run actual Chrome, Google's Chrome on their Windows machine and wanted to have Edge, there is apparently some feature in Edge that would allow you to have IE tabs within the Chromium Edge browser, if presumably you went to some who-knows-what website that actually required IE, like real old Internet Explorer function that hadn't been updated to run on contemporary browsers, you could integrate that into Chrome.



And of course we also, a couple weeks ago, we talked about this bizarre add-on that Microsoft had officially created which was available both in the Google Chrome extension store and as a Firefox extension, which when you attempted to go out onto the Internet, would grab your browser and run it under Edge, if you were doing this in Windows.  And it was like, okay, I'm not sure why that's a good thing.  But anyway, they did that.



So there are, as this official Microsoft Chromium Edge site states, three update channels are available, which will be familiar to many people who enjoy playing on the bleeding edge.  There's the Beta channel, which is the most stable browser.  And that code is updated about every six weeks.  Then there's the Dev channel, which will be more stable than the on-the-fly nightly updating Canary.  But the Dev channel would be not yet ready for Beta.  And it would be the one that's working toward the next major release.  And the Canary channel is like the cutting-edge state of the browser, with code commits from that day, so it's updated nightly.  And that's where prototyping of the newest features that would be two major versions away from the current release would be hashed out and experimented with.



So this is neat.  I mean, this sort of feels like part of the new Microsoft, Microsoft beginning to do popular things the way other companies have been doing them and wanting to play with the rest of us also.  Microsoft's Joe Belfiore stated in an announcement, he said:  "In these first builds we are very much focused on the fundamentals and have not yet included a wide range of feature and language support that will come later."  He said:  "You'll start to see differences from the current Microsoft Edge, including subtle design finishes, support for a broader selection of extensions, and the ability to manage your sign-in profile.  We look forward to people starting to kick the tires and will be refining the feature set over time based on the feedback we receive."



And then the one significant thing, and we saw this already in the leaked release, but it's still there in the official, is you can enable support for Google Chrome extensions in Microsoft Edge's insider build.  You select Extensions under the browser's main menu, the Edge menu.  And then down on the bottom left is "Allow extensions from other sources."  If you turn that on, then you're able to use Google Chrome extensions in this Microsoft browser.



So I guess the sense is that I guess Microsoft is hoping that this will cause fewer people to choose Chrome over Edge if they are essentially the same.  You know, I don't know.  It seems to me like Chrome has such a powerfully strong positive grasp and reputation on the 'Net now that people just like the idea of not using, for some reason, I mean, I'm amazed.  Everywhere I go I see that little Chrome logo in people's, you know, on their desktop or down in their quick launch area of Windows.  And it's like, okay, well, it's obviously prevalent.  So it'll be interesting to see in terms of share what happens.



There was a really interesting chart that I saw as part of this news that showed all of the Google-specific features which Microsoft had ripped out of their version of Chromium for Edge.  I mean, it was like four columns of 20 items each.  It was just pretty much everything.  And again, naturally, Microsoft doesn't want to be promoting Google properties in their take on Chromium.  But they really did have to, you know, there was a lot that is there in Chrome that will not be in Edge.  And actually, maybe that sort of answers my question.  If people want those things, they're not in the Edge version, they're in the Chrome version.



JASON:  Do you think that this transition is the type of thing that people who follow tech might actually notice, but the everyday Microsoft Edge user is going to be completely oblivious to?



STEVE:  I agree.  I don't think they would have any way of recognizing that that's what's going on.  Certainly developers will care because suddenly Edge will be able to run Chromium extensions.  In fact, it does run the work-in-progress SQRL login extension right now.  So the SQRL project just got another major browser that it's compatible with.  So that's an example of, probably more than anything, that may be one of the big benefits is that we know how incredibly popular browser extensions are.  People want to customize their browser in all kinds of different ways.  I mean, I run with a bunch of extensions in Firefox, like the one that gives me the sidebar of vertically oriented little tiny tabs so I can have 400 open.



JASON:  Oh, you're one of those.



STEVE:  Uh-huh.  Yeah, I have a scrollbar on my tab column.



JASON:  How do you do that?  I don't even understand.  I don't understand people with that many tabs.  It just breaks my brain.



STEVE:  Anyway, so it may be that that's the hook is that you'll be able to use that rich extension library in Edge.  And so there was no way, you know, Microsoft tried to replicate that, and it just didn't happen.  It's like Paul talking about the Windows Store and just how pathetic it is, just how awful the apps are there.  Microsoft keeps trying to copy Apple with iTunes; and, oops, that didn't work.  And here again they tried to create an extension experience for Edge, and no one cared.  They just used Chrome because all the extensions that they wanted were there.  And so Microsoft said, okay, fine, we'll just put Chrome inside our Edge wrapper, and everyone can use the extensions that they want, and maybe we won't lose so many of them.



JASON:  Yeah.  Well, and it's not without precedence, either.  Microsoft did this also with its mobile strategy.  Now basically we're getting Microsoft apps on Android instead of Microsoft controlling its own OS.  So, yeah, this is just kind of the modern Microsoft era is realizing, it's kind of strange when they make this decision, but realizing that they're willing to kind of admit to a certain degree, whether they call it defeat or not, admit defeat in one area and shift their strategy in a new way like this.



STEVE:  Yeah.  So yesterday the U.K. government announced a suite of online safety laws which, I mean, and this is what we saw coming.  This is sort of like, you know, we know that the GDPR happened within the EU.  So now the intent here in the U.K. is to hold the publishers of online social media platforms liable for the harmful behavior spreading through their platforms.  And I guess my feeling is this is the inevitable legislative blowback from what has been the previous practice of providers, social media providers wanting to try to sell the idea that they're just providing a utility, and that they're not in any way responsible for the actions of the people who use this information utility that they're providing.  And unfortunately, that strategy has, as we know, backfired a lot.  This paper, which I think was 102 pages, and believe me I'm not going to drag our listeners through it...



JASON:  Page one.  Chapter two.



STEVE:  But for the next three months - and so this is not yet legislation.  This is the slow grinding wheels of legislation beginning to turn.  And so the first couple of teeth of those gears are meshing.  So this is intended to be open and available for three months, until July 1st, so just a little less than three months.  And to give our listeners some sense for it, they said, sort of in the introduction, what they call the "Online Harms White Paper" sets out the government's plans for a "world-leading package of online safety measures that also supports innovation and a thriving digital economy."  Uh-huh.  "This package comprises legislative and non-legislative measures and will make companies more responsible for their users' safety online" - and of course we're going to march out the kids, so - "especially children and other vulnerable groups.



"The White Paper proposes establishing in law a new duty of care toward users, which will be overseen by an independent regulator.  Companies will be held to account for tackling a comprehensive set of online harms ranging from illegal activity and content to behaviors which are harmful, but not necessarily illegal.  This consultation aims to gather views" - that is, this paper, this thing that's out for the next 90 day - "aims to gather views on various aspects of the government's plans for regulation and tackling online harms, including" - and we've got five bullet points here - "the online services in scope of the regulatory framework," which is to say which online services would be in scope for this framework; "options for appointing an independent regulatory body to implement, oversee, and enforce the new regulatory framework; the enforcement powers of an independent regulatory body; potential redress mechanisms for online users; and measures to ensure regulation is targeted and proportionate for the industry."



So, and I pulled a bunch of points out.  There were 49 numbered paragraphs in this 102-page thing.  And I think I pretty much characterized it well enough.  I mean, we know what its focus is.  Paragraph 12 had some bullet points which I'll just quickly cover.  They said:  "Our vision is for a free, open, and secure Internet; freedom of expression online, yada yada" - it actually didn't say that - "an online environment where companies take effective steps to keep their users safe and where criminal, terrorist, and hostile foreign state activity is not left to contaminate the online space."



So basically we know what they're saying.  They're saying we're going to make the likes of Facebook and Google and Twitter much more responsible for the content they're carrying than they have been to date.  So things are going to change is the whole issue here.  And so on.  "Rules and norms for the Internet that discourage harmful behavior."  They said:  "The U.K. as a thriving digital economy, with a prosperous ecosystem of companies developing innovation in online safety; citizens who understand the risks of online activity, challenge unacceptable behaviors, and know how to access help if they experience harm online, with children receiving extra protection; a global coalition of countries all taking coordinated steps to keep their citizens safe online; and renewed public confidence and trust in online companies and services."



So this is the beginning of probably a reform of some sort for, which we've seen in various forms in the past, we've seen people talking about it, this is the beginning of something.  And we'll certainly, to the degree that it affects the security and privacy space, we'll be talking about it, as I'm sure for years to come.



JASON:  Yeah.  And I'm always curious on something like this, or at least in the past short term with GDPR and all these other things that are happening overseas around privacy and protection and security, how this ultimately trickles down to places outside of the U.K. because that creates a very large, not impossible I imagine, but a large hurdle for technology companies to be heavily regulated in one area and not in the other.  Maybe it's just easier for them to roll it out to everyone.  You know what I mean?



STEVE:  Well, and Jason, you can't go anywhere now without acknowledging cookies.  Everywhere you go, you have to say, yes, I know, okay, fine, I know.



JASON:  True.  Perfect point.



STEVE:  So that's hit the entire globe because of GDPR saying, you know, we're going to sue your pants off unless you let people know you use cookies.  Well, everyone does.  So it's like, what?  It's like, look, we have a web page.  Yes, fine, I clicked.  I acknowledge you.  I acknowledge you have a web page.  That's why I'm here.



JASON:  I acknowledged that when I hit Enter on the search query.



STEVE:  That's right.  Oh.  



JASON:  Yeah.  Yeah, it'll be interesting to see how this plays out.  And as, like, I think the cookies example is the perfect example; right?  Like through this, at least here in the U.S., we in essence have become somewhat desensitized to what that actually means, what that acknowledgment by clicking okay actually means.  It makes it very easy for those regulations - we hear about regulations in the U.S. all the time now as relates to technology companies.  It makes it even easier, less of a hurdle, for those efforts to roll out over here in a more official sense.  So probably the same could be said about this, if and when it becomes what it is.



STEVE:  Well, and it's just cluttered our browsers with visual spam, the fact that you go to a site, and you have to acknowledge that they're using cookies.  In fact, when I was researching this story, the U.K. government site put its banner at the top and had me click on it to acknowledge that it was improving my experience online.  It's like, no, you're not.  You just made me click this.  That did not improve my experience online.  Sorry about that.  And the fact that I clicked it to acknowledge that it improved my experience, well, you gave me no choice.



JASON:  No choice.  You must like this.  Click here to like this. 



STEVE:  Yes.



JASON:  Ah, yeah.



STEVE:  So I just, thank goodness, suffered through the Windows 10 1809 update this morning.  It took several hours on this system that I'm talking to us on.  That's why I'm saying I was like, I was holding my breath that it was going to finish in time for the podcast because it sat at 18% for, like, a long time.  And I thought, okay, this better not be proportionate, or I'm not going to be talking to our listeners today.  I mean, I would have scrambled around and set up Skype somewhere else, but that would not have been fun, either.



JASON:  Do it from your phone.



STEVE:  Yeah.  And as we said, we just last week, this very laboriously troubled - this was the October 2018 update, which was so fraught with problems that only last week did Microsoft finally decide that they had consensus from all parties, and sort of at this point permission, to try again to roll it out.  So the point is that next month - so much time went by that it's supposed to be twice a year; right?  Every six months?  Well, next month, in May, is the next one.  We're already there.



So what's interesting is that, in a very nice change, I think, they are going to what they're calling an improvement to the Windows 10 update experience with control, quality, and transparency.  Last Thursday, on the 4th of April, Mike Fortin, who's VP of Windows, he said:  "In previous Windows 10 feature update rollouts" - so those are the semiannual, right, the twice a year, not the monthly security fixes, but these are, like, oh, we got a whole bunch of new stuff for you.  Whether you want it or not, we're going to make you take it, by the way.



But the feature rollout, he said:  "The update installation was automatically initiated on a device once our data gave us confidence that devices would have a great" - great - "update experience."  That's right.  That's a great experience.  "Beginning with the Windows 10 May 2019 Update" - so that's next month - "users will be more in control of initiating the feature OS update."  So basically it's going to be on-demand to some degree.  He said:  "We will provide notification when an update is available and recommended based on our data, but it will be largely up to the user to initiate when the update occurs."  But again, with limits, as we'll see.



"When Windows 10 devices are at, or will soon reach, end of service, Windows Update will continue to automatically initiate a feature update.  Keeping machines supported and receiving monthly updates is critical to device security and ecosystem health.  We are adding new features that will empower users with control and transparency around when updates are installed.  In fact, all customers will now have the ability to explicitly choose if they want to update their device when they check for updates, or to pause updates for up to" - wait for it - "35 days."  Okay.  So not 365, 35.  So, okay.



He says:  "We're taking further steps to be confident in the quality of the May 2019 update."  And you can imagine that's true after the fiasco of the previous October 2018 update.  He says:  "We will increase the amount of time that the May 2019 update spends in the release preview phase.  We will work closely with ecosystem partners during this phase to proactively obtain more early feedback about this release.  This will give us additional signals to detect issues before broader deployment."  All of which they failed to do last time, of course.  "We are also continuing to make significant new investments in machine learning technology" - so they're bringing AI in - "to both detect high-impact issues effectively at scale and further evolve how we intelligently select devices that will have a smooth update experience."



Okay.  In the show notes I took a picture of the screen which they showed in this announcement.  And this is what I like.  There's the standard Windows Update, then the Check for Update button, and then there's a new section that stands off, sort of a pullout, and it says "Feature Update to Windows 10, version 1903," which is not yet, of course, but that's what it'll be.  And it reads:  "The next version of Windows is available with new features and security improvements.  When you're ready for the update, select 'Download and install now.'"  And then there's a link to do that.  So I really like that.



So what this does is it removes it from the news appearing on your screen that, oh, we've just started a multi-hour process of giving you a bunch of new features you didn't ask for.  Oh, and you can't use your machine in the meantime.  They've switched it to "We've got new stuff available for you.  Click here when you're ready to get it."  Unfortunately, you still have to have it.  It's not like they're making this entirely optional for a long period of time.  Looks like it's up to a little over a month that you can postpone this, and also only when you go to manually check for updates.



So I guess this is intended for advanced users or update- and security-conscious users who do go to proactively see whether their system is up to date.  Then they will receive the news that, oh, look, in this case the 1903 Windows 10 feature update is now deemed ready for this machine I'm using.  I'll kind of keep that in mind for when I don't need to use the computer for a few hours, and then let her rip.  So anyway, nice to see some improvements in, you know, like clearly Microsoft is understanding and listening to some of the feedback that they're getting about this because, I mean, I just did this.  I just went to 1809.  And like I said, I was terrified.  It had the morning, basically, and it took it.



JASON:  What is that rule that, like, no matter how much time you have, you always fill it with the amount of work that you have.  Apparently Microsoft is adhering to that closely, as well.



STEVE:  That's right.



JASON:  Yes, you were brave to do the update before the show.  But you made it.  That's the point.



STEVE:  Yeah, I did.  I'm here.



JASON:  You're here.  All right.  So you're going to talk a little bit about this little portion of the phone that drives me insane every single flipping time I need to unlock this phone.



STEVE:  Oh, interesting.  I look forward to hearing your feedback.  So as we know, and this is what you're talking about, is that Samsung's recently released Galaxy S10 top-of-the-line smartphone has maybe a not-so-spiffy new fingerprint reader.  But at least it's kind of spiffy technology.



JASON:  It's spiffy technology.  I'll give it that.



STEVE:  Yeah.  We all recall Apple's fingerprint reader.  We talked about it at the time, couple years ago.  It uses capacitive technology to image the ridges of our fingertips.  When it was released it was noted that, since it was capacitive, it would not be spoofed by flat images.  And as we all know, it was only a few days after it got into the hands of some creative hackers that a fake thumb with ridges, much like the Ruffles potato chips, was created to spoof Apple's capacitive technology.  So not unspoofable after all.



So now we jump forward a few years to Samsung.  Everyone these days is all freaked out over the seamlessness of their screens, with much angst like over the Apple iPhone ears, right, the little side things created by the encroachment of the iPhone's 3D facial recognition technology onto the screen.  So Samsung, determined to minimize the encroachment of any UI technology, set their engineers the task of incorporating a fingerprint reader into the screen so that there would be no physical, set-aside, set-off zone of any kind.  They succeeded and created an acoustic ultrasound imaging technology whose transducer is placed behind the screen and which operates through the screen.  Apparently, based on Jason's experience, not as well as the engineers at Samsung were hoping.



JASON:  Your mileage may vary.



STEVE:  I want to hear about that in a second.  So we're talking about this because, as with Apple's capacitive 3D fingerprint imaging technology, it didn't take clever hackers long to spoof the Samsung ultrasonic fingerprint either.  Last Wednesday a Reddit user calling himself darkshark9 posted his hack of his Samsung S10.  I've got a link in the show notes to a video of him doing this.  He unlocked his Samsung Galaxy S10 using his 3D printed fingerprint picked up from a photo of a wineglass taken using his own smartphone.  However, to make his accomplishment more striking, he noted that the fingerprint image could be captured at greater distance using a DSLR camera to steal one's fingerprint further away than having to near focus.



And so, reading from what he wrote, he said:  "I pulled the image into Photoshop and increased the contrast and created an alpha mask.  I exported that over to 3DS Max and created a geometry displacement from the Photoshop image, which gave me a raised 3D model of every last detail of the fingerprint.  I popped that model into the 3D printing software and began to print it."  He says:  "This was printed using an Anycubic Photon LCD resin printer, which is accurate down to about 10 microns,"  he said, "in Z height, 45 microns in x/y, which is more than enough detail to capture all the ridges in a fingerprint."  He says:  "It printed perfectly."  And he said:  "Print time was only around 13 minutes."



He said:  "It took me three prints trying to get the right image height."  And he said in parens, and this is typical, he says:  "I forgot to mirror the fingerprint on the first one," meaning, you know, you've got to flip it over because what you're pushing against the screen is the reverse of the image that was taken when you look at the fingerprint that you've asked to be printed.  But he said:  "But, yeah, third time was the charm.  The 3D print unlocks my phone, in some cases just as well as my actual finger does."



So where does this take us?  My feeling is we could attempt to make the recognition process much more robust.  So both fingerprint readers have been spoofed.  We know that Samsung's face recognition was spoofed.  Although it is more difficult to do, Apple's face ID has been spoofed with a fake 3D face model.  So, yeah.  These are not unspoofable technologies.  We could go to further extremes to make the process more robust and less spoofable.  In the case of a fingerprint reader, we could run a weak AC signal through the fingerprint tissue to determine its impedance.  That would be much harder to spoof.  All existing spoofs would fail that test.  Or we could send several different frequencies of light into the finger and determine some measure of relative spectral tissue absorption.  We could also watch that over a span of time, like a span of a few heartbeats, to detect capillary pulse using photo - I can't ever say this.  Photo - I can't believe I can't say it.  I can't say it.



JASON:  My favorite part of the show, right now.



STEVE:  Plethysmography.  Normally it runs off the tongue, but I need some more coffee.  Anyway, so we know that Apple did go to great lengths to make their face ID largely immune to simple spoofs.  You can't show it a flat photo because they've got infrared imaging to obtain a 3D model of the user's face, specifically so that showing them a photo, showing it a photo would not fool it.  But capturing 3D images of people is no longer the stuff of science fiction.  Consumer apps now do that regularly.  So that's really not that big a problem.  And we have 3D printers, as this existing hacker just noted.  So getting 3D models is not difficult.



So I think that the right way to think of this is soberly and realistically.  This is another example of the classic tradeoff of convenience for security.  Are biometric systems as secure as non?  And the answer is no, they're not.  They are not as secure as a long password.  But they are incredibly more convenient to use.  And so the question you have to ask is are they secure enough for your application?  And applications vary.  Are they secure enough for a phone owned by an NSA person who's keeping secrets on it?  No, they're not.  Are they secure enough for your mom, who just wants to pick her phone up and answer it?  Yeah, probably they are.



So unless the technology in our current biometrics is raised to the point where their cost starts becoming prohibitive, because doing all this extra stuff is going to cost more in a consumer setting, and it's going to make them more prone to refusing to accept that you are really you when you are because you're sad today, and so the facial recognition got tightened up, or you changed the glasses that you're wearing, and they no longer match the profile of what you had before, or who knows what?  I know, for me, Touch ID doesn't work when my fingers are cold, oddly enough.  So maybe there's a thermal sensor in there.  I don't know.



The point is that the more we do to make them much less easily spoofable, the harder it's going to be to convince them that we are who we are, that it really is us.  Inherently, when you reduce the chance of false positives, you also, I mean, sorry.  When you reduce the chance of false, yeah, false positives, you also reduce the chance of true positives because you've so much tightened down the system that it becomes, I mean, the fundamental problem is that the signal, from a signal recognition standpoint, the signal we are trying to recognize is soft.  And it's a fingerprint, which is inherently a soft signal.  Things can happen to it.  You can get cut.  It can be a different temperature.  You can have scars on it.  And it can be a spoof.  And certainly a camera looking at a face is a soft signal, whereas a long password is an incredibly hard signal.  It is not a soft signal.  But soft signals, biometrics, are much easier to use.



So when I saw this, it's like, aha, I spoofed the Samsung S10.  It's like, yeah, everybody has spoofed everything.  If it's biometric, it is spoofable.  I mean, but it is also, except maybe Jason in the case of your Samsung S10, much more convenient to use than a system that is less spoofable.  So anyway, I just sort of wanted to take a moment to acknowledge that this whole domain of biometrics has the problem that it is convenient, but we are making a big tradeoff in terms of absolutely lockdown security.  So tell us about your S10.



JASON:  Well, I just want to add to that real quick what you're saying as far as biometrics.  You know, increasing the accuracy  means that we're less likely to get in at a convenient rate is essentially how I take that.  One of the things that I've always wondered is, okay, so we've got face scanning.  We've got iris scanning, although the S10 got rid of the iris scanner.  We've got fingerprint sensors.  So we've got multiple biometric aspects.  Why not combine them so that you have kind of a combined effort of more likelihood that it knows it's you?  Well, the face scan matches.  So does the fingerprint.  Therefore much more certain that this is the person who it claims to be.



But what you're saying there makes me realize why that's probably a bad idea because the more you do that, the more variables, the more kind of room there is for it to get it wrong.  And the worst-case scenario would be that you could never get it 100% right, and you're locked out of your phone entirely because there's no way to get in because you can't get both of them to sync or to happen at the same time.  So maybe that's not a good idea.  But that always seemed to be, to me, the obvious direction is, well, they'll just combine them.  I'm already used to putting my finger on the fingerprint scanner.  And if I'm already looking at my phone, then use both of those data points and let me in.  But it's probably much harder than that.



STEVE:  You know, for years the access to my datacenter, where I have a physical rack of servers, it used a biometric hand reader, which was looking at the physical size of the fingers of my hand.  And so it had, like, pegs.  And you'd stick your hand in it, and then it looked down on it and measured my finger sizes.  And so you wonder, with all the technology that Apple has, and the fact that they've got this 3D multipoint scanner, if you couldn't show it your face and then also hold your hand up and let it measure your hand and decide if, you know, get like an additional factor for you.



JASON:  Well, it's interesting that you mention that.  The LG G8 ThinQ is a new phone that LG showed off.  And it has vein detection, where you hold your hand over the front-facing camera.  And so it's similar, in a way.  It's not quite the same, but it actually authenticates around the veins in your hand and your wrist.



STEVE:  That's probably a pretty good signal because, I mean, that's going to be, like, very unique from one person to another.



JASON:  Probably so.  A little bit of social educating the users around not feeling ridiculous holding their hand over their phone.  But, I mean, maybe that ends up being a more secure direction to go.  I don't know.  As for the fingerprint sensor on the S10, I would just say my frustration comes out of the fact that just on a daily use basis, because it's in the display, and it's kind of hidden down here, it takes a little bit of time.  It's not immediate, the way I'm used to with like a Pixel, where I know exactly where my fingerprint goes.



STEVE:  Right.



JASON:  It's just not as consistent as I'm used to.  And so more often than not, I end up doing two, three, four touches and then entering my PIN to get in.  And it's just so much time spent when it works the other way.  And so it just got frustrating over time.



STEVE:  Do the apps draw a circle and show you where on the display the sensor is?



JASON:  Yeah.



STEVE:  So at least you know where to put it?



JASON:  Yeah, I mean, I'm trying to see if maybe - it's probably hard to see.  Maybe in my single you can see it.  But there's a little area down here that...



STEVE:  There is an indication.



JASON:  There is an indication sphere.  So that little fingerprint dot down there, that will appear when it's time for me to authenticate on an app, and the display is on.  If the display is off, I don't know if I'll be able to get it - oh.  It realized that I held it up, and it turned the display on.  But if it's off, I should be able to do that.  



STEVE:  That's interesting.



JASON:  Of course I'm hitting it sideways with my thumb, so it's just it's doing a little haptic kick, but it's not actually unlocking.  But if you know where it's at, you can get into it even when the display's off.



STEVE:  And so your sense is, I mean, if you had your choice, you would have had them leave the fingerprint reader where it was because you knew where it was, it worked, it was reliable, and it just, I mean, you know, it was less hassle.



JASON:  After two weeks of using the S10 I still don't have the in-display sensor nailed.  And I thought after two weeks of solid use I'd get used to exactly where my finger needs to go.  I'd get better at scanning it so that it would let me in more often than not.  And it's still not there.  So I'm open to new technology 100%.  But in my two weeks of usage it's been more headaches than it's been, oh, my goodness.  You know what I mean?  And so to that end, yes, I want a system that works better than that.  That's how I feel about it.



STEVE:  And so the official pronunciation is photo plethysmography.  I was practicing it.  Photo plethysmography.



JASON:  I saw your mouth moving.  I thought you were talking to JammerB on the talkback.  But you were pronouncing plethysmography.  That's hilarious.  That's great.  Hey, but you got there.  You got there.



STEVE:  Photo plethysmography, yes.  There's one for the cocktail party.



JASON:  No kidding.  Say that five times fast.



STEVE:  So this is a bit of a self-serving study because it was contracted for by an Arxan Technologies that bills itself as the trusted provider of application protection solutions.  So they're based in San Francisco.  They contracted with another firm, the Aite [A-I-T-E] Group, to look at the security of financial apps on the Android Mobile platform.  And so, not surprisingly, they're not good, that is, the security of these mobile financial apps is wanting.  But the details, as always, are where there's some interest.  Their PR, their press release, highlighted the results of this contracted-for analysis of a group of popular mobile financial apps.  Their press release reported the discovery of widespread security inadequacies and protection failures among consumer financial applications leading to the exposure of source code, sensitive data stored in apps, access to backend servers via APIs, and more.



So at the company that was contracted, the senior cybersecurity analyst is Alissa Knight.  She's with this Aite Group who authored the study, which was titled:  "In plain sight:  The vulnerability epidemic in financial services mobile apps."  And, you know, given what they found, I would tend to agree.  There are a couple charts down below, a couple pages down, Jason, that you might want to put onscreen, which show some little itty-bitty pie chart diagrams, but you can get some sense for it.



So Alissa Knight examined the mobile apps of 30 financial institutions downloaded from the Google Play Store across eight financial sectors including retail banking, credit card, mobile payment, cryptocurrency, HSA, retail brokerage, health insurance, and auto insurance.  Using tools readily available on the Internet, and of course those now include the NSA's Ghidra that we'll be talking about next, "Knight found nearly all of the applications could easily be reverse engineered, allowing access to sensitive information stored inside the code, such as improperly stored personally identifiable information, account credentials, server-side file locations" - okay, server-side file locations - "API keys, live deployment, and QA URLs used by the developers for testing the apps."  In other words, these things weren't even beginning to be securely designed, or designed to impede their abuse.



"The research highlights a systemic lack of application-appropriate protection such as application shielding, threat detection, encryption, and response technology across financial services.  Analysis of the mobile financial industry applications highlighted major deficiencies in application design, including easily reverse engineered code that exposes serious vulnerabilities including in-app data storage; compromised data transmission due to weak encryption and insufficient transport layer protection; and malware injection and tampering."



The Aite Group's key research findings included lack of binary protection:  97% of all apps - which suggests that they found one that had it because that would be 3%, so that would be one of the 30 that they looked at - tested lacked binary code protection, so one of them had it, making it possible to reverse engineer or decompile the apps exposing source code to analysis and tampering.  Unintended data leakage:  90% of the apps tested - so that suggests that three out of the 30 had it, 27 didn't - shared services with other applications on the device, leaving data from the financial institution's app accessible to any other application on the device.  Whoops.



Insecure data storage:  83% of the apps tested insecurely stored data outside of the app's control, for example, in the device's local file system, external storage, and copied data to the clipboard allowing shared access with other apps, and exposed a new attack surface via APIs.  In 80% of the case they found weak encryption, that is, weak implementation of encryption algorithms or incorrect implementation of a strong cipher, allowing adversaries to decrypt sensitive data and manipulate or steal it as required.  And 70% of the apps used an insecure random number generator, making the values produced easily guessed and hackable.  And of course the need for high-quality entropy in applications is something we have talked about many times.



Alissa Knight, who did this analysis, wrote:  "During this research, it took me 8.5 minutes on average to crack into an application and begin to freely read the underlying code, identify APIs, read filenames, access sensitive data, and more. With financial institutions holding such sensitive financial and personal data and operating in such stringent regulatory environments," she wrote, "it is shocking to see just how many of their applications lack basic secure coding practices and application security protections.  The large number of vulnerabilities exposed from decompiling these applications poses a direct threat to financial institutions and their customers.  These resulting threats ranged from account takeovers, credit application fraud, synthetic identity fraud, identity theft, and more."



She said:  "It's clear from the findings that the industry needs to address the vulnerable epidemic throughout its mobile apps and employ a defense-in-depth approach to securing mobile applications starting with application protection, threat detection, and encryption capabilities implemented at the code level.  Of all the findings, the most shocking was without a doubt," she writes, "the SQL queries exposing information on the backend databases hardcoded into the app, along with the private keys being stored unencrypted in different subdirectories."



So, yikes.  In other words, these readily reverse engineerable apps are using readily discoverable private keys to access sensitive financial institution backend databases via poorly encrypted SQL query credentials.  So we don't know anything about the security or accessibility of the backend databases.  But if the same developers who implemented the frontend had anything to do with the backend, it doesn't look good.  We've got no reason to believe at this point that the companion iOS versions of these apps are not every bit as poorly designed.  As we know, iOS is more tightly closed, but it's now standard practice for a common codebase to be used across multiple target platforms with only the UI customized for the particular platform.



On the other hand, if the attacks are against financial institution backend databases and services, going through the Android version of the app would likely be quicker and easier since, as we know, Android is deliberately open, which is one of its benefits for those people who would prefer to use an open source platform.  And the fact now that this knowledge of the vulnerability of these apps, although they were not specifically revealed, but it's not hard to guess what 30 of them might be on the Android platform, that's worrisome because now the bad guys know that these things are poorly designed and easily reverse engineered.  That's now public knowledge.  And thanks to the NSA's release of Ghidra, the bar to reverse engineering them has not simply been lowered, it's been dropped on the floor.  So reverse engineering this binary code is going to become a hobby for hackers.



So I have in the show notes the two charts that demonstrate the lack of binary protection that was found, the prevalence of insecure storage, unintended data leakage, client-side injection, weak encryption, implicit trust of all certificates.  Oh, my goodness.  That's like...



JASON:  Nothing.



STEVE:  Like nothing.  Execution of activities as root; world readable and writable files and directories is, like, prevalent; private key exposure, same thing; exposure of database parameters and SQL queries; insecure random number generator is a big problem.  So, yikes.



And then on the heels of that we have last Thursday's widely anticipated release of the full source code for the NSA's Ghidra reverse engineering tour de force application which appeared on GitHub.  I've got the link in the show notes to that on GitHub to remind our listeners, although I'm sure anybody, I mean, we've talked about this extensively because it is exciting, really, for the white and the black hat hacker community.  It's written in Java.  It's got cross-platform Windows, Mac, and Linux support.



It has incredible processor support:  x86 16-bit, 32-bit, and 64-bit code; ARM and AARCH64; PowerPC both 32- and 64-bit; VLE; 16-, 32-, and 64-bit MIPS support; micro; 68000 in all varieties; Java/DEX bytecode reverse engineering; PA-RISC;  the PIC family of microcontrollers 12/16/17/18/24-bit; SPARC 32 and 64; the CR16C; the Zilog Z80 instruction set; the 6502 that the Commodore 64 and the Apple II used; the 8051, which is very popular in industrial controls; my favorite, the MSP430, the little TI microcontroller; the AVR8, the AVR32, and more.



So, wow.  Like anything you could imagine, any IoT device, mainstream desktop software, this thing will reverse engineer it, provide a flow of control diagram, identify the APIs that the app is calling to.  Those allow you, knowing what the APIs are that section of code is calling gives you a sense for what that region of code is doing.  You can then begin to  interactively, because the whole thing is an interactive disassembler, or almost a decompiler, allow you to figure out, as for example this person did, of these Android apps, exactly how a piece of code is working.  The official site is https://ghidra-sre.org - that "-sre" is Software Reverse Engineering - dot org.  So ghidra-sre.org.  And, yeah, I think we're going to end up seeing some consequences of this.



Of course the tool that used to be quoted was IDA.  That was the Interactive Disassembler, which cost more than thousands of dollars per seat.  And I have a feeling that those guys are going to have a hard time selling IDA anymore.  I mean, probably upgrades and feature improvements for people who already know it, what we hear of Ghidra is that it is - there's a steep learning curve, although there's tutorials and videos and all kinds of support available online.  I have a link in the show notes to a threefold keyboard short keys guide.  I mean, it's the full package.  So I think probably in the future that's what the bad guys are going to be using because why not?



Oh, and it already has its first bug, which thanks to its wide exposure was found by a guy, a Matthew Hickey, who goes by the handle HackerFantastic.  He reported a security issue when it was first released.  He noticed that Ghidra opens debug port 18001, but it was bound to all of the system's network interfaces.  That's a mistake.  It should just have been bound to the system's localhost port.  When a user launches Ghidra in its debug mode, that port will be open; and that would have allowed anybody with access to the machine over the Internet to remotely execute arbitrary code on the analyst's system.  Whoops.  So that got fixed.  So that's the benefit.  The NSA just got a benefit from their posting this thing on GitHub and making it widely available.



So again, props to the NSA.  I've not had a chance to play with it.  My world doesn't require much reverse engineering.  I'm normally doing forward engineering, as our listeners know.  But I think we're going to be seeing the fruits of much more readily available reverse engineering in the future.



JASON:  But you know what will never change?  Facebook.



STEVE:  Oh, goodness.



JASON:  At least the story around Facebook that we're so used to.



STEVE:  Did you believe this, Jason?  Oh.  Okay.  So I actually had to go back and make sure this wasn't posted on April 1st.



JASON:  Right.



STEVE:  I mean, it's so bad.  And even though it's of Facebook, I can still hardly believe that I'm saying this.  Facebook last week was actually asking some of its users, I mean, with a straight face, to verify their identity by providing their private email address password.  I'm not kidding.  I have a screenshot.  It's actually the Picture of the Week at the top of the show notes, but I have it also here, a screenshot of this, which has been confirmed.  Daily Beast wrote about it.  It was picked up, of course, by the tech press.  Facebook has confirmed it.



So users at some point, while using Facebook, this was an interstitial that would pop up.  It says "Confirm your email address."  Okay?  It says "To continue using Facebook," I mean, I can't believe this is not malware.  "To continue using Facebook, you'll need to confirm your email address.  Because you signed up with" - and this is blotted out in the screenshot because it was a real email address at, in this case it was gmx.net was the email provider.  You can do this automatically through gmx.net.  And then it shows in this form your email is already there, whatever it is at gmx.net.  And then the next field says email password and a blank where the user is being asked to give Facebook their email password, which is none of Facebook's business.  And nobody else has ever asked this, as far as I know, anywhere.  But why not? 



JASON:  But don't worry.  They're not going to hold onto it.  Don't worry.



STEVE:  That's right.



JASON:  So you can trust us.



STEVE:  And so you fill that in, and then there's a button, "Connect to gmx.net."  In other words, instructing Facebook to log in as you to your gmx.net in this case, email to make sure that the password you gave Facebook works.  This is not that I cannot pronounce plethysmography.  This is that I'm still - I'm speechless in this case.  It's unbelievable.



JASON:  Yeah, and I mean, and somebody in chat actually poses the question that would be the alternative to this.  Is it [chops], I think, in chat?  Says why would they simply not send a message to the email account you gave them and have you confirm that you received it?  Which is what we see nine times out of 10 anyway.  That's what everybody does.



STEVE:  As everybody else in the world does.  Yes.  It's like, you know, please go to your email.  You will find a link from us.  Click the link to confirm that you are the owner of this email account.



JASON:  Right.



STEVE:  It's like...  



JASON:  It's so weird.  It's so weird.  The thinking is so bizarre.



STEVE:  You have to wonder, at Facebook, what process could a company of their size have that would allow, I mean, it's one thing like for some idiot to, like a developer person, to propose this.  But for like his boss not to say, "What?"



JASON:  Wait a minute.  You don't do that.



STEVE:  Like, "What?"



JASON:  There's somebody on staff that must have thought that.  Wait, wait, you're not supposed to do this.  This is what malware does.



STEVE:  The Daily Beast wrote:  "Just two weeks after admitting it stored hundreds of millions of its users' own passwords insecurely" - remember that I didn't even cover it on the podcast because it was like, okay, why?  Who can keep up with all of the shenanigans of Facebook?  But we learned that Facebook employees were able to look at their users' Facebook logon passwords that were stored unencrypted at Facebook.  Anyway, Facebook, two weeks after admitting it stored hundreds of millions of its users' own passwords insecurely, "Facebook is demanding" - and this is Daily Beast's words, it's a little strong in my opinion, "demanding," but okay - "some users fork over the password for their outside email account as the price of admission to the social network."



Daily Beast says:  "Facebook users are being interrupted by an interstitial demanding they provide the password for the email account they gave to Facebook when signing up."  And then they quote what I just read from the dialogue.  And then:  "Small print below the password field promises 'Facebook won't store your password.'  But the company," writes the Daily Beast, "has recently been criticized for repurposing information it originally acquired for 'security reasons.'  In a statement emailed to The Daily Beast after this story was published, Facebook reiterated its claim it doesn't store the email passwords.  But the company also announced it will end the practice altogether."  Oh, gee, how nice.



Facebook wrote:  "We understand the password verification object" - I'm sorry, I even have a hard time saying it.  "We understand the password verification option isn't the best way to go about this, so we are going to stop offering it."  Oh, we're going to stop offering you the option to tell us your email password.



JASON:  Gee.  Thanks.  So nice of you.



STEVE:  Un-effing-believable.



JASON:  But not at all surprising.



STEVE:  Oh, lord.



JASON:  All right.  What do we have in store for us next?



STEVE:  So this was just sort of on the techie edge, but it crossed my radar, and I wanted to share it with our listeners because, again, I'm sure there will be a bunch of people here who care.  With this next update, the feature release, the May 1809 - wait, no.  No, no.  The one we just had, the one last week is the October 1809.  The one I just spent the morning hoping would complete in time for the podcast.  Next month's is 1903, I think it was.  Anyway, so 1809 is the most recent feature release.  Microsoft disclosed a change in the default removal policy for external storage media plugged into any computer running this most recent version.



In their summary they said:  "Windows defines two policies, Quick Removal and Better Performance" - hmm, which would I prefer? - "defines two policies, Quick Removal and Better Performance, that control how the system interacts with external storage devices such as USB thumb drives or Thunderbolt-enabled external drives."  And here it is.  "Beginning in Windows 10 version 1809, the default policy is Quick Removal.  In earlier versions of Windows the default policy was Better Performance."  They note:  "You can change the policy settings for each external device, and the policy that you set remains in effect if you disconnect the device, then connect it again to the same port on the computer."



Okay.  So translation.  Historically, Windows has always enabled write caching for all mass storage, both internal and external.  This is especially crucial for external nonvolatile thumb storage drives for two reasons.  First, they are so very slow to write.  They often read quickly, but they've got notoriously slow writing performance.  And second, writing, as we know, inherently degrades the storage cells of nonvolatile memory and shortens the overall device life.



Okay.  It turns out that users were not reliably using the Safe Eject function for their external USB storage drives.  They were simply pulling the drives out of the machine before the drive's writing of cached data had been fully flushed and written to the drive.  So after all these years, Windows has decided to err in the direction of caution because apparently this was causing lots of problems with data corruption on thumb drives and scary notices popping up on the screen saying you pulled your thumb drive out too early.  They decided to protect users from themselves by disabling USB device write caching by default.



There are several problems with this, in my opinion.  First of all, I get it that, yes, I can see where Microsoft would err in this direction.  I'm hoping that somehow users will be notified that they have an option to change this.  People who are responsible and who understand what's going on get many benefits from write caching to external storage.  So first of all, two kinds of caching, read caching and write caching.  Read caching keeps what has been read from a slower device, whether internal or external, in RAM, so that if it's being asked for again, it's already there.



A perfect example are the allocation bitmaps for clusters on the drive.  There are bitmaps that store bits which indicate whether clusters are available or not.  While a file is being written, and new clusters need to be allocated, the system is constantly reading from that bitmap.  And as it allocates sectors, it's flipping bits.  It's setting bits in the bitmaps, saying these clusters are now no longer free for use.  So having them in memory speeds up the checking, the reading, substantially.



But here's the other thing, is allowing that cached memory to be write cached means that all of those individual writes setting bits are allowed to accrue.  And only after that region has quieted down, has calmed down, will that bitmap finally be written once out to the drive, whether it's internal or external, if write caching is enabled for the external drive.  So not having write caching on an external drive which is relatively write, not only slow, but intolerant, meaning it hurts it to write to it, it is shortening its life.



So Microsoft unfortunately felt the need to flip the default.  And I can understand it because scary dialogs popping up and corrupting external drives, that's a problem.  But for we who listen to this podcast and who are able, who know to right-click on the drive in Windows Explorer and click Eject and then wait for the notice that it is now safe to eject this drive, the reason that is all happening is that any pending writes are being flushed to the drive so that the cache is no longer "dirty," as the jargon goes, in our machine.  It's all been pushed out to the drive, which has accepted it, and it is now safe to pull it off the USB port



So the good news is there is the option, there's a Policy tab on the drive's properties.  So under the Disk Management app in Windows, you can right-click on the computer.  You'll find Management.  Go to Management.  Then go to Disk Management.  That'll show you a bunch of drives.  You right-click on one of them, do Properties.  That pops up a dialog with a bunch of tabs.  The Policies tab, the first thing there, it's called the Removal Policy.  And it has Quick Removal, which is now the default.  And it says, "Disables write caching on the device and in Windows, but you can disconnect the device safely without using the Safely Remove Hardware notification icon."



The unselected thing is Better Performance, which says, "Enables write caching in Windows, but you must use the Safely Remove Hardware notification icon to disconnect the device safely."  And then below it is write caching policy.  It's grayed out completely if you are asking for Quick Removal because there is no write caching.  If you select Better Performance, then the write caching policy options light up, and you have the option of enabling - it says, "Enable write caching on the device.  Improves system performance by enabling write caching on the device, but a power outage or equipment failure of removing it prematurely might result in data loss or corruption."  And then underneath that:  "If 'Enable write caching on the device' is enabled, then you have the secondary option of turn off Windows write cache buffer flushing on the device."



And it says:  "To prevent data loss, do not select this checkbox unless the device has a separate power supply that allows the device to flush its buffer in case of power failure."  So there they're saying we're going to allow write caching to be additionally "lazy," as the term is, which will give you further performance boost and further life extension of any USB devices that you have plugged in at the cost of then being really important that you don't yank the USB device out because basically you've said I will take responsibility for notifying Windows when I'm going to remove this thing because, even with write caching on, but they didn't say it, but the write cache buffer flushing off, you're allowing the device, the write cache to remain dirty permanently until you shut the system down.  So again, additional performance, but you really then are taking responsibility for getting the data written to the drive.  Again, you get better performance, but at that price.



So anyway, I just wanted to bring our listeners up to speed on that.  This just changed with the most recent release of Windows 10.  And so consider which storage you're using, how you're using it, and where you want to be responsible.  And for the common user, again, reading from USB is not a problem.  This is all about writing back to it.  It's better if you take responsibility, but some people can't do that.



A couple bits of miscellany.  I have a tip.  As I was saying to Jason before we began recording, I thought I knew my way around Windows pretty well.  I ran across a thing, a tip, a shortcut, a feature that I really want.  And I used to install a third-party thing in order to get it.  So Brad Silverberg, who was once upon a time a neat VP at Microsoft, I had a great relationship with him in the early days of Microsoft.  We worked together on a number of things.  I mean, it was a great working relationship.  He left a long time ago.  He's one of the good Brads.



One of the things he told me when we were talking about Windows is that he felt that one of the most underappreciated features of Windows Explorer was the ability to use "copy" on files.  That is, we're all familiar with copy and paste of text within and between documents.  But one of the ways to, for example, move a file around in Windows or between drives is drag and drop; right?  You select it, or maybe multiple, by using Ctrl to toggle selections, or Shift to grab a range.  And then you will left-click drag, or in some cases right-click drag, if you want to move them all somewhere else on the drive or move them to a different drive.  He was noting that, and it is certainly the case, that there are instances where the drag-and-drop style is inconvenient.  For example, especially later in Explorer, sometimes you have to maneuver or navigate to a different area of the file system, and you'll lose your highlighted selection, or it'll be off the screen or something.



Well, Brad noted something that I had used a lot, is you can right-click on that file or files and, for example, select Copy.  And nothing happens except that the system remembers that set of files that you have kind of like pre-copied.  And then you go somewhere else, select a drive, right-click on it, or select a directory, right-click on it, and click Paste.  And it performs the equivalent operation of having done a drag-and-drop.  The point was, he thinks that's really cool.  He really felt that was underappreciated.  And so I assume our listeners knew about it.  But if not, there's a really cool tip.



But on my Windows XP machine which died, I had a - I think it was a tool from Microsoft in the old days that was Send As.  It was an add-on to the Properties menu of files that allowed you to "Send to clipboard as."  And that's, yeah, that's what it was.  And so you could send things to the clipboard "as."  The one I used all the time was like, I would right-click on a filename that was a long filename with spaces and hyphenations and nightmares, way down some gnarly path in Windows, and I would right-click on it and say, "Send to clipboard as name."  And what that would do is it would paste that, the entire path and filename, as text onto the clipboard, which is super handy because you could then paste it into email.  If you're a command prompt guy like I am, you could do Alt Space and then EP for Edit and Paste, and it would appear, it would paste it into wherever the cursor was on the command line and so forth.



Anyway, as our listeners know, my Windows XP machine died.  I'm now on Windows 7.  Yesterday I had exactly this problem, and I had not yet dug up that "Copy to clipboard as" widget.  It turns out it's built in.  It is built into Windows.  It's always been there, and I never knew about it.  And here it is.  If you right-click on a file, you get the standard list of things, you know, Cut, Copy, Paste, whatever, Properties and so forth.  If you hold the Shift key down when you right-click, one additional item is added to that context menu, "Copy as path."  And so rather than copying the file, the way Brad talked about as being a cool way of getting ready to paste it somewhere, or maybe in some cases you could even use Cut in order to do a move, if you hold Shift key down when you right-click, you get one more item on there, "Copy as path."  And so my world is complete.



JASON:  Why do we have to work so hard for that one extra thing to appear in the context menu?



STEVE:  Wouldn't that be nice, just for it to have it there?  



JASON:  It's just another item.  I don't know.



STEVE:  I know, I know.  I agree.  But now our listeners know that they can hold Shift down when they right-click and grab, you know, copy the entire...



JASON:  At least it's possible.



STEVE:  ...path name of the file.  That's like, yay.



JASON:  Yes.



STEVE:  So I have shared a tip.  And I'm looking at time and what we have left to do.  So let's talk about built-in click tracking going mainstream.  As I teased at the top of the show, this came to my attention because the current version of Chrome 73 has this enabled, but can be disabled.  That "can be disabled" is no longer true in the staged but not yet released 74, and in the much more raw Canary 75.  We're losing the ability to turn this thing off.  Firefox has it off.  It can be turned on.  Brave, which is the privacy-focused browser, as we know, doesn't even allow you to turn it on.  It's not an option.



Okay.  So here's what's going on.  In the HTML tag, probably the very first, okay, well, I guess there had to be HTML and title and body.  Those tags had to exist in order to create a page.  But the other one was the <a> tag; right?  Just the open angle bracket "a," then you say space, href=, quotes, and then a URL, close quotes, then you close the angle bracket, and then you have the name of the link, and then you close the enclosing <a> tag, and that's - you just created an HTML link which, when you click it, takes you to where - jumps your browser to where that URL is referred to.  That's the whole hypertext, you know, web that was originally envisioned by Tim Berners-Lee.  So anyone who's done any HTML coding knows about that.



It turns out that there are many options that can also be added to that <a> tag, inside that <a> tag, in addition to that href.  If we click on the link, what we're really telling the browser is reload this page with the contents of the URL at that domain that we're provided by the server at that domain.  So if we're clicking on links on the same site, well, then the server knows that's what we're clicking on.  I mean, it knows that we're moving around its site.  If we click on an offsite link, what we're doing is we're telling our browser, reload this page with the contents of a page on a different server, from a different domain.  And essentially we are leaving that site.  So we might suggest that it's not any business of the site we're on where we went.  And in fact the site we're on would not know.  I mean, because we're on our browser, on text that our browser has loaded, and we're clicking on a link that takes us to a different domain.  So we have the right to do that.



But of course Google, for example, would disagree strongly with that philosophy.  When Google brings us, presents us with a big list of search results, they are very interested to know which link we click on of those results as we leave their site.  I mean, the whole point is we are leaving Google.  We're going somewhere else.  Thank you very much, Google, for the list of search results.  We came to you because we want to leave and go somewhere else that you found for us, thank you.



But of course Google has evolved a lot since those early days.  So they want to know - and they could argue they want to know because it will help them present better search results if they know which links people like from what their automated systems produce.  But as I said, things have changed a lot.  Now Google wants to know who we are based on which link we chose and build a profile of us.



Okay.  So in the old days the traditional way of doing this was with what was called an HTTP redirect.  And if in Firefox you right-click on a Google link on Google search results and copy the links, you know, the link address, where the link points to, and paste it into Notepad, you will see that the link does not point to the target site.



In fact, what you see in Google's results is fake.  That's not where the link points.  It's Google's results are not showing you the truth of those links.  You right-click on a link to copy the link address, paste that into Notebook, what you get is something entirely different.  It refers to www.google.com, and that link's actual destination is way back somewhere in the URL's tail.  So in this fashion a link clicked in Firefox first jumps the browser back to Google, passing it the address of the actual target so that Google, after recording that fact and the user's browser cookie, et cetera, for tracking purposes and, as I said, to presumably maybe make their future search results better, and to certainly learn more about you, then Google redirects the user to the target site.  This is the way it's always been.



And you were putting up on the screen, and I'll just mention that, for those listening, in the show notes I captured the actual URL from a Google link.  Last night I put "oh my god" into Google, and I got a bunch of searches.  One apparently is a movie from 2009, and so I just right-clicked on the IMDB link for a movie called "Oh My God" in order to see what came up.  This is the link.  And sure enough, it is - oh, I'm sorry, no, I did this all in Firefox.  And sure enough, the <a href=", and so it starts with /url?.  Okay, so of course the fact that there's no domain there, the backslash means it's going to stay on the current site.  So this link is to Google, providing a bunch of gibberish stuff, all kinds of who knows what, I mean, it's unreadable nonsense that Google understands because it produced it in the first place.  And then Google will end up sending you, your Firefox browser, on to this IMDB page.



So what's interesting is that the behavior is entirely different if you right-click on the same link in Chrome.  Do the same search, same link in Chrome.  There you receive the actual target URL directly, to which the browser will be targeted.  And I show that also in the show notes.  In Chrome you do the same thing, you get <a href="https://www.imdb.com/title/ and then the title of the page.  In other words, in Chrome it goes direct.  How does Google know what link you clicked on?  You know they're not going to let go of you unless they find out who you are and what you clicked on and where you went.



Well, the answer lies in the second argument to that URL, which is ping=" and there it is, /url? and then more gobbledygook.  So this is using a little known parameter, long since been around, as I said, like at least a decade this has been kind of talked about.  It's been around.  It's been there.  Never really formalized.  It got formalized in HTML5.  So in addition to an <a>, the anchor is what <a> stands for, the anchor tag having an href, which is what it's always had that takes you somewhere, now it can, as of HTML5, also contain a ping parameter and data.  And what does it do?  Well, like the name says, it pings any, and I do mean any, other server on the Internet with data identifying you because it's a ping from you, so it will contain a cookie from your browser for that domain.  And it is actually issuing a POST, an HTTP POST.  You know how we have GET and POST.  So it sends a POST with a length of four characters, P-I-N-G.  And in the headers, the query headers to the POST, is additional data.



So why the difference?  Because Google knows that Chrome, their own browser, supports native URL following ping tracking so that it doesn't need to bother with the time-consuming URL redirect dance.  Google generates, as I noted, very different results pages when it's rendering for Chrome versus another browser, where it cannot rely on ping tracking being present.  So it was at least 11 years ago I found a reference to it.  And unlike URL redirection, which is I would say arguably being abused in a way similar, as I mentioned at the top of the show, in a way similar to how third-party cookies are being abused, where URL redirection itself has many non-tracking purposes, ping tracking has no other purpose than tracking.  That's what it does.  It sends a ping when someone clicks a link.  And it sends a ping somewhere else when someone clicks a link.



So for many years the purists doing the web engine development held out and kept ping tracking marginalized and was not something that could be relied upon as being present.  That's true even today over in Firefox.  But as with so many other skirmishes, that war against tracking is being incrementally lost.  In today's Chrome 73, as I mentioned, it's enabled by default, but it can be disabled.  As of 74 and 75 and presumably hereafter, it's just there.  It's part of the protocol, it's part of Chrome, and it'll be on.  And one has to imagine that'll be the case with Microsoft's adoption of Chromium.  Apple's Safari also has it stuck on, as does Opera.  So at the moment, Firefox and Brave are the two holdouts.  They both have it disabled by default.  Firefox does have the option to enable it.  Oh, and get a load of this.  The parameter list for ping is actually space-separated URLs.  So not even one, but multiples are allowed.



So essentially this - oh, I was confused by my notes.  I said that the POST contains the single word "ping" because I have the details here.  Its query parameters provide Ping-From and Ping-To and the Content-Type as text/ping.  So it is made for tracking.  That's its purpose.  So what I found most interesting is that there is also no same origin policy control over ping destinations.  The browser is not constrained to only ping back to the page's origin domain.  The page can instruct the browser to ping anyone, anywhere that the page's code requests.



And at first blush, as I was thinking about this, this might seem irresponsible of the designers.  But I'm sure those arguing for this flexibility noted that URL redirects can also jump to anywhere without restriction, so why artificially restrict the built-in facility that's being offered up as it is arguably cleaner, if a bit creepier, replacement, that is, in the URL ping argument.



The two things that it has going for it is that I do agree that it's cleaner, that is, the href shows the actual destination of the link, and then the ping shows the site or sites who are going to be notified when you click on that link.  So it's a little more straightforward.  The other thing, though, is - and I think this is the part of Google that is really wanting to optimize and improve speed and improve security.  And there's a lot of Google that is that way.  This allows these processes to be done in parallel, which I would argue makes it worthwhile.  A redirect is inherently a serial process.  In the case of clicking on a Google link, you are going to Google.  You're staying on Google, pulling up, like making a query to Google that then logs whatever it does, figures out all of this gobbledy-gook in the URL, and then it gives your browser a 302 redirect to the destination, and you go there.



I would argue that doing this in parallel probably gives the user a snappier response because their browser is immediately jumping to the site given in the link.  And the documentation makes it clear that the ping is done asynchronously in parallel, thus not delaying their arrival at the new site by bouncing them among one or more tracking sites first.  I don't know if anyone has ever noticed their browser's URL address field when this redirection chain gets really ridiculous, you'll see it flashing with all kinds of nonsense.  Sometimes that occurs.  You'll notice it like when a site is unresponsive or slow, your browser has a chance to update the URL field, and it's like, what the heck?  And then you may, if you're lucky, finally get to where you're going.  Well, that obviously delays you getting to where you want to go.



And so the use of ping instead of this at least allows this to be done in parallel and doesn't allow any of the redirection sites to waylay you if they would want to because of course your browser is turning, as it jumps from one of these to the next in a redirect chain, is turning control over to that site.  It doesn't have to forward you on to the destination if it didn't want to.  So this is cleaner from that standpoint, and probably faster.  So I don't know when Firefox and Brave will finally capitulate.  I think that they might as well do so, frankly, although it would be sorry to see it happen, because redirection chains work to solve the same problem, and they cannot be blocked without breaking lots of other useful things.



So I don't like the idea of making tracking explicit like this.  It seems like we've lost another bit of our online freedom.  But as I said, it reminds me of the way things once were with JavaScript, where it was practical to turn it off and turn it on only on sites that needed it.  Now you really can't use the web without JavaScript running code on your browser.  So I think similarly there is no practical means of blocking tracking unless you really, really want to spend your life trying to do so.  I just think this is a battle that the modern web has forced us to lose.



And so those of us who consider ourselves purists are like, well, okay.  I would argue we do get something in return, which is faster response because this tracking can now be done in parallel, not in series.  At some point I think Firefox and Brave - maybe not Brave.  Brave may just hold out, although in this case they're not really giving us any substantial privacy improvement because, as we've seen, Google issues a URL with redirection if it can't use ping.  So we might as well just let it use ping and have our links followed more quickly with the ping happening asynchronously and in parallel in the background.



JASON:  Hmm.  It seems, I guess, what you just said there kind of answers the question that I had, which was just stripping out the ping entirely.  But then it's going to fall back on a different tracking mechanism anyways.



STEVE:  Right, right.



JASON:  Because it seems so easy to just do that.  I'm sure that would be very easy, to just look for the ping, remove it, but there you go.  All right.  Have we missed anything?  I'm digging into the cracks to see if there's any new insecurities.



STEVE:  We're at two hours, and that's our goal and our show.



JASON:  I think we did it.  Right on.  Steve, wonderful stuff here.  You know all about GRC.com.  Well, if you don't, you should know all about GRC.com.  That's where you can find everything that Steve is working on.  Of course SpinRite, which he didn't get a chance to talk about today, but SpinRite, the best hard drive recovery and maintenance tool.  You can get your copy there.  Information about SQRL, updates, where you're at, where you're going with SQRL.  People are eagerly following that, and you can find that at GRC.com.  Audio and video of this show, of course, transcripts of the show that you can only find there:  GRC.com.



And then of course our website is TWiT.tv/sn.  There you're going to find also audio and video of all the episodes of Security Now!, including this one.  If you want to start, go back in time, you could do that.  You can binge on all of the episodes back to back for literally weeks and weeks to get through them all.



STEVE:  A surprising number of our listeners do.



JASON:  I'm not surprised. 



STEVE:  And more recently we've just been doing pure news because, I mean, we've been doing two hours a week of pure news.  There's so much to talk about.  But back, I don't know why it was, I guess things were quieter once upon a time, but we really, we did some deep dives into the fundamentals of processor technologies, how the Internet works, I mean, there's a bunch of really good, timeless tutorial stuff back there.  So I would commend our listeners to considering it, if they're looking around for something edifying to listen to, after they catch up on all the news of the week.



JASON:  Absolutely.  That's right.  That's right.  TWiT.tv/sn.  And also you can subscribe there, of course.  This is a podcast, so all the subscription details are there for audio and video.  If you want to check us out live, this show records live every Tuesday starting at around 1:30 p.m. Pacific, 4:30 p.m. Eastern, 20:30 UTC, although that might be out of date.  It might be different from that.



STEVE:  Maybe it's 19:30, I think.



JASON:  19:30, thank you.



STEVE:  I think it goes between 19 and 20, yeah.



JASON:  I lose track.  It's one of those.  Just go to TWiT.tv/sn, you'll find the actual details.  And I'm sorry I can't be more informational or solve that problem for you right now.  But Steve, you've solved many problems for people on this episode.  Really appreciate it.  It's always fun doing a show with you.  Thank you, Steve.



STEVE:  Thanks, Jason.



JASON:  We'll see you.



STEVE:  Until next time with you.



JASON:  That's right, until next time.  But next week it'll be Leo returning on Security Now!.



STEVE:  Okay, buddy.



JASON:  Take care, everybody.



STEVE:  Bye.



JASON:  Bye.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#710

DATE:		April 16, 2019

TITLE:		Dragonblood

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-710.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we discuss a malicious use of the URL tracking "ping" attribute, more on WinRAR, more third-party AV troubles with Microsoft and other new trouble from last week's Patch Tuesday, good things that Patch Tuesday accomplished for Microsoft and for Adobe, another security-tightening change being proposed by Google, Russia's Roskomnadzor finally lowering the boom on Facebook, and the incredible TajMahal APT framework.  We touch on a bit of miscellany, answer a SpinRite upgrade question, and share some closing-the-loop feedback from our listeners.  We close with a look at Dragonblood, the first effective attack on the new WPA3 protocol (which didn't take long).



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  I'm back.  We're going to talk about the best version of Windows, one you probably can't get a hold of.  We'll also talk about URL ping tracking, a little backtracking from Steve.  And then the continuation of the WinRAR nightmare.  It's all next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 710, recorded Tuesday, April 16th, 2019:  Dragonblood.



It's time for Security Now!, ladies and gentlemen.  I give you Steve Gibson, our man of the hour.



STEVE GIBSON:  Welcome back from the sunny equatorial Hawaii; yes?



LEO:  Oh, man.  I just - it's so nice there, 80 degrees every day.  If it rained, it would rain for a few minutes torrentially, and then the sun would come out.



STEVE:  A little humid for me.  I've never been a big fan of humidity.  I need evaporative cooling in order to survive, and there's a lack of that down on the Equator.



LEO:  I embrace it.  Remember, you're wearing shorts and Hawaiian shirts.  You're not fully dressed at any given time.



STEVE:  Okay.  That's true.



LEO:  So did anything happen while I was gone?



STEVE:  Oh, my goodness.  In fact, one of our stories, it was the topic of last week's podcast, was URL ping tracking.  Because it turns out that there's an attribute in the good old <a href>, HTML <a href>, which are the hyperlink click on it that explicitly pings one or more servers when you click on the link.  And it was brought to the forefront because the next two versions of Chrome will be removing the ability to disable it.  Now, it really doesn't matter because it's enabled by default, and so nobody - I love the term the "tyranny of the default" because, as we know, default is what everyone runs on unless something cataclysmic happens or they're listening to this podcast.  But so that was the topic last week.



And so our first topic is the - it turns out I was a little short in my imagination because I said that the only purpose it could possibly have was for tracking.  Well, it turns out we have a malicious use of ping tracking in URLs has appeared.  We've got more on WinRAR that just is the vulnerability that just keeps on giving.  More third-party AV troubles last week with Patch Tuesday with third-party AV and of course Microsoft Windows.  We've got other new trouble from last week's Patch Tuesday.  We also have good things that Patch Tuesday accomplished, both with Microsoft and also with Adobe, who did a Patch Tuesday.



We've got another security tightening change being proposed by Google.  We've got Russia's Roskomnadzor finally lowering the boom, Leo, on Facebook; and the incredible TajMahal APT [Advanced Persistent Threat] framework that Kaspersky has uncovered.  We'll then touch on a bit of miscellany, answer a SpinRite upgrade question, share some closing-the-loop feedback from our listeners, and then we take a look at this week's topic, which is Dragonblood.  That was the name given to the first effective suite of attacks on the forthcoming WPA3 protocol, which we've looked at briefly.



And our listeners may remember that when I heard about it I was all excited, and I went over to the Wi-Fi Alliance, and it looked like they had changed their stripes, and they were making the specs available.  And so I was going to roll up my sleeves and have some fun spec reading.  And when I clicked on the links it was like the table of contents was all you got, which was like the worst kind of tease because it's like, ooh, here's all the things we're not going to tell you about the WPA3 protocol because you have to be a member and pay dues and then be in the cloistered community.  And of course at the time I commented on what a mistake it was, and what a mistake they continue making by attempting to develop a specification that is as crucial to the health of everything, of the world's networking, as our WiFi protocol, which they insist on keeping closed.



And in fact what's interesting is that these academic researchers - who, by the way, were the people that designed or found the K-R-A-C-K, the KRACK breach in WPA2, so they know their way around WiFi - they comment several times in their 16-page research paper about how this could have all been avoided across the board if this WPA3 development had been done in the open as it should have been, rather than behind closed doors.



So anyway, I think another great podcast for our listeners as we - it's funny because Elaine also corrected me.  I misspoke a couple weeks ago I guess about where we were in the history of the podcast.  She said, "Steve, just to correct, in four months you'll be ending year 14."



LEO:  Yes, yes.



STEVE:  So I think I keep forgetting that I'm as old as I am.



LEO:  This is - we just had the 14th anniversary of TWiT was I think this past weekend, April 15th, so yesterday.  And of course I neglected to say a word because I don't really pay attention to that kind of stuff.  Been doing it a long time, you and me, Mr. G.



STEVE:  Indeed.



LEO:  But we shall do it for several hundred more.



STEVE:  Yes.



LEO:  Two hundred and eighty-nine, to be precise.



STEVE:  That ought to do it.



LEO:  And then everything will be patched.



STEVE:  By then, yes, we will have solved the world's security problems, and it'll just be like, okay, did anything happen, Leo?  No.  Okay.  Thanks.



LEO:  I did the math at one point.  I think it's September 2024.  We're here at least, Steve has promised, at least through then.  If we can convince him to move to hexadecimal numbering, maybe even longer.



STEVE:  So I had two pictures this week because I had mentioned to our listeners maybe a month ago, some time ago, that I'd run across - maybe it was listening to Paul and Mary Jo - a comment that the Windows 10 Long Term Service Channel does not come with all of the preloaded crap that you get with the normal consumer Windows 10.



LEO:  But not even the consumer, the business Windows 10.



STEVE:  Well, the professional, Windows 10 Professional.



LEO:  Yeah, Pro, yeah.



STEVE:  And so the first picture here - so what I did was, because I have access as an MSDN developer, for which I pay Microsoft hundreds of dollars a year to have access to all the versions of everything for software testing, I thought, okay, I've got to give this Long Term Service Channel deal a try.  So I took an empty laptop, brand new SSD.  And first I installed a clean install of Windows 10 Professional.



LEO:  Not even Home.  This is the Pro version.



STEVE:  This is the Pro.  I mean, and I put "professional" in quotes in my title because I've never, I mean, this is what I've been complaining about.  If they had named it Windows 10 Arcade version, then I would have no problem with it.



LEO:  I know, I don't get this, either, yeah.



STEVE:  You know?  We've got sausages and hamburgers, and we've got some guy pruning some hedges with his clippers, and we've got...



LEO:  Because the pros love solitaire.  Everyone knows.  They've got to play Candy Crush.



STEVE:  Over in the menu we've got two different Candy Crushes.  We've got Candy Crush Friends and Candy Crush Saga.



LEO:  Yeah, and Cooking Fever.



STEVE:  It's un-effing-believable.



LEO:  I have a PowerShell script I run first thing, before I even open that menu, to delete all that crap.  It's terrible, I agree.  It's ridiculous.



STEVE:  It's just unbelievable.  And so then I thought, okay.  I mean, here I am.  Why do I have a screenshot?  I hit Print Screen twice.



LEO:  And then wipe the drive.



STEVE:  Then I used 3D Print, which was whatever that is installed automatically.  But I pasted the screen and saved it off to an attached drive, wiped the hard drive, and then I used an ISO of LTSC, the Windows 10 Long Term Service Channel.  That is the second picture in this week's show notes.  And it's just, I mean, it should be called "Ohhh."  I mean, it's for...



LEO:  Now, how do you get this besides MSDN?



STEVE:  Or it ought to be called NCSC.



LEO:  What's that?



STEVE:  The No Crap Service Channel.  I mean, look at it.  It's empty.  And look at the menu.  Okay, so there's a "D" for Dell because I was doing this on a Dell laptop.  There's nothing else.  I mean, just under "W" are the various Windows things.



LEO:  This is nice.  This is clean.  This is the way you want it.



STEVE:  There's no Cortana.  That doesn't even have Edge.  There's no media crap.  I used the "N" version, which is the European version, because of course they're more upset with Microsoft about installing things that they feel is too strong.  So, I mean, it's just - there never was anything there.  So it is the case that consumers cannot get this.  This is an enterprise build, so this is available to enterprises.  And the point is that it doesn't do this constant feature updating, although it is constantly rereleased with the updated set of features.  And each release has a 10-year commitment from Microsoft for being kept up to date from a security standpoint.  So the best thing Microsoft could do would be to give regular people the option to have this.  There's no reason, I mean...



LEO:  They did for a while do the Windows 10 Signature Edition.  And they tried to talk OEMs...



STEVE:  Do you think Netflix is paying them to...



LEO:  Yes.  Of course it is.



STEVE:  I mean, I'm looking at Netflix here on the screen.



LEO:  These people aren't there for free.  They're all paying for it.  And the OEM adds more; right?  That's when the OEM puts McAfee or Symantec on there and all that other crap.



STEVE:  Well, and then you have all these tease-y versions of stuff.  You have these stubs where it's like, the first time you click on something it's like, oh, you're interested in this?  Hold on a second.  We'll download that for you.



LEO:  Yeah, because they're not really installed.  They're just a picture, yeah, yeah.



STEVE:  Exactly.  And in fact, if you open the Start Menu the instant it boots, what comes up are a whole bunch of blank tiles with download arrows because even the ISO cannot hold all of this crap ware.  It won't fit on one disk anymore.



LEO:  I have to admit, this looks good.  I want this.  This is the menu.  The only thing I note is there doesn't seem to be a browser.  How are you supposed to put Chrome on there, or Firefox?



STEVE:  Yeah, that's a good question.  I guess you'd have to, you know...



LEO:  Your network IT guy.



STEVE:  Yeah, get the network install version and then stick it on a thumb drive in order to bring it over.  But, I mean, it's just a breath of fresh air.



LEO:  I know.



STEVE:  It's just like, okay.  Although I'm still really happy with 7, I am LTSC man here on out.  I mean, I'll pay my MSDN dues in order to have access to this thing because it is just like, oh, look at it.  It's just wonderful.



LEO:  So, I mean, I can duplicate pretty much this, but I have to run a bunch of scripts, and I have a bunch of tools.  And I do notice that, if you delete those big tiles in the Start Menu, your Start Menu slims down.  It doesn't stay open that wide. 



STEVE:  Correct.



LEO:  It looks like this.  So you can get it close to this.  But it would be nice to have a version.



STEVE:  Yes.  And until now that's what I've been doing.  I first of all go through, I mean...



LEO:  You decrapify, yeah.



STEVE:  Yes.  I also run a couple PowerShell scripts to remove all of that junk.



LEO:  Right.



STEVE:  And then you go through and just delete, delete, delete.  It takes like, you know, an hour or two in order to prune a system to where it's like, okay, this is what I wish I had been given.  But, oh.  Anyway, I just wanted to plant this idea somewhere that, you know, there are a bunch of people who would like to own, who would like to have this LTSC.  And I guess the problem is no one's buying Windows 10 anymore.



LEO:  Yeah, they give it away.



STEVE:  It comes preinstalled on anything that you get.  And so because no one has to pay for it because of all the jumping tiles that you have to tolerate.  Yeah.



LEO:  Yeah.  I know what you mean.



STEVE:  Okay.



LEO:  I just got my ThinkPad Extreme, which I love, my X1 Extreme.  Amazing laptop.  Love it.  But of course the first thing you have to do is decrapify it.  And then I got your great TeraByte software Image for Windows.



STEVE:  Nice.



LEO:  And I love that and immediately image it.  Fortunately, Microsoft does put a recovery image on the hard drive anyway.  So it takes that, makes a recovery USB key, makes it very easy.  And now I can get back to the - I basically built my own LTSC.



STEVE:  Yeah.



LEO:  Because you pay, it's like 800 bucks a year for MSDN.



STEVE:  Yes, that's what it is, yes.  I mean, so it's not nothing.  But it does, well, for me it pays for itself because I have access to whatever I need when I'm testing software on various platforms.



LEO:  Yeah, yeah.



STEVE:  Okay.  So I need to start right out, as I said before, acknowledging a failure of my imagination.  Our listeners will recall that last week's podcast, as I mentioned at the top, was URL Ping Tracking, where I described the HTML5 feature of the "ping" term which can be added into the <a href=> anchor tag in order to cause the browser to asynchronously send a ping POST to anywhere.  And I commented at the time that - oh, and Leo, you did miss some fun stuff because I looked at the source of a Google search page from Chrome, you know, right-click and then view source.  And the URLs were clean because in every one of the URLs was a ping reference in order for Google to track which link I clicked.



And I acknowledge that there are reasons why Google would need to know what we clicked on, other than for just tracking us and profiling us and building up a profile of us for their advertising business.  And that is it helps their search results if they present a bunch of URLs, and they see what people tend to choose from a page that a human has never looked at before.  Now a human is looking at it.  So there's some value there.



If you bring up the same page under, for example, Firefox, what you see is that the URLs do not point to where the link is taking you, but they all of course point back to Google, and in the URL tail is the actual destination.  So when you click on the link, you go back to Google.  Google registers that, sees where you're actually wanting to go, and then redirects your browser there.



So the point is that, without this ping tracking, we're using URL redirection in order to achieve the same thing.  And I got some feedback from my listeners who were disappointed last week in my kind of resignation to the fact of tracking.  It's like I said, you know, well, the browsers are going to end up giving up on this.  We are going to have de facto ping tracking because it's now in the HTML5 spec.  Chrome is removing it from even the ability to disable it.  I did learn that uBlock Origin automatically blocks it, so there's another little benefit of using uBlock Origin.  I mean, it comes for free.  It just blocks the ping tracking.



Anyway, so back to my failure of imagination.  It turns out that the sole purpose of the ping term is not only tracking.  It has been used as an effective DDoS attack.  It turns out that it allows JavaScript to edit the ping term and to then programmatically click the URL to launch these ping queries at any other website.  One of the things that I noted last week is that there is no same-origin protection for this ping.  That is, you can ping anywhere, not just the origin from which the page came.  And I talked about how at first blush that's like, wait a minute, you know, is that good?  Except that I'm sure that people who are working on the spec noted that, well, the URL could go anywhere, so URL redirection has no same-origin restrictions, so why should ping tracking?



Well, one of the consequences of no same-origin enforcement for the ping that has already been done in the wild is that these pings can be aimed at a site that you wish to attack, and JavaScript is able to trigger the URL, which then triggers an offsite ping.  What happened was that Imperva Research uncovered a DDoS attack utilizing these HTML pings to perform a distributed denial of service attack on various gaming websites.  In one attack which they monitored, which peaked at 7,500 requests per second, a total of 70 million requests were generated from approximately 4,000 IP addresses over the course of four hours, which substantially loaded, basically buried the targeted server under relatively expensive requests.



I mean, they are short queries.  But in terms of an HTML request, that's more than, you know, 7,500 per second is more than most sites are equipped to handle.  As we know, Safari and Opera, we covered this last week, offer no provision for disabling this behavior.  It's enabled by default in Chrome, and Google is planning to, I think we're on 73 now with Chrome; 74 and 75 have removed the option to disable it.



It is still disabled by default under Firefox and Brave.  Firefox offers you the option to enable it.  Brave doesn't even offer you the option to enable it.  So good on them.  But it does look like, as a consequence of this kind of abuse, that our browser designers are going to need to come up with some way to preserve this functionality, which they've pretty much all capitulated to.



I also mentioned last week that this ping term, it was familiar to me when I saw that Chrome was removing it, which is what put it back on our radar for last week.  It's been around for a decade.  But it's sort of like no one was in a hurry to do it because it was just pure and simple tracking.  I mean, that's what it was for was for "link auditing" is where it's euphemistically described in some places.



So I think what they're going to need to do is to come up with some way to prevent its abuse.  Maybe prevent script from modifying it in the DOM.  I don't know what they'll do.  Or maybe reconsider not putting a same-origin policy limit on it, as so many other things in our browsers currently have.  And we often talk about the same-origin limitation being hugely responsible for security.  If you could only ping back to the site which had issued the page, then you could make it that site's responsibility to ping other people if it chose to do so.



And just for the record, Leo, because this is, I mean, the coolness of this is that - and I'm sure it's part of the reason that Google likes it is that it is an asynchronous query.  That is, if you use the old-style URL redirection approach, then when you click on a link you go back to the site that issued the page first.  And then it redirects you to your target.  If you use the ping approach, the browser does both at once.  The URL you're clicking on is your target.  So you go directly to that page while, in the background, the browser launches a separate thread which follows the ping reference in the URL to notify the site.



LEO:  Yeah, you wouldn't want it to hang things up while it did all that.  That would be - yeah. 



STEVE:  Right, right, right.  And so if you were to impose a same-origin policy, then you could still get the speed increase of asynchronous operation.  But the site that was pinged back to could, if it chose, issue its own pings to other third parties, rather than having it done by the user's browser.  So it's actually a little cleaner, too.  It'd be nice to see that maybe people will say, oh, whoops, we need to just impose a same-origin policy.  I mean, it probably takes half a line of code to do that because all of the logic is already in our browsers for taking care of this.



WinRAR.



LEO:  Oh, boy.



STEVE:  I know.  We've been talking about it every single week since it happened.  Sophos's Naked Security, the title of their report from yesterday said:  "An Ancient WinRAR vulnerability made public in February is now well on its way to becoming one of the most widely and rapidly exploited security flaws of recent times."  That's what's happened.  And the reason is, as we've discussed before, there are so many copies of this out in the world now, half a billion copies.  And there wasn't an upgrade mechanism.  And again, I'm not faulting the WinRAR guys.  I don't know if it was last week or the week before, Leo, but I did get email from them.  I think it may have been last week, so you wouldn't have heard this.  But because I'm a registered user and have supported them also because it has been my favorite tool, although I did hear Paul say that he likes 7-Zip now as...



LEO:  Yeah, I've been using 7-Zip, too.  I haven't used WinRAR in a long time.



STEVE:  Yeah.  Anyway, a lot of people have it, and that's why it's being exploited so widely.  So their coverage from yesterday was titled "Flood of exploits targeting ancient WinRAR flaw continues."  The latest evidence is a report from Microsoft's Office 365 Threat Research team which identified it as being used by the MuddyWater - there's a name for you, the MuddyWater Advanced Persistent Threat Group, yeah, muddies the water - to target organizations in the satellite and communications industry.  And it turns out, as we've said, WinRAR was far too tempting for cybercriminals to ignore, and within days stirred up a hornet's nest of exploits to the tune of 100 exploits or more.



So Microsoft's blog about recent targeted attacks serves as yet another warning to organizations or individuals.  If anybody listening to this still hasn't done it, I don't know why, but maybe you missed a few podcasts like, you know, since February.  If you haven't updated or removed WinRAR yet, you really do need to do that.  We need word to get around as much as possible since, as we've noted previously, unregistered users, or users who no longer maintain their registered email accounts, that is, you may have registered it 10 years ago, and then with an email account you no longer have, so the WinRAR guys who did send out a notification to all the email that they had, you may not have received it because that was so long ago.



So I'm glad that Microsoft has done this because, again, the more attention this gets, the more people will have the opportunity to fix this.  Otherwise it's just going to sit there and be a means for bad stuff to get into people's systems.  Microsoft detected the threat to their Office 365 early last month, in March.  The APT attackers used a Word attachment, claiming to be from the Ministry of Foreign Affairs of the Islamic Republic of Afghanistan.



LEO:  Oh, boy.



STEVE:  So I'm not sure who they're targeting.  If I were to receive that email, it's like, I don't think so.  But opening it triggers a download from a OneDrive link, which has since been shut down, which downloads an archive containing a second Word file, within which is embedded a macro which initiates the payload in the form of a PowerShell script which opens a command backdoor, allowing the attackers to deliver the malicious ACE file which contains the exploit.  That is known as a chain because there are so many steps in it, or so many links in the chain, so many steps involved.  So it's a bit convoluted because the attackers need to induce the user via a bogus warning dialog.  Apparently they're in a hurry to get the system restarted.  So they show a warning dialog, insisting that the user needs to restart their PC.  And as we know, the ACE exploit causes something to be put in the user's Startup folder so it doesn't actually get invoked until they do restart their PC.



So as has been noted in the coverage of this, while the entire exploit chain will not succeed every time, it's a numbers game targeting multiple individuals inside a specific organization, thanks to the nature of "This is the Ministry of Foreign Affairs of the Islamic Republic of Afghanistan."  So, yeah, not aimed at everybody.  Sophos said no one should assume that, just because the attacks detected so far had been connected to nation-state actors, that this will always be the case.  Commercial exploits won't be far behind.  And they said WinRAR's half a billion reported users is a lot of victims to aim at.  So just, again, another reminder.



So it turns out that there were a bunch of problems caused by last Tuesday's patch update, Patch Tuesday update.  It resulted in relatively widespread problems for users of a number of major third-party AV systems.  As we noted before we began recording, I think we were talking about this, Leo, the widespread problems were predominantly caused, although there were some Windows 10, there were problems under Windows 7, 8.1, Windows Server 2012, and Server 2012 R2, which were causing these systems to freeze, be unable to boot, or to hang on installing updates.  And it also appears that some Win10 users were also affected.



According to support articles from Microsoft, Avast, Avira, and Sophos, there is a conflict between some of the recent updates and AV software including Sophos's Endpoint Protection, Arcabit,  AVG Business Edition, Avira antivirus, and Avast for Business and CloudCare.  In the case of Sophos, in their support article they state that the updates could cause Windows to fail to boot.  Reports from users also indicate that the update process may hang at the configuring updates stage.  Everybody involved is aware of the problems.  And at least in the case of Sophos, Microsoft has written:  "To prevent further issues, Microsoft has placed a block on the conflicting updates so that they are not offered to users running Sophos Endpoint until a solution is made available."



I have details in the show notes.  But basically it's all of the updates from April 9th, both the security-only update and the monthly rollup affecting Windows 8.1, Windows 7 SP1, Windows Server 2012, in every case both the security-only update and the monthly rollup containing that same update.  Avast has looked at the same things, and they've identified the updates which are causing problems, and they are not the same ones that are causing problems for Sophos.  So they had actually several different ones.  Avira has looked at the problems, and they found that they were having problems both under Windows 10 in their case and under Windows 7.  They suggest uninstalling these Windows updates for now.



And then Sophos needed to provide some support for their customers that were getting stuck when Windows fails to start, freezes, or gets stuck at configuring updates.  Their recommendation is to boot into safe mode, disable Sophos antivirus service from safe mode, reboot into normal mode, uninstall the respective Windows Knowledge Base updates, reenable the Sophos antivirus service, and then of course reboot in order to bring everything back together.  And they said, if enabled, tamper protection will need to be disabled to reenable the service.  So a lot for end users to deal with.



On the other hand, I guess this is what comes with the territory if you want to use a third-party AV now because we have been talking about now an increasing number of problems that third-party AV is having with newer versions of Windows.  Also Gnter Born of Borncity reported, based on the version of Windows 10 that you had, that there were various updates for 1709, 1803, 1809, and 1903 which were causing problems.  So he enumerated those.  I have these in the show notes, for anyone who's interested.  BleepingComputer, paraphrasing from their report, they said users are reporting that, after installing this week's Microsoft April 2019 Patch Tuesday updates, that Windows has suddenly become slow, and programs are taking "forever," in quotes, to open.



And I think this was Lawrence who wrote that:  "We have received emails and seen reports from users who have stated that this week's updates are also causing Windows to become very slow.  The reports have been from users running Windows 10 and Windows 7.  The issues that users are experiencing include Windows taking a long time to start or reboot, unable to start programs, a lag in games, excessive disk activity, video streaming issues, and other similar problems."



He said:  "For example, in a comment at BleepingComputer a reader has stated that their Windows 10 computer has become extremely slow and that rebooting/starting Windows takes forever.  Users on Reddit," and he had six references, "and elsewhere," two references, "are also complaining that Windows has become very slow since installing last week's updates."



He wrote:  "Normally, when a user has an antivirus program, Windows Defender will disable its real-time protection."  He said:  "It seems that, for this user at least, Windows Defender is being enabled automatically, even though the user had Avira installed on the machine.  Having two antivirus programs performing real-time protection," he wrote, "could definitely cause slowdowns and other issues."  He says:  "At this time there's nothing from Microsoft that states they're aware of the reported issues.  The only reference to Windows being slow since the updates is from a support article posted yesterday by Avira" - is it Avira?  Avira?  Anyway - "that is simply titled, 'Why does my system run very slow?'  This article states that, if Windows 10 has become slow, you should remove the 509 update.  For Windows 7 users, they state you should remove the 472 and the 448 updates for Windows 7."



He says:  "As these instructions are for users of their software, it may not apply to everyone."  BleepingComputer has reached out to Microsoft, but has not heard back.  Two hours after they published that, they updated their article to note that Computer World's Woody Leonard also reported he is seeing users having slowdown issues on Windows 10 after installing these updates.  And then two hours after that, they updated again, saying that BleepingComputer has been told by a source familiar with the matter that these issues are being caused by conflicts between the recent updates and AV software.  In other words, it's not just complete freezing and failure to reboot or hanging, but it may be that the system finally does get going, but then is operating very slowly.



So it really sounds like, you know, if we step back from this, Microsoft made some changes that caused AV software to no longer interact properly with Windows, not just older ones, but even with Windows 10.  So this needs to get sorted out.  And just a heads-up if any of our users have encountered this.  In the show notes I pretty much captured everything with which updates can be uninstalled in order to get systems running again.



LEO:  His point is probably it'd behoove people just to stop using third-party AV; right?



STEVE:  Well, our listeners know that that's how you and I feel.  We have talked about Windows Defender, the fact that, looking at it, it is now doing an at par version, I mean, an at par level of protection.  When we most recently looked at it, I think I recall that it was finding everything that others were.  It had a slightly higher false positive detection rate.  But I've never had it do a false positive.  Although actually...  



LEO:  No, I don't think that's an issue at all.  I've never even seen a virus warning ever.  Ever.  



STEVE:  Actually, was it last week or the week before, I deobfuscated a URL to something which was in the show notes.  And I got reports from our listeners that Windows Defender was calling the show notes, like flagging them as dangerous.  So someone is watching.



LEO:  That's funny.



STEVE:  Windows Defender is watching.  And yeah, Leo, I mean, I just, I mean, I understand people develop an affection for the AV that they're using.  They have come to rely on it.  They have a subscription.  They've paid for a year of it.  But the fact is we saw this before with third-party firewalls where, to do their job, they had to sink their hooks deep into the OS, into the kernel, and it was causing problems.  Then, famously, with XP, they had a firewall, but it was disabled by default.  But it was there.  And then finally with SP2 they enabled it by default.  And now third-party firewalls are kind of like, okay, you know, Windows comes with one.  Well, Windows comes with a free good AV.  And we are now seeing, I mean, I'm not a conspiracy guy; but boy, you know, suddenly there seems to be all of this trouble with third-party AVs.



LEO:  Oh, you think Windows ain't done till the AV won't run?  Is that it?  Aha.  I think it's, I mean, in fairness to Microsoft, I think it's more likely that these AVs hook so deep into the OS.



STEVE:  They do, yes.



LEO:  They hook into the kernel.  I mean, that's the problem, really.



STEVE:  Yup. 



LEO:  Although you've got to wonder, they're not small companies.  They're well-known names.  And there are hundreds of millions of Avira users and other users.  Microsoft knows that.  You'd think they'd test with that.  



STEVE:  Well, yes.  And Microsoft is, as we know, is releasing these rollups ahead of time.  I've already got next month's rollup sitting there.  I'm not touching it, lord knows.



LEO:  Right, right.



STEVE:  But, you know, they have the preview...



LEO:  Are you on the insider ring?  Is that why?



STEVE:  No, you always get that, you know, under optional updates is the preview of the next month rollup.  And so since those are causing problems, you would think that the AV guys are like trying them and saying, whoa, Microsoft, agh.  Or working to fix their problem preemptively before Microsoft is able to roll these things out officially.  I mean, you have to be way on the inside to know.  But boy, for what it's worth, you know, I've got AV.  It's scanning all of my stuff all the time, and it's called Windows Defender.  And it's not causing Windows any trouble.  So, and I know you're in the same position.



LEO:  That's so, yeah, yeah.  Why should I install those patches, Steve, given what you've told me?  Why?  Give me 74 good reasons.



STEVE:  Certainly covering what a disaster, well, especially two out of 74.  Last Tuesday's patch did fix some very important things.  It repaired 74 security flaws, including two actively exploited Windows zero-days.  So our listeners may remember that makes it the second month in a row that a pair of actively exploited zero-days were patched by Patch Tuesday.  The two zero-days specifically are very similar.  Both are elevation of privilege vulnerabilities impacting the Win32k.dll, which actually should seem familiar or sound familiar because that's what it was also last month, which is of course one of the core components of the Windows OS kernel.  Those two problems were discovered and responsibly reported separately by two different security teams, the Alibaba Cloud Intelligence Security Team and Kaspersky Lab, who have been very busy.  We'll be hearing more about Kaspersky in a bit. 



And Microsoft describes the two zero-days identically in their coverage.  They said, and it's a little bone chilling:  "An elevation of privilege vulnerability exists in Windows when the Win32k component fails to properly handle objects in memory.  An attacker who successfully exploited this vulnerability could run arbitrary code in kernel mode," which is even worse.



"An attacker could then install programs; view, change, or delete data; or create new accounts with full user rights.  To exploit this vulnerability, an attacker would first have to log onto the system," and as we know, or be under the user's login.  They said:  "An attacker would then run a specially crafted application that could exploit the vulnerability and take control of an affected system.  The update addresses this vulnerability by correcting how Win32k handles objects in memory."  So yay for that.



We have no further details about the exploitation of the vulnerabilities, which is good because we know that it takes people some time to get these things patched, and this would be something bad guys would love to jump on.  So, but we do know that they were found in the field being exploited.  So thus they are zero-day vulnerabilities.  And Kaspersky has reported to Microsoft six Win32k elevation of privilege zero-days in the past six months, all of which they found being exploited by a nation-state-affiliated hacking group.



So we could safely assume that the one Kaspersky found this time is probably number seven in that particular hit parade.  And what this suggests is that somebody somewhere has a bunch of potent zero-days which are being found by, in this case Kaspersky, by observing their use in the wild.  So who knows how many more exist that have not yet been found.  Aside from the Windows zero-days, not surprisingly among the remaining 72 flaws which were fixed, there were three Office Access Connectivity bugs that can allow attackers to execute code on vulnerable systems, all which can be exploited remotely.



Another code execution bug impacts the Windows GDI+ component when parsing EMF files.  And that's a worrisome vulnerability since it can be exploited merely by convincing users to visit a website or by emailing users a malicious file because anything that can cause the EMF image to be displayed can potentially invoke this flaw in GDI+.  So those are bad, and those are fixed.  So yes, despite all the problems with last week's Patch Tuesday - whoops.  Sorry about that.



LEO:  What does that mean?  I've never heard that one.  Was that Pebbles?



STEVE:  That's my "you've got mail," which is a sound file that I got from CompuServe, of all things.



LEO:  Oh, man.



STEVE:  So, yes, I have been around for a while.



LEO:  But Steve.  By now you surely don't have to note - I turn off mail notifications first thing I do.  You've got mail?  That's like a big deal?  Don't you get mail every three seconds?



STEVE:  No.  No, I only - I get no spam, just like Dvorak.



LEO:  You're so smart.  You've so smart.  If I had announcements every time I got mail, I'd never get anything done.



STEVE:  Actually, I just turned off Indiegogo email this morning because I got so tired of them just - they've just gone promotion crazy.  And I thought, okay, you know... 



LEO:  I have a folder just for that crap.



STEVE:  No more.  No more.



LEO:  Yeah.



STEVE:  Okay.  So anyway, definitely install the Patch Tuesday updates.  They're going to fix some things you're going to want fixed.  But beware, if you are also using third-party AV, that you may, I mean, the good news is, before Windows does this every month they do a, what is it, a checkpoint, a System Restore point.



LEO:  Right.



STEVE:  And so you can certainly back out and then go, whoops, I think I'll wait awhile until my AV and these most recent updates make peace with each other.  And in the meantime, in the interim, just be a little more careful than maybe you would otherwise be.



And not to be left behind, Adobe also released 40 patches last Tuesday.  It was a large security update for them, covering their Bridge CC, Adobe Experience Manager Forms, InDesign, Adobe XD, Dreamweaver - get this - Shockwave Player and Adobe Flash Player, and also Acrobat and Acrobat Reader.  The vulnerabilities fixed include some which can lead to arbitrary code execution, sensitive information disclosure, and remote code execution in the context of the current user.  When I read that Adobe's Shockwave Player was suffering from a total of seven serious security flaws, all critical memory corruption issues, exploitable for the purpose of executing arbitrary code, I thought, Shockwave?  You've got to be kidding me.



LEO:  Oh, it's around.



STEVE:  Oh, Leo.  We're no longer allowed to use Windows 95, Windows 98, NT, 2000, or XP.  But someone somewhere is still using Shockwave?  That just doesn't seem right.  So actually it turns out, when I dug in a little bit deeper, turns out I spoke too soon, since these were the last updates that Shockwave will ever receive.  So Adobe wrote:  "Effective April 9, 2019" - which was last Tuesday - "Adobe Shockwave will be discontinued, and the Shockwave Player for Windows will no longer be available for download."



LEO:  Oh, my god.  Hallelujah.



STEVE:  I know.  But come on.



LEO:  I can't believe you still could.



STEVE:  Last week?  Yeah, exactly.  They said:  "Companies with existing Enterprise licenses..."



LEO:  That's the problem, right there.



STEVE:  "...for Adobe Shockwave continue to receive support until the end of their current contract."  So no more contract renewals.  So enterprises, I mean, lord knows, I mean, if you're still using Shockwave, you probably are not within reach of this podcast, unfortunately.  But, you know, it just had seven remote code execution, arbitrary code execution vulnerabilities fixed last week, which suggests there are probably more because Shockwave, how old is it?  Come on.



So anyway, if somehow, I mean, it is way time to switch to HTML5.  Anything Shockwave could do, you can do now with JavaScript and HTML5.  So hire a programmer; you know?  Maybe Rasmus Vind, who did the great work for me with SQRL and the SQRL forums.  Maybe you can get him to fix your website or your corporate whatever it is if you need Shockwave because, boy, you really shouldn't.



Also Adobe's Flash Player had an out-of-bounds read and a use-after-free flaw fixed, either of which could result in data leaks or the execution of arbitrary code.  Acrobat and Reader also received a substantial update last Tuesday with a total of 21 security issues resolved, 10 leading to information disclosure and 11 other bugs that could be exploited to execute arbitrary code.  Which is to say, evil PDFs could be formed which, when viewed with Reader, would execute code on your system.  So you absolutely want to update your Acrobat and Reader.



And be careful about, I mean, mostly you want to be careful about what you click on in email that you receive or in sketchy websites that you visit.  I mean, the overall best advice is just exercise caution.  And it's difficult to always do because sometimes we get excited about something that we're being offered.  But really, be careful.



Google, you know, I continue to be impressed with - some things they do don't impress me.  Famously we know that their CRL set approach to dealing with certificate revocation is so badly broken that it's not even worth doing.  But they do, I mean, they're responsible for many of the improvements that we're seeing on the web.  And for that I thank them and salute them.  And of course we talked about the changes that they're making to Android in order to improve its security.



In this case Emily Stark with the Chrome branch of Google posted to the World Wide Web Consortium list an item titled "Blocking high-risk non-secure downloads."  She wrote:  "Hi, webappsec friends."  So web application security friends.  She said:  "Over in Chrome land, we've been considering how to drive down non-secure downloads, particularly high-risk ones like executables."  She wrote:  "I wanted to see if other browsers would be interested in joining us on this adventure.  We want to achieve the right balance between compatibility/user-disruption and security improvements."  Of course that's always the challenge is tighten things down, restrict things that have traditionally worked, thus making them not work when they're deemed to be unsafe, yet not cause too much disruption.



She said:  "So we will likely start by treating certain high-risk downloads initiated from secure contexts as active mixed content and block them.  We're still finalizing our metrics before we can share them publicly, but right now it's looking like it will be feasible to block a set of high-risk file types, meaning executables and archives, as determined by the Content-Type header or sniffed mime-type.  We will likely focus on protecting desktop users because Android and Safe Browsing already provide protection against malicious APKs."



She said:  "We're not planning to focus on non-secure downloads initiated from non-secure contexts at the moment, because users at least see the 'Not Secure' omnibox badge on those pages."  She says:  "Feedback welcome.  Thanks, Emily."  Then in a follow-up reply to someone else's query about which types Google was considering, she wrote:  "We're looking at EXEs, DMGs, and CRXes as executables; and zip, gzip, rar, tar, bzip, et cetera as archives."



In response to a query from ZDNet, a Mozilla spokesperson said: "We are interested in exploring these ideas further in conversation with Google and other interested parties.  The general idea aligns with the steps we have previously taken to protect users from insecurely delivered content."  Okay.  So what she said there is key.  She said initiated from secure contexts as active mixed content and block them.



So the idea is that, right now, browsers make a distinction between active and passive mixed content.  First of all, "mixed" means from content which is fetched without HTTPS security, so HTTP, from a page which was delivered over HTTPS.  So, for example, passive content are things like, for example, an image.  Browsers will typically still allow you to fetch an image over HTTP, although some of them complain in very subtle ways; but they don't, like, show it as a broken link and refuse to load it.  However, active stuff like, for example, JavaScript will not be tolerated.  You cannot load JavaScript from an HTTP URL which exists in an HTTPS content because that's active mixed context.



So what they're considering doing is that moving user-initiated - and that's the other distinction, clicking on something.  For example, you can certainly click on an HTTP URL on an HTTPS page.  Meaning a user action to switch contexts from HTTPS to HTTP is always permitted.  That you can do.  But what they're talking about now doing is, if that HTTP link were to download an EXE or a Mac DMG or any of a bunch of archives from an HTTP URL, Chrome is proposing saying, uh, no.  Now, maybe it's bypassable.  They would bring up an interstitial and say, "Hold on, you're asking to download something from a non-secure source.  Are you sure you want to proceed?"  Or maybe they're just going to decide to be more heavy-handed and decide there is no case for still allowing that to be done.  That seems a little extreme to me.



But it does look like Firefox is interested in following.  And what we're seeing overall is a continuous set of incremental moves, moving the entire web over to HTTPS.  Of course we have  the Let's Encrypt effort, which for the lowest quality of certificate has at least automated those so that you no longer have the excuse of the entry barrier of needing to pay money to be secured.  That we have.



So I think it'll be maybe an interstitial at first.  That might help people on sites who are pulling things from HTTP.  I mean, maybe just laziness.  Maybe, for example, that domain already supports security, and they just didn't put an "S" on their URLs because they didn't bother to.  And if they did, then it would no longer be a mixed content fetch.  So we'll see how this shakes out.  But again, I tip my hat to Google.  I think that they're moving us in the right direction, and it's all for the best.



So Leo, I said at the top of the show that Russia's Roskomnadzor...



LEO:  You just like saying it, that's all.



STEVE:  Which I do love saying, has finally lowered the boom on Facebook.  We covered this pending and growing issue previously.  As we noted at the time, last December, Russian Internet watchdog Roskomnadzor send notifications to both Twitter and Facebook, asking them to provide information about the location of servers that store the personal data of its citizens.  And we'll remember that Roskomnadzor, also known as the Federal Service for Supervision in the Sphere of Telecom, Information Technologies, and Mass Communications - certainly it's easier just to say Roskomnadzor.  It's the Russian telecommunications watchdog that runs a huge blacklist of websites banned in Russia.  And of course back in 2016 LinkedIn was banned.



So though the social media platforms Twitter and Facebook were given one month to reply back in December, they chose to stick to their guns and to not disclose this information.  As a result, Moscow's Tagansky District Court imposed a whopping 3,000 rubles fine...



LEO:  How much is that?



STEVE:  ...on Twitter last week and the same on Facebook today.  What is 3,000 rubles in U.S. dollars?



LEO:  That sounds like a lot, yeah.



STEVE:  Yes.  Brace yourself:  $47.



LEO:  Oh, yikes.



STEVE:  Yup.



LEO:  What are they going to do?



STEVE:  I don't know if Mark fills his tank or if he charges his battery, but less than a tankful of gas these days.  That fine, turns out, was the minimum that Russian courts could impose on companies for violating Article 19.1 of the Administrative Code of the Russian Federation:  Failure to provide information.  The maximum amount of the fine under this article is a whopping 5,000 rubles, or $78.  So they probably weren't too concerned either way.



LEO:  They didn't even throw the book at them.  That's what's funny.



STEVE:  Now, Twitter and Facebook are not off the hook, however, since Russia law does give them the ability, well, to completely ban non-complying social media companies as they banned LinkedIn back in 2016.  So we'll see what happens.  Of course Telegram, we covered that fiasco with all of the blocking and other problems, and AWS got blocked, causing all kinds of other sites to get blocked, and it was a mess.  So we'll see how this goes.  It will be interesting to see because, you know, we're again watching the evolution in so many different directions of what the Internet means to the world.



Kaspersky Lab named the massive APT framework suite TajMahal because the stolen data was transferred to the attacker's command-and-control server in an XML file named "TajMahal."  They described this as a state-of-the-art, high-tech, modular-based malware toolkit that not only supports a vast number of malicious plug-ins - and it's funny because, I mean, I looked.  If you click on that link on the show notes, Leo, it's SecureList.com at the beginning of the TajMahal article.  They enumerate the 80 modules that are contained in this thing.  And it is a little chilling to think - and the reason I wanted to share this with our listeners is just to get a sense for just the practical reality of the kind of instrumentation which exists now on the Internet. 



Okay.  So they describe this as a state-of-the-art, high-tech, modular-based malware toolkit that not only supports a vast number of malicious plug-ins for distinct espionage operations, but also comprises never-before-seen and obscure tricks.  So they're learning things, Kaspersky is, from what they see happening out in the world.  Evidence shows that the system has been in operation for at least five years, and they only discover it in the autumn of 2018, so late last year.  And so that also suggests that who knows how many other similar systems are in use now, and for how long they will be, for how long they have been or will be until they're discovered, because here's one that made it for five years.



Malware samples that they examined suggest that the cyberespionage group behind this attack has been active since at least August of 2014.  The system pinged Kaspersky's radar late last year when the attackers used it to spy on the computers of a diplomatic organization belonging to a Central Asian country whose nationality and location have not been disclosed.  So presumably they were under some sort of Kaspersky system protection, and Kaspersky's intruder detection system said whoops, what's this?



Kaspersky wrote:  "TajMahal is a previously unknown and technically sophisticated APT" - we know that's Advanced Persistent Threat - "framework discovered by Kaspersky Lab in the autumn of 2018.  This full-blown spying network consists of two packages named Tokyo and Yokohama.  It includes backdoors, loaders, orchestrators, C2 communicators, audio recorders, keyloggers, screen and webcam grabbers, documents and cryptography key stealers, and even its own file indexer for the victim's machine."  They wrote:  "We discovered up to 80 malicious modules stored in its encrypted Virtual File System."  So it installs its own file system, an encrypted virtual file system within the victim file system.  And they said: "This was one of the highest numbers of plug-ins we've ever seen for an APT toolset."



They said:  "Just to highlight its capabilities, TajMahal is able to steal data from a CD burned by a victim as well as from the printer queue.  It can also request to steal a particular file from a previously seen USB stick; next time the USB is connected to the computer, the file will be stolen.  TajMahal," they said, "has been developed and used for at least the past five years.  The first known legit sample timestamp is from August 2013, and the last one is from April 2018.  The first confirmed date when TajMahal samples were seen on a victim's machine is August of 2014."



And I have a picture in the show notes that shows, that sort of depicts this.  The first box:  Extensive modular APT framework.  Initial attacks and infection methods unknown.  Then the Stage 1 Tokyo package consists of three modules with backdoors, a PowerShell script, contacts the command-and-control server, and remains in the victim as backup.  That then in turn loads Stage 2, which is the Yokohama package consisting of up to 80 modules, installing an encrypted file system within the target system, and containing plug-ins, libraries, configuration files, and more.  It's able to hunt for documents, visual and audio files, website cookies, and Apple backup lists.  It can also take info from the printer queue, burned CDs, and previously installed USB sticks.  Stolen data is sent to the command-and-control in an XML file called "TajMahal," and thus the name of this thing.



So Kaspersky concluded their report saying that:  "The TajMahal framework is an intriguing discovery that's of great interest, not the least for its high level of technical sophistication, which is beyond any doubt."  They said:  "The huge amount of plug-ins that implement a number of features is something we have never seen before in any other APT activity.  For example, it has its own indexer, emergency command-and-controls, is capable of stealing specific files from external drives when they become available, and so on.  The question is, why go to all that trouble for just one victim?" they ask.



They say:  "A likely hypothesis is that there are other victims we haven't found yet.  This theory is reinforced by the fact that we couldn't see how one of the files in the VFS [Virtual File System] was used by the malware, opening the door to the possibility of additional versions of the malware that have yet to be detected."



So anyway, I wanted to just sort of update everybody on this is cyber warfare.  This is nation-state-scale advanced persistent penetration targeted attacks.  It installs itself in a system, sets up shop, and is incredibly capable.  I mean, it's like having an agent of the attacker sitting there in the machine, watching it do things.  It can be, once the remote attackers figure out what's going on, what this computer is doing, who it belongs to, what kind of things it's likely to see, they're able to essentially set up triggers which will be triggered when specific files and events occur causing it to spring to life, grab those things, and then send them back to the mothership.



So, I mean, again, at the beginning of this podcast back on the single-digit episodes, this would have seemed like sci-fi.  It's just like, oh, really?  Come on.  And now it's just like, oh, yeah, we're just updating everybody on what's happening out there.  Wow.



A little bit of miscellany.  I wanted to just mention for Firefox users a handy add-on that I started to use for the first time yesterday.  It's called Auto Tab Discard, for those of us who like to run with lots of tabs.  One of the things that I've noticed is that restarting Firefox dramatically drops the amount of memory it's using because, as you go to tabs, like bring up a web page, today's web pages are becoming huge.  In some cases I'll see that uBlock Origin will have blocked 50, five oh, things from some web page.  And even with it in place, this page is multiple megabytes of stuff that it has loaded.



The point is when you switch away from that tab, that page remains in memory.  And so as you are doing research, pursuing links, clicking things, kind of building up a list of stuff you want to get back to, which is why my tabs tend to accumulate, so does memory.



And so what I had been doing in the past was just closing Firefox and then reopening it, and Firefox would reload the tabs, but not their contents, not until I brought them to the foreground.  Well, Auto Tab Discard does that for me.  You can set it up, it's tunable, so it can do it for you continuously in the background.  What I do is, because it also has an appearance on the toolbar, I just have it set so that, when I click its toolbar, it'll flush all the memory used for all of the noncurrent tabs.  And I've just watched my memory consumption drop, for example, by 4GB, which is just shocking to me to say.  But that's the case.



So I just wanted to mention Auto Tab Discard is the name.  And if anybody else is a Firefox user and a heavy user of tabs where over the course of a session you visit many different pages, you may want to consider it.  And it can also be set up to, like, not do this until it begins to run, your system begins to run short of memory, or some number of tabs have been left open and so forth.  So all kinds of settings.



A listener of ours, Andy Weaver in Bath, U.K., with the subject of SpinRite.  He said:  "Really enjoy Security Now!.  The subject coverage is always interesting and just the right level of technical detail for my level of tech knowledge."  So I'm glad for that, Andy.  He said:  "Wondering about the long-awaited SpinRite update.  If I buy now, will I be entitled to the promised update features when released?"  And he says:  "Many thanks.  Live long and prosper."



And so I just wanted to mention, yes, absolutely.  Not only you, but somebody who bought SpinRite 6.0 15 years ago, in 2004, will also be entitled, for free, to what I do to it moving forward through the 6 series.  6.1 will, as I've been saying, I already know how much faster it will be because I was benchmarking an early version of it before I paused, well, okay, stopped working on it in order to do SQRL.  I have had people upset with me for taking this long.  But when they've seen what SQRL does and how it works, they've actually forgiven me.  So maybe other people who are impatient waiting for me to get back to SpinRite will also understand why I felt that I just had to get this thing, SQRL, out to the world before I got back to SpinRite.  And in return, everybody, I mean, even people who bought SpinRite a long time ago, will be getting a much enhanced product for free.



So yes, anybody who has any version of SpinRite 6 will get all of the updates at no charge.  And a little beyond that, another benefit for already owning it is, as I have said recently, I'm going to make the functional pre-release versions available to anybody who has SpinRite 6.  And I also think, I'm going to discuss this with my team, but we're already considering no longer offering pre-version 6 upgrades at that point because SpinRite 6 will be 15 years old, and I think that's long enough, especially when the 6.1 release and on are going to be so radically different from what SpinRite 6 had ever been before.  So I'm happy to make them all available to owners of SpinRite 6.  But I don't think we will continue to upgrade people from beyond 15 years ago.  I think that seems more than fair.



A bit of closing the loop with our listeners.  Roy in Israel says:  "Another reason to use a password manager or SQRL."  He says:  "Hi, Steve.  I've been listening to the show for a few months now.  I found this out when one of our support engineers opened a bug about the ability to retrieve the password in our service login page using a simple inspect-and-replace type method within Chrome."  He says:  "Of course there was nothing we can do about it, but I wanted to share this anyway so people understand how dangerous is the simple password manager which is embedded within our browser."  And he gives a link to MakeTechEasier.com/see-password-in-browser.  He says:  "Thanks for a great show.  Roy."



And that actually brought to mind something I did with Lorrie a couple months ago.  There was some site that she needed to know her password for, which Chrome knew.  And so with her watching, I drilled into just the standard Chrome options and clicked a few times and got to my Chrome's memorized password list and brought up the password and showed it.  And her eyes just, like, went wide open, the fact that all the passwords which she had had Chrome memorize for her were right there to anyone, free for the taking, anybody who was sitting in front of her computer.  So she said, "What?"  And I said, "Oh, yeah, it's convenient to have your browser do that for you.  But if you do that, anybody who can access your computer can have all the passwords of all of your accounts."



LEO:  They need your login, obviously.



STEVE:  Yeah.



LEO:  They have to log in.  They can't just...



STEVE:  Yeah, if they have access to your computer, then they have access to everything in your browser.  Okay.  So also, just a reminder to our listeners, Scott in Boston.  He says:  "Security Now! feedback:  uBlock Origin disables ping attribute by default."  Oh, and this is how I learned of it.  He said:  "Hi, Steve.  Per your story last week on the formalization of the ping attribute in the HTML spec, it looks like uBlock Origin already blocks sending that info."  He says:  "It's in the Settings tab under Privacy as 'Disable hyperlink auditing.'"  And he says on the tab it reads:  "Checking this will prevent hyperlink auditing.  Hyperlink auditing is best summarized as 'phone home' feature, or more accurately 'phone anywhere,' meant to inform one or more servers of which links you click on and when."



He says:  "The explanatory link goes to a page that defines hyperlink auditing as encompassing the ping attribute as well as a DOM method called 'navigator beacon.'"  So he says:  "I believe both of those are blocked by uBlock Origin when 'Disable hyperlink auditing' is checked."  And I think he said it was done by default.  I'll have to check mine to see if it's on by default.  But thank you, Scott, for the heads-up, and another nice benefit of uBlock Origin.



Richard in York, U.K.  He says:  "I've been listening to Security Now! for a few years and absolutely love it.  More than that, it's been super useful, as well.  But in 709" - meaning last week - "you have revolutionized my working on Windows."  He says:  "I am forever needing to type in/copy file paths for various things, and I can hardly believe that 'copy as path' has always been there, and I didn't know about it.  Thank you so much for letting the world know about that.  It seems like a very little thing, but it is so incredibly quick and useful."  And then he says, in asterisks, "Mind blown."



And Leo, since you missed it, I never knew that when you right-click on something in Windows to get the properties list of things you can do, if you shift right-click, the right-click menu gets additional things, one of which is "copy as path," which allows you to put the entire file system path onto the clipboard for subsequent pasting somewhere else.



LEO:  Very handy.  That's nice, yeah.



STEVE:  Yeah.  I explained to our listeners that I used to use, in XP, an add-on - "Send to clipboard as" it was called - and that I missed it because I hadn't yet added that to Windows 7.  And it turns out it was there all along.  Oh, and I also saw from some other listeners that there are additional things you get in other places when you shift right-click, like the "Send to" option under there also gets a whole bunch more things.  Actually, it almost like doubles its size.  You get just a gazillion folders and shortcuts that "Send to" can send things to.  So additional things to explore with shift right-click.  So thank you, everybody, for that.  Oh, and that was the next tip I had from an anonymous sender who mentioned that the "Send to" menu is also dramatically enhanced.



So WPA3 is still suffering from being closed.  And as I said at the top of the show, I just - it's so critical that something that is as mission critical as WiFi be secure.  And the idea that security researchers and academics are unable to examine the protocol while it's in development is unconscionable.  And these guys say as much, having just found a bunch of problems with it.  So WPA3 is, as it begins to roll out, the actual devices are becoming available.  And so in service the researchers are beginning to play with it, and faults are arising.



LEO:  So they don't get the code from the Wi-Fi Alliance.



STEVE:  No.



LEO:  They've got to get a device.



STEVE:  Yup.  They got no help.  Yup.



LEO:  Reverse engineer it, hack it, and then tell them, "Guys, you did it wrong again."



STEVE:  Yup.  So Mathy Vanhoef, a researcher who was at the University of Leuven, KU Leuven, two years ago, where he discovered and revealed a severe flaw in WiFi Protected Access II, which is what we're all still using today, he named that attack KRACK, K-R-A-C-K, for the Key Reinstallation Attack.  So today he's now at the New York University Abu Dhabi and working with another researcher at Tel Aviv University and also KU Leuven.  Their new research paper is titled "Dragonblood:  A Security Analysis of WPA3's SAE Handshake."  SAE stands for Simultaneous Authentication of Equals.



Anyway, we've mentioned here in our preliminary discussion of WPA3, that's all we've been able to have because I can't get my hands on a spec in order to comment on it or even describe it to our listeners.  Eventually it'll leak out, despite their best efforts, because this is the Internet, and somebody will republish the PDF of the spec.  So, you know, someday.



Anyway, so WPA3 Personal is the protocol which replaces WPA2's Pre-Shared Key, the PSK, with that protocol, the Simultaneous Authentication of Equals.  It's intended to provide more robust password-based authentication.  So that would be the protocol we would use in our homes, for example, where we would come up with a password for our WiFi and then give it to all of our devices.  But instead of using the Pre-Shared Key protocol that we have today in WPA2, we'd be using this SAE, the Simultaneous Authentication of Equals protocol.  But no users would even know that.  They would still be having, like, oh, what's my WiFi password?



So our interface to it would look the same.  That protocol, SAE, is known as Dragonfly; and it appears to contain, as these guys have found, a number - and this is what they said - of fundamental design flaws which expose users to password partitioning attacks.  In their abstract - it was about a 16-page technical paper.  And since I have no access to the spec, I have excerpted sort of the key pieces of this to give our listeners a sense for what they have done and what they found.



So in their abstract they said the WPA3 certification - and that word is important because it turns out it's a certification.  "The WPA3 certification aims to secure WiFi networks and provide several advantages over its predecessor WPA2."  And they do agree with that.  They agree this is better than what we had before.  Unfortunately, it could have been better still, had anybody - well, anyway, I'll stop beating that horse.  "Such as protection against offline dictionary attacks and forward secrecy."  Yay for that.



"Unfortunately," they write, "we show that WPA3 is affected by several design flaws, and analyze these flaws both theoretically and practically.  Most prominently, we show that WPA3's Simultaneous Authentication of Equals handshake, commonly known as Dragonfly, is affected by password partitioning attacks.  These attacks resemble dictionary attacks and allow an adversary to recover the password by abusing timing or cache-based side-channel leaks.  Our side-channel attacks target the protocol's password encoding method.  For instance, our cache-based attack exploits SAE's hash-to-curve algorithm.



"The resulting attacks are efficient and low cost.  Brute-forcing all eight-character lowercase passwords" - okay, listen to that.  "Brute-forcing all eight-character lowercase passwords requires less than $125 in Amazon EC2 instances.  In light of ongoing standardization efforts on hash-to-curve, Password-Authenticated Key Exchanges" - so-called PAKEs, that's Password Authenticated Key Exchanges - "and Dragonfly as a TLS handshake, our findings are also of more general interest."  That is to say, just beyond its particular use in WPA3.  They said:  "Finally, we discuss how to mitigate our attacks in backwards-compatible manner and explain how minor changes to the protocol could have prevented most of our attacks."



In their introduction they said:  "The Wi-Fi Alliance recently announced WPA3 as the more secure successor to WPA2.  Unfortunately" - this is them writing - "it was created without public review, meaning experts could not critique any of WPA3's new features before they were released.  Moreover, although the new handshake of WPA3 was designed in an open manner, its security guarantees are unclear.  On one hand there is a security proof of a close variant of WPA3's handshake; but, on the other hand, another close variant of the handshake received significant criticism during its standardization.  These issues raise the question whether WPA3 is secure in practice.  We remark that WPA3 does not define new protocols, but instead mandates which existing protocols a device must support.  This means WPA3 is not a specification, but a certification.



"Put differently, devices can now become WPA3-certified, which assures they implement certain protocols in an interoperable manner.  The only novelty of the WPA3 certification is a transition mode where WPA2 and WPA3 are simultaneously supported for backward compatibility with WPA2.  Although WPA3 follows recommended practice by existing standards, we believe more openness to alternate protocols could have increased its security.



"In this paper we perform a security analysis of WPA3's Simultaneous Authentication of Equals handshake.  This handshake was designed to prevent dictionary attacks and constitutes the biggest improvement over WPA2.  We systematically analyzed its security by reading specifications, inspecting formal proofs, and auditing open-source implementations.  This analysis revealed several design and implementation flaws.  For instance, when verifying the assumptions made by the formal proof of the SAE handshake, we discovered both timing and cache-based side-channel vulnerabilities in its password encoding method.  We empirically confirmed all our findings against both open source and recently released proprietary implementations of WPA3."  Exactly as you said, Leo.  Once they had some hardware in their hands, they verified that these problems survived certification.



They said:  "All combined, our work resulted in the following contributions," and there are six.  They said:  "We provide a self-contained and high-level description of WPA3 and its SAE handshake."  So basically they have provided what unfortunately the Wi-Fi Alliance has refused to provide, as a consequence of lots of research and reverse engineering and comparing in the field implementations some early open source implementations of the spec and so forth.



They said:  "We show" - second contribution - "that the anti-clogging mechanisms of SAE is unable to prevent denial-of-service attacks.  In particular, by abusing the overhead of SAE's defense against already known side channels, a resource-constrained device can overload the CPU of a professional Access Point."  So something like an infected IoT device could perform a denial of service that would bring down a strong CPU of a high-performance access point running WPA3.



Third contribution:  "We present a dictionary attack against WPA3 when it is operating in transition mode.  This is accomplished by trying to downgrade clients to WPA2.  Although WPA2's four-way handshake detects the downgrade and aborts, the frames sent during the partial four-way handshake provide enough information for a dictionary attack."  In other words, this transition mode which is new to WPA3 was not properly designed.  They said:  "We also present a downgrade attack against SAE, and discuss implementation-specific downgrade attacks when a client improperly auto-connects to a previously used WPA3-only network."



Fourth:  "We empirically investigate the feasibility of timing attacks against WPA3's SAE handshake.  This confirms timing attacks are possible and leak info about the password."  Five:  "We present a novel microarchitectural cache-based side-channel attack against the SAE handshake.  This attack leaks information about the password being used.  Our attack even works against hash-to-curve algorithm implementations that include countermeasures against side-channel attacks.  This type of attack against hash-to-curve algorithms is of independent interest due to current standardization efforts surrounding hash-to-curve methods."



And, finally, sixth:  "We show both theoretically and empirically how the recovered timing and cache info can be used to perform an offline password partitioning attack.  This enables an adversary to recover the password used by the victim."



So I had here also in the show notes the conclusions and recommendations, but most of it is a rehash in other words of what I already said.  So they basically tore WPA3 apart and implemented a practical offline attack on WPA3's personal equivalent, that is, the SAE, the Simultaneous Authentication of Equals protocol, which will be used by all of us in our homes once WPA3 happens.  They worked responsibly with the Wi-Fi Alliance, who then of course issued the Security Update April 2019 in a CYA fashion.



The Wi-Fi Alliance wrote:  "As with any technology, the robust security research necessary to remain ahead of emerging threats will occasionally uncover new vulnerabilities.  Security researchers identified vulnerabilities in a limited number of early implementations of WPA3 Personal and immediately brought their discovery to the WiFi industry."  Oh, my goodness.  Yeah.  Because, of course, the WiFi industry, that is, these guys, didn't make any of this available to these researchers earlier.  They had to wait for it to be available.



"There is no evidence," writes the Wi-Fi Alliance, "of the vulnerability being used against WiFi users maliciously" - yeah, because no one has this yet, fortunately - "and Wi-Fi Alliance has taken immediate steps to ensure users can count on WPA3 Personal to deliver even stronger security protections."  They have two bullet points.  "WiFi CERTIFIED" - in all caps, which is they got a trademark on it - "WPA3 Personal now includes additional testing within our global certification lab network to encourage greater adoption of recommended practices" - yeah, meaning that translation is they've gone "Whoops" and figured out that they need to fix the existing devices against these things that these guys found - "and Wi-Fi Alliance is broadly communicating details on these vulnerabilities and implementation guidance to device vendors as the industry begins to bring WPA3 Personal to market."



They write:  "These issues can be resolved through a straightforward software update, a process much like the software updates WiFi users regularly perform on their mobile devices.  WPA3 Personal is in the early stages of deployment, and the small number of device manufacturers that are affected have already started to deploy patches to resolve this issue."  In other words, we didn't offer it to researchers early enough, so devices are already in the field which are broken and now need to be patched.  So whoops, we're doing that.  We're making patches available and hoping that people will patch the few devices which are already out there because, as is, they're broken and can have their passwords hacked using this technology, which is now public.



They said:  "Users can refer to their device vendors' websites for more information.  As always, WiFi users should ensure they have installed the latest recommended updates from device manufacturers.  Security is and always will be a dynamic endeavor, and Wi-Fi Alliance will continue to maintain strong security protections for WiFi users through its Wi-Fi CERTIFIED trademark program."  Thank you very much.  In other words, WPA3 will be made secure despite our efforts to keep it hidden as long as possible.  After it becomes deployed, it's no longer possible to hide it.  So researchers will then be able to have access to what it actually turned out to be, show us where we made mistakes, and hope then that deployed WPA3 instances will someday be updated to close those problems that we created.  Thank you very much.  And that, Leo...



LEO:  Bravo.



STEVE:  That, Leo, is our podcast for the week.



LEO:  And once again, all is right with the world.



STEVE:  On that note.  I did forget to mention, I should have said, at the end of that "conclusions and recommended" chunk that I removed, they mention that several simple changes could have been made - okay.  I should say:  "In light of our presented attacks," they wrote, "we believe that WPA3 does not meet the standards of a modern security protocol."



LEO:  Oh, boy.



STEVE:  Whoops.  "Moreover, we believe that our attacks could have been avoided if the Wi-Fi Alliance created the WPA3 specification in a more open manner.  Notable is also that nearly all of our attacks are against SAE protocol encoding method, in other words, against its hash-to-group and hash-to-curve algorithm.



"Interestingly, a simple change to this algorithm would have prevented most of our attacks.  In particular, the peer's MAC address can be excluded from the SAE password encoding algorithm, and instead included later on in the handshake itself.  This allows the password element to be computed offline, meaning an adversary can no longer actively trigger executions of the password encoding method."  Essentially what they're saying is it dramatically reduces the ability to probe an existing system for clues which they were able to use.



And they said:  "Moreover, this would mean that, for a given password, the execution time of the password encoding method would always be identical, limiting the amount of information being leaked.  Surprisingly, when the CFRG was reviewing a minor variant of Dragonfly, they actively discussed these types of modifications.  However, to our surprise, this change was not incorporated into any of the Dragonfly variants.  We also conjecture that resource-constrained devices may not implement all the side-channel countermeasures, as these may be too costly on lightweight processors.  Additionally, correctly implementing our suggested backwards-compatible side-channel countermeasures is non-trivial."



Meaning in some sense the cat is out of the bag; and, if anybody was implementing WPA3 in resource-constrained implementations, it may not be practical, as the Wi-Fi Alliance said, to fix this through a software update.



LEO:  Wow.



STEVE:  Yes.  They said:  "This is worrisome because security protocols are normally designed to reduce the chance of implementation vulnerabilities.  Finally," they said, "we believe that a more open process would have prevented, or at least clarified, the possibility of downgrade attacks against WPA3 transition mode.  Nevertheless, although WPA3 has flaws, we still consider it an improvement over WPA2."



So eventually we'll get there.  And again, as I said before all this happened, this idea of developing a protocol this important to the Internet in the dark, behind closed doors, it's just wrong.  And here we're seeing a perfect example of that happening as soon as WPA3 began to creep out into the world.



LEO:  Do you know, when they say "resource constrained," if they mean CPU or RAM or what?



STEVE:  Yeah.  They must mean a processing attack.



LEO:  Processing power.



STEVE:  Yeah, processing power.



LEO:  Yeah, because a lot of routers are just, you know, very low...



STEVE:  Oh, my god, they barely get off the ground, yes.  



LEO:  Yeah, yeah.  On the other hand...



STEVE:  Well, and light bulbs.



LEO:  Yeah, light bulbs.



STEVE:  Yeah, not even routers.



LEO:  That's true, the WPA3 client needs to be updated, too; right?



STEVE:  Exactly.  



LEO:  Oh, that's a problem.



STEVE:  Yeah.



LEO:  I don't expect a lot of processor in my light bulb.



STEVE:  No.  I mean, the good news is they caught this fast.  And so let's hope that this gets fixed.  And eventually...



LEO:  Yeah.  I don't know how many, you know, WPA3 devices there even are so far; right?



STEVE:  Yeah.  I'm aware of none so far.



LEO:  Yeah.  So maybe they can fix it.



STEVE:  Whew.



LEO:  But this is why this process was so broken.  You called it.  You said this would happen.



STEVE:  Yup.



LEO:  You can't develop in secret.  You can't, especially an encryption protocol, you just can't do it in secret.  That just doesn't work.  They should have known from WEP, let alone WPA3.



STEVE:  From WEP.  From WPA1.  From WPA, I mean, every single one of these...



LEO:  Every single one.



STEVE:  ...this has happened.  And it has been a disaster.



LEO:  Just nuts.  Thank you, Wi-Fi Alliance.  Well, Steve, once again we've come to the end of a fabulous episode of "Game of Thrones."  We will be back next week with more Dragonblood, more fire, and another very uncomfortable seat of swords.



Steve is the man in charge here at GRC.com.  That's where you'll find SpinRite, the world's best hard drive recovery and maintenance utility.  Also find the show.  He's got audio versions of the show and transcripts, very good transcripts by Elaine Farris.  So that's a good place to go.  That's all free.  Everything's free except SpinRite.  So buy SpinRite and then use everything else.  That's kind of how it works.  You can also leave him questions.  He's on the Twitter at @SGgrc.  And you could go to GRC.com/feedback.  There's a form there.  There are also great forums if you want to participate in the creation of SQRL.  We're getting close.



STEVE:  Yup.



LEO:  GRC.com.  We are at TWiT.tv, and you'll find audio and video of the show at TWiT.tv/sn, or you could subscribe in your favorite podcast application.  You'll get a copy of everything the minute it's available.  We will do the next episode as we always do, on Tuesday, 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  Thanks so much to Jason Howell for filling in last week, and I'll be back for the foreseeable future.



STEVE:  Yay.



LEO:  Yay.  Thank you, Steve.



STEVE:  Okay, my friend.  Talk to you next week for Episode 711.  Bye.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#711

DATE:		April 23, 2019

TITLE:		DNSpionage

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-711.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we discuss Google's use of their Sensorvault tracking to assist law enforcement.  It's time to update Drupal again.  And, speaking of "again," Facebook.  We also look at Russia's newly approved legislation moving toward an Internet "off switch," a reminder that "USB Killers" are a real thing, the news of Marcus Hutchins's plea deal, an actively exploited Windows zero-day, a bunch of Microsoft Edge news, the Win7 end-of-life notices, something from the "I did say this was bound to happen" department, and some miscellaneous news.  Then we examine the latest detailed threat research from Cisco's Talos Group about the leveraging of DNSpionage.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Lots to talk about.  Google's Sensorvault:  how the feds and law enforcement are using it to track us.  Also the final fate of Marcus Hutchins, and an inexpensive little device that can kill any computer.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 711, recorded Tuesday, April 23rd, 2019:  DNSpionage.



It's time for Security Now!, the show where we protect you and your loved ones and your privacy, and we teach you about computers.  And here's the guy who does it all, Mr. Steve Gibson of GRC.



STEVE GIBSON:  And I'm giving the Vulcan salute with my right hand today, which feels a little awkward, but that's because the microphone is blocking my left hand access.



LEO:  What's interesting about you is you are ambidextrous.  I can only do the Vulcan salute with my right hand.  And oddly, I'm a lefty, but I cannot get that finger to move over on my left.



STEVE:  In other news...



LEO:  That's okay, change the subject.  Go ahead.  I know you never really cared about my Vulcan salute.  I know, Steve.



STEVE:  For our listeners who don't know what we're talking about, there was a cherry picker operating outside of the window that provides the left illumination for me during the podcast all morning.  And so I moved the microphone over onto the left side so it's pointing to the right because this microphone is very directional, and in the past we've had noises outside, and it does a good job.  So anyway, that's the back story there.



I wish I could take credit, Leo, for the title of this podcast, DNSpionage.



LEO:  Oh, I like it.



STEVE:  Because it's a beautiful name.  But it's credited, or at least the first time I have seen it was in a Cisco Talos security research report which is the topic of the podcast.  For a number, well, I was going to say weeks, but maybe it's months now, I've been sort of following this news.  It never quite seemed like it rose to the level of getting into a lot of detail.  But Talos just published a detailed research blog posting on the nature of and details of the nation-state level DNS Espionage, or DNSpionage, that they've been tracking all year.  And it's really interesting, both from the fact that it's actually happening, that somebody is going to these extents.



This is all a consequence of the fact that the web has largely moved to HTTPS.  It used to be, when we didn't have security up all the time, you could just bounce somebody, do a man-in-the-middle attack and intercept communications, and that was all you needed.  Now you have to have certificates.  And we briefly touched on this a while ago because I remember talking about how - and Cisco's Talos Group mentions this, how the automated certificate services, Let's Encrypt has one, apparently Comodo of course has one now - they're being abused because as soon as you're able to somehow poison DNS, you are then able to prove ownership of the domain that you don't really own, which is the only thing that the automated DNS server generators require.  Which unfortunately then allows a full TLS interception that raises no alerts on the user's browser.



Anyway, we'll get to all that.  Lots of other stuff to talk about for this Episode 711 this week.  We have Google's use of their Sensorvault is the name they have for this thing, which is their vault of all tracking information for all of their properties of all of their users on Android and iOS.  So it must be a big vault.  And they use it to assist law enforcement.  And that's a little creepy inasmuch as, you know, we're always being tracked all the time.  We know that now.  We know that disabling location tracking services for Google, like on Google Maps or whatever, doesn't actually do that, doesn't do what the user expects.  You have to go to much greater lengths in order to actually turn that off.  We've covered that in the past.  But anyway, so we'll talk about that.



We've got an important update to Drupal again.  And speaking of "again," Facebook.  Unbelievable, Leo, what we learned about what they were actually doing with the fact that they were asking people for their email account logins.  I mean, that was bad enough.  It turns out what they were doing with them is just - it's just unbelievable.  And I say in the show notes, and I can't wait to say it, Zuckerberg is saying they're going to rebrand Facebook as a privacy service.  Well, okay.  Just scrap the whole thing.



LEO:  Yeah.



STEVE:  I mean, it's like...



LEO:  It's based on the opposite.



STEVE:  Start over.  Yes.  Set up another facility somewhere.  Tell people, okay, people, forget everything you ever knew about Facebook.  We're now privacy.  Start coding.  Because, I mean, well, anyway.  They discovered logs that said, oh, it looks like we - it's just unbelievable.  Anyway, we also take a look at Russia's newly approved legislation moving forward towards their Internet off switch.  I wanted to remind our listeners that USB Killers are a real thing.  You can buy them on eBay and even from Amazon.  And a student of a college in New York recently pleaded guilty when the prosecution got a video of him using them.  We'll talk about that.



Also we have Marcus Hutchins, who has pleaded guilty to two out of 10 counts, and we have his posting and a follow-up tweet.  We want to touch on that briefly.  An actively exploited Windows zero-day.  A bunch of Microsoft Edge browser news.  The appearance of the Windows 7 end-of-life notices.  Something from the "I did say this was bound to happen" department.  We have some miscellaneous news, and then we're going to get to examining this Cisco Talos Group research that they titled "DNSpionage."  So lots of stuff for today.



LEO:  Love that name.



STEVE:  And, oh, boy, a real one-of-a-kind Picture of the Week for the podcast.



LEO:  But fitting along with many previous Pictures of the Week.  It's a good one.



STEVE:  Ah, yes.



LEO:  You want me to show the Picture of the Week now?



STEVE:  Sure.



LEO:  All right.



STEVE:  But everybody cover your eyes because we don't want you to see the new passwords that these people...



LEO:  Yes.  Secrets are revealed in here.



STEVE:  ...have chosen for themselves.  Now, this is probably - this actually ups the ante, I think, on the My Internet Passwords Book which you can purchase and leave lying around your desk.



LEO:  But at least that only you have access to, in theory.



STEVE:  No, well, maybe.



LEO:  If you're at home.



STEVE:  So here, posted on the wall apparently, with the large headline "Password Change Sign Up Sheet," and it explains, if that wasn't clear:  "If you'd like to change your password, please fill out the form below, and we will change your password on the system you indicate."  So we have the first column is full name; and Kyle Smith, Liz Jones, Jack H., Big Ed, and Sam Adams, I think it looks like, have so far requested that they have some of their passwords changed.  And then the second column is which system - Yardi, whatever that is, email, et cetera.  And we see email, phone, Facebook, and Pike Pass, whatever that is.  And then the third column is their current password.  The fourth column is their new password.



So, for example, Kyle, who was the first to step up and get crazy with changing his password here for his email, explained that his current password is apparently Scooter49$, and his new password is Skeeter4UZ, and then he says in parentheses "(all uppercase)," even though that's not how he filled out the form.  Liz would like her phone password changed from 89621 to 4281.  And Jack is really - Jack H., he's walking on the wild side.  He's asked to have his email password changed from, yes, Password, to, yes, Password2.  We can't really see what Big Ed is asking his Facebook to be changed from, it's like redst@pler, to mmmkay or something.



LEO:  Mmmkay.



STEVE:  These are not really clear.  Mmmkay, right.



LEO:  It's from "South Park."



STEVE:  And Sam Adams and so forth.  Anyway, everybody gets the idea.  So I don't know what company these people work for, but you should just run away very quickly, as quickly as you can.  It's sort of, I mean, maybe this whole thing is a joke.  There is a yellow post-it note sort of stuck on the bottom saying "Come see me - Shawn."



LEO:  This to me is the funniest part of all.  If you put your name on this list, maybe you ought to come see me.



STEVE:  Yeah, anyway.  So anyway, just another, you know, security is hard, and we're working every day to make it easier for you.  So if you'd like your password to be changed, and you don't seem to be able to do it yourself, that's okay.  We'll take care of that.  Boy.



Google is officially using this tracking technology to help catch bad guys.  We know that Google tracks us everywhere, even when we have Google Location History - that's the actual name - turned off.  Last August on this podcast we talked about the fact that many of Google's apps, when running on either Android or iOS, continually monitor their users' location.  Apps such as Maps or the Weather Update service on Android continuously monitor the precise latitude and longitude of the device they're installed on.  But back then we talked about how the movements of a Princeton professor were continuously tracked, even while Location History was disabled.  And our listeners may remember that in the research that was shown to demonstrate this, they deliberately removed from that where a lot of the points where because it was this professor's home.  And for the sake of his own privacy it was removed.  But before that was done, it was there.



In response to the Associated Press investigation of this back then, Google responded:  "There are a number of different ways that Google may use location to improve people's experience, including Location History, Web, and App Activity, and through device-level Location Services."  They said:  "We provide clear descriptions of these tools, and robust controls so people can turn them on or off and delete their histories at any time."



But at the time, Jonathan Mayer at Princeton said that it was his feeling that, if there was something called Location History that was there and obvious, that turning it off ought to be all that was required, not that users be expected to dig into the settings of every different Google application which is doing this in order to figure out how to turn it off on that app.  And so it's actually quite involved to do so, as we talked about at the time.



Google explains that it uses location tracking features to improve its users' experience, like personalized maps, recommendations based on places you visited, help finding your phone, real-time traffic updates about your commute, and delivering more useful ads, so like ads relevant to where you are.  In fact, since then we've talked about how it's a little unnerving, if you're in the emergency room in your local treatment center, that you're getting ads from so-called "ambulance chasers," suggesting that you call them if you need some representation about the accident you've just suffered.  So, I mean, this happens to people.



So an interesting feature of this which has recently been receiving more attention, I ran across a number of references to it when looking for things to update our listeners about in the past week, is that Google may also share its users' location data with federal authorities who are conducting criminal investigations when Google is asked to do so with a warrant, so with a search warrant.  And for what it's worth, the system works the way we would have designed it - you and I, Leo - if someone had asked us.  So law enforcement first needs to obtain a so-called "geofence" warrant.  And the news for me is that there is such a thing.  You can get a geofencing warrant now.



LEO:  I know, I was very disappointed to hear that this was even legal.



STEVE:  I know.  Authorities then reach out to Google armed with that warrant.  Oh, and I'm sure you know, Leo, also that it has mistakenly jailed people.  So it's not like it's foolproof.  Again, it comes down to people.  Authorities reach out to Google with this geofencing warrant for the purpose of learning about every smartphone that was in this geofenced radius of a crime, around or proximate to the crime.



After receiving the warrant, Google queries this massive sensor vault database to gather its first pass data, which is all possible phones located within that region in some time window, and forwards that to investigators.  So basically the big, the wide net.  And for this first pass, each device is anonymously identified with an ID, nothing about the device itself.  Investigators review the data, look for patterns of the devices near the crime scene, and then make a second request for additional data about specific devices by their anonymous ID that appear to be relevant, whatever that means.  Again, this is all kind of gray area and soft.



So what Google then replies to this more limited set of specific devices with device movement out of the geofence, that is, within the region broadly, which presumably then allows investigators to further determine and narrow their search down to a few devices, which they then claim to have strong reason to believe may be useful for providing information crucial to the case as either suspects or witnesses to what went down.  At which point Google then reveals the owner's name, email address, and other data associated with the devices.  And as I mentioned, the system is not perfect.  It has resulted in false arrests.



But anyway, I wanted to bring this to our users' attention.  And, Leo, what thoughts do you have?  You apparently are chagrined, as I am, about it.



LEO:  I don't really blame Google.  I mean, if they're getting lawful warrants, they have to provide that information.



STEVE:  Right, right.



LEO:  I wish they wouldn't make it too easy.  But I don't feel like it's - I worry that it's not constitutional, that it's what we used to call a "fishing expedition."  If you say, well, we don't know who committed the crime, what we'd like to do is just know everybody who was in the area at that time...



STEVE:  Yeah, good point.



LEO:  That's the very opposite of a specific warrant, which in the past I thought had always been required.  I'm no lawyer, so I'd love to hear the legal opinion on this.  But in the past I always thought you had to have a specific, not only a specific person in mind, but a specific thing you're looking for on that person, instead of just saying, well, we just want to know everything that happened around there.



Now, some people have likened it to video cameras.  So if you have a video camera in a 7-Eleven, of course the police could look at the video camera and see everybody who was in the 7-Eleven prior to the robbery or whatever.  So some say it's kind of like that.  But I think it's - I honestly feel like your location should be more private than that.



STEVE:  Yeah.  And you know, I think maybe the line it crosses, and the video camera is a perfect example, is the concept of an expectation of privacy.  We know that there is a formal concept of an expectation of privacy.  And maybe it's time for us to lose that if we have a smartphone in our pants.  But wow, I mean, that...



LEO:  And by virtue of having a smartphone doesn't mean you're in public.



STEVE:  Right.



LEO:  Right?  You could not be in public.  You could be in the house across the street.  But the smartphone still pinpoints you.



STEVE:  Yes.  And certainly you're able to look up and wave to the camera at the 7-Eleven store.  I mean, so it's there.  It's typically exposed.  I mean, and it also serves as a deterrent.  The 7-Eleven owners certainly know that you seeing this camera, I mean, the reason we know these things serve as deterrents is many of them are fakes.  All they have is an LED lighting up a little red light, and people are like, ooh, I'd better not do anything bad here.  So they are proven deterrents.  So that suggests that the presence of cameras removes any expectation of privacy.



And so, again, I also am not an attorney.  But you just don't think that having a phone in your pocket automatically means that law enforcement could retrospectively, at any time in the future, determine that you were walking through a certain area or driving past something or who knows what.  I mean, so it has been successfully used to solve crimes where there were no witnesses.  As I mentioned, there was an innocent guy who was jailed for a week because he was the only person, based on this data, that law enforcement believed could have done it, even though he was innocent.  And it wasn't until they actually found the bad guy a week later that they said, oh, our bad.  Sorry about that.  And again, those things happen even without this technology.



But anyway, I wanted our listeners - I thought this was interesting and important for our podcast listeners to know is going on, that there is something called, formally...



LEO:  Sensorvault.



STEVE:  ...Sensorvault.



LEO:  Geez, Louise.



STEVE:  Formally called Sensorvault at Google.  And under subpoena, or under search warrant, rather, they will provide successive iterative levels of detail in order to aid investigations.  So, I mean, you know, really it's inconvenient to turn your phone off, but that's what people have to do now if you want to move around without being tracked.



LEO:  Yeah.  And, you know, if you're smart, and you're going to commit a crime, and you turn off your phone, then this technique isn't going to work at all.  It's only going to bring in innocent people.



STEVE:  Right.  Or witnesses.  I was thinking the same thing about...



LEO:  No, maybe witnesses, yeah.



STEVE:  ...about turning the phone off.  And so it would allow the police to go find - but think about that.  Police knock on your door and say, "Hi there.  On such and such and such and such, you were in the area of.  And we don't think you committed the crime, but we want to know what you saw."  It would be a little unnerving to be preemptively asked by law enforcement.  And, you know, bad things happen.  So anyway, it's a little worrisome.



Speaking of a little worrisome, this is only moderately critical.  It is time to update Drupal again.  Later, toward the end of the podcast, there's a reference to Drupalgeddon, which was the serious vulnerability which the Drupal content management system suffered that caused lots of upset.  A year ago we had, on April 18th, a moderately critical cross-site scripting problem.  That was on April 18th.  On April 25th a year ago was a highly critical remote code execution flaw which affected a third-party - that was caused by third-party libraries that Drupal was hosting.  A year ago on August 1st another problem with third-party libraries.  On October 17th, multiple vulnerabilities.



This year on January 16th a critical problem that affected Drupal due to third-party libraries.  Also on the 16th, arbitrary PHP code execution due to a problem with PHP in this case, which Drupal was using.  That was a critical vulnerability.  A month later, on February 20th, a highly critical - that's their term - remote code execution that led to arbitrary PHP code execution.  A month later, on March 20th, a moderately critical cross-site scripting vulnerability.  A little bit less than a month later, April of this year, a moderately critical cross-site scripting problem that was in jQuery v3.4.0.  There were also multiple vulnerabilities from a PHP templating engine on the 17th of this month.



And so that's 10 significant vulnerabilities in 12 months.  We can no longer really count this as extreme by today's measures.  After all, Microsoft patched, what was it, 74 vulnerabilities in one month?  Two of them were zero-days.  And in each of the past three months they've patched two zero-days.  But on the other hand, those vulnerabilities do cover quite a lot of code real estate for Microsoft.  And in Drupal's defense, a lot of this is coming from third-party libraries that they are importing and using in their system.



But it does make it very clear that, today, creating secure systems, offering secure systems means there really needs to be a way for keeping code up to date.  And it's becoming crucial for widely deployed software.  We've been talking about WinRAR every podcast for the last month.  And the reason it's become such a huge target is that there is no mechanism for notifying the half a billion people who have downloaded it.  Or was it half a million?  Half a million, sorry, 500,000.



LEO:  No, it was half a billion.



STEVE:  Was it half a billion?



LEO:  Yes.



STEVE:  Oh, 500 million.  Right, right, right.



LEO:  Remember there's a lot of Windows users out there.



STEVE:  Yeah, and a lot of time has gone by while people have been doing this.  Anyway, so the fact that it's going to stay available for some period of time makes it a big target.  So, for example, I have prototyped, and I'll be talking about it a little bit, my own real-time updating system.  I first implemented it in the DNSBench benchmark, that allowed it to check for whether new versions were available.  And of course in the SQRL client, since it's very likely that someone's going to find something I've messed up, I've had an auto-update facility built in from the beginning which actually is causing us a little trouble at the moment, but we'll get past that.



So it really feels to me as though what we are coming to is a clear need for anything which certainly is an attack surface to have some means of notifying users, at least notifying its users if there's something they need to do.  Unfortunately, over a long span of time, email is probably inadequate.  There are certainly people who downloaded WinRAR 10 years ago.  You can use it for free.  Licensing it is optional.  So I was glad to get email.  I did license it from them and own it because that's a tool that I enjoy using.  But not everybody does that.  I mean, the reason there's half a billion of them is that it's free.



People sometimes say, "How many copies of the DNS Benchmark have been downloaded?"  And I say, "I don't know, like Never10, four million or something."  The point is that these things are downloaded at that volume because they're freeware.  They're not nagware.  They're not shareware.  They're just free.  It's here you go.  And if I were to attempt to charge something for it, it would be an annoyance more than anything else, for everybody.  So it just makes sense for them to be free.  But the problem is, in the case of WinRAR it has, as we know, completely out of the blue, created a serious vulnerability which is hurting people.  So it just has to be, moving forward.  And we've talked about this even in IoT devices, which have to provide some means, somehow, of keeping themselves up to date.  And certainly routers.  We've also been talking about that.  Routers have to have a means now for fixing themselves.



And Leo, I can't believe, I mean, two weeks ago, when you were on the beach somewhere, Jason and I talked about Facebook and the unbelievable fact that for apparently - I've done a little more digging - since May of 2016 they were in some instances asking people for their email password.  It's like, I couldn't believe it when this news surfaced a couple weeks ago.  Their password.  So as we know, if a reputable, security-oriented, privacy-minded entity wants to confirm that you are in control of an email address, they send a message to that email account with a link.



LEO:  Everybody does this.  This is normal.



STEVE:  How hard is that?  Yeah, like what is the - well, Leo.  There's another shoe that has dropped.  Believe it or not, they were actually - Facebook, Facebook was logging into those email accounts, not only to confirm that they could, but to download and store all of that user's contact information, without their permission.



Business Insider last Thursday ran the headline "Facebook says it 'unintentionally uploaded' 1.5 million people's email contacts without their consent."  In exclusive reporting, Bob Price wrote:  "Since May 2016, the social networking company has collected the contact lists of 1.5 million of its users new to the social network.  The Silicon Valley company said the contact data was 'unintentionally uploaded...'"



LEO:  Hmm, yeah.  Oh, it was a slip of the finger.



STEVE:  Yeah.



LEO:  How do you unintentionally collect somebody's contact list?



STEVE:  Exactly.  That's code.  You have to have code.



LEO:  Completely - yes.



STEVE:  "Unintentionally uploaded to Facebook" - and get this - "and is now deleting them."



LEO:  Oh, yeah, I bet.



STEVE:  Uh-huh.  Well, and of course that doesn't matter, Leo, because after you've ingested it...



LEO:  Right, it's in the social graph.  They've got your graph now.



STEVE:  Exactly.  So of course this revelation comes after the pseudonymous security researcher e-sushi, that was his name, "noticed that Facebook was asking some users to enter their email passwords" - this is what we talked about two weeks ago - "when they signed up for new accounts to verify their identities."  And of course, anyway, Business Insider then discovered that if you entered your email password, a message briefly popped up saying it was "importing your contacts" without asking for permission.  And I actually have a screenshot from the Business Insider coverage of this in the show notes.  



"At the time it wasn't clear," wrote Business Insider, "but Wednesday [last Wednesday] Facebook disclosed to Business Insider that 1.5 million people's contacts were collected this way and fed into Facebook's systems, where they were used to 'improve' Facebook's ad targeting..."



LEO:  Of course it was.



STEVE:  "...build Facebook's web of social connections, and recommend friends to be added.  A Facebook spokesperson said before May 2016 it offered" - now, get this, Leo.  "A Facebook spokesperson said before May 2016 it offered an option to verify a user's account using their email password and then voluntarily upload their contacts at the same time.  However, they said, the company changed the feature, and the text informing users that their contacts would be uploaded was deleted, but the underlying functionality was not."  Which is Facebook speak for, "Some users were saying no.  So we decided..."



LEO:  We just did it without asking.



STEVE:  "...they meant to say yes."  Because they were confused about how beneficial this would be.



LEO:  Did you have to give them your email password?  Could you have said no?



STEVE:  Yes, you could.



LEO:  But a lot of people just say, oh, yeah, fine, okay.



STEVE:  Yeah.  It wasn't at all clear that this was a voluntary thing.  So there were other means you could use.  But of course Facebook wanted their email password, and they wanted to flesh out their social graph, so they didn't - it's like Microsoft.  Would you like to upgrade to Windows 10 now or tonight?  It's like, I don't want Windows 10.  But we famously remember the choice that was finally given.  Now or later?  It's like no, neither.  Anyway, wow.  So 1.5 million people's contact books were directly harvested by Facebook.  And of course now it's been ingested, so it doesn't matter if they're deleting it.



A Facebook spokesperson said in a statement to Business Insider:  "Last month we stopped offering email" - after they were caught - "stopped offering email password verification as an option for people verifying their account when signing up for Facebook for the first time."  Okay.  Last month; right?  Since May of 2016?  "When we looked into the steps people were going through to verify their accounts, we found that in some cases people's email contacts were also unintentionally uploaded to Facebook when they created their account."  That's what Facebook is saying now.



LEO:  Unintentionally.



STEVE:  Yeah.  Facebook is saying that we weren't aware this was happening, so we stopped offering that, and oh...



LEO:  It's still happening.



STEVE:  Oh, my goodness.  The message went away, but the conduct...



LEO:  It's just it's so blatant.  It's so awful.  It's so terrible.  I recently created a new private account on Instagram, and they just beat the drum.  Give us your contacts, give us your contacts.  And it's just very clear how valuable that information is.  And don't forget, when you give somebody your contacts, you're giving your friends' personal contact information to a third party.  Not just yours, but your friends'.



STEVE:  Mom, bless her soul, when she was alive, for like my birthday or Christmas or whatever, or Valentine's Day, would send me those greeting cards.  And I kept telling her, "Mom, you're putting my email address, which I consider private information, into a website that sends a free greeting card.  Mom, I love you, but I don't want" - because now, I said, now that is valuable information to that website.  I'm getting spam from that website saying, oh, don't you want to send a greeting card to someone?  No.  I don't.



LEO:  Well, and of course it's gotten far worse because it isn't just, oh, we can sell a mailing list now, and spam, because actually Facebook doesn't want to do that.  They want to own your addresses and all your friends' addresses.  They don't want to give it to a third party.  But they use it in ways far, far worse.  They know everything.  It's terrible.



STEVE:  So there's another piece of this also.  Facebook has said it didn't store the passwords.



LEO:  Yeah, right.



STEVE:  Okay.  Not that it needed to.



LEO:  No.



STEVE:  It used them, sucked everything dry, and then said, okay.



LEO:  We're done.



STEVE:  We've got all we need.  Okay.  So but in yet another Facebook privacy blunder which came to light recently, like last month, the company confirmed that it improperly stored hundreds of millions of user passwords in plaintext rather than as hashes.  At the time Facebook said that this plaintext password storage error affected hundreds of millions of Facebook Lite users, tens of millions of other Facebook users, and tens of thousands of Instagram users.



That Facebook disclosure was just updated last Thursday to say that the number of affected Instagram accounts was much higher.  Thursday's update said, and get this language:  "Since this post was published, we discovered" - now, that's actually the word they used - "we discovered additional logs of Instagram passwords being stored in a readable format.  We now estimate that this issue impacted millions of Instagram users.  We will be notifying these users as we did the others.  Our investigation has determined that these stored passwords were not internally abused or improperly accessed."



Okay.  Now, how could they possibly make such an assertion after having "discovered additional logs of Instagram passwords being stored in readable format"?  It's very clearly a total and utter unorganized disaster over there.  As I said, if Zuckerberg wants to try to rebrand this as a privacy service, he just needs to scrap it.  Just really...



LEO:  No, he needs, I mean, truthfully not trust anybody over there anymore.



STEVE:  You're discovering logs of plaintext passwords?  So, like, is there no management of any privacy information?  It's just amazing.  And again, if you really wanted to give them the benefit of the doubt, you would say that it's a bunch of Young Turks with freshly minted computer science degrees and an environment of let's try stuff and see what sticks.  Wow.  



LEO:  You know, at this point I can't even be outraged anymore.  It's just...



STEVE:  No, no, no.



LEO:  They're obviously malefactors.  They're not - you can't excuse this any longer.



STEVE:  It's too bad.  So Russia has moved closer to adopting, and I can't wait to watch this happen, the Internet Master Cutoff Switch.  And of course, Leo, my favorite organization over there in Russia is behind it.



LEO:  Oh, yeah.  Oh, of course they are, Rossonomabravo.



STEVE:  Rossmonkozdor, whatever the hell it is.  Russia's lower chamber of parliament has backed a bill which privacy advocates fear could lead to the creation of a censorship system similar to - oh, you think? - China's Great Firewall.  The Associated Press reported Thursday that the State Duma, which is the lower house of the Federal Assembly of Russia, has advocated for the bill overwhelmingly.  The new regulations - if also accepted by the upper chamber, which belongs to the Federal Assembly, and then signed into law by President Vladimir Putin, and who imagines he won't just, I mean, he's just like waiting - would require Internet Service Providers to route Russian Internet traffic locally through the country.  In other words, to have the capability of doing that.



This would give Russian authorities the opportunity to use equipment and software to establish man-in-the-middle communication eavesdropping, as well as to block and censor global content that Russia does not want its citizens to have access to.  Although the Russian government has said that it will bear the cost and will reimburse ISPs, the country's ISPs would be required to provide equipment to exchange points approved by Russia's telecoms watchdog, here it comes, Roskomnadzor; and a localized domain name system, like their own DNS, would be prepared to support the localization of content.  And so I guess that creates Ruskynet or something.



But the point is they want the ability to raise the shields around Russia and still have a working countrywide network.  At the moment, for example, if DNS queries are going outside, and you raise the shields, well, everything breaks.  So they're basically going to need to create, I mean explicitly create a different kind of subnet which can function if it needs to take itself off of the Internet.  Advocates of the bill claim that this whole concept would be a protective measure, only meant to protect the Internet within the country should a hostile entity cut off access.  Okay.  As well as a means to insulate Russian traffic from potential cyberattacks by foreign entities by removing traffic rerouting outside of the non-Russian systems.



However, of course, many others believe that this creates top-level control, which would give Russian lawmakers an overarching authority to control the web within the country, as well as to monitor its citizens' online habits.  I mean, you can imagine that part of this would be everybody using a system would have a root cert in their root store for a central Russian authority which would then explicitly allow all man-in-the-middle filtering to be conducted.  So, you know, of course, I mean, that's what's going to happen.



So anyway, I did want to mention that last time we talked about this our listeners reminded us that the full, unfiltered Internet is also in orbit above us.  So in fact we are not strictly limited to what landlines or short-range WiFi is able to carry.  There are satellites up there, and the Internet is in the sky.  So although it would certainly be an effective deterrent for most Internet connectivity, people who still really wanted to get unfiltered probably could.



So ZDNet carried the story of a 27-year-old Indian national who graduated two years ago with an MBA from the College of St. Rose in New York.  He was just charged, I'm sorry, he just changed his plea from not guilty to guilty.  And it occurred to me that the change in plea may have had something to do with the videos that he made of himself killing the college's computers, which the prosecutors got their hands on.  The incident took place on February 14th, according to court documents obtained by ZDNet.  He recorded himself, videoing himself while destroying some of the computers.  On these videos he was seen to say:  "I'm going to kill this guy."  "It's dead."  And "It's gone, boom."  This guy destroyed 59 computers, but also seven computer monitors and computer-enhanced podiums that had open USB slots.



ZDNet wrote, they said:  "He did it using USB Killer, a weaponized thumb drive that he purchased from a well-known online store that sells these types of devices."  And my guess is that was probably eBay, although Amazon has them.



LEO:  Hak5, too.  He sells a lot of hacking stuff.



STEVE:  Yeah.  So as we know, we've talked about these in the past, but I just kind of wanted to - we haven't talked about them for a while - just to note that they are real.  I have a picture of one at the top of the story in the show notes, just sort of showing what one looks like.  But if you go to eBay and put "USB killer," China is happy to sell you one for about 40 bucks.



LEO:  I wonder if that's what the Chinese spy had in her possession when she broke into Mar a Lago? 



STEVE:  No, well, I mean, she might...



LEO:  And the Secret Service agent stuck it into his computer to see what was on it.  And was shocked, shocked I tell you, to see it launch some software.



STEVE:  Yes.  What a surprise.



LEO:  What a shock.  We've never seen this happen before, they said.  Literally.  All right.



STEVE:  Okay, well, that was different because that was malware.



LEO:  Well, who knows what it was?  It was installing something; right?  What does this do?



STEVE:  Well, this charges internal capacitors up to 200 volts.



LEO:  Oh, it actually fries it.



STEVE:  And then blasts the USB port with 200 volts.  And that goes right into the central chipset of the motherboard and kills the motherboard.



LEO:  Oh, it's a computer killer.



STEVE:  It's a computer killer, yes.



LEO:  And that little USB condom you sent me, would that save me?  No.



STEVE:  I don't think so.  It would blast right through that because the USB condom allows power to go through, but it doesn't allow the data to go through.



LEO:  Just not data, right.



STEVE:  So what this thing does is it's a little inverter.  You plug it in, and it goes, very much like in the old days, remember, when your flash had to recharge, it would go [mimics sound].  So that was the xenon strobe in the flash needed a high voltage in order to create a plasma in the gas.  So it would take the battery and run a little inverter to charge up the flash's capacitors; and then, on cue, it would dump that high voltage into the xenon tube and create a flash.  This is similar.  It charges high-voltage capacitors in the thumb drive up to 200 volts and then lets it loose into the USB port, certainly killing the port, and almost always killing the entire computer because now most of the chipsets directly drive the USB ports themselves.  So, I mean, it is...



LEO:  This is malicious.  This is not a hacking [crosstalk].



STEVE:  It's pure malice.



LEO:  It's just being mean.



STEVE:  Pure malice.



LEO:  I love it that they brand it "USB Killer."



STEVE:  Yeah.  You can see it says HV+ and HV-, and it also shows DC-200V.  So it is a little 200-volt power supply that discharges...



LEO:  Geez Louise.



STEVE:  ...itself back into the port that gave it the five volts in the beginning.  Only takes a few seconds, and it will kill anything it's plugged into.  I looked around...



LEO:  This is, by the way, version 2.0  Accept no substitutes.



STEVE:  Ah.  Well, actually Hong Kong now has version 3.0.



LEO:  You know, to be fair, you could just bring a hammer.  Right?



STEVE:  Yeah.  



LEO:  I mean, if you want to destroy a computer.  I guess the benefit of this is it's somewhat more surreptitious.  You could just say, well, I don't know.  This computer doesn't seem to be working anymore.



STEVE:  Well, and that's the problem.  And that's why I want just to put it back on our listeners' radar again is that anybody can buy this.  All it takes is somebody slipping one of these things into a USB port which is available and counting to - not even counting to 10, and that machine is dead.  



LEO:  Wow.  Wow. 



STEVE:  I mean, dead dead.



LEO:  Time to glue up your ports.



STEVE:  Yeah.  So he destroyed 59 computers.



LEO:  What?



STEVE:  Seven computer monitors, because of course computer monitors also have USB hubs in them now.



LEO:  Sure.



STEVE:  And computer-enhanced podiums that had open USB slots.  So the guy's a creep.  He's facing, let's see, in total the equipment damages were $51,109, along with $7,362 in employee time for investigating and replacing the destroyed hardware.  He's facing 10 years, up to 10 years in prison...



LEO:  Good.



STEVE:  ...and a fine of up to a quarter million dollars, followed by a term of post-imprisonment supervised release of up to three years.  So there are some USB plugs, but none of them seem to actually lock in place.  There isn't a universal thing that a USB plug could really, like, lock into.  You'd think - there is a tongue sticking out.  So you'd think someone could come up with something that would go in, but would refuse to come out.  But I looked around and didn't see anything that was readily available because really, I mean, not that lots of us are worried about that, but boy.



LEO:  Sort of jerky thing to do.



STEVE:  It really is.



LEO:  Jerky.  He even made YouTube videos of it.  That's how he got caught.



STEVE:  Oh, boy.  No kidding.



LEO:  Yeah.  Yeah.



STEVE:  Yeah, that would do it.



LEO:  I think he wanted to get caught.  He was a jerk.



STEVE:  Yeah.  He got his MBA from the school that he then zapped.



LEO:  Fried.  Geez.



STEVE:  Yeah.  So now going from bad guys to good guys, Marcus Hutchins, Leo.



LEO:  I still feel kind of bad for him.



STEVE:  I do.  I do.  I mean, yes, when he was a kid, when he was a kiddy, he got his...



LEO:  A teenager, yeah.



STEVE:  A teenager.  He got his start hacking and doing some things that were arguably malicious.  But he matured, and for a long time he was doing good.  I mean, we knew of him because in May of 2017 he was the guy - so, what, just two years ago next month.  He was the guy who stopped the WannaCry ransomware outbreak in its track.  He reverse engineered WannaCry because he was now a security researcher wearing a bleached white hat, so no doubt about the fact that he had, like, seen the error of his ways and was doing good.



Our listeners will remember because we talked about it at the time, he noted that WannaCry, which was a worm that was devastating the Internet, 200,000 systems in a 150 countries were affected by this, and it had caused billions of dollars' worth of damages.  It was just starting, because it was a worm, its growth was just starting to go exponential when he stopped it.  And he did so by registering a domain name that he saw being referenced by the worm.  And when he looked up the domain, it was unregistered.  So he thought, huh.  That's interesting.  He registered it.  And he gave it a home, and the worm stopped.



It turns out, as far as we know, we never knew who created WannaCry, but it looked like somebody built in a cutoff switch for it in the form of this domain name.  Marcus, registering the domain name, stopped WannaCry.  Unfortunately, he was nabbed by Las Vegas PD at McCarran Airport after the Black Hat and DEF CON conferences a couple summers ago and detained, trying to go back to his home in the U.K.



And he's in the news now because he just posted to his blog, MalwareTech.com, the public statement.  He said:  "Legal case update:  As you may be aware, I've pleaded guilty to two charges relating to writing malware in the years prior to my career in security.  I regret these actions and accept full responsibility for my mistakes.  Having grown up, I've since been using the same skills that I misused several years ago for constructive purposes.  I will continue to devote my time to keeping people safe from malware attacks."  And then on Sunday, on his Twitter page, he tweeted:  "To be clear, this statement wasn't required by the plea deal.  It was my decision to post it."



LEO:  Good for him.



STEVE:  Yeah.



LEO:  Do they say what his sentence will be?



STEVE:  Let's see.



LEO:  Maybe they haven't sentenced yet.  Maybe he's just...



STEVE:  Two of 10 counts in the Eastern District of Wisconsin on Friday, one charge for distributing Kronos, which is the banking malware, and the other charge for conspiracy.  And my feeling is it appears the U.S. has decided to make an example out of Marcus.  According to court documents, he now faces up to 10 years in prison and half a million dollars in fines.  The good news is he also had some good legal representation who does think that he is being mistreated because of who he has been ever since.  And really, it does really seem overboard.  There are really, really bad people really, really doing bad things.  And it'd be nice if they got a lot more attention than Marcus, who made some mistakes when he was younger.



One of the Windows zero-days which was patched two weeks ago has now, since then, come under heavy use in the wild, which is being used to facilitate full system takeover.  And what's interesting to me is the change in usage of this.  We're learning more about one of those two zero-day flaws Microsoft patched two weeks ago since it's now in active use in advanced persistent threat campaigns.  As we noted last week, it was discovered by two researchers at Kaspersky's Lab on St. Patrick's Day of this year, when they found it being used against one of their customers who is under their protection.  It's a use-after-free bug in the Windows kernel win32k.sys module.



The flaw allows a local privilege escalation, and it's being used in advanced persistent threat campaigns targeting 64-bit versions of Windows from 7 up through older builds of Windows 10.  The attackers are using the bug to establish persistent backdoors in targeted machines, gaining the ability to run arbitrary code in kernel mode.  An attacker could then install programs; view and change and delete data; create new accounts under full user rights.  And Microsoft has admitted that that's what this allows.



What's most likely in this instance is not that the fix was reverse engineered, as is often done, to discover previously unknown bugs once they've been patched, but rather that those who were previously deploying this potent flaw in very limited and only in highly targeted attacks now know that its useful lifetime, now that it's been patched, is extremely limited.  So prior restraint has been discarded, that is, restraint in the use of this.  You know, it was valuable when it was a zero-day.  Now not so much.  So they are racing to exploit it into what is I'm sure a rapidly dwindling base of still vulnerable machines, any which have not yet been patched from two weeks ago on April's Patch Tuesday.



So that's yet another sort of interesting dynamic in the way the world is evolving.  We're seeing flaws that come to light that are not known by anyone, suddenly being exploited after their patches are reverse engineered.  Here we're seeing one that was known to a few people, at least one, that had been exploited with a profile of very selective targeting, suddenly being released for much wider targeting, almost certainly not because it was reverse engineered, but because the people who were using it selectively thought, oh, crap, the jig's almost up with this.  Let's get as much use out of it as we can before it won't work anymore.  So, wow, another interesting dynamic.



Some interesting Edge news.  And I was very impressed with Lawrence Abrams' coverage of this over at Bleeping Computer.  I have the link to this coverage in the show notes.  He walks us through a chain of events that could cause Edge to inadvertently be run with admin privileges.  What he noted is that - so we know that we've got a forthcoming version of Edge which is available currently.  Although it doesn't explicitly say that Windows 7 users can download it, it works under Windows 7.  And it does explicitly say that it will eventually be formally released for 7, which is neat.



It's, as we know, based on the Chromium engine.  And the first thing Microsoft did was strip out all of the Googlization that had been done to Chromium, and now they're Windowizing it, or Microsoftizing it by building their own stuff back in.  One of the things they just added is a notice if Edge is launched with admin privileges.  A popup comes up saying "Administrator Mode Detected."  And then it says "Close Microsoft Edge and relaunch in non-administrative mode for best performance."  Well, it's really not best performance.  It's best security.



So Lawrence takes us through a scenario where to edit the hosts file it would be necessary to run Notepad as an admin.  Then, if when editing the hosts file you encounter a suspicious entry, and I think in his example he said www.malware or malicious.com or something.  It's like, yikes; you know?  So if you then highlighted it and right-clicked on it, there is an opportunity, say "Search with Bing," which would launch Edge.  And because the children of processes inherit the rights and the account of their parent, and since Notepad had been launched with admin rights in order to allow you to edit the hosts file, Edge would then be launched with admin rights.  And if you didn't have this notice warning you, you might do some searching with Bing and then decide, okay, fine, well, whatever.



But if you left Edge running, it would keep running with admin rights.  And if you were then to download something, or to go somewhere malicious, you're now browsing with your system's largest attack surface exposed to the Internet.  And god help you if you were to download a program and run it because of course you are able to run things from within the browser.  You would be running it with admin privileges.  Which would remove all constraints over what it would be able to do.



So first piece of news is Edge is getting a notice like this.  It is the case that someone could be running in admin mode without it occurring to them.  And tip of the hat for BleepingComputer doing such a nice job of covering this little bit of news.



Also in Edge news, Microsoft's new Chromium-based Edge browser is in the process of gaining the ability to run within Microsoft's very powerful and useful Windows Defender Application Guard sandbox.  It can actually run in there now; but a bunch of warning messages which are very useful to inform the user when something has been blocked, like you can't download files from within there and save them on your system because that's the whole point, those messages are not yet present.



So it's probably worth waiting for its official release.  And of course I've often lamented the need for a really, really strong isolated sandbox, I mean, really like to the level of a VM, for our browsing.  This integration of Chromium with Edge having Windows 10 latest security features, including allowing Edge to run under Microsoft's Application Guard, the Windows Defender Application Guard, I think this is a huge win.  Apparently it takes a while to get it up and launched, but I would argue if at any point you're doing something that you think might be a little sketchy or going somewhere sketchy, having this there is going to be very useful.  So tip of the hat to Microsoft.  I mean, our browser is our system's attack surface these days, that and email.  And protecting it is a great thing.



And also I was a little disturbed to see that, while we're on the topic of Edge, it's interesting to note that, as the new Chromium Edge reacquires some of the unique capabilities of its Edge HTML-based predecessor - so to explain that, Microsoft basically went all the way, developed Edge HTML as their own rendering engine.  Then they decided, okay, for whatever reason, we're no longer going to continue supporting it.  We're going to switch to Chromium.  It's what everyone apparently wants.  We get to leverage the open source community and so forth.  We'll wrap Edge around the Chromium guts.



Well, what that meant was initially all of the previous Edge HTML stuff disappeared, whatever it was that Microsoft liked.  So they're now beginning to reincorporate that into the Chromium engine contained by Edge.  So Edge will be dynamically changing its user agent string to show different faces to different sites.  Which, you know, it's not quite a face plant for me; but it's like, this is not the way the Internet is supposed to work.  But, for example, at the moment, when visiting Netflix, HBO Now, HBO Go, Napster - I didn't even know Napster was still a thing - or Sling, Edge will display its Edge persona.  That is, to those domains, to those websites it says we're the old HTML Edge browser.  But when visiting Facebook, Messenger, or there's an Australian media streamer stan.com.au, Edge masquerades as Chrome and shows its Chrome user agent.



So this is a horrific kludge.  On the other hand, the user agent field has a long, horrifying history of not really being what it pretends to be.  I remember we've talked about this years ago on the podcast, how annoying it was that looking at the IE user agent field years ago, and in fact any browser's user agent field, they all had Mozilla in them for some reason.  It was like, what is Mozilla doing in there?  It's got nothing to do with Mozilla.  It was like, yes, but some sites want to have Mozilla in the user agent field.  So we're giving it to them.  It's like, what?



But, yeah.  If anyone's interested, there's a wonderful blog posting I found, the "User Agent String History," which details accurately and from the viewpoint of someone who's been through all of this, the historical step-by-step from Netscape Navigator on of what the various browsers did, noting that at one point Opera gave you a dropdown choice, either radio buttons or a dropdown list box of which user agent you wanted to use for your Opera browser.  It's like, oh, lord, really?  But anyway.



I don't know where this is going to shake out, whether Microsoft always intends to have a per-site choice of browser impersonation in its user agent field.  I hope not.  I hope this is just some transient thing.  But apparently there are things that Edge HTML offers that Netflix, HBO Now, HBO Go, Napster, or Sling take advantage of if it's present.  And Microsoft has made it present, and so Microsoft wants it used.  I don't know.  Anyway, just an observation.  And it is nice, though, that we're getting Edge moving along.



My best friend sent me a text with a photo of his screen, saying, "Whaaat?"  And he got the "After 10 years, support for Windows 7 is nearing the end."  We've talked about this forthcoming notice.  It hasn't hit me yet.  The good news is, in little itty-bitty fine print in the lower left, you're able to say "Don't tell me any more."  So when everybody gets it, they can just say, okay, yeah, I know.  And it is a little annoying that Microsoft is using this to sell PCs.  They're literally, in the link you click for "Tell me more," they are saying, "You need a brand new computer in order to efficiently run Windows 10.  So click here to shop for a new Windows 10 machine."  And it's like, wow, okay.



And finally, well, not finally.  Well, maybe it is finally, oh, it is finally in the news department.  This is from the "I did say this was bound to happen" department.  We just have in the news that Mozilla's Firefox browser will be enabling hyperlink auditing, a.k.a. URL ping tracking, by default in a forthcoming release of Firefox.  Larry was on the ball again over at BleepingComputer, and they're following this.  I don't know if he listens to the podcast, but of course URL ping tracking was our topic two weeks ago.



Quoting from his coverage, Mozilla feels, as I do, that it's a performance improvement.  He said:  "While some users feel this feature is a privacy risk, browser developers feel that trackers are going to track, so they might as well offer a solution that provides..."



LEO:  Trackers gonna track.



STEVE:  Trackers gonna track, "...that provides better performance.  In a post by Apple, the WebKit developers explain that hyperlink auditing pings are a performance improvement because, unlike other tracking methods, they do not block or delay the navigation to the requested site."  That's of course my observation from two weeks ago.



Quoting Apple in BleepingComputer's coverage:  "Just turning off the ping attribute or the Beacon API doesn't solve the privacy implications of link click analytics.  Instead, it creates an incentive for websites to adopt tracking techniques that hurt the user experience."  And we talked about that, too, the 302 redirect chain, which is the alternative.  "In effect," writes Apple, "the choice between supporting ping and not is not one of privacy; rather it is a choice between a good user experience and a bad one."



Larry writes:  "After reading Apple's post, I contacted Mozilla to see if they agreed with the views expressed in the WebKit article.  Mozilla told BleepingComputer via an email that they agreed with Apple's views on hyperlink auditing.  Furthermore, they stated that the only reason it's not currently enabled by default in Firefox is because their implementation is not ready."  Okay, now, I find that curious because it can be turned on.  If you go to about:options, as I talked about two weeks ago, it's there, and you can turn it on if you want.



Anyway, Mozilla wrote:  "We agree that enabling the hyperlink ping attribute that is commonly used for hyperlink auditing isn't a question of privacy, but a matter of improving the user experience by giving websites a better way to implement hyperlink auditing without the performance downsides of the other existing methods listed in the WebKit.org blog post.  In fact, we already support the sendBeacon API.  And the reason we don't yet enable the hyperlink ping attribute is that our implementation of this feature isn't yet complete."  So I guess they want to do a few more things.



And finally, writes Larry:  "When we asked if they felt that users should at least be given the ability to disable the feature if they wish, Mozilla stated that they did not believe it would have any 'meaningful improvement' to a user's privacy."  So they're not even going to let it - they're going to turn on by default, and they're going to take away the option to turn it off.



Mozilla says:  "We don't believe that offering an option to disable this feature alone will have any meaningful improvement in the user's privacy since websites can, and already do, detect the various supported mechanisms for hyperlink auditing in each browser; and disabling the more user-friendly mechanisms will cause them to fall back to the less user-friendly ones without actually disabling the hyperlink auditing functionality itself."  Meaning users are better off if it's on and stuck on.



Brave states it will continue to block this feature.  Larry wrote:  "After Mozilla's response, we also contacted Brave Software to ask if they had any plans to enable hyperlink auditing in their browser."  Brave wrote:  "Disabling hyperlink auditing is a crucial privacy feature, and Brave has always disabled this by default."  That was written by Catherine Corre, Head of Communications at Brave Software.  She finished, saying:  "Brave users expect this protection from our browser."  So there.



And of course I know that this is a controversial topic after what some of our users felt was my own capitulation on this issue.  I received a bunch of angry and annoyed feedback through several channels.  But it is really, truly, I would argue, almost impossible any longer to actually not be tracked on the Internet.  It's like, you just, good luck with you.  It's just - you just can't arrange not to be tracked.



And Leo, I think time for our last break, and then I'm going to share some interesting miscellany about my newly relaunched and relocated blog, my experience with a newly minted Authenticode certificate, and what Cisco's Talos Group have found.



LEO:  I'm just curious about this URL hyperlink auditing.



STEVE:  Yeah.



LEO:  So we don't allow tracking cookies of any kind.  Our advertisers are always saying can you put one on, but we don't do it.  We have some banner ads on our website.  But we do allow UTM trackers, which basically is a referrer.  So if you go to a sponsor page or a banner, and you click the link, and you go to the website, they're informed where you came from.



STEVE:  Right.



LEO:  You came from TWiT.  And the value of that for us is that they can track whether they're getting results from banners on our website.  Is that what you're talking about?



STEVE:  No.



LEO:  No.  Okay.



STEVE:  So you know how a standard <a> tag has a url= and, you know, the link.  And so when you click on it, it takes your browser there. 



LEO:  Right.



STEVE:  There's another verb that can go in an <a> tag, ping=.



LEO:  Right.



STEVE:  And that could have a list of URLs.  And so the reason the browser...



LEO:  Oh, I see.



STEVE:  Yeah.  So the reason the browser vendors prefer it is it creates a fork.  When you click on that URL, your browser immediately goes to the URL in the URL field, but then asynchronously it sends POST queries to the list of URLs in the ping argument.



LEO:  You can see how inefficient that would be if it were blocking because you'd have to wait till all of those POSTs happen before you got to the web page.



STEVE:  Correct.  Well, in fact, if you don't have the ping, what you have to do is the URL cannot take the user to its destination.  It has to follow a chain of those websites following a redirection chain until it gets to the last one, which then finally takes you to where you want to go.



LEO:  Really slows things down, yeah.  That's crazy, yeah.



STEVE:  Yes, yes.



LEO:  Yeah.  And the reason I'm asking is I said, yeah, it's okay for us to do UTMs after looking into it because I felt like, well, that's kind of an unobtrusive, fairly unobtrusive way of just sourcing where they got the hit from.



STEVE:  Yup.



LEO:  And our advertisers do want to know that.  So that's the only kind we allow.



STEVE:  Yup.



LEO:  Yeah, okay.  Thank you, Steve.  All right, Steve.  



STEVE:  So I originally had two blogs hosted at WordPress.  I had a CNAME record in my DNS which mapped steve.grc.com to one of those, and blog.grc.com to another.  That worked years ago.  But today it doesn't because it's hostile to TLS since WordPress doesn't have certificates for those domain names.  So it was referring - so someone doing https://blog.grc.com, when it was hosted at WordPress.com, would get error messages.



LEO:  The wrong certificate; right?



STEVE:  Yeah, exactly, would get the wrong certificate.  And it had been - it was something that sort of - it was just sort of chafing for a long, I mean, you know, I've got my own servers.  I've got a facility.  I've got bandwidth.  I've got all the other plumbing required to do this.  What I did not have was a server running, like a mature, safe, sandboxed server running PHP.  I do have that now because the forthcoming SQRL web forums are hosted on PHP.  So now that I've got a mature PHP server facility set up at GRC, I decided that it was time to, first of all, consolidate those and set up my own WordPress blog there.  Today - and it's in place.  I announced it.  I tweeted it.  I posted to both of the old blogs for all the subscribers that I had amassed there that it was now just blog.grc.com.  Anybody who's interested can go to blog.grc.com and see the inaugural posting there.



So I'm glad I did it.  And I guess I would do it again.  But frankly, simply setting up a blog at WordPress - and we should mention just for the record they're a sponsor of the show.  While you've been talking about them during this podcast, I just haven't had much to say.  But having installed it and then bolted it down, I really appreciate that bolting it down is crucial.  And of course I knew how to do that.  But most people just want to blog; you know?  They're not hosts of a security podcast.  And of course after I thought I had done everything I knew to do, I went looking around the 'Net for advice from those with more WordPress-specific experience than I have, and also those who have some scars from the arrows in their backs in the past.



LEO:  Who doesn't? 



STEVE:  Well, yeah.  And, I mean, I found people who, like, whose self-hosted WordPress sites had been hacked, and who said, you know, ooh, here's what I learned from the experience.  So I was pleased, first of all, that no one had anything to say that I hadn't already arranged at least as good and sometimes a superior solution for.  For example, everyone mentions the need for a really strong password.  Yeah.  And then also some advice to adding an add-on to have a password lockout.  Well, I of course have a 32-character total gibberish password that I've never attempted to even look at.  But my WordPress login page cannot even be reached by anyone who is not at one of a very few well-known IP addresses.  So I've gone even further.  You can't even access that page from the Internet.  So yes, I did belts and suspenders.  So I seem to be pretty well protected.



But in my roaming I encountered a site that made me think of the things you have talked about, Leo, about WordPress.  It was titled "The Top 10 Security Mistakes That Self-Hosted WordPress Blogs Make."



LEO:  I bet I've made every one of them.



STEVE:  And it's not long.  I just want to - I've excerpted from this.  The blog post said:  "According to Forbes, one out of every six websites on the Internet is powered by WordPress, nearly 60 million in all, with 100,000 more popping up each day."  A hundred thousand a day.  "WordPress.com currently hosts over 56 million blogs.  As of this writing, WordPress stats did not include the number of self-hosted blogs, but," this person writes, "rest assured there are many of us.  I've been using WordPress since Gold days."  Don't know what that means.  Was there something called WordPress Gold?  Anyway...



LEO:  I don't remember, no.



STEVE:  "And it only gets better with each release.  In the past I have been the victim of two WordPress hacks.  At the time of the first hack I was on a managed VPS.  All maintenance and administrative tasks, including software updates, was administered by the hosting provider.  In my case, the software was rarely updated.



"Running a self-hosted blog comes with myriad responsibilities. It is not like you can merely install it and be done with it. Your first priority should be to familiarize yourself with the platform, along with the pros and cons of self-hosting or hosting your blog at WordPress.com.  If you self-host, you will need to be somewhat tech savvy.  If not, hire someone who is.  When you self-host you are responsible for technical maintenance:  backend configuration, backups, blog security, logs, spam filtering, and updates.  Take the time to find a reputable and reliable hosting service.  Do your research first.  You don't want to end up on a server that is easily compromised, is slow to update software, has bad tech support, or has too much downtime.



"The fact that hackers and cybercriminals favor targeting WordPress is for the same reason they favor exploiting Microsoft Windows.  It's popular.  I have seen a lot of site admins downplay the importance of updating CMS software and hardening company blogs.  This is especially prevalent with small businesses and startups that rely solely on development teams to schedule site updates and releases.  I have also seen many home businesses slap together self-hosted blogs because they noticed that cPanel had a Fantastico, Softaculous, or Installatron auto-installer."  Yeah, right, like the one click, get a blog.



LEO:  Yeah, I've used that, yeah.  Mm-hmm.  



STEVE:  Uh-huh.  "And they think that all they have to do is populate their blog with posts, widgets and plug-ins."



LEO:  So easy.  Yup.



STEVE:  He says:  "For the love of Matt Mullenweg, please check out WordPress.com."



LEO:  Yes.  Let them do it.



STEVE:  So anyway, yes, that's a plug.  And it's a sincere plug.  I mean, if you are a propeller head, even Drobo will run a WordPress blog.  But you really, really, really need to be thoughtful about it.  And there is something...



LEO:  You have a responsibility all of a sudden.



STEVE:  There is some jot thing that WordPress has, which I'm signed up for.



LEO:  Jet.



STEVE:  Jet, yeah.  And so it is a series of...



LEO:  Love Jets.  Love Jets.



STEVE:  Yeah.  So it's a series of services that's not free, but it's inexpensive.  It's like three bucks a month or something.  And so you get Akismet to do your spam filtering.  You get something else that does daily backups.  You get something else - and then WordPress is also constantly checking for any updates and has the ability to proactively update your code if something bad happens.  So, yes, I'm self-hosted.  But even so, I'm also subscribed to this little, you know, have them looking over my shoulder because they own Jet, by the way.  It's Jetpack.



LEO:  Yeah, yeah, yeah, it's so good.



STEVE:  That's it, Jetpack.



LEO:  Yup.  I highly recommend it.



STEVE:  So next little piece of miscellany is I've had an interesting experience.  We are just at this very moment going through some upheaval over the expiration after three years of my then, or up to now, longstanding Authenticode code signing certificate.  As this was approaching - and in fact, Leo, during that last sponsor interval I went to the Win10 system that I'm talking to you through Skype on because the Edge SmartScreen was blocking the SQRL client because it is signed with a new certificate.



LEO:  Oh, yeah.



STEVE:  That has not yet acquired a reputation.  So what happened is, after three years of having a spotless reputation with the Authenticode code signing certificate that I had from DigiCert, those three years were coming to an end.  As this was approaching, I noticed that DigiCert was offering EV code signing, which I'd never really paid any attention to, even though Microsoft introduced it seven years ago in 2012.  So it turns out that to perform EV code signing, a developer must have a physical encryption dongle.  And it came in a cute DigiCert box.



LEO:  Oh, that's cool.  



STEVE:  Yeah, and you can see where there is that little divot in the box is now empty because my hardware dongle which has the private key bound into it and will never let it go and which performs the actual encryption for me, it's stuck into my computer so that I'm able to perform EV code signing.



Anyway, so it turns out that for EV code signing a developer must have a physical encryption dongle, and the secret key must be buried deep inside it.  And this of course prevents that secret from being exfiltrated electronically over a network.  So of course that's what I got from DigiCert.  I just showed the box.  And I'm really tickled to be able to EV sign my work with it for the next three years.



However, the trouble with any new certificate is that it will initially not have accumulated any reputation.  And I mean literally.  I mean reputation, even though it's Gibson Research Corporation, and I have a reputation, the cert doesn't.  And remember that, technically, the certificate itself is asserting who signed it and that what was signed has not been modified.  But that's all it's asserting.  So nothing would prevent a malicious actor from obtaining a certificate under the name Gibson Research Corporation and signing malware with that certificate and putting it out onto the Internet.  I mean, I hope it doesn't happen.  It never has.



And the idea is that, in the same way that certificate authorities are supposed to make sure that a domain name really owns that domain, nobody should be able to get Gibson Research Corporation other than me, who had to prove that I am Gibson Research Corporation.  But from a practical standpoint, nothing prevents it.  Which says that it's the reputation of the certificate because what nobody else can duplicate is my certificate.  They could get a fraudulent one under the same name, but it would have a different hash.  It would be a different certificate, not mine.



So what I'm now in the process of needing to do is to get this new certificate recognized as trustworthy.  And first of all, this happened over this past weekend, and all the AV systems had a meltdown.  Which is unfortunate because there's nothing malicious about what I'm doing.  But this demonstrates to what degree the antivirus systems have become heuristic.  They look inside.  VirusTotal even runs the downloadable in a sandbox and watches what it does, watches its communications.



One of the things that the SQRL client does when you first run it is, before it even installs, it sends out a ping to check for an update to prevent the user even from installing an older one, just because I'm a belt and suspenders person.  It turns out that that behavior of sending out that ping was upsetting some of the AV scanners.  So I slightly modified it so that it wouldn't do that as often and as reliably as it was before.  Still now we're in this position of this brand new cert needing to prove itself.



So what I was doing was with SmartScreen, after Edge refused to download it, saying it was unknown, I right-clicked on the downloads and said this is not malicious.  Then I had to fill out a little form about why I didn't think so.  Oh, and there was a checkbox, you know, are you a user, or did you create it?  And I said I created this.  Believe me, it's not malicious.  So anyway, the other thing I did was I re-signed four of the top five downloads from GRC:  DNSBench, InSpectre, Never10, and LeakTest, believe it or not is still one of the top five.  I re-signed them with this new EV cert because about 3,500 copies of those in aggregate are downloaded every day.



So that will, you know, I just need to get its use out there so that, when people start wanting to play with SQRL, they're not being alarmed by AV misfiring.  And none of the previous SQRL releases, you know, we've been working with SQRL for years, and I've never had this trouble.  But what's interesting is that three years ago, when the certificate that has just expired was new, that's when I was doing Never10, and the same thing happened then.  It was a new certificate.  It had not earned a reputation yet.  And so during the beta testing of Never10, people were saying, hey, I'm getting AV warnings, and SmartScreen is saying we don't know you and so forth.  It went away after a few days, after enough people said, yeah, no, I'm sure this is fine.  This is good.  This is a good guy.



So anyway, I just sort of wanted to share a little bit of experience from the field.  And one thing that I'm wondering is whether I had cross-signed or co-signed the existing software with both the old and the new cert before the old one expired, if that would have had some effect.  That is, could I - because it is possible to co-sign code.  So if I saw that the end was coming, could I get an update from DigiCert, then start co-signing the code.  It would be trusted because it contained the cert which had established three years of good reputation and also trusted because, well, and for that reason, but then also convey its trust to this new cert.  I don't know if that would work, but I'm going to try it in three years because this is a pain in the butt.  You know, our AV stuff has just gone so overboard, I mean, it's become so heuristic.  There's zero actual reason to believe any of my apps are malicious.  None of them are.



But all the AV, unless you have a reputation, and that's where it really comes down to.  It comes down to getting a certificate.  In fact, when I was doing a little browsing over the weekend I saw some posts from a guy who's, like, he said he has some accounting software.  He updates it every month.  It's constantly having this problem of scaring its users because Microsoft's SmartScreen says I don't know what this is.  This is not often downloaded.  You should not trust it.  And he says it's freaking people out.  And in this log of - it was a question he posted over on SourceForge or somewhere.  And a lot of people said you need a cert.  Get a cert.  A cert is the way that you establish a reputation independent of the code which it signs.  And that makes sense.  But on the other hand, it takes some time to establish that reputation.



Oh, and one of the postings on my new blog was from someone who, on April 19th he sent:  "Dear Steve.  As what might be called an 'historic' user of SpinRite, I have two questions for you."  He says:  "One, do you still make available a retail version of the product?"  And I didn't know what that meant, but he said:  "Or is SpinRite a 'download only' at this point?"  And then he said also:  "Two, searching high and low for my several versions of SpinRite, I have yet to find the original book/software/serial numbers.  So if I could provide you privately my name, address lived at when purchased and registered, for the versions I own, would you have existing records to verify my status as an owner?"



He says:  "Any response will be appreciated, as were a  number of your utilities - LeakTest, DCOMbobulator, Never10."  He says:  "But above them all, I'd hardly be able to tell you how many times SpinRite saved flaky, error-ridden disks, be they floppies or hard drives."



LEO:  Floppies?  What are those?



STEVE:  Yeah.  So I replied...



LEO:  Does SpinRite work on floppies?



STEVE:  Oh, yeah.



LEO:  I never even thought of that.



STEVE:  Yeah, it's super useful for floppies because they have no error correction built in, and so they really do need it.  So, yeah.  In fact, people often will have something on a floppy that they, like after a decade it's like, oh, my god, that's my only copy of this.  And it's like, yeah, SpinRite will bring it back to life.



LEO:  Cool.



STEVE:  So anyway, I replied to Madman.  He posted as Madman in MN, so Minnesota.  So I said to him, my next post here at https://blog.grc.com will be about my roadmap for SpinRite, since people who don't listen to the Security Now! podcast will not have heard about my plans.  And having them documented will be useful in any event.  So, one, thank god we no longer have any physical shipment of SpinRite, only the download.  That makes the lives of my little three-employee company, counting myself, so much more sane.



And to his point two I said we still maintain a database of every copy of SpinRite ever purchased going back 30-plus years now.  We have pre- and post-online databases, and the pre-online database is written in FoxPro, a dBase II clone.  And literally she has FoxPro running in a DOS box.  I said, if you will write to Sue at our sales email, which is always sales and then the current year and then @grc.com, she'll be glad to look you up and verify your status.



And I said, since SpinRite v6 has been in use for the past 15 years - v6 - we are giving serious consideration to terminating upgrades from earlier versions once v6.1 is formally released, under the thinking that, since v6.1 is going to be so much faster and more capable than v6.0, and we're going to be giving it away to all v6.0 owners going back 15 years, that should be a sufficient commitment to our previous customers.  And that anyone who's still interested in SpinRite at all will have upgraded to v6 sometime in the past 15 years, so they'll be covered.



And I finished, saying:  "I thank you very much for your interest and support.  All of those other things you mentioned that I have done, I've been able to give away because people purchased SpinRite."



LEO:  Bravo.



STEVE:  So DNSpionage.  I can't do a better job of summarizing this than Cisco did, so I'm just going to share the beginning of a very long post.  There's no need to get into the weeds.



So their title was "DNS Hijacking Abuses Trust in Core Internet Service."  They wrote:  "This blog post discusses the technical details of a state-sponsored attack manipulating DNS systems.  While this incident is limited to targeting primarily national security organizations in the Middle East and North Africa, and we do not want to overstate the consequences of this specific campaign, we are concerned that the success of this operation will lead to actors more broadly attacking the global DNS system."



They write:  "DNS is a foundational technology supporting the Internet.  Manipulating that system has the potential to undermine the trust users have on the Internet.  That trust and the stability of the DNS system as a whole drives the global economy.  Responsible nations should avoid targeting this system, work together" - that is, you know, that is they believe this is a nation-state.  So they're saying responsible nations should avoid targeting this system for attack, work together "to establish an accepted global norm that this system and the organizations that control it are off-limits, and cooperate in pursuing those actors who act irresponsibly by targeting this system."



Then they said, under executive summary:  "Cisco Talos has discovered a new cyberthreat campaign that we are calling 'Sea Turtle,' which is targeting public and private entities, including national security organizations, located primarily in the Middle East and North Africa.  The ongoing operation likely began as early as January 2017" - so more than two years on now - "and has continued through the first quarter of 2019.  Our investigation revealed that at least 40 different organizations across 13 different countries were compromised during this campaign.  We assess with high confidence that this activity is being carried out by an advanced state-sponsored actor that seeks to obtain persistent access to sensitive networks and systems.



"The actors behind this campaign have focused on using DNS hijacking as a mechanism for achieving their ultimate objectives.  DNS hijacking occurs when the actor can illicitly modify DNS name records to point users to actor-controlled servers.  The Department of Homeland Security (DHS) issued an alert about this activity on January 24, 2019, warning that an attacker could redirect user traffic and obtain valid encryption certificates for an organization's domain names.



"In the Sea Turtle campaign, Talos was able to identify two distinct groups of victims.  The first group we identify as primary victims includes national security organizations, ministries of foreign affairs, and prominent energy organizations.  The threat actor targeted third-party entities that provide services to these primary entities to obtain access."  In other words, the threat actors went after, for example, the DNS registrars for the companies they wanted to target.



"Targets that fall into the secondary victim category include numerous DNS registrars, telecom companies, and Internet Service Providers.  One of the most notable aspects of this campaign was how they were able to perform DNS hijacking of their primary victims by first targeting these third-party entities."  So it's an indirect attack in that sense.



"We assess with high confidence that these operations are distinctly different and independent from the operations performed by DNSpionage, which we reported on in November of 2018.  The Sea Turtle campaign almost certainly poses a more severe threat than DNSpionage, given the actors' methodology in targeting various DNS registrars and registries.  The level of access we presume necessary to engage in DNS hijacking successfully indicates an ongoing high degree of threat to organizations in the targeted regions.  Due to the effectiveness of this approach, we encourage all organizations globally to ensure they have taken steps to minimize the possibility of malicious actors duplicating this attack methodology.



"The threat actors behind the Sea Turtle campaign show clear signs of being highly capable and brazen in their endeavors.  The actors are responsible for the first publicly confirmed case against an organization that manages a root server zone, highlighting the attacker's sophistication.  Notably, the threat actors have continued their attacks, despite public reports documenting various aspects of their activity, suggesting they are unusually brazen and may be difficult to deter going forward.  In most cases, threat actors typically stop or slow down their activities once their campaigns are publicly revealed.



"This post provides the technical findings you would typically see in a Talos blog.  We will also offer some commentary on the threat actors' tradecraft, including possible explanations about the actors' attack methodology and thought process.  Finally, we'll share the indications of compromise (IOCs) that we have observed thus far, although we are confident there are more that we have not seen."



So that's sort of the sum of it.  I then grabbed a few things out of the rest.  Under assessing Sea Turtle DNS hijacking methodology, they said:  "It's important to remember that the DNS hijacking is merely a means for the attackers to achieve their primary objective.  Based on observed behaviors, we believe the actor ultimately intended to steal credentials to gain access to networks and systems of interest.  To achieve that goal, the actors behind Sea Turtle, one, established a means to control the DNS records of the target; two, modified DNS records to point legitimate users of the target to actor-controlled servers; then, three, captured legitimate user credentials when users interacted with these actor-controlled servers."  In other words, a classic man-in-the-middle attack.



They said, under initial access:  "The threat actors behind the Sea Turtle campaign gained initial access either by exploiting known vulnerabilities or by sending spear-phishing emails.  Talos believes that the threat actors have exploited multiple known CVEs to gain either initial access or to move laterally within an affected organization.  Based on our research, we know the actor utilizes the following known exploits."  So believe it or not, from 2009, so 10 years ago, a PHP code injection vulnerability affecting phpMyAdmin.  From 2014, so five years ago, a remote code execution affecting the GNU bash system, and we talked about it at the time.  That was that SMTP; that's the Shellshock exploit.  Still there are systems that have not been fixed, that are five years later still being used.



Two years ago, a remote code execution by an authenticated user with elevated privileges against a Cisco switch.  Two years ago, a remote code execution for Cisco's integrated service router 2811.  Two years ago, a remote code execution affecting Apache web servers running Tomcat, still effective.  A directory traversal allowing unauthorized access to Cisco's adaptive security appliances and firewalls.  And, finally, from last year, Drupalgeddon, a remote code execution for websites built with Drupal that have still yet not been patched.



So those are the way people get in or move laterally.  And then they said, under credential harvesting, which is the actual goal, man-in-the middle servers:  "Once the threat actors accessed a domain's DNS records, the next step was to set up a man-in-the-middle framework on an actor-controlled server."  They built man-in-the-middle servers that impersonated legitimate services to capture users' credentials.  Once these credentials were captured, the user would then be passed to the legitimate service.  To evade detection - and remember, you can't have a man-in-the-middle server with HTTPS unless that man-in-the-middle server is serving a valid TLS certificate.



So, they wrote:  "To evade detection, the actors performed certificate impersonation, a technique in which the attacker obtained a certificate authority signed X.509 certificate from another provider for the same domain, imitating the one already used by the targeted organization.  For example" - this is from Cisco.  They said:  "If a DigiCert certificate protected a website, the threat actors would obtain a certificate for the same domain but from another provider, such as Let's Encrypt or Comodo.  This tactic would make detecting man-in-the-middle attack more difficult, as a user's web browser would still display the expected SSL padlock" - or the equivalent - "in the URL bar.



"When the victim entered their password into the attacker's spoofed web page, the actor would capture these credentials for future use.  The only indication a victim received was a brief lag between when the user entered their information and when they obtained access to the service.  This would leave almost no evidence for network defenders to discover, as legitimate network credentials were used to access the accounts."  So a perfect man-in-the-middle attack that is breaching DNS, obtaining a certificate from one of these instant cert providers now, and then using that in order to prevent any indication from users' browsers from being triggered.



And, finally, they finish:  "How is this tradecraft different?  The threat actors behind the Sea Turtle campaign have proven to be highly capable, as they have been able to perform operations for over two years and have been undeterred by public reports documenting various aspects of their activity.  This cyberthreat campaign represents the first known case of a domain name registry organization that was compromised for cyberespionage operations.  In order to distinguish this activity from previous reporting on other attacks, such as those affiliated with DNSpionage, below is a list of traits that are unique to the threat actors behind the Sea Turtle campaign.



"One, these actors perform DNS hijacking through the use of actor-controlled name servers.  Two, these actors have been more aggressive in their pursuit, targeting DNS registries and a number of registrars, including those that managed country-level TLDs.  These actors use Let's Encrypt, Comodo, Sectigo, and self-signed certificates in their man-in-the-middle servers to gain the initial round of credentials.  And once they have access to the network, they steal the organization's legitimate SSL certificate and use it on actor-controlled servers."  Did you hear that?  Once they get in, they steal the organization's legitimate SSL certificate and use it then on actor-controlled servers.  It's diabolical.



LEO:  Mm-hmm.



STEVE:  So, yeah.  This is a reason we need to get DNS secured, because DNSSEC would prevent this kind of attack, yet we don't have it yet.



LEO:  Well, there you go.



STEVE:  DNSpionage.



LEO:  DNSpionage.  My friends, we have come to the conclusion of this fine episode of Security Now!, Episode 711.  But we are not at the end of the conversation.



STEVE:  Oh, no.



LEO:  No, no, my friends.  It's an ongoing process.  If you want to participate during the live version of the show, because we do it about 1:30 p.m. Pacific, 4:30 Eastern, 20:30 UTC on Tuesdays, you can go to our website, TWiT.tv/live.  We stream audio and video.  You can listen or watch there.  You can also join us in the chatroom at irc.twit.tv, where it's always going on, even when Steve's not.



You can also - let's see, what else? - get versions of the show, if you want to listen after the fact on demand, from Steve's site, GRC.com, the Gibson Research Corporation.  He's got nice audio files, but also transcripts to make it really easy to follow along as you listen to the show.  He pays for those, and I appreciate him doing that because it really is a great value to listeners.  Just go to GRC.com.



While you're there, pick up a copy of SpinRite, world's finest hard drive maintenance and recovery utility, even for floppies.  You can also get lots of other stuff, as you heard, including info on SQRL, ShieldsUP!.  There's so much stuff there, GRC.com.  Leave questions there for Steve at GRC.com/feedback, or tweet him.  He accepts direct messages at @SGgrc.  And we have audio and video of the show, if you want to watch, at our site, TWiT.tv/sn.  Or you can subscribe to your audio or video version, the version of your choice, and you'll get it every week, the minute it's available.  And every podcatcher has Security Now!, 15 years in the making.



STEVE:  Oh, and also, now that my blog is relaunched, go over to blog.grc.com and subscribe, sign up so that you get news of stuff.  I'm going to make it more active than it has been in the past.



LEO:  Nice.



STEVE:  It was a very sleepy place until now.



LEO:  Yeah.  Well, good.  Blog.grc.com.  Steve, have a great week, and we'll see you next time on Security Now!.



STEVE:  Thank you, my friend. 



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#712

DATE:		April 30, 2019

TITLE:		Credential Stuffing Attacks

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-712.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we look at more privacy fallout from our recent coverage of Facebook and Google.  We examine the uptake rate of recent Windows 10 feature releases.  We finally know the source of the AV troubles with the April Patch Tuesday updates.  We look at the NIST's formal fuzzing development, consider the source of a massive and ongoing database data leak involving more than half of all American households, note that Windows Insiders are already finding that their systems won't update to the May 2019 feature update, and address the concerns of United Airlines passengers who have noticed and been understandably upset by seatback cameras pointing at them.  Finally, we have the "Cranky Old Guy Tip of the Week," touch on a bit of miscellany, then take a look at what many in the security industry are watching with concern:  the large and emerging threat of website credential stuffing attacks.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here with lots to talk about, including the congressional questions for Google about Sensorvault, the changing velocity of Windows updates, and NIST goes a-fuzzing.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 712, recorded Tuesday, April 30th, 2019:  Credential Stuffing Attacks.



It's time for Security Now!, time to protect yourself online with the Securer in Chief, Mr. Steven Gibson of the Gibson Research Corporation.  Hi, Steve.



STEVE GIBSON:  And Leo, the microphone is back on the side where it's supposed to be.  Actually it's our inclement weather today has got - the house painters have taken a day off because it looks like it's going to, well, actually it has been raining overnight the last two nights down here.  So I don't have to have the microphone over on the other side to keep us from hearing that.  The weekly garbage dump truck pickup is over.  So, boy, we're just raring to go here with Episode #712 for the final day - my god, where is this year going?  It's the end of April already.



LEO:  Wow, yeah.



STEVE:  I remember it was, like, no time ago it seemed like we were talking about April Fools and how nice it was that the industry had avoided any April Fools stuff.



I want to talk about something that I've referred to only a couple times before.  But a report, an extensive report on the economic power of credential stuffing attacks is what named today's podcast "Credential Stuffing Attacks."  It turns out that another thing we've never really talked about in detail is this notion of this dark web.  And I know Brian Krebs spends a lot of time skulking around in there, I'm sure anonymously.  But sort of like the Silk Road operation years ago that got a lot of press attention, how there actually is a dark web where there is commerce going on, the exchange of value, illicit products.  And, for example, famously, large numbers of presumably still valid credit cards and the required information in order to fraudulently use a credit card has long been one of the commodities being traded there.



Well, it turns out that, as a consequence of the growth of an automated marketplace, there are now like the equivalent of eBay and Amazon, where there's no human interaction necessary.  There's, like, automated stores set up in this dark web.  And that has made practical, along with all of these database losses of LinkedIn and Twitter and endless companies who have lost control of their databases, and now the proliferation of botnets whose agents can serve as proxies so that username and password guessing can be distributed.  If one person is sitting on a website trying to guess a username and password, they'll get locked out, if the security on the site is any good, after X number of attempts.  But if those guesses are coming in from hundreds of thousands of individual bots, then there's no practical way to lock them out.  It looks like a large number of users.  And you can't block valid users from maybe making a mistake.  So it's a mess.



Anyway, that's how we're going to wrap up, but there's lots of other stuff to talk about.  We want to look at more privacy fallout from our recent coverage of the problems at Facebook and Google, specifically the vault that we talked about last week.  Congress stood up and took notice, or sat up and took notice.  We're going to examine the uptake rate of recent Windows 10 feature releases, which has a really cool graph that we'll get to later.  We now finally know the source of all those AV troubles that the Patch Tuesday April 9th of this month had, like exactly which one of those updates caused everybody trouble.



We're going to look at NIST, the U.S. organization; formal fuzzing development; consider the source of a massive and ongoing database data leak involving more than half of all American households.  There's an open database which has not yet been attributed to any owner which has got a ton of information on it.  And actually, the people who found it have put out a call to help them identify the owner.  So we'll share that with our listeners.  And maybe one of our listeners has an idea who might be behind this.



We also note that Windows Insiders are already finding that their systems won't update to the next major feature update.  That's this forthcoming May 2019 feature update.  You're not going to believe why.  And we address the concerns of United Airlines passengers, who noticed and have been understandably upset by seatback cameras pointing at them.  Finally, we have, from me, the Cranky Old Guy Tip of the Week.  We touch on a bit of miscellany.  And then, as I said, we're going to take a look at what many in the security industry are watching with some concern, which is this large and emerging threat of website credential stuffing attacks.  So I think another great podcast for our listeners.



LEO:  Excellent, excellent.  Steve?



STEVE:  So that would be V-E-E-P-E-E-N-N-N-N-N.



LEO:  No, VPN is spelled V-P-N.  Yeah, you're right.  Somebody's going to spell it no matter what.



STEVE:  Oh, yeah.  So our Picture of the Week relates to one of the articles we're going to get to about midway through.  The chart that we'll see later shows the time varying effect of Windows 10 feature updates over time.  This one is a pie chart showing, as of this month, right now, what the distribution is.  And it's probably, I mean, because the - well, I should first say that basically two thirds of the Windows 10 systems are still running the October 2018, that is, the 1803 release, have not moved...



LEO:  It's actually even worse.  It's Spring 2018.



STEVE:  Oh, that's right.



LEO:  It's 1803, yeah.



STEVE:  As opposed to 1809.



LEO:  Yeah.



STEVE:  Yes.  Right, right, right, Spring of 2018, 1803 - have not yet moved to the problematical, which is the most recent, October 1809.  And of course we've been talking extensively about what a fumble that was.  And the reason I'm not sure this is really at this point very representative is it has only relatively recently been made more broadly available.  And what's interesting, and we'll be discussing this a little bit later, is that it's not exactly clear why, since it is now available, and Windows 10 systems will be updating people, why it's stuck at about a third.



And what's interesting is that we're, what, a few weeks away from its replacement.  So there's been some conjecture in the industry that many people, and maybe enterprises, may just be thinking, you know, let's just skip 1809 altogether and wait to see how the next one fares.  So it'll be interesting to see what - and what is that, 1904, 19 oh something.  I thought it was 1903, but...



LEO:  Well, it's been 19H1 for the longest time, but now I think it's 1903 or maybe 1904, yeah.



STEVE:  Yeah, yeah.  Okay.  So we'll be getting to that in a minute.  As we know, we talked about it last week, Facebook was hit with the heavy fine from Russia of $46.53.  Wow.



LEO:  That's like one nanosecond interest on their cash.



STEVE:  Yeah.  However, the news came as a result of last Wednesday's quarterly report to the shareholders, that they're expecting that they may be hit with a little bit more substantial fine from the U.S. Federal Trade Commission, maybe as much as $5 billion, related not even to the most recent problems, but several years back to the whole Cambridge Analytica data disclosure.  So what appears to be happening is that, as has to be done when you're a publicly traded company, you generally want to paint a very sober picture of your publicly traded company's expectations so that you're not hit with a bunch of shareholder lawsuits.  I mean, that happens anyway, but you certainly want to minimize it and establish the best position you could.



In their written release - which corresponds to this set-aside of $5 billion, which they said was "in connection with the FTC's investigation of its user data practices."  In their written release they said:  "In the first quarter of 2019" - I love the wording of this - "we reasonably estimated a probable loss and recorded an accrual of $3 billion in connection with the inquiry of the FTC into our platform and user data practices, which accrual is included in accrued expenses and other current liabilities on our condensed consolidated balance sheet.  We estimate that the range of loss in this matter is $3 billion to $5 billion. The matter remains unresolved, and there can be no assurance as to the timing or the terms of any final outcome."



So anyway, as we know, a year ago, last March in 2018, the FTC announced that it was launching an investigation into Facebook's data privacy practices, which were triggered by the Cambridge Analytica revelations.  The FTC is specifically investigating whether Facebook has violated a consent decree - I mean, so this is the problem is in 2011 Facebook agreed that they essentially would not do this.  And so now the question is, having made that agreement eight years ago, did they violate that?  And the consent decree that Facebook agreed to said that they would agree to receive explicit permission from users before sharing their data with third parties.



And then Mark Zuckerberg, in a conference call last Wednesday with the shareholders, said that he plans to forge ahead with what he calls, and we talked about this also last week when I said they ought to just, you know, scrap it, I mean, it's just a disaster.  They're discovering logs?  Like, oh, look.  I mean, if we're to believe this, there's no way to paint this so that it seems good, they're discovering logs of millions of additional usernames and passwords that they didn't know they had.  Anyway, he said he wants to create a "privacy-focused social network."  He did not refer to the FTC fine or the company's even more recent data handling problems.  However, the company's CFO, I guess you pronounce his name Wehner, W-E-H-N-E-R?  He commented that:  "We anticipate ad targeting-related headwinds," Leo.



LEO:  [Makes wind sounds]



STEVE:  Isn't that a shame?  They're having to tack against the headwinds.  "We anticipate ad targeting-related headwinds will be more pronounced in the second half of 2019."  Wow.  Ad targeting-related headwinds.  Anyway, he noted that he expects new regulations to affect the company's business going forward.  Uh-huh, meaning they may not be able to play as fast and loose as they have before.  So anyway, I just wanted to note that the world is paying attention to these things, and the U.S. government is taking this a little more seriously than Russia has so far.



And also last week we talked about Sensorvault, which is the geofencing concept where an article, I think it was The New York Times' revelation that Google was honoring subpoenas for geofenced data turning over anonymized exact location records for some period of time to law enforcement, who would then study them based on the facts of the case that they were trying to solve; return to Google with a much narrowed list of devices by anonymized ID and then get expanded coverage; look at those, whittle it down further, and then finally get the actual identities of the people.



Well, this generated all kinds of concern and resulted in a letter written by representatives in Congress, the Committee on Energy and Commerce that oversees this kind of stuff.  I'm going to just share this with our listeners because it was addressed to Mr. Sundar Pichai, CEO of Google:  "Dear Mr. Pichai:  We are writing in response to concerning reports about a massive database of precise location information on hundreds of millions of consumers known inside Google as 'Sensorvault.'  According to recent reports, Google tracks and stores precise location information on a huge volume of consumers, including practically every consumer with an Android mobile device, in some cases storing information dating back to 2009.  The potential ramifications for consumer privacy are far reaching and concerning when examining the purposes for the Sensorvault database and how precise location information could be shared with third parties.



"First, according to the reports, Google collects precise location information in numerous ways, including from the location history function on Android Phones, Google searches, and Google apps that have location enabled.  Second, precise location information is reportedly collected even when people are not making calls or using apps, which enabled Google to track the 'whole pattern of life' of an individual.  Finally, Google reportedly never destroys any of the precise location information it captures in the Sensorvault database and has therefore compiled an extraordinarily detailed picture of the movements and whereabouts of a vast number of consumers stretching back more than a decade.



"As part of the Committee's ongoing commitment to protect the privacy of the American people and to understand the benefits and risks of various data collection and use practices, we would like to know the purposes for which Google maintains the Sensorvault database and the extent to which Google shares precise location information from this database with third parties.  To that end, please provide answers to the following 10 questions by May 7, 2019."  So what's that, like next Monday or something.



"1.  What information does Google store in the Sensorvault database, and for what purposes does Google use this information?  Please describe each use in detail.  If the types of information in the database or the purposes for which such information is used have changed over time, explain any such changes in chronological order.



"2.  Please describe which affiliates and subsidiaries of Alphabet have access to or use the data or analytics derived from the data in the Sensorvault database.



"3.  Does Google maintain other databases of precise location information?  If so, how does the Sensorvault database differ from other such databases, and how is the data from such database used?



"4.  Who is able to access the information in the Sensorvault database?  Include in your response both the number of Google employees with access to the Sensorvault database and the roles and responsibilities of any persons with access.



"5.  What are the sources from which Google collects the information maintained in the Sensorvault database and any other database identified in response to question 3?  Specifically, describe any Google services, mobile applications, devices, or any other means through which Google obtains the information."  Then we have two parts of 5, "a" and "b."  "If consumers are required to opt-in to the collection of precise location information, describe with specificity how consumers opt-in to the service, and the notice provided to such consumers about the purposes for which Google collects the information.  If any such opt-in has changed over time, describe those changes."  And, b, "If consumers may opt out of the collection of precise location information, describe with specificity how consumers opt out and the notice provided to such consumers about the purposes for which Google collects the information.  If any opt-out has changed over time, describe those changes.



"6.  To the extent that a consumer has requested that precise location data not be shared with Google, through opt-outs or other mechanisms, do Android phones continue to collect precise location data on the device or store location information on the device?  If so, to the extent that the consumer subsequently allows location data to then be shared with Google, is that formerly stored precise location information transmitted to Google?  Under what other circumstances would the device continue to transmit precise location information to Google when a consumer has requested that precise location information not be shared with Google?



"7.  How accurate is the precise location information stored in the Sensorvault database?  Include in your response both the accuracy of the precise location information and the accuracy of attributing such information to a single individual.



"8.  What controls, if any, does Google provide to consumers to limit or revoke Google's access to the information stored in the Sensorvault database? Does Google provide consumers a means to delete data stored in Sensorvault?  If so, describe in detail how consumers may do so.



"9.  What is Google's retention policy with respect to precise location information stored in the Sensorvault database?  To what purposes does Google maintain precise location information going back to 2009?"



And, finally:  "10.  Does Google share, sell, license, or otherwise disclose precise location information, including de-identified data, from the Sensorvault database with any third parties other than law enforcement?  If so, identify the types of businesses receiving such information and the purpose for disclosing any such information.  If the data is de-identified, provide a description of how it is de-identified."



And they said, finally, wrapping up this letter:  "In addition to providing the written responses requested, please make arrangements to provide Committee staff with a briefing on these topics to occur no later than May 10, 2019.  Thank you for your attention.  If you have any questions, please contact" and then blah blah blah, a couple phone numbers, and signed by four people on the Committee.



So it looks like Google is going to be explaining to Congress exactly what this is, why they have it, how they use it, and how that's changed over time, and who they make it available to, and under what terms and conditions.  So I think we're going to be finding out much more about this in the near term, and that's probably all for the good because this is apparently - I didn't realize it went back 10 years.  But it's every movement that Android users and now - and they didn't mention it, but we know that Google apps on iOS are doing the same thing.  So it's not just Android devices.  And as you said, Leo, it's a little distressing that Google has this and is making it available.



Okay.  So the chart that I have, and it's worth displaying it on the next page, Leo, it's the bottom of page 4 of the show notes.  This is an interesting chart which it took me a while to visually parse what I was looking at there.  What's interesting is that each of those waves is the onset at the bottom, and then the slope is the rate of adoption of successive feature updates to Windows 10.  And what's significant is that, for example, what was the Fall 2018, which was the one that now has about two-thirds adoption, it was almost a straight line upwards.  I mean, everybody just jumped on it and said yahoo, and up they climbed up that curve.  Compared to the curve on the far right of this chart, which demonstrates a much different adoption rate.  There's a tiny little upwards tick.  Then it stops.



Well, we know why, because Microsoft realized, oh, crap, and they just shut it down completely.  And so it goes horizontal with no additional adoption.  You remember they took it back, essentially; no additional adoption for a couple months.  And then, as they began to make it available, we see it beginning to sort of creep up again, not vertical by any means, at a relatively slow pace.  And it actually looks like it's leveling off there toward the end of the chart.  It's like it slowed down again.



So it turns out that there's a Lithuanian-based company called AdDuplex which calls itself the largest cross-promotion network for Windows Store apps and games which empowers developers and publishers to promote their apps for free by helping each other.  So sort of a third-party add-on tool.  They have an SDK that is in about 5,000 Windows Store apps which gives them coverage of sort of a semi-random sampling of more than 100,000 Windows 10 machines.  I looked for any other, after seeing this, I looked for any other similar representation, as they have with this chart, and couldn't find any.  What I did find was that they are also updating this every month in order to create this tracking database.



So it's going to be interesting, I mean, this is sort of what we would expect to see on this most recent October 2018 feature update which has been so painful.  We don't know, I mean, although - well, for example, all of my non-LTSC machines, that is, that are not on the Long Term Service Channel that I've mentioned recently which is so nice, except that it is a problem because, I mean, I've run across a problem which is that the Windows Store app is not in the LTSC, and I would like to have Edge, which requires the Windows Store app in order to install it.  Probably I could use the pre-release version of Edge, but I haven't gone that far.  And certainly I could use Chrome or Firefox.  So maybe somebody who wants to use the LTSC would just use Chrome or Firefox and not have Edge.  I need to have it as a developer.  So, I mean, in that sense the LTSC is almost a little too lean.  But it is really beautifully feature stripped.



But my point was that I have a number of non-LTSC standard Windows 10 builds, and they're all at 1809 across the board.  So whatever it is about my systems, they've immediately jumped to this latest one.  Which sort of begs the question, why are we not seeing, now that Microsoft has made it available, why are we not seeing its adoption take off?  And in doing some research out in the industry, I saw some speculation that users are gun shy now of 1809, with it having had such a rough start.  Except, okay, admittedly our listeners, and certainly listeners of Windows Weekly and other users of Windows 10 who are very much in the loop, they would be aware of this.  But most Windows 10 users are not going to be.  They're just wanting whatever Microsoft says they need, and they just click "okay," and off they go.



So I think it is probably to what degree enterprise has had Windows 10 forced down their throat.  Enterprise is probably saying, whoa.  And there has been speculation that I saw suggesting that maybe 1809 is just going to get skipped over completely, that there's just not going to be any adoption of it because we're looking at the next one being available so soon.  We'll just wait and see how the next one takes off.  So anyway, just sort of some interesting dynamics of the shape of these curves.  Previously they were all very rapid.  But this one has given everybody second thoughts.



And speaking of trouble being caused by Windows Update, BleepingComputer's Lawrence Abrams did some creative sleuthing, and he appears to be the first person to figure out and to clearly disclose exactly which of the April 2019, that is, this last month's updates, was responsible for so many of the enterprise AV systems having trouble.  As we know, in eight separate support articles for the April Windows Update, Microsoft explained that, if users have enterprise antivirus software from Avast, Avira, McAfee, or Sophos installed - and by the way, McAfee was recently added to that list - Windows may become, as in Microsoft's words, "unresponsive upon restart after installing."



Lawrence had observed that both McAfee and Avast had made passing references to CSRSS being related to the problems.  CSRSS, people who are Windows savvy have probably seen it sitting in their list of processes running.  It's the client-server runtime subsystem - that's what CSRSS stands for, Client-Server Runtime Sub-System - within Windows.  So Lawrence looked through the list of updates released on April 9th and noticed that there was a security update released.  It was CVE-2019-0735, which was a Windows CSRSS elevation-of-privilege vulnerability.  The security update stated that an elevation-of-privilege vulnerability exists when the Windows client-server runtime subsystem fails to properly handle objects in memory.  An attacker who successfully exploited this vulnerability could run arbitrary code.  An attacker could then install programs; view, change, or delete data; or create new accounts with full user rights.



Microsoft wrote:  "To exploit this vulnerability, an attacker would first have to log onto the system.  An attacker could then run a specially crafted application to take control of an affected system.  The update" - this update, Microsoft's - "addresses the vulnerability by correcting how the Windows CSRSS handles objects in memory."  So that appears to have been the culprit that caused those third-party AV tools so much trouble after the April 9th updates.  And the good news is that the vulnerability was privately disclosed to Microsoft and was not known to be actively exploited in the wild.



So I guess, had users known at the time that that one was the one causing the problems, that one could have been backed out until all of the AV guys caught up.  They all have now.  There were emergency updates that were pushed.  I remember I saw in the notice from Avast they said restart your system and don't even log in.  Just wait 15 minutes because Avast would come up, and then it would perform an emergency update of itself so that, if you gave it 15 minutes to get all that done, then you logged in, you would be okay.  And of course that was only the case after they had figured out what this update that Microsoft made had broken and then were able to engineer themselves around it.



So sort of a mess, but we're beginning to see, I mean, as we've talked about this often, in order to perform the deep sorts of checking that antivirus now has to perform, because Windows has not provided, because Microsoft has not provided the kind of official hooks into the kernel that the AV companies want, the AV companies are having to reverse engineer these pieces of Windows and install their own deep hooks into the software, which inherently makes them prone to this kind of problem.  Microsoft certainly has the right to change their code when they need to, especially when they're fixing an identified problem that could allow hackers to run code in the kernel.  So it's a mess, and there's really no good solution for it at this point.



The NIST, the U.S. National Institute of Standards and Technology, has gone fuzzing.  After more than, it turns out, 20 years of steady improvement, the U.S. NIST believes that it has reached an important milestone in the development of a technology it named "Combinatorial Coverage Measurement."  So Combinatorial Coverage Measurement is the official name for what hackers commonly call "fuzzing," which as we've discussed often is just throwing a bunch of stuff at something - a web app, ports that are open on a server, whatever - and seeing if you can crash it because, as we know, a crash is always the preamble to looking then in detail at what it was that you threw at what you threw it at, see exactly what happened, and then examine whether that can be leveraged into a much, by really understanding the nature of the crash, that's where these hacks begin.  You then turn that into an exploit, and off you go.



So this Combinatorial Coverage Measurement (CCM) is part of a larger research toolkit which the NIST calls "Automated Combinatorial Testing for Software" (ACTS).  So CCM is an algorithmic approach used to test software for interactions between input variables that might case unexpected failures.  In the NIST's coverage of this, they said:  "This is potentially useful for testing and hardening complex and important mission-critical systems such as aircraft, cars, and power plants where problems can be life-threatening.  Many complex systems operating in real-time must continually receive inputs from large arrays of sensors which might generate unexpected conflicts the software cannot resolve.  The systems' designers work to identify and counteract these problems by modeling as many interactions as they can before the software is used in the real world.  But what if something that could happen is missed?"  They say:  "This is where ACTS and CCM come in."



And of course what happens is, as the system grows, we've seen how this exponentiates, in the same way that adding each binary bit to a number doubles the number of possible states in a system.  In other words, the length of a binary number grows linearly, while the possible combinations grows exponentially.  So the problem becomes one of modeling enough interactions from enough variables to somehow spot all the possible combinations that might lead to an issue.  And the problem is, when the system gets sufficiently complex, while it would be nice to try them all, you just can't.  There are just too many possible combinations.



So over the course, they say, of two decades of work, their solution to this problem that they've been working on has been improving.  They said as the result of a collaboration with the University of Texas, Austria's SBA Research, which describes itself as the research center for information security in Austria, and Adobe, who is one of several large companies using the toolkit, NIST now thinks that the 2019 version of this CCM, the combinatorial solution, has made a significant leap forward.  With the help of a new algorithm developed by this Austrian SBA Research, NIST's tool has jumped from being able to model a few hundred variables to up to 2,000 from five-way combinations of inputs.  This means that it can now be practically applied to much more complex systems than previously.



So NIST is making this available to partners that it's working with in situations that call for, essentially, sort of more formalized academic rooted combinatorial testing, which we know as "fuzzing."  And in fact these systems really are far more complex.  Fuzzing isn't dealing with 2,000 inputs.  Fuzzing is dealing with a few inputs.  And that's enough to typically bring our systems down to their knees.  NIST is looking at, for example, all the sensors on an aircraft and verifying that, in every situation that they can find, they're able to root out a problem that could cause the system trouble before it ends up being life-threatening.  So it's nice to see that we're having this kind of research at the academic level, basically an outgrowth for what the hackers have been doing sort of on an ad hoc basis until now.



So an organization, vpnMentor, has reported an unknown data breach which is exposing 80, eight zero, million U.S. households.  It's hosted by a Microsoft cloud server, exposing 24GB of data.



LEO:  Holy cow.



STEVE:  I know.



LEO:  That is a ton of data.



STEVE:  24GB is exposed, which includes the number of people living in each household with their full names, their marital status, their income bracket, their age, and more.  It's got the full street address, cities, counties, states, and zip codes; the exact longitude and latitude; their full names (first, last, middle initial); age; date of birth; title; gender; marital status; income; homeowner status; and dwelling type.



LEO:  See, all that information is very interesting because you log into a dating site, they're not going to get that information.



STEVE:  Exactly.



LEO:  So what could it be?



STEVE:  Well, and this is the question.  And the reason this kind of hit me is, in scanning through interesting news of the past week for this podcast, I'm continually seeing articles about data breaches.  I mean, they're just kind of boring, normally.  We know that they're happening all the time.  It just gets to be another form of Internet background radiation.  It's like, yeah, yeah, yeah, another company has lost its data.  For example, I just thought, okay, like what?



And so there are three recent headlines that I skipped over:  medical information of almost 150,000 rehab patients exposed; 540 million Facebook records leaked by public Amazon S3 buckets, it's like, yeah, okay, fine; 257,000 legal documents leaked by unprotected Elasticsearch server.  And it's like, these are things I've encountered in the last few weeks.  And it's like, yeah.  I mean, what is there to say about it?  But here we've got more than half of U.S. households with, as you said, Leo, a really interesting set of specific data.  And normally it's possible to identify the owner of the data and then notify them that they've left their fly open.  But in this case there's no attribution to this data.  We know what it is.  We don't know whose it is.



So the research team at vpnMentor has been unable to figure out who all this data might belong to.  And they've essentially created an open call for help.  They said:  "The research team is currently undertaking a huge web mapping project."  And it's going to be sort of interesting to see what else they find.  "They use port scanning to examine known IP blocks.  This reveals," they wrote, "open holes in web systems which they then examine for weaknesses and data leaks.  Usually the researchers suspect where the leak is coming from.  They can then examine the database to confirm its identity."  Then they wrote:  "We then reach out to the database's owners to report the leak and, where possible, alert the people affected.  This helps build a safer and more protected Internet," yada yada.



"Although we investigated the database online, we didn't download it."  They wrote:  "Our researchers felt that downloading it would be an ethical breach as they would then illegally own personally identifiable datasets without people's consent."  On the other hand, it's flapping in the breeze.  But anyway, they said:  "This time it's different.  The database that the team discovered includes identifying information for more than 80 million households across the United States.  As most households include more than one resident, the database could directly impact hundreds of millions of individuals.



"vpnMentor is calling on the public to help identify the database and close the leak.  Unlike previous leaks we've discovered, this time we have no idea who this database belongs to.  It's hosted on a cloud server, which means the IP address associated with it is not necessarily connected to its owner.  The data includes uniform entries for more than 80 million households, making it almost impossible to narrow down.  The only clue we found lay in people's ages.  Despite searching thousands of entries," they write, "we could not find anyone listed under the age of 40."



LEO:  Oh, interesting.



STEVE:  Yeah.  They said:  "Interestingly, a value for people's income is given.  However, we don't know if it's a code for an internal ranking system, a tax bracket, or an actual amount.  This made us suspect that the database is owned by an insurance, healthcare, or mortgage company."  And mortgage is what I'm thinking, like something related to housing, because they talk about, what was it, they had a dwelling type and homeowner status.  And they said:  "However, information one may expect to find in a database owned by brokers or banks is missing.  For example, there are no policy or account numbers, Social Security numbers, or payment types."



So they said:  "Help us identify this database.  We want to contact the database's owners."  And I wonder why they just can't talk to Microsoft because Microsoft must know, be able to equate an IP in their cloud to its owner.  They said:  "We want to contact this database's owners and let them know that their data logs are exposing millions of households.  Help us solve this riddle.  What service is used by 80 million homes across the U.S., but only the U.S., and only by people over 40?  What service would collect your homeowner status and dwelling type but not your Social Security number?  And what service records that you're married, but not how many children you have?  If you can help us identify this database or know who owns it, please contact us at info@vpnmentor.com.  The 80 million families listed here deserve privacy, and we need your help to protect it."



And notice they have not indicated what the IP is or where it is.  They don't want to expose this because unscrupulous types will immediately suck out that 24GB and put it to malicious use.  So they've found it.  And if anybody has an idea, info@vpnmentor.com.  So an interesting call for - and this is the world we're in today, as I find myself often saying on this podcast.  It's like, wow.



And speaking of the forthcoming May 2019 Windows 10 feature update, Leo, you're not going to believe this one.  Microsoft is explaining to those on the Windows Insider program, who are trying to apply that May 2019 feature update ahead of its official release, as they are able to, that the reason for the "This PC can't be upgraded to Windows 10" is because it has a USB device or SD card attached.  You heard that correctly.  I'm not kidding.  Microsoft's exact text reads:  "If you are part of the Windows Insider Program, and you are trying to upgrade to the May 2019 Update for Windows 10 (Windows 10 version 1903), you may experience an upgrade block and receive the following message:  'This PC can't be upgraded to Windows 10.'  An external USB device or SD memory card that is attached to the computer could cause inappropriate drive reassignment on Windows 10-based computers during the installation of the May 2019 update.  For this reason, these computers are currently blocked from receiving the May 2019 Update.



"This generates the error message that is mentioned in the 'Symptoms' section if the upgrade is tried again on an affected computer."  And they said:  "Example:  An upgrade to the May 2019 Update is tried on a computer that has a thumb drive inserted into a USB port."  Yeah.  Mine does.  "Before the upgrade, the device would have been mounted in the system as drive G, based on the existing drive configuration.  However, after the upgrade, the device is reassigned a different drive letter.  For example, the drive is reassigned as drive H."



And then they said:  "Note:  The drive reassignment is not only limited to removable drives.  Internal hard drives can also be affected."  Whoops.  Anyway, this will presumably be fixed before the big rollout of next month's release.  Otherwise, the adoption of this release may not proceed any faster than the last one.  So this appears - Windows 10 just in general appears to be an increasing challenge for Microsoft.  And Leo, I've heard Paul and Mary Jo just sort of, I mean, at this point Paul has his head in his hands, saying, "What is going on up there?"



LEO:  Well, I mean, to be fair, I remember this is a problem on mobile devices, too, is removable USB storage.  It often confuses them.  In fact, Windows Phone for a long time wouldn't work with SD cards, micro SD cards.  Apple doesn't even give you the option.  And on Android they go back and forth about how to treat SD cards.  So it's not unreasonable to say, because we can't be sure that that device will be attached, you don't want to have it attached during installation.  You can plug it in afterwards and use it.  You just can't have it attached during installation because it's removable.  I don't know if that's - I'm sure they could fix it because we've never seen it before.  But I can understand why that might be an issue.  And as for drive letters, that's just a convention anyway, what the drive letter is.



STEVE:  Well, except that it has never been an issue before.  We've had all of the previous feature updates.



LEO:  Right, right.  It has been an issue in other operating systems, though.  It's not unusual to hear about problems with removable storage devices because, if you're going to use it, then you have to know that it's going to stay there.  So, I mean, admittedly, it's not a good thing, and they'll fix it.



STEVE:  Right.



LEO:  But I can understand how it could happen, I guess is what I'm saying.



STEVE:  Yeah.  So United Airlines made a little mistake.  They purchased seatback displays that had embedded cameras.  Passengers became quite upset at the idea of a camera staring at them throughout their flight.  And this is exactly where and when and, I mean, I hope you want to have a physical mechanical sliding door, a shutter to shut the camera, and have it on the seatback, and have it labeled with clear instructions, and maybe even train your maintenance staff that goes through between flights and puts all the seatbelts in the correct place to just get into the habit of closing all of those shutters.



Anyway, so what happened was it turns out that they purchased a bunch of these, didn't think twice about it, installed them in seatbacks, and there was an immediate hue and cry among their passengers, wondering why there was a camera staring at them, which did not provide any means for it to be closed.  They were quick to respond because of course United Airlines has had some PR problems lately with that passenger who was forcibly removed, and then unfortunately there was I guess a puppy that was forced to be stored in the overhead, and it did not survive the flight, unfortunately, or tragically.  So United Airlines has apologized for the confusion, said that these cameras were intended to be deployed in some future mode for teleconferencing.



But our takeaway is, and we've talked about this often, there is no good way to have a camera aimed at someone where you don't have a mechanical shutter.  It's nice to have a little light next to it that shows when it's on, but that's just not as good as a mechanical shutter.  And years ago we talked about cutting off the sticky part of a Post-it note and just putting it over the camera.  And in fact you often now see pictures of people's laptops that have a little chunk of a Post-it note over their camera because it's just simple and easy.  And there are, you know you can buy aftermarket low-profile shutters, if you are in the routine of actually using your camera, and you want to be able to open and close it because there have also been lots of examples of bad guys getting into people's computers and turning on their cameras.  We've talked about them years ago.



So this week I offer what I call my "Cranky Old Guy Tip of the Week."  This just happened this morning, when doing some web browsing to a site where I was pursuing something to flesh out the coverage of one of these things.  I just got tired of Firefox popping up that question about whether I wanted to allow this website to send me notices.  I cannot imagine a single instance where I want a website to have the privilege to put notices down in the notice area of my OS.  Just not a single instance where I want that.



So I thought, you know, I'll bet Firefox, I'm hoping Firefox has a way just to turn this off, this whole web notifications thing.  So I opened another tab, put in about:config, and then in the search bar I put in "notifications."  And sure enough, in fact, if you put in, and I would recommend for any of our listeners who feel similarly, if you type "webnotifications" as all one word, you get four hits.  The first one is dom, D-O-M, as in document object model, dom.webnotifications.enabled.  It defaults to true.  Double-click it, and flips to false.



And what's interesting is that I also went to - I did some googling because I was curious about the API.  And I chuckled because Mozilla's page about the web notifications API itself pops up a web notifications prompt.  So you can verify, I have the link in the show notes for anyone who's interested, you can verify that you have turned them off by refreshing, for example, you could go to this notifications API page first.  And sure enough, Firefox will pop up a little "Do you want to allow this website to send you notifications?"  And you've got Allow, Not Now, or then in a dropdown you can say Never.  I don't even want to be asked.  And so this turns off the ask.  And once that's been set to webnotifications.enabled to false, then sure enough, you're no longer asked, which is a great relief.



And Chrome makes it possible, too.  In the case of Chrome, under settings, go to Advanced, which extends the settings page to reveal a whole bunch more stuff.  And the first section there is Privacy and Security.  Under Privacy and Security, click on Site Settings, and then five entries down you'll find Notifications.  It's a little less clear how to do this there.  But the first option at the top, there's a switch that says "Ask before sending," and it says, "(recommended)."  Anyway, when you toggle that little slider switch to off, the text changes to blocked, and it has the same effect.



So if all of these, I mean, to me this feels like what is going to be one of the most abused things that we can imagine.  I mean, as bad as unsolicited pop-ups.  And we're seeing those more and more now, too.  But the good news is, for both of these two browsers, it's pretty simple to turn them off and just not even be asked anymore because I just can't imagine an instance where I would want to do that.



And a couple bits of miscellany for our listeners.  Leo, for the last year and a half I have been spending my time with a fabulous woman in a location that has essentially zero bars of Verizon.



LEO:  You must really like her.



STEVE:  Oh, my god.  We're both Verizon users.  And this place, there's an area known as Turtle Rock.  And when I mentioned it to the Verizon employees, they all go, oh, my god.  Apparently the associations up there, the housing associations refuse to permit cell towers.  You know, it's NIMBY.



LEO:  Not in my backyard, yup.



STEVE:  Exactly.  And as a consequence it just has this reputation, like, forever, that it's just like the cellular graveyard.  It's the cellular dead zone.  And what occurred to me is that, okay, 10 years ago maybe that was feasible because people were still heavily reliant on landlines.  But that's going away now, and the world is going to cellular.  And it's becoming much less practical.  Well, I knew this was a problem when we decided this is where we wanted to settle down.  And so I thought, okay, fine.  We've got Verizon WiFi; right?  We can do WiFi calling.



Well, I don't know what the problem is.  It works on my iPhone 6.  My iPhone 10 refuses to have it work, as does Lorrie's iPhone 8.  It just - I managed to get it on.  Then it went away.  And it's like it's always on on my iPhone 6, but not - and of course the idea is that, if you have that, then you're able to make your calls over your WiFi and your cable modem bandwidth or whatever.  It may be that the reception is so poor on these phones that it doesn't even know you're in a Verizon zone enough to hand off to WiFi.  Anyway, this is my long-winded introduction to the Samsung 4G...



LEO:  I was going to say femtocells, yup.



STEVE:  Oh, my lord, is it a win.



LEO:  Yeah.  We use a femtocell here.  We don't have - for some reason T-Mobile doesn't work here.  Yeah, it's great.  It uses your Internet.  It's like your own cell tower.



STEVE:  It is life-changing.  So for any of our listeners, I mean, it's like, why didn't I do this a year and a half ago?  Lorrie spends a lot of time there on the phone, and it's just been a constant headache for her.  And I'm feeling, like, negligent that I allowed this to go for 18 months without fixing it.  Because finally I said "enough" last Friday.  And so this is where I went to the Verizon store, purchased one of these things.  There's no service fee.



LEO:  Actually, it's always a good idea to say first, "I can't get your service in my house, so I'm quitting," because they will often give you one for free.  I mean, they're not hugely expensive, but still.  Because they don't want to lose you.



STEVE:  Interesting.



LEO:  We got our T-Mobile femtocell for free.



STEVE:  Interesting.  Didn't even - I'm too much of a...



LEO:  250 bucks, big deal.



STEVE:  It was, it was 250 bucks, one time.  And it's funny, too, because we are planning eventually to move somewhere more permanent, and Lorrie's been worried.  She says, "Well, what about..."



LEO:  Wait a minute, Steve.  What?  You and Lorrie together?



STEVE:  Well, we're getting along really well.



LEO:  Oh, my god.  You've not left your apartment in 23 years.  Wow.



STEVE:  So anyway, so she's worried, like this has been such a problem for her that it's like, what if this is a problem when we move?  So I've explained that we will absolutely never again have this problem because basically we get to take this with us wherever we go.



LEO:  Well, now, one little caveat.  Before you move anywhere, check to make sure they've got good high-speed Internet.



STEVE:  Yeah, yeah.



LEO:  Because you do have to have that to use this.  I wouldn't move anywhere that didn't have good high-speed Internet.  My god.



STEVE:  No, no.  We couldn't possible live without that.



LEO:  You couldn't do the show, Steve, more importantly.



STEVE:  No.  Could not, well, I'll probably still keep this place.  This is like my man cave, and then - because it's nice...



LEO:  I have a relief, yeah.



STEVE:  ...to go somewhere to make a mess.



LEO:  You need your man cave.



STEVE:  That's right.  Anyway, I just wanted to share this fabulous experience.  I was a little annoyed, for what it's worth, that you cannot do a whitelist.  I wanted to whitelist our phones because I didn't want to be giving absent Verizon 4G LTE coverage to my neighborhood.  It's like, they can get their own.  And I didn't want them using my bandwidth.  It turns out this is on purpose.  Basically I'm subsidizing...



LEO:  You're doing them a favor, yeah.



STEVE:  Yes, I'm subsidizing Verizon's lack of 4G LTE coverage throughout my neighborhood.  Well, actually it turns out...



LEO:  Turtle Rock, we now have a cell tower.  It's in Steve's house.



STEVE:  Turns out it's not a problem because, if I go out into the street, it's already, you know, it falls off.



LEO:  It's only 50 feet.  It doesn't go very far, yeah.



STEVE:  Yes.  So it is, it's 50-foot radius, 7,500 square feet.  So it beautifully, it's like all bars anywhere in the house now.  And Lorrie is just dancing.  So anyway, I wanted to share the fact that...



LEO:  Every cell company generally offers something like that.  Technically it's a femtocell because femto is tiny.



STEVE:  Yes.  Femto is part of the default password.  I was very impressed with the security, too.  You log in with a fixed-string prefix, and then the last four digits or the last four characters of the MAC address printed on the bottom.  So there's no default password.  You've got to have physical access to the device in order to know what the MAC address is, in order to log in.  And then of course, once you do that, then you change it to whatever you want.



LEO:  Oh, okay, yeah.  You can get somebody's MAC address if it's on the air; right?



STEVE:  Yup, yup.



LEO:  Yeah, femto is 10 to the negative 15th.  So it's a really small cell tower.  But they don't call them femtocells.  AT&T has another name.  Every company has another name.  But they all offer them because they don't want to lose you as a customer.  If you can't use it in your house, what good is it?



STEVE:  Well, it works.  I just wanted to tell everybody, this little thing has, like, changed our life.



LEO:  Nice.



STEVE:  So, yay.  Let's see.  From my blog on the 28th, which was last Sunday, Derek Hamilton posted, he said:  "Hi, Steve.  You've mentioned many times on Security Now! about the big release event you are going to do on SN.  Are we counting weeks, months or longer here?"  He says:  "I can't wait to watch.  Thanks."  And so I just thought I would take that opportunity to note that we are at Release Candidate 3 of the SQRL app for Windows.  And it's way ahead of the other clients just because it had a much longer head start.  But I'm like, as far as I know this is done.  And I want to play a little bit with the text.  But we're, like, there.  And then you and I, Leo, need to figure out when we can come up - and you'll get to meet Lorrie - and put together a little bit of a show of what is SQRL and how it works and so forth.



LEO:  A ketogenic dinner is on us.  Actually, no.  You eat pasta now.



STEVE:  I do.



LEO:  I'll eat the keto.  You eat the pasta.



STEVE:  I've been rescued from ketosis.  And speaking of SQRL, I wanted to also mention that last week's mess with the just-minted EV code signing certificate appears to be history.  Windows Defender was first to recognize that SQRL was not malicious and to start believing my newly minted EV code signing cert.  And then the Windows SmartScreen folks responded quickly to my notifying them that this was from and by a legitimate developer.  And so all those problems went by the wayside as quickly as I could ask.  So I was very impressed by that.



So, website credential stuffing attacks.



LEO:  Mmm, stuffing.



STEVE:  Leo.  I think you need to have your lunch, Leo.



LEO:  I haven't had lunch, you got it.



STEVE:  So credential stuffing attacks - it's sort of an odd term, but it's what the industry has adopted - are Internet-based mass username and password logon guessing at scale.  So this requires not only the technology to implement the attacks, which is now widely available, with so many hundreds of thousands of Internet-connected computers, routers, IoT devices, whatever, available to compromise.  But to be profitable these attacks also require the presence of a sufficiently mature marketplace into which it turns out a frighteningly large number of discovered working logon credentials can be resold.



So I know we've never really talked much about the so-called "dark web," but it's a real place.  This is, as I mentioned at the top of the show, this is where Silk Road was conducting its commerce.  And the point is that this is not a hacker trying to get into some person's account by guessing a known person's unknown password, or knowing something about them or their lives and so forth.  This is a shotgun scattershot numbers-based attack where the millions now we're at of paired usernames and passwords that have leaked out onto the Internet and have been sucked up into databases, those are being broadly used, blindly, against websites with the hope that they get in.  And if they get in, they log out because that particular individual, that is, this is an automated attack.  They have no interest in what's there.  What they want to do is they want to put that up for sale.



So the site is Recorded Future.  We've mentioned them from to time in the past.  They're a well-known security research group.  They titled this, I think very appropriately, "The Economy of Credential Stuffing Attacks" because it's the economics of this which it turns out has made this very profitable.  And unfortunately, as we have found, when it's possible to make money at something, you get more of it rather than - like once upon a time viruses were just sort of, as we've often talked back in the beginning of the podcast 14 years ago, it's like, okay, well, the good news is they're not really doing anything malicious.  They're just annoying.



Well, of course that changed.  As soon as you could have a way of transferring funds, like through bitcoin, then you started getting crypto malware because you could hold somebody ransom for the data on their computer.  And of course we know what has happened there.  This promises to be something similar.



So these guys begin by explaining.  They said:  "This report covers the current threat landscape of credential stuffing attacks.  It reviews the most popular tools used by cybercriminals to initiate credential stuffing, and describes some of the most popular marketplaces that sell compromised credentials.  This report contains information gathered using the Recorded Future Platform, as well as additional open source, dark web, and underground forum research; and will be of most interest to analysts protecting e-commerce, telecommunications, and financial organizations from credential stuffing attacks, as well as those looking for investigative leads on threat actors performing such attacks."



And then to set up their presentation they explain:  "The rapid proliferation of automated marketplaces" - and that's the key - "on the dark web, fueled by the widespread availability of support infrastructure such as account-checking software" - meaning brute forcing usernames and passwords - "email and password combo lists, and proxy service providers, has created the perfect attack landscape for the abuse of thousands of popular web services such as ecommerce, financial services, travel websites, and telecommunications companies."  They say:  "It's safe to assume that almost every large organization with an online retail presence has had their users exposed to credential stuffing attacks in the past few years, with some companies having upwards of millions of exposed login credentials available for purchase on the dark web at any given moment."



Then they have five main takeaways.  They said:  "The first widespread credential stuffing attacks were observed in late 2014, coinciding with the proliferation of automated underground marketplaces.  When selling accounts, attackers offered the quick and easy monetization of compromised account credentials.  Some actors who engaged in credential stuffing attacks remain active today."  So this has been - they've been present for five years.



Second:  "With an investment of as little as $550" - and we break this down in a minute - "criminals could expect to earn at least 20 times the profit on the sale of compromised login credentials."  And actually the return actually shows it turns out to be much greater return for investment.  They said:  "Third, the overall supply of compromised login credentials across several large marketplaces exceeds tens of millions of accounts."  I'll say that again.  The overall supply, that is, for purchase of working compromised login credentials, across several large marketplaces, exceeds tens of millions of accounts. 



They said:  "Fourth, we have identified at least six popular variants of account-checking software," that is, basically that's the software that the bad guys run where they feed in a database that they have purchased into the software which they have purchased, which operates against a network of proxies which they rent, in order to generate hits, successful login hits which then this software collects, which they then compile and turn around and put up for sale on the dark web.  And there are purchasers, not surprisingly, for this freshly farmed information.  So they said: "We have identified at least six popular variants of this software used by cybercriminals."  They said:  "However, dozens of lesser known variants can also be found on the dark web."



And, finally, fifth:  "While some companies may choose to implement multifactor authentication, which blocks the credential stuffing attack vector, organizations may not be prepared to choose security over convenience."  And probably the number one takeaway from this is that anything you do to block the feasibility of blind username and password login - oh, and CAPTCHAs don't solve the problem.  All of these software frontends have capture-defeating features.  It has to be something like multifactor authentication, something that removes the vulnerability from brute forcing.



So in their report they write:  "Around late 2014 and the beginning of 2015, we observed the widespread adoption of new dark web business models specifically tailored to facilitate a high volume of trades in a fully automated manner.  Designed to emulate legitimate retail platforms such as eBay and Amazon, these so-called 'automated shops,' as they're known, allow even low-level criminals to become vendors of stolen data, such as in this case compromised login credentials, without having to worry about maintaining their own infrastructure or marketing campaigns."



I mean, and so think about that.  That's like the eBay model.  If somebody's got some garage junk that they want to sell, if it weren't for eBay, how would you do that?  You'd have to put up a website somewhere and somehow tell people to go to your PayPal account.  And if that didn't exist, that would be a problem.  So eBay has solved that problem for anyone who wants to sell some random thing.  As they say, there's a buyer for everything.  So the equivalent now exists recently, as of about four years ago, on the so-called "dark web."



They said by and large the adoption of account marketplaces, that is to say, marketplaces for selling login account information, was made possible primarily by the proliferation of account-checking software, which is known by the slang "checkers," used as the main tool in credential stuffing attacks.  And then I'll note that we previously heard about the related marketplaces for stolen credit card information.  So what we have on the dark web is an underground, effectively retail, automated marketplace for buying and selling of stolen goods - in this case large quantities of recently tested and verified usernames and passwords for specific websites, the pairing of all of those together.



Anyway, they continue to say:  "Compromised account credentials were always a valuable commodity in the dark web.  The number of transactions was relatively small, and they were primarily conducted either on a peer-to-peer basis or via semi-automated markets such as AlphaBay, Silk Road, and Hansa Market.  In older models, buyers received their wares only after the seller manually approved the deal and delivered the purchased data.  Moreover, sellers had to maintain the listings and communicate with the buyers personally.  However, with the advent of automated shops, the need for manual engagement was eliminated, and the business of compromised accounts fully transitioned from peer-to-peer dealings to a much more democratized, open to everyone, marketplace.



"For a nominal 10 to 15% commission deducted from the amount of each sale, members can upload any number of validated compromised accounts which, in addition to email and password, often include data such as the accountholder's city or state of residency, transaction history, and/or account balance, all of this valuable data to fraudsters seeking to buy accounts tailored to meet their specific needs.  The vendors' main focus is replenishing the stock; while all customer support, remittances, and dispute resolutions are handled by the shop's support team.  



"At first, only a handful of select vendors became the primary suppliers of stolen data; but as the tradecraft was shared among members of the criminal underground, the business of stolen credentials has grown exponentially.  Since regular Internet users tend to reuse the same passwords across multiple websites, threat actors quickly learned that instead of attempting to obtain access to an individual account, which may take a very long time, they should instead focus on hacking multiple random accounts, that is to say across websites, thus reducing their efforts."  In other words, if they find one particular username and password that works at one site, quickly check that same pair at all the other sites that they are checking against on the off chance that the user has reused their password, as certainly used to be the case and, among uninformed, un-security-informed users, is still the ongoing practice.



Then they said:  "A combination of several elements made the hacking of various online service accounts not just effortless, but also incredibly lucrative."  And this reiterates what they said:  "To launch account brute forcing, also known as credential stuffing attacks, an attacker only needed" - and here are the requirements - "brute-forcing software, a database of email and password combinations, and access to a pool of proxies."



They said:  "Early versions of checkers [as they're called] were made to target a single company and were sold for between $50 to $250, depending upon the tool's capabilities.  These tools would attempt to log into a website using an email and password combination obtained from a random database often obtained on the dark web.  If a combination worked, it would be marked as valid.  If not, the software would simply pick another combination from the list and attempt to log in again.



"For valid logins, more expensive and complex checkers would also collect additional information from the compromised account, such as linking banking and payment card information, account balances, the owner's address, and even transaction history.  Until this day, the ingenuity of the method truly lies in the economy of scale, allowing criminals to process hundreds of thousands of combinations in a very short period of time.  Eventually, several dominant players such as Storm, Black Bullet, and Sentry MBA entered the market with more robust tools, supporting an unlimited number of custom plug-ins."



Naturally, as we've seen before, the software is only going to get better in time.  So what they're saying is that it used to be that you'd get like the PayPal cracker, or the Amazon cracker, or the Twitter cracker or something.  Now they are now multisite crackers that are able to span a wide number of ecommerce-related sites.  And I have a picture on the page below where I am, showing a bunch of the sites that are attacked and the value that a typical attacker can expect to get per site.



Anyway, they said they supported an unlimited number of custom plug-ins, also called "configs," which essentially offered hackers the capability to target almost any company with an online retail presence.  What had initially started as several hundred or several thousand compromised accounts quickly ballooned to hundreds of thousands, or even millions of accounts.  I mean, this feels like science fiction, but these guys have the numbers to demonstrate it.  And they've been poking around in the dark web, where this is all going on, and seen the size of the databases of compromised accounts that are on offer for sale.



They said:  "Some of the most prominent account shops have tens of millions of compromised accounts for sale at any given moment."  If this isn't enough to get people to change their passwords to random strings, I don't know what is.  "Although the competition quickly brought the average price of a single compromised account down from over $10 to a mere $1 to $2, the overall profitability of credential stuffing attacks increased significantly through sheer volume.  According to underground chatter observed over time, the average success rate for credential stuffing is anywhere between 1 to 3%.  Hence, for every one million random combinations of emails and passwords, attackers can potentially compromise between 10,000 and 30,000 accounts.  Moreover, the same database could then be reused over and over again to hack dozens of different websites, yielding even higher profits."



So this page, or this graph from their report on the next page shows that the so-called "checking software" would cost somebody wanted to set himself up as an attacker $150.  They would then purchase an email password database containing 100,000 records for another $150, and then would rent a proxy network, rent access to a proxy network through which to launch these attacks for $250 per week.  What this then generates is - assuming that you are attacking Amazon, PayPal, eBay, Expedia, Airbnb, FedEx, Credit Karma, Online Video Service, and Xfinity.  They sum up the average price available for each one of those different services' compromised accounts, and they vary.



For example, eBay you get $3.50.  For Amazon, you can sell an Amazon-compromised account for two bucks on the dark web.  Credit Karma brings two bucks.  Xfinity brings 3.50.  Airbnb $1.50.  Expedia only $0.50.  PayPal, you can sell a compromised PayPal account for a buck.  Anyway, they look at between 1 and 3% versus 100,000 records, show the amount of profit per.  And they end up with a gross profit of just shy of $20,000 for the accounts that you can generate from this attack using that investment.  So they're talking about they look at the gross profit margin.  And so for a direct cost of $550, your return on the dark web is $20,000.



They go on in their report to give the - they fully examine the capabilities and features of six specific tools which are being used to carry out these attacks.  I won't go into each of them in detail since anyone who's interested can follow the links that I have, both to their site and to their PDF of this report at the top of this coverage in the show notes.  My goal here has been to share and develop a real awareness of the existence of this very active underground market and, for bad guys, the compelling financial incentive that promises to keep it alive and growing for the foreseeable future.



And you can imagine now that what this means is that, when bad guys discover email and password databases, they're not going to immediately publish them.  They may not even immediately offer them for sale.  They may use these tools to exploit them themselves before - because the more that same database gets resold and reused, there will be a drop in the value as people realize that somebody got into their account.  They quote, "I've been hacked, my eBay account was hacked" or whatever, and then they'll change their password, thus rendering that entry in the database no longer effective.



But still, to me, it was fascinating that, as a consequence of the creation of an automated eBay-style marketplace on the dark web, the fact that we now have botnets which can serve as proxies so that these probes and attacks to log in are coming from hundreds of thousands of different IPs, not just one IP that it would be easy for a website to blacklist after 10 failed account login attempts.  You can't do that with hundreds of thousands of IPs because then you risk blacklisting legitimate users.  And of course it spreads out the guesses so that you're not going to be hitting from a single IP multiple login failures.  This is real.  And unfortunately, it's only going to grow over time because, as I said, it's not just anymore for the lark, it's for turning these things into money.  And here is now a way to turn a leaked database into cash on the dark web.  Wow.



LEO:  Yeah, it sounds like it's really almost hacking as a service.



STEVE:  Yeah.



LEO:  It's like you can really - everything you need.



STEVE:  You buy one of these, you buy one of these, you buy one of these, and you set yourself up, exactly.



LEO:  Yeah.  It's all cloud-based these days.



STEVE:  Yup.



LEO:  It's modern.  So really you wouldn't need much of a fancy system at home to do this.  



STEVE:  In your mom's basement, as they say.  Yup.



LEO:  It's the will and the knowledge, and the knowledge is out there.



STEVE:  Well, and willing to put yourself at risk because you need to protect your IP.  The feds are going to come looking for you, too.  So, I mean, it's not like it's free money that you're getting.  You're committing a crime when you're doing this.  So, yikes.  



LEO:  Yeah, although the feds in Bulgaria are maybe not as scary.



STEVE:  Yeah, well, and Russia, if Russia's hitting Facebook with a $48 fine, they're not that concerned, either.  



LEO:  They don't care, right, exactly.



STEVE:  And in fact some of these tools were translated from Russia.



LEO:  Russia, yeah, of course.



STEVE:  Because the market is there.



LEO:  They're perfecting it for us.



STEVE:  Wow, just amazing.



LEO:  Yeah.  It's a brisk market.



STEVE:  The world we're in today, Leo.



LEO:  It's amazing.  Steve, thank goodness we've got you to tell us all about it.  So we're all set.  We're all ready.  This is the Security Now! podcast in a nutshell.  Everything that's going on all around you in the darkness.  But Steve brings light to it all every Tuesday, around about 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  You can tune in TWiT.tv/live and watch or listen as we do it live.  If you do that, go to the chatroom, irc.twit.tv.  They're fascinated by you, Steve.  They're looking for pictures of Lorrie already.



STEVE:  Oh, no.



LEO:  Might want to use a pseudonym in future.  No, no, these guys are benign.  They're nice.  They're not the bad guys, they're the good guys in the chatroom.  Once we chop it all up and make it a podcast, Steve puts it up on his site at GRC.com, plus gets Elaine to write what he said in pure English.  That's an amazing feat in and of itself that she performs each and every week.  And you'll find that at GRC.com, as well.



When you're there, check out SpinRite, the world's best hard drive and recovery and maintenance utility.  That's Steve's bread and butter.  It's the only thing he charges for.  He has lots of other great stuff on the site.  So browse around.  You can leave him questions there, too:  GRC.com/feedback.  But he's also on Twitter and takes direct messages from anybody at @SGgrc.  But, you know, you've got a good class of people following you, so I think that's safe, as well.



STEVE:  Yeah, true.



LEO:  We have audio and video of the show at our website, TWiT.tv/sn.  But of course, as always, we encourage you to subscribe.  Whatever podcast application you use, you'll be able to find Security Now!, one of the longest running podcasts on the air still today.  I was just going through my subscription list.  Half the podcasts were gone.  Not gone like gone, but they hadn't made a new podcast in two or three years.  We make one every week.  Steve gets mad if we don't make one every week.  So come on by and enjoy the frothy, delicious Security Now!, now with bergamot.  All right, everybody.  See you next time.  See you, Steve.



STEVE:  Thank you, my friend.



LEO:  Have fun in Turtle Rock.



STEVE:  Right-o.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#713

DATE:		May 7, 2019

TITLE:		Post-Coinhive Cryptojacking

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-713.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm 



DESCRIPTION:  This week we look at the mess arising from Mozilla's intermediate certificate expiration (the most tweeted event in my feed in a LONG time!), Google's announcement of self-expiring data retention, another wrinkle in the exploit marketplace, Mozilla's announcement about deliberate code obfuscation, a hacker who hacked at least 29 other botnet hackers, a warning about a very popular D-Link netcam, who's paying and who's receiving bug bounties by country, another user-agent gotcha with Google Docs, a problem with Google Earth on the new Chromium Edge browser, and a bit more about Edge's future just dropped at the start of Microsoft's Build 2019 conference.  Then we take a look at the continuing and changing world of cryptojacking after Coinhive closed their doors last month.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Lots to talk about.  Firefox broke on Friday night.  Steve has a deep dive into what really went wrong.  We'll talk about Microsoft and Google securing their browsers and why you might want to be a little bit careful about using a particular D-Link camera.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 713, recorded Tuesday, May 7th, 2019:  Post-Coinhive Cryptojacking.



It's time for Security Now!, the show where we cover your security and your privacy online.  Thank goodness we've got this guy right here to explain it all to us, Steve Gibson of the Gibson Research Corporation.  Hi, Steve.



STEVE GIBSON:  Leo, it is great to be with you once again.  This is the latest occurring first Tuesday possible of the month, being the 7th of, what is it, May, which is to say that next Tuesday will be Patch Tuesday, also as far into a month as possible.  I don't know why that matters, but just a random observation.  I guess what occurs to me is I'm always checking back to see what happened in the previous week.  And so it occurred to me, it's like, oh, look, Wednesday was the first, so that means we're going to have a very late Patch Tuesday, and I don't need to worry about it this time.



Today's podcast is titled "Post-Coinhive Cryptojacking" because we discussed this months ago when the announcement was made by the ill-reputed Coinhive guy that he was shuttering his, well, he doesn't really have doors.  His ports.  Shuttering his ports.  And that's like, sorry, we're going to be closing down.  And of course we talked about it at the time, how the relative collapse in the previously very high valuation of cryptocurrencies no longer really meant that it was worth the substantial reputational damage that this guy was taking from the fact that his service was so much abused by mal actors who were foisting his script onto unwitting users' browsers.  So he closed his doors a month ago, on March 8th.  Wait.  April 8th.  And so here we are - maybe it was March 8th.  Maybe it's been two months.



Anyway, we are now after Coinhive.  And the question was what are we going to see in terms of replacement strategy for, okay, we no longer have an online service, so what fills the vacuum?  Well, we know what the answer to that is a few months downstream, so we're going to talk about that.  But as I assembled the overall news of what happened in the past week, I thought, you know, this is sort of the web browser podcast because there's just a lot of browser news.  And in fact the screw-up of Mozilla's over the weekend was one of the most oft-tweeted things into my Twitter stream that we've seen in a long time because Mozilla did something that messed things up for add-ons to Firefox.



Anyway, lots of stuff to talk about.  We're going to look at this, as I said, the mess with Mozilla's intermediate certificate expiration.  Google's announcement, welcome announcement, and I guess very surprising in retrospect since we've talked about what the Congress, the letter that had been written last week, their self-expiring data retention system, which is as far as I know not yet available, but due imminently.  Another wrinkle in the exploit marketplace; the marketplace for zero-day exploits has surfaced.



We've got Mozilla's announcement about deliberate code obfuscation in their browser add-ons, essentially following Google's previous position.  News of a hacker who hacked at least 29 other botnet hackers.  So, as they say, sort of a variation on "no loyalty among thieves."  A warning about a very popular D-Link netcam.  There's little question that many of our listeners will have one of these.  And so we need to sort of warn everyone.  An interesting graphic that I ran across on a story that was sort of about other stuff, and I didn't care about the story, but I loved the graphic, which shows who's paying and who's receiving bug bounties by country, which was sort of interesting.



We have another user-agent gotcha with Google Docs.  A problem with Google Earth on the new Chromium Edge browser.  A bit more about Edge's future just dropped yesterday at the start of Microsoft's Build 2019 Conference.  Then, as I said, we've got a little bit of miscellany, and then we'll take a look at this continuing and changing world of cryptojacking after Coinhive shut down their service.  And this Picture of the Week, since there was really nothing else that jumped out at me, is just too fun.  I don't know if you've seen it before, Leo.



LEO:  It's kind of an old joke, but I like it that the guy actually built it.



STEVE:  Yes, exactly.



LEO:  I'm very impressed.



STEVE:  Simple, clean, and I don't know if it was a science fair project or what it was.



LEO:  A decision tree.



STEVE:  Yeah.



LEO:  An engineering flowchart, if it will.



STEVE:  Exactly.  The universal solve it problem flowchart.



LEO:  This engineer was pretty smart.  I like this picture.



STEVE:  Yeah, it's a great picture.  Okay.  So we'll describe it for those who do not have video.  First the two "no problem" outcomes.  So this is titled "Engineering Flowchart."  And it's a simple two-layer binary tree.  So does it move is the main issue here.  Does it move?  So if you follow the "no it doesn't move" branch, then of course the next question is, should it?  And if you follow the no again - so does it move, no; should it, no - then you end up at the no problem output.



On the other side of the tree, we have does it move, and we go in the yes direction.  And then the question is, well, should it?  And if the answer is yes, then once again it moves, and it should, no problem.  So then the other two outcomes of this two-layer binary tree deal with the problem case.  So does it move, no; and then should it, yes.  And of course that takes you to the can of WD-40 because it doesn't move and it should, it's supposed to.  And then of course the engineer's universal fix-it-all is the fourth branch, the does it move, yes; should it, no.  So that takes you to the roll of duct tape.  If something is moving, put some duct tape on it and stop it.



LEO:  I like it that this guy actually built this out of wood and with real duct tape and real WD-40.



STEVE:  Oh, it's beautiful.



LEO:  Yeah.



STEVE:  Yeah, it's absolutely beautiful.  Two of your TWiT people, Leo, have responded to my sending everybody this week's show notes because Google red-flagged it and refused to allow it to be downloaded, saying that this was potentially malicious and dangerous.



LEO:  Oh, they've all been getting training.  Okay.



STEVE:  So it turns out that it's probably my coverage of Coinhive and its successor because I have links to - I have URLs in the PDF that refer to those by name.  And so Google said, "Agh, don't download this."



LEO:  That's good.  That's good.



STEVE:  So it's like, okay.



LEO:  I think that's a good thing, yeah.



STEVE:  Cool your jets.  That's all right.  It'll be just fine.  Okay.  So the most oft-tweeted alarm sounded on Saturday due to a minor oversight over at Mozilla, entirely understandable.  I mean, so, yes, it's the case that some communication broke down somewhere because someone should have been notified proactively about the expiration of a certificate.  It's not clear to me whether - and I did do some digging, but I couldn't really get a detailed understanding.  I wasn't able to, I mean, I guess I didn't go as far as I could have.  But what happened was an intermediate certificate in the chain of certification that was signing a collection of still-in-use popular add-ons for Firefox expired.



So this is different than the problem I described that I ran into two weeks ago when I was forced to replace the code signing certificate that I've been using in good standing for three years.  That trouble was that the replacement certificate was unknown to Windows and other third-party AV.  So as we discussed many years ago, when first explaining the details of certificate-based security, certificates form a chain of trust with a so-called "trust anchor," which is typically a self-signed certificate - that is, it's vouching for itself - that's implicitly trusted due to its residence in the system's root certificate store.



So we want to protect entry of certificates into that store because anything there is implicitly trusted.  It signs itself.  So that's why we've talked over the years about the concern of the growth of that store, how many CAs' self-signed certificates now appear there.  A new policy of Windows is that that store will only be populated on demand.  So instead of there being what I discovered in a Windows XP machine years ago and talked about it with some alarm on this podcast, it's like there were 400-plus certificates in my store.  And it's like, what happened?  I remember when there were 12.



So anyway, so you start with this trust anchor.  And it's possible for the working certificate, that is, the one at the end of the chain that is the one you care about, to be directly signed by the root certificate.  But that's not typical any longer, nor really practical.  We don't want our root certificates to be expiring often, for one thing, because in some settings it can be difficult, tricky, in some cases impossible, to securely replace those roots on the fly.



So we want the roots to be long-lived.  But with long life comes the danger that they might get loose, that the corresponding private key for them could get loose, in which case it would be valuable, and it would have a long life of potential exploitation.  So that's not what we want to do.  Also, every time that root certificate private key is used, it has to kind of come out of hiding.  It has to, if it's like being stored offline, as we hope it is.  It should be not available electronically.  So whenever it has to be used, it's got to go online somewhere in order to be used.



So each of those events creates that little, you know, no matter how small it is, diminishingly small opportunity for it to get loose.  I'm sure it's in some sort of a hardware security module, an HSM, in order to protect it.  But still, you just - you want to minimize using it.  So for all of those reasons, what's normally done is that the root signs an intermediate certificate that has a longer life still than the certificates it will in turn sign, but less long than the root certificate.  And it's just - it's sort of like a more handy common daily use cert.



So that's why in general today's certificate chains don't just have two certificates, the actual working certificate and the root that signs it.  Typically there's at least three, with an intermediate certificate sort of being a compromise between short life and the very long life of the root, and also one that it could be revoked or killed, or if something happened at least it wouldn't be living as long as the root.



So what happened over this past weekend is that in the months leading up to Saturday, this last Saturday, no one at Mozilla had noticed that the intermediate certificate which was essentially the anchor or the signer of a bunch of popular Firefox browser add-ons still in current use, including uBlock Origin and LastPass, would be expiring.  And unless the user was running the Canary branch or had their browser in developer mode, and there is also a way to turn off signing verification, which no users typically do, any of those things would override the verification of the signature of the add-ons.  Then Firefox protects its users from malicious add-on impersonation by validating the signature of every add-on, every time they're loaded by the browser.



And in fact I ended up not having a problem because I famously never shut down my Firefox.  I mean, I have to go, when I hear that there's like some update available, I go, oh, and go into the About Firefox, and then it kind of wakes it up and says, oh, wow, look, you really need to catch up.  So then I do.  But anybody who shuts their computers down or closes their browser and then opens it on demand, they would have been hit with the unwelcome news that a whole bunch of their add-ons were not being trusted by the browser because essentially the process of verifying the signature requires following this chain all the way back to the root.  And in this case the certificate that was vouching for the add-on had expired.  And at that point, sorry, we stop right there.  We don't trust the thing that is signed at that point.



So there was a bunch of "what to do" advice flying back and forth over the weekend, including those measures I talked about, like getting the Canary release or putting the browser in developer mode.  The one thing that any user could have done, if they were desperate, and so I'm sharing this now for the next time this happens because this can, a user could have backdated their system's clock by one day because the validity period of the certificates is checked against what the system believes is now.  And Leo, in fact, I know that you often, when you get people calling your KFI radio show, your Tech Guy show on the weekends, and they say, "I can't get into any of my secure sites, they're all broken, my browser's blah blah blah blah," you say, well...



LEO:  We know what that means.  



STEVE:  Yes.  What is the date on your computer?  And they look, and they go, oh.



LEO:  How did it get to 1970?



STEVE:  That's right.  Because not only is there a not-valid-after, there's a not-valid-before on these certificates.  And so if your computer, like you lost your BIOS setting for the clock in your BIOS, and your computer turns on, and it believes it's 1970, then none of your certificates are valid anywhere because they've all been issued since then.  So anyway, the point is that it's possible to use this little hack.  You turn your date back a day.  Then you launch Firefox.  It checks all of your add-ons.  Everybody's happy.  Then you put the date back to today.  So that way you're not persistently running with a backdated date so that your other things aren't confused, timestamps on files and so forth.  But just during that brief period when Firefox loaded the add-ons, you say yes, everything's fine, look, oh, that intermediate certificate's going to be expiring tomorrow.  Well, no, it already did, but we fooled the system.



So anyway, that little hack works.  And so all of our listeners can tuck it away for the next time this happens again.  The Mozilla guys scurried around, fixed the intermediate certificate, but they needed to update Firefox.  And so we went from 66.0.3 where the problem was - and, oh, by the way, all previous Firefoxes - to 66.0.4, which fixes the issue.  So anyway, an interesting little glitch.  And I'm sure they'll figure out how did this, you know, how did we fail to get notified of this?  Because, I mean, we've talked about this happening.  It does happen from time to time if, you know, the email address that your certificate authority or certificate provider has for you expired, maybe it was going to somebody who was on vacation, I mean, who knows how it happened?  But anyway, kind of easy to fix.  But it certainly was a source of concern for everybody over the weekend.



Following up on last week's news about the Sensorvault at Google and the news that, as far as the reporting indicated, unless otherwise instructed, they've been retaining all of the detailed, highly accurate location information for basically just "hoovering" it, to use the verb, like everybody that uses their stuff anywhere for all time has just gone into some massive server farm somewhere so that - for purposes we're not really clear about.  But as we discussed, and as the reporting on this discovered, they were honoring so-called "geofencing" warrants which were causing them to turn over successively more personal and accurate location information on people who were believed to be instrumental, either as perps or as witnesses, to crimes for law enforcement.



So what just dropped from the Google blog was the news that they're going to give people some choice.  And they said in their blog:  "Whether you're looking for the latest news or the quickest driving route, we aim to make our products helpful to everyone.  And when you turn on settings like" - okay, they're on by default; right?  "When you turn on settings like Location History or Web & App Activity" - again on by default - "the data can make Google products more useful for you, like recommending a restaurant that you might enjoy or helping you pick up where you left off on a previous search.  We work," they wrote, "to keep your data private and secure, and we've heard your feedback that we need to provide simpler ways for you to manage or delete it."  Well, they certainly heard Congress's feedback.



They said:  "You can already use your Google account to access simple on/off controls for Location History and Web & App Activity."  And as I'll describe in a minute, it's actually "on" or "pause."  They don't actually do "off."  They just do "pause" because they're optimistic that you're going to come back at some point.  They said:  "And, if you choose, to delete all or part of that data manually.  In addition to these options, we're announcing auto-delete controls that make it even easier to manage your data.  Here's how they'll work."



And they said, and I have a picture of the - apparently this is not yet available.  They haven't, as far as I know, produced a date certain yet.  But we have seen what it's going to look like.  And, yeah.  So Leo has it on the screen.  We have the first option is "Keep until I delete manually."  Second option, "Keep for 18 months, then delete automatically."  Or "Keep for three months, then delete automatically."  So they said:  "Choose a time limit for how long you want your activity data to be saved, three or 18 months, and any data older than that will be automatically deleted from your account on an ongoing basis.  These controls are coming first to Location History and Web & App Activity and will roll out in the coming weeks.  You should always be able to manage your data in a way that works best for you, and we're committed to giving you the best controls to make that happen."



So that's their statement.  I think that setting a three-month auto storage expiration policy is probably a nice compromise for users of Google services who still desire the promise of location-based enhancement of their online experience without the creep factor of everywhere they have ever been for the last 10-plus years being silently logged, retained, and searchable in a Sensorvault database somewhere.  And for users who are sure that they want no retention right now, it can be paused, as I mentioned, as Google insists upon phrasing it, and then scrubbed through a manual process.



While signed into Google, you click your profile picture, then you click on the Google account button that clicking on your profile picture reveals.  In the left-hand column there's Data & Personalization category.  So when you select that, you'll find Web & App Activity and Location History.  You select each of those in turn, then flip the toggle to Pause, and then confirm with Google that that's what you really intend.  And then in that confirmation dialog under Web & Activity, it explains pausing this setting doesn't delete any of your past data.  You can see or delete your data and more at myactivity.google.com.  And there's a link there in that dialog.  And similarly, from the confirmation dialog that comes up when you pause Location History, it explains pausing this setting doesn't delete any of your past data.  You can see or delete your data and more in this case at maps.google.com/timeline.  And again they provide that as a link.



So I'm not really much of an uber privacy nut, Leo.  I feel sort of like you do, that, well, this battle has been kind of lost.  And many of our listeners were unhappy recently to hear my capitulation on that front.  But I have to say that going over to myactivity.google.com and scrolling back through days of stuff I've done, nominally in private, I mean, not like I was really worried about it, but I was by myself with no one obviously looking over my shoulder.  When you scroll back through that, it's a little sobering, knowing that all that's being archived somewhere for god knows what purpose.  You know, it's a little bit creepy.



So for what it's worth, if anyone's interested, while you're logged into Google, go to myactivity.google.com.  I remember doing it before and kind of thinking, whoa.  And it can be handy, I have to say.  If I remembered that I was being watched that closely, there have been times where it's like, okay, what was that I was searching for the other day?  Well, Google has it all there.  They've sucked it all up, and they're keeping it.



So anyway, it's nice that Google is responding.  And really I think a year and a half or three months, those are probably two good choices.  And it'd be interesting to see.  I'm sure Google will be watching with interest, as the news of this gets around, what percentage of their users choose which or neither of those options.  I'd love to know what the demographics are.  There is sort of this tease of like, ooh, well, maybe I am getting value from this, so do I really want to turn it off?  I'm kind of in that camp.  Although I certainly respect that people are like, no, I don't want to be followed or tracked at all.  So the good news is you can certainly delete that and also have Google doing it on an ongoing basis.



And so I suppose when they show up to fulfill their request for Congress to get an update on this, they'll be able to say, well, yes, we're now offering these features.



So in the evolving marketplace of this crazy world of exploits, an apparently Ukrainian talented and prolific hacker has been found to be himself - if it's a he, and if it's a self, we're not really sure, at least that's the way this person is presenting themselves - has been selling zero-day exploits to various APT, Advanced Persistent Threat groups.  Kaspersky Lab has been watching the network comings and goings of a prolific hacker whose name is believed to be Volodimir, a Ukrainian name, who goes by Volodya as a nickname.  And this nickname often appears in the code that this guy sells.



For the past three years this hacker has been selling Windows zero-days to at least three different cyberespionage groups, as well as into cybercrime gangs.  This as a consequence of Kaspersky's monitoring of I guess the dark web comings and goings.  The hacker's activities reinforced beliefs that some government-backed - and I should mention this is high-end - government-backed, state-sponsored cyberespionage groups are not only developing these things themselves, but also regularly purchasing zero-day exploits from third parties on the dark side of the 'Net.



These APT groups, believed to be operating out of Russia and the Middle East, have often been spotted using zero-days which were developed by real-world companies acting as sellers of surveillance software and exploit brokers for government agencies.  Kaspersky's recent reporting shows that these APT groups do not shy away from dipping into the underworld hacking scene to acquire exploits that were initially developed by what appear to be lone hackers and sold to cybercrime groups.



This Volodya character, who Kaspersky Lab characterizes as one of the most prolific vendors of zero-days, first came to light three years ago, which is why we know this has been going on for three years, back in 2016, when he first sold, or offered for sale, an exploit under the name BuggiCorp, B-U-G-G-I Corp.  And that sounds familiar to me, so I think we probably talked about it on the podcast.  At that time, this hacker's actions were in the news because he put a Windows zero-day for sale, not on the dark web, but on what was at the time an infamous Exploit.in cybercrime forum.  And back then it was a surprise to see a hacker openly advertising a Windows zero-day in public, since those transactions typically happened in private.



So as a consequence, the world watched as this BuggiCorp dropped his initial asking price a couple times, from initially $95,000 down to $85,000, at which point he sold it to a cybercrime group and began to develop a reputation for producing high-value zero-days.  Now he has sort of an exclusive, dedicated clientele and is continuing to sell other zero-days that he's developing privately, in some cases with prices reaching $200,000.  Kaspersky has a group with the abbreviation GReAT, G-R-E-A-T, stands for Global Research and Analysis Team.  They are the elite APT tracking unit which has been watching this Volodya, knowing that he's fluent in Russian, although they believe he has a Ukrainian origin due to his name.  They believe he's the author of the recent Win32k elevation of privilege vulnerability, which was fixed in March and which we talked about.  That zero-day which has since been patched was in use by a cybercrime group focused on financially focused threats.



And this zero-day is only the latest of this Volodya's accomplishments.  That earlier one we talked about three years ago was also a Win32k privilege of elevation vulnerability that was also found leveraged in the wild and had been used by and apparently sold to that Russian Group, Fancy Bear, which we also know of as APT28, Pawn Storm, Sednit, Sofacy, and Strontium.  No one could really agree on their name.  Or maybe they were not known to all be the same group until later.  Anyway, as we know, they are one of the infamous groups that were behind the 2016 attack on the Democratic National Convention headquarters, the DNC.



Kaspersky has been watching Volodya and has seen him selling zero-day exploits essentially continuously through the years to these high-end APT groups that they follow and noted also that, in additional to zero-days, he's also been selling one-days, which is to say, the moment something is patched that he did not develop, he quickly reverse engineers the patch and puts it on the market, recognizing that it has been patched.  He's not going to get the kind of money that he will get for a zero-day, but it's been patched, but not everybody's applying patches immediately.  So there is some short-term value to an exploit which is sufficiently valuable, but which is not yet seen, the patch for which has not yet been deployed in systems.



So as Kaspersky sums it up in their reporting of this, this Volodya appears to be making a profitable career out of selling zero-day and other exploits and building quite a portfolio.  They said in their reporting that they don't know that Volodya is not the front for a larger group or a team of exploit developers, or even maybe an exploit brokering company that fronts for other independent brokers.  With anonymity comes a lot of uncertainty.



But anyway, I wanted to share this because I thought it was interesting.  This sort of fleshes out another wrinkle of the nature of the contemporary underground ecosystem for exploits.  And this just, again, another piece of information that makes me shake my head.  I keep flashing back on that very first scene from the first "Matrix" movie where Neo was selling a hack for something to his friends.  That movie was released on March 31st of 1999, so just over 20 years ago.  Back then, I remember, it seemed like such a stretch, like real sci-fi fantasy.  But it turns out just to have been amazingly prescient.  We're in that world today.



LEO:  Well, we don't know if the machines are running everything.  But other than that, yeah.



STEVE:  You know, there is a compelling book, apparently, that suggests, I'm not saying, but I think it's called "The Simulation..."



LEO:  Elon Musk believes the simulation theory.



STEVE:  I know, I know.  Like with a high degree of certainty.  Like what is it, like...



LEO:  It's a billion to one that it's not a simulation.



STEVE:  That it's like we're not in a simulation.  It's like, okay, Elon.



LEO:  Yeah.  Elon's smoking something, I think.



STEVE:  Long as we all play by the same rules of the simulation, then it'll all turn out okay.  So Firefox has announced that they're following Chrome in banning browser extensions containing obfuscated code.  We talked about this decision that Google had made for Chrome last October.  And the Chrome ban took effect on January 1st of this year, so right at the strike of midnight 2019.



Google's Analytics, as we discussed at the time, had determined that 70 percent of malicious browser extensions deployed mechanisms of deliberate code obfuscation.  And that made their task of manually inspecting the code for malicious behavior so much more and unnecessarily difficult.  It was like, why are we putting ourselves through all this?  Like going to all the work of untangling code which has been deliberately obfuscated.  They were, for a while.  Finally they just said, forget this.  The simplest solution is just to say no more deliberately unreadable code.



So last Thursday the Mozilla folks decided to follow Chrome's example, and they updated their policies which would go into effect on June 10th, so exactly one month after this notification.  They said about their guidelines, they said:  "Add-ons may contain transpiled" - that's a concatenation of "translated" and "compiled," so transpiled - "minified or otherwise machine-generated code, but Mozilla needs to review a copy of the human-readable source.  The author must provide this information to Mozilla during submission, along with instructions on how to produce a reproducible build."  So Mozilla can verify that the source that was provided equals the binary that the author also provided.



They wrote:  "The provided source code will be reviewed by an administrator and will not be redistributed in any way.  The code will only be used for the purpose of reviewing the add-on.  Failure to provide this information will result in rejection.  Add-ons," they continue, "are not allowed to contain obfuscated code, nor code that hides the purpose of the functionality involved.  If external resources are used in combination with add-on code, the functionality of the code must not be obscured.  To the contrary, minification of code with the intent to reduce file size is permitted."



So as an industry we're sort of creaking forward, step by step, figuring out how to do all of this.  I think this just makes sense.  If users are downloading add-ons that they're going to be getting from some sort of curated store, you know, add-on facility, which is what Chrome and Mozilla are both offering, due to the power of the add-on there has to be some mechanism for oversight.  So I just say bravo to Firefox for doing the same.



Actually, Google had the guts to do it first.  The Mozilla people probably waited to see how that was going to work out, allowed Google to take the arrows in their back if they were going to, or use their strength as the number one browser in the industry and to have sort of first mover position, and then to follow.  So anyway, I imagine that will be the policy, which Google has set for Chrome, now Firefox, and I don't know where Microsoft stands.  But certainly with this foundation it becomes the expected protocol for people who want to create add-ons for web browsers.  And I think it makes a lot of sense.



So a researcher, Ankit Anubhav, is an IoT botnet researcher whose work we've looked at previously.  Unfortunately, his name looks like you would say Ankit, A-N-K-I-T.  I was very nicely informed by a Twitter follower that it's Ankit [pronounced un-ky] Anubhav.  So thank you for providing the pronunciation help.  He's the principal researcher at NewSky Security who arranged a discussion and interview with a hacker who calls himself Subby, S-U-B-B-Y.  What transpired was interesting and entertaining.



So first of all, as we know, typical IoT botnets like Mirai and QBot rely upon obtaining access to their target devices, that is, the things that they're trying to attack and compromise using typically the device's weak or default credentials or some sorts of known problems with the router or smartphone, or not smartphone, smart camera, netcam and so forth.  However, as it turns out, the hackers themselves behind these are not very security conscious or, it turns out, actually, very tech savvy.  They are using very poor, weak, and often default passwords to protect their own command-and-control servers.



So this would mean that in theory another black hat could come along, figure out the IP addresses of the command-and-control servers, and then brute force those to obtain control of the command-and-control servers to basically commandeer the botnets away from the people who gathered the bots in the first place.  Rather than building their own, they could just steal them from a different bot herder.



So as Ankit learned during his interview, this is exactly what happened in this case.  The hacker hacker, who calls himself Subby, brute forced at least 29 different IoT botnet command-and-control servers, finding that they were in fact using extremely trivial credentials.  I have a screenshot from the blog posting describing this interview that shows, for example, we've got in one case at 139.xx.xx.31 a botnet family named Frosty on port 8372 was using username "root" and password "root."



LEO:  I like Emily/rawr.  That's my favorite.  Yeah, they're not trying to secure these.  I see root/root a lot.  It doesn't look like they're really trying to secure it.



STEVE:  Well, actually they are, Leo.  



LEO:  But that's dopey.



STEVE:  Well, wait till you hear how dopey these people are.  I mean, it's a little shocking to understand the nature of them.  So here is, sort of in Q&A form, the transcript of Ankit's interview with Subby.  Ankit wrote:  "I decided to ping Subby" - and English is not his first language, so we'll forgive him.  He said:  "I decided to ping Subby to know more answers besides the data, like why and how he is doing this, and what is the motive.  Some of the excerpts from the interview are as follows."  So Ankit asks:  "What technique are you using for brute forcing the servers?"  Subby replies:  "I have a network of honeypots configured to capture binaries over Telnet SSH.  The captured C2 IPs are then..."



Okay.  So what he's saying is his honeypots capture the attempt to infect his honeypot as if it were an IoT device.  He pretends to allow that to have happened, which allows him to then obtain the command-and-control server IP that that infection would then have attempted to reach out and connect to.



So Subby says:  "The captured C2 IPs are then" - and of course C2 is command-and-control.  "C2 IPs are then port scanned via NMAP to find the C2 port.  For brute forcing I'm using a dictionary-style attack, coupled with a password list which has common username/password combos.  In addition to this, each C2 undergoes a random-style password attack which continues up to six alphanumeric characters under the user 'root.'  I changed the user to something specific if I have prior knowledge of the C2.  Each cracked password is added to the password list used when brute forcing the C2s in the future."



So he obtains the IP from his honeypot that pretends to get infected.  Then he goes out and he brute forces the login for that server.  Oh, and the reason he's not obtaining the port is that typically the IoT's port, its connection port will be different than the port used to log into it for command-and-control.  So he does an NMAP scan to find other open ports and then brute forces those.



Ankit then asks:  "As you have found out, many of the credentials are very weak.  Why do you think this is happening?"  Subby says:  "It's obvious as to why this is happening.  A large percentage" - Leo, are you sitting down?  "A large percentage of botnet operators are simply following tutorials which have spread around in the community or are accessible on YouTube to set up their botnet."



LEO:  They're script kiddies.



STEVE:  Yes.  "When following these tutorials, they do not change the default credentials.  If they do change the credentials, the password they supply is generally weak and therefore vulnerable to brute forcing."  Ankit asks:  "How much total bot count you have achieved brute forcing these C2s?"



Subby replies:  "Within the first week of brute forcing I surpassed 40,000 devices.  This was quite an inflated number due to possible duplication.  It is well documented that botnet operators like to artificially increase their bot count.  I estimate the number to be closer to 25,000 unique devices.  I was able to get a reliable network traffic graph produced of the traffic generated from all the botnets compiled, and it was just under 300 gigabits per second.  This high number was achieved because of the vast amount of Digital Ocean servers on many of the botnets.  It's well known that Digital Ocean are relatively slow in comparison to other hosts when dealing with abuse complaints.  Since then, the number of C2s vulnerable to brute forcing has lowered considerably," and he says, "(30-40%).  This is likely due to how vocal I've been when brute forcing the servers.  I have actively contacted botnet operators, letting them know that I managed to obtain access to their C2."



Ankit then asks:  "Why are you doing this?  Are you doing this for DDoS?"  Subby replies:  "The main reason I undertook this task initially was to see how well brute forcing would work on C2 servers and whether it would be an efficient way of getting access to devices, rather than having to use exploits or the usual loading onto devices with weak passwords via Telnet and SSH."  He says:  "Since Mirai was released, Telnet has slowly become saturated, and it's hard to get a decent number of bots."  I assume what he means is that any previously exposed Telnet has already been used and then closed, or the authentication increased so that it's not obvious how to get in.  Basically, by "saturated" meaning that large and easy opportunity has just been drying up.



And then, finally, Ankit writes in his conclusion:  "In one previous case, we observed the SQL database of an IoT botnet having root:root" - meaning username "root," password "root" - "credentials before; but as we see now, the problem is bigger and not a one-off case.  Pure novices in the field of IoT are increasing.  We are not talking about script kiddies" - meaning that they're like a level above - "but such low skilled actors who are unable to set up a botnet from source, yet they want to launch a DDoS by doing nothing other than pressing a button.  We also observed mistakes as novice as not replacing the botnet dummy C2 IP with their own."



And he cites an example:  "Unstable, the Turkey-based author of Z3hir IoT botnet, has gone to the extent to release a video where he tells how to replace the dummy C2 (0.0.0.0) with the attacker's IP.  When asked about the video, he said:  'Yes, these script kiddies are not changing IPs, and they are blaming me when the botnet does not work.'  Recently [someone tweeting who goes by] @VessOnSecurity also observed a similar case where his honeypots found attacks with a dummy C2 of INSERT-IP-HERE, pointing to the fact that the threat actor forgot to change the dummy C2 IP to insert the real one, yet proceeded to attack the IoT devices, and subsequently the honeypots, with a non-functioning command-and-control server."



And so he says:  "Interestingly, despite not knowing what they were doing, script kiddies often succeed, thanks to good support and tutorial videos by threat actors.  In many cases having a secure password and updating the IoT devices can save one from these low-hanging fruit attacks."  So, wow.  Amazing that that's the nature of this.  But again, you're going to have people who are highly skilled, operating in the dark web, selling zero-day exploits for $200,000 on one end of the scale.  And on the other end you're going to have interested individuals who haven't done any of this sort of work before obtaining a kit off the shelf and really lacking any concept for how to even, you know, which buttons to push in order to make it go and make it do something.  Amazing.



I wanted to bring this news from ESET researchers to our listeners' attention in case anyone had a D-Link DCS-2132L netcam.  It is apparently a very popular camera, been on the market for many years.  A ton of them exist in the wild.  And they have never been secure.  ESET's blog posting and disclosure at WeLiveSecurity.com was titled "D-Link camera vulnerability allows attackers to tap into the video stream."  And the subtitle was "ESET researchers highlight a series of security holes in a device intended to make homes and offices more secure."  So as I said, it's the DCS-2132L.



They wrote in their posting:  "Many people are looking to improve the security of their homes or offices by installing smart cameras.  With a direct connection to the Internet, their surveillance stream is just a few clicks away and available at any time.  Yet this kind of convenience can quickly turn sour," they wrote, "if the camera suffers from a security vulnerability that opens the door to unauthorized actors.  As shown by ESET smart home research, this is the case with the D-Link DCS-2132L cloud camera, which allows attackers to not only intercept and view the recorded video, but also to manipulate the device's firmware.  The most serious issue," they said, "with the D-Link cloud cam is" - get this - "the unencrypted transmission of the video stream."



That's right.  You heard that correctly.  It's an Internet-connected streaming video security camera that does not encrypt its video.  That is, a passive eavesdropper of the camera's stream is getting an MJPEG or an H.264 unencrypted video stream just by eavesdropping.



ESET writes:  "It runs unencrypted over both connections" - meaning, and this is also even further stunning - "between the camera and the cloud and between the cloud and the client-side viewer app," they wrote, "providing fertile ground for man-in-the-middle attacks and allowing intruders to spy on victims' video streams."  And so in my words, in other words, they didn't really pay any attention to security at all.



And you could almost, I mean, if you wanted to forgive them for not encrypting the camera's feed to the cloud, making some excuse like, well, there's only a camera and a barely capable widget in there.  Like we used up all of our budget just to create a WiFi connection.  We didn't have any ability to do security.  You could, if you wanted to forgive them for that, okay.  But there's no way to forgive them for the stream from the cloud servers back to their app not encrypting.  I mean, it's just amazing.  So they just didn't bother with that, either, with no encryption.



The viewer app and the camera communicate via a proxy server connecting on port 2048 using a TCP tunnel which is based on a homegrown D-Link tunneling protocol.  But none of it is encrypted.  The actual payload, which is the sensitive contents, is in the clear so that all you have to do is sniff.  And so they use the term, ESET uses the term "man in the middle."  It's a man in the middle intercepting network traffic.  It's worth noting, though, again, it doesn't have to be an interception.  A passive eavesdropper is all this takes.  So they responsibly notified D-Link.



Oh, there's other problems, too, I should also mention.  There is, in addition to unencrypted audio and video, the device supports on-the-fly firmware updating, but offers no firmware update authentication mechanism.  Which means that any sufficiently motivated adversary could look at what the protocol is - it's not documented, but it's trivial to reverse engineer, the ESET folks did - figure out what's going on, and essentially submit a POST to the camera which causes it to update its firmware from a source that the attacker can provide, allowing firmware to be changed for any purposes whatsoever.  And there is no protection against a third-party maliciously supplying firmware to the camera.



So more than six months ago ESET notified D-Link.  Apparently D-Link responded immediately, saying thank you for the news.  We'll notify the proper parties and get right on that.  Nothing happened for six months.  The most recent firmware is dated 2016, so no indication that there has been any change being made by D-Link; and, as far as they know, none forthcoming.  So the camera is currently on the market.  I put a picture of a snap I took of the camera being sold on Amazon for 60 bucks.  It's moderately well rated.  It's being sold both by brick-and-mortar and online.  And I'm just saying, if you own one of these, in fact what ESET said was be careful where you aim it.  Be careful where you point it.  You do not want this aimed at anything that is highly secure in your residence or in your business because it's not encrypting.



LEO:  Are other cameras encrypting?



STEVE:  Many cameras are, yeah.  I mean, even, you know...



LEO:  Not all.



STEVE:  Well, we talked about even, long time ago, even baby cams that were doing sort of some encryption, sometimes it was sketchy, but they were making an effort.  There's just none.



LEO:  They don't have a lot of processor in these devices.



STEVE:  That is true; they don't.



LEO:  I mean, you could, if they had the horsepower...



STEVE:  Although it's able to update its firmware.  So, I mean, if it's able to update its firmware, you'd think, I mean...



LEO:  Put a certificate on it of some kind.



STEVE:  And encryption is just, you know, simple symmetric encryption, even if they didn't do any fancy public key, private key encryption is just not difficult to pull off any longer.



LEO:  At least do a ROT13.  Do something.



STEVE:  Yes, exactly.  Do a scramble.  Wow.  So I didn't have a story.  This was part of a different story that didn't really make it onto the headline for the podcast.  But I liked this graphic.  This was a very cool graphic.  This is a visualization of the flow of software bug bounty money, that is, offerings and payouts, from those putting up the bounties on the left to those collecting the bounties on the right.  And of course our listeners who are not seeing the video and don't have the show notes can't see this.  But essentially it shows the largest piece of the pie, 15, nearly just shy of $16 million over this period of time was sourced, bounties offered by those in the U.S.



In second place with a huge drop, at 1.2 million, is Canada.  Third place, another drop nearly to a quarter of Canada.  Germany comes in at 458,000, Russia at 308,000, Singapore 256,000, U.K. at 252.  So that gives you a sense.  U.S. at 16 million, Canada 1.2 million, then a quarter million for Germany.  So that's the sourcing.



On the right side of the chart we have the nationality of the recipients of the bounties.  So the largest, but not nearly as much as the offering, but the largest is the U.S. at 4.150 million.  Number two, close to the U.S., is India at 3.1 million.  Then Australia at 1.23 million, Russia at 1.3 million, the U.K. at 916.  So on the offering side USA, Canada, Germany, Russia, in that order.  On the recipient side, but more evenly spread, USA, India, Australia, and Russia.  And it's interesting that Australia is so well represented there.  There's nobody, they're not offering any bounties over on the left, but they're collecting.  They're in number three position, collecting...



LEO:  Ain't no fools, mate.



STEVE:  ...a 1.3 million chunk.



LEO:  We're taking the money.  We're not giving it.



STEVE:  That's right.



LEO:  By the way, somebody, ScooterX in our chatroom, sent me a link to the Wyze Cam.  You know, those are those, I mean, you can't get cheaper than them, $20 streaming video cameras.  They probably have very, very simple processors.  They use TLS, AES 128-bit encryption to protect the security of the live stream and playback data.  Every device has its own secret key and cert so they can validate the identity during a handshake.



STEVE:  Wow.  Wow.



LEO:  I mean, they do it right.



STEVE:  That's what you want, yes.



LEO:  And that's a cheap - there's no way anybody's using a slower, lower processor than the Wyze Cam.



STEVE:  Yeah, so rip those D-Link suckers out by their roots.



LEO:  Yeah.  Yeah, 20 bucks for a Wyze Cam and replace it.



STEVE:  Wow.  Yes, yes, yes.  So we have sort of another interesting user-agent gotcha with Google Docs and the new Microsoft Edge browser.  We were just talking about this mess with user-agent headers and the idea that websites are expected to tune the code they produce for this or that browser.  And frankly, my approach would always be to simply choose, well, if I were developing web-side stuff, the lowest common denominator feature set that all browsers share and write to that so that the same thing works independent of and across all web browsers.



But I understand that when what's being delivered is a cutting-edge web-based application, the lowest common denominator might be too low.  And this is the situation with, for example, Google Docs, which has always presented a warning message when a web browser is not known to be capable of running its online app.  Microsoft's new and forthcoming Edge browser, which as we know is based on the common, and increasingly common, Chromium engine would be expected to be able to run Google Docs without trouble.  So observers and early adopters and probably a lot of our listeners - because we now know that Microsoft has made it officially available for download.  It's officially available under Windows 10.  It also runs under Windows 7.  And they will be making it available for Windows 7, which I think is great because it's a nice browser.



Anyway, so they observed, with some puzzlement, when Edge showed the message "The version of the browser you are using is no longer supported."  Well, first of all, what?  No longer?  It turns out, upon examination and experimentation, that Google Docs maintains an explicit whitelist of known compatible web browsers.  And at the moment, this new Chromium Edge is transmitting a slightly altered user-agent header that Google Docs' browser checker didn't recognize.



We were previously talking about how the user-agent header in the context of Microsoft's Edge was presenting different user-agents, depending upon the domain being visited.  Remember Netflix did one thing, and some random site actually in Australia, as I recall, was doing something else.  I mean, it was saying it was a bit of a chameleon.  So it would presumably be simple for Microsoft to present a Google Docs-compatible user-agent header to Google Docs, or Docs could be instantly taught about the user-agent header which is being produced by Microsoft's forthcoming browser.  And this is one of those weird  things where, like, both of them could fix the problem.  I wonder who's going to?  I'll be mildly curious to see whether Google Docs just instantly fixes it, as they could; or Microsoft says, oh, well, we'll just pretend to be what Google Docs wants when we're at Google.  Who knows?



And in something that's sort of related, Google Earth won't run under Microsoft's Edge browser, but not for the same reason.  Turns out there's a very different and true incompatibility in the case of Google Earth.  As we just said, with Google Docs it's like, eh, I mean, it will run, it's just that the user-agent header is presenting something that Google Docs isn't sure about.  Okay.  Turns out that when users try to launch Google Earth, which is supposed to work with Microsoft's new Chromium Edge, they get an error that, Leo, you probably have seen before because it rang a bell with me.  The error is "Aw snap!"



LEO:  Oh, yeah.  Yeah, I've seen that many times, actually.



STEVE:  Yeah.  "Aw snap!  Google Earth isn't supported by your browser yet.  Try this link in Chrome instead.  If you don't have Chrome installed, download it here.  Learn more about Google Earth."  And so there's a few links, and they're obviously trying to move you over.



So Eric Lawrence, who is Microsoft's product manager for Edge, explained on a Twitter thread that the issue arises from the fact that the Chromium-based Edge browser does not ship with the Portable Native Client.  That was the PNaCl.  Remember there was Native Client, and then we went to Portable Native Client.  That's the architecture, the Portable NaCl, PNaCl, is the architectural neutral version of Native Client, which was being used by Google when they converted Google Earth into a web app back in 2017.  Google's plan is to move Google Earth over to the universally agreed upon now replacement for NaCl or PNaCl, which is WASM, WebAssem.  But Google's running a little behind schedule on that project.



Jordon Mears, whose title is Lead Manager for Google Earth on Web, said:  "It has always been our intention to bring Earth to as many people as possible, on as many browsers as possible. Parallel to our efforts in building the first iteration of Earth on Web, Google and the Chromium team have been active participants in the W3C process for WebAssembly."  He continued, saying:  "WebAssembly also has the advantage of being supported by the four major browsers:  Chrome, Edge, Firefox, and Safari."



Okay.  So what's a little weird is it's not yet supported by - it's not officially released yet for Edge.  He said:  "So since the April 2017 launch of Earth on Web, the Earth team has been working to port Earth on Web over to assembly language from Native Client."  Yet they haven't yet finished that, despite the fact that a demo of Google Earth running in WebAssembly on Chrome, Firefox, and Chromium was presented by Google's leader of the Chrome Web Platform Team back at the Chrome Dev Summit in 2017.



So anyway, it's taking them longer than they planned.  And I presume that it means that Microsoft will be waiting a while for Google Earth to be fully finished for operation on WebAssembly, and that they will not bother with the effort to bring the Portable Native Client (PNaCl) up on their version of Chromium Edge, since it's already a deprecated technology, which would sort of just be a throwaway.  So it wouldn't really make any sense for them to do that.



And yesterday, while we're on the topic of Edge and browsers, we're currently in the middle - am I getting my months wrong?  Are we in the middle of Microsoft's three-day Build?  Or was that a month ago?



LEO:  No, it's going on right now.



STEVE:  That's what I thought.  Yes, May, right, right.  So it was 6, 7, and 8.  So we're in the middle of Microsoft's three-day Build 2019.  And Microsoft's CEO, who we know is Satya Nadella, kicked off the 2019 conference with a keynote that touched on Microsoft's plans for its much-anticipated Chromium-powered Edge web browser.  Among the list of goodies, the things that I pulled out that would be of interest to our listeners, Microsoft plans to have updated privacy tools.  They'll be acquiring some of the new privacy-centric features to help users understand how their data is being used by sites across the web and to provide some controls over various features of the browser, including adding the innate ability to block trackers.



The privacy panel looks like it will allow users to select among different levels of information sharing, probably not being super granular the way Firefox's content blocking feature is, but offering users sort of a little more easy to use, you know, how much content blocking do you want to apply?  There will also be, as we touched on briefly, an IE11 mode for enterprises that have software that won't run under Chromium that was written just specifically to Internet Explorer 11.  So you'll be able to run the Microsoft Edge browser in IE11 mode in order to accommodate that need.



And then there will be advanced developer tools.  Microsoft Edge will allow sort of a common set of tools for web content, progressive web apps, and WebView for developers.  So anyway, I'm excited that I'll be able to get that on Windows 7 to go alongside Firefox and Chrome.  That'll be nice.



And one quick note.  I have been talking about, and I can't think of the name of it now, that really cool, there it is, Filemail, the Filemail site, and bullish about it because it was nicely built and offered an apparently free service.  Well, I ran into, a couple days ago, a stumbling block with it that I wanted to note, and that is that they don't say anything anywhere until you hit the block.  But you're only able to upload two files per day per upload IP.  And there was something I was sending someone where I ended up needing to send them a third thing.  Had I known, I would have put it all together, but I just didn't know.  And so they said, no, sorry, can't do it unless you purchase their plan.  And fortunately I remembered Firefox's Send system, which is free and has no such limit.



So I just wanted to just put that back in our listener's radar one more time:  send.firefox.com.  Without identifying yourself in any way to Mozilla, like if you're otherwise a completely committed Chrome user, and you have no Mozilla account, you're able to create an identity with Mozilla.  I use it for synchronizing Firefox across multiple systems.  But without doing that, you're able to send files up to a gig.  If you sign into Mozilla, then the limit increases to 2.5 gigs.  So it's a bit of an incentive, if you need to move really large things, to identify yourself to them.



But I just wanted to remind our listeners:  send.firefox.com.  It is point-to-point encryption.  It encrypts in the browser.  It uses a technology such that Firefox is - no man in the middle is able, including Firefox as the intermediate, is able to decrypt it.  You obtain a link that allows someone at the other end to download it, and then they're able to get it, with their browser doing the reciprocal decryption upon receipt.  So anyway, nice service from those guys.  And I just wanted to put it back on our listeners' map.



And Leo, that brings us to our subject, Post-Coinhive Cryptojacking.



LEO:  Time to talk about Coinhive.  We got the replacements for Coinhive.  Everything you can use if you don't want to use Coinhive.



STEVE:  Actually, Leo, you should bring up the site that I ended up finding.  Let's see, where did it go here?



LEO:  Is this through Malwarebytes?



STEVE:  No, it's at the bottom of page 15.  And our listeners can, too:  www.webminepool.com.  Yes, we have a service for you if you are missing Coinhive.



LEO:  It's not popping up.



STEVE:  Oh, really, www.webcoinhive.com.



LEO:  WebMinePool.com.



STEVE:  Yeah, WebMinePool.com.  Interesting.



LEO:  You killed it, Steve.



STEVE:  You may have some protection in place.  I was able...



LEO:  Oh, you know what?  Of course I do.  That's why I can't get there.  We've got Cisco Umbrella sitting on us and our Sophos.



STEVE:  WebMinePool.com. 



LEO:  See if you can get there because I bet you that's our - we have all, you know, you know Russell.  We've got the Sophos.



STEVE:  Yup, came right up for me.  We'll see what the chatroom says for www.webminepool.com.



LEO:  Russell's keeping me from going there, and that just shows you, that's probably Cisco Umbrella, one of many secure - I shouldn't say what our security stuff is.  That's probably keeping us from going there.



STEVE:  Yeah, there's sort of a cool end-user thing.  You can check out your current hash rate.  You click on it, and it'll show you the rate at which your computer is able to mine.  You can increase the number of threads which are mining and the percentage of your CPU that you want it to take up.



LEO:  I kind of prefer that our employees don't do cryptomining on our hardware and network.  Because, you know, this is a personal thing, you know.



STEVE:  Okay.  So what happened after Coinhive shut down, aside from that.  As we previously discussed, and as I said at the top of the show, the highly controversial Coinhive browser-based Monero coin mining facility voluntarily closed its doors.  Actually it was two months ago.  I said a month ago.  It was on March 8, so it has been nearly two months.  At the time, it was expected that the vacuum would quickly be filled with alternative mining solutions.  So what has happened?



Interestingly, the head of Threat Intelligence at Malwarebytes, Jerome Segura, decided to answer that question and put up a blog posting at Malwarebytes titled "Cryptojacking in the Post-Coinhive Era."  And he wrote:  "September 2017 is widely recognized as the month in which the phenomenon that became cryptojacking began.  The idea that website owners could monetize their traffic by having visitors mine for cryptocurrencies in their browser was not new, but this time around it became mainstream, thanks to an entity known as Coinhive."



He wrote:  "The mining service became a household name" - and I was thinking, okay, I'm not sure whose house he's in, but it wasn't a household - I'm not sure that Coinhive was a household name, but within certainly our podcast listener community.  "Became a household name," he wrote, "overnight, and quickly drew ire for its original API, whose implementation failed to take into account user approval and CPU consumption.  As a result, threat actors were quick to abuse it by turning compromised websites and routers into a large illegal mining business."



He writes:  "The ride was wild; but, as we came to see, short-lived, as Coinhive shut its doors in March 2019 following months of steady decline and loss of interest in browser-based mining."  And I'll interject here, as I said, that as we talked about at the time, it was largely a function of the general collapse in the cryptocurrency valuations from their highs of a few years before that, which did a lot to take the wind out of the market for monetizing the arguably stolen CPU cycles of innocent web users.  Remember that the browsers responded and started fighting back, and things like uBlock Origin and the like - that's interesting.  uBlock did not block WebMinePool.  Because I've got it running.  It shows a block of something on my browser, but not that.



And as I said at the time, and I say now, with user permission, with a user's knowledge, I think this is an interesting idea for monetizing, for some sites that do it responsibly and want to monetize the time that their users spend.  We've talked about ad blockers.  We're seeing some pushback against them.  From time to time you get a pop-up saying, "I see you're using an ad blocker.  Would you please consider supporting the site?"  Well, if using my processor while I'm rummaging around was an option to give revenue to them that they sanctioned, it's like, okay, as long as everybody agrees.



So anyway, what the Malwarebytes guy wrote, since what they have is they have a presence in many people's machines, and they are doing real-time traffic analysis in order to protect the machines, he wrote:  "Interestingly, we still detect thousands of attempts for Coinhive-related domain requests, even though the service announced it was shutting down on March 8th.  Over the past week, our telemetry recorded an average of 50,000 attempts per day.  Digging deeper, we see that a large number of websites and routers have never been cleaned [of course], and the bits of JavaScript requesting the Coinhive library are still there.  Evidently, with the service down, the necessary WebSocket that sends and receives data between client and server will fail to connect to the server, resulting in zero mining activity or gain."



Yet it's very much like Code Red and Nimda; you know?  You put a sensor out on the 'Net, you still get Code Red and Nimda pings because there are still some of those viruses, those worms, still alive, trying to reproduce.  Similarly, it's going to be a long time before every router that has a resident copy of a Coinhive miner in it gets rebooted and then doesn't get reloaded into RAM, presuming that it wasn't made persistent.



So this kind of stuff, these things have a very long tail before they finally go away.  And for anyone who's interested, I have a picture from February 27 through the end of April.  It's relatively, you know, it's declining from its peak.  But the decline looks like it's leveled off to like those things that are going to probably be around for a long time.



And so he poses the question in his blog:  "Is cryptojacking still a thing?"  He says:  "To answer that question, we go back to the early adopters of browser-based mining, which were torrent sites.  Visiting a proxy for The Pirate Bay with our browser, we spot something familiar enough.  Our system's CPU usage maxes out at 100%.  So," he writes, "yeah.  Apparently in some corners of the web cryptomining itself remains alive and well."



He says:  "As we'll recall, this is what started the cryptojacking trend back in 2017 when users weren't told about this code running on their machine, let alone that it was hijacking their processor for maximum usage.  In this instance," he writes, "the mining API was provided by CryptoLoot, which was one of Coinhive's competitors at the time.  Malwarebytes reports that while they are seeing nowhere near the same level of activity as they saw during the fall of 2017 and early 2018, according to their telemetry, they are still blocking more than one million requests to CryptoLoot each day."  So that says there is still an effort by mining to get its script loaded into people's systems for the purpose of mining.



There are a few other services out there, and it's worth mentioning CoinIMP, which, he writes, they've seen used more sensibly on file sharing sites.  And he says:  "Router-based mining still going.  While the number of compromised sites loading web miners was going down in 2018, a fresh opportunity presented itself thanks to serious vulnerabilities affecting MikroTik routers worldwide.  By injecting mining code from a router and serving it to any connected device behind it" - we've talked about this in the past where, for example, if you were behind an infected router, and you went to an unencrypted landing page like your ISP's DNS intercept, where it said, oh, the site you're looking for isn't available, brought to you by Cox.  How about some of these?  That would immediately pin your system's CPU because the router you got that message through took advantage of the fact that the page you received was not encrypted in order to insert its JavaScript into your page header.



So he says:  "By injecting mining code from a router and serving it to any connected devices behind it, criminals could finally scale the process so it was not limited to visiting a particular website, therefore generating decent revenues."  On the other hand, they could only do this for any non-HTTPS page that the browser loaded.  Once upon a time, 10 years ago, that would have been very lucrative.  Now, not so much because pretty much everything is encrypted.



He says:  "The number of hacked routers running a miner has greatly decreased.  However, today we can still find several hundred that are harboring the old, now inactive Coinhive code, and have also been injecting a newer miner."  Okay.  So this brings us to WebMinePool.com, meaning that naturally it is being used for malicious purposes.  WebMinePool.com is now being injected into people's systems by compromised routers.



And so anyway, he concludes with "Campaigns gone missing:  Perhaps the biggest change in cryptojacking-related activity is the lack of new attacks and campaigns in the wild targeting vulnerable websites."  He says:  "For example, in the spring of 2018, we saw waves of attacks against Drupal sites where web miners were one of the primary payloads."  And in fact we've addressed the observation that that's changing now previously.  He says:  "These days, hacked sites are leveraged in various traffic monetization schemes that include browser lock, fake updates, and malvertising.  If the content management system is Magneto or another ecommerce platform, the primary payload is going to be a web skimmer."  And we know that those are trying to obtain the user's credit card information as it's submitted to an ecommerce site.



He says:  "We might compare cryptojacking to a gold rush that didn't last too long as criminals sought more rewarding opportunities.  However, we wouldn't rush to call it fully extinct.  We can certainly expect web miners to stick around, especially for sites that generate a lot of traffic.  Indeed, miners can provide an additional revenue stream that is as concluded in this Virus Bulletin paper" - and they have a link to it that says - "dependent on various factors including, of course, the value of cryptocurrencies which historically has been volatile."



So I think that's where we are.  The Coinhive died.  That was the preeminent cryptomining tool.  Everybody was using it.  There were also-rans. They now have a larger share.  And then there's this WebMinePool, which you guys in your network, Leo, you're probably glad are unable to access, thanks to your protection.  It's alive and well.  So I think, just like all of the worms that have existed in the past, the malware that's out there, there's, like, a place for and a diminishing presence of cryptojacking.  And it's never going to go away.  It'll sort of fade into the past.  It doesn't make the money that it once did, so the world has moved to things like the ransomware in order to attempt to get into people's systems and extort money.



So anyway, sort of a podcast all about web browsers this week, just as a consequence of how much browser stuff is in the news.  And of course we know that it makes sense for us to talk about that because that is the surface that we visit the Internet via.  It is the attack surface that we expose.  So we've got to keep our browsers secure.



LEO:  It's better than spending every week talking about Adobe Reader, so that's - there's improvement.



STEVE:  Yeah, there has been some drift, hasn't there.



LEO:  There's been a little drift.  Well, that's what you see.  You see how they started securing Windows pretty well, so the attacks moved to Adobe, Flash, and Reader.  And now I guess, I think probably it's not that Adobe's secured it so well as that most people just stopped using those; right?  Flash is dead, and you don't even need Reader if you have Edge.  So now, well, let's go after the browsers.  Of course, makes perfect sense, yeah.  Although I think all the browser companies are doing a pretty good job of securing them.  So you think making Edge Chromium-based will make it easier to secure?



STEVE:  Yes.  And I really would love to know what's behind that.  Have you and Paul and Mary Jo, like, have they provided some illumination into what's the thinking behind the scenes?



LEO:  Oh, yeah.



STEVE:  Why, after this huge investment in their own HTML engine, they said, uh, okay.



LEO:  No, and in fact they've talked more about it at Build this week.  And it's pretty clear that the issue was Edge adoption was really lagging.  And it just makes more sense to unify behind a single engine.



STEVE:  Yeah. 



LEO:  And Microsoft doesn't - it's a very different Microsoft.  I don't think they care about proprietary lock-in as much as they used to.



STEVE:  Wow.



LEO:  I know.



STEVE:  I'm taking a while to get used to that change.



LEO:  You saw they're putting a Linux kernel in Windows.



STEVE:  Yes.



LEO:  Did you see that?



STEVE:  A full Linux kernel, yeah.  And I thought it was a joke. Someone tweeted it to me this morning.  They're jumping up and down because Windows is going to have a console with tabs.



LEO:  Oh, yeah, a nice terminal, a tab terminal.



STEVE:  I mean, I thought it was a joke, like wait a minute, what?



LEO: No, it's such a different Microsoft than the one you and I are used to.  And I think that's a good thing.  They're smart.  I mean, it's turned them into trillion dollar company.  I mean, they're clearly - the market's embracing what they're doing, and I think it's smart.  And this is just part of it.  Why spend a lot of energy on something really nobody was using?  I mean, it was in a single-figure percentage adoption, even though it was the default browser. 



STEVE:  Wow.



LEO:  I know.



STEVE:  So a deliberate attempt to say, okay, I don't want this, I want Chrome.



LEO:  Yeah.  Or something, yeah.



STEVE:  Wow.



LEO:  So, yeah, I think it was the right thing to do.  And you know there's still a lot of questions.  I think Microsoft's smart to build an Internet Explorer mode in, so that way they can eliminate that additional install plus serve all those people running legacy code that runs in IE8 only, you know.  But I also think that they - I think it's a different company.  They used to say, well, the big change was first we want to be everywhere our customers want to be.  Nadella would say this.  But the best experience will always be in Windows.  Now they don't even say that.  They've moved on.



There was a - I'm trying to find it, but I don't see it.  But it's worth searching for, if you're interested, an interview I think Nadella or the Edge team gave about the decision, this last couple of days at Build.  And it makes sense.  We did talk about it on Windows Weekly.



STEVE: Yeah, cool. 



LEO:  Steve, we done.  Thank you for a very compelling and interesting show, as always.  People who are interested in this matter, and I think we all are, should tune in every Tuesday, around about 1:30 Pacific, if we don't have a Google I/O to get in the way.  That's 4:30 Eastern, 20:30 UTC.  The live streams, audio and video, are available at TWiT.tv/live.  You can also ask your voice device, you know, if you ask your Echo, listen to TWiT Live, it'll start playing whatever's going on.  Or you could say listen to Security Now! podcast, and it'll play the most recent version of the Security Now! podcast.  For some reason, I don't know, I'm a little weird, I say "listen."  Most people say "play," like they're commanding it to play.  For some reason I think "I want to listen to."



STEVE:  Yeah.



LEO:  It's just me, I guess.  I'm weird.  So say "play," it's okay, that works, too.  You can also download a copy of the show  from Steve's site.  So you have a regular 64Kb version, but you also have a super small MP3 for people who don't want to waste bandwidth.



STEVE:  Actually, no.  I have...



LEO:  You used to have a 64Kb version; right?



STEVE:  Yes.  I have the super small one.  But the standard size one just links to yours.



LEO:  Right, right.



STEVE:  And so it's just a convenient length for that.  And I do bounce through Podtrac, even for the small download, so that we get credit for the fact that the podcast was downloaded.



LEO:  Thank you.  We asked you to do that, and I really appreciate it, yeah, because we want to make sure everybody knows how many people listen, including our advertisers.



STEVE:  Yeah.



LEO:  GRC.com.  He does have something that you can't get anywhere else.  That's those great transcriptions written by Elaine Farris.  So if you like to read along while you listen, and I know for a lot of people it helps in the comprehension, GRC.com.  While you're there, get a copy of SpinRite 6, the industry's number one hard drive recovery software.  It's good for maintenance, too.  That's Steve's bread and butter.  Everything else there including ShieldsUP! is free.  I'm just looking at the site:  102 million shields tested, and counting.



STEVE:  Yeah.



LEO:  It's kind of amazing.  GRC.com.  You can leave feedback for Steve or questions at GRC.com/feedback.  Or tweet him.  He accepts direct messages at @SGgrc.  That's his Twitter handle:  @SGgrc.  We have audio and video of the show at our website, TWiT.tv/sn.  And of course the best thing to do would be subscribe.  Then you don't even have to think about it, but just get the new Security Now! the minute it's available very Tuesday evening.



STEVE:  Yup.



LEO:  Thanks, Steve.  We'll see you next time on Security Now!.



STEVE:  Thanks, Leo.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#714

DATE:		May 14, 2019

TITLE:		Android "Q"

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-714.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we look at a widespread problem affecting all WhatsApp users, many interesting bits of news arising from last week's Google I/O 2019 conference, a worrisome remotely exploitable flaw in all Linux kernels earlier than v5.0.8, the just released hours ago new set of flaws affecting all Intel processors known as ZombieLoad, a bit of miscellany, and some odds and ends.  Then we take a deep look into the significant security enhancements Google also announced in their next release of Android:  Q. 



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  He's going to do a rundown of some of the nice new security features in Android Q.  Some rare praise from Mr. G. for Mr. Google.  We'll also talk about that WhatsApp flaw.  A new attack on Intel processors called ZombieLoad.  I love the name.  And some very nice features coming to Google Chrome soon.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 714, recorded Tuesday, May 14th, 2019:  Android Q.



It's time for Security Now!, the show where we cover your privacy, your security, all that stuff, with this guy right here, this cat.  This guy is all about security, Mr. Steve Gibson of the Gibson Research Corporation.  Hello, Steve.



STEVE GIBSON:  Mr. Laporte.



LEO:  How are you today?



STEVE:  Great to be with you once again for Episode 714.  This is the second Tuesday, as I have been saying we would get to, as close to the middle of the month as we're able to have a Patch Tuesday.  I have no idea what's happening on Patch Tuesday because everything else is...



LEO:  Wasn't last Tuesday Patch Tuesday?



STEVE:  No, because Wednesday was the 1st, so we missed it by one day.  Which makes this the second Tuesday of the month.  So I did see that there's lots going on.  We'll cover that, if there's anything particularly interesting, next week.  In the meantime, everybody knows don't wait too long because lord knows what just got fixed.



LEO:  Oh, yeah.  I see cumulative update for .NET framework for Windows 1809, security update for Flash Player, cumulative update for Windows 1809, malicious software removal tool.  Yeah, so it's the usual bundle of goodies.



STEVE:  Oh, yeah, the usual suspects.  I started out with - let's see.  Oh, I was going to talk about a forthcoming change in Chrome's policy for cookies, which really represents some nice forward motion.  But then I started looking more closely at last week's Google I/O announcements of the security, well, security improvements - I was going to say security and privacy, but, no, just security - in Android's forthcoming Q release.  And I was so taken by them and so impressed by them that I thought, okay, that's what we're going to end the podcast talking about is I want to spend some time looking at the forward movement over time and what I consider to be a tremendous and tremendously useful focus that Google has had on Android.  I mean, they don't really have to.  But I guess they can because they're Google, and they've got resources exuding out of their pores.  And so why not really make it as good as they can?  And, boy, it's - where are we?  We're at Q.  Well, okay.  I don't know what happens when we run out past Z.  But of course we don't even know which dessert Q is going to be.  So that's still the pending question, I think.



But anyway, lots to talk about.  We've got a widespread problem affecting all 1.5 billion WhatsApp users.  Not super worrisome because it's going to be used in targeted attacks, but the back story is interesting.  We've got, and I heard you talking about it a little bit on MacBreak Weekly, we have many interesting bits of news arising from last week's Google I/O 2019 conference surrounding the changes being made to Chrome.  And since Chrome is the most used browser, and it's based on Chromium, that now, of course, as we know, Microsoft has picked up with their next version of Edge, this becomes extra interesting.



We've got a worrisome remotely - listen to this, children - remotely exploitable flaw in all Linux kernels earlier than 5.0.8.  It's not the end of the world because it's very difficult to do.  But it's in there.  So end users, again, aren't going to be in a huge problem because it's a network-level exploit.  But that does mean that Internet-facing Linux servers need to get themselves updated because what's difficult to do now becomes script-kiddie fodder six months from now.  So you don't want to be caught with your kernel down.



We have also the just-released hours ago, I mean, like this morning it came out of embargo, it's from some people, some researchers we've covered extensively.  They were the guys that did the Rowhammer attacks earlier.  Back last March, not this most recent one, but still a fond memory, March 2018, in the middle of the month, they found another way of exploiting the microarchitecture of all Intel chips since 2011.  And they said, uh, Intel?  Anyway, so the news of that has been embargoed until this morning.  So it's called ZombieLoad.



LEO:  Oh, geez.  That's a great name, man.



STEVE:  That's right, ZombieLoad.  And there's a 15-page research paper that I have not yet had a chance to dig into because it just came out of embargo.  So but I have a sense for it, and we'll talk about that.  We've got a bit of miscellany, some odds and ends, and then we're going to take a deep look at the really significant security work that Google has been doing on Android.  And I conclude by saying that this is becoming a serious asset for humanity.  



LEO:  A serious asset for humanity.



STEVE:  Unlike the microwave that is our Picture of the Week that is a serious impediment to dinner.



LEO:  You want me to show that now?  "Please wait while we update your system to the latest firmware.  When it completes, you'll see the screen go blank as it restarts.  It may take up to 30 minutes."  I want my Hot Pocket.  You think that's real?



STEVE:  I think it's real.  It looks like, well, it was tweeted to me by one of our listeners who said, "I've got to wait until my oven installs new firmware before I can make dinner."  And he said:  "#TheFutureIsNow," meaning, uh, yeah.



LEO:  All right.



STEVE:  So it's not crucial unless you happen to be a human rights activist attorney-like person who might be subject to targeted attacks.  But what came to light is that there was a flaw in all mobile versions of WhatsApp for both Android and iOS, both the standard and the business editions, as well as the Windows phone, which leveraged a buffer overflow in the VoIP, the Voice over IP protocol stack, which allowed a remote code execution when specially crafted packets were received by the WhatsApp app.



And what this meant was that, even without answering your phone, a remote code execution vulnerability could be leveraged to install, and it was being used to install, some very potent spyware known as Pegasus which is produced by an Israeli, I don't want to call them "malware maker."  It's the NSO Group that was just purchased for a billion dollars.  So they're a legitimate organization.  They sell their stuff, they say, only to legitimate law enforcement agencies, yet some of their attacks have been spotted being misused.  That is, being used for purposes that they claim they would not sanction.  On the other hand, you wonder whether an organization like this doesn't probably look the other way and say, well, you know, they told us they wouldn't do that.



Anyway, there are updates to WhatsApp.  This came to light last Friday.  And the communications from WhatsApp and Facebook has been a little bit confused and muddled.  There was some statement that said that it was fixed on Friday.  But then another group within Facebook said that they changed the WhatsApp servers to mitigate this on Friday, but that just yesterday there was a new release for all of the WhatsApp code across the board.  So anyway, it's good that this was found.  We don't know how long it has been there, and who has been a victim.  But it did let me know that there are 1.5 billion instances, 1.5 billion installs.



Oh, and I meant to say you do not even need to answer the incoming call.  So the idea would be that the perp can target you by your WhatsApp, and you don't even have to be there.  They're able to turn on the camera, rummage through your device, listen to what's going on.  I mean, basically it's installing this Pegasus advanced system penetration malware which is able to jailbreak and/or root any targeted mobile device.  So that's been shut down.  We don't know what other tricks these people have up their sleeve.  It always seems that as many of these things as we foreclose, new ones are being found.  But at least this is one fewer that is in operation.



Many things were announced last week at Google's I/O conference.  And several of them, as I mentioned, I want to talk about Android Q at the end of the podcast.  But there was an interesting change that has been announced and will be forthcoming two versions of Chrome from now, which represents a difficult, yet useful change to the handling of cookies.  It's difficult, I think, because any change to something as ubiquitous as the way cookies are handled in our browsers is going to be difficult.



Yet there is a push, and I'm glad to see it, to sort of fix the problems, the acknowledged problems with cookies.  Google has a person there who's been sort of on the forefront of this, spearheading the effort, named Mike West.  And last Tuesday, on the 7th, the IETF Network Working Group Internet Draft titled "Incrementally Better Cookies" was published.  And he recognizes that incremental is all we can get, but incremental is better than nothing.



So let me explain all of the background here because there's a lot going on.  So in the introduction to this, Mike writes:  "The HTTP State Tokens" - now, that's something we had never talked about before.  That's like the dream is to do something other than cookies for HTTP state.  As we've talked about, the whole reason that Netscape originally introduced a cookie to HTTP queries was because the way the browser and the server works, it's a stateless interaction.  The browser makes a query.  The server answers the query.  And that's it.  And then the browser makes another query.  And then the server answers that query.  But there's nothing to tie those events together.



So, for example, there was really no way to have the notion of a session, like a persistent session that spanned queries.  There was no notion of a way to log into the server.  All of this, thanks to Netscape, we got with cookies.  Unfortunately, we got a lot that we didn't bargain for with cookies, as well.  As I have often bemoaned, the fact that they were never meant to be used in a third-party context, only in a first-party context - and of course it's famously the third-party context that has created tracking.  But on the flipside, it's created revenue which fuels organizations like Google, which is able to produce good things and is, over time, I really do feel, working to improve some of this foundation that we have.



So Mike says:  "The HTTP State Tokens proposal aims to replace cookies with a state management mechanism that has better security and privacy properties."  Arguably, cookies have neither.  He says:  "That proposal is somewhat aspirational."  He says:  "It's going to take a long time to come to agreement on the exact contours of a cookie replacement, and an even longer time to actually do so."  So everybody here is being realistic about this.



He says:  "While we're debating the details of the new state management primitive, it seems quite reasonable to reevaluate some aspects of the existing primitive:  cookies."  He says: "When we can find consensus on some aspect of HTTP State Tokens, we can apply those aspirations to cookies, driving incremental improvements to state management in the status quo."  Meaning move these things forward.



So he writes:  "Based on conversations at the HTTP Workshop 2019 and elsewhere, I'd suggest that we have something like agreement on at least two principles."  So two things have emerged.  He says:  "First, HTTP requests should not carry state along with cross-site requests by default."  And when I read that, I have to say I did a double-take.  It's like, what?  HTTP requests should not carry state along with cross-site requests by default, which is to say, okay, third-party cookies should not be.  Which would, like, what?  Anyway, and then the second thing is "HTTP requests should not carry state over non-secure channels."



Okay.  So that's a little bit less aggressive hope, the idea being, I mean, for example, and we've talked about, back in the day of Firesheep, it was because cookies that you negotiated over a secure channel, when you were doing username and password back and forth, those cookies then continued to persist on the nonsecure transactions with, for example, Facebook and other websites.  And because they were nonsecure, you could sniff them and then impersonate users.  It's hard to imagine that it wasn't that long that was happening.  But we fixed that by switching over to HTTPS and TLS connections.  Everything is encrypted now on all major websites.  And it's possible to mark a cookie with the SECURE tag, which will tell the browser "never send this except over a secure connection" in order to prevent its leakage if someone asks for a nonsecure asset from the same server.



So those are the two things, the two principles he's suggesting that they have something like an agreement on.  So but the one that really brought me up short was this notion of not carrying state for cross-site requests.  So traditionally cookies have had two tags that they could be set with.  One was HTTPONLY, and the other was SECURE.  I just talked about the SECURE tag.  If a cookie is tagged as SECURE, the browser knows never to send it with a nonsecure query.  The other one is HTTPONLY.  What that says is only allow the cookie to be seen over HTTP.  And what that means is specifically blind any scripting.



So the idea being is keep this away from JavaScript because we know that there have been ways to inject JavaScript into web pages where we didn't expect it to be injected.  And if malicious JavaScript got onto a page, and it was able to query the cookie, and in fact JavaScript is normally able to query cookies that it has potentially - like it's supposed to have access to.  Like if malicious JavaScript got into the domain of a secure cookie, it could see it.  So flagging the cookies, so when the server sends the cookie to the browser, if it marks it, tags it as HTTP only, then the browser won't let any script running even on that site's own page.  The web server is saying, I don't even want our own script to be able to see these cookies.  Only send them back with queries so that we have state maintenance, and for no other purpose.  Anyway, so those have been the two existing tags.



Well, it turns out that there is another newer one which is already supported by all of our browsers except IE.  So all of the significant browsers - and I don't know, maybe Microsoft will update IE, or maybe they're just going to leave it at 11 and never push it any further.  I don't know.  And that tag is SAMESITE.  SAMESITE is optional, as are these other two, HTTPONLY and SECURE.  And it has, if it's not present, which is the normal case, it has one meaning, and then it can be explicitly given three different meanings.  So here's what they are.  This SAMESITE tag can be set to None, to Lax, or to Strict.  If it's not set, then previously, or I should say right now, if it's not set then that's the same as None.  That is, if it's not present at all.



What Google is proposing to do in, I think we're at Chrome 74, and so this will be appearing in Chrome 76, is they're going to change its "not present" meaning from None, which it's always been until now, to Lax, which is more strict than None, but less strict than Strict.  Okay.  So here's what's happening.  If the cookie is tagged as being Strict SAMESITE, then that says that it should only be sent to the - it should only be sent back to the server in a first-party context.  That is, essentially, do not allow this cookie to be sent in a third-party context, meaning if any asset on another site refers to the domain that set the cookie, for example an image, for example.



So Site B shows an image that it has pulled from Site A.  The normal behavior would be for the browser that has cookies for Site A to return those cookies along with its query for this image from Site A.  But if that cookie has been received with a SAMESITE set to Strict, the browser will know not to do that, not to send cookies out for peripheral assets, you know, images and other widgets, to third-party servers.  So, and that's an arguably useful security benefit because there are a number of ways, like cross-site request forgeries, that malicious script and queries can break this cookie protection.  So that provides some protection from that.



So setting the SAMESITE to Strict means, essentially, restrict this cookie for first-party access only.  If it's set to Lax, which it currently would have to be explicitly set for, not two Chrome versions from now but at the moment, if it's set to Lax, then SAMESITE cookies are also withheld on cross-site subrequests, like we were just talking about, like images and things.  But not if the user navigates to a specific URL from some other site.



So, for example, by following a link, if the user clicks a link, then that cookie that the browser has would be sent with the link.  So, for example, to immediately identify the user as having a logged-on session with a site that the user is going to.  So that's the so-called Lax handling.  So passive things are still blocked, but links are still going to be - will be honored.  So but that also says that, under strict handling, a URL click would not provide the cookie.  It wouldn't be until you actually are on the site and you have a page that then cookies would be first-party.  And of course the default behavior is either it's not set, or you can say SAMESITE=None.



Okay.  So what Google has announced they're going to do, they announced this last week, is in two releases from now they're going to change the default "no specification," that is, where cookies are set with no tag at all, no SAMESITE specification, they're going to change that from the way it has always been, which is to say, unless we're told to make it strict or lax, we're going to assume it's no protection.  They're going to change that to Lax, which means suddenly all of the passive queries for things like images, which of course are web beacons, right, I mean, those are the famous little one-pixel queries that have always been used for tracking, those all go dead.  They all go silent two versions from now.



So essentially Google is saying to all of the organizations and all of the tracking systems that exist, if you want to continue receiving cookies from all of the debris that you spread out all over the Internet, you need to start switching those cookies to explicitly say SAMESITE=None so that they will continue to be - so the users' browsers will continue sending back the cookie content for the domains they have in a third-party context.  And that's a huge change.  I mean, that's significant.  I'm, like I said, when I read that it was like, whoa, okay, really.  So again, props to Google for moving this forward.



Google has also said at the announcement time that what this does is it serves to mark cookies as being explicitly third-party, that is, right now there's no marking.  As I have always said, this is sort of a mistake that this was allowed to happen this way, that has been well leveraged.  And of course it glues the whole Internet together these days, so it's not like it's going to go away.  But what this does is it marks cookies as explicitly intended for third-party access, which then allows Google to start doing some instrumentation.  They've said they're wanting to experiment with showing cookies to users, giving users more explicit control over this.



And so requiring tracking cookies to essentially declare themselves as being for that purpose is an interesting piece of instrumentation that Google hasn't had until now.  And again, as we've seen, Google tends to be the leader in this.  They can afford to be because they're the majority browser on the Internet.  And this is all in Chromium, so presumably Edge will inherit this.  And I would imagine that Firefox, with the Mozilla team, will probably be following along.  Everybody understands the SAMESITE tag except for IE.  And so it's a trivial change, essentially, to just change the "there's no explicit statement about this, so we have been considering it as being promiscuous" with None, no SAMESITE exclusion.  We're just going to change how we handle that unless we're told otherwise.



So I think what will happen is we'll quickly see, I mean, it doesn't foreclose anybody, it doesn't prevent anybody from immediately marking their cookies as SAMESITE=None.  But it means they're going to have to do so if they want things like passive images on web beacons and so forth to be able to be used for tracking people.  They'll have to be explicit about it.  And so I think that's all for the best.  And again, props to Google for pushing this.



They've also announced that they're going to do some anti-fingerprinting in Chrome.  We don't know yet exactly what that means, what they're going to be doing to change.  We've talked about fingerprinting in the past a number of times.  Fingerprinting is - there's no way to consider this other than being sneaky, a sneaky attempt to track users in a way that was absolutely and definitely never intended.  You could argue that tracking users with a cookie is, well, okay, it wasn't intended, but your browser received a unique token from a website.  And now, as you go other places, and that website is somehow able to keep some content on those other sites that refer back to it, well, then it gets that unique token, and that's where tracking comes from.



Not so with fingerprinting.  Fingerprinting uses a whole set of sort of obscure, never intended for identification things, like the user-agent header that the browser sends back with every query.  We've talked about that the last couple of weeks because user-agent headers have been, you know, Edge is changing what it declares itself to be, depending upon which sites it's visiting.  Those user-agent headers often have a bunch of version numbers in them.  And while many browsers will have the same version numbers, many more other browsers will have different version numbers.



So although the user-agent header can't be used to uniquely identify a single browser, what it can do is, in the whole set of browsers in the world, it can immediately create a subset of those that that one browser occupies based on its unique, well, not its unique, it's based on a user-agent header that it shares with a subset of the world's browsers.  And then, for example, it turns out that different web browsers issue their headers, their query headers in slightly, subtly different sequences.  Like there's date and user-agent and itag and a bunch of other standard headers.  Turns out that different browsers just send them out in a different order, since the order doesn't matter to the HTTP protocol.  So that's another little signal that somebody who's trying to fingerprint a browser can use in order to further subdivide the set of browsers into a successively smaller set.



And we talked about, for example, it's possible to enumerate the fonts that are installed in a computer.  Once again, many computers will have exactly the same set of fonts installed.  But then again, many computers will have different sets of fonts installed.  So you can sort of do all of these overlapping Venn diagrams and, with a surprising amount of specificity, narrow a browser down to, not probably to one, but to many fewer than all in the world, in a way that was entirely unintended by anybody.  But once again, as we've seen, people are clever who are motivated to be so.



So Google announced last week that they're going to be blocking certain types of fingerprinting.  We don't know what that means.  Maybe they're going to randomize the order of the fields in the user-agent.  Maybe they're going to randomize the sequence of headers in their queries.  Who knows what they're going to do?  But they're going to do something.  The first major browser which really worked hard to block this fingerprinting technology was the Firefox version that Tor was using.  Of course, if anywhere you would want to not have your browser, which you're using through the Tor anonymizing network, to be basically allowing you to be whittled down from anyone in the world to a tiny little subset of possible users based on fingerprinting.



So anti-fingerprinting technology is relatively mature.  There's been a lot of work done to thwart the deanonymization of users, thanks to the work that the Tor version of the Firefox browser has adopted.  So we don't know yet what it's going to be.  I'm looking forward to finding out.  I imagine in time, once they finally bring this to us, we'll find out what's going on.  As Google phrased it when they were explaining this, they said:  "Because fingerprinting is neither transparent nor under the user's control, it results in tracking that doesn't respect user choice."  They said:  "This is why Chrome plans to more aggressively restrict fingerprinting across the web.  One way in which we'll be doing this is reducing the ways in which browsers can be passively fingerprinted, so that we can detect and intervene against active fingerprinting efforts as they happen."



So again, it'll be real interesting to see what they do because there are so many little signals, like I was explaining.  I just gave a handful of them.  Panopticlick is the site where you can visit it, and many people have.  And it does fingerprinting for you, and it shows you how many bits of entropy your browser has been essentially reduced down to.  And what we would expect then is, when visiting the Panopticlick site with this next version of Chrome, that suddenly it's going to be far less sure than it has been in the past about who we are.  So again, bravo.



And one more neat thing, that I hope other browsers are going to get, coming in Chrome is no more messing with the Back button.  I'm sure all of us have entered a search term into our favorite search site and received a list of search results.  We look them over using our prior experience of various domains that we recognize and then click on one that looks like maybe it's going to have what we're looking for.  We browse around a little bit, maybe just look at the first site, and then press our browser's Back button.  And nothing happens.  Or maybe we go to a different page on the same site, not one that we were at before.  In other words, the site we were at has been screwing around with the Back button.



I've had this happen at Microsoft's developer site pages due to the redirection system they built.  Microsoft has created quite a tangle with their attempt at a browser-based centralized identification and authentication system where you get redirected and bounced around over to Live.com and then a few other places.  And then the problem is redirects look like, to the browser, places you have visited.  So when you hit Back from one of those, you don't go back to where you were before.  You go nowhere.  Typically, it bounces you forward again to the same page that you're on.



So that's annoying.  And of course what I've taken to do, and what I imagine many of our savvy listeners have done, is you hold the Back button down, and after a little bit of an "are you sure" delay, you get then a dropdown list of all the URLs that are in this - essentially it's a most recently visited list, an MRU list, showing the URLs that it has in a stack.  And typically what you'll do, for example, say that you were at Google, using Google search, you'll see several other weird-looking URLs and then finally Google's logo.  And so I'll choose that one, and now I'm back to the search results.



Anyway, so there's a way around it.  Sometimes, and I do this at Microsoft, if you hit the Back button several times quickly, you're able to step back before it's able to redirect you forward again, and so you're able to sort of hit back a couple times quickly to escape from that trap.  Anyway, what Google announced is - and it's weird, too, because this has been a problem forever with browsers, ever since JavaScript was able to manipulate the stack.  And that's what's happening is that JavaScript is able to push URLs onto the prior pages visited stack so that, when you hit the Back button, it takes you back to a page that the page you were on specified, rather than actually back to the page you feel that you were on before.



So again, not a hard problem to solve.  Thank you, Google, for bringing this to us with Chrome.  They're going to mark items on the stack which were visited as a result of previous user interaction, which is exactly what we mean when we hit the Back button.  Which is to say, if I hit Back, I'm taken to the previous URL, the most recent previous URL where I was actually doing something, where interaction took me away from it.  So they're going to fix the Back button to prevent this from happening.



And in some of the editorializing I was reading about this, there was some hope that maybe Google, thanks to all the instrumentation that they have in Chrome, would actually notice those sites that were manipulating that stack to their users' annoyance, and maybe ding them in search results so that they're discouraged from messing with the previous pages visited stack.  So we can hope for that.



LEO:  This will do nothing to fix the problem of you must reload this form to continue, probably.



STEVE:  No, unfortunately.  That's...



LEO:  That's annoying, too.



STEVE:  I know.  That's a very good point, Leo.  That is annoying.



LEO:  I understand why that's not really related, but...



STEVE:  Yeah.  So as I mentioned at the top of the show, all Linux kernels before v5.0.8 are vulnerable.  The good news is a very difficult to exploit, yet the bad news is a very potent, if it can be exploited, vulnerability that can be used for remote code execution.  So this is known to the Linux community.  A patch was released in late March and in mid-April.  So it's a very complex problem.  It was given - the NIST assigned it a high severity score of 8.1, yet a low exploitability index of 2.2.  So it ended up netting out at an impact of 5.9 in this rating system that we have.  So individuals have nothing to worry about.



But I would argue that cloud-based servers, because as we know, things that start out being difficult to exploit end up being figured out over time.  And it's certainly going to be the case that there will be Linux systems on the Internet that are not going to be updated, that no one is paying attention to, that are just sitting there, running, doing their job, that somebody is going to be poking at.  It takes advantage of what's known as the RDS, the Reliable Datagram Sockets protocol, which is transported over TCP.  There's a use-after-free problem with it that can be exploited.



So I guess I would say to our listeners, if you are responsible for Linux servers that are Internet facing, you probably already know about the problem.  You have probably already updated yourself.  If not, probably do so.  It's not a "run around hair on fire" kind of problem, but definitely worth doing.  I expect that a few months from now we'll be talking about cryptocurrency miners being installed on systems that didn't get updated because the bad guys figured out how to do this.  I mean, but it allows a 100% complete takeover of the server remotely.  Again, I mean, it would be a real problem.  It would make Heartbleed look like nothing by comparison, if it were easier to do.  But as we've seen, these things only get easier over time.  Which leads me into ZombieLoad.  



LEO:  Whoo.



STEVE:  Okay.  So as I mentioned, I don't remember if it was before we began recording, Leo, or at the top of the show.  But this just came out of embargo this morning.  So I've got links in the show notes for anybody who wants to dig into it.  I will cover it in some detail next week.  I've been working on this podcast all morning, so I didn't have a chance to get into it.  I have verified, however, that this day's, that is, today's Patch Tuesday, probably by no coincidence, has the updates for the firmware that Intel has been working on for a year.



Intel was informed of this more than a year ago, on March 18th, by the guys who first verified that this was possible.  If you did not have updated firmware - and all versions of Windows 10 64-bit now as of today, today's Patch Tuesday, do have updated firmware on all processors where this is a problem.  These are all machines made since 2011, so the last eight years' worth of Intel processors.



This is very reminiscent of Meltdown and Spectre, and it's considered to be positioned somewhere between the two.  Less easy to do, that is to say more difficult, than Meltdown, but much easier than Spectre in terms of pulling it off.  There is proof of concept available.  The nature of what's happening is it's, again, it's a cross-core, or, I'm sorry, a cross-thread common core exploit.  So, for example, turning off hyper-threading, where two threads are able to share the same physical core, that would be the only recourse if we didn't have now firmware available.



Intel has known about this, as I mentioned, for more than a year.  So as this comes out of embargo, it's synchronous with available firmware across the board for all processors.  I would imagine that, since Linux is able to install firmware at boot, that it will be updated.  And as you mentioned, Leo, macOS has been updated.  We now know that as of today Windows 10 is being updated.  I imagine that Windows 7 users are probably left out to dry because Microsoft wants us all to be using Windows 10.



We'll see where this goes.  But it's also the case that, if you have a motherboard from a reputable manufacturer, just as happened last year for the Spectre and Meltdown firmware updates, you could well see, like for example Dell is maintaining their systems to a high level of firmware release from Intel.  So I wouldn't be at all surprised if Dell laptops and motherboards, and probably Lenovo also, do get some BIOS updates.  That's the way we'll see them in systems where we're not able to get them updated through Windows 10 in Microsoft, is if our manufacturers support updating the BIOS.  Then the BIOS is also able to patch Intel microcode on the fly.



So I'll know more next week.  I don't know that there'll be much more to say.  It is yet another microarchitectural flaw in the - really, for so long, as we've discussed, Intel was able to leverage lots of ways of speeding up the systems.  But doing that left behind some state which affected other threads.  And researchers coming along after the fact have been able to sense the state that was left behind by other threads.  And in fact in the proof of concept, it's possible for one thread to determine previous websites that another user has visited, even though, for example, in a cloud setting you would hope to have much better virtual machine isolation.



This just breaks VM isolation in a way that is able to leak browser history, user-level secrets, passwords, keys, and system-level secrets, including disk encryption keys.  So, I mean, it was - you could just imagine the poor Intel engineers who have their head in their hands, saying oh, my goodness, you know.  Can we not have any processor performance increases without it causing a problem?



So anyway, more on that next week.  I do have - no, I'm not going to go any further.  That pretty much covers it.  Because I had more written here in my show notes, but the fact that this has been patched for everybody who's on Windows 10 (64) means that it is not something that we need to be worrying about from this point forward.



I spent about four days last week adding support for Controlled Folder Access to my SQRL client for Windows.  Controlled Folder Access is something that we mentioned back when it was introduced and announced by Microsoft in the now-infamous October 2018, what is it, 1709 Fall Creators edition.  What was odd was that I had experimented with it and turned it on on one of my Windows 10 machines.  And my own SQRL client was saying that there was no - that it couldn't find its SQRL identity on that machine.  That is, the user's SQRL identity that I had previously loaded there.  And so it had been on my list of things to look at before I declared that it was finished.



And as I think I mentioned before, I'm like right at that finish line at this point, where I've had a number of releases, about once a week for the last three weeks, that I thought that I was declaring as being the release candidate for the client.  Anyway, finally I looked at it, and I realized that, if Microsoft ever got serious about Controlled Folder Access, then I'd have a problem that would confuse users.  And that's what I wanted to avoid.



And we've seen what Microsoft does historically.  Windows XP had a firewall, except there were lots of commercial firewall vendors at the release time of XP.  And I don't know what Microsoft's thinking was, but it was there, but it was off by default.  And our advice was always turn it on.  Anyway, it wasn't until Service Pack 3 of XP that Microsoft finally turned it on by default.  And by then it didn't have much impact because third-party firewalls had sort of gone away and weren't such a big deal anymore, or they figured out, I mean, they saw the handwriting on the wall, and they figured, well, we'd better go find out something else to do. 



What Controlled Folder Access does is it is blanket prevention for any program that Microsoft hasn't blessed, prevention against modifying any of what is typical user content.  So like everything in the My Documents folder tree - My Movies, My Photos, My Music.  Basically they're billing it as protection against ransomware.  As we know, what ransomware typically does is it wants to, and does if it can, encrypt all of the user's valuable content, their documents, the stuff that is not readily replaceable.  The Windows core system, nobody cares about that because you can reload that.  But you can't recreate your Ph.D. thesis if you're in the middle of working on it, and suddenly it's encrypted, and you don't have any current backups.



So Microsoft has this now in Windows 10 since 1709, the Fall Creators edition.  And it will probably always be there because they generally don't take things away that they put in.  It's off by default.  And it turns out that it's not ready for primetime.  When you turn it on, first of all, Microsoft says programs that Microsoft doesn't recognize will be blocked.  Well, okay.  So they didn't recognize my SQRL client.  I get that.  But they didn't recognize Internet Explorer, either, theirs.



So it turns out that, if you have it on much, you pretty much have to start whitelisting everything.  Well, the good news is there's a whitelisting facility.  And this thing's very noisy at the moment.  Like doing anything, you get these little notifications sliding out from the lower right.  And it says, "We just blocked something."  And it's like, yeah, no kidding.  Nothing works anymore.  But if you then click on that, you're taken to a user interface page where you're able to ask for a list of recently blocked apps.  And so it's easy to find what it was you were trying to do that they blocked and go, okay, yeah, well, that was me, on purpose.  This wasn't ransomware.  And so anyway, you start building up a whitelist of things that you're doing that it's blocking.



And I can understand how this being built in, many people will like it, will feel comfortable with it.  They're happy adding things to the whitelist.  And of course after a week you probably - you stop getting false positives because you've added everything that you're actually using to the whitelist.  And if disaster befell you, and you did get hit with ransomware, it wouldn't be able to access any of the stuff in the areas of the system where you're probably creating content.



Anyway, so essentially what I did was I incorporated an awareness of Controlled Folder Access into the SQRL client so that, when it's installed in any system from 1709 later, it preemptively gives itself access, that is, it whitelists itself at that phase of installation where the user has elevated its privilege to allow itself to be installed.  It also says, oh, while I'm here I might as well just put myself on the whitelist in case, whether Controlled Folder Access is enabled or not, so that at some future time either the user or Microsoft decides to enable it.



And this is my point about XP and the firewall lesson is that Microsoft generally sort of rolls these things out gradually.  So it's there, but it's not on.  They've got instrumentation watching everybody.  And so maybe they're sucking in data.  Maybe they're waiting for Windows Defender to get smarter about what things people are letting through.  Who knows what's going on behind the scenes.  But my point was I didn't want suddenly all SQRL users in the future to have their identities not seen.  So anyway, that's where four days went last week was adding an awareness of that, figuring out how to do that.  And that's in place now.



Now I return to working on the final document, which is what I had mentioned I'm now at work on, the SQRL Features, Capabilities, and Implementation document.  My plan was to work up, to essentially put those all on matured web pages.  But when I sat down to get to work, I thought, you know, I think I would just rather have this be in a document that people can download so that you can take it away, you can edit it, you can mark it up, you can do whatever you want.  And it sort of starts at the beginning and successively gets increasingly more detailed all the way down to the protocol implementation so that managers can read the beginning of it and understand what the features and capabilities are, and people wanting to implement it can start with that, but then go into much detail.  So once that's finished, the project is finished.



I will be giving a second presentation to a "local to me" group, and I've asked them for permission to mention it on the podcast.  I don't know how many people who listen who are in the Los Angeles area might be interested in attending.  But it is the Open Web Application Security Project, L.A. Chapter.  It's Wednesday after next, which is May 22nd, at the OWASP L.A. Monthly Dinner Meeting.  And so if you google OWASP L.A. Monthly Dinner Meeting, they organize through Meetup.com, and you could say that you want to be there, register yourself, and say hi to me as a Security Now! listener.  And I have another invitation pending at OWASP in Dublin.  I am in the process of getting my passport renewed so that I will be able to accept that invitation.



LEO:  Oh, good.  That's exciting.



STEVE:  Yeah, those are fun.



LEO:  How long has it been since you've been out of the country?



STEVE:  Leo, not since I was with you in Toronto and Vancouver.



LEO:  Oh, yeah.  I guess that's out of the country.



STEVE:  That's why I got my passport, when you said, "Hey, you know, you want to come and do some shows with me in Toronto?"  And I said, "Oh, I'm going to need a passport."



LEO:  Wow.  So I'm thinking Lorrie's going to take advantage of this, and you'll be going to Paris soon.



STEVE:  Well...



LEO:  You better.



STEVE:  We'll see.  If somebody in Paris wants a SQRL presentation...



LEO:  Oh, come on.  You can take her to Paris.  You don't have to have a business reason.



STEVE:  No, I've got work to do on SpinRite.  I've got a bunch of people who are saying get back to SpinRite.



LEO:  Oh, okay.  Good, good, good.  No, that's good.  Yup, that's good.



STEVE:  Anyway, and I did want to also mention that blog.grc.com, where I have been, did expire.  I had the domain-forwarding option, and so I posted yesterday that I had 24 hours left.  Anybody who now goes to blog.grc.com will come to the new blog which I've put up.  And I plan to explain, to lay out my roadmap for SpinRite, since it's beginning to take up a little space in my brain once again.



LEO:  Yay.



STEVE:  Since SQRL is pretty much behind us.



LEO:  Oh, hallelujah.



STEVE:  Yeah.



LEO:  I speak for everyone when we say 6.1, 6.1.



STEVE:  Getting back to it.  Yup, yup.  Yes, indeed.



LEO:  Yes, indeed.



STEVE:  So, okay.  A bunch of stuff to talk about with Android Q.  As I mentioned before, I was very impressed with the things that Google has been doing and the investment that they're making in this.  And so there are several different aspects.  One, as you mentioned, was Project Mainline.  Stephanie Cuthbertson, who's the Senior Director for Android, explained last week during Google I/O, she said:  "Your regular device gets regular security updates already, but you still have to wait for the release, and you have to reboot when they come.  We want you to get these faster.  Even faster."  And she said, "And that's why in Android Q we're making a set of OS modules updateable directly over the air, so now these can be updated individually as soon as they're available and without a reboot of the device."



And of course what that says is that they're doing something more like the model that we've now gotten used to with Chrome, where it's just sort of a rolling latest release, and it's being updated constantly.  Essentially that's what this is known as.  It's known internally as Project Mainline.  Google developers have spent the last year working to split 14 OS core components into separate modules.  So even though they're core OS modules, they will be behaving more like applications, which are able to be updated on the fly.  Google has said that it's going to be pushing the update to all devices which support the mechanism.



Essentially what it'll do is it'll be able to stop that subcomponent of the OS, to halt it, update it, and then restart it without essentially any impact on what the user is doing.  So no need to reset and reboot the OS.  It'll just do it on the fly.  So the modules are ANGLE, APK, the Captive Portal Login, Conscrypt, the DNS Resolver, the Documents UI, External Services, the Media Codecs, the Media Framework - and of course we know the media platform has been a big source of problems in the past, so it's very cool that those can be on-the-fly updated - Network Permission Configuration, Network Components.  And once again those are Internet-facing, so they tend to be problematical.



So again, if you wanted to choose things that you'd be able to update on the fly, that's where it would be.  Also the Permission Controller, Time Zone Data, and Module Metadata.  So those are all internal core services.  The user doesn't directly see those, so they're not like apps.  But they are often the components where the most security problems can be found.  So again, this was not an easy piece of work. 



What's interesting is that - and I wished it was going to be more available.  The Verge, who did some reporting on this, learned that as individual device makers will be able to opt out of the new feature, although I don't know why they would choose to, just seems like a big bonus and plus for their users.  But individual device makers will be able to say no, we don't want to do that.  Also it turns out that only devices which initially ship with Android Q will be able to have this functionality.



So although older devices will be able to be upgraded to using Q, they won't be able to take advantage of Project Mainline, I guess because it's just very tricky to do, and maybe there's some hardware-side support that's necessary.  Or maybe it's from a security standpoint. It could be that, in order to prevent any malicious abuse of this technology, Google has had to incorporate some things from the start that prevent it from being tampered with.  I'm just guessing there.  But for whatever reason, unfortunately, devices that aren't purchased with Android Q will never, although you'll be able to get all the benefits of Android Q, you won't get this particular on-the-fly updating of these components.  So anyway, still a nice step forward to Q from Pie, which is where we've been.



Also they announced additional encryption benefits.  They explained that storage encryption is one of the most fundamental and effective security technologies, as we know, but that current encryption standards require devices that have cryptographic acceleration hardware.  Because of this requirement, many devices are not capable of using storage encryption.  But remember that we talked last year with their innovation of Adiantum, which was their very cool storage encryption technology which, unlike AES, which is the technology that is often used - the problem with AES is that its pure software implementation is necessarily slow because the bit twiddling that has to be done to implement the AES cipher requires many instructions, many generic instructions in order to pull off the bit twiddling that AES requires.



That's why Intel has the so-called AES-NI instruction enhancements, "NI" standing for New Instructions.  And essentially those move into firmware in order to speed them up, the bit twiddling that the AES cipher requires.  So there is a very fast stream cipher which can be implemented in software, known as ChaCha20.  It only uses the simple basic instructions that are fast on all processors.  But the challenge there was to come up with a cipher which would not expand the size of the block.  Remember we talked about this before.  It's fine in a stream cipher, like over TLS, to have to add a little bit of padding or an initialization vector to the cipher.  In a communications protocol, you don't care if it makes it a little bit longer.  But you can't do that on a block storage protocol because there is nowhere extra to store any additional stuff.



So Adiantum is Google's solution for very fast, non-size-increasing encrypted content.  And we get that with Android Q.  In fact, all Android Q devices are going to be required to support full storage encryption because it'll be built into Q, and it's free.  It no longer costs anything in terms of performance, even if you are on a non-AES-NI-compatible or capable platform that would otherwise normally make it much smaller.



They said:  "Our commitment to the importance of encryption continues with the Android Q release.  All compatible Android devices newly launched with Android Q are required to encrypt user data, with no exceptions.  This includes phones, tablets, televisions, and automotive devices.  This will ensure the next generation of devices are more secure than their predecessors, and allow the next," as Google said, "the next billion people coming online for the first time to do so safely."



They also said:  "However, storage encryption is just one half of the picture, which is why we are also enabling TLS 1.3 support by default in Android Q."  Of course we've talked about that.  It makes handshakes quicker.  It supports a next generation of later, more secure ciphers.  It explicitly drops support from the older SHA-1 hash-supporting cipher suites and so forth.  So that'll also be built into Android Q.  So another very nice bump forward in security.



Under platform hardening, this is where I was most impressed.  I first have sort of an overview of that.  Then I'm going to dip into it a little bit further.  They said:  "Android utilizes a strategy of defense-in-depth to ensure that individual implementation bugs are insufficient for bypassing our security systems."  Again, individual implementation bugs are insufficient for bypassing our security systems.  So the idea being that they've recognized that, despite a history of attempts to write secure software, stuff still gets through.  So the solution is to acknowledge the reality that stuff is going to get through and to make the consequences of stuff getting through much less severe.



So they said:  "We apply process isolation, attack surface reduction, architectural decomposition, and exploit mitigations to render vulnerabilities more difficult or impossible to exploit, and to increase the number of vulnerabilities needed by an attacker to achieve these goals."  And I just love the phrasing of that.  It's a sober and realistic expression of the truth that we have seen of the challenges facing any highly targeted platform, and Android is arguably the number one most targeted platform now that exists.



They said:  "In Android Q we have applied these strategies to security-critical areas such as media, Bluetooth, and the kernel.  We describe these improvements more extensively," they said, "in a separate blog post," which is where I'm going to go next.  But the highlights include a constrained sandbox for software codecs, which is what we'll be focusing on in a second; increased production use of sanitizers to mitigate entire classes of vulnerabilities in components that process untrusted content.



Now, actually I've referred to this both in my own code and in general.  The idea is that you perform sanity checking on parameters.  When you have something that is parameterized, you say, rather than just assuming it's correct, you clamp it with a sanity check to say, wait a minute, does this make sense?  Can a bitmap image actually be this big?  I mean, is it sane?  And so you can catch a huge class of problems just by applying what they call "sanitizers" and I call "sanity checking."



They said also a shadow call stack which provides backward-edge control flow integrity.  Control Flow Integrity (CFI) is a mitigation that we've talked about also in the past.  Also protecting address space layout randomization against leaks using execute-only memory, which is again another means of hardening.  And just also tightening up heap-related vulnerabilities, making them much more difficult to exploit.  They explain under authentication that Android Pie, as we know, introduced the BiometricPrompt API to help apps utilize biometrics including face, fingerprint, and iris.  They said since launch they've seen a lot of apps embrace the new API.



Now with Android Q they said:  "We've updated the underlying framework with robust support for face and fingerprint.  Additionally, we expanded the API to support additional use cases, including both implicit and explicit authentication.  In the explicit flow, the user must perform an action to proceed, such as to tap their finger to the fingerprint sensor."  Or, if they're using face or iris to authenticate, for example, where they might already have their face in view, the user must still explicitly click an action button to acknowledge, to proceed.



They said:  "The explicit flow is the default flow and should be used for all high-value transactions such as payments."  And, for example, on the Android client for SQRL, the user is being asked to verify the identity of the site they're wanting to authenticate to and then acknowledge that explicitly - which is just a touch of a button on the screen in order to say yes - because, if we use the implicit flow, that would happen just instantaneously.  I mean, SQRL is instantaneous, virtually.  But in this instance we want to make sure that the user knows, has acknowledged that they're wanting to use their identity to authenticate to the site.  And so we want to just say, look, just make sure this is where you want to be.  And then you tap the button, and off they go.



They said:  "The implicit flow does not require an additional user action.  It is used to provide a lighter weight, more seamless experience for transactions that are readily and easily reversible."  So you can imagine if you have something that just doesn't matter, if the camera sees you, recognizes who you are, just allow it to happen without any intervention.  So that would make it additionally, for those sorts of things, really very smooth.



Then they finished up on that, saying:  "Another handy new feature in BiometricPrompt is the ability to check if a device supports biometric authentication prior to invoking BiometricPrompt.  This is useful when the app wants to show an 'enable biometric sign-in' or similar item in their sign-in page or in-app settings menu."  In other words, apparently until now there hasn't been a way for the app to determine whether that's available or not.  "So to support this," they said, "we've added a new BiometricManager class.  You can now call the canAuthenticate() method in that class to determine whether the device supports biometric authentication and whether the user is enrolled."



So again, very cool stuff.  And they did say - they gave us a little bit of a tease.  They said beyond Android Q they are considering, they're looking into an Electronic ID support for mobile apps, so that your phone can be used as an ID, such as a driver's license.  They said:  "Apps such as these have a lot of security requirements" - tell me about it, because that's exactly what I've implemented with SQRL - "and involves integration between the client application on the holder's mobile phone, a reader/verifier device, and an issuing authority backend system used for license issuance, updates, and revocation."  And of course, as our listeners know, that's what I explicitly avoided needing to have in the SQRL solution.  It's a two-party solution, not a three-party solution.



Anyway, they said:  "This initiative requires expertise around cryptography and standardization from the ISO and is being led by the Android Security and Privacy team.  We'll be providing APIs and a reference implementation of HALs [Hardware Abstraction Layers] for Android devices in order to ensure the platform provides the building blocks for similar security and privacy-sensitive applications.  You can expect to hear more updates from us on Electronic ID support in the near future."  And so Leo, what I still want to know, the outstanding question.



LEO:  Yes.



STEVE:  What dessert is Q going to be?



LEO:  No one knows.  There is nothing it could be, yeah.



STEVE:  Anyway, the last thing I wanted to touch on was the specifics of the security enhancements.  I've got a pie chart here in the show notes which shows from their own metrics a really interesting pie chart, the vulnerabilities by component.  And it won't surprise any of our longtime listeners that by far the largest source of vulnerability, this big huge blue chunk of the chart, which is easily a third of the whole pie, is media.



We know what a problem media is.  The codecs, the so-called compressors/decompressors, they are interpreters because media has become so complicated that essentially you are running an interpreter to read metadata and metatags in the media which then describes the content which follows, little blocks of content which follows, audio chunks and video chunks and so forth.  And so you're running interpreters which are just - it's the nature of developers to assume that the content they're receiving was authentically created by a compressor when they're being asked to decompress it.  It's just difficult not to think that way.  So fully a third on that pie chart is media.



The next biggest thing, not quite another third, but still large, is Bluetooth.  Number three is the kernel.  Number four is NFC, near-field communications.  Then the framework, the system, graphics, miscellaneous things.  Then we're getting down into little tiny pieces of the pie:  camera, WiFi, and other.  So Android Q, which is now available to developers in beta, has continued to tighten up its code and to harden its various attack surfaces.  And by examining the problems that they have found, they were, well, and rather than thinking, oh, well, that's a one-off that will never happen again, again I applaud the maturity of their approach.  They said, okay, we're having problems in this particular area of Android.  Let's assume we're going to have more problems in the future.  What can we do to fix it?  So number one problem was media.  Number two was Bluetooth.



So as their engineers explained, they said:  "In Android Q, we moved software codecs out of the main MediaCodec service into a constrained sandbox."  They wrote:  "This is a big step forward in our effort to improve security by isolating various media components into less privileged sandboxes."  And they quote a blogger, Mark Brand.  They said:  "As Mark Brand of Project Zero points out in his 'Return to libstagefright' blog post, constrained sandboxes are not where an attacker wants to end up."  Meaning breaking out of a codec you want to get into the kernel, which until now is where you've been able to get to.  No longer.



They said:  "In 2018, approximately 80% of the highly critical, high severity vulnerabilities in media components occurred in software codecs, meaning further isolating them is a big improvement.  Due to the increased protection provided by the new mediaswcodec sandbox, these same vulnerabilities will receive a lower severity based on Android's severity guidelines."  Meaning, again, if there's a problem, they're not going to hurt users so much.



So I mentioned before that this was an iterative process, that is, over time.  So we have before N.  Then we have N, O, P, and Q.  Prior to N, these media services were all inside a single monolithic media server process, and the extractors were run inside the client.  And of course this is where problems like Stagefright were able to cause such problems so that, for example, just as we recall, sending someone a text message was able to compromise their device.



Then in N they explained that:  "We delivered a major security re-architect, where a number of low-level media services were spun off into individual service processes with reduced-privilege sandboxes.  Extractors were moved into the server side" - which is to say out of the client - "and put into their own constrained sandbox.  Only a couple of higher level functionalities remained in Mediaserver itself.



"Then in O and P, the next generation of Android, these services were 'treblized'" - they turned it into a verb, treblized - "and further deprivileged, separated into individual sandboxes and converted into HALs," into Hardware Abstraction Layers.  Essentially, and we talked about this at the time also, they moved this away from the third parties.  Previously those had been third-party components.  And so by pulling them into hardware abstraction layers, they were able to isolate what needed to be per-hardware implementation and essentially move more of it under their own control.  They said:  "The media.codec service became a HAL, while still hosting both software and hardware codec implementations."



And now finally in Q, which is, as I mentioned, at beta, not yet in people's hands, but it's on the way, the software codecs were extracted from the media.codec process and moved back to the system side.  So again, they're continuing to take more responsibility.  But this is where we want to have it because they're also now able to add on-the-fly updates to these things.  They said:  "It becomes a system service that exposes the codec HAL interface.  SELinux policy and seccomp filters are further tightened up for this process.  In particular, while the previous media.codec process had access to device drivers for hardware accelerated codecs, the software codec process has no access to device drivers."



So again, by continuously working to see where their problems are, and incrementally changing the hardware architecture, they're able to redesign the system to dramatically enhance the security.  And I'm just - I'm very impressed by the maturity of the development philosophy that I see in those details.  Google is clearly very focused upon carefully studying the sources of problems, applying sound engineering solutions, inventing new solutions when they need to.  And they're just patiently evolving an increasingly more stable and capable software operating system platform.  You know I've given them a lot of heat in the past over the problems that they've had on the security side.  But I'm impressed by what I'm seeing.  I really believe that Android is shaping up to be an asset of tremendous value for the industry.



LEO:  I'm glad to hear it, since that's pretty much what I carry around with me all the time.



STEVE:  Yup, yup.  It's becoming more and more secure.  The ability to have always present whole device encryption, I mean, on any hardware, on any hardware that has Q they'll be able to have whole device encryption.  And the fact that they're now able to - and you can see how by pulling pieces back under their own control, they're then able to enhance it with on-the-fly encryption, meaning that the manufacturer only needs to deal with a much smaller aspect of the hardware interface to Android, and Google is able to then be the caretaker, and a responsible caretaker, for a much larger piece of this.  You know, it's increasingly becoming the Apple iOS model, where in this case Apple has the whole pie.  Google is increasing their share, but also being a really very responsible caretaker for it.



LEO:  Nice.



STEVE:  So bravo, yup.



LEO:  I just got my April Update on my Samsung Galaxy S10 Plus, so it's all working out.



STEVE:  Yay.



LEO:  Steve Gibson is at GRC.com.  You should be at GRC.com.  Go there.  Find out what's going on with SQRL, its imminent release.  SpinRite, of course, his bread and butter, the world's best hard drive maintenance and recovery utility; Shields UP!; all the great stuff he does there.  And this show is there.  You can download 16Kb versions for the bandwidth impaired, full 64Kb versions for those who like to spend their bandwidth with abandon.  Also handwritten transcripts, which I suppose is the lowest bandwidth version of the show.



STEVE:  Yes.



LEO:  Probably the smallest file size, thanks to Elaine Farris.



STEVE:  And even compresses way down, too.



LEO:  Yes, yes.  If that's what you care about.  If you're on a boat, and you're using satellite Internet.  We have audio and video of the show at our site, TWiT.tv/sn.  And you can of course always subscribe.  And that's probably the best thing to do.  If you want to watch us do it live, TWiT.tv/live.  That's where the streams live, audio and video.  And we do the show normally about 1:30 Pacific, 4:30 Eastern, 20:30 UTC of a Tuesday afternoon.  If you do come by live, join us in the chatroom at irc.twit.tv.  That's where the cool kids hang out.  Steve, have a great week.



STEVE:  Thank you, my friend.  We'll see you for 715.



LEO:  Hallelujah.  See you then.



STEVE:  Next Tuesday.  Bye.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.










GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#715

DATE:		May 21, 2019

TITLE:		CPU.fail

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-715.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  As expected after last week's Tuesday morning end-of-embargo on details of the next round of Intel processor information leakage problems, we will take a closer look at the new challenges they create and the impact of their remediation on system performance and stability.  But before that we look at last Tuesday's patches from Microsoft, Adobe, and Apple.  We examine a new big security problem for Cisco that even has stock analysts taking notice.  We check in on the ongoing troubles with the cryptocurrency market, see what Johns Hopkins associate professor Matthew Green tweeted about the trouble with Google's Titan Bluetooth dongle, and deal with yet another monthly problem with Windows 10 updates.  We touch on a bit of miscellany, then wrap up with a look at the new so-called Microarchitectural Data Sampling vulnerabilities.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here to explain all, to show us why access control lists may not always work as you expect, the story behind the failure of the Google Titan security key, and a patch for Windows XP?  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 715, recorded Tuesday, May 21st, 2019:  CPU.fail.



It's time for Security Now! with Mr. Steve Gibson, the star of the show, the king of Internet security and privacy.  There's no one better able - well, I kind of crowd you myself, but...



STEVE GIBSON:  Yeah, thank you very much.



LEO:  Yeah, at least I didn't put you on the Iron Throne.



STEVE:  I was just going to say, we didn't have any chance to talk about the "Game of Thrones" finale.  And we don't want to do any spoilers for those who are...



LEO:  We can't.



STEVE:  No, we can't say anything about it.



LEO:  But you could tell me if you liked it.



STEVE:  Yeah.  I mean, I just - I'm sort of sorry it ended.  Like I could have just kind of kept...



LEO:  I was so sad.



STEVE:  Yeah, I could have kept watching them run around in circles for, like, another 15 years.  So...



LEO:  I do think eventually people want to move on.  Although, and I don't think it's a spoiler to say so, the way it ended left lots of room for sequels.  And in fact I had heard that there will be.  HBO is making some spinoffs and so forth.  You could do both prequels...



STEVE:  No kidding.  Good.



LEO:  But, well, I'm not going to say anything about who or anything.  But there were people at the end...



STEVE:  They can certainly reuse - they can reuse a lot of those costumes.  They've got lots of spears.  They've got lots of shields.  They've got all kinds of stuff.  It's like, hey.



LEO:  Regardless of how you feel about the storytelling.  It's, again, no spoiler to say a lot of people on the Internet - there's even a one million signature petition saying HBO should reshoot the whole entire thing, the last season.  They blew it.  I don't - I disagree with that.  But I found it quite enjoyable.  But like you, I was mostly sad that it was over.  Somebody said, and I think this might be true, this may be one of the last times, if not the last time, the whole - a vast swath of people sits down on a Sunday night to watch a show because, A, fragmentation.  There are going to be so many different - there are more and more places to watch stuff.  And, B, a lot of these places dump the whole series all at once.



STEVE:  In binge mode, yes.



LEO:  Yes.  And so I wonder if there'll be an event like this again in our history.  I don't know.



STEVE:  Well, the good news is Lorrie is someone like me who is able to watch something that's really good again.  I've watched "Breaking Bad" now all the way through twice.



LEO:  Oh, we're doing that.  It's such a good show.



STEVE:  She fell in love with it.  And so we're just, like, waiting for it to be long enough to watch it again.  And so I'm so happy that I'm with someone.  The woman I married years ago absolutely wouldn't even consider it.  She says, "Well, we know what happens."  I say, "But it's art.  It's artwork.  It's a process."  "No."  And it's like, "Okay, fine."



LEO:  No, I'm with you.  And Lisa and I in fact are watching "Breaking Bad" again.  And what a great show.  That was so well done.  Man.



STEVE:  It was just - and it's the humor that was so much a part of it was they had fun.



LEO:  Yeah.  It was a great show.  And the plotting and the acting and everything.



STEVE:  Anyway, no humor this week.



LEO:  Oh, no.  Oh, no.



STEVE:  No humor for you.  Today's title is "CPU.fail," which actually is the website that was created to contain, not one, not zombie - I keep wanting to say "ZombieLand" - ZombieLoad.  But rather four different new problems of which ZombieLoad was but one.



LEO:  Oy.



STEVE:  So, yeah.  And what's significant...



LEO:  This is a gift that just keeps on giving.  It's another one of those speculation errors; right?



STEVE:  Well, yes.  Well, it's worse than that.  Remember that, of the two, we had Meltdown, the Meltdown and then the Spectre problems.



LEO:  Right.



STEVE:  Meltdown was easier to do and was a bigger problem.  Spectre was always sort of more itself speculative and less obviously a big problem.  Well, this is the logical extension of Meltdown into much bigger problems.  I mean, basically I'm reminded that I'm stepping on my own dialogue from the end of this podcast.  But Bruce Schneier gave us what I often quote him saying, "Attacks never get weaker, they only get stronger."  The guys who did Meltdown basically never stopped working on this, and they made them practical.  I mean, they reduced this to, yes, we can actually obtain a 128-bit AES key from, I mean, and they demonstrate it, from another process running in a shared hosting environment.



The good news is, first of all, once again the industry is taking it seriously.  And also, this only really impacts, as has always been the case, environments where you have inherently untrusted code sharing the same physical chassis.  Which is not the typical listener.  It's not the end-user mode.  It's the cloud environment.  And that's where the problem really is.  So this has all been good for locking that down.  But we have a lot of other fun stuff to talk about.  We will end up talking about that in more detail.



Also, we now have some visibility into last Tuesday's patches from Microsoft, Adobe, and Apple.  Microsoft and Adobe were biggies.  Apple, you know, we never really get that much out of Apple.  We know that they did respond to the microcode problem with patches.  And we have a little bit of something from them.  We also take a look at a big new problem that has just come to light from Cisco that's big enough that it's got stock analysts taking notice and saying, okay, like maybe this is going to have a financial impact on them.



LEO:  Yikes.



STEVE:  Yeah.  We check in on the ongoing troubles with the cryptocurrency market that we haven't dipped into recently.  Also Matthew Green kind of lost it on Twitter over his reactions to Google's Titan Bluetooth dongle screw-up.



LEO:  Oh, yeah, yeah.



STEVE:  But it's really fun.  So we're going to follow his Twitter stream a little bit.  Also we deal with yet another monthly problem that Microsoft just seems to be having now every month with their Windows 10 updates.  We'll touch on a bit of miscellany, and then wrap up by talking about what is now being called "microarchitectural data sampling vulnerabilities," which is the umbrella under which all of these problems can sort of be aggregated - microarchitectural data sampling vulnerabilities, or as I also called it later on, "Speeding up is hard to do."



Here we have the Picture of the Week, Leo.



LEO:  Yes.



STEVE:  And I just, okay, I love this picture.  It's, you know, it's perfect.  And we've done some things similar.  I think I remember one where there was a path, a walking path, and someone had stuck a locking fence door on the path, and you could see the grass was worn on either side because people said, okay, this path is closed.  I'm going to go around the door.  I mean, it was just, like, so dumb.  But anyway, that's our Picture of the Week is it shows this one-lane roadway that separates the outside, there's like public cars and things, like a public road, from the inside, with this dropdown arm, you know, the motorized arm that blocks the road.  But there's nothing to prevent people from saying, okay, I'm in a hurry, I forgot my pass, I'm not supposed to go through here, whatever.  And you can clearly see that it looks like nobody bothers to go through the arm.  They just say "Forget this" and drive around.



Anyway, it's just wonderful.  And so on one hand, okay, so here it is.  I mean, this clearly exists somewhere in nature.  You have to ask yourself, what were they thinking?  Like what was, I mean, this cost some money, right, to like put this out there on this road.  And to imagine that somebody is going to be inconvenienced, what are they are going to do?  They're going to stop and go, "Oh, darn."



LEO:  They just drive around it.  But we've seen this before.  We saw a gate, remember?  You had a Picture of the Week that was a gate that people just went around; right?



STEVE:  Yes, yes.  It's just, okay.



LEO:  It just shows that, if you're going to build a fence, you've got to build a pretty long fence.



STEVE:  Yeah.  Anyway, I've had this for a long time.



LEO:  I love it.



STEVE:  And nothing else on the radar this week.  So I thought, okay, it's time just to say, what were they thinking?  Because this is not, I mean, this is - you could argue this is dumber than the password entry lock where the password is written above it.  Because at least there, if you just changed your mind, you could take the password off and change the combo.  Here it's like, okay, I mean, I guess they could strengthen this by putting fencing around.



I mean, basically nothing about this is defensible.  I mean, like, there's no actual way, as you can see from the picture, to prevent somebody who's on the outside from coming onto the inside, or escaping, if that's what this is trying to - it's not clear what direction this is trying to control.  Anyway, I just - this is just nuts.  So I just - it's a great example of security not really working.



So Windows XP, Leo - and I said that correctly.  Not Windows 8; not Windows 7.  Windows XP was patched last week.



LEO:  I know, I know.



STEVE:  For the second Tuesday of May.



LEO:  Well, it may be more accurate to say a patch was offered because it wasn't automatic.



STEVE:  Correct, correct.  So you know something is worrying Microsoft deeply when they're willing to reach back to Windows XP and produce, I mean, they don't even want to keep 7 current for like the Spectre and Meltdown stuff.  We don't have, even now, complete updates for all versions of Windows 7 for that because there's a strong push, as we know.  I've now twice had the warning come up that all security for Windows 7 is being discontinued, so we want to help you get your ducks in a row to move to Windows 10 before February of next year.  And it's like, okay, well, my ducks are fine, thank you very much.  We'll see what happens.



Anyway, so this is XP.  Simon Pope, the director of incident response with the Microsoft Security Response Center, the MSRC, titled his announcement posting as a plea, with the headline "Prevent a worm by updating Remote Desktop Services."  He said:  "Today Microsoft released fixes for a critical Remote Code Execution vulnerability" - it was given CVE-2019-0708 - "in Remote Desktop Services, formerly known as Terminal Services, that affects some older versions of Windows."  And their announcement says "The Remote Desktop Protocol" - which of course we talk about often.  "RDP itself," he says, "is not vulnerable."  Okay, well, that's sort of a technicality.



He says:  "This vulnerability is pre-authentication and requires no user interaction.  In other words, the vulnerability is wormable, meaning that any future malware that exploits this vulnerability could propagate from vulnerable computer to vulnerable computer in a similar way as the WannaCry malware spread across the globe in 2017.  While we have observed no exploitation of this vulnerability" - this is Microsoft saying - "it is highly likely that malicious actors will write an exploit for this vulnerability and incorporate it into their malware."



And I have to say that I love the fact that the world is finally sitting up and taking sober responsibility for these sorts of problems.  We know that anyone can make a mistake, and that such mistakes can go unseen for years, as indeed this one did because it existed in Windows XP from day one, and sometimes even for decades.  So it's not the fact of the mistake that matters, it's how those who are responsible for fixing it deal with that responsibility.  So this is, I have to say, another bravo for Microsoft.  On the other hand, they didn't really have much choice.



Simon continues his posting, saying:  "Now that I have your attention, it is important that affected systems are patched as quickly as possible to prevent such a scenario from happening.  In response, we are taking the unusual step of providing a security update for all customers to protect Windows platforms, including some out-of-support versions of Windows."  He says:  "Vulnerable in-support systems" - meaning those that are still receiving updates - "include Windows 7, Windows Server 2008 R2, and Windows Server 2008."  But that means specifically Windows 8, 8.1, and 10 are not vulnerable to this.  So this is older versions of Windows.



He said:  "Downloads for in-support versions of Windows can be found in the Microsoft Security Update Guide.  Customers who use an in-support version of Windows and have automatic updates enabled are automatically protected."  So Leo, as you said, if you're in support and have auto updates, you'll be automatically protected.  But unfortunately earlier versions, out-of-support systems, he says, "include Windows 2003 and Windows XP.  If you are on an out-of-support version, the best way to address this vulnerability is to upgrade to the latest version of Windows."  Uh-huh.  On the other hand, if you're running XP, maybe you'll have, like, 2GB of RAM, and sorry.



LEO:  Good luck.



STEVE:  Yeah.  He says:  "Even so, we are making fixes available for these out-of-support versions of Windows," and then there's a KB for anyone who's listening, KB4500705.  So if you have XP.  But it's worth saying, okay, well, we'll cover this in a little more detail about what this means in a second.  But I was just going to say that, you know, you'd have to be exposed in some way.  Your instance of Windows desktop remote terminal services, whatever you want to call it, Remote Desktop Protocol, would have to have an exposure to the public.  So that's unlikely if you're behind a router.  On the other hand, as we'll talk about in a second, there's a lot of those.



So they said:  "Customers running Windows 8 and Windows 10 are not affected by this vulnerability, and it is no coincidence" - this is them stroking themselves - "that later versions of Windows are unaffected.  Microsoft invests heavily in strengthening the security of its products."  I would argue it's probably a coincidence, but anyway.



LEO:  Yeah, I find that hard to - well, we didn't fix it in any of these other versions, but we need to fix it on...



STEVE:  Yes, given the fact that what was it now, I don't have the number in front of me, 70-some - oh, there it is, 79 vulnerabilities were fixed in Windows 10 this month.  Again.  So they said:  "Microsoft invests heavily in strengthening the security of its products, often through major architectural improvements that are not possible to back port to earlier versions of Windows.  There is partial mitigation" - this is Microsoft - "on affected systems that have Network Level Authentication (NLA) enabled.  The affected systems are mitigated against wormable malware or advanced malware threats that could exploit the vulnerability, as NLA requires authentication before the vulnerability can be triggered.  However, affected systems are still vulnerable to Remote Code Execution exploitation if the attacker has valid credentials that can be used to successfully authenticate."



Okay.  So what that means is that we've been talking recently about all of these problems with RDP.  I've said to our listeners over and over, there is no safe way to have this port 3389 publicly exposed.  You just can't.  I'm a big fan of Remote Desktop.  It's super convenient.  Windows is inherently GUI by nature, so it wants GUI administration.  Consequently, it's the way I remotely administer GRC's servers.  But you will find no port 3389 or any other port listening on GRC's network that's got Remote Desktop Protocol exposed.  There is no possible safe way to have it exposed.



So, okay.  So RDP runs over a well-known port, 3389.  So, not surprisingly, scanning the Internet's IPv4 address space for instances of that port accepting TCP connections is trivial.  Now, get this.  About 11 million, 11 million IPv4 addresses are currently accepting connections on port 3389.  And because people have looked more closely after their eyes bugged out, somewhere around 4.1 million of those IP addresses respond as RDP servers today, 4.1 million exposed RDP servers.  That's insane, but it's the case.  And it certainly explains and warrants Microsoft's deep concern when we absolutely know that some probably significant percentage of those will be XP, Windows 7, Server 2003, Server 2008, or Server 2008 R2 machines.



So what we now know is that there is an exploit which will certainly be reverse engineered which allows all of those 4.1 million RDP-responding servers that are XP, 7, or any of those servers responding.  This exploit allows them to be taken over remotely.  And if they happen to have NLA enabled, which is not the norm, but if they did, then even those that we've been talking about whose credentials have been brute forced can now have remote code execution leveraged against them in order to allow the attackers to get more than they were able to before.  So although on the other hand if you have a remote desktop protocol that gives the connector sufficient privileges, you've got effective remote code execution anyway because you can send anything through that connection that you want to and then run it at the other end.



So, okay.  So just for the sake of our listeners, anyone who wishes, any individual or enterprise who wishes to make Remote Desktop Protocol available remotely must simply place it behind a VPN.  That's all you have to do.  Install an instance of OpenVPN server using certificate-based authentication.  Set up an instance, one or more instances of OpenVPN clients with authorized client machines, giving them individual certificates signed by the server's private key.  Now you have a robust means of managing a set of VPN clients who you can then selectively give access to remote desktop protocol through the VPN.



Again, unfortunately, I mean, it's almost unfortunate that there's even a password on Remote Desktop Protocol because, if there weren't one - well, I guess I don't mean that because people still would put it on the public Internet and go, oh, well, I guess we're not supposed to password-protect it.  But the point is the password doesn't work, as we've seen, except on more recent versions of Windows.  Now anybody can get to these things.  So given that there are many millions of probably exploitable RDP servers that will probably not be updated, we're almost certainly going to see this.  We'll be talking about this a month or two from now, the RDP worm that hit.



To their credit, again, Microsoft did what they could.  They offered a patch to old systems that are probably online; but, as we have so often seen, probably not going to get patched, probably never going to get patched.  And now they're going to get taken over as soon as some miscreants reverse engineer this problem, knowing first of all that there is a patch, looking at the patch, see what got changed.  I mean, all the tools are in place.  The skill set is in place.  It's going to happen.



So we don't know that they will create a worm.  In this day and age they'll probably just infect them with cryptominers and hope that they're running powerful servers that they can use for mining cryptocurrency, and they'll just suck all of the CPU cycles out of those.  Or maybe they'll do a worm.  The point was that Microsoft said it was wormable because it required absolutely no user interaction, just like WannaCry, where if it was able to get into a system, then it was able to do so autonomously, didn't require anything, to then be able to turn around and start scanning for more vulnerable systems.



Well, now we have Shodan, which is the way we know that there are 11 million open 3389 ports, and 4.1 of those respond to the remote desktop protocol.  There's no question that a heavy percentage of those are going to be older machines that are not going to get patched.  So we will likely be talking about the other shoe to drop on this one before long.  And as I said...



LEO:  [Sigh].



STEVE:  Yeah.  Security is hard to do, Leo.  Boy.



LEO:  So this has probably been in this forever; right?  I mean, this is not...



STEVE:  Yes, yes.  It's been in the original Windows XP, when was that?



LEO:  Oh, geez.



STEVE:  That was, what, 20 years ago.



LEO:  Twenty years ago?  Yeah.



STEVE:  Yeah.



LEO:  So that's interesting, I mean, it doesn't...



STEVE:  It finally came to light.  And so this is why, as we've been saying, it really is necessary - and we're there now, but we weren't there 20 years ago - for updates to automatically be installed, like for systems to reach out to somewhere and check to see whether there are updates available.  And it's worth noting, I don't know if anybody has tried to set up XP.  I do from time to time.  In fact, I had to do it last week because somebody who was testing an update to the SQRL client for Windows, mine, GRC's that I've written, had a problem.  I'd made some changes.  I would have to look at my notes to remember what they were.  But I had to set up a Windows XP.



Well, Windows XP no longer is able to even get updates because Microsoft changed the Windows Update format.  And so you have to manually go get an update to Update, and update Update before it's able to update anything else.  And so it's like, even the update won't update unless you first update it, which is sort of, you know, like wait a minute.  Why?



LEO:  Huh?  Tell me that again?



STEVE:  Yeah.  So anyway, Microsoft and Adobe both released their regularly scheduled patches last Tuesday, second Tuesday of May.  Microsoft's was 79 vulnerabilities, 22 of them labeled critical.  Of the 22 vulnerabilities, 18 affected their various scripting engines and browsers, and the remaining four were remote code execution in, as we've just described, Remote Desktop.  There was also one in DHCP Server; one in the GDI+, the Graphics Device Interface extension; and also one in Word.  And of course, as we'll be talking about later, Microsoft also released what they called "guidance" for the recently disclosed so-called Microarchitectural Data Sampling, now called MDS techniques, that we'll get to.



Adobe on its Patch Tuesday fixed one problem in Flash.  It's nice that they stopped messing with it so they're not creating new problems, but they're still finding older ones.  On the other hand, there were also problems in Acrobat and Reader and also Media Encoder.  As I said, Flash only got one fix.  Media Encoder had two fixes.  Acrobat and Reader, 83 vulnerabilities in those.  Because, again, if you just keep stirring the code, you're going to keep creating new problems.  And that's what we keep seeing.



There were four remote code execution vulnerabilities.  I did a little bit of digging, and I could not determine whether the Windows DHCP Server problem, which is a remote code vulnerability and is ranked critical, whether that affected it for its receiving packets from the public side, that is, on the WAN side, or on the LAN side.  A DHCP server is, for example, may be standing in for DNS.  And so it's telling all of the clients on the LAN side to refer to it for DNS.  So then it's turning around and making DNS queries on behalf of its clients.  That's sometimes the way things are configured.  But it's oftentimes the case that DHCP Server will be making public queries.



What has come to light is that there is a way to provide a malformed reply packet to the Windows DHCP servers in order to cause a remote code execution vulnerability.  As a consequence, for enterprise customers especially, this has jumped up to the top of the "must patch immediately."  Here we are a week downstream.  Hopefully IT people who are responsible already understood that this was important and fixed it because once again what we're seeing is the vulnerabilities are being reverse engineered and are being weaponized very quickly.  The other problems, these other 22 vulnerabilities are remote code execution, privilege of elevation, and our regular cast of characters.



For the Zero Day Initiative blog post last Tuesday, Dustin Childs, who's Trend Micro's communications manager, wrote, speaking of these vulnerabilities:  "They would first need to gain access to run code on a target system, but malware often uses elevations like this one."  So there was one that was not remote code execution, but it was an elevation of privilege, and we've been talking about this.  They use elevations like this one to go from user to admin code execution.  And again, there are no details provided for this.  But they are being used in limited attacks in the wild.  So again, this is one of those let's definitely get this patched.  And with any luck, all of the people listening to this podcast have done so soon after, knowing that it was Patch Tuesday time.



I want to talk about an exploit that doesn't technically have a name, Leo.  Unfortunately, the people who found this and have weaponized it and will be discussing it at length this summer at the Black Hat Conference 2019, decided to name it with three emojis.



LEO:  Oh.



STEVE:  I know.



LEO:  "An exploit has no name" would have been better.



STEVE:  Yeah.  So much like the Artist Formerly Known as Prince...



LEO:  Oh, impossible.  What are the emojis?



STEVE:  So they are three angry cats.



LEO:  And that's the name, Three Angry Cats.



STEVE:  So they're saying, if you must have some way to refer to it, they recommend Thrangry Cat.



LEO:  Thrangry Cat.



STEVE:  Thrangry Cat.



LEO:  That's a terrible name for an exploit.



STEVE:  It's awful.  So let's take our second break, and then we will dig into something that - Thrangry Cat, yes, it's bad.  And what's significant is, unfortunately, this is bad, enough that, as I said, financial institutions are worrying about Cisco's stock price because it affects hundreds of millions of deployed routers, switches, and firewalls.  When you scroll through the list, I mean, it's like it's hundreds of individual devices that Cisco makes, deployed in hundreds of millions of publicly accessible locations, and it is remotely exploitable.  So, yes, the cats are Thrangry.



LEO:  Thrangry.



STEVE:  Thrangry.



LEO:  Thrangry.  I'm thrangry.  Ah, Steve.  All right, Steve.  What's next?  Thrangry.



STEVE:  So a new and very serious vulnerability present in hundreds of millions of Cisco routers, switches, and firewalls.



LEO:  Geez.



STEVE:  Yeah.  InvestorPlace.com noted:  "Cisco earnings were great, but beware Thrangrycat.  Cisco's latest security weakness is something to consider if looking to buy."  So, I mean, it's really shaking things up.  And I'm not that impressed with Cisco's handling of this.  We'll get to that in a second.  They go on to say that they have no idea what to make of Red Balloon's description.  That's the group that found this and did some very seriously good engineering.  I just wish Thrangrycat was, I mean, I guess I wouldn't mind if that was the official name.  But, no, they've decided to name it as three emojis.



Okay.  So the security firm is Red Balloon.  They've identified a vulnerability, actually two, and in the first case have received a CVE of 2019-1649.  And, yes, it's known - I have the picture of the three unhappy-looking cats in the show notes.  Oh, and it doesn't look like they printed well.  When I created the PDF, it just shows little blacked-out shadows on the show notes.  So that's another reason why you don't want to [crosstalk].



LEO:  It's not in mine.  The computer you're using probably doesn't have the [crosstalk] cats.



STEVE:  Well, no, so you have the big three ones.  But if you scroll down, I actually embedded the name throughout the text.



LEO:  Yeah, yeah, it works, yeah.  I think your computer doesn't have the requisite emojis.



STEVE:  Oh, you're right, it does show on yours.



LEO:  It looks cute, in fact.



STEVE:  Okay.  Well, are you on a Mac?



LEO:  I am on a Mac.



STEVE:  Ah.  So, Thrangrycat affects multiple Cisco products that support - oh, and Leo, if you want to, if you scroll down, there's a Cisco link.  Look at the list of stuff in their disclosure.  Oh, my goodness.  It's just, I mean, the list of their hardware, it's everything they've been doing.  Okay.  So I want to back up.  Thrangrycat affects multiple Cisco products that support, okay, this is Cisco's own proprietary, they call it the "Trust Anchor module," TAm.



LEO:  Is that secure boot or...



STEVE:  Yes.  It's their version.  As its name suggests, the Trust Anchor Module is a critical component of Cisco's hardware-based secure boot functionality which has been implemented in nearly all of Cisco's enterprise devices since 2013.



LEO:  Well, that explains the length of this list.



STEVE:  I know.



LEO:  All, basically, all.



STEVE:  I know.



LEO:  Holy cow.



STEVE:  So they came up with this, and they said, oh, this is wonderful.  Let's put this everywhere.  Let's put this on everything.  Secure boot.  So it promises to ensure that the firmware, as we know, with secure boot, running on hardware platforms, is authentic and unmodified.  Unfortunately, it turns out that's a promise it's unable to keep.  In the words of its discoverers:  "Red Balloon Security, Inc.," they wrote, "is disclosing two vulnerabilities affecting the products of Cisco Systems, Inc.  The first, known as" - and there you have the three icons, the three cat heads, I'll just say Thrangry since that's what we have to do - "allows an attacker to fully bypass Cisco's Trust Anchor module via Field Programmable Gate Array (FPGA) bitstream manipulation.



"The second vulnerability is a remote command injection vulnerability against Cisco's IOS XE version 16 that allows remote code execution as root.  By chaining the Thrangry and remote command injection vulnerabilities, an attacker can remotely and persistently bypass Cisco's secure boot mechanism and lock out all future software updates to the Trust Anchor module (TAm).  Thrangry," they write, "is caused by a series of hardware design flaws within Cisco's Trust Anchor module.



"First commercially introduced in 2013, Cisco's Trust Anchor module is a proprietary hardware security module used in a wide range of Cisco products, including enterprise routers, switches, and firewalls.  TAm is the root of trust that underpins all other Cisco security and trustworthy computing mechanisms in these devices."  In other words, they put all their eggs in that basket, and then the eggs broke.



"Thrangry allows an attacker to make persistent modification to the Trust Anchor module via FPGA bitstream modification, thereby defeating the secure boot process and invalidating Cisco's chain of trust at its root.  While the flaws are based in hardware, Thrangry can be exploited remotely."  There again, the flaws are based in hardware.  Thrangry can be exploited remotely without any need for physical access.  "Since the flaws reside within the hardware design, it is unlikely that any software security patch will fully resolve the fundamental security vulnerability."



So we've not on this podcast talked about FPGA bitstream programming before, as we know.  We have talked about FPGAs, Field Programmable Gate Arrays.  An FPGA, field programmable, as its name suggests, is a means for creating soft hardware.  It's a massive array of logical hardware elements - like AND gates, OR gates, selectors and such - that can be dynamically configured with RAM that is loaded into it.  So the idea is that you have an outboard small EPROM or PROM which, at power up, produces a bitstream which this FPGA sucks into itself in order to turn it from just this massive generic unprogrammed blob into a piece of hardware.



And, I mean, the FPGAs these days have gotten unbelievably powerful.  You can implement full processors in FPGAs that run at full processor speed.  Because they're generic, they're not as efficient as if you actually did a masked programmed array, or actually designed the chip that you want.  But you can do full-on real hardware this way.  So the problem here is that something that Cisco did wrong allows this bitstream to be manipulated such that the hardware root of trust is corruptible.  From their Q&A, answering the question how widespread is this, they explain:  "This vulnerability affects Cisco products with an FPGA-based TAm.  Cisco released their list of more than 100 product families containing this vulnerability."  And as I said, Leo, it's a horrifying list.  I mean, it's like everything that they've done for the last six years.



So they asked themselves the question:  "What are the implications of demonstrating modification of the FPGA bitstream?"  They write:  "Our findings support the practical exploitation of FPGA-based devices via direct bitstream analysis and modification."  So they reverse engineered the bitstream on one device that they got their hands on, took a look at it.  And as is so often the case, you know, we've talked about how difficult it is to write code that interprets securely because the people writing the interpreter just - it's so difficult to take an adversarial posture.  It takes an adversary to have an adversarial posture.  Well, these guys have that.  And so they looked at what Cisco did, and said aha, and realized that there were some problems here.



So they said:  "Through our research we developed a series of techniques to reliably add, subtract, and alter FPGA behavior without any need to perform register-transfer level (RTL) reconstruction."  Which is to say an RTL is one of the languages used for dealing with FPGAs.  It's the way you describe this.  So basically they didn't need to make a complete change to rewrite the behavior of the whole thing.  They were able to edit some of that in order to cause a degradation of security, which probably is not that difficult to do.  Any change that you make is going to do something that is non-optimal and is certainly not going to increase its security any.  It has the reverse effect.



So they said:  "By demonstrating successful FPGA modification on the Xilinx Spartan 6 LX45T" - which presumably is the FPGA that Cisco chose - "we find that our bitstream manipulation techniques present a range of potential applications for persistent FPGA implants, physical destruction of embedded systems, and attacks against FPGA-based systems, such as software-defined radios, advanced automotive driver assist modules, weapons guidance systems, and more."



Then they asked themselves in their Q&A:  "Have these vulnerabilities been exploited in the wild?"  They answer:  "We are unaware of any use of this exploit in the wild, but the potential danger is severe."  "What action can be taken?"  They answer:  "Please consult Cisco's official security advisory.  We did not receive early access to Cisco's security patch, and will be analyzing the patches as they are made publicly available.  Since Thrangry" - and, I mean, it is angry - "is fundamentally a hardware design flaw, we believe it will be very difficult..."



LEO:  Oh, boy.



STEVE:  Yes, yes, yes.  "It will be very difficult, if not impossible, to fully resolve this vulnerability via software patch."



LEO:  Yikes.



STEVE:  So, yeah.  Okay.  So as I mentioned, I was unimpressed with Cisco's own vulnerability disclosure because that was in something of a panic.  They downplayed the problem's severity.  They wrote:  "A vulnerability in the logic that handles access control to one of the hardware components in Cisco's proprietary secure boot implementation could allow an unauthenticated local" - it also works remotely - "attacker to write a modified firmware image to the component."  Okay, that's technically true, but not only local.  "This vulnerability affects multiple Cisco products that support hardware-based secure boot functionality."  So, yes, right out of the mouth of an attorney.  It's not incorrect; but, boy, it doesn't characterize the nature of the problem.



So Cisco continues:  "The vulnerability is due to an improper check on the area of code that manages on-premise updates to a Field Programmable Gate Array part of the secure boot hardware implementation.  An attacker with elevated privileges and access to the underlying operating system" - okay, both which these Thrangry people, the Red Balloon people, demonstrated - "that is running on the affected device could exploit this vulnerability by writing a modified firmware image to the FPGA."  In other words, Cisco's basically confirmed what the Thrangry exploit is, which is the bitstream can be changed.  The TAm module that anchors secure boot depends upon it not being changed.  Therefore, secure boot can be bypassed.



Cisco continues:  "A successful exploit could either cause the device to become unstable and require a hardware replacement, or allow tampering with the secure boot verification process, which under some circumstances may allow the attacker to install and boot a malicious software image."  So since 2013 Cisco has had a secure boot trust anchor in hardware that now these people demonstrate across their devices, apparently all of them, based on Cisco's own list, can be compromised.  There is a remote compromise available.  So these guys are going to be demonstrating it in Vegas at Black Hat 2019.  And they allege this cannot be fixed in software.  So this has got to just be Cisco's worst nightmare.  Wow.



Cryptocurrency hacks are still growing.  We've not talked about, you know, we talked about bitcoin before it was a big deal.  We talked about it, I'm sorry to say, back when it was possible for a Core i7 running overnight to generate a bitcoin, as mine did.



LEO:  Or two, or 50.



STEVE:  Fifty.



LEO:  Worth around $35,000 right now.



STEVE:  Yes.  And then I reformatted the hard drive because...



LEO:  Oh, you did?  You killed the wallet?



STEVE:  I killed the wallet, yeah.



LEO:  That was expensive.



STEVE:  Well, it was expensive because it peaked just shy of $20,000 per coin.



LEO:  Yeah.  That was really expensive.



STEVE:  Oh, boy, yeah.



LEO:  That's, what is that, a million dollars?



STEVE:  Thank you, Leo.  Yes.



LEO:  What is that, like a million?



STEVE:  That was an expensive reformat operation.  And, yes, I'm sure that the underlying data is gone because I then reinstalled Windows on top.



LEO:  Did you have it in a wallet?  Didn't you have it in a wallet, though?



STEVE:  Yeah, but I didn't take it seriously.



LEO:  And you didn't keep the wallet ID or anything like that.  Because you could, if you just had wallet.dat, you could get it all back.



STEVE:  No, that was on the hard drive.  And so that's what got obliterated by a fresh install.



LEO:  So kids, back up your wallet.dat.  And by the way, if you password protect it, don't do as Leo did and forget the password.



STEVE:  Well, I'm sure back then I was using a password, one of a few that I'm almost sure I would have been able to use.



LEO:  That's what I thought, yeah.  And I did back up my wallet.dat.  I have that.  And I'm able to rebuild the wallet. If you just have that little file you can rebuild the wallet.  But then you need to unlock it.  I only have seven.  You got 50, dude.  



STEVE:  Yeah.  Cryptocurrencies have enjoyed a recent resurgence.  I don't know - I didn't look at it recently.  I didn't look at it today.  But they spent a long time wallowing down around $4,000 for a bitcoin.  And it's about doubled, last time I looked, around 8,000.  So anyway, but where are we?  We've talked a lot recently about Coinhive and about browser-based mining.  We haven't checked in on the larger cryptocurrency world recently.  There's a nice publication that I keep an eye on, the Hacker News.  They have some good writers, and they pull pieces together sometimes.  They wrote up an interesting summary snapshot with information gleaned from several different sources, including an outfit called CipherTrace that did a 2018 cryptocurrency report.



So with a bit of paraphrasing for length and a bit of editing for size, I wanted to share what the Hacker News wrote.  They wrote:  "Though once synonymous with underground networks and black hat hackers, bitcoin and other cryptocurrencies have gone mainstream over the past two years.  In 2017, we saw the skyrocketing of bitcoin to an all-time high of close to $20,000, followed by a significant decline the following year.



"But beyond the ups and downs in the market for the world's largest cryptocurrency is a much more sinister story revolving around cyberattacks of the economy's newest asset class.  In 2018 it's estimated that as much as $1.7 billion" - that's USD billion - "$1.7 billion worth of cryptocurrencies were swindled away from investors," and they said, parens, "(likely more) through a variety of means."  And you and I have been talking about this, and we see this going on in the news all the time.



"Whether accomplished through hacking, phishing, or other forms of scamming, it's clear that the crypto industry is facing a serious dilemma with security.  For a technological movement based on decentralization and the advantages it offers for security, the number of breaches occurring is startling."  They wrote:  "Cryptocurrencies offer users a way to send money without the need for a third party, yet the industry as a whole is dealing with more security vulnerabilities than centralized financial firms doing the same thing.  During the same period, more traditional companies that transfer money and banks have seen nowhere near the same amount of issues with hackers.  So what's the problem?



"While cryptocurrencies and blockchain technology are decentralized in nature, there are many aspects of the cryptosphere that aren't.  The number one culprit in 2018" - and we know this from watching - "was cryptocurrency exchanges.  Unlike the underlying technology behind currencies like bitcoin, Ether, and Litecoin, cryptocurrency exchanges are centralized and not yet regulated to the same extent as most financial firms.  According to data from CipherTrace's 2018 Cryptocurrency Report, $950 million of the total $1.7 billion stolen were from exchanges and infrastructure services.  Exchange services are a particular pain point for the industry because they're one of the easiest ways for users to get started with cryptocurrencies as some even handle fiat currency."



So anyway, I won't go on.  I have more in the show notes if anyone is interested.  But basically this tells us what we already know, and what we have seen, which is that we're seeing lots of startup me-too exchanges; and, if you build it, they will come.  They collect a bunch of money, and whether it's an insider job - sometimes, you know, the people running the firms are accused of faking the loss or faking an exterior attack.  Sometimes they probably are.  As we know, security is difficult.  And if these things are not well designed - and it's also the case that over time you generally mature your security.  Hopefully it gets better over time.



So anything brand new, where there's lots of money involved, you're going to have people trying to poke it.  And when it's a virtual currency, theft is electronic.  So anyway, I just thought it was interesting to get a sense of scale.  On one hand, sure, there's a lot of glamour behind this.  On the other hand, boy, if you actually do have any substantial wealth in online coin, be very careful where you have it stored because, boy, it's like there's just been a long history of these problems.



LEO:  Yeah.  I mean, people would say, oh, Steve, you should just put it on Coinbase, and you'd be fine.  Or Mt. Gox.  If you guys would just put it into Mt. Gox, you'd be fine.  Both of us had the good sense not to do that.  I would feel a little bit worse if it was stolen than if we just lost it out of stupidity.



STEVE:  Well, Leo, I mean, remember, back then none of this existed.



LEO:  Yeah, it was all made up, yeah, yeah, yeah, exactly.



STEVE:  I mean, to our credit, we did a podcast explaining what the blockchain was when no one had ever heard the term before.



LEO:  True, true, true.



STEVE:  So, I mean, otherwise, I mean, I know that the drive was formatted because I went back and retraced my steps because for a while I was saying, oh, I think it might be on a computer back behind some boxes.



LEO:  [Crosstalk] in the corner, yeah, yeah.



STEVE:  Yeah.  And so finally I thought, okay, let's get serious about this, Gibson.  And so I went back, and I absolutely verified, I know which machine it's on, I know which one it was on.  I remembered running this thing overnight, waking up and going - and then I reported to our listeners, hey, remember when I said I was running a little miner?  Well, turns out...



LEO:  I remember it vividly.



STEVE:  ...the next day I had 50 bitcoin.  It's like, woohoo.



LEO:  Right.



STEVE:  Well, boohoo.



LEO:  On the flipside, it didn't cost us anything to create them.



STEVE:  That's true.  I do like to earn my money.  But still, free money, ouch.  Anyway, okay.  So we have a little bit of comic relief here on a podcast that needs some.  Matthew Green.  Matthew Green, whose full name is Matthew Daniel Green, our oft-quoted cryptographer and security technologist.  He's an associate professor of computer science at Johns Hopkins Information Security Institute.  He got himself a bit worked up over the details of the recent flaw discovered in the configuration of Google's Titan Bluetooth security dongles.



So probably anybody who has one has already received email from Google.  Google's very good about communication.  Google is sending out replacements.  If yours is marked with a T1 or T2 down in the bottom of the back of it, then it's vulnerable.  I've got a link here, Leo, to Matthew's Twitter stream, but I also have it in the show notes.



So, first of all, Matthew quotes from the Bluetooth spec.  He has a screenshot which I reproduced here.  Reading from the Bluetooth spec.  Get this, listeners:  "The encryption key (K.sub.C) is derived from the current link key and may vary in single-byte increments from 1 byte" - what?  Okay, the encryption key...



LEO:  One byte?  Let me see.  There's 15 possible choices.



STEVE:  From 1 byte to 16 bytes in length, as set during a negotiation process that occurs between the master and slave devices.  During this negotiation, a master device makes a key size suggestion for the slave.



LEO:  Actually just one.



STEVE:  Yeah.  How does that sound?  Then you get to choose between any one of 256 keys.



LEO:  Well, that should be enough for anybody.



STEVE:  Ah, who needs more than that?  The spec says:  "The initial key size suggested by the master is programmed into the controller by the manufacturer and is not always 16 bytes."



LEO:  That's like a new kind of security; right?  You don't know what it is.



STEVE:  That's right.  "In production implementations, a 'minimum acceptable' key size parameter can be set to prevent a malicious user from driving the key size down to the minimum of 1 byte, which would make the link less secure."  It actually says, "which would make the link less secure."



LEO:  A lot less, yeah, yeah.



STEVE:  Oh, a 1-byte key?  Really.  So Matthew, now Matthew, after posting this screenshot from the spec, Matthew tweets:  "Like, what kind of idiot protocol lets users negotiate a 'maximum key size' that can be as small as 1 byte."



LEO:  Now, this is a Bluetooth spec, not a Google spec, we should point out.



STEVE:  Correct.  This is Bluetooth.  He says, parens:  "(A default that, fortunately, should be higher in more recent versions.)"  He continues, this is Matthew Green quoted on Twitter:  "Don't rely on Bluetooth security protocols for anything, ever.  Just treat them like a particularly inefficient version of Base64 encoding."



LEO:  I like this line.  Keep going.



STEVE:  He says:  "What would you possibly do with a 1-byte key?  Is anyone at Bluetooth SIG even awake when these idiot decisions get made?"  He says:  "Hey, maybe we should call some experts and ask them if 1 byte is a good minimum size for an encryption key.  Nah.  We've got all the expertise we need right here.  That's 256 whole keys."  Then he tweeted:  "Bluetooth" - and I didn't get this reference, but he explains it.



LEO:  Oh, I get it, yeah.  Yeah, yeah.



STEVE:  He says:  "Bluetooth is the Michael Bay movie of encryption protocols."  And I thought, huh?  Then he explains:  "It doesn't make any effing" - and he didn't say "effing," he used the whole word.  He says:  "It doesn't make any effing sense, and then it explodes."



LEO:  That's a Michael Bay movie in a nutshell.



STEVE:  And then somebody tweeting as Tibor Jager, who is enjoying Matthew Green's tirade, adds:  "I like the way they use the phrase 'less secure.'"  To which Matthew tweets in reply:  "What could it possibly be less secure than?"



Okay.  So what's Matthew so worked up about?  It seems that a team of security researchers at Microsoft discovered a serious vulnerability in the Bluetooth version of Google's Titan security keys - okay, now, remember everybody, these are the ones that everyone is using today, right, the T1 and T2 versions, right now, until last week - a serious vulnerability that cannot be patched with a software update.  So Google has announced that it will replace all affected keys at no charge.  Thank you, Google.



In their security advisory published Wednesday, Google said a "misconfiguration in the Titan Security Keys Bluetooth pairing protocols" could allow an attacker who is physically close to your security key, which is to say within 30 feet, to communicate - that's not bad, 30 feet - to communicate with it or the device to which your key is paired.  And we know from Matthew that this misconfiguration allows the use of a 1-byte key.



As we covered at the time last summer, the Titan security key system consists of a USB dongle to provide hardware-based two-factor authentication for online accounts, which require the highest level of protection against phishing attacks.  Google sells actually a pair - for the highest level you need two - sells for $50 in the Google Store a pair, which is a USB security key with NFC and a battery-powered micro-USB equipped for recharging Bluetooth NFC key which is used for multifactor.  



LEO:  That's really used on iOS because iOS doesn't support either USB or NFC.  But everywhere else you can use the other secure key.  But on iOS you have to use a Bluetooth key.



STEVE:  Right.  And for something, it's one of their things that you have to have both, like the highest level of authentication.



LEO:  You have to own both.  If you want to use Google's, I forgot what they call it, but the high-end security - which I've done, I set up.  And it doesn't have to be the Google pair.  You can get a Bluetooth.  But they want you to have both a Bluetooth security key and a hardware security key.  But you don't need to use both.  Either one will do.



STEVE:  So anyway, according to Google, the vulnerability only affects the Bluetooth Low Energy, the BLE version of the Titan security keys that have a T1 or T2, that's for Terminator 1 or Terminator 2, sign on - I don't know what it stands for - on the back.  Other non-Bluetooth security keys, USB or NFC-supported versions are safe to use.  So as Matthew says, he's not a fan, obviously.  He would consider Bluetooth security, that phrase, to be an oxymoron.



LEO:  They had to do it because of iOS.  And that's less important now because Apple's opened up the NFC on iPhones.  So it would really - it was for legacy support more than anything else.  You could get away with just a YubiKey with the NFC now.



STEVE:  So the attack scenarios Google's Cloud Product Manager Christiaan Brand described in Google's blog post, he said:  "When you're signing into an account on your device, you are normally asked to press the button on your BLE security key to activate it.  An attacker in close physical proximity" - okay, but, you know, 30 feet - "at that moment in time can potentially connect their own device to your affected security key before your own device connects.  In this set of circumstances, the attacker could sign into your account using their own device if the attacker somehow already obtained your username and password and could time these events exactly."



So, you know, not the end of the world, but it certainly defeats the multifactorness, which is why you have this whole thing in the first place.  And then he continues:  "Before you can use your security key, it must be paired to your device.  Once paired, an attacker in close physical proximity to you could use their device to masquerade as your affected security key and connect to your device at the moment you are asked to press the button on your key.  After that, they could attempt to change their device to appear as a Bluetooth keyboard or mouse and potentially take actions on your device."



So anyway, Microsoft, as I mentioned, initially discovered the vulnerability and reported it to Google quietly, as well as to Feitian, the company that makes these Titan keys for Google and also sells the same product under the trademark ePass, which is its own brand.  They also made a coordinated disclosure about the vulnerability the same day as Google and offered the same sort of free replacement program for their own users.  Anyway, so...



LEO:  It's a Bluetooth LE issue, isn't it. 



STEVE:  Yes.



LEO:  This is part of the LE spec.  LE's always made me a little nervous, to be honest.



STEVE:  And the spec has been improved subsequently.  But again, why did it ever, why did it EVER say that you could negotiate down to a 1-byte key?  I mean...



LEO:  There must have been a reason, like some dumb hardware or something.



STEVE:  Yeah, exactly, or something really underpowered.



LEO:  Remember keyboards.  Keyboards.  Remember they had - we mentioned this years ago, that a number of Bluetooth keyboards used, what was it, ROT13 or something for the encryption key.



STEVE:  Yeah.  They weren't even Bluetooth, they were just low-energy radio.



LEO:  Yeah, just wireless.



STEVE:  And so every time you hit a key, it incremented a counter that was XORed with the ASCII of the key.  Oh, boy.



LEO:  That's it, XOR, yeah.



STEVE:  I mean, that would be a beautiful, not even a grad project.  It would be a...



LEO:  Middle school.  Middle school.



STEVE:  It would be a nice high school, something like a high school project.



LEO:  Yeah, what's wrong with this?  Yeah.



STEVE:  For a class, you know, crack this keyboard.



LEO:  They're probably all still using them.  I bet you there are plenty of them in there.



STEVE:  No doubt.



LEO:  Yeah.



STEVE:  So we did have an iOS update.  We're now at 12.3.  Once again, I don't know what it is with me, maybe Apple just rolls it out slowly.



LEO:  They do.  They stagger them.



STEVE:  Yeah, because it wasn't, I mean, and this happened last week.  But when I was pulling the show notes together last night, I thought, oh, really?  So I went into general settings in my iPhone and said, "Got anything for me?"  It said, "Oh, yeah, we got an update.  Download and install."  It's like, okay, fine.



LEO:  Microsoft does this, too.  Microsoft calls that being a "seeker."  If you're a seeker of an update, they will give you the update.  But they don't alert you to it until later.  But you will eventually get an alert saying, hey, there's an update.



STEVE:  Oh, yeah.  And I do on some devices that I don't really care about or use often.  It'll be like, I'll look, and I'll see the little red, a little tag in the badge.



LEO:  Yeah, yeah.



STEVE:  It's like, oh, I guess there's something I'm supposed to be doing here.  So anyway, they fixed 42 CVE designations, 20 of which, not surprisingly or unusually, were in WebKit.  So about half of them affected the public-facing side of iOS, which is the browser based.  And as we know, Apple tends to be the most tight-lipped of all companies about their vulnerabilities.



There were a couple that jumped out.  There was one, CVE-2019-8585, which affects CoreAudio.  The suggestion that I've seen is that it might give malware a route to compromise an iOS device using a malicious movie file.  If that's the case, it would be serious since it's probably not necessary for the target victim to do anything if, for example, they were just watching, I mean, if they were just receiving it through a text.  Or maybe if it was displayed in a website it might be able to get into their system.



There's another one, 8593 was the AppleFileConduit, and 8605 in the kernel, either of which might allow someone to gain system privileges.  So anyway, we don't know much more.  Probably won't.  I heard you talking, I thought it was interesting, Leo, on MacBreak Weekly about the prevalence of intrusions into iOS.  



LEO:  Oh, yeah, it's an interesting story.



STEVE:  How really at the state level there really is a lot of that going on.



LEO:  Yeah, well, we just don't know because Apple's so locked down it's impossible to kind of look at an iPhone and see if it's infected.



STEVE:  Right.



LEO:  There's no Process Explorer or anything like that.



STEVE:  Right.  So I guess what you'd have to do is, what, just like do a complete wipe and reload from time to time, and then just be really careful about, I mean, the overwhelming, I think the best possible advice is not to just download everything that crosses your path if possible.  Or if you do, you know, recognize that everything you download you are trusting to some level because it's very difficult for these systems to be kept secure.



I did want to mention, as I mentioned at the top of the show, once again we've got a problem that is occurring to some set of people after last Tuesday's Windows 10 Updates.  Microsoft has acknowledged, and they're calling it a "known problem" - well, it's known now - that's causing some user systems to freeze.  Leo, you may get people calling you on your show on the weekend.  This occurs when users have created a restore point before applying the updates, and then for whatever reason subsequently decide they want to back out of the updates.  Which of course should be fine since that's the whole point of having a checkpoint, having a restore point.



It turns out that after this past Tuesday's updates, when users attempt to do this, their system is bricked.  They get a stop error.  You cannot do anything more.  There is a - I have it in the show notes - a link to, if this has affected anybody, or anybody who knows somebody who has been affected by this, there is a process you can go through.  It's detailed and, I mean, it's not fun.  You need to intercept the boot.  I'm skipping over a bunch of this because I don't want to go into it in too much detail.  Intercept the boot.  Get into the Win10 restore environment, the rescue environment, and then jump through a bunch of hoops.



Microsoft explains, they said:  "In this scenario, the system is not restored" - oh, so they said - imagine this.  I mean, this is them speaking.  "You cannot restore the system to a restore point after you install a Windows 10 update.  Consider the following scenario:  You install Windows 10 on a clean computer.  You turn on system protection and then create a system restore point that is named R1.  You install one or more Windows 10 updates.  After the updates have finished installing, you restore the system to the R1 restore point.



"In this scenario, the system is not restored to the R1 restore point.  Instead, the computer experiences a Stop error.  You restart the computer, but the system cannot return to the Windows desktop."  Under cause, they say:  "This is a known issue in Windows 10.  During the system restore process, Windows temporarily stages the restoration of files that are in use.  It then saves the information in the registry.  When the computer restarts, it completes the staged operation.  In this situation, Windows restores the catalog files and stages the driver .sys files to be restored when the computer restarts.  However, when the computer restarts, Windows loads the existing drivers before it restores the later versions of the drivers.  Because the driver versions do not match the versions of the restored catalog files, the restart process stops."



So I have no idea what they just said except it bricks Windows.  So for workaround they say:  "To recover from the failed restart, after the failure occurs, you should be able to restart the computer and then enter the Windows Recovery Environment (WinRE).  To do this, you may have to use a hardware restart switch, and you may have to restart two times."  And then they go on.  So for what it's worth, again, another month and another problem with Windows 10 updates.  And boy, they really do seem to be having a problem with this.



Oh, also wanted to mention that yesterday, Monday, what would that be, May 20th, Microsoft Edge for macOS was officially released.  There's a website, MicrosoftEdgeInsider, all one word, MicrosoftEdgeInsider.com.  And now, under the platform, they're going to be doing a multiplatform release of Edge, not only for Windows 10 and eventually for Windows 7, but also for Mac.  And so the Canary Channel version, which is the rawest, the least stable, is officially available for download.  There's a download button.  The links to pre-release versions leaked a week or two ago, so some people were playing with it.  So I'm not sure who would want to run Edge on macOS.  But if there's some reason you have for doing that, it's now officially sanctioned.



LEO:  I think a lot of people play with Chrome and would be interested in alternative, you know.



STEVE:  Yeah.  Because, I mean, you've already got Chrome on macOS, so...



LEO:  Right, right.



STEVE:  Yeah.  So last [crosstalk].



LEO:  [Crosstalk] want to use it is on Windows.



STEVE:  Yes.  Yes, yes, yes.  And there I think it makes lots of sense.  Last week I mentioned that tomorrow I will be giving a SQRL presentation to the May OWASP Los Angeles monthly dinner meeting in the Los Angeles area.  At the time there were 51 people signed up to attend.  At this point we have 76.  So I don't know if they're regular members who were planning to go and show up, or it looks like maybe we'll have a bunch of Security Now! listeners.



LEO:  Oh, nice.



STEVE:  So if that's the case, I look forward to being able to say hi to some of our listeners who I never really have the opportunity to meet.



LEO:  How nice.



STEVE:  So that'll be cool.  Teams from the Graz University of Technology, whom we have spoken of before, KU Leuven, and Cyberus Technology have been at it again.  And if we ever wanted, as I mentioned at the top of the show, a more perfect example of Bruce Schneier's sage observation that attacks never get weaker, they only ever get stronger, well, we have it here.  What these guys have done, quietly - although they've known for a year, none of us have; they have been sitting, probably impatiently, on this - is a truly significant advancement to the practicality of what was the mostly theoretical attack on speculation and microarchitectural performance boosting that we began talking about right from the start of last year.



So the term "microarchitecture" is all throughout this, so let me just explain.  The architecture is what the code that runs on the processor sees.  So it's the registers.  It's the stack, the various execution units.  It's the view that the processor provides to the code.  It was quite a while ago that that stopped being the only architecture in the chip, especially for the Intel chips, because the instructions are so complicated that, as they kept adding new features, the idea of designing this instruction set, implementing the instruction set as gates, just as simple logic, became impossible.  Especially when they started wanting to get really fancy.



So what happened was they created a processor inside the processor, the so-called "microcode."  So that runs, the microcode runs on the microarchitecture, which is this processor running at the clock speed that is given to the chip, 3GHz or whatever it is, and that processor essentially implements the instruction set that the outside world sees.  So, and that's how, when Intel produces an update to the firmware of the chip, that's the microcode that implements this.



And so, for example, when they talk about microarchitectural data sampling vulnerabilities, the lid that's been torn off of the Intel architecture, unfortunately, is that there are all kinds of - because you have a processor in a processor.  Because there's a whole 'nother dance going on separate from the one that's public.  All of the attention up until now has been on the security of the public processor.  That is, it's been scrutinized, and it's been looked at, and it's been validated as secure.  Nothing leaks at that level.  But as soon as we started looking past that, we got the Spectre and Meltdown problems at the beginning of last year.



So this is just, you know, this is so tasty for researchers.  No one who's really into this has been able to put it down.  And what came to light last Tuesday morning as this podcast was already set up and ready to go was essentially, I would say it's more than the next shoe to drop.  It's an anvil dropped.  Because what happened was the change from theoretical to absolutely proven.  This is probably why Intel was really in a panic.  Intel has known of this for a year.  And so the good news is they've taken responsibility.  I would, oh, my god, I would do anything to be a fly on the wall of, I mean, like, to know really what has been going on inside their meetings because, I mean, it has to be some serious discussion about how they got into this position and how these problems occurred.



But anyway, what we have now, after more than a year of trouble, is the so-called - it's sort of been generalized to "microarchitectural data sampling vulnerabilities."  And to that original work, which first brought us Meltdown, we have on this site, CPU.fail, ZombieLoad, that we had fun just saying last week.  We have something called RIDL, FALLOUT, and Store-to-Leak Forwarding.  Each of those is a different exploitation of subtle features of Intel processors which have been produced over the last eight years, since around 2011, which as a consequence make it extremely difficult for it, the processor, to keep its secrets as it was always designed to.



That is, I mean, until the beginning of last year, we had these processors sitting on servers in cloud environments with VMs sharing a single core among any random set of parties, whether friendly to each other or not.  That all changed at the beginning of last year because it turns out that at that time it was theoretically possible.  A couple examples were shown.  Mostly it was done like, oh, goodness, here's a problem, we need to fix this before it's too late.



Well, what we got just last Tuesday in the form of firmware updates for the microarchitectures, at some significant performance hit, I've seen some benchmarks that look like it takes maybe 10 to 12% off the top, which is a chunk of performance.  Because unfortunately, as we've seen, in order to, you know, these problems are a result of the past of other threads of execution leaving trails, leaving hints in the microarchitecture which clever researchers are able to tease out over time.  And again, these attacks have only gotten better.



Okay.  So ZombieLoad, I'm not going to go into infinite detail because I just want to give our listeners a sense for this.  ZombieLoad resurrects private browsing history and other sensitive data.  It allows the leakage of information from other applications from the operating system across virtual machine boundaries in the cloud and from trusted execution environments.  The RIDL attack allows the leakage of information across various security domains from different buffers such as line fill buffers and load ports.  Those are microarchitecture attributes that are not surfaced at the processor level, but they're all part of the plumbing that's going on behind the scenes.



RIDL demonstrates attacks on other applications, the operating system, virtual machines, and trusted execution environments.  So there's a lot of overlap between these, but they are different types of attacks against different facets of the microarchitecture.  The FALLOUT attack allows reading what the operating system recently wrote and figuring out the memory position of components of the operating system, thus strengthening other attacks.  In other words, it helps to defeat KASLR, Kernel Address Space Layout Randomization.



And, finally, the Store-To-Leak Forwarding exploits CPU optimizations introduced by another microarchitectural component, the store buffer, to also break address randomization, monitor the operating system, or to leak data when combined with some aspects of Spectre.  Spectre created these gadgets which were little pieces of code which were used.  Store-To-Leak Forwarding is able to reuse some of these Spectre gadgets, essentially making the attacks stronger.



Microsoft, for their part, last Tuesday on May 14 wrote:  "On May 14, 2019, Intel published information about a new subclass of speculative execution side channel vulnerabilities known as Microarchitectural Data Sampling."  And of course now we know the data being sampled is somebody else's data, if you happen to be sharing an Intel processor with somebody else.



Microsoft said:  "An attacker who successfully exploited these vulnerabilities may be able to read privileged data across trust boundaries.  In shared resource environments such as exist in some cloud services configurations, these vulnerabilities could allow one virtual machine to improperly access information from another."  So this sounds like a repeat of what we've been talking about all of last year.  "In non-browsing scenarios on standalone systems, an attacker would need prior access to the system or an ability to run a specially crafted application on the target system to leverage these vulnerabilities."



What they didn't explicitly say is that code running in a browser is able to determine what the browser has been doing.  So it's a potentially significant privacy violation just using a browser, at the browser level.  So they allocated, or I should say the CVE system allocated four different CVEs:  Microarchitecture Store Buffer Data Sampling, which was given the acronym MSBDS; Microarchitecture Fill Buffer Data Sampling, MFBDS; Microarchitecture Load Port Data Sampling, MLPDS; and Microarchitectural Data Sampling of Uncacheable Memory, MDSUM.



So over on the CPU.fail site they did a little Q&A to sort of help demystify this.  They asked:  "Am I affected by this bug?"  The answer:  "Most certainly yes."  "Are these software bugs?"  "No, these bugs are in the processor.  Software can work around these bugs, which costs performance.  Future processors will have integrated fixes."  And that is no doubt true.



They ask:  "Can I detect whether someone has exploited this leakage against me?"  They say:  "We have no data on this.  The exploitation may not leave any traces in traditional log files."  "Can my AV detect or block these attacks?"  They said:  "While possible in theory, this is unlikely in practice.  These attacks are hard to distinguish from regular benign applications.  However," they write, "your AV may detect malware which uses the attacks by comparing binaries after they become known."  They ask:  "Has this been used in the wild?"  And they answer:  "We don't know."



So they wrote a 16-page detailed paper that I'm not going to drag us through.  And I've already pretty much explained what I had here, that I quoted from the abstract at the top of the paper.  They conclude, though, saying:  "With ZombieLoad, we showed" - and this is in these 16 pages.  "With ZombieLoad we showed a novel Meltdown-type attack targeting the processor's fill-buffer logic.  ZombieLoad enables an attacker to leak recently loaded values used by the current or sibling logical CPU.  We show that ZombieLoad allows leaking across user-space processes, CPU protection rings, virtual machines, and SGX enclaves.  We demonstrated" - and they did this - "the immense attack potential."



And that's what has changed.  I mean, the takeaway here is this went from being, well, okay, yeah, in theory this could happen.  This research changed this to making these practical attacks.  They said:  "We demonstrate the immense attack potential by monitoring browser behavior, extracting AES encryption keys, establishing cross-VM covert channels, and recovering SGX sealing keys," Intel's secure enclave.  "Finally, we conclude that disabling hyperthreading is the only possible workaround to mitigate ZombieLoad on current processors."



So for what it means to us, as I said, this is primarily a further collapse in interthread isolation.  Interthread isolation only matters if, one way or another, a system has a malicious thread running on it.  In the case of a user's machine, a malicious thread we would call malware.  In that case, you've already got malware on your system, and so you're in trouble.  This could be a way for malware to extract encryption keys from your system.  Like, for example, if you were using BitLocker and depending upon BitLocker's encryption of your hard drive, it would be possible for malware to reach into the kernel to get the key that it would not otherwise have access to because it would be well protected, and then make off with it or send it somewhere, use it, do whatever it wants to.  So but in general, once you've already got a malicious thread on your system, you're already in trouble.



The real problem, as I have mentioned, is in the cloud environment, where you might very well have a huge heterogeneous environment with a datacenter full of servers that are just mixing and matching and running anybody's code on any piece of hardware, moving VMs about without any concern.  That's the way a lot of these systems are set up now.  The problem is a bad guy gets in there on a system that has not been mitigated against this set of problems, and you've got trouble.  I know under Linux with these patches applied and a state-of-the-art Linux, I've seen some benchmarks that look like it takes about, as I said, about 10 to 12% of performance right off the top.  It looks like it's a larger hit on the later processors because the later processors more thoroughly take advantage of sophisticated microarchitecture in order to speed things up.



So as I said, my subtitle was "Speeding Up Is Hard to Do."  Basically, I mean, I don't know what Intel is going to do.  Probably what it means is that we're going to have to build into our operating system some explicit flushing or maybe an additional level of blinding between threads.  Right now there is very little thread isolation between hyperthreads because hyperthreading is basically a second set of registers that you just - you do a very fast context switch from one bank of registers to the next.  It was a clever, very inexpensive way of allowing a processor to continue running when it would otherwise have had its thread stall, waiting for something.



Intel said, hey, wait a minute, you know, with almost no additional logic we can create a lightweight context switch just by creating a second logical thread on the same hardware.  Yes, they were right.  But, boy, you don't want to have that other thread sharing the same core, which is what happens.  You don't want to have it be malicious.



So already we have this notion at the architecture level of context switching.  That's what we do when we push all of the threads on the stack and then load all the registers from a different thread that has been stopped.  That's a context switch.  What I think we're going to end up seeing is Intel having to, in order to save the fundamental design of their chips and implement a system which is secure, we're going to have to get something like a context-switching mechanism at the microarchitecture level.  It should have always had it.  Nobody was looking.  And nobody was this clever about and I guess really appreciated how much information could be obtained from little subtle variations in timing.  At this point it really breaks the barrier, certainly between a pair of threads running on the same core.  And of course, as we know now, since cores share caches, and we're able to probe cache contents, nothing is safe.



So at a cost of performance today, we're getting firmware updates.  Microsoft has said they're only going to be doing it for Windows 10.  So if you are running Windows 7 in an environment where you're at risk - and again, there's a bunch of caveats.  In an environment where you're at risk, then it's worth updating your BIOS.  I guess in general there's no reason not to update your BIOS if one is available from your manufacturer.  But unless you are in an environment where you really are running threads that have a probability, a likelihood, a reasonable possibility of being malicious, if that's not your environment, all of this is interesting, and it's being fixed for you, but probably not a source of - it should not be a source of great concern.



LEO:  Good.



STEVE:  Yes.



LEO:  And I wonder, when they fix it with hardware mitigations, if they can do it without penalty.



STEVE:  I think we're going to get performance back, yes, because, I mean, this really did catch them off guard.  And, I mean, as I've said before, I'm amazed they can fix this with firmware changes.  I mean, that suggests that chips are way more programmable than I would have imagined.  And so the fact that they can actually add what amounts to major features, basically there are feature registers that only the kernel has access to.  Sometimes you only have, like, access at the BIOS level at boot time, and then it's all locked down.  But they're adding bits that were never assigned to feature registers which are controlling significant aspects of the microarchitecture.  I'm amazed that they can do that in microcode.  I mean, it's like the whole thing really is deeply programmable, which surprises me.



LEO:  Well, I mean, all they're doing is saying don't do speculative execution, in effect; right?



STEVE:  Yeah, but they never had a reason not to.  So why let it be turned...



LEO:  Right, why would they give - why is that switch there?  Yeah, yeah.



STEVE:  Exactly, exactly.



LEO:  Well, I mean, what it tells you is that a lot of the way processors work is not in the processor.  It's in the microcode, which is programmable.



STEVE:  Right, right.



LEO:  So, I mean, I'm being - this is a very stupid way of looking at it, but maybe they just did a branch around the speculative part, like there's a chunk of code that says, well, let's see if we can figure out ahead of time what he's going to do.  Just jump over that stuff.  Keep on going.  Don't try.  Don't attempt it.  And the reason I ask if hardware fixes can still give you the performance is I imagine they're just going to eliminate speculative execution in the hardware, which would give you a hit unless they come up with something other than that.



STEVE:  Well, all they have - okay.  Knowing what they know now, they could segment the history...



LEO:  Right.  We've talked about this before.  Because the problem is a leak of information from one thread to another.



STEVE:  Yes, yes.



LEO:  If you don't make that possible, if you hide it, then you don't have to worry about it.



STEVE:  Yes.  If the context switching goes deeper into the architecture than just at the architecture level, if the context switching is pushed into the microarchitecture, then at a cost of significantly more complexity, you get performance, and you get interthread isolation, and that's what we need.



LEO:  Right, right.  Really interesting stuff.  As always, Steve, thank you.  I wanted to say hi to you from a guy named Aaron Miller.  He's a college kid at the Buckeye Career Center in Philly.



STEVE:  Cool.



LEO:  Big listener of the show.  And he and his buddies just won second place in the national BPA Awards.  BPA is Business Professionals of America.  And they have a network design competition.  And there they are with their - that's Aaron on the far right.



STEVE:  Oh, very, very cool.



LEO:  So he says, "I owe it to Steve and Security Now!."  But the other thing which I loved is his network design included Canaries.



STEVE:  Aha, very nice.



LEO:  Not only does he listen to you, he listens to our ads.  So Aaron wanted to say hi.  He and his family, David and Sharon and Aaron were all visiting.  Philadelphia, Ohio, by the way.  If you wonder why there's a Buckeye Career Center in Philadelphia, PA, it's Philadelphia, Ohio - New Philadelphia, Ohio.  Want to get that right.



Thank you, Steve.  We see people all the time who say, "Say hi to Steve.  Tell Steve I owe it all to Steve."  A lot of people who are in IT say, "Yeah, I listen to Steve."  In fact, I met a guy - you'll love this - on Sunday.  No names.  He's in the Air Force.  He is at Stanford right now studying security.  He is going back to work in North Carolina at Fort Bragg, where he'll be responsible for securing our nation's infrastructure against election fraud and other influence from other nations.



STEVE:  Nice.



LEO:  And he says, "And I listen to Steve religiously."  I said, "Wait a minute, you're operating at this high a level?"  And he said, "Yeah, and I still learn a ton of stuff from Steve."  So that's pretty high praise, Steve.



STEVE:  Neat.  Very nice.  Very cool.



LEO:  Hey, we are done with Security Now!.  You can go to the website, Steve's website, GRC.com, and get yourself a copy.  He has regular 64Kb MP3s, but also 16Kb MP3s.  They don't sound perfect, but they have the advantage of being teensy-weensy.  But the smallest version of the show you can get from him is the handwritten, human-written transcript from Elaine Farris, who writes a very nice transcript of the whole show.  So you can read that.  Best solution is to read while you listen, and use your underliner and notes.  And then maybe you can win second place in the BPA Nationals.



Steve also has lots of other things at GRC.com, his website, including of course his bread and butter, SpinRite, the world's best hard drive maintenance and recovery utility; and lots of information about other things, including SQRL, soon to be a major motion picture.  Or something.  He's been writing it for years.  We are at TWiT.tv/sn.  We have video there, and of course the best thing to do is subscribe in your favorite podcast program.  That way you'll get a copy of it the minute it's available every Tuesday afternoon.



Quick reminder, we record Tuesdays, 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  You can tune in and listen or watch live at TWiT.tv/live.  If you do that, hang in the chatroom, nice people who are all listening at the same time so you can kind of have a conversation about what we're talking about amongst yourselves.  I'm in there, too:  irc.twit.tv.  Steve, thanks so much, and we'll see you next time on Security Now!.



STEVE:  Thank you, my friend.  Till next week.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#716

DATE:		May 28, 2019

TITLE:		RDP:  Really Do Patch

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-716.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm 



DESCRIPTION:  This week we primarily focus upon the almost certainly impending doom of the Internet, as the Windows Remote Desktop Protocol saga finishes out its second week with a great deal of news and new evidence-based expectation for the end of humanity as we have known it.  Okay, well, maybe it won't be quite that dramatic, but it already makes last year's Meltdown and Spectre flaws seem quaint.  But before we get to that, we take a look at the FIVE new zero-day exploits just dropped by SandboxEscaper, Google's discovery and confession of 14 years of cleartext password storage, Microsoft's just-released Win10 Feature Update 1903, Firefox's release 67, and some interesting new data about the prevalence of validly signed malware.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We'll talk about the rise of ransomware, the return of SandboxEscaper, and the new Firefox 67 and why you might want to install it.  And then Steve will announce the demise of the Internet.  Yes, it's all next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 716, recorded Tuesday, May 28th, 2019:  RDP:  Really Do Patch.



It's time for Security Now! featuring Mr. Steven "Tiberius" Gibson, the man in charge, the guy who knows all, tells all, sees all.  He's for the last, what is it, what did we say, 13 years been covering the security scene on Security Now!.



STEVE GIBSON:  No, no, we're close - well, okay.  We're closing in on the end of 14.



LEO:  Holy cannoli. 



STEVE:  Yeah, two more months.  Elaine corrects me whenever I'm wrong.  And it's August.  This August we finish year 14 and just go cruising right into number 15.



LEO:  Lordy, lordy, lordy.



STEVE:  If, that is to say, Leo, if there is still an Internet.



LEO:  You think there's a chance there won't be?



STEVE:  Well, given what is apparently about to happen, we'll see.  Today's podcast 716 of Security Now!, RDP stands for Really Do Patch.  We're going to primarily focus upon the almost certainly impending doom of the Internet.



LEO:  Well, wait, hold on there.



STEVE:  As the Windows Remote Desktop Protocol saga finishes out its second week with a great deal of news and new evidence-based expectation for the end of humanity as we have known it.



LEO:  You've been reading too much Peter F. Hamilton.  I'm just going to say it right now.



STEVE:  Now, maybe, maybe it won't be quite that dramatic.  But it already makes last year's Meltdown and Spectre worries seem quaint by comparison.



LEO:  Yeah, yeah.



STEVE:  But before we get to that we're going to take a look at the five, count them five, new zero-day exploits that were just dropped by our hiking friend and photographer SandboxEscaper.



LEO:  Oh.



STEVE:  You remember her, Leo.



LEO:  Oh, yeah, yeah.



STEVE:  She's got that really cool little one-person tent thingy that I thought was so cool.



LEO:  She's the one who was really miffed.



STEVE:  Oh, believe me, she is.  In fact, I forgot to obscure the F-bomb in her tweet that I have a screenshot of in the show notes.  When I was just scrolling through it before we began I thought, ooh, I didn't blur that.  She's not doing well. 



LEO:  Oh, boy.



STEVE:  Except, boy, is she, like, a gifted hacker.  Anyway, so we're going to update everybody on that.  We've got Google's discovery and confession of their 14 years of cleartext password storage.  Microsoft's just released, finally, Windows 10 Feature Update 1903, and we'll talk about the new features there.  And how to get it because none of my machines wanted to get it by themselves.  So there's a way to kind of give it a little kick, for those listeners of ours who want to play with it.



We also have the release 67 of Firefox with some new features, and some really interesting new data about the prevalence of validly signed malware.  Our listeners will remember that I was annoyed when I had to replace a perfectly good, but now expired after three years, Authenticode signing certificate with a new one because none of the AV knew the new certificate.  Even though it was from me, hello, they didn't care.  And we're going to look at, like I didn't realize this was as bad as it was, and it won't surprise anyone to know who the number one certificate provider is for malware.  More than half of the malware is being signed by certificates from just one provider.



LEO:  Wow.



STEVE:  So, and we already know who, but we'll get there in time.  So I think another great podcast for our listeners.  And really, you know, enjoy the Internet while it lasts.



LEO:  Well, you know, on the bright side we'll all have more time off.



STEVE:  We really...



LEO:  I'm going to buy some books.



STEVE:  We'll talk to friends that we haven't actually seen in the physical presence because there won't be any Internet soon.



LEO:  Holy cow.



STEVE:  So, yeah.  So if you, like, wanted to download something, get it now because you won't be able to.



LEO:  You know, the ratings are fine.  You don't really need to puff this stuff up.  I'm just saying.



STEVE:  Leo, Leo, we're going to be gone.  We won't be able to do a podcast.



LEO:  That's true, with no Internet.



STEVE:  It'll be - this is, like, 716.  Remember this podcast, 716.  I thought I was going to make it to 999.  No.  There will be no Internet to convey this quality content.



LEO:  Holy camoly.  Well, we'll find out what the heck he's talking about in just a second.  All right.  Why is the Internet over, Steve?



STEVE:  You know, I was just thinking, it's sort of a self-fulfilling prophecy because, if the Internet's gone, we won't be able to deliver the podcast, but we'll also have nothing to talk about.



LEO:  Right.  So it's perfect.



STEVE:  It's like, yeah, it's just sort of like, okay, it's a wrap.  Okay.



LEO:  That should have been the title of the podcast.  It's a wrap, folks.



STEVE:  It's a wrap.



LEO:  See you in Fiji.



STEVE:  Our Picture of the Week sort of surprised me because I realized I'd had it in my backlog for so long that it was now no longer current.  What this shows is - it's really sort of interesting.



LEO:  It looks like the London Underground map.



STEVE:  It does.  And it takes a while to sort of parse it visually.  This is the ransomware timeline up to just shy of 2017.  So, first of all, it doesn't contain the last two years that have been, if anything, even more busy.  But you can see where it began, what, like almost in the middle of 2012 was the first instance.  Then we have 2013, maybe six or seven more.  2014, maybe twice that many.  2015, oh, it's beginning to catch on.  People are thinking, hey, I can make some money.  And then, boy.



LEO:  Boom.  



STEVE:  Month by month, January, February, March, April, May - and it bounces back and forth.  May, June, July, August, September, October - 2016 is just insane.  And I would imagine we really couldn't show 2018 and 2019 because the print...



LEO:  It wouldn't fit.



STEVE:  ...would have to be so fine that you wouldn't be able to read it.  You would just think...



LEO:  Crazy.



STEVE:  Okay, don't know what that is.  But anyway, I mean, those are all individual instances of encrypting ransomware which have landed on the industry, and people have been hoping to make some money at.  So unfortunately, they're doing it enough to keep it happening, as we know.



So we talked, I think it was, what, last year?  SandboxEscaper is this person's Twitter handle, and I've never seen a reference to their first name.  There's a blog which is no longer public that you and I checked out at the time when we were first covering her work.  And we discussed this person a little bit.  It wasn't clear what this person's birth sex was or how they wanted to be referred to, so we're just saying "her" because that seems like the right thing based on conversations in the industry.  But this person is a gifted hacker who, unfortunately, is also sort of unhappy with the industry.  We know they love hiking and camping and photography.  That was what the blog was full of.  That and zero-days that they were dropping onto the world without giving Microsoft, apparently, first of all, at the time it seemed, without being interested in earning any money from them.  Maybe that's changed a little bit.



So anyway, so the blog is currently closed to the public.  Her GitHub, which was active a week ago, has been closed after putting five zero-days, exploits against Windows, out in public.  Being zero-days, we know that means that there's no notice.  So a week ago tomorrow, so that's last Wednesday the 22nd, apparently in a low moment she posted on Twitter.  And I have the image of it.  Wednesday, May 22nd, 2019 she says:  "There's two more bugs on GitHub.  Eff this" - and she didn't say "eff" - "this shitty industry.  I don't plan to make a career in it anyway.  I hate all the people involved in this industry.  Everyone just thinks they know better.  Everyone just loves pointing fingers.  Bunch of apes.  Bye."  Posted by SandboxEscaper at 2:47 p.m.  So as I said, not having a good day.



So in this very few short days, what we've received from this admittedly gifted hacker are five brand new zero-day exploits against Windows 10.  One turns out to have been already fixed, which she later confirmed.  But that still leaves us with four that are currently unpatched.  And they were dropped, presumably on purpose, right after, I mean the day after this month's Patch Tuesday, thus creating the biggest problem for Microsoft.



Although I ought to also mention these are not, like, end-of-life problems.  These are not like the Remote Desktop Protocol problem.  These are privilege elevation discoveries that she made which are definitely a concern because, as we've seen, when those are combined with other exploits that would otherwise be limited to the privilege and the context of the current user, this allows them to escape that and get into the kernel and make themselves persistent and so forth, do dramatically more damage.  Because we really do depend upon the privilege controls in Windows for our security.



So the security industry, I mean, we sort of don't know which way to turn, whether it's this or the Remote Desktop Protocol problems.  She whipped the industry into something of a froth with her release of these because she also preannounced that more were coming.  So in looking around the industry, I sort of landed on Davey Winder's thoughtful reporting for Forbes which I'll share because he sort of explains it and puts it in context.  We've got two pieces, one last Tuesday and then one the following day, on Wednesday.



On Tuesday he wrote for Forbes:  "New Windows 10 Security Exploit Can Read All Your Files - What You Need to Know."  He writes:  "A security researcher with a history of releasing zero-day exploits for the Windows operating system has struck again, this time just days after the latest Patch Tuesday security updates were rolled out.  Which means that it's unlikely there will be a fix for Windows 10 users until June 11 at the earliest.  So what did SandboxEscaper just drop into the Windows threatscape, what are the risks, and is there worse to come?"



He says:  "A security researcher going by the name SandboxEscaper has posted a proof-of-concept demo for a Windows zero-day exploit online.  This local privilege escalation (LPE) exploit is the fifth in a series of zero-days that SandboxEscaper has dropped into the Windows environment over the last year."  So fifth, considering the ones we've already talked about previously, not some that were even going to happen after this reporting.



He said:  "The latest proof of concept doesn't enable anyone to actually access your computer, but it does provide a method by which those who do so can upgrade their system privileges to an administrator level and in so doing grant them carte blanche to your data.  SandboxEscaper has previously used the Windows Task Scheduler tool" - which is the one we most recently discussed - "for nefarious purposes, and this latest zero-day is no exception.  It uses it to import and run a malformed task file that exploits a vulnerability in the way Task Scheduler handles discretionary access control list rights for such files without permissions, giving full control to any user, rather than just the system admin who would normally have it."  Oh, and it was initially released for 32 bits, but it's been confirmed to work under 64 bits also.



He says:  "So what was the motivation?  As mentioned, SandboxEscaper has a reputation for releasing exploit code without any prior disclosure to Microsoft.  Reporting on one of these last year, Forbes contributor Marco Chiappetta suggested that 'depression may have been a factor in SandboxEscaper's decision to post the exploit,' and quoted her as saying, 'I screwed up, not Microsoft.  They are actually a cool company.'" And then finishing with "Depression sucks."



"However, in her latest blog postings announcing the new exploit, SandboxEscaper writes, 'I don't owe society a single thing.  Just want to get rich and give you efftards in the West the middle finger.  I'm donating all my work to enemies of the U.S.'"  Then he says:  "Make of that what you will.  The timing is also interesting as it comes straight after the monthly Microsoft update cycle, which means it leaves the window of exploit opportunity open until June 11, when the next cycle is scheduled."  And of course, as we know, when something has really been bad, Microsoft has broken their own schedule in order to push out something more quickly.  I don't think these rise to that level, in my own opinion.  You know, local privilege escalation is not good, but it's also not RDP.



So, he asks, "Is there worse to come?"  He says:  "It appears that this isn't going to be the last we hear from SandboxEscaper either.  In that same series of blog posts, she says that she has four more unpatched zero-days.  'If any non-Western people want to buy LPEs,' she writes, 'won't sell for less than 60K.'"  And then he quotes Ian Thornton-Trump, head of security at AmTrust International, saying he told him during a conversation this morning that as far as the economics of selling exploits are concerned it's kind of a, well, he said "sh*thead" move.  "You can understand why, as Microsoft is known for having a pretty generous bug bounty program."  We've talked about that recently, in fact.  It is pretty good.  Although, again, I don't know that these would rise to the level of being a high payout, which researchers use to cash in on their findings without taking the criminal route to riches.  He said:  "It's sad that folks burn the opportunity to contribute to the information security community."



So what can we do to mitigate the risk?  Given that it's unlikely, based on responses to the previous exploits released by SandboxEscaper, that we will see any patch to this zero-day until the next Patch Tuesday, and I concur with that opinion, he says:  "What can you do to mitigate the risk?"  He quotes, again, Thornton-Trump from AmTrust, saying:  "I will tell you that anything that interacts with the task scheduler is going to be pretty unsubtle and fairly easy to detect."  So in other words, maybe an update to Windows Defender and other third-party AV would be enough to catch these things in the short term, until Windows is able to go through a full cycle.  And that's sort of the pattern that we're beginning to see now.



He says:  "Of course, that doesn't mean it will be an impotent threat, and zero-days must always be considered a very real and present danger to data."  That said, Davey quotes Thornton-Trump as saying he isn't panicking over this, and pretty much I agree.



So then the following day he posted on Forbes:  "Rogue Security Researcher With Grudge Against FBI Goes on Windows 10 Exploit Spree."  The beginning of his posting basically reprised what he had written from the day before, which I've cut out.  And then he goes on, saying:  "Things have just become a little more problematical with SandboxEscaper releasing on May 22nd another two of the four remaining zero-days she claims to be in possession of.  The first is similar to the local privilege escalation exploit released on the 21st, but this time exploiting a vulnerability in the Windows error reporting service.  It's harder to exploit, and SandboxEscaper admits as much, to the point of conceding it's 'not that much of an issue.'  It is, however, still a vulnerability that can be exploited, and others could well find more efficient methods to do so until it is patched.



"The second zero-day targets IE11, specifically allowing for the injection of malicious code.  Again," he writes, "this would not seem to be a critical vulnerability as the proof of concept code appears to reveal that it isn't a remote exploitation threat, but rather something a threat actor with access to the machine could use to disable Internet protected mode for further attacks."  So again, something that might be chained with other exploits, not itself a huge problem.



He says:  "I think that these zero-days are all worrying, but not critical, as they all require the attacker to already have access to the target system, or possibly use these exploits alongside a remotely executable one that amounts to the same.  As such, the immediate threat to most users would appear quite low."  I don't know that I would go quite that far.  For myself, remember that we've recently seen several examples where a local privilege escalation vulnerability has been combined with one or more different exploits in a chain to create a quite significant threat.



For example, in March, we talked about Google's report that a then-unpatched local privilege escalation vulnerability in Windows was being used in combination with an unrelated exploit in Chrome.  On its own, neither exploit was able to do much damage, thanks to the mitigations built into both Windows and Chrome.  But together, the exploits allowed hackers to remotely execute malware of their choice.  So again, they couldn't have done that were it not for the combination of both.  So when you pair a privilege escalation vulnerability such as what SandboxEscaper published, with something else that also itself doesn't seem to be a problem, you know, that's what we're beginning to see.



For example, I think, if I recall, every single one of those Pwn2Own major cash awards involved a chain of several exploits, cleverly put together, in order to get ownership or code running on the target machine.  So that was a comment I made at the time is that we're no longer seeing a world where just one problem - well, except in the case of this remote desktop protocol that we'll be talking about a lot here shortly.  Normally it is a chain of these sorts of things.  And we also have a malicious side of the industry that wastes no time jumping on these things.  So you've got to know that the instant these zero-days dropped, there were bad guys immediately looking to see how they could be leveraged while they were still useful and had not been blocked.



So anyway, Davey continues and finishes, saying:  "That said, there are still two more exploits in the SandboxEscaper arsenal."  Although since we haven't seen them, maybe she did find buyers for them, who knows.  He says:  "And we will have to wait to see what they bring when, and suggest it isn't going to be if," he says, "they are released."  Although maybe she didn't find buyers, as I said.  "Given the events so far this week I suspect we won't have long to wait."



And he says:  "The motivation behind the release of these exploits doesn't seem to be financial.  The exploits themselves are not without value to both vendors and threat actors alike, but given their relatively low threat impact, probably wouldn't be worth a fortune in bug bounties" - and I agree with that - "or if sold via an exploit broker.  There are clues in the SandboxEscaper blog as to the real reasoning, and they are not subtle.  The motivation would seem to be getting back at the U.S. for a perceived injustice."  He wrote, and this is the result of his digging, and I was unable to get into the blog because by the time this was all public, the blog - actually she took her blog private and posted that it was for now access by invitation only.  So who knows what was happening.



But he writes:  "The most telling is the confession that she has 'most definitely given portions,' she says, 'of my work,' meaning her work, 'to people who hate the U.S.' because 'that's what happens when the FBI subpoenas my Google account and intrudes my privacy.'  SandboxEscaper goes on to suggest that the people who have access to the exploits 'are going to use those bugs to get back at U.S. targets,' before finishing with 'an eye for an eye.'"  And then he writes:  "It's not just the FBI and the U.S. that are on the receiving end of this apparent hatred.  Some of it is reserved for the information security industry itself."  And then he quotes that posting that I had quoted at the top of this.  So anyway.



So then finally, in an update of that - that was the 22nd.  So then on the 24th, on Friday, he said, he updated his posting, said:  "SandboxEscaper has now confirmed that the Windows error reporting bug was apparently patched this month, and so that's one fewer to worry about.  Unfortunately, she also now has released two more zero-day exploits," which he enumerates, also local privilege elevation.  So that makes a total of nine exploits, eight of which are zero-days released across the last 10 months by SandboxEscaper.



He says:  "It also marks the end of the exploit spree, at least for now, as there is no further information to suggest she has any more exploit bombs ready to drop."  He writes:  "I would also like to add that mental health issues in the information security industry are rife; and, reading her blog entries, it certainly appears that depression has played a part in SandboxEscaper taking this destructive path with her undoubted abilities.  I sincerely hope," he says, "despite what she has done, that she can get some help with all this and find some inner peace."



And of course, as I had commented, I remember taking a very close forensic look at some of her earlier work.  I think it was the task scheduler piece that she did where there was a detailed analysis of it.  And I remember saying on the podcast how very impressed I was.  I mean, it was some beautiful work.  And so sad that this is how she's chosen to use her very obvious skill.  So that's the way it is.



It turns out that, following in the steps of Facebook and Twitter, Google has become the latest technology giant to confess that it accidentally stored G Suite enterprise passwords unprotected in plaintext on its encrypted servers.  I have a link in the show notes to their blog post.  The blog post, published last Tuesday, was titled:  "Notifying administrators about unhashed password storage."  This was posted by Google's Suzanne Frey, who's a VP of Engineering for Cloud Trust.



She wrote:  "Google's policy is to store your passwords with cryptographic hashes that mask those passwords to ensure their security."  I cut out a bunch of her explanation about hashing of passwords because we all know about that.  She said:  "However, we recently notified a subset of our enterprise G Suite customers that some passwords were stored in our encrypted internal systems unhashed.  This is a G Suite issue that affects business users only.  No free consumer Google accounts were affected."  And she wrote:  "And we're working with enterprise administrators to ensure that their users reset their passwords.  We have been conducting a thorough investigation and have seen no evidence of improper use or access of the affected G Suite credentials."



She says:  "In our enterprise product, G Suite, we had previously provided domain administrators with tools to set and recover passwords because that was a common feature request.  The tool, located in the admin console, allowed administrators to load or manually set user passwords for their company's users.  The intent was to help them with onboarding new users."  For example, a new employee could receive their account information on their first day of work, for example, and so already have an admin-set password in it, and then also for account recovery.  "The functionality to recover passwords this way no longer exists."



She said:  "We made an error when implementing this functionality back in 2005," thus the 14 years.  She said:  "The admin console stored a copy of the unhashed password.  This practice did not live up to our standards."  Certainly Google knows about hashing passwords.  So, yes, just a mistake.  She says:  "To be clear, these passwords remained in our secure encrypted infrastructure."  So the point being, although they themselves were not hashed, the servers themselves were encrypted.  However, of course, the concern is that employees who would have access to the server content unencrypted would then also have access to the passwords in plaintext.



So she says:  "This issue has been fixed, and we have seen no evidence of improper access or misuse of the affected passwords.  In addition, as we were troubleshooting new G Suite customer sign-in flows, we discovered that starting in January 2019" - so just the beginning of this year - "we had inadvertently stored a subset of unhashed passwords in our secure encrypted infrastructure.  These passwords were stored for a maximum of 14 days.  This issue has been fixed; and, again, we have seen no evidence of improper access to or misuse of the affected passwords.  We will continue with our security audits to ensure that this is an isolated incident."  And so on.



So as we know, this can happen.  We recently talked about Facebook "discovering logs" of unprotected passwords for hundreds of millions of its users, both Instagram and Facebook.  And nearly a year ago, Twitter reported a similar security bug that unintentionally exposed passwords for its 330 million users in readable text on its internal computer system.  So I guess we have to hope that, for the sake of our whole industry, that our whole industry's legacy behavior will be fixed.  So like, you know, everybody will know you just don't do this.  You hash passwords.  That we cannot mistakenly have code that leaves passwords lying around in plaintext.



And of course hopefully where we will end up being is adopting systems eventually like SQRL that we'll be looking at very shortly in some more detail, which in the first place gives websites no secrets to keep.  That's one of my favorite slogans for SQRL is "It gives websites no secrets to keep."  They get a public key that they don't even have to keep secret because there's nothing malicious that can be done with it.  So I have a feeling we'll be migrating in that direction sooner or later.



As I mentioned at the top of the show, Microsoft has released Windows 10 version 1903, which is also now formally called the May 2019 Update.  This is unlike the monthly security updates.  This is the one which they do twice a year which adds features to Windows 10.  None of my systems - I Skype over a Windows 10 machine, and I have a couple Windows 10 machines set up in VMs.  I also have a couple laptops.  Unfortunately, as I've mentioned before, when I discovered the long-term servicing channel, the LTSC version of Windows 10, I immediately fell in love with it.  I mean, it makes Windows 10 bearable because you're not having Candy Crush Saga put onto your Professional Windows 10 setup.  So but of course the long-term servicing channel, I mean, the whole point of it is that you have an install that Microsoft is committed to providing security updates for over the long term, rather than forcing you into this every six month update cycle.  So of course you explicitly cannot do feature updates on those.



So while covering this, I updated my Windows 10 machines, went to Windows Update, brought them current, and then pressed Check for Updates, and nothing happened.  So we know that, although it is officially released, Microsoft is throttling its availability, so it may not be available.  I will get to a URL.  I created a bit.ly shortcut that takes users directly to a small downloadable from Microsoft which will trigger the on-demand update of Windows 10 to version 1903.



LEO:  Ooh, don't do that.  No, no, no.  Don't do that.  Why would you want to do that?



STEVE:  We have listeners, I'm sure, who are going to want to play with 1903.



LEO:  Microsoft, no, my strong advice, if Microsoft does not offer you 1903, it's not necessarily because they're staging it.  It may also be because there's hardware compatibilities.  They check issues, and they don't install it or offer it to machines that are not supposed to run it.  When you override that, well, good luck.



STEVE:  Well, this does not override that, Leo.



LEO:  Oh.



STEVE:  This performs all of those tests.  And this is the officially sanctioned Microsoft link.



LEO:  Oh, I see.



STEVE:  Yes.  It's possible to dig down through some menus and get there, but it's just, I mean...



LEO:  I still wouldn't do it.  I would wait until Microsoft offers it to you, honestly.



STEVE:  Yeah.



LEO:  Why not?  Unless you're, like, I have to have the newest thing.  But you know better than anybody how dangerous that is.



STEVE:  Yeah.  So I have it installed on some VMs.  It's not on any production machine of mine.  So anyway, it is there.  You can ask Windows Update for that.  They call it the Windows Update Experience, which is the way you're able to get that.  And so you can click a couple buttons in order to ask Microsoft for it.  Because, I mean, it is officially available now.



Okay.  So what's new?  We get the Windows Sandbox, which is probably the biggest new feature of it.  It's a lightweight virtualization sandbox which allows a Windows 10 - this is Pro - let's see.  It's not Home.  And it's not Enterprise.  It's Pro.  There's two versions of Windows, Pro and one other.  I can't remember the name of it now.  Oh, it says "Pro and above."  It requires a 64-bit processor.  You do have to have virtualization enabled in the BIOS.  And it's going to eat up a lot of RAM.  Windows 10 will run in 4GB, but when you launch this it immediately starts to take up about the same amount.  So you probably need to have 8GB of RAM in the system.



Microsoft is billing this as a way for power users to run things in a true Windows 10 VM that they don't want to allow have access outside of the VM.  There are some downsides, which are that every time you start this, it starts with a fresh image.  So I guess the good news is nothing that you do in there persists.  But that can also be a pain, for example, if you wanted to check Word or Excel extensions or add-ons or downloads.  Well, you're not going to have any apps installed in that whenever you run that.  It starts blank every time.  Which is, again, sort of a mixed blessing.  It does use the Hyper-V system.  So if you install this sandbox, then it will fight with VMware and Virtual Box that want to use the same virtualization features on your system.



And I guess Microsoft must figure that, well, somebody who is a VM user, like who's using a full VMware or Virtual Box, well, they're probably not the profile of the user who might want to use this Windows 10 Sandbox.  But it's there.  The way you get to it is in the features and settings.  You click that, and it brings up a list of all of the installed features under Windows 10.  And if it's available, you'll see that Windows Sandbox is listed down toward the end of that list.  When I first fired it up in a Windows 10 VM, it was grayed out because I had not enabled virtualization for that VM.  So I shut down, turned that on, then I was able to install Windows Sandbox and play around with it and get some sense for how it works.



Anyway, that's pretty much the biggest new feature.  There's a light theme for Windows 10, sort of the flipside for those who like the dark theme.  And then there's also the new update controls that we've been talking about where users are able to defer updates for some length of time.  But again, Microsoft is still going to keep people moving forward through the update channel if that's the version of Windows you're using.  Oh, and Search and Cortana are split, so that they're no longer merged.  There are separate appearances of them on the taskbar.  And I meant to look at that and forgot when I was playing with it this morning.



So anyway, that's all there.  And I imagine, if you don't ask for it, Microsoft will roll it out over time.  Oh, and they did solve the problem with external drives being connected.  For a while, remember, for the insider and fast ring, they were no longer installing 1903 because there was a problem with drive letters being changed on the fly.  That they sort of incrementally fixed and got to a point where they are convinced that it's now working, so 1903 is rolling out again.



So Mozilla just released Firefox 67 with a bunch of welcome enhancements.  They have improved its use of memory, meaning using less, and also managed to increase its speed.  It has a smarter page renderer that no longer waits to load features that are not necessary to display a web page.  So it brings the page up, and then other non-display things that the page has asked for it then is able to catch up with and load in the background.  So apparently, I mean, it's like way snappier, they're saying  40 to 80% faster on common pages that people load.  So that should improve the experience.



Also, I was just talking maybe a couple podcasts ago about all the memory that tabs consume, for those of us who use lots of tabs.  And I was talking about an add-on, Auto Tab Discard, which was sort of smart about releasing memory from tabs that you weren't using.  Turns out that's now build into Firefox 67.  It will be smart about that.  If it sees that you're beginning to consume memory, what they said in their announcement is that, when less than 400MB is available, just to give you some headroom, Firefox will start suspending and unloading the least recently used tabs.  So that's nice.  I mean, you can always just click on it, and then it just reloads it.



So it does slow down instant tab switching, if that's the case.  But if you've got lots of tabs open and not enough memory to contain them all - and, boy, I'm surprised by how much memory web pages take these days.  I mean, I watch it as I open tabs and think, what are these doing?  But, you know, that's the nature of the world now.  67 also adds fingerprint and cryptomining blocking features natively, which is kind of cool.  You need to go to - in Firefox there's Standard and Strict, and then there's a third, which is Custom.  You need to go to Custom Content Blocking, which brings up a dialog that now has two by default unchecked checkbox options, one for cryptominers and one for fingerprinters.  So it will introduce some technology to block both of those behaviors.



Oh, the other nice thing is that it used to be that the private browsing mode did not bring with it the extensions which have been installed outside of private browsing.  And that of course created some inconvenience for users, for example, who depend upon LastPass in order to log into anything anywhere these days, as I do.  We're not supposed to know our own passwords any longer.



So one of the new features in 67 is that, on a per-extension basis, you're able to permit extensions to also appear in private browsing mode.  When I upgraded to 67, it defaulted to them all being enabled for private browsing.  If you go to the add-ons page, you'll see like a purple tag saying "Allowed in Private Windows."  You can selectively turn those off if you don't want those extensions to also be over on the private side.  Then moving forward, whenever you install a new extension, Firefox will prompt you for whether or not you also want that to appear in private windows.  So you've got complete control over whether or not they are there.



Oh, and their built-in Login Credential Manager for storing and retrieving the browser's native username and password storage will now also be available there.  So a lot of the things, I mean, it used to be a very restricted environment which in some cases made it awkward for people to use the private browsing mode, so they've made that much smoother.



Finally, they're getting ready to test what they're calling their WebRender rendering engine.  They'll be rolling it out slowly.  It will be using the system's GPU for 2D rendering of web pages to further improve rendering speed.  It'll be first available for Windows 10 users who have NVIDIA graphics cards, and over the course of the year it's expected to be rolled out further.



I mentioned, at the top of the show and also previously, how annoyed I was, but I could understand, the fact that a newly issued Authenticode certificate from DigiCert was not initially trusted.  And thinking about it a little bit, I could understand why that was the case because, as we know, certificates function on a default trust model.  That is, by default we trust a big bunch of certificate authorities, and we also by default trust any certificate which they have signed, unless and until we find out otherwise.



So there was an interesting piece in Medium which was discussing a nice piece of analysis done by a group that posted their work on GitHub.  I have the Medium link in the show notes.  And also, Leo, that second link, the GitHub link, is probably worth scrolling down through, if you're curious.  It is a list of malware signed with valid security certificates.  And for what it's worth I understand this problem now more than I did.  The group of researchers are with Chronicle.  They scanned VirusTotal to gain a deeper understanding of the issue.



As Medium explained, for this investigation researchers only included Windows standard Portable Executable, as they're called.  All Windows EXEs are called PE, Portable Executable files.  They filtered out samples that had less than 15 detections.  So for what it's worth, I don't think mine ever had that many, so it would have never even been a false positive in this research.  They aggressively filtered out what they called "grayware" files, and calculated the distinct individual number of samples each signing Certificate Authority was responsible for.  So they collected data within the past year, that is, the 365-day span starting on May 7th, 2018, ending on May 8th, 2019.



So what did they find?  Okay, this is VirusTotal, malware signed with certificates that have initially been trusted.  They found 3,815 distinct separate malware, pieces of malware, that met all of their filtering criteria.  So in other words, there is a lot of true malware now being signed by valid certificates which are being issued by trusted certificate authorities.  And as I teased at the beginning of the show, suggesting that we can guess who the number one CA was providing certificates that malware is using, that's our old friends, Comodo.  



LEO:  I almost said the Hong Kong Post Office.  But then I knew  it's got to be Comodo, yeah.



STEVE:  It's got to be Comodo.  And remember it was a couple months ago that I ran across some malware where the note was that it was signed with a valid certificate, and the certificate was Sectigo.  And I thought, huh?  Never heard of them.  And I said at the time, I did a little bit of digging.  Turns out that Comodo renamed themselves Sectigo because, of course, who would want to be Comodo with the reputation that they had acquired?  So it turns out that, if you sum the certificates issued by Comodo and Sectigo, because after all they're the same company, they provided the certificates which signed 1,957 of those 3,815 malware samples.  That's more than half.  51.3% were all signed by certificates issued by that one certificate authority.



I have in the show notes a graph that you had on the screen a second ago, Leo, which demonstrates just, I mean, just the dramatic skew that exists.  Comodo was at 1,775.  Then it dropped to less than a third of that, meaning that Comodo is more than three times more than the second greatest number of malware samples signed, and that's a Thawte, you know, T-H-A-W-T-E, at 509.  Then at less than, well, okay, almost just half of that is VeriSign.  And then Sectigo is also - of course that's the same as, as we know, Comodo - is at 182.  And then the fifth is 131 with Symantec.



So as the Chronicle guys note, there's a precipitous drop-off in the numbers showing a decidedly non-uniform skew.  And I don't know why.  When I looked through that list on GitHub, there were some weird-looking names.  I think that what happens is Comodo has resellers, and so they're selling through third parties.  And maybe that's the problem is that they're not vetting their third-party resellers well.  There was something like Lemon Lime, or Lemonade or something, it was a name that came up in a lot of the certificates that had to have been - in fact, those also had Comodo in their name.  But it was like Lemon Time or Lemon - I don't remember what it was.  I was looking through it, thinking what the heck is going on here?



So anyway, so there is a certificate revocation process in place.  Certificates are being revoked.  The problem is that, unlike the real-time communications certificates that we have with websites, for example, when a web server's cert expires, nobody will ever trust its signature who has their clock set correctly.  But think about it.  You don't want that to be the model for code signing because you could have a repository where a perfectly legitimate signed piece of code has a certificate.  After all, the certificates, as we know, only last three years.



So the way this is handled is that, as long as the certificate signing time is timestamped and the certificate was valid at the time of timestamping, then the thing that certificate has signed is considered valid in perpetuity, unless that certificate is specifically revoked.  So what happens, for example, when I'm signing things, there's a URL as part of the signing process that makes a real-time query to DigiCert's time server.  The hash of what's being signed is sent to DigiCert.  They sign it and return it with a timestamp.  And all of that is bound into the final signature.  So you have an absolutely guaranteed timestamp on the signing.



But the point is that then, even after that certificate expires, the package continues to be valid, which is what you want for validly signed things.  You're saying this was legitimate at the time it was signed.  And if for some reason you had a need to continue updating the signature, you can certainly just re-sign things with a new Authenticode certificate.  But that's not the way things operate in this static-signing model as opposed to the dynamic communications model that we have between web browsers and servers.



So anyway, we have the same problem, it turns out, with revocation of Authenticode and code-signing certificates that we have with web certificates, and that is that certificates are being revoked, and it is the case that, when notified, the certificate authority will check.  What these guys that did the research did was that they had to reprocess the malware that VirusTotal already had in order to cause VirusTotal to recheck the certificate which was valid at the time it was initially ingested by VirusTotal, but which may have been revoked subsequently.  VirusTotal wasn't automatically going around and doing that.



But by forcedly causing VirusTotal to rescan the malware that it had, they found, I think the number was 21 percent of the instances had certificates that had subsequently been revoked.  And of course that's ahead of them expiring, and of course you're able to revoke a certificate even if it's been properly signed because, if any certificate has been used to sign malware, you have to assume that that certificate will be used to sign other pieces of malware, and you want to wipe them all out and cause them to no longer be trusted.



Anyway, so where we are today is we're in a world where, unfortunately, it is very possible for malware one way or another to arrange to get a certificate which is trusted at the time it is signed and will then be trusted by systems that are trying to decide whether or not they want to trust software until they earn a reputation, which is what I've gone through now.  Even if I freshly sign something with the certificate which is now more than a month old, it's acquired a reputation.  I think there's, like, two off-brand AVs out of more than 70 over on VirusTotal, only two that are saying we're not sure about this.  Like DNS8 or something is one of them, and there's some other.



So anyway, it's no longer causing us a problem.  Microsoft recognizes it.  And I just think this is the world we're in now.  I understand now why any new certificate needs to be regarded with some skepticism.  The good news is, thanks to certificates, it is possible to earn a reputation.  And once you have earned a reputation, which you're able to hold onto for up to three years, then you're able to no longer have a problem with AV, which is, you know, it's not that the code is doing anything malicious.  It's just that AV is really suspicious.  It has to be.



LEO:  Can we talk about the end of the Internet?



STEVE:  I think we've got to do that.  So I chose to discuss this RDP problem further since it's been quite a while since we've had one of these truly perfect, which is to say actual and serious, Internet-wide threats to observe and discuss in real-time for the podcast.  And by all appearances, this one is not going to disappoint.



Okay.  So it turns out - I will get to this - the scanning by the bad guys has begun.  But let me sort of take us through the last week first.  As we know, we have a wormable Windows Remote Desktop Protocol vulnerability which is bad enough that, two weeks ago, Microsoft reached all the way back to Windows XP to offer a patch for those systems.  We now also have a name for it that we didn't last week. It's known as BlueKeep, CVE-2019-0708.



So last Thursday, on May 23rd, Dan Goodin for Ars Technica wrote:  "It's been nine days since Microsoft patched the high-severity vulnerability known as BlueKeep, and yet the dire advisories about its potential to sow worldwide disruptions keep coming.  Until recently, there was little independent corroboration that exploits could spread virally from computer to computer in a way not seen since the WannaCry and NotPetya worms shut down computers worldwide in 2017.  Some researchers felt Microsoft has been unusually tight-lipped with partners about this vulnerability, possibly out of concern that any details, despite everyone's best efforts, might hasten the spread of working exploit code.



"Until recently, researchers had to take Microsoft's word that the vulnerability was severe.  Then five researchers from" - that's five researchers at one firm, but we'll talk about others - "five researchers from security firm McAfee reported last Tuesday that they were able to exploit the vulnerability and gain remote code execution without any end-user interaction.  The post affirmed that CVE-2019-0708, as the vulnerability is indexed" - and of course we know it as BlueKeep now - "is every bit as critical as Microsoft said it was."



McAfee's team wrote:  "There is a gray area to responsible disclosure.  With our investigation we can confirm that the exploit is working, and that it is possible to remotely execute code on a vulnerable system without authentication."



The next day, last Wednesday, we saw two more posts about BlueKeep.  One, from security firm ESET, was succinctly headlined:  "Patch now!  Why the BlueKeep vulnerability is a big deal."  In it, ESET's Security Evangelist wrote:  "Right now, it is only a matter of time until someone publishes a working exploit, or a malware author starts selling one on the underground markets.  Should that happen, it will probably become very popular among less skilled cybercriminals and also a lucrative asset for its originator."



Security vulnerability researchers at Check Point via Twitter.  @Eyal Itkin tweeted:  "The last three days were intense, but with help from the @_CPResearch_ team, we now have a" - so this is Check Point.  We have ESET; we have McAfee; we have Check Point now.  "We now have a working BSOD" - meaning Blue Screen of Death - "proof of concept for CVE-2019-0708.  Time to catch some sleep."



Kaspersky security researchers interested in reverse engineering.  This is Boris Larin at Kaspersky tweeted:  "We analyzed the vulnerability 0708 and can confirm that it is exploitable.  We have therefore developed detection strategies for attempts to exploit it and would now like to share those with trusted industry parties.  Please contact  nomoreworm@kaspersky.com."



Also Chaouki Bekrar, the founder of Zerodium.  He tweeted:  "We've confirmed exploitability of Windows Pre-Auth RDP bug 0708 patched yesterday by Microsoft.  Exploit works remotely, without authentication, and provides system privileges on Windows Server 2008, Window 7, Windows 2003, and XP."  He added:  "Enabling NLA mitigates the bug."  Then he said:  "Patch now or GFY."  And we'll let our listeners figure out what GFY stands for.



Also Valthek, who is a well-known malware analyst with more than 20 years of experience, he tweeted:  "I get the CVE-2019-0708 exploit working with my own programmed PoC," he says in parens, "(a very real dangerous proof of concept)."  He says:  "This exploit is very dangerous.  For this reason, I don't will said to anybody or any enterprise nothing about it.  You are free of believe me or not.  I don't care."



Anyway, he actually had this confirmed.  The senior principal engineer and lead scientist for McAfee said:  "After many hours, @ValthekOn was able to get a working proof of concept for this.  We're not going to reveal technical details or release code.  We urge everyone to patch.  It is really nasty."  And then a bunch of other people were - that was a tweet from Christiaan Beek at McAfee who included a bunch of other researchers in his tweeting so that they would see this also.



So I'm trying to see if there's anything else here.  McAfee did  one of the two most complete breakdowns.  And so there's additional information.  And I'm going to sort of - I want to, without getting into the weeds because there's no point here, I'm going to sort of explain what's going on to give everybody a sense for it.  So I'm going to share the description of the problem.  It uses a bunch of terms and presumptions that we have not defined, and there's really no point.  But you'll get a sense for it.



So McAfee wrote:  "The Remote Desktop Protocol (RDP) enables connections between a client and an endpoint" - that is, you know, the server - "defining the data communicated between them in channels."  That is, that's the way the protocol works.  It has virtual channels.  "The virtual channels are bidirectional data pipes which enable the extension of RDP.  Windows Server 2000" - that is way back then, Windows Server 2000 - "defined 32 Static Virtual Channels (SVCs) with the release of RDP 5.1, but due to limitations on the number of channels" - which is to say they probably, what, so 32 means six bits, so they probably only allocated, they only allowed six, no, I'm sorry, five bits for 32.  They allowed five bits in the protocol somewhere, so that wasn't enough.



"So due to limitations of the number of channels, they further defined what was known as Dynamic Virtual Channels (DVCs), which are contained within a single dedicated SVC, a static virtual channel.  SVCs are created at the start of a session and remain until session termination, unlike DVCs, which are created and torn down on demand.  It's this 32 SVC binding which CVE-2019-0708 patch fixes with the - and then there's two functions, IcaBindVirtualChannels and IcaRebindVirtualChannels functions in the RDP driver termdd.sys.  As can been seen in Figure 1" - and this is from McAfee's disclosure - "the RDP Connection Sequence connections are initiated and channels set up prior to Security Commencement, which enables CVE-2019-0708 to be wormable since it can self-propagate over the network once it discovers open port 3389."  In other words...



LEO:  Now, just to be clear, 3389 has to be open, and it has to be on a machine that supports RDP.



STEVE:  Correct.



LEO:  So Windows 7 Home or XP Home doesn't have RDP support.



STEVE:  Correct.  Oh, correct.  And this is - it's worth mentioning.  This is not a concern for the typical Home end user.



LEO:  Because you don't turn it on; right? 



STEVE:  Well, not only is it not on, but you're also behind a NAT router.  And so you're not going to have that exposed.  And anybody running Windows 10 is also okay because this only affects 7 and XP.  And not even Windows 8, for anyone who still has that.  So, yes.  So the concern here is the publicly exposed instances on the Internet, and that's where we're getting to.



LEO:  Or inter-LAN communication.



STEVE:  That's true, too, yes.



LEO:  And that was the big problem with WannaCry because of EternalBlue, which allows you to use SMB-1 to worm across the LAN.  So if you have a bunch of RDP ports open on your local LAN, that's as bad; right?



STEVE:  That is true.  Yes, yes, yes.  So McAfee explains, without us defining these terms:  "The vulnerability is due to the MS_T120 SVC name" - that's the Static Virtual Channel, the MS_T120 SVC, the Static Virtual Channel name - "being bound as a reference channel to the number 31 during the GCC Conference Initialization sequence of the RDP protocol."  Again, that's all gobbledy-gook, but just sort of - you sort of hear that.  "This MS_T120 Static Virtual Channel name being bound as a reference channel to the number 31 during the GCC Conference Initialization sequence of the RDP protocol.  This channel name is used internally by Microsoft, and there are no apparent legitimate use cases for a client to request connection over an SVC [Static Virtual Channel] named MS_T120.



"However, during GCC Conference Initialization, the client supplies the channel name which is" - remember, that's the client meaning something outside, or soon malware - "supplies the channel name which is not whitelisted by the server" - which is to say, which does not need to be whitelisted is what McAfee really meant to say, which does not need to be whitelisted - "meaning an attacker can set up another Static Virtual Channel named MS_T120 on a channel other than 31.  It's the use of MS_T120 on a channel other than 31 that leads to a heap memory corruption and remote code execution."



So essentially this was - there was some feature that Microsoft has always had in the protocol which normal clients don't use, but which is in the code.  And if it's used on channel 31, the Static Virtual Channel 31, no problem.  That's the only way Microsoft ever would use it for whatever purpose they might have.  But it turned out someone discovered, well, and I don't remember now the genesis of this, whether - I don't think we ever heard who found it.  You know, nobody else came out with a public disclosure.



So this may have been privately reported.  Maybe Microsoft found it themselves.  But one way or the other, if somebody hooks up to an RDP protocol server, from XP through Windows 7, and asks for this MS_T120 to be bound to a Static Virtual Channel other than 31, the server crashes in a way that basically everybody who has looked at it, we went through all of those tweets and all the companies, that is to say, this is not hard to do, which is the reason everyone's running around with their hair on fire is that everyone who's looked at it has been able to make this happen.  And now here we have public disclosure of exactly what's going on, how to do this yourself.



So McAfee says:  "The MS_T120 reference channel is created in the rdpwsx.dll and the heap pool allocated in rdpwp.sys.  The heap corruption happens in termdd.sys when the MS_T120 reference channel is processed within the context of a channel index other than 31.  The Microsoft patch adds a check for a client connection request using channel name MS_T120 and ensures it binds to channel 31 only [in those two functions] IcaBindVirtualChannels and IcaRebindVirtualChannels, within termdd.sys."



Okay.  So translating this, as I said, there was some flexibility.  What happened was Microsoft fixed this by enforcing the use of MS_T120 only for channel 31.  It turns out that there's another reverse engineering elsewhere on the Internet that is also public, where this is all shown.  So this was a very simple thing to fix.  And in fact there is a 30-byte patch involving 22 instructions from the 0patch guys, you know, those guys who make the micropatches?  Such that you don't even need to reboot the server to fix this.  You can do a non-reboot, in-memory patch to close this problem.



And one of the things I want to address here in a second is the problem that we have with our laws at the moment, and the fact that Microsoft can't do this themselves because we probably really are facing an event on the Internet, and it could be avoided technically, but it cannot be avoided legally.



Anyway, McAfee says:  "After we investigated the patch being applied for both Windows 2003 and XP and understood how the RDP protocol was parsed before and after the patch, we decided to test and create a proof of concept that would use the vulnerability to remotely execute code on a victim's machine to launch the calculator application, a standard litmus test for remote code execution."  And then they reiterated, saying:  "There is a gray area to responsible disclosure."  Meaning that that's - they don't want to go any further.



They said:  "With our investigation we can confirm that the exploit is working and that it is possible to remotely execute code on a vulnerable system without authentication."  And then they confirm that:  "Network Level Authentication should be effective to stop this exploit if enabled; however, if an attacker has credentials, they will bypass this step."



They said:  "As a patch is available, we decided not to provide earlier in-depth detail about the exploit or publicly release a proof of concept."  Not that it matters because, again, this is just not that hard to do.  "That would, in our opinion, not be responsible and may further the interests of malicious adversaries."  And they said:  "It is important to note as well that the RDP default port can be changed in the registry and after a reboot will be tied to the newly specified port.  From a detection standpoint, this is highly relevant."



Okay.  So I guess they're saying there may also be not yet discovered RDP servers that are running on non-default ports that nobody has found yet.  So of course I want to make sure people don't think that using a non-default port is meant as a security solution.  It certainly isn't.  We know how to fix this problem, and that is to use a VPN only with strong security to allow access to the Remote Desktop Protocol.  As I said last week when we first talked about this, that's the way to do this is run OpenVPN on a machine and use its very strong authentication with certificates in order to enable access to the server.



So as of today we have Zerodium, McAfee, Kaspersky, Check Point, MalwareTech, and Valthek all having confirmed that they've successfully developed exploits for BlueKeep.  None of them is publishing, but there have been cross-verifications of each other's work.  So this strongly suggests that the vulnerability is decidedly not difficult to weaponize. 



A security researcher, Sean Dillon at RiskSense, has created a tool to allow companies to test whether their own PC fleets have been correctly patched against the BlueKeep flaw.  So Leo, just as you were talking about within an Intranet, there are now, and even since I wrote this, I have found some others.  RiskSense is packaged as a docker.  I've got the link to the GitHub docker in the show notes.  It has been turned into a Metasploit module, so Rapid7 now has it in Metasploit.



And Robert Graham, who tweets as ErrataRob, and that's where his blog is, he trimmed down the RiskSense work, produced a nicer, leaner, and higher speed "C" scanner.  I've also got - he's posted his on his GitHub page as rdpscan.  So I first have his initial take, and then I have an update from this morning.  So Rob first wrote a couple days ago of rdpscan for this vulnerability, BlueKeep vuln.  He said:  "This is a quick-and-dirty scanner for the 0708 vulnerability in Microsoft Remote Desktop.  Right now" - okay, now, this is no longer current.  I will update this in a second.



He said:  "Right now there are about 700,000 machines on the public Internet vulnerable to this vulnerability, compared to about two million machines that have Remote Desktop exposed, but are patched or safe from exploitation.  Many expect that in the next few months a devastating Internet worm will appear similar to WannaCry and NotPetya.  Therefore, scan your networks and patch your systems.  This tool makes it easy to scan your networks to find vulnerabilities.  To use this tool, you can download a binary to run from the command line, or you can download the source and compile it.  For Windows, there's a precompiled binary available."



He writes:  "The tool is based entirely on the desktop patch from," and then he cites the code from RiskSense.  He says:  "I've simply trimmed the code so that I can easily compile on macOS and Windows, as well as added the ability to scan multiple targets."  He says:  "This is only a couple days old and experimental.  However, I am testing it by scanning the entire Internet with the help of masscan" - which is his parallel scanner - "so I'm working through a lot of problems pretty quickly.  You can try contacting me on Twitter (@erratarob) for help and comments."



So that was yesterday, so dated 5/27.  He says:  "You Linux peeps get only source as usual.  It seems to be working well on all three platforms."  This morning he updates.  Oh, well, first GreyNoise Intelligence over the weekend said BlueKeep scans started.  They tweeted, @GreyNoiseIO:  "GreyNoise is observing sweeping tests for systems vulnerable to the RDP BlueKeep."  Now, of course some of these could be, for example, ErrataRob doing a scan.  So certainly we know that we're going to have some white hat researchers scanning in order to help us appreciate the size of this threat.  It's clear also, well, actually in this case we know that it's not Rob.  I'll explain why.  They said:  "...vulnerable to the RDP BlueKeep vulnerability from several dozen hosts around the Internet.  This activity has been observed from exclusively Tor exit nodes and is likely being executed by a single actor."



ZDNet reports:  "Intense scanning activity detected for BlueKeep RDP flaw.  A threat actor hiding behind Tor nodes is scanning for Windows systems vulnerable to BlueKeep flaw."  And they wrote:  "While the infosec community was holding its collective breath, thinking attacks may never start, things changed over the weekend.  On Saturday, threat intelligence firm GreyNoise started detecting scans for Windows systems vulnerable to BlueKeep.  Speaking to ZDNet, GreyNoise founder Andrew Morris said they believe the attacker was using the Metasploit module developed by RiskSense to scan the Internet for BlueKeep vulnerable hosts.  So far we've observed only scans, no exploitation attempts.  But it appears that at least one threat actor is investing time and effort compiling a list of vulnerable devices, presumably in preparation for the actual attacks."



And of course, as I've noted, at least six research groups have announced that they've come up with private BlueKeep exploits, and there have been two detailed write-ups, one from McAfee that I shared, and there's also one from WazeHell, W-A-Z-E-H-E-L-L dot I-O.  So I think it's only a matter of time before we see more.  Now, this morning, Robert Graham posted to his own Errata Security blog the title:  "Almost One Million Vulnerable to BlueKeep Vuln."



He starts his posting saying:  "Microsoft announced a vulnerability in its Remote Desktop product that can lead to robust, wormable exploits.  I scanned the Internet to assess the danger.  I find," he writes, "nearly one million devices" - and actually it was 923,000 is his count, although some successive rescans found some additional ones, so it looks like it's approximately 950,000, thus nearly - "one million devices on the public Internet that are actually vulnerable to the bug."  And I'll break this down in a second.



He says:  "That means, when the worm hits, it'll likely compromise those million devices.  This will likely lead to an event as damaging as WannaCry and NotPetya from 2017, potentially worse, as hackers have since honed their skills exploiting these things for ransomware and other nastiness."



He said:  "To scan the Internet, I started with masscan, my Internet-scale port scanner, looking for port 3389, the one used by Remote Desktop."  And of course we know that's by default.  There may be other RDP servers on different ports.  He says:  "This takes a couple hours and lists all the devices running Remote Desktop, in theory.  This scan" - so just the scan for TCP connection acceptance on 3389 - "returned 7,629,102 results.  However," he writes, "there is a lot of junk out there that'll respond on this port.  Only about half are actually Remote Desktop.  Masscan only finds the open ports, but is not complex enough to check for the vulnerability.



"Remote Desktop," he writes, "is a complicated protocol.  A project was posted that could connect to an address and test it to see if it was patched or vulnerable.  I took that project and optimized it a bit, called," he says, "rdpscan, then used it to scan the results from masscan.  It's a thousand times slower, but it's only scanning the results from masscan instead of the entire Internet.  The table of results is as follows."  And I have them in the show notes for anyone who's interested.



So we have 1.447 million that timed out, meaning nothing happened.  1.414 million he says are safe.  The target appears to have been patched.  1.294 million unknown.  The connection was reset.  So again, probably just a closed port.  1.235 million, those are safe.  It looks like credentials are required.  Remember, these are all IPs that did respond to 3389.  So then we hit 922,671 are vulnerable, successfully verified that RDP is there and not patched.  651,000 received a FIN, meaning a closed port.  438,000 connection timeout.  And I won't bother enumerating.  They fall off from there.



He says:  "The various unknown things fail for various reasons.  A lot of them are because the protocol isn't actually Remote Desktop and respond weirdly when we try to talk Remote Desktop to them.  A lot of others are Windows machines, sometimes vulnerable, sometimes not, but for some reason return errors sometimes.  The important results are those marked vulnerable.  There are 923,671 vulnerable machines in this result.  That means we've confirmed the vulnerability really does exist, though it's possible a small number of these are honeypots deliberately pretending to be vulnerable in order to monitor hacker activity on the Internet.



"The next result are those marked safe due to probably being patched.  Actually, it doesn't necessarily mean they are patched Windows boxes.  They could instead be non-Windows systems that appear the same as patched Windows boxes.  But either way, they're safe from this vulnerability.  There are 1.414 million of them.  The next result" - anyway, he talks about the safe ones and goes down.  Then he talks about:  "But since a lot of those unknowns could be due to transient network errors, then in theory I should be able to rescan them and get some more positive results.  I did this," he says, and here's what he found.



So of those that were unknown, 28,000 were safe, saying that the target appeared patched.  He found another 20,000 new vulnerable ones, and a third scan found an additional 6,000 vulnerable ones.  So he says:  "The upshot is that these tests confirm that roughly 950,000 machines are on the public Internet, right now today, that are vulnerable to this bug.  Hackers," he writes, "are likely to figure out a robust exploit in the next month or two and cause havoc" - a month or two?  A week or two - "and cause havoc with these machines."  He says:  "There are two things you should do to guard yourself."



Okay, and I'm not even going to bother with that.  Of course the problem is these are non-maintained machines.  I mean, they would not be vulnerable today if they were being maintained.  If anyone knew that they had a machine out on the public Internet that was listening to any security alerts, any podcasts, any people tweeting security dire warnings, they would have fixed this.



So here's the problem.  We have a real problem that needs to be fixed right now.  Code Red was not the last worm.  Nimda was not the last worm.  MSBlast was not the last worm.  WannaCry was not the last worm.  NotPetya was not the last worm.  And whatever this worm, I mean, this will be, what, maybe the Blue worm.  Who knows what we're going to call it.  It won't be the last one, either.



What's upsetting is that this can be patched in RAM.  If it were legal for Microsoft to do this, they could, before ever having gone public - I'm sure they know how many machines are there.  They were probably looking at them themselves before they decided how to handle this.  They could reach in there and close this problem and then back out so that, even without rebooting these systems - because this patch that's been developed does not require a reboot.  Even without rebooting them, they could close this hole which is there and keep what is absolutely, definitely certain to happen.  I mean, there's just no way this is not going to happen with all the attention this is getting.  They could keep it from happening, but they cannot do that legally.



And so it seems to me that we need some sort of a worldwide consensus that allows white hats with proper oversight, for example, a body composed of individuals from the security industry and appropriate government leaders, to whom Microsoft could present in private, in secret, under obvious nondisclosure because these would all be white hats, could explain the problem and get an agreement that they're going to fix this problem before it wreaks havoc on the Internet.  But today it's not legal for Microsoft to do this.



So it seems to me, since this is not the last time this is going to happen, and none of the last five worms that have been really problematical have been the last worm, we have to assume this is not the last worm.  We need to be proactive and somehow get some global treaties with the other nations that we care about, or maybe just do it unilaterally with the U.S.; say look, folks, if software from a supplier that we care about in the U.S. or a friend of the U.S., an ally, if the problem can be proactively fixed in a way that can be shown not to hurt somebody who is unaware of the problem, then it needs to be made legal to reach out and do that because otherwise we're going to have happen what is almost certain that we'll be talking about on next week's podcast, or the week after.



LEO:  Well, we know we're going to be talking about malware, ransomware spread by this.  I mean, you know.  And I don't think it's an accident, that its name is an accident since EternalBlue is the NSA exploit that helps people worm their ransomware from machine to machine.  So what is it, Solid Blue?  What do they call it?



STEVE:  It's BlueKeep.



LEO:  BlueKeep.  This makes a lot of sense.  It's keeping Blue active.  And ransomware is just getting worse and worse.  So it's not the end of the Internet, though.  Let's be fair.



STEVE:  I know.  I was just kidding.



LEO:  But it isn't good for a lot of businesses.  Man, this ransomware seems out of control.  Steve, this show will appear in a variety of places.



STEVE:  Right, hopefully.



LEO:  If the Internet doesn't disappear between now and tonight, Steve will post it on his website, GRC.com.  He has audio and transcripts there.  He also has SpinRite, the world's finest hard drive recovery and maintenance utility, dare I say really the only hard drive recovery and maintenance utility, the only one worth a darn.



STEVE:  Yeah.



LEO:  And you can get that there, his bread and butter, but lots of other stuff including the latest on SQRL, Vitamin D, ShieldsUP!, Don't Shoot the Messenger, DCOMbobulator, I could go on and on.  It's basically a rathole of fun.  Jump right in.



STEVE:  That's right.



LEO:  Go all the way down.  If you want to go to our site, which is much less interesting, we do have at least this show at TWiT.tv/sn.  You can watch us live.  We do it live Tuesdays, 1:30 Pacific, 4:30 Eastern, 20:30 UTC at live.twit.tv.  Audio and video streams are there.  Or you can ask your Echo:  "Echo, listen to TWiT Live."  I think if you say "TWiT Live" it should be enough, but sometimes you have to say "on TuneIn."  You can also listen to the most recent version of Security Now! by just saying "Echo, listen to Security Now! podcast," and it'll play it.  So it's a nice way to keep up.  Or subscribe in your favorite podcast application.  That way you'll have it wherever you are, whenever you want it.



STEVE:  You know, I forgot to mention, I gave the SQRL presentation last Wednesday...



LEO:  How did that go?



STEVE:  ...to the Los Angeles Chapter of OWASP.  These rehearsals are being useful.  Lorrie, who has sat through both, said it was unbelievably better than the first one, which [crosstalk].



LEO:  I like her.  Good.



STEVE:  She said it was unbelievably better.  And one of the things that I did was, and I said I was going to at the beginning, I untethered myself from my PowerPoint presentation.



LEO:  Yes, good.



STEVE:  Because I'm just - I'm never in sync with it.  I'm running off on different directions and things.  So what I've done is, what I realize what I want is, and I'll have this for the next one I do, whenever that's going to be - we're trying to negotiate the right time for the Orange County OWASP Chapter.  That'll happen.  Also the guys in Dublin confirmed that they still want me, and they're going to fly Lorrie and me out there to present to them.



LEO:  Nice.



STEVE:  So what I'm going to do is, the only thing I really need, because I know this thing so upside down, backwards and forwards, that I don't need prompting by bullet points.  What I need is diagrams because pictures of different little algorithms and things, those are the things that it's fun to be able to refer to.  And it turns out that with Office 2016 there was a new feature added where you can do a slide of thumbnails, and then I'll be able to tap one, and that'll zoom in.  So I can randomly access the images of the various things, sort of like as I'm going, in any sequence that happens.  So anyway, I'm  rehearsing all of this so that when we do the presentation in your TWiT studio, Leo, it'll just be slick.



LEO:  Nice.  I can't wait.  And yeah, I completely agree.  I abandoned PowerPoint slides a long time ago because, well, for one thing, people just read those instead of listening to you.



STEVE:  Right.



LEO:  The way we are, if there's a slide on the screen, we're going to look at that and not listen to what you're saying.  So I think you're right.  Diagrams yes, presentation no.  I think you're exactly right.



STEVE:  Yup.



LEO:  I can't wait to see it.  Did anybody record it?



STEVE:  No, it was not recorded.  I've had a bunch of people asking if it had been.  Really, I just know what a quality production your guys will produce, and I'm sort of - I guess I'm sort of biased toward that.



LEO:  Thank you.  Good.  We'll give you a nice video the minute you want it.



STEVE:  We'll take it one step at a time, yeah.



LEO:  Good, good.  All right, my friend.



STEVE:  Okay, well, assuming that we're still here next week and that Skype is still working.  We'll see.



LEO:  I downloaded and ran that scanner.  It's a command line utility.  I didn't find any open RDP ports on our network.



STEVE:  Nice.



LEO:  Yeah, nice.



STEVE:  Whew.



LEO:  I'm not surprised.



STEVE:  Okay, buddy.



LEO:  See you next time.



STEVE:  Talk to you next week.  Bye.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#717

DATE:		June 4, 2019

TITLE:		The Nansh0u Campaign

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-717.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we check in on the BlueKeep RDP vulnerability.  We look at the planned shutdown of one of the, if not THE, most successful, if one can call it that, affiliate-based ransomware systems.  We update you on the anti-robocalling problem and then look at the recent announcements by the Russian and Chinese militaries about their plans to move away from the Microsoft Windows OS.  We also look at Apple's announcement yesterday of their forthcoming "Sign in with Apple" service, touch on the state of SQRL, and then share a bit of fun feedback from a listener.  We finish by examining the interesting details behind a significant old-school persistent campaign, the Nansh0u campaign, apparently sourced from China, which has successfully compromised many tens of thousands of servers exposed to the Internet.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We're going to talk about a nation-state exploit that's now being used by script kiddies all over the world.  More about the RDP exploit.  It's not going to end the Internet, but it might be close.  And we'll also talk about what Apple's doing to simplify and privatize single sign-on.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 717, recorded Tuesday, June 4th, 2019:  The Nansh0u Campaign.



It's time for Security Now!, the show where we talk about security, privacy, how computers work and all that jazz with this guy right here, "James Tiberius" Gibson.  No, wait a minute.



STEVE GIBSON:  I'm saying, wait, James?  What?



LEO:  I'm confusing you with Captain Kirk.  It's Steve Gibson of the Good Ship GRC.com.  Hi, Steve.



STEVE:  Ah, the Lollipop, yes.



LEO:  Yes.  Hey, somebody sent me - and, shoot, I threw out his name, but he sent me a bunch of PopSockets with a familiar logo on it.  You probably know who this is.



STEVE:  I know him.  And in fact I wore one of his shirts.  I keep meaning to tell our listeners that there are SQRL T-shirts and hoodies on Amazon now.



LEO:  Merch, baby.



STEVE:  Merch, yes.



LEO:  And apparently PopSockets, as well.



STEVE:  Now, you know, when he was talking about a PopSocket, that's not a term I'd ever heard of, very much like when...



LEO:  Yeah, because you're not hip with the kids.



STEVE:  Well, it's when you first mentioned "podcast," and I said, "A what?  A what cast?"



LEO:  So I'll show you.  You adhere this to the back of your phone.



STEVE:  How is that a good idea?



LEO:  Well, it's a bad idea if you do as I do, do wireless charging.  But a lot of people...



STEVE:  Yeah, that would be a problem.



LEO:  Yeah.  But a lot of people - I could do it real quickly here.  A lot of people like this because it gives - you see it all the time with the kids because then - okay.  That's popped in.  And then when you pop it out - I shouldn't do it right after I adhere it, but I'll do it anyway.  Now it becomes a little phone holder.



STEVE:  Ah.



LEO:  And pop it out.  And see, like that?



STEVE:  And so, yeah, yeah.  I mean, no, I mean, I have to explain, I should say that, yes...



LEO:  You're an old guy.  That's the problem.



STEVE:  Well, no.  Since I've realized that there is going to be a SQRL logo on something, I should figure out what it was.



LEO:  What it is.



STEVE:  And so I did solve the mystery of the PopSockets.



LEO:  Oh, you knew.  Okay.



STEVE:  Yes.  In no way related to Pop Rocks.  That was something else that was a passing fad.



LEO:  So thank you.  I wish I knew your name.  I do know it was on the package.



STEVE:  It's Mike.



LEO:  Something, that was it, yeah.  Thank you, Mike.  And I now own like a dozen PopSockets with the SQRL logo on them.  Mike knows I have a lot of phones, I guess.  Anyway, that's pretty cool.  And all that merch is on Amazon.



STEVE:  It is very cool.  And but what it is - yes, it's on Amazon.  If you put in "SQRL T-shirt," I think that will take you to - and in fact I was wearing one at the L.A. OWASP meeting a couple weeks ago.



LEO:  Nice.  When you did your presentation.



STEVE:  Yeah, and in fact the shirts are very nice.



LEO:  Very nice.



STEVE:  Very sheer.  I like a sheer T-shirt because then you can breathe.



LEO:  There's an image, yes. 



STEVE:  One of these days we're actually going to get down to the podcast.  Although I realize, Leo, this is how many of your podcasts go.



LEO:  We are somewhat discursive here at TWiT.  Is that what you're saying?



STEVE:  We have Security Now! Episode 717 with a name I cannot pronounce.  What would that be, the Nansh0u Campaign?  



LEO:  Nansh0u, yeah.  What is it?



STEVE:  Nansh0u.  It's Chinese.  It was a string that was found in some malware.  And we're going to talk about that because what's interesting is it is very sophisticated, state-level malware that unfortunately has many signs of amateur origin, suggesting that - this is also something we could have predicted, that inevitably state-level quality tools are now beginning to find themselves in the hands of amateurs.  And so that suggests that there's going to be more rather than less trouble going forward.  So, yes.  Three digits is probably not enough for the numbering of these podcasts.



We're going to check back in on the BlueKeep RDP vulnerability.  We look at the planned shutdown of one of the, if not THE most, well, I guess you'd call it "successful," if you can call it that, affiliate-based ransomware systems announced that this is the final month for which the affiliates will be able to earn revenue from their victims.  And we update on the anti-robocalling problem, take a look at the recent announcements made by the Russian and Chinese militaries about their plans, believe it or not, to move away from Microsoft Windows.  It's like, wait.  You guys are still using Windows?  What?



We also look at Apple's announcement yesterday of their forthcoming "Sign in with Apple" service, and touch on the state of SQRL.  Then we'll share a bit of fun feedback, and then dig into this Nansh0u campaign, which many indications indicate it was sourced from China.  It has successfully compromised more than 50,000 MS-SQL Servers which are exposed to the Internet.  And, I mean, that should be an oxymoron or an oxy - what would you call an oxy sentence?  Anyway, I guess there is no such thing.  But, I mean, you should not have the phrase "MS-SQL Servers exposed to the Internet."  I mean, I guess there are some use cases.  But, boy, it always seems like a mistake.



And anyway, so I think another fun podcast and interesting podcast for our listeners.  And we're also going to touch on, follow-up on last week's dire predictions of the end of the Internet.  I hope our listeners know that we were having fun with that, and that I didn't really think it was going to happen.  But I'm going to elaborate on why I don't think that this will be used as a worm.  I think the days of the worm have passed, well, at least for this class of vulnerability.  The point is you don't need a worm to exploit this.  And so there's a place for worms.  This isn't the place.



LEO:  Actually, I just saw that they've analyzed the malware that shut down the city of Baltimore for three weeks, and EternalBlue was not used to spread it throughout the network.  So I guess what you're saying is you don't need special illicit worm code to spread ransomware all over.



STEVE:  Right.



LEO:  And now back to Steve, coming to you at the speed of light.



STEVE:  Saw on the Hacker News, I just liked the image that they had created.  They had the Windows logo with the word "BlueKeep," and then underneath it, "Powering 1 Million Computers."



LEO:  That's the estimate of the number of computers that are vulnerable to BlueKeep?  A million?



STEVE:  A million, yes.



LEO:  Wow, wow.



STEVE:  Remember that last week, and this has - actually, we're going to talk about this right now because Microsoft posted last Thursday, posted a blog posting sort of with an update reminder about this problem.  Now, I don't think I've been explicit in saying that, unfortunately, reminding people isn't going to be effective because anybody who is in the loop, anybody who is conscious understands about this problem.  And if they're aware that they have a problem, they will have fixed it by now.  So I don't get what everyone running around warning people about this does.  I mean, maybe marginally it's useful.



But all indications are that we have - it was Robert Graham at Errata Security who did a full scan of the Internet.  Then from all of the 3389, which is the default RDP port, he then analyzed the protocol, figured out which of those were true Remote Desktop Protocol services that were exposed, and then probed them in a presumably benign way to determine which of those were vulnerable to this problem.  And from that he obtained the IP addresses of 950,000 presently vulnerable Windows machines that actually have RDP exposed, that have not been updated, that require no authentication in order to get on and get in.



So again, I mean, that was already two weeks after the dire warnings and the updates were made available, and nobody seems to be taking action, again, because I don't think anybody is listening.  Whoever is behind those million machines, they're asleep at the switch.



So last Thursday Microsoft titled their blog post "A Reminder to Update Your Systems to Prevent a Worm."  And in a minute I'm going to explain why I don't think that's the problem that we're going to have.  But they said:  "On May 14, Microsoft released fixes for a critical Remote Code Execution vulnerability, CVE-2019-0708, in Remote Desktop Services - formerly known as Terminal Services - that affects some older versions of Windows.  In our previous blog post on this topic we warned that the vulnerability is 'wormable,' and that future malware that exploits this vulnerability could propagate from vulnerable computer to vulnerable computer in a similar way as the WannaCry malware spread across the globe in 2017."  And again, I'll explain why I don't think that's going to happen.



But they said:  "Microsoft is confident that an exploit exists [yeah, duh] for this vulnerability; and, if recent reports are accurate, nearly" - this is Microsoft - "nearly one million computers connected directly to the Internet are still vulnerable to CVE-2019-0708.  Many more within" - and this is the point you made last week, Leo.  "Many more within corporate networks may also be vulnerable.  It only takes one vulnerable computer connected to the Internet to provide a potential gateway into these corporate networks, where advanced malware could spread, infecting computers across the enterprise.  This scenario could be even worse for those who have not kept their internal systems updated with the latest fixes, as any future malware may also attempt further exploitation of vulnerabilities that have already been fixed.



"It's been only two weeks since the fix was released, and there has been no sign" - this is Microsoft still - "no sign of a worm yet.  This does not mean that we're out of the woods."  Actually, I would argue we will never be out of the woods.  They said:  "If we look at the events leading up to the start of the WannaCry attacks, they serve to inform the risks of not applying fixes for this vulnerability in a timely manner.  Our recommendation remains the same.  We strongly advise that all affected systems should be updated as soon as possible."  They said:  "It is possible that we won't see this vulnerability incorporated into malware.  But that's not the way to bet."



Then in their blog post they remind us about the timeline of the EternalBlue exploits.  They said:  "Almost two months passed between the release of fixes for the EternalBlue vulnerability and when ransomware attacks began.  Despite having nearly 60 days to patch their systems, many customers had not.  A significant number of these customers were infected by the ransomware."  So anyway, I thought that was the most interesting part of this was a reminder that this doesn't - even though the problem exists instantly, it does take a while for tests to be run, for code to be written, for something to be done.



And as we know, we had fun last week with this, you know, the end of the Internet and asking our listeners to quickly download the podcast because they may not be able to get it in a couple days.  But I think there's, in all seriousness, I think there's a very different way to think about this.  To me, this doesn't feel like a pending worm.



One uses a worm for its multiplying effect when it's necessary to brute force attack the possibly weak but still present credentials, or an authentication weakness which is believed to exist in many machines which may be unknown.  So a worm is therefore a good way to collectively pound on the Internet as a whole to break into unknown targets that are resisting that break-in.  And the reason it's a big deal is that, that way, you can get newly exploited machines to chime in on the attack of other vulnerable machines which haven't yet fallen.



But that's not the scenario we have here.  Here there's no need to pound on anything, and the vulnerable machines are not unknown and needing to be pounded upon to be found.  As we discussed last week, simple IPv4 scanning, which is now widely available, the code is in Metasploit.  The idea of scanning the whole IPv4 Internet space, that once upon a time was daunting when you had 4.3 billion IP addresses, now it's like, eh, yeah, I just, you know, I mean, Bob - Robert scanned the Internet over the weekend, and did it several times, and to collect his address of 950,000 currently known vulnerable IPs.  He has them.  He has a list.  And before long, if not already, he won't be the only person to have such a list.  So I don't really see that a worm makes any sense at all.  Yes, it is...



LEO:  But internally, like on a LAN.  You know, once you get it on one machine, affecting the other machines on the LAN.



STEVE:  But even there, you know your subnet mask, so you know the exact IP range of the LAN.  And so you can just scan, like instantly scan.  From one location you can scan the entire extent of the LAN, looking for port 3389, and then [crosstalk].



LEO:  So [crosstalk] get in on 3389, you don't really care.



STEVE:  Correct.  And what the worms were traditionally doing is they would spread because each new instance would then start randomly looking for other machines and then pounding on them to get in.  But there's no pounding necessary.  The door is wide open.  I mean, what will be interesting is to see whether a month or two from now the apparent number has dropped precipitously because that would tell us that residence has been taken up by something that has closed the door behind itself, which is the sane thing to do.  Were I not wearing my bright shiny white hat here, a scan of the whole IPv4 space would have found the problem.



It's no mystery how to exploit it any longer.  You install whatever it is you want to.  And what really seems to be what people want to do these days is make money.  And the way they make money is they participate in mining pools.  So, I mean, what I seriously expect, if someone said to me, "What is the number one most likely scenario that you expect to see for this nearly one million exposed machines," I would say get the IPs, work up an attack that installs a persistent cryptominer that participates in a mining pool, get it going, close the door behind yourself so that you're not competing with other people who are coming along later, and that's what you do.



You then, if you are the first in and the first to set up a miner and to close the vulnerability behind yourself, you now have potentially one million cryptomining machines actively participating in mining.  And you don't know how long they're going to last.  You're going to end up with many of those forever.  Some may, because they've become bogged down by the use of their CPU, maybe then the fact that the machine's no longer working right will come to the owner's attention.



Again, the fact that all these dire warnings are being spread, that has no effect whatsoever.  Whoever these belong to, these million instances, they're not listening.  I mean, they're not getting auto updates, or this would have been solved.  They're old machines that have just - they're, as we often say, in the closet somewhere.  Well, in that closet that thing's going to be generating more heat before long because its CPU is going to be pinned by a bad guy who doesn't care at all about throttling the CPU to prevent its discovery.



What they want is, they want as many cycles shared in this mining pool as possible to generate revenue for as long as it lasts.  That's what I think is going to happen.  And it'll have no effect whatsoever on the Internet.  In fact, it will have the effect of cleaning up a million potential vulnerabilities that could have been used for much more harm.  But instead, the first person in is just going to use it make money.  And so it would be really interesting if we see a few months from now a repeat 3389 scan looking for the same profile and, oh, look, they've all disappeared.  I wonder where they went.  Well, they got taken over, and someone closed the door behind them.  I think that's the most likely scenario.



Okay.  So I've mentioned this ransomware before.  This is an interesting profile.  And we don't know whether - it would be really nice to know how much money these guys made.  They're claiming, in the 18 months since they launched this Ransomware as a Service, that their ransomware service took in a total of $2 billion, from which they received a commission because basically they set up affiliates.  They set up franchises.



Okay.  So what's interesting, the reason we're talking about it now is that they've announced this is the final month of this.  So we noted in our Picture of the Week last week, we showed the timeline of the explosion in ransomware.  And we stopped tracking it, I mean, whatever that image was, it was in my catalog of interesting security-related things to share on the podcast from 2017.



LEO:  I just don't think you could fit 2018 or 2019.



STEVE:  Exactly.  Exactly.  So this strain of ransomware is called GandCrab, G-A-N-D capital C-R-A-B, GandCrab.  We've touched on it briefly.  Nothing really made it stand out until now, when this Ransomware as a Service has announced that they're going to shut down their online portal, which is where their affiliates or distributors or franchisees sign up and pay to get access to custom builds of this GandCrab malware, which then the affiliates distribute via whatever means they want - email spam, exploit kits, RDP maybe, or whatever means.  And then when an infected victim pays the ransom demand, because the GandCrab originators still hold the keys to this ransomware that they're making available to their affiliates, then they earn a commission.



And you can sort of imagine the pitch:  Yes, you, too, can form your own moneymaking, highly profitable ransomware franchise, encrypting the contents of unwitting strangers' hard drives and extorting many hundreds of dollars from each of them at no cost to yourself.  For just a relatively small piece of the action, we'll manage all of the messy little details for you.  You just go out there, find fresh new victims, and arrange to get them to run the ransom malware we'll provide to you at no additional charge.



We'll also keep it updated with all the latest cutting-edge AV-avoiding tricks and technologies.  We'll even go so far as to purchase bright shiny new code-signing certificates from Comodo, still - against all reason - one of the most trusted names in certificate authorities.  That way the malware you distribute won't raise any questions when your trusting victims press Go.  What could be easier?  Burdened by the weight of massive student loans?  You don't need Bernie Sanders.  Retire that college debt from the comfort of your own parents' basement and show them just what an enterprising little cuss you can be.



So, yes, you can imagine that they generated many affiliates who were...



LEO:  That's good marketing, baby.



STEVE:  Who were actively redistributing their malware and apparently generating lots of money.  So anyway, those were the days.  As far as we know, this dream is coming to an end at the end of this month.



LEO:  Aw.



STEVE:  Last Friday, on May 31st, as I had mentioned before, GandCrab's operators announced their intention to shut down their illicit operation.  A number of news outlets picked up on this.  I have a screenshot of the posting that they put up.  And they wrote, and this is - they're not English speakers natively, so it's a little awkward.  But they said:  "All the good things come to an end.  For the year of working with us, people have earned more than $2 billion.  We have become a nominal name in the field of the underground in the direction of crypto fiber."  I don't know what that means.  "Earnings with us per week averaged $2.5 million.  We personally earned more than $150 million per year."



LEO:  What?  No.



STEVE:  Well, you know, we don't know.  They said:  "We successfully cashed this money and legalized it in various spheres of white business, both in real life and on the Internet.  We were glad to work with you.  But as it is written above, all good things come to an end.  We are leaving for a well-deserved retirement.  We have proven that by doing evil deeds, retribution does not come."



LEO:  Yet.



STEVE:  "We proved that, in a year, you can earn money for a lifetime.  We have proved that it is possible to become number one, not in our own words, but in recognition of other people."  So, and then now they say "In this regard," and they mean with regard to shutting things down, "stop the set of adverts.  We ask the adverts to suspend the flows.  Within 20 days from this date we ask adverts to monetize their bots by any means," meaning, say, threaten their victims with final payment or else.  And then they said:  "Victims, if you buy, now."  And they said:  "Then your data no one will recover.  Keys will be deleted."  Then they signed off, saying:  "That's all.  The topic will be deleted in a month.  Thank you for all the work."



Anyway, BleepingComputer, one of our favorite outlets for news, and especially that tracks ransomware, said that they had reached out and asked these guys to please publish their keys after they shut down.  Others have, and that way there would be the ability for victims who would otherwise have all of their data lost to still be able to recover it.  But anyway, we don't know.  It's certain, you know, these particular guys, the GandCrab guys, were known for pulling stunts, and having a little bit more interaction with the security community than usual, and also of being jokesters.  So all these numbers could be made up.



But it is the case that independently this particular ransomware is one of, if not the most pernicious and prevalent ransomwares in the - I hesitate to call it "the industry" or "the marketplace."  But in the world.  So they may have been raking in some money over the course of this.  So, yeah.  Goodbye and good riddance, but wow.



And it's interesting, too.  They did something, given their numbers, that demonstrates they did something different than the Coinhive guy.  Remember that one of the things that always surprised me was that the guy behind Coinhive was taking a large piece of the action.  Even if these numbers are inflated, they suggest that these GandCrab guys were taking a much smaller piece of the action, which represents a kind of maturity in terms of, if you want to incentivize your affiliates, your entrepreneurs, then let them have most of the ransom and just take a relatively small piece for yourselves.



And if this is true, that strategy may have worked for them because, again, if they actually did generate over the course of 18 months, which this thing appeared on the scene in January of 2018.  So here we are at the beginning of June 2019, 18 months later.  And they said, yeah, we're going to retire now because we've - what do you call it?  Laundered.  We've laundered our illicit earnings by investing in legitimate businesses and online, and so we're going to go find a beach somewhere.  Crazy.



So there is incremental forward progress on the robocalling front.  We've talked previously about, I mean, I just love saying SHAKEN and STIR.  Those are the two telecommunications protocols we touched on before which are in the process of being gradually deployed by all the major telecommunications carriers.  The event which triggered our previous discussion was the first intercarrier test of this SHAKEN and STIR technology, which was at that time between AT&T and Comcast.  Apparently there had been previous intracarrier testing, but that doesn't do anybody any good because the whole point is what SHAKEN and STIR do is they apply certificate-based technology to caller ID signing to strongly prevent caller ID spoofing, which is so rampant today that, I mean, it's not worth bothering or even believing the caller ID you see.



I know that, for myself, when the phone rings, I'll look at it.  And it'll be my area code, and oftentimes it'll be my prefix, which is supposed to mean, oh, it's somebody in your neighborhood or a neighbor or something.  I mean, they're just making that up.  They know the number that they're dialing, so they're able to spoof the number they're dialing from to encourage you to answer.  And it's a little bit mindboggling that there's, until now, well, in fact even now, absolutely no control over that.  I know that Lorrie had taken to continually blocking all of the numbers that were coming in.  But of course they never repeated, so the blocking was serving no purpose whatsoever.



So anyway, the reason this comes up again today is the U.S. Senate just passed, on a vote of 97 to 1, and I can't imagine who the one was who would say no, we don't want this, a bill known as TRACED.  Which of course, you know, somewhere, Leo, there's people who are good at coming up with names for things that have fun acronyms.



LEO:  Yes.  They all work for the House.



STEVE:  Exactly.  So TRACED is Telephone Robocall Abuse Criminal Enforcement and Deterrence Act, T-R-A-C-E-D, which will now go to the House of Representatives, where it's expected to pass by just as large a margin as it did.  Then it will go to our dear President's desk, who will, I imagine, sign it into law.  When the bill becomes law, as it is expected to, it will then empower our FCC, the Federal Communications Commission, to impose significant fines up to - get this - $10,000 per call for illegal robocalls.  The legislation would also increase the statute of limitations for bringing such cases, thereby giving the FCC regulators more time to track down the offenders.  This law will also create an interagency taskforce to address the problem and pushes the major carriers in the U.S. - Verizon,  AT&T, T-Mobile and so forth - to continue moving forward on the deployment of interprovider SHAKEN and STIR protocol in order to perform end-to-end authentication, which is what we need.



So all of this is looking good.  This is the most recent bill.  There have been 13 that preceded it, and this one is the toughest, I guess because everyone is just really getting fed up with this.  And in fact this bill has the backing of all 50 state attorneys general, 35 of whom told the FCC last October that they were all pulling their hair out over the enormous problem which was beyond anything their own states' law enforcements could handle.



Earlier this year in February, Ajit Pai, our FCC Chairman, reiterated his call for a robust caller ID authentication system to be implemented this year, in 2019.  And so we need the technology in order to create the ability, to enable the ability to enforce the law that we have.  And so the good news is we are, even though this takes quite a long time to happen, we are finally beginning to move forward to this.



I skipped another little piece from the middle of last month because it didn't quite make the grade, but it fits in with this because on May 15th, a couple Wednesdays ago, the FCC announced a new measure that would grant mobile phone carriers the authority to block robocalls.  The new rule would make it easier for mobile carriers - AT&T, Verizon, T-Mobile and so forth - to automatically register their customers for call-blocking technology, rather than, as it is now, requiring consumers to opt in on their own.



And this is actually my favorite:  It would also allow customers the option to block calls coming from phone numbers not on their Contacts list.  Which in my opinion would be a tremendously useful filter.  If nothing else, maybe just automatically send them to voicemail or put them somewhere else.  But the idea of blocking something not on your Contacts list would mean that you could actually pay attention to your phone when it rings because it's somebody you know, rather than making you look at the number and go, ugh, you know, just another unknown caller.



So anyway, Ajit Pai said two weeks ago:  "Allowing call blocking by default could be a big benefit for consumers who are sick and tired of robocalls."  Yeah, no kidding.  So anyway, these things never happen quickly, but it looks like the problem has gotten so bad that we're finally getting some traction on it.



LEO:  Steve, you're not looking for work, are you?  Because if you are, it's a great place.



STEVE:  Oh, boy.  I have so much to do.



LEO:  Isn't it nice not to have to search for work anymore?  I have to say.



STEVE:  I'm not looking for anything more to do.



LEO:  When's the last time you actually went out to get a job?  Never.  You've been an entrepreneur from day one, practically; right?



STEVE:  Yeah.  There were a couple, like in between, when I was calling myself a "consultant," unquote.  



LEO:  Yeah, oh, yeah.



STEVE:  And I did some consulting work.  I wrote a Windows Manager for DOS that ran in text mode. 



LEO:  Nice.



STEVE:  There was another one.  I can't remember now which one it was.  There were some early ones that were multitasking on DOS.  And so I did some fun things.  I developed EKG equipment, long-term ambulatory EKG monitoring, back before we had digital.  It was a very slow-moving tape, and it recorded, it FM-modulated EKG on...



LEO:  Oh, my god.  Would you have to carry that under your arm as you walked around?



STEVE:  They were called Holter monitors because a Dr. Holter figured out that sometimes you don't see arrhythmias occurring...



LEO:  Unless you're in life.



STEVE:  Unless you're actually out.



LEO:  In situ, yeah.



STEVE:  So you'd do it for 24 hours.  So, yeah, I did a bunch of different kind of interesting things.  And then of course the PC came along, and I looked at the Color Graphics Adapter, the CGA, that was just, like, flickering incredibly whenever it scrolled.  And I thought, okay, there's got to be a way to fix this.  And so I created my first product for the PC, which was FlickerFree.



LEO:  Nice.



STEVE:  And of course before that was the Apple II, and I did the light pen.



LEO:  So you always really have been an entrepreneur, honestly.



STEVE:  I've, yeah, I certainly have...



LEO:  You're a self-starter, yeah.



STEVE:  ...the soul of an entrepreneur.  And it's funny, too, because in this story we're going to talk about, we'll get to China in a second, but they're actually thinking that they're going to roll their own OS from scratch.  And, you know, I don't think you can anymore.  But anyway, let me start at the beginning.  So Russia first.  For their military systems, Russian authorities just recently announced that they are continuing to move toward implementing their plan to replace Windows with a locally developed operating system, and they're doing it in what I think is the sane way, which is based on a Linux distribution.



LEO:  Yeah, of course.



STEVE:  Called Astra.



LEO:  Yeah.



STEVE:  I mean, it's complete auditable.  You have the source code.  You can have your Russians rummage through it and look for any problems.  And we also know that, I mean, we don't know - as I was thinking through this last night, putting this together, I thought, well, you know, we were worried about the elliptic curve dual whatever it was, DRBG random bit generator a few years ago because we didn't know the source of some of the magic numbers, and there was some concern that there may have been some NSA influence.



And in this post-Snowden era, of course, we now understand the strong interest that our intelligence services do have in influencing things.  So, similarly, we don't know that there are not Linux developers with the NSA.  In fact, we do know that there are some, that there's a government side to Linux for, like, SELinux for creating a more secure kernel and so forth.  So, I mean, that's a little bit dicey.  But again, the task of creating an OS truly from scratch, you know, unless you'd be happy with DOS, I just - I don't think that's on the table anymore.



Anyway, Wikipedia - I didn't know what Astra Linux was so I looked in Wikipedia.  They said:  "Astra Linux is a Russian Linux-based computer operating system developed to meet the needs of the Russian army, other armed forces, and intelligence agencies.  It provides data protection up to the level of 'top secret.'"  And as I'll discuss in a second, there's a level above that.  But "...up to the level of 'top secret' in Russian classified information grade.  It has been officially certified by the Russian Defense Ministry, the Federal Service for Technical and Export Control" - that's the FSTEC that we'll get back to in a second - "and the Federal Security Service."



So what happened was last month the Russian Federal Service for Technical and Export Control, that's that FSTEC, granted Astra Linux the security clearance above top secret to "special importance."  Which means the OS can now be used to handle Russian government information of the highest degree of secrecy.  And before this...



LEO:  And good news for modern PCs, comes on CDs.  Excellent news.  No more floppy booting.  This is it.  This is it.  It's on a CD.



STEVE:  Yeah, a CD; right.  Nice.



LEO:  A CD.  Maybe a DVD.  Maybe they're - that's great.  Or you can download.



STEVE:  CD was 700MB; right?



LEO:  Yeah, yeah.



STEVE:  So I'm not sure if that would...



LEO:  Is on CD now in a...



STEVE:  Download now, yes.



LEO:  Download, please.  That's funny.



STEVE:  So anyway, before this the Russian government - and again, this to me makes no sense.  They had been using a special version of Windows which had been modified, checked, and approved for use by the FSB.  So... 



LEO:  I'm not sure, yeah.



STEVE:  I don't know what that means.  Maybe Microsoft had a deal with Russia to get Windows into Russia?  I don't know.  But anyway, now the Russian military will begin their move to Astra Linux now that the FSTEC has given it the grade above top secret that is special importance.  So they're now, by law, able to use it.  And since it got secret and top secret, Astra Linux has slowly made its way into other government agencies and is currently in use at the Russian National Center for Defense Control and other government and military agencies.  A year and a half ago, at the start of 2018, the Russian Ministry of Defense announced their intentions to transfer military systems from Windows over to this Astra Linux, citing their fears, which in my mind are entirely reasonable...



LEO:  You bet.



STEVE:  ...that Microsoft's closed source approach might hide Windows backdoors - well, okay, I'll talk about a monoculture in a second - Windows backdoors that could be abused by U.S. intelligence to spy on Russian government operations.  And as our listeners know, I've long been shaking my head in amazement that Russia could still be using Windows for anything because that just boggles my mind.  So anyway, for this past 18 months the company RusBITech has been pursuing the Russian government certification process to obtain this much-sought "special importance" classification for their Astra Linux, which did finally happen last April 17th.



Oh, and in addition to the FSTEC certification, Astra Linux also received Certificates of Conformity from the FSB - we know those guys, that's Russia's top intelligence agency - as well as the Ministry of Defense.  So that opens the door for full adoption by all of Russia's top military and intelligence agencies.  So they're moving to Linux, which makes sense.



What doesn't make sense to me is what's happening in China.  The Chinese military also plans to replace Windows - and again I say, "Huh?  Really?  You're using Windows?" - due to fears of U.S. hacking.  But in their case they have said they're moving to a custom OS, not Linux.  Now, okay.  It must be that they're not starting from scratch, that there already is some kind of working Chinese OS.  I don't know.  But according to reports, officials in Beijing have decided - now, what the reports say, they've decided to develop a custom Chinese OS to replace Windows on computers used by the Chinese military.  The report did not come by way of official Chinese channels, but by way of a Canadian military magazine, Kanwa Asian Defence.



As the reporting goes, due to the Edward Snowden, Shadow Brokers, and Vault 7 leaks, Beijing officials are now quite well aware of the U.S.'s heavy arsenal of hacking tools - I mean, you know, we're all, we globally, the whole world is now well aware of that - which are able to target anything from, as we know, smart TVs to Linux servers, routers, common desktop operating systems like Windows and Mac.  Since these revelations have shown that the U.S. can hack into almost anything, the reporting has characterized the Chinese government's plan as that of adopting a "security by obscurity" approach by running a custom OS that will make it harder for foreign threat actors, mainly the U.S., to spy on Chinese military operations.



I characterize the move differently, though.  If you can do this, I mean, if it can be done, I think it makes a lot of sense; though, as I said, I think it's going to be a heavy lift to create a desktop-class OS from scratch.  I don't see it as security through obscurity, but rather as a healthy move away from a monoculture OS environment.  Monocultures are inherently fragile, and increased heterogeneity among OSes hugely reduces the threat from both accidental mishap and deliberate attack.  With someone like SandboxEscaper spewing out privilege elevation exploits daily, lord only knows how many ways into Windows our own OS intelligence services have already discovered and are stockpiling.



And as I've said before, I'm utterly amazed that Windows has been allowed to exist inside rival foreign governments for as long as it has.  It must be that any Windows machines being used in sensitive environments are well isolated and never experience encrypted real-time communications with the Internet.  I mean, there's no way that you could put a Windows machine somewhere sensitive and let it have access to the Internet.  You don't know what it's doing.  Everything it's doing is encrypted, and it's just spewing, I mean, I don't know if you've ever looked at how chatty Windows 10 is now.  It's nuts.  And again, you have no idea what's going on.



So there's just no way that that's the way Windows systems are being used.  Or probably these things are old XP systems that have not been updated forever, but they're in sequestered networks somewhere, not subject to attack and not communicating the way modern Windows systems are.



Anyway, the task of developing the new OS and replacing Windows goes to the Internet Security Information Leadership Group, which was first reported by Epoch Times, citing the May issue of that Kanwa Asian Defence magazine.  The magazine said this new group answers directly to the Central Committee of the Chinese Communist Party and is separate from the rest of the military and intelligence apparatus.  So there is now a new Internet Security Information Leadership Group that is in charge of developing the Chinese OS.



It would be fun to be assigned the task of creating a new OS from scratch.  But it would also be so tempting to borrow from the existing extensive open source code base.  The problem is there is no way to do that without the possibility of inadvertently carrying over unsuspected vulnerabilities.  At the same time, look at how long it took us, "us" the industry, to get something as relatively simple and straightforward as a TCP/IP protocol stack that wasn't an utter disaster of edge cases and security flaws.  Leo, when we first started the podcast 14 years ago, I mean, we were still finding problems in production TCP stacks.  And in fact I remember we talked about some instance where a new system was brought online.



LEO:  Yeah, I think it was like Windows Vista.  And they had rewritten the entire TCP stack from scratch.



STEVE:  Right.



LEO:  Something like that; right?



STEVE:  And they brought forward some problems that had been resolved years before.



LEO:  Yes, exactly.



STEVE:  They returned.



LEO:  Yes.  I think that was Vista.  It was a version of Windows.  I remember that.



STEVE:  Yeah.  And so, I mean, there's certainly something to be said for code which has had the crap pounded out of it, I mean, which is where any - well, in fact, one of the versions of Windows didn't carry their own stack forward.  I remember they took the BSD stack.  That may be what we're remembering was that Vista just said, okay, we're just going to - we can't recreate this wheel or reinvent this wheel.  We're just going to take the BSD stack.  There was one version of Windows where we learned officially they were taking the open source stack from one of the BSDs, and they were going to start there because otherwise, I mean, because they recognized how easy it was to make these mistakes.



And that's the point.  On one hand I can see the value of starting from scratch.  I mean, if you absolutely had a blank slate, then no previous influence, no previous malicious influence, if it existed, would be carried over.  But the problem is you have the mixed blessing because none of the lessons, I mean, the hard fought, hard won lessons that you only get by putting these things out on the frontline and having the world's hackers pound on them, and then going, ooh, ouch, okay, and then fixing this; and ooh, ouch, and fixing another thing.  And finally that process stops because there's nothing left to fix.  You get there eventually.  And then you hope that you can add something to it without breaking something because you don't want to mess with this.



So I don't know.  I mean, I really see China sort of between a rock and a hard place on this because there's, you know, if Russia has a version of Linux which is descended from the Linux the rest of us have, then it has the benefit of the lessons, but you will never know that there hasn't been some as-yet-undiscovered influence that went into the code.  The flipside is with, if you start from scratch, good luck.  You can't write a browser renderer from scratch, and a JavaScript on-the-fly compiler with state-of-the-art performance.  I mean, just think of how sprawling today's functional desktop has become.  Maybe they don't want - maybe they're happy with email and directory listings, in which case, yeah, you could give them that in a couple months.



LEO:  Or maybe they're going to actually not rewrite it from scratch, but they'd like you to think they're rewriting it from scratch.  Might be a little security through obscurity.  They'll probably just steal FreeBSD's code, dress it up.



STEVE:  Yup.  Yup.



LEO:  I mean, I don't blame them.  Why should you tell the world what your kernel is?



STEVE:  Yes.  And you can then imagine that there would be an effort on the part of the world's intelligence agencies...



LEO:  Yeah.  Now we know what they're using.



STEVE:  ...to get a copy of the Chinese OS.  And then they'll go, oh, we know what version of Linux that was taken from.



LEO:  Yeah, true.  I can download Astra right now.  I'm sure you could start hacking on it; right?



STEVE:  So one of the interesting announcements that everybody brought to my attention, and I know you were talking about it both at the time and, I'm sure, during your previous podcast, was Apple's announcement of Sign in with Apple.



LEO:  Yes, which they're going to enforce.  Jim Dalrymple, or maybe it was James Thomson, told us they're going to require it on the App Store.



STEVE:  Yup.  I have that in our show notes.  So during yesterday's, for those who don't know, during yesterday's Apple Worldwide Developer Conference 2019 kickoff, Apple announced their launch of a privacy-respecting Sign in with Apple OAuth redirection service, similar to the very familiar Sign in with Google and Sign in with Facebook that is becoming increasingly ubiquitous.



And they did something clever, though.  They beat all other OAuth providers to the punch by adding a feature to their forthcoming OAuth redirection service which I just sort of named as I was putting the show notes together Managed Random Email Address Provisioning.  And I'll explain why I called it that in a second.  It's a brilliant idea, and it's a reason for using Apple's OAuth service over others, even if one isn't that concerned about OAuth's tracking.



Okay, but let's back up a little bit.  First, we know OAuth is a convenience, but it comes at the clear cost of explicit and high-reliability tracking.  When our browsers are redirected through that chosen third party's authentication provider - Facebook, Google, whatever - that third party knows who we are since we have an account with them.  And they know to which site we are signing in since they're negotiating our identity with that site on our behalf behind the scenes.  And all of this uses first-party session cookies, so no kind of tracking blocking, third-party cookie blocking, fingerprint, none of that applies.  None of that works.  So it is the number one most reliable means for a third party to know who you are and where you are, where you're going, that exists today.



So it's no surprise that Facebook and Google have said, oh, yes, use our OAuth service.  And we know that there's just no way that Facebook and Google, both having strong financial interests in compiling activity profiles about everyone, are not actively leveraging for their own purposes what is essentially very specific and clear "who you are and where you're going" information.  So by comparison, traditional browser tracking is inherently soft and fuzzy and requires some degree of behavioral inference.  Not so with OAuth.



So on the tracking privacy front, Apple's announcement was clearly brilliant from that standpoint, and a poke in the eye at all other OAuth trackers.  The problem is that, for the experience to be seamless, our browsers need to be maintaining an active session state with the OAuth provider so that our browsers can bounce through them transparently.  But currently I have no such relationship with Facebook.  And although I'm currently an avid Apple device consumer, I don't currently have such a relationship with Apple because Apple is not otherwise a provider of online services for me.  I recognize, I mean, I'm a Windows guy, not an Apple guy.  So I'm not logged into my iCloud account or anything on my Windows system.



But I do have a relationship like that with Google.  Every one of my various browsers maintains persistent sessions with Google because Google already offers many online services which I use.  And while I dislike the idea of being tracked, it's not really something I worry about that much.  As our listeners know, I hate the idea from a technology standpoint of it not being under my control if I were to care a lot about it.  That really seems wrong.  And I certainly understand that many people do care.



But remember, as part of the OAuth transaction, the site being signed into can and does request data from the site you're signing in with, such as your username and your email address.  Since this happens behind the scenes, users may not even be aware of it.  Or, if they are shown that this will be happening, they're typically not given the option to have that information provided or not.



Now, Apple actually does.  I have a screenshot in the show notes that shows that happening.  But what this means is, and what may mean a great deal to people who already strongly trust Apple, and in a moment we'll see why deep trust is required, is that, rather than providing the service that I'm being signed into, my real Apple email - actually, that would really annoy me because that email account is unique, and I only use it with Apple, so it is completely clean.  Nobody else has it or knows it.  I get no spam or any nonsense there.  So I really like the fact that, if I get email, it's absolutely from Apple.



So Apple will be offering the option to proactively mask their user's actual email by providing a randomly chosen email relay address.  And on the screen of the WWDC conference, Craig put it up that the example was fc452bd5ea@privaterelay.appleid.com.  So the idea would be, if you say "Sign in with Apple" to some third-party site, at your choice, Apple will make up an email address and provide it as yours to that third party.  And I called this Managed Random Email Address Provisioning since also onstage yesterday Craig indicated that Apple users would have some means for curating and removing the relay addresses from any sites from which they no longer wish to receive mail.



But this does place Apple in the enviable position, should they ever decide to leverage it, and I'm not suggesting that they would, but they will be receiving and viewing, if they wished, and then forwarding all of the email being received.  In that sense it's not unlike Google with Gmail, who as we know sees all of our email, runs their bots through it, and does whatever they do with it.



Anyway, in the show notes I have a picture that we have seen where it says "Use your Apple ID," and in this instance it's kim_kilgo@icloud.com, "to sign into Fretello and create your account with the information below."  And then it shows Name, Kim Kilgo.  And it looks like you could hit X and remove that.  I don't know what happens if you do, if you fill in a blank, or if they're just not going to provide that to the third party.



LEO:  I think you just choose one of those three.



STEVE:  Oh, you think it's not...



LEO:  Oh, I see what you're saying because it's Name and X.  Yeah, maybe you don't give them your name.  No, you could get rid of that and choose something else.



STEVE:  And then you have radio buttons for either Share My actual Email, the iCloud.com account, or you can click a button and say Hide My Email.  And then it says Forward To and then your iCloud account.  So anyway, so what happens is Apple would make up an email address which would be - which they would record in a database associated with you so that, when subsequent email comes in there, they would relay it, they would forward it to your actual iCloud account.  So that would mask your iCloud account email from the third parties and give you apparently administrative control over it.



There would be some interface.  You log into iCloud with your browser to Apple, and then you would get a list of all of the sites who have a forwarding for you.  And who knows, maybe you could either remove the forwarding - I guess, no, you wouldn't be able to do that.  Well, yeah, you could cancel the forwarding, in which case their email would no longer go through to you.  Otherwise you'd have to go to the site and then update your email address with something else.



So anyway, it's probably not a big deal that Apple is seeing your email pass by.  Again, we trust them.  They make a big point of being privacy enforcing.  As I mentioned, Google already gets and scans the email.  But anyway, it is something to keep in mind.  For me, my Google email is already my designated junk email bucket since its anti-spam filtering is so good.  And as I said, my Apple email account is absolutely pristine because I've never used it for anything else.



So for me, I think I'd rather continue using Sign in with Google and have that email address being disseminated - and, boy, I mean, it's a junk pile - and keep my account with Apple clean.  But, Leo, as you mentioned, and as I did pick up, interestingly, one reason to think that this will become prevalent, at least from within apps over which Apple has control, like those for iOS, is that offering it will not be optional.



Down in the fine print at the end of Apple's new App Store Review Guidelines, Apple states:  "Sign In with Apple will be available for beta testing this summer.  It will be required as an option to be offered to users in apps that support third-party sign-in when it is commercially available later this year."  So that is to say, any app that offers Sign in with Facebook, Sign in with Google, will be required to also offer Sign in with Apple.  So as soon as that starts to happen, it's going to appear.  So any apps which offer OAuth third-party login will have Sign in With Apple also.



So anyway, I mean, I think this is good news.  I know that there are people who dislike the idea.  I mean, actually, Leo, I was surprised that it got as much traction as it did, as much pickup in the industry.  But it's interesting to know that the downside of "Sign in with ..." is that you are trackable by the third party that you're using as your authentication provider.  I didn't realize that was as widely understood as it is.  So that's good.



LEO:  Yeah.  It's a setting, by the way, I love the setting in Brave, the privacy browser based on Chrome.  You can turn off those Facebook/Google/Twitter bugs.  LinkedIn also is in that list.  Block them.



STEVE:  Except you can't do it in this case.  Yeah, you can't do it in this case.



LEO:  Not in apps.



STEVE:  No, I mean, you cannot use OAuth without having that trackability.  I mean, the trackability...



LEO:  Oh, yeah, yeah, yeah, yeah.  So what it does is it turns the bugs off so you can't use it.  Basically you have to log in with an email address.



STEVE:  Well, no, because the bug...



LEO:  Which I choose to do because honestly, when I had to leave my Facebook account, it disconnected a lot of accounts I'd used the Facebook login for.



STEVE:  Yes, yes.  Now, our listeners know that there's no way I can talk about the problems with OAuth and what a mess...



LEO:  Without SQRL.



STEVE:  ...all of this is, without noting that there is a working system which is now in place, doesn't have a lot of traction yet, but that's to come.  It's now in place.  It has none, not a one, of these issues and problems.  And that, of course, is the system that has been my primary focus for the past five years.  There cannot be any tracking nor any form of information disclosure because SQRL was designed, as our listeners know, for all of these reasons to be a fully functional two-party authentication system between just you and the site you are wishing to establish a relationship with.



SQRL instantly, uniquely, and pseudonymously identifies its users to new websites and revisited websites without revealing any information of any kind whatsoever.  The site receives only a long unique string that only that SQRL user will present to only that website.



I know that I've been talking about this for years, but it's all finally finished.  The SQRL Forum site currently has 1,275 registered and active members, all of whom are using SQRL clients daily with Windows, Linux, iOS, Android, Firefox, Chrome, and Edge, to log in without using or needing to supply usernames, passwords, or email addresses.  I am very nearly finished with the first section of the final, my final piece of work, which is the SQRL explainer and protocol specification document.  I had hoped to have the first section ready to announce today and to offer today.  But I missed that mark.  I still need to complete the identity replacement and the SQRL service provider API overviews.  But it will be ready by next week, and I'll have a link for everyone to download the PDF then.



And by then I will also already be working on the second half, which is the fully articulated SQRL Protocol Specification.  And as with all of the first half of that document, all I'm doing is documenting what has already been done, what already exists and has already been proven, and is in active use.  So my own SQRL reference client for Windows appears to be completely finished.  It's been several weeks since its last update and the release of that, which was a couple tiny little bit of text tweaks.  I have one button whose jargon was changed to reduce some confusion.  So I expect to make it widely public shortly also.  So in other words, everything is finally just about ready.  Whew.



LEO:  Phew.



STEVE:  Yes.  I had one little bit of fun closing-the-loop feedback regarding, remember, with RDP, we were having fun calling that Really Do Patch.  Christophe Vanlancker tweeted, he said:  "Dear @SGgrc, you mispronounced," he said, "RDP is Ransomware Deployment Protocol."



LEO:  Much better.  Oh, my god.



STEVE:  And as for SQRL and everybody waiting for that to get done, of course that's because they want me to get back to SpinRite 6.  I found a nice note from a listener.  He said:  "Hello, Mr. Gibson.  I've been a listener on Security Now! for a few years and always enjoy seeing a new episode show up on my phone Wednesday mornings.



"After listening to Episode 702 and after hearing, not even seeing, you and Leo talk about the PiDP-11" - which of course you have blinking behind you, Leo.  He said:  "I knew I had to have one."  He said:  "Beyond building it and getting it running, my main interest was using the front panel switches to do something interesting, such as toggling in the bootstrap loader as you mentioned doing on other PDP machines during the episode.  I wasn't able to find any instructions or examples of people doing this on the PiDP-11, so I decided to figure it out for myself.



"After finding the bootstrap code for the RT-11 operating system, I published a short YouTube video today discussing how to configure the PiDP-11 software..."



LEO:  I had no idea the switches were even connected to anything.



STEVE:  "...and toggle the correct switches in order to boot RT-11 OS, which is included in the PiDP-11 software distribution.  The Google Group discussion thread is" - and he gives us a link.  "And the video itself is" - and then there's a link to the video.  He said:  "Both you and Leo might be interested in trying this, as it is fairly quick and entertaining to toggle switches and see something useful occur."



LEO:  Wow.  I have to give our creator of this...



STEVE:  Oscar.



LEO:  ...Oscar Vermeulen a lot of credit because I just figured the switches were cosmetic.  I had no idea...



STEVE:  Oh, no, no, no.



LEO:  ...they were connected.  They must be connected to the GPIO bus on the Raspberry Pi.



STEVE:  Well, yeah, indirectly.  So, I mean, they actually emulate the PDP-11.  And we talked about it at the time.  He was so determined to make the switches function correctly that he sat down with a friend in front of an actual PDP-11 and videotaped using the switches.  Because, for example...



LEO:  The direction has to be right.  But I just thought that was for the physical motion.  I didn't realize they were actually connected.



STEVE:  Oh, yeah.  So, for example, in fact, you just did it.  You just entered an address that you had put into the switches into the address register.  Then you toggle them to something else, and you deposit that data into that location.  And then the address register auto increments so that you don't have to manually go back and put a different address in.  Instead, you're just able to auto increment the address register and enter successive pieces of data.  So, yeah.  It's the way those machines work.  And it's very cool.  So anyway, then...



LEO:  By the way, that's the video that he created.



STEVE:  Ah, okay.



LEO:  I have no idea what I'm doing.



STEVE:  So anyway, he finished with what he called the "obligatory SpinRite story."  And he said:  "This is a fairly mundane success story."  And of course, for me, no report of SpinRite success is mundane.  He said:  "I have a NetBSD server which performs a nightly backup to an eight-year-old 2TB hard drive."  He said:  "Since this is a nightly task that has run for years, I've been able to watch the hard drive transfer rate steadily decreasing over the past few months until the point where it was obvious that something was wrong, even though the SMART data that I had also been collecting didn't seem unusual or to be deteriorating.



"I had been meaning to run SpinRite on that drive, but didn't get around to it until last week; and when I tried to halt the system, I discovered it was running so slowly that several prior nights' worth of backups were still running, and the drive was completely unresponsive.  I eventually just had to force power down the machine.  I ran SpinRite on Level 4 for," you know, basically overnight, or actually over a day.  He said:  "...for a little over 24 hours.  There were no errors reported; and, again, the SMART data seemed okay.  I reinstalled the drive into the server, and it is now working correctly and performing at its normal speed!  Thanks again for the podcast and SpinRite!  Signed, Jeff Thieleke."  So Jeff, thank you for your report about, well, about your cool project of figuring out how to use a switch register to put the RT-11 bootstrap into the machine.



LEO:  Wow.  Where did he get the paper tape reader?  That's what I want to know.



STEVE:  And then also your SpinRite story. 



LEO:  It's really cool, yeah.  Really neat.  You want to talk about, what is this?



STEVE:  The Nansh0u Campaign.



LEO:  Nansh0u, coming up.



STEVE:  Nansh0u.



LEO:  Nansh0u.  All right.  Let's find out about this malware here.



STEVE:  So as I mentioned at the top of the show, and I'll explain a little bit as we get into the details, the thing that's disturbing is that this is a relatively straightforward attack.  It is ongoing.  It shouldn't ever happen, for a number of reasons.  I guess that's true of everything we talk about on this podcast.



LEO:  It shouldn't, none of this should happen, but it does.



STEVE:  Yeah.  But what's really interesting is the guys that tracked this down at Guardicore Labs noticed some things that caused them to believe that this was professional, state-level, state actor tools in the hands of non-state actors.  So I have the link to their complete write-up, which I've paraphrased here.  They wrote:  "During the past two months, Guardicore Labs' team has been closely following a China-based campaign which aimed to infect Windows MS-SQL and PHPMyAdmin servers worldwide.  We've taken a deep look into the inner workings of the campaign - the tools in use, the vulnerabilities exploited, and the extent of damage caused."  And what's very cool about this is, you know, the reason I like to share these things is the degree of forensic detail that I think our listeners will find really interesting.  I did.



They said:  "Breached machines include over 50,000 servers belonging to companies in the healthcare, telecommunications, media, and IT sectors.  Once compromised, the targeted servers were infected with malicious payloads.  These in turn dropped a cryptominer and installed a sophisticated kernel-mode rootkit to prevent the malware from being terminated."



So I'll just pause here and say notice that, again, what's being installed in these things is things that make money.  When we first saw, when we touched on the first instance of cryptomining years ago on this podcast, I said, "Ohhh, this is not good."  Because, you know, viruses were just sort of like, okay, well, they were an annoyance, but there was really no - there's no reason for it to be done.  It bothered people.  Maybe some people got their systems compromised.  But the incentive was missing.



Well, that changed when cryptocurrency happened because now there was an incentive, the universal incentive of money.  And so that's what we're seeing here, 50,000 servers that had a hackable MS-SQL server that now have 50,000 instances of a cryptominer, mining for a shared pool.  And not even mining correctly, as I'll get to in a second.  So it's like, they got the parameters, in some cases, the parameters backwards.  So it's like, oh, my goodness.  But anyway, still, the motivation is there.  And what they're installing is cryptominers.



So in the show notes I have a picture of a graph of just one month of observation of this campaign showing from early April, on 4/13, April 13th, there were 24,087 infections; and a month later, exactly a month later, on May 13th, 47,985 infections.  So it's sort of a, almost, you'd call it a straight line for all intents and purposes.  It's got little jiggedy-jags because you're brute forcing usernames and passwords.  What's distressing is that it works.  I mean, again, there's a mixed blessing associated with publicizing the success of a campaign like this because somebody who would think, oh, that can't possibly work, well, whoops.  Yeah, it worked 50,000 times.



So they wrote:  "In the beginning of April, three attacks that were detected in the Guardicore Global Sensor Network caught our attention."  Okay, so that says they've got honeypots or a network of stuff that are able to pick up these things.  They said:  "...caught our attention.  All three had source IP addresses originating in South Africa and hosted by VolumeDrive ISP."  And they said in their notes, "See our IoCs."  So that's Indications of Compromise.  "The incidents shared the same attack process, focusing on the same service [meaning MS-SQL] and using the same breach method and post-compromise steps."  So that said to them, okay, there's something going on here.  There's a common actor.



"Looking for more attacks with a similar pattern, we found attacks dating back to February 26th, and over 700 new victims per day.  During our investigation we found 20 versions of malicious payloads, with new payloads created at least once a week and used immediately after their creation time."  In other words, they found timestamps, like compilation timestamps, still embedded.  And so they were being tweaked, built, and then immediately deployed.



"This timeline," they said, "combined with a set of five attack servers and six connect-back servers" - normally referred to as command-and-control servers - "suggests an established process of continuous development which was well thought out by the attackers.  Having access then to the attacker's infrastructure, we were able to monitor the relevant file servers and draw insights on the campaign's scope and size.  The graph on the page above" - the one I showed - "shows how the number of successfully infected machines doubled within the time span of one month.  Each attack started with a series of authentication attempts to an MS-SQL server" - oh, I said MySQL.  I meant MS, sorry.  Well, any SQL server - "eventually leading to a successful login with admin privileges.  Then a sequence of MS-SQL commands was executed to accomplish the following."



So basically they brute forced their way in to get in with admin privileges.  Then they configured the server settings to allow what they called a "smooth and error-free attack flow."  They created a Visual Basic script file in the ProgramData folder which was numeral 2.vbs, executed this script and downloaded two files into that same folder over HTTP, then ran those two files in a single command line.



They wrote:  "The attacker's servers were all running HFS - HTTP File Server - serving files of different types.  One text file included the string Nansh0u [N-A-N-S-H-0-U] on which we based the campaign's name.  The attacker's infrastructure contained all the modules required for a successful end-to-end attack on MS-SQL servers:  a port scanner, an MS-SQL brute force tool, and then the remote code executor.  The port scanner used by the attacker had been known since 2014," so off the shelf.  "Our attacker used it to detect MS-SQL servers by scanning IP addresses and checking whether typical MS-SQL ports were open.  The ports examined were 1433, 2433, 1533, 1143, 9433, and 5433.



"The results of the scanning were then fed to the brute force module.  The brute forcing tool attempts to log into each MS-SQL server using tens of thousands of common credentials.  Once authentication succeeds, the server's address, username, and password are saved to a file for subsequent use.  The dictionaries of common credentials used by the attacker can be found in the campaign's Indications of Compromise repository on GitHub."



And in fact I have the link, as I mentioned, to the GitHub repository on GitHub.  I think I looked, it was like a 1.x megabyte file, so just a massive text file of usernames and passwords which are used in a simple brute force attack.  So not passwords like would be generated by LastPass, but like brain-dead passwords that no admin should ever use for their server.



"By running the port scanner and the brute force tool, the attacker gained a list of breached servers' IP addresses, ports, usernames, and passwords.  The next step was to log into the victims and infect the machines.  On the HFS (HTTP File Server), in a folder named 'chuan,' C-H-U-A-N, which is Chinese for 'infect,'" they wrote, "we found two interesting components.  The first was a script named 'Mssql.log.'  The script's commands were the exact ones we saw in the incidents that triggered our original research.



"The second file was an executable named 'Usp10.exe.'  This program receives a server's address and credentials, as well as the contents of the Mssql.log.  Its functionality is straightforward.  It logs into the breached server and executes the Mssql.log script, proceeding with the attack on the victim's side.  Usp10.exe is the attack module responsible for writing and executing the downloader script (2.vbs) on the MS-SQL victim.  Once the MS-SQL script is done running on the victim machine, the malicious payload is executed.  In all the attacks we have seen as part of this campaign, the attacker executed a command line with two executables."  And then they show c:\windows\system32\cmd.exe, which as we know is the standard Windows command interpreter, the standard command line, what you used to call the "DOS box."  And so then it executes programdata\apexp.exe and programdata\tl.exe.



They wrote:  "This command executes a privilege escalation exploit" - so once again here's a perfect example, in the wild, of why a privilege escalation exploit by itself doesn't, you know, it isn't remote code execution, but it factors into and is crucially important for virtually all other non-directly remote code execution exploits.  This is only one step removed.



So "This command executes a privilege escalation exploit that runs a malicious payload with system privileges.  In the attacker's arsenal are two versions of the PE exploit" - PE is Windows' Portable Executable format - so "apexp.exe and apexp2012.exe, and many payload versions.  This apexp.exe and apexp2012.exe," they write, "are two exploits of a known privilege escalation vulnerability from 2014 (CVE-2014-4113).  Passing any program to these executables will run it with system privileges.



"Apexp.exe is known as the Apolmy [A-P-O-L-M-Y] exploit, and it affects both desktop and server versions of Windows from XP to 8.1 and Server 2003 to 2012 R2, respectively.  This," they write, "is a weaponized exploit with production-level code.  Apexp2012.exe, on the other hand, resembles more of a proof of concept than an operational exploit and is designed to work on Windows 8.1."  They wrote:  "We found the latter available for download on a Chinese forum seemingly used as a hacker community forum.



"While both versions use the same vulnerability, they execute kernel-mode code for different purposes.  Apolmy version copies the system process access token to its own process.  With that token, the exploiting process runs the payload with full control over the victim machine.  The second version uses a method commonly popularized.  Here, the exploit adds the SeDebugPrivilege to the token.  Using this Windows privilege, the attacking exploit injects code into the Winlogon process.  The injected code creates a new process which inherits Winlogon system privileges, providing equivalent permissions as the prior version."



They didn't say this, but the reason you would use that second one, which seemed less professional, is that it might be that the earlier exploit had been patched.  After all, it's been known since 2014.  So one would hope that maybe the server that unfortunately has its MS-SQL server available through brute force admin login, it might have had the first problem patched, but no remediation for the second.



So then they said:  "We collected 20 payload samples from both the attacker's servers and Guardicore Global Sensor Network.  Each payload is in fact a wrapper and has several functions:  execute the crypto-currency miner, create persistence by writing registry run-keys, protect the miner process from termination using a kernel-mode rootkit, and ensure the miner's continuous execution using a watchdog mechanism.  The payloads spawn one of two processes, either dllhot.exe or canlang.exe, depending upon the payload, which in either case mine a private cryptocurrency named TurtleCoin for four different mining pools.



"Many of the payloads drop a kernel-mode driver, named randomly and placed in AppData/Local/Temp. Its compile time suggests that it had been created in 2016.  Nevertheless, most AV engines do not detect the driver as malicious.  We found that the driver had a digital signature issued by VeriSign.  The certificate, which is expired, bears the name of a fake Chinese company, Hangzhou Hootian Network Technology.  Guardicore Labs contacted VeriSign and provided them with the relevant details, which resulted in the certificate's revocation."  Not that it matters because I think they said it was expired.  Maybe not.  I remember taking a look at it myself and noticing that it did not look like the malware was timestamped.  So if the certificate expired, the malware would no longer be trusted.



Anyway, they said:  "This would have been less awkward had the driver not been packed and obfuscated.  Unlike many other malicious drivers, this driver is protected and obfuscated with VMProtect, a software tool that attempts to frustrate reverse engineers and malware researchers.  The driver is designed to protect processes and prevent the user from terminating them.  It creates a device named SA6482, allowing processes to communicate with it.  The device receives process IDs meant to be protected," they said, "in our case, the cryptominer's process ID.  The driver protects the process by registering a callback on the process and thread objects.  These callbacks are triggered with every process to the protected process or any of its threads and allows drivers to modify the access rights given in each access attempt."



So anyway, the point is - I'm going to skip.  There's a little bit more detail.  Everybody gets the idea that this thing, it gets in through a brute force attack on a publicly exposed SQL server using an exhaustive list, but obviously an effective list because they've got more than 50,000 instances now compromised that way, of usernames and passwords.  And it doesn't care about where it is.  It doesn't try to do anything other than make money immediately.  Sets up a cryptominer that mines with a pool and has at it, and then installs a driver, a kernel driver whose job is to protect the mining process, to basically fight back against it being terminated.  So that technically is a rootkit because you would not then be able to terminate the process.  And even if you restarted the server, it's arranged to get itself to restart and reprotect itself.  So you'd have to go to some additional lengths in order to keep that from happening.



They noted that, as I mentioned, that the tools, the techniques that were used belonged to nation-state level hackers, but were, due to a lot of the things that they saw, apparently being used by much lower level hackers.  The attacks used advanced encryption technologies and rootkit hiding, but the attacks themselves were not very sophisticated.  At one point they noted that many of the programs found on the server - oh.  The servers themselves had no authentication protection.  Once they identified the IPs, they were able to immediately browse the servers in order to then monitor the attacks.  So nothing was preventing that from happening.  And a bunch of the programs found on the server were written in something known as EPL, which is Easy Programming Language.  It's a proprietary Chinese-based programming language designed for rapid application development.  So that made them think that this was not fully nation-state level attack.



So anyway, it's interesting that - it's hard to understand how MS-SQL admin account would be exposed to the Internet.  Maybe there's some reason.  But if there is, do not use a password that's going to be on somebody's password guessing list, or this is what happens to you.  So anyway, I just thought it was a really interesting forensic look at how somebody is finding machines, brute forcing their way in, and setting up cryptocurrency mining.  And this basically connects back to the beginning of the podcast, when I said, given that there are, okay, so this was going in the hard way over the course of several months to obtain 50,000 machines.  Now we have a million open RDP machines just waiting, just begging to have cryptominers set up in them and have the door close behind them.  I'll be really surprised if that's not exactly what happens.



LEO:  Of course, a Windows XP machine isn't going to be a great cryptominer.  But what the hell.



STEVE:  Yeah.  Yeah.



LEO:  It's free.



STEVE:  Well, and no longer do you need to mine individual coins.  You now just contribute your resources to a pool.



LEO:  Yeah, a million of them.



STEVE:  And so any cycles that it has available, exactly, you've got numbers on your side.



LEO:  Yeah, yeah.



STEVE:  Yup.



LEO:  All right.  There you go.  I'm not surprised.  Anybody using PHPMyAdmin, there's a lot of people who, I mean, I did this, you know.  Your first time you set up WordPress you set up your MySQL.  You're following a cookbook on the web.  You might not make sure it's got a good password because you want to remember it, and not even realize it's online.  I could see how that can happen.  This is all designed to make it really easy for people to do this kind of stuff.



STEVE:  Yes.  And if it's on a - they did say MS-SQL.  It's oftentimes...



LEO:  Oh.  It's SQL Server.  It's not MySQL.



STEVE:  Right, right.



LEO:  Oh, oh, oh.



STEVE:  But it is oftentimes the case that you will unwittingly bind the server to asterisk rather than to the local interface, and so you are unwittingly...



LEO:  Opening it up, yeah.



STEVE:  Opening that port to the rest of the world, too.



LEO:  In other words, it's an easy mistake to make, but it's why we always emphasize you shouldn't be running servers if you don't know what you're doing because a server is open to the public.



STEVE:  Right.  Right.



LEO:  That's the problem, yeah.



STEVE:  And just stick it behind a router.  If you stick it behind a router and then only...



LEO:  But then it doesn't serve.



STEVE:  Yeah.  Well, but then you only open the ports that you need to.



LEO:  Right, right, right, yeah.  Of course.  I mean, securing this isn't hard.  It's just that people aren't paying attention.



STEVE:  Right.



LEO:  They don't know what they're doing.  They're following a recipe they read somewhere.  Ah.  Oh, well.  Steve, as always, an eye opener.  And that's why people tune in every Tuesday around about 1:30 Pacific, 4:30 Eastern, 20:30 UTC to listen to or watch Security Now!.  Steve has copies of the show at his website, GRC.com, along with transcripts.  I know a lot of you like to read while you listen.  And that's a great way to do it.  Those are very well-written, human-transcribed transcripts of the show.  They even get our exclamations like "Whoo" or "Wow."  I don't know how Elaine transcribes those, but I'm thinking W-H-O-O?



STEVE:  They're all there.



LEO:  They're all in there.  While you're there, pick up a copy of SpinRite, the world's best hard drive recovery and maintenance utility.  If you have a hard drive, you need SpinRite.



STEVE:  And even if your hard drive is just running slowly.



LEO:  Yeah.  There's a reason.



STEVE:  That's right.



LEO:  That's a sign.  That's a symptom.



STEVE:  And SpinRite can fix it.



LEO:  Yeah.  Steve's got a lot of other stuff there.  In fact, it's a great place to hang out:  GRC.com.  You can leave Steve feedback at GRC.com/feedback, or follow him on Twitter, @SGgrc.  And you can always Direct Message him there.  There are a couple ways to reach Steve.  We have audio and video of every show at our website, in fact of all of our shows, TWiT.tv - TWiT.tv/sn for Security Now!.  It's also on YouTube.  I mean, it's everywhere.



If you have a podcast program you like, you could subscribe there.  In fact, that's probably the best way to get a copy of the show.  And don't forget you can ask your smart voice assistant.  In many cases, all it takes is "Play Security Now! podcast," and you'll be able to hear the latest episode.  Or if you want to listen live, "Play TWiT Live," and get what you want.  Thanks, Steve.  We'll see you next week on Security Now!.



STEVE:  Thanks, buddy.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#718

DATE:		June 11, 2019

TITLE:		Update Exim Now!

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-718.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we catch up with the continuing antics of SandboxEscaper.  We give an update on the status of the still-not-yet-widely-exploited BlueKeep vulnerability, and also look at a new botnet which is pounding on RDP servers (but not yet using BlueKeep).  The FBI has issued an interesting advisory about not trusting secure sites just because they're secure, so we'll examine that.  The popular VideoLAN player receives an important update thanks to an interesting source, Microsoft's Edge browser takes another step forward, and Mozilla reorganizes a bit.  Then I'm going to share my must-have Utility of the Week, a just-released sci-fi movie on Netflix, and a bit of closing-the-loop feedback from the Twitterverse which resulted from my, as planned, first formal full release of SQRL.  We'll close with a look at the critical need for anyone running the Exim mail server to update immediately.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  SandboxEscaper is back with, yes, yet another zero-day.  Not too much to worry about.  The NSA has an advisory.  And it's the release of SQRL.  What?  It's all next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 718, recorded June 11th, 2019:  Update Exim Now.



It's time for Security Now!, the show where we cover your security online and your privacy, how things work, how things don't work more often.



STEVE GIBSON:  Yeah.



LEO:  With Steve Gibson.  He's right there.  He's the man in charge at the Gibson Research Corporation, @SGgrc, the guy who discovered the first spyware.  That's how old he is.  Actually, that wasn't - how long ago was that, Steve?



STEVE:  That was a long time ago.



LEO:  Ten years?  Twenty years?



STEVE:  I think it was before the podcast, and we're closing in on the end of year 14.



LEO:  Wow, wow.



STEVE:  So that was OptOut, which is what I called this thing because it was advertising.  I guess it was spyware.  Well, it was spyware because it was in people's machines.  It was watching what they did.  It was profiling - yeah, okay, it was spyware.



LEO:  It was spyware.  Anything that does that is...



STEVE:  Yeah.  And they didn't ask for it.  The idea was it was a way of monetizing freeware so that you would show ads in a window on like the UI of freeware.  And, oh, boy, I mean, it caused quite a ruckus when, you know, what happened was I was experimenting with an outbound firewall with - shoot, I just had it, and I just lost it, my favorite firewall back then.  Well, it has been a while.  Anyway, it was the one that did outbound blocking.  No other firewalls did that.



And so I set it up, turned it on, and then up popped a notice saying that something, Aureate or Radiate or something, wanted to connect to the Internet.  And back then, those were still the days when you sort of had some idea of what was going on in your computer.  I mean, we've lost all hope of that now.  But it was like, wait a minute.  What?  I didn't put that in my computer.  So I tracked it down, and I found out what it was.



Oh, ZoneAlarm is the name I was trying to come up with, the ZoneAlarm firewall that allowed you to do outbound control for exactly this reason, because something could be in your machine that you didn't know about.  And of course now, again, as I was saying, actually I have a recommended Free Tool of the Week that's going to give our listeners a little bit more control over this.  I think everyone's going to get a kick out of it.



But this is Episode 718 for June 11th.  And we were talking before the show.  There is the number one most popular email server on the Internet, meaning the server that is running on machines deliberately connected to the Internet, facing the Internet, to transfer email, an email server.  Once upon a time it was Sendmail in the early days.  There have been several successors since then.  What is now the most popular one, it think that the number, I have it in the show notes, I think it's 57% of all email servers on the Internet are Exim, E-X-I-M.



And they're all, well, I can't say "all."  But unless you've updated that email server in the last month, if you had any version in the last three years, they are all vulnerable - and we're talking, I think it was 570,000 of them - to remote command execution.  Not remote code execution.  We have to do some fancy return-oriented programming or overflow a stack or bust, you know, like have a buffer overrun or something.  This is a means of actually executing Unix or Linux commands as root from, like, remotely.  So this is really bad.



The reason, if you'd fixed it in the last month, you're not affected, is that they did a new release, and they didn't even know they had a problem.  They didn't know they had this problem, and they fixed it just kind of, oh, by the way.  But it's been there since 2016, for more than three - I think it was April of 2016 - more than three years.  So anyway, that's going to be our main topic.



We've got a bunch of stuff going on in the past week.  We're going to catch up with the continuing antics of SandboxEscaper.  We also update on the status of the still-not-yet widely exploited, but expected any moment now, BlueKeep vulnerability.  We also take a look at a new botnet which is pounding on, unfortunately, RDP, the Remote Desktop Protocol servers, but not using BlueKeep to do so.  It could, but that's not what it's doing.  It's probably BlueKeep just is too new.



We also have the FBI issuing an interesting advisory about not trusting secure sites just because they're secure.  Which I thought was really interesting, and we'll talk about it because it's sort of - it tells us that, you know, the FBI gets all these calls.  They're understanding what people's problems are in some ways differently than, well, than I am.  Leo, you also get those calls on the weekend.



LEO:  Oh, yeah, absolutely.



STEVE:  So you probably know.  But it's like the world, like the general population is about a decade behind.  So we'll talk about that.  Also the very popular VLC, the VideoLAN player, received an important update thanks to an interesting source, that we sort of foretold about at the beginning of the year.  We'll come back to that.  Also Microsoft's Edge browser takes another step forward.  Mozilla reorganizes themselves a bit.  I mentioned also before we began recording, but now I'll say it officially, I have a must-have Utility of the Week for Windows users which I went searching for when I was sort of reminded of something that I'll explain, that I think everyone's going to get a kick out of.  I want to share with you a bit, without any spoilers, about a just-released on Friday really good sci-fi movie on Netflix.



LEO:  Oh, okay.



STEVE:  And I want to warn our listeners, I'll just say it right up front, do not watch the trailer because I was very disappointed in how much spoiler was in the trailer.  So we'll tell everybody about it, but don't watch the trailer.



LEO:  Okay. 



STEVE:  It's just there's too much there.



LEO:  I hate it when they do that.



STEVE:  It was so annoying.  It was like - and as the trailer was spooling itself into my brain I was thinking, no, no, no, I don't want to be seeing this now.



LEO:  Don't tell me, don't tell me, don't tell me.  Oh, that's too bad.



STEVE:  Yeah.  Anyway, so we've got that.  We also have a bit of closing-the-loop feedback from our Twitterverse which resulted from my, as planned, first formal release of SQRL, which has happened.



LEO:  What?



STEVE:  Yes.



LEO:  You buried the lead, dude.



STEVE:  Well, yes.  Just my last five years.



LEO:  Only five, yeah, six years in the making, okay.



STEVE:  Then we'll close with a look at the critical need for anyone running, or anyone who knows of anyone who is running, the Exim mail server to update immediately.



LEO:  Yikes.  Yikes, yikes, yikes.  Well, lots to talk about.



STEVE:  The Picture of the Week, I didn't have anything more pressing, so it is the first page of the 17-page document, which is finished, as the beginning of the formal, the full SQRL specification.



LEO:  Wow.



STEVE:  And so it's called "SQRL Explained."  And it's written sort of to be a - it's not a user manual because the goal is that SQRL doesn't need a user manual.  I mean, if you just use the app and take a moment to read the screens, it just tells you what you need to do.



LEO:  So you don't even need a user manual.  But this is for people who conceptually want to understand it.



STEVE:  Yes.  Somebody who read this would come away understanding everything that SQRL is:  how it's able, for example, if your identity got stolen, how it is able to prevent bad guys from changing it, and how you are able to get it back, to take back a stolen identity; how in a two-party system where there's no one to say, whoops, I forgot my password, how we perform password recovery without anyone to ask; and all of the different components that are part of the system.



And so, you know, GRC.com/sqrl.htm is a page where there are only two things.  There is that document, the 17-page PDF; and a link to the client because I also released version 1.0 of the SQRL client for Windows, which is sort of the reference client.  The other clients - there's one for iOS and Android.  It also runs under Mac and Linux with Wine.  But there's also a Firefox extension that runs with Chrome and Edge.  And they're not as feature complete because they're sort of following along behind.  But anyway, it is there.



There are, as I said, more than 1,300 accounts now over in the SQRL forums.  So there's an active community of people who have been playing with this and helping me to test it and know it really well, if anyone has any questions.  And so this is what I've been waiting for almost six years...



LEO:  Oh, man.  This is so exciting, Steve.



STEVE:  ...to tell everyone about, yeah.



LEO:  Yeah.  And there'll be one more landmark:  the first site to use it; right?  The first publicly...



STEVE:  Yes.  That would be, well, now, yes.  The SQRL forums, of course, don't really count because they're mine.  And so of course, thanks to Rasmus Vind, who is the PHP coder who I worked with in order to add SQRL support to the XenForo forum software, of course we have SQRL login there.  And I also have some other demo servers and sites that people can use.  There's one written in Java.  Jeff Arthur, who's doing the iOS client, he has one.  So there are sort of test servers.  But yes.  From the beginning there's been a strong enterprise interest, I think because large enterprises need some way of managing their employee identity that makes sense to them.  And so I got a lot of inquiries in the beginning.



My next piece of work is to follow on this 17-page explainer, you know, the idea is that this would be useful for people who want to understand what SQRL is sort of at the higher technical level.  There's no crypto, but there's some pretty block diagrams, and I explain things the way I do the podcast so that I think people will be able to probably enjoy learning something at the same time.  But then what follows will be the so-called "on the wire" protocol, that is, how you encode the data, base-64 URL, what data you encode, what you put where and so forth, so that somebody would be able to take that and actually create implementations of this that would be compatible with all the clients and all the servers that exist so far in order to start making this actually happen.



So anyway, we'll talk about this a little bit more down in our closing-the-loop section because I tweeted this yesterday, or maybe it was this morning, and got a couple pieces of feedback that were exactly what I would expect to have because there's something of a mixed blessing here that is just the case, which is there is - and I've mentioned this before.  There is more upfront investment required than with usernames and passwords.  Really not much more than with a login manager because this is sort of a variation on that.



But it's true that you sort of, in a sense, you get what you pay for, except that with SQRL all of the payment - well, first of all, it's free.  But I mean payment in terms of effort.  It's just getting it set up once.  Once that's done, then you get to reap the benefits of this updated architecture and approach, potentially forever.  And then all subsequent logins are like way easier than any other solution provides.  But it is the case that, with usernames and passwords, there's nothing for you to do except invent a password.



On the other hand, you have to then have a good password.  It has to be a different password for every site.  You have to somehow memorize the password or record it or write it down.  And then when the site gets breached, you have to change the password and blah blah blah blah blah.  And of course also it may not be very secure if you don't do all those things.  So even though there isn't much investment asked of you if you use a username and password, you also have ongoing pain forever as you use that.  And SQRL eliminates all of that.



So anyway, we'll talk about this a little bit more in our closing the loop.  And for what it's worth, this was a major milestone in this multiyear project to just basically create and propose a solution.  I have no means to make the world accept it.  The goal is that, or my hope is that it'll sort of seep out.  Over time it will be understood.



For example, all we need is a WordPress plug-in.  No one has written one.  Rasmus wasn't interested.  When someone does a WordPress plug-in, then suddenly, as we have said, more than half of the websites on the Internet could use this to allow their users a much - their visitors a much easier way to log in, if you wanted to post a reply to someone's blog, than having to create an account.  And so there's some incentive, you could imagine, for offering something easier.  And then when people kind of get used to that, it's like, wait, why doesn't everyone do this?  Well, yeah, exactly.  So anyway...



LEO:  People are saying, why doesn't TWiT implement it?  I would, except we don't have user accounts, so there's no login on TWiT.



STEVE:  Yeah.  It's like GRC.  I have no accounts at GRC.



LEO:  You can't use it, either; right?  Yeah.



STEVE:  But we do in the forums so people will be able to experience it.



LEO:  Forums it makes a lot of sense, yeah.



STEVE:  Makes a lot of sense.



LEO:  Yeah.  And I would love to see a big name company like Twitter or Facebook or Google implement this just as a secondary way of logging in.  And that's what you're going to need, I think, is somebody like that saying okay.  I can see enterprise jumping on it.



STEVE:  Well, yeah.  And because it's free...



LEO:  That won't be public facing.



STEVE:  ...it's not difficult to implement.  And, well, and that's the other cool thing is imagine that enterprises, some enterprises did.  Then their employees would have created a SQRL identity, and they'd have a SQRL client on their various laptops and computers, and maybe on their smartphones.  And they would be able to use that for their enterprise.  But then if they happened to go to a website that offered SQRL, there's nothing more they need to do.  They literally click "Sign in with SQRL," and they're done.



So, and it's just - it's so right when you start using it.  I mean, and of the people who have experienced it, they're like, oh, this has to happen.  We have to make this happen.  What can we do to make this happen?  And it's like, I know.  We just, you know, one step at a time.  It had to exist first.  Finally it now exists.  And it took long because it's really nailed.  I mean, it is, as I've said before, and as I say when I talk about it, I have an answer, an answer exists for every possible "but what if."  But what if this?  But what if that?  It doesn't matter what you ask that we have an answer for it.



And in fact there is a "what if" page at the SQRL forums, at sqrl.grc.com, that is that.  I compiled a list.  I solicited "what if" questions from everybody in the forum.  If you scroll through that, there is no "what if" that wasn't asked, and there's an answer for every possible thing that could go wrong.  So anyway, obviously I'm a little excited, but this was a big milestone.  So it is there at grc.com/sqrl, or you can do a .htm, /sqrl.htm.  Again, you'll find the downloadable 17-page PDF and a link to the client.  Which is, I think, 278K.  It's multilingual.



LEO:  Wow.



STEVE:  I know.  It's so big, I hated it, Leo, because there were some...



LEO:  You know hard drives come in gigabytes now, terabytes; right?  You know that; right?



STEVE:  There were some crypto libraries that just didn't make sense for me to write from scratch, so I thought, okay, fine, I'll just add this big blob to my code.  So it would have been a lot smaller, if not.  But there were some places it did not make sense for me to do from scratch.  So, yeah, it is cute and tiny.  It checks in for updates and auto updates itself.  In fact, it auto installs.  There's no separate installer.  You just run it, and it goes, oh, I'm not here yet.  Would you like me to stay?  And you say, oh, yes.  And anyway, I think our listeners will get a kick out of it.  So it's there.  It exists.  And onward.



Again, my next piece of work - I know everybody's waiting for SpinRite.  I've just got to get the - basically the specs are written, scattered across some old web pages, and some that I have been keeping updated for the other developers who've been writing pieces.  I just need to pull it all together.  So that's the next phase is to finish this document that I have started with the full, over-the-wire implementation spec.  And then it's time for SpinRite.  So we're getting there.



LEO:  Nice.  Congratulations.



STEVE:  Thank you.  SandboxEscaper dropped another zero-day.  And the good news is we don't have to worry about this one because it's kind of a crock.  It's a second bypass for a problem that Microsoft kind of already patched last April, a couple months ago.  Microsoft described the problem they were fixing as:  "An elevation of privilege vulnerability exists when Windows AppX Deployment Service improperly handles hard links.  An attacker who successfully exploited this vulnerability could run processes in an elevated context.  An attacker could then install programs; view, change, or delete data."



Then Microsoft said:  "To exploit this vulnerability, an attacker would first have to log onto the system," meaning they would have to be local.  "An attacker could then run a specially crafted application that could exploit the vulnerability and take control of an affected system."  So, you know, it's a privilege elevation flaw.  You have to be there.  You've got to be able to run something.  But when you do, you can break through Windows' what appear to be relatively soft barriers.  And it's unfortunate that that's the case.  In fact, she indicated that she's just finding these LPEs, these Local Privilege Escalations, everywhere.



So what we have is another - there was one way to bypass it that she produced a couple weeks ago, and now she has another.  But what I'm beginning to think, based on the way this looks and from the quality of her previous work compared to this, this feels like, as I mentioned, sort of a crude hack that she stumbled upon while working towards a different exploit.  So I'm not going to go into great detail because I don't think it matters.  It's kind of a funky way of obtaining this elevated privilege that involves deleting a subdirectory of Microsoft's Edge browser files, then launching Edge twice.  The first launch, without these files present, results in Edge crashing.  So it's, again, inelegant.  Then the second launch, if it's done programmatically by clicking Edge's quick launch icon down in the start bar or the tray, causes a mistake in setting of Windows Access Control, which could then theoretically be exploited.



So, you know, it's really a mess.  She's in her typically grumbly mood, as she so often is.  And my guess is that she couldn't sell this.  The guys at 0patch, you know, the micropatch guys, told Threatpost, who had some coverage of this bug, that it wasn't even critical enough to warrant one of their little micropatches, and that they had been unable to reliably reproduce it.  They wrote:  "We know of no one being successful at it," and they said, parens, "(it could just be really difficult to reproduce, or depending on some external factors that were not present in our testing environment)."



So this happened late last week.  And of course today is Patch Tuesday for June.  This is the second Tuesday of June.  So certainly I haven't looked at what Microsoft did for today, but I would be very surprised if they had time or probably even sufficient concern, really, to look into this one and fix it.  And her proof of concept code has once again been removed from GitHub.  If you go to SandboxEscaper's account over on GitHub, it's wiped.  I mean, it's got, like, submissions and downloads and blah blah blah blah, all the various - there's like six or seven different categories.  I looked because I was curious yesterday, and there's just nothing there because Microsoft keeps cleaning it up.  Just like, you know, of course they took over GitHub.  So they just keep - she posts things.  I don't know why she posts them there.  But they just get wiped clean.  So there's nothing there at the moment.



Anyway, it feels to me like this is such a crappy hack, and she is so good, I mean, I've complimented her in the past.  Last year when we saw some of these earlier ones, they were like, whoa.  This is a gifted hacker who is producing these things.  And given what we have learned from her recently, that there is, you know, she mentions wanting to be able to sell these for $60,000.  That's the number that she keeps using.



My guess is that we're not seeing her best work; that her best work, presuming that she is continuing to produce at that quality, is being sold to people with deep pockets and who have a need for high-quality working Windows exploits.  And so what we're seeing are kind of debris that doesn't really make the grade, that she's unable to monetize, so she dumps them on GitHub just to watch Microsoft come along and sweep them up and scrape them off.



Anyway, and on the second page of the show notes I have a posting of hers from 5:19 this morning.  She says:  "This is the second bypass" - or I guess it was, like, not this morning.  It was late last week.  "This is the second bypass for CVE-2019-0841.  I won't ever be part of the infosec community, people have made that clear to me a long time ago, in many different ways.  I'm just a harmless crazy person now, sharing harmless LPEs.  Whatever.  Bye.  P.S.:  I have one more zero-day."  Then she says:  "*growls and wanders off into the arctic*."  So there's an update on our SandboxEscaper.



As for updates, we're still watching BlueKeep.  There was a newly published, fully working proof of concept exploit developed as a new Metasploit module for BlueKeep.  And, now, I did say that it was fully published, except that I also know that he said he plans to keep the module private.  So I guess it is an existent Metasploit module which has been demonstrated.  It was developed by Zerosum.  And unfortunately I don't have his actual handle in the show notes because the "e" and "r" of Zero spelled out, Z-E-R-O, they're actually upside down in his handle.  And I didn't want to bother worrying about how to do whatever character-set Unicode was necessary to faithfully reproduce his handle.



But anyway, he is a well-known, respected reverse engineer.  He's the guy who did the first non-destructive, that is to say, non-crashing probe for the BlueKeep update.  And that's the code that Robert Graham at Errata Security then based his scan, his RDPScan on, in order to scan the whole 'Net that we talked about last week and the week before.



So now what we have, this new thing, which is it exists, but it is not - it's still private.  It demonstrates how an unauthenticated attacker can achieve full access to a victim machine in about 22 seconds.  What's different about this is it bypasses the one mitigation that Microsoft and the FBI and the NSA and everybody else has been saying would protect you, which is this NLA, this Network Level Authentication.  He worked around that.



What he did was he combined a different Windows credential harvesting tool that we talked about a long time ago, Mimikatz, M-I-M-I-K-A-T-Z.  Mimikatz was created a long time ago, in the early days of NTLM, Microsoft's own NT LAN Manager protocol, to demonstrate some of its security problems.  And it's continued to evolve through the years, keeping up with LANMAN as necessary in order to track the changes that Microsoft has made.  So by pairing the zero authentication RDP exploit with Mimikatz, he's ended up with an extra potent tool that he is not releasing because he feels it is too potent.



He tweeted:  "Still too dangerous to release, I'm sorry.  Maybe after first mega worm."  Which is to say, after it sort of no longer becomes an issue because all the RDP servers on the planet have been compromised.  Then he may let it go.  Anyway, so he knows what he's doing.  He has advanced the state of the art in this BlueKeep vulnerability.  And so far we are still not seeing what people are talking about.  And as I said last week, to me I don't think it's going to be a worm.  I mean, maybe at this point it will be just because why not; and because everyone's saying that there will be one, someone will create one just to say, okay, yeah, I did one.



But last Tuesday, a week ago, while we were doing Podcast 717, the NSA got into the BlueKeep act, publishing their own security advisory.  They said, from Fort Meade, dated June 4th, they said:  "The National Security Agency is urging Microsoft Windows administrators and users to ensure they are using a patched and updated system in the face of growing threats.  Recent warnings by Microsoft stressed the importance of installing patches to address a protocol vulnerability in older versions of Windows.  Microsoft has warned that this flaw is potentially wormable, meaning it could spread without user interaction across the Internet.  We have seen devastating computer worms inflict damage on unpatched systems with wide-ranging impact and are seeking to motivate increased protections against this flaw.



"This is the type of vulnerability that malicious cyber actors frequently exploit through the use of software code that specifically targets the vulnerability.  For example, the vulnerability could be exploited to conduct denial of service attacks.  It is likely only a matter of time before remote exploitation code is widely available for this vulnerability."  That I completely agree with.  They said:  "NSA is concerned that malicious cyber actors will use the vulnerability in ransomware and exploit kits containing other known exploits, increasing capabilities against other unpatched systems."



And again, I think that is what we're going to see.  And since this is Patch Tuesday, note that it was last Patch Tuesday that we learned about this.  This is when all of the systems going back to even Windows XP, the older systems, got themselves patched, and Microsoft said, "Oh, by the way, there's something really bad that we just fixed.  Everybody please update."



And as we know, a couple weeks went by.  Robert Graham scanned the Internet, I mean, and really, really, really scanned it, looked at all things that answered on all the various ports that might be typically running RDP, weeded out those that weren't actually RDP.  Of those that were RDP, then further weeded out the ones that were actually vulnerable, and that's where we got the 950,000 confirmed vulnerabilities that aren't going to go away by themselves, no matter how many pleas from people are being made.  These systems are just unattended.  



I did see some tweet, actually, in the last week, from someone saying that, yeah, we're not happy about it, but we cannot take down the servers that we have, even to fix something this bad.  And I'm thinking, what?



LEO:  Oh, then I'll take them down for you.



STEVE:  What are you smoking?  Yeah, exactly, Leo.  And I guess this guy didn't know about the 0patch, the micropatch, because that micropatch fixes any server that's vulnerable without taking it down.  So somebody, if anybody is hearing this and saw that tweet, I saw it go by, and I didn't have a chance at that time to catch it.  But that guy needs to be told, and anybody else, you do not have to bring - you don't have to down a server and update it to fix this.  You can apply the micropatch from 0patch.com in order to fix it.  So do so.



Oh, and an update from Robert Graham.  RDPScan, his tool, is now in its fourth release.  And so you can find it.  I have the link in the show notes.  But on GitHub.com it's /robertdavidgraham, G-R-A-H-A-M.  That'll take you to his page where you can then find RDPScan. And under the releases there are now four of them.  He's just been fixing a couple little minor bugs that have cropped up over time.  And there is a downloadable Windows binary which is super useful for scanning intranets to make sure that an internal vulnerability is not leveraged.



LEO:  Stevie?



STEVE:  So there is a new botnet in town, which itself is not news.  What's interesting is that this one has decided - and the researchers, the guys that have been tracking it, are not quite sure what's going on.  It's in the wild.  It is spreading.  It is actively searching the Internet for either patched or unpatched RDP servers.  Though as I mentioned, it's known as GoldBrute because it's written in Java, and the Java class is named GoldBrute.  Who knows why, except that it is a brute forcing bot.  Its being written in Java means that it needs to drag an 80MB Java runtime along with it.



LEO:  Oh, geez.  Aren't you glad you didn't write SQRL in Java?



STEVE:  Not exactly like, yeah, like my 287K SQRL client that is loaded with text and is multilingual and so forth.  Anyway - or multilingual-able.  Our listeners will remember that I found a site that would allow us to handle translation.  I'm going to hold off on that a bit till we see if there is actually a demand for it in other languages.  And, if so, then we will pursue that.  All of the foundation is there for that.  But anyway, this GoldBrute is, yes, maybe that's why it's called GoldBrute, because it is a brute, needing an 80MB Java runtime in order to run itself.



Anyway, here's what we know.  We know that a search using Shodan turns up that original 2.4 million IP addresses reflecting machines that are reachable over the web that have some version of Remote Desktop Protocol enabled.  If it's present, if RDP is visible, then we know that about a third of them, because 950,000, about a third of those will be older versions that you don't even have to guess the credentials because the problem that Microsoft fixed last Patch Tuesday was the ability to just jump right onboard without any credentials.  The other two thirds are newer, or patched, and so you do need to provide a logon username and password.  You need to provide credentials.  Thus you would need to brute force them.



Okay.  So apparently the GoldBrute botnet is not discriminating.  But as I mentioned, it does not currently avail itself of the RDP flaw.  It's using a pure username and password guessing in attempts to break into all of these 2.4 million machines.  And as we know from previous reporting, for whatever reason, many of these machines will fall to persistent brute force credential stuffing attacks.  We've seen this before.  There are lots of servers out there that have malware on them, presumably because somebody was able to guess right and install something.



So GoldBrute's scanning, its own scanning, has turned up 1.5 million RDP-serving machines.  The code being loaded does not reveal the final purpose of the hacked Remote Desktop servers, which is to say it appears to only be - it only exists for the sake of its own existence.  There is no persistence mechanism, meaning it doesn't modify the file system in any way.  Simply rebooting the machine which has been infected by GoldBrute will remove all traces of it.  So it exists and it lives entirely in RAM.  So one theory is that the botnet is being used simply to compile a credential list of available RDP servers which might then be offered for sale on the Dark Web sort of as an access for service sort of thing.



The researcher who's been following this says there's only one command-and-control server, and he knows this both by watching it work in a honeypot, and also by the fact that it's Java.  They've been able to do some decompilation and see what's going on inside.  So there's one IP, which is 104.156.249.231.  That IP is where all these bots are phoning home to, which is a location in New Jersey.  And I was curious, so I did a little bit of spelunking, and that IP is under the control of a cloud services provider Vultr, Vultr.com.  Www.vultr.com takes you to their home page, and they're just sort of a - they're just some cloud service provider.  And so I guess somebody is renting some space at that IP address and has all of these GoldBrute bots connecting there.



What's there is a port 8333 service which these bots used to connect an encrypted web socket connection to.  8333 is commonly used for bitcoin connections, but this isn't a bitcoin transaction, so that's a little strange, except maybe it's considered now a well-known port, so the person thought, well, I'll just hide my command-and-control server behind port 8333 so I look like a mining pool or something.  So it's like, okay.



So what happens is the individual bots scan the 'Net at random, find RDP answering servers, and they report them as found, that is, when found, back to the central C2 server.  And so it's that server which has accumulated a total of 1.5 million of these reports from the bots.  And then, after reporting the addresses of 80 of these potentially victimizable RDP servers, the command-and-control server then chooses a number of targets which that bot should brute force.  So the new bot finds 80 new targets, phones those home, and then from the command-and-control server receives username-and-password pairs for it to attempt to use in logging into that site.



So standing back sort of from a distance, what this looks like is the central command-and-control server is using its botnet to coordinate a diffuse scanning operation, and also a diffuse brute force attacking operation, doling out usernames and passwords and IP addresses and saying, okay, give this a try.  And then when it says, no, that didn't work, it's like, okay, give this one a try.  Okay, that didn't work.  But also try a different IP.



So what this does is it distributes the attack so that, from the viewpoint of the server under attack, it's getting different attempts from all over the Internet so that, if it had logic to blacklist an IP that was pounding on it for doing a credential stuffing attack, then the fact that they're coming in from all over the Internet would prevent IP address blacklisting.  So, I mean, that's the only thing that really makes sense based on what we're seeing.  And once a successful credential attack has succeeded, then the remote RDP server then has the bitcoin - I'm sorry, I got myself confused because it's actually downloading a jar file named bitcoin.dll - I guess, again, to sort of pretend to be like something going on with bitcoin mining, although it's actually a Java jar file that is handed to the Java runtime in order to execute.



So it zips.  It has the Java class and runtime zipped up.  It transfers that to the newly infected target, which then unzips it, installs the runtime, and then runs this bitcoin.dll in order to bring up another instance in RAM of a new bot which then checks in with the mothership.  It checks in.  It begins scanning, tries to find new things to attack.  And after it finds 80, it gets some new machines to probe.  So an interesting sort of odd take at a botnet, but one which is apparently becoming successful and doesn't take advantage of the RDP exploit.



So the researcher wrote that:  "After six hours," they wrote, "we received 2.1 million IP addresses from the C2 server, of which 1.596," so actually almost 1.6 million, "are unique."  And he said:  "Of course, we didn't execute the brute force phase."  So anyway, so they're running a honeypot.  They've seen what thing is doing.  And we have sort of an interesting botnet whose mission is to work on brute forcing RDP servers that it's able to find.



And I mentioned this interesting letter, or I guess public service announcement, from the FBI.  The FBI has reminded us not to place trust in a website just because it's secure.  I have the link.  It's ic3.gov.  And they said:  "Cyber Actors Exploit 'Secure' Websites in Phishing Campaigns."  So this is the FBI sort of talking to the general public, saying:  "Websites with addresses that start with HTTPS are supposed to provide privacy and security to visitors.  After all, the 'S' stands for 'Secure' in HTTPS:  Hypertext Transfer Protocol Secure.  In fact, cybersecurity training has focused on encouraging people to look for the lock icon that appears in the web browser address bar on these secure sites."  When is the last time there was a lock icon on a URL, Leo?



LEO:  No, they still...



STEVE:  Oh, it is?



LEO:  Yeah, there's a little padlock.  And good cybersecurity training would encourage people to click on that lock, but I'm sure that's what the FBI's going to...



STEVE:  They said:  "The presence of HTTPS and the lock icon are supposed to indicate the web traffic is encrypted and that visitors can share data safely.  Unfortunately, cybercriminals are banking on the public's trust of HTTPS and the lock icon. They are more frequently incorporating website certificates," and then the FBI writes, "third-party verification that a site is secure, when they send potential victims emails that imitate trustworthy companies or email contacts.  These phishing schemes are used to acquire sensitive logins or other information by luring them to a malicious website that looks secure."



And so the FBI had their boilerplate recommendations.  "The following steps can help reduce the likelihood of falling victim to HTTPS phishing.  Do not simply trust the name on an email.  Question the intent of the email content.  Second, if you receive a suspicious email with a link from a known contact, confirm the email is legitimate by calling or emailing the contact.  Do not reply directly to a suspicious email.  Three, check for misspellings or wrong domains within a link, for example, if a domain should end in .gov, but ends in .com instead."  Of course, that's sort of an FBI-oriented perspective.  And they said:  "Do not trust a website just because it has a lock icon or HTTPS in the browser address bar."



So of course none of this is news to us.  We know that the only assurance actually being provided by HTTPS and TLS is that our connection to the web server is encrypted, and the server provided a certificate matching at least some of the domain name shown in our browser.  In other words, barely anything of any consequence any longer.  But the FBI, as I've mentioned, I think must be sending this out because they have their finger on the pulse of the public more than we might, or more than I might.  As I said, Leo, I think you probably do from people you talk to on your weekend show all the time.



LEO:  Yeah.  I say this on the radio constantly.  I don't even mention that you can't trust HTTPS.  I say click the lock.  Click the padlock.  You've got to see who owns that certificate.



STEVE:  Right.  Right.  And so what I found interesting about this and wanted to share with our listeners is that it's interesting that the public understanding of things like this probably lags about a decade behind where we are.



LEO:  Yeah.  Yeah, probably.



STEVE:  You know, 10 years ago, before Let's Encrypt, before automated certificate issuance and the promiscuous use of wildcard certificates, you know, asterisk dot anything, having a certificate kind of meant something.  But it's true that, during the past five years especially, its meaning has become quite watered down, which is an unfortunate consequence of the push for encrypting everything.  After all, if you do encrypt everything, and then you make it easy for the bad guys to even encrypt their things, then yeah.  You've got encryption, but now it doesn't mean anything special anymore.



LEO:  I just checked Chrome and Firefox.  Both still have padlocks.



STEVE:  Oh, okay.



LEO:  Next to the site address, which...



STEVE:  Oh, yeah.  And even Firefox.



LEO:  Yeah, you remember that.



STEVE:  Yes, I see the little green padlock for me.



LEO:  Yeah, and actually I don't know if Edge does.  I'll have to check.  But I just train my audience, I say, "Click that padlock link and make sure the site that you think you're on matches your certificate."  Otherwise - and I even tell them, it's not just hackers.  It could be your boss, could be your business, could be your Internet service provider.  There's a man in the middle, and you don't want a man in the middle of your secure transaction.



STEVE:  Right.



LEO:  Yeah.  I think people get that.  But clicking that padlock is something I'm trying to train my audience into doing.



STEVE:  I think that's good.  And of course 10 years ago...



LEO:  You didn't have to.



STEVE:  Back when certificates meant more, security experts were all jumping up and down, extolling the virtues of the unbroken key or the closed padlock.



LEO:  Right, right.  That's why people think this; right?



STEVE:  Exactly.



LEO:  Yeah.  Key's not broken, it must be good.



STEVE:  Exactly.  So, yeah, it's secure, so I can trust it.  It's like, no, unfortunately, it no longer means what it used to.



LEO:  Yeah.



STEVE:  Which is too bad.  This is sort of a cool piece of consequence.  I mentioned at the top of the show VLC, the VideoLAN media player, which is very popular.  I have it installed on my various machines because it's a very good media player.  It received 33 security bug fixes, two of which are rated high severity.  So first off, if you are also a VLC user, just run it.  I ran it this morning, and it immediately popped up with a notice that it had an update and was fixing some severe - it was shown in red - security flaws.  They said, the notice said:  "VLC 3.0.7 is an important security update to VLC 3.0 branch, improving HDR, 10- and 12-bit rendering, and Blu-ray support, in addition to numerous security" - and that's where the word was in red - "issues fixed."



Back at the beginning of the year we mentioned that the EU had started sponsoring bug bounties in the hopes of improving the security of popular open source projects which their institutions were using and relying upon.  And, it turns out, it appears to be working.  The president of VideoLAN, Jean-Baptiste Kempf, said:  "This high number of security issues is due to the sponsoring of a bug bounty program funded by the European Commission during the Free and Open Source Software Audit (FOSSA) program."



So that's very, very cool.  The EU produced a bounty, said we're offering prize money for security problems found in VLC.  And as a consequence, this broke a record.  There have never been 33 problems found, including two high severity ones.  So these two big problems and this release of 3.0.7 occurred last Friday.  One of the two problems was an out-of-bound write vulnerability; the other was a stack buffer overflow bug.  Developers behind the software said that the patches were two of the 33 being pushed out for the media player.



So this is not super crucial.  The only likely threat would be from a targeted attack against an individual or an organization that was known by the attacker to be using VLC.  So somehow an attacker would have to get someone to use a vulnerable version.  And again, the moment it pops up now, it displays this notice.  So I guess if you ignored the notice and hadn't already updated, then it could get you.  But again, it's very unlikely.



Basically, a specially crafted malformed media file would have to be played by the susceptible version of the player and then, well, and then it would execute the attacker's code in your machine, probably in the context of the logged-in user.  That is, I don't think you would get, depending upon where the codec was - and it's not clear to me that it would be running in the kernel.  They are supporting GPUs, though.



So if there were drivers that were vulnerable, and NVIDIA did just update their drivers to fix some high-security vulnerabilities, it's conceivable that they could get elevated privilege.  But this is, again, this is why user-level privilege escalation is a problem is that it does come in very handy in order to make attacks substantially more powerful, although even where the attack itself doesn't give someone significant attack posture.



Also, Microsoft's Edge browser has taken another step forward.  The Microsoft Edge development builds, the ones that you can download in advance if you're interested, now allows, now supports a feature that we knew has been coming, which is the ability to essentially relaunch a page in IE11 mode.  You would first go to a site and, presumably, it wouldn't work.  Or maybe there will be a way eventually for the site to say, "I need IE11 mode."



Anyway, at this point it's under the menu.  You open the Edge menu, go to More Tools, and then under More Tools, as of this latest Edge development build, there is an option, "Show this page in Internet Explorer," which will re-render that site in the old IE11 code, which is presumably useful for legacy sites maybe being used internally in corporations that have code that isn't under Edge and doesn't make sense for them to move to Edge.  This, of course, makes it possible for them not to have to do that.



So we have that now at the development level.  I'm sure, once Edge is officially released, it'll just be there.  And maybe they'll have some way of flagging these sites as needing to come up under IE11 right from the get-go, or some way of causing that to happen.  For now, you can just ask the site to be re-rendered that way.



And Mozilla has been reorganizing things.  They've officially changed their logo to sort of an updated Firefox logo.  So Mozilla is sort of more - they're kind of cozying up to Firefox because of course that's mostly how they're known anyway.  There's a new Firefox logo; a new Lockwise logo.  Lockwise was originally named Lockbox, and that's a password management service, which is free, which Mozilla is offering to allow Firefox users to synchronize their saved browser login credentials among iOS, Android, and desktop versions of Firefox.  So that's now been named Lockwise and has a new logo.



They have their Monitor service, which we discussed back in September of last year when it was announced, which is a service that is being done in connection with Troy Hunt's HaveIBeenPwned service.  Basically, you submit your email address to Monitor, and then they will periodically poll HaveIBeenPwned to see whether your email address appears in any data breaches and proactively notify you if that has happened.  And that's what's different about what Troy is offering.



And, by the way, Troy was in a London security conference last week and announced that HaveIBeenPwned is outgrowing his one-man-size shop.  And so I'm not sure what that means, but it looks like it's increasing in popularity, and so we may be seeing some changes coming there, as well.  And the fourth service is Send, Firefox Send.  It also got its logo updated with this new look and style.  And so all of this gets wrapped around services that Mozilla is offering under the Firefox moniker.  So anyway, just some nice news there.



There was another piece of news that caused me to go find something which has been scrolling and sort of distracting me a little bit, actually, during the podcast because it's so cool.



LEO:  Is that what I've been hearing in the background every once in a while?



STEVE:  No, actually that was my email, and I finally closed that.  I had a couple pieces of email come in.  It's actually a little more interesting than email.  It is showing every DNS query that my system is making in the background.  And our listeners know why that's interesting because anybody who's looked at their data is just like, what the heck is going on?  I mean, our systems are noisy now.  There's just a lot of stuff happening.  And anyway, so in the news was that Mark Russinovich, our friend over at Microsoft who's now the CTO of Azure at Microsoft - and he of course used to be at Sysinternals.  And everyone worried when Microsoft bought Sysinternals.  The good news was they didn't do bad things to it.  They kept it there, and all the tools are still free, and they've continued to update them.



Well, there's one tool, Sysmon, which logs things into the Windows Event Log.  The news is that it's getting a new feature, which is the ability to log DNS queries into the system's event log.  Well, okay.  First of all, I don't think that's useful.  For one thing, there's a lot of DNS queries that a system makes.  And the event log is not a very useful place for them to go.  I mean, it's not very accessible.  But I remembered something that I remembered seeing from a prolific coder who codes in my style.  You probably know the site NirSoft, Leo.



LEO:  Oh, yes, of course, yeah.



STEVE:  Yes.  N-I-R-S-O-F-T.



LEO:  It's been around for decades.



STEVE:  Yes.  Well, since early 2000s, actually.  So a decade.  Or, no, you're right, almost two.  So the guy's name is Nir Sofer.  N-I-R is his first name.  Sofer, S-O-F-E-R, is his second name.



LEO:  He's so prolific.  This guy has so many tools.



STEVE:  Yes.  And they are lightweight, tiny little things.  So this thing is DNS Query Sniffer.  And I recommend it for Windows users.  It is just - first of all, it's one exe.  I was a little jealous.  It's a little bit smaller than my SQRL client.  Of course it does actually much less.  But it is neat.  It simply shows you a running list of DNS queries that your system is making.



And there are a couple things you want to do, which I poked around at.  There is a Resize Columns.  You want to tell it to do that.  But the most important one is Auto Scroll on New Line.  That's a checkbox under the Options menu.  And that way it does what its name sounds like.  It continually auto scrolls.  Ah.  And I just saw sqrl.ver.grc.com.  That was my installed SQRL client doing a once hourly DNS query to see if there's a new version.  I did a very, very lightweight check for version.  I used DNS in kind of a cool way.  And I just saw it, just caught that happening.



Anyway, tiles.services.mozilla.com.  I got some akamaiedge.net stuff happening, mozilla.com, safebrowsing.googleapis.com.  There's some wallstreetjournal.net.  Maybe I have an old tab open in Firefox, or maybe - anyway, it's just very cool.  The idea is you sort of want to have an idea of why anything you see there is happening, very much like what happened as we were talking about at the top of the show which drove me to create OptOut, where ZoneAlarm popped up an alert for a program I didn't know was installed.  Well, this would have worked.  This would have done the same thing.  Basically, as we know, you could hard code an IP address which would prevent making a DNS query.  But nobody does that.  So this is just a super lightweight way of getting a sense for what's going on on your network, thanks to Nir Sofer at NirSoft.com.



LEO:  Dot net.



STEVE:  Thank you.  NirSoft.net.  And so it's called DNS Query Sniffer.  And it showed all the various LAN adapters I had installed.  It showed a bunch because I'm a user of VMware.  So I saw the virtual LAN adapters that VMware workstation installs.  I also have, of course, OpenVPN installed, so I saw the TAP adapter that OpenVPN installs and a couple others that are just on my motherboard.  So I chose the one that was the IP address of my machine to my router, and I'm watching stuff happen.  So it's very cool.  And I commend its use to our listeners.



Speaking of commending, the movie whose trailer you do not want to see, but which I otherwise wholeheartedly recommend, is titled "I Am Mother."



LEO:  I think I saw the trailer for that just by accident.



STEVE:  It is great.



LEO:  It was a very short trailer, so maybe it didn't have all the spoilers in it.  I think I saw an ad, a network ad for it.



STEVE:  Maybe.  Anyway, it just came out on Netflix on Friday.  Lorrie and I watched it Saturday and really liked it.  So again, I commend it without reservation.  It's two hours long.  You've got to pay attention.  There was one review that I really liked.  Someone created a review, or created an account on IMDB, specifically because he had got a few things that it was easy to miss.  I mean, so I should just explain, it's post-apocalyptic sci-fi, and well done.  It got awards at Sundance.  What's her name, the one-name star?  Oh, Rose Byrne voices the robot, does a great job of that.  Hilary Swank is the most recognizable star in the film.  But anyway, not super expensive.  Good special effects.  I just, you know, we talk about sci-fi here.  We know I'm a fan of sci-fi.  So for anybody else who is, "I Am Mother" on Netflix.  Really nice...  



LEO:  Good.  I'll be watching it tonight.  That's great.



STEVE:  Really nice two hours.



LEO:  Yeah, that's good.



STEVE:  So after tweeting about the release of SQRL, I have three tweet responses.  Klaus Pinhack wrote:  "Nice work," smiley face.  "Setup of app and ID took me about 15 minutes.  No problem for me, but too long for my neighbor."  And again, this relates back to what I was saying is that I've thought about this a lot.  There are things I could have done, there are things SQRL could be which would have removed features from it, which would have then made it susceptible to "what if" attacks, meaning like, okay, what if I'm crossing a border, and the border agent takes my phone and maybe forces me to give them my fingerprint and unlocks it so I no longer feel like I can trust the security of my identity.  Okay.  I'd rather have an answer to that.



Of course, I actually got a call from a good friend of mine who was once a Microsoft high-end, high high high-end developer, whose Google account got hacked, which allowed access to all of the usernames and passwords that he had been storing in Chrome, which was synchronizing through Google to any instance of Chrome that got installed.  So he had a 100% loss, a complete compromise of all of his usernames and passwords.  This happened a couple days ago.  So the point is I wanted an answer for every possible thing that could happen.



So as a consequence of that, you do have to do a few things upfront to establish that kind of security, that kind of beachhead.  And so I have something that cannot be any simpler than it is to also offer the features it does.  My feeling is, once it's built into iOS, once it's built into Firefox and Chrome, once it's built into Windows, then it will be much less of a big deal.  So it is in some ways a demonstration of the fact that it is possible to solve every problem.  And, at this point, it's sort of all up to the user.  And I'll be super interested to hear what the listeners to this podcast think.



Again, it's there now.  Everybody can get it.  So it'll be interesting to see what people think.  But, you know, I understand.  I completely get it that, yeah, it's not as simple as having really bad security.  But anything less than this is really bad security, I mean, that things can happen to you that there's no recovery from.  This allows you to recover from anything that can happen.  And it's way more secure.  For example, as I said before, SQRL gives websites no secrets to keep.  The things SQRL gives a website is a public key.  They can publish it.  It doesn't matter.  So when they get breached, nothing happens.  You don't have to change anything.  You don't have to change your password there, and so forth.  And anyway, yes, I'm excited.



David Eckard wrote:  "Downloaded the Android app.  Setup less than fall-off-the-log easy."  He says:  "Ripe for a demo on YouTube for setup."  And I hope people will make some.  Please do.  "Among other things, I learned that I need to be able to actually type the password on the client."  And again, remember that that's so that someone doesn't take your phone and use your SQRL client, impersonating you to your SQRL client.  But if your phone supports any kind of biometrics, then that's all supported, too.  So when I present SQRL, I just let the phone look at me and I'm logged in, which makes gasps from the audience like, oh, my god.  It's like, yes, and this works anywhere, or will, as soon as other sites start supporting it. 



And, finally, Yosef N. Berger.  He said:  "@SGgrc I just started messing around with SQRL, and I noticed there is a setting, Set Password Verify Time.  I didn't see it mentioned in the FAQ and after a cursory watch of the forum patient - of the forum patient see any mention."  I'm not sure what he meant there.  "Could you go into what it does?  Does it have any bearing on security?"



And so I'll just mention, our listeners will understand, that with SQRL - and actually it is explained in this 17-page explainer that's now public - you are able to say, "I want my password to take five seconds to brute force."  And the idea is you only need to enter it once per session.  And then after you can use just the first four characters, or the first N characters.  You could make it just one character if you wanted because the idea is you're saying "I am still here," rather than "This is who I am."  And you are able to set that time, if you don't want it to be five, if you think five seconds seems too long, you can turn it down to one second.



But what it would mean is that, if somebody were brute forcing your password, it would probably take them one second, or that is to say, one fifth as long if you changed it from five seconds to one second per guess of your password.  So there's a modest security cost.  On the other hand, it is very difficult to brute force and accelerate that one second because this is deliberately GPU and ASIC hardened.  It requires 60MB per decryption, which no ASIC or GPU has per core.  So it's not like running an SHA-256 50,000 times, which hardware has gotten very fast at.  It is actively fighting against being accelerated.  So there's a lot of brute force protection built in.



And, finally, I had a note to us about, tangentially, sort of about SpinRite.  This guy said - this is Craig Clarke, who's the Client Engagement Officer, said:  "Hi, Steve and Leo.  Long-time listener of Security Now! since Episode 1, in" - okay, I don't know how to pronounce this place in Australia.



LEO:  Adelaide.



STEVE:  Adelaide, thank you.  That's the way it looked.  I was sure I was going to mess it up.



LEO:  He's the Client Engagement Officer at the Australian Taxation Office, just to be clear.



STEVE:  Ah.  He says:  "I have purchased SpinRite and have used it to restore many drives, floppy disks from my deceased father-in-law, and an iPod with a spinning hard drive."  He says:  "I'm very much looking forward to the next update to SpinRite."  He says:  "Just a reminder to change SQRL pages from being under your website's Research tab to its own tab for ease of finding."



LEO:  Oh, good point.



STEVE:  Yeah, and I haven't done that.  I forgot that.



LEO:  Promote it.  It's not research anymore.  It's real.



STEVE:  That's right, "...and indicate that the research has been concluded."



LEO:  Woohoo.



STEVE:  He says:  "I'm sure that traffic to your website will soon spike to a very high level.  Maybe you will need CacheFly."



LEO:  Good.



STEVE:  "Please use personal email address for correspondence."



LEO:  I know someone there.  I can help you with that.  That's awesome.  Thank you, Craig.  Congratulations, Steve.  That's really great.



STEVE:  Well, we're getting there.



LEO:  Yup.  Steve, Exim, E-X-I-M.  Talk about this.



STEVE:  So according to a recent survey of all mail servers visible on the Internet, 57%, that is, 507,389, are running Exim.  And after seeing what Qualys found, I'm very glad I don't run Exim.  I'm Windows-based rather than Unix or Linux based, and Exim is running on the most popular OS for the Internet, which of course is Unix or Linux.  And if everyone listens no further, if you have in any way or if you are in any way connected to one of those more than half a million email servers present on the public Internet, you really should update to the latest release of Exim immediately.



The vulnerability, as I mentioned at the beginning of the show, was accidentally, but fortuitously, patched with the release of Exim version 4.92 back in February.  It was four months ago, not four weeks ago.  So back on February 10th, so almost exactly four months ago.  It was accidental because the Exim team had no idea that they were fixing a major security hole.  Qualys was just doing a review of the code, and in doing the code review they found a big problem.



Now, okay.  The news hit last Wednesday, that is, Qualys went public with this.  They sent a note to the Debian group because Debian's email server is by default Exim.  So the news of this breach - and this was a full disclosure.  So all the bad guys know exactly how to pull this off.  And that's what I'm going to explain here in a minute.  But what's interesting is that, due to the weird nature of this, in its default configuration it takes seven days to cause a vulnerable remote email server to execute commands under the control of the attacker.  So if upon the release of this news last Wednesday someone began to attack your email server immediately, they're still waiting until tomorrow to be able to execute the commands that they've set up.



LEO:  That's so weird.  That's kind of unprecedented.  I don't...



STEVE:  I know.  It really is.  It's because of an expiration in an unsendable piece of email.



LEO:  Ah.



STEVE:  It defaults to a week.



LEO:  Okay, yeah.



STEVE:  And so what happens...



LEO:  It keeps trying for seven days, yeah.



STEVE:  Yeah.  So what happens is you actually - you keep the connection up, and you send it a byte every four minutes because there's a different expiration after five minutes.



LEO:  Oh, how funny.



STEVE:  And by sending it a byte every four minutes, you keep it waiting, and you do that for seven days.  And then a different expiration kicks in which, due to this weird code path that they found, causes the - basically you put the commands you want to have it run as the email address before the @ sign on the domain.  And what happens is the way this thing trips over its own feet is that, after a week of your patiently sending it a byte every four minutes, it will finally give up.  And it ends up running this email address through a function which will accept the E-X-E-C, the exec function, and take the rest of the parameters in the address as commands at root, commands with root privilege.



So again, this is one of those problems that, because it was introduced back in - I've got it in my show notes here.  I don't see it in front of me right now.  I'll get to it.  But it was introduced in 2016, early in 2016, so most of 2016, all of 2017, all of 2018, and up until the beginning of 2019.  But the point is probably any system that came up then that has not been updated in the last four months would have had one of the vulnerable versions of Exim.  And of all of the more than a million Internet email servers, 57% of them that are answering on port 25, SMTP, and whatever other ports may be susceptible, there are, we know 507,389 of those are vulnerable.  So this seems bad.



And it also seems the idea that an attacker with some patience, you've got to have a little patience, you've got to wait a week, could then execute the commands of their choice as root on an email server.  I mean, you know, these are going to be email servers of big targets, potentially.  You know, IBM and Fortune 500 companies are going to be running Unix servers with Exim on it.  And let's hope that they've been keeping them current, that is to say that they have updated, in the last four months, a server that went online sometime in the previous three years.  Because, if not, I wouldn't be surprised if they got some commands being run on them as root.



So let's see.  In my notes I have, yeah, Qualys wrote to the Linux distro maintainers that the vulnerability is "trivially exploitable" and expects attackers to come up with exploit code in the coming days.  Exim 4, the affected version, is currently the default MTA, the Mail Transfer Agent, on Debian Linux systems.  A large number of Exim installations exist, especially within ISPs and universities in the U.K.  Exim is also widely used within the GNU Mailman mailing list manager, and cPanel.



Wikipedia notes that "Exim's security has had a number of serious security problems diagnosed over the years.  Since the redesigned version 4 was released, there have been four remote code execution flaws and one conceptual flaw concerning how much trust it is appropriate to place in the runtime user.  The latter was fixed in a security lockdown in revision 4.73, one of the very rare occasions when Exim has broken backwards compatibility with working configurations."  And of course now we have another biggie.



Qualys put out a security advisory, calling this "The Return of the Wizard:  RCE in Exim," so remote code execution.  And for anyone who's interested, this is CVE-2019-10149.  And I pretty much covered this in summary, so I'm just going to try to find things that are important things that I didn't.



They wrote:  "During a code review of the latest changes in the Exim mail server, we discovered an RCE vulnerability in versions 4.87 to 4.91 inclusive."  And I tried to look to see whether the HELO message divulges the version of Exim.  Sometimes the HELO message, or there's also a version of it, EHLO, on more advanced, more recent versions, sometimes says, you know, Exim version something.  Which of course the bad guys would love to have because then immediately on answering any connection, TCP connection over port 25, the server would be basically waving a flag saying "Please hack me.  I'm vulnerable."  I don't know whether Exim does, so that would be one thing to find out.  But of course, even if not, chances are very good that it would be vulnerable.



They said:  "In this particular case, RCE means Remote Command Execution, not Remote Code Execution.  An attacker can execute arbitrary commands with exec as root."  No memory corruption, no return-oriented programming, nothing fancy, no buffer overflows and so forth is required.  They said:  "This vulnerability is exploitable instantly by a local attacker," so that's also worth noting, "instantly by a local attacker," they said, "and by a remote attacker in certain non-default configurations," that is, instantly exploitable, "by a remote attacker in certain non-default configurations.  To remotely exploit this vulnerability in the default configuration, an attacker must keep a connection to the vulnerable server open for seven days by transmitting one byte every few minutes.  However, because of the extreme complexity of Exim's code, we cannot guarantee," they wrote, "that this exploitation method is unique.  Faster methods may exist."



They said:  "Exim is vulnerable by default since version 4.87 released on April 6, 2016," they said, and then they have some code here, "when #ifdef EXPERIMENTAL_EVENT became #ifndef DISABLE_EVENT, and other versions may also be vulnerable if EXPERIMENTAL_EVENT was enabled manually."  They said:  "Surprisingly, this vulnerability was fixed in version 4.92 released on February 10, 2019."  So anyway, I have a link in the show notes at this point to their comment; also a note at bugs.exim.org talking about this.  I won't go into this in any much greater detail.  In the show notes I have, for anyone who's interested, all of the details about what they go through.



Down at the very end of this they explain about default configurations, non-default configurations, local exploit and so forth.  But under default configuration, which of course is the thing of most concern, I'll summarize this a little bit.  They say:  "We connect to the vulnerable Exim server and send a mail that cannot be delivered because we send more than" - then there's a setting, received_headers_max.  They said:  "We send more than that many received headers in the email envelope."  So that causes a delivery fault.



They say the recipient address, as in the RCPT TO, of our mail is "postmaster," and its sender address, that is, the MAIL FROM is - and here's where the exploit is.  It's ${run{, then the commands they want to have executed as root, then close both curly braces, then @ sign, and then the domain where that's the domain that is under their control.  So basically, so it's the MAIL FROM.  So what happens is, as I mentioned before, it's when this vulnerable server finally tries to send back a bounce message that it encounters this, basically an executable account name at the destination domain.  And, unfortunately, it executes the commands which are contained in the account name.



So then they said, step two:  "Because our mail cannot be delivered, Exim connects to [the target domain's MX]," they say, "where we listen for and accept this back connection, and starts sending a bounce message" to that $run blah blah blah target.  They say:  "We keep this connection," that is, its attempt to send a bounce message, "open for seven days," basically preventing it from successfully sending the bounce message for a week.  And they say seven days, the default timeout_frozen_after setting in the service, "by sending a byte to Exim every four minutes."



They said:  "This works because Exim reads the response to its SMTP commands into a 4096-byte buffer," which is the DELIVER_BUFFER_SIZE, "with a five-minute timeout."  So, that is, they use four to be conservative, to slip underneath the five-minute timeout, and that is reset every time a byte is read.  So that just keeps getting reset to another five minutes every time a byte comes in.  They just keep it trickling in.



And, finally, after seven days, step four:  "After seven days we complete our lengthy SMTP response with a permanent delivery failure."  For example, they return a "550 unrouteable address" response to Exim's attempt to deliver the delivery failure message.  This freezes the bounce in a function known as post process one.  "This function should actually discard the bounce instead of freezing it, which would prevent us from reaching the vulnerable code because it is older at that point than two days old, which is the default ignore bounce errors after timeout."  But this doesn't happen.  "In this particular case, the message age is not the bounce's real age, over seven days, but its age when it was first loaded from Exim's spool when it was just a few seconds or minutes old."  So there's a mistake in the code path there.



And, finally, step five:  "Exim's next queue run, every 30 minutes by default on Debian, loads the frozen bounce from the spool, sets process recipients to RECIP_FAIL_TIMEOUT (this time the message age is the bounce's real age over seven days), and executes the vulnerable code and our commands," which is in the original sender address, which is interpreted by "expand string," that will invoke exec as root.  Oh, and they said:  "Note:  To quickly test this remote exploitation method, the days in Exim's default timeout_frozen_after and ignore_bounce_errors_after can be replaced by hours, and the default retry rule can be edited."  So they're able to speed this whole thing up, basically in order to run this in the lab and see if this all works.



So what we have is we have what they described as trivial, I mean, yes, it requires a little doing.  But it's going to be too tasty for hackers not to play with, the idea that - it's kind of cool almost that you send your commands to an email server, you accept its attempt to bounce it back, you hang the attempt for a week, and then it executes the command you gave it as root.  It's going to happen.  So unfortunately, it could happen to somewhere on the order of half a million, that is, more than half of the email servers currently accepting email on the Internet.



So for all of our listeners, you don't want you or your corporation to be among those.  All you have to do, since if the attack began when this first came to light last Wednesday, you still have a day before a week has gone by if somebody started to attack your server immediately.  So sounds like a good idea to update to the latest version of Exim, and you'll be okay.



LEO:  Yeah.  Did you see the story - it broke this morning, so you probably didn't have time to get it into the show - about the Customs and Border Protection losing the face recognition information to thousands of people who've crossed the border from Canada?  Did you see that story?



STEVE:  I did not see that story.



LEO:  One more reason to be really concerned about the use of face recognition in kind of non-safe ways.  I mean, Apple's face recognition lives on the iPhone and stays on the iPhone.  But the Customs and Border Protection says photos of travelers into and out of the country were accessed.  And you're going to love how they were accessed.  A subcontractor downloaded them, and they were later stolen from the subcontractor in a malicious attack.  In violation of CPB's policies they downloaded - yeah, some policy.  "Hey, don't do that.  That would be bad."



But apparently they didn't protect it.  They transferred copies of license plate images and traveler face images collected by the Customs and Border Protection to the company network.  It's fewer than 100,000 people - which means it's 99,000, right? - who had gone through a few lanes at a single land border over a period of a month and a half.  It's not from airlines.  It's not passport or other travel documents.  But it underscores how dangerous this is, to give an image of your face to anybody, especially to the government.



STEVE:  Yes.



LEO:  Yeah.  Can't change your face or fingerprint very easily.  That's the problem.  We've talked about that at Disneyland.  They don't do it anymore, but they used to collect fingerprints.  Somebody used his elbow, remember that, to get into Disneyland?



STEVE:  Right, yeah.  We used to talk about using your knuckle instead.



LEO:  Use a knuckle.



STEVE:  Just give it your knuckle.



LEO:  Steve, always a pleasure.  I know what I'm watching on TV tonight.  I can't wait.



STEVE:  Oh, it's a really nice two hours.  I think you will...



LEO:  And pay attention, it sounds like, because there's a lot of...



STEVE:  Pay attention.  



LEO:  There's twists.  It's complicated, yeah.



STEVE:  Yes.  And unfortunately I don't think we can talk about it next week, you know, because I don't want to spoil it for anybody.



LEO:  No, no, no.  Yeah.



STEVE:  But we'll have to talk about it in a couple weeks because it was - a couple comments that were made at the very end are, like, oh.  They're really good, yeah.



LEO:  "Billions" had a big twist last night.  I don't know if you watched it yet.



STEVE:  Oh, do you mean the season finale?



LEO:  Yeah.



STEVE:  Yeah, I love that show.



LEO:  They've now found their niche, which is - because they did it once before, but they didn't do it - they haven't done it for all the seasons, which is you set people up for the whole season, and then in the last episode everything you thought you knew is wrong.



STEVE:  Yeah.



LEO:  Which I love.  Big payoffs are always fun.  Steve, there's another thing we have to set up, which is a time for you to come up here.



STEVE:  Yes.  Yes, yes, yes.



LEO:  Let's start exchanging emails because we want to do that.  And you tell us who else you want to have on the show.  We should find out when Father Robert's coming to town.  I understand - this is sad news.  He was supposed to go to DEF CON and Black Hat.  And that higher authority that he works for has...



STEVE:  Oh, wait, how high?  



LEO:  Pretty high.  All the way up.  All the way up has nixed that trip.  But we do know that he'll be coming back to town at some point.  So I don't know if we want to wait that long.  I'll tell you what.  I'll contact Robert.



STEVE:  What really works is having an audience.  And so I was thinking, I wanted to propose to you that we have...



LEO:  Oh, do it somewhere.



STEVE:  Well, like if there's anywhere we could set up a whole bunch of chairs, and I do a presentation to as big an audience as we can fit in the studio.



LEO:  We can get about 30 or 40 people in our studio there.  I think we could - that'd be enough; right?



STEVE:  Yeah, yeah, just so there's sort of some crowd dynamic, and I can take questions.  Because what really works is for me just sort of jumping around up in front of a screen showing pictures and describing the whole thing.



LEO:  Oh.



STEVE:  And then having people say, well, wait, what about this and what about that?  So I would imagine...



LEO:  That would be really fun.  Maybe have to book a hall for that.  That sounds like something we'd like to do, and maybe down in San Francisco so we could get the SQRL show on the road.



STEVE:  As long as we have your professional photography.  That's really the only thing that is missing.  And I'll bring the popcorn.



LEO:  I inadvertently just took delivery from Amazon of a five gallon bag of caramel corn, which of course I can't eat because I'm keto.



STEVE:  Ooh, ooh.



LEO:  I meant to send it to my mom, but I'm going to bring it to the office and let these guys eat it.



STEVE:  Well, you've got a lot of young bucks there, so...



LEO:  By the way, thank you for the keto suggestion.  People, he's got a lot of great information about keto on his website.  And I've been doing it now for three months, lost 15 pounds, my blood sugar is normal again.



STEVE:  Congratulations.



LEO:  It's really been a great thing.  My blood pressure is normal again.  Yeah, I'm very happy.  So you had said it all along.  But what turned the corner for me is doing it under a doctor's supervision with accountability.



STEVE:  Yeah, yeah, nice, nice.



LEO:  Because I measure my vitals every day, and so that's important.



Steve does this show every Tuesday.  As you can tell, it's kind of a time for me and Steve to get together.  But we're glad you listen, too.  It's about 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  You can watch live or listen live at TWiT.tv/live.  If you do that, join us in the chatroom at irc.twit.tv.  I would love to have you download it, too.  I mean, you can get your 16Kb version if you're bandwidth impaired, a 64Kb version if you're not, and very nicely curated transcripts at Steve's site, GRC.com.  He's got it all there, including show notes.



By the way, if you're there, you might want to pick up a copy of SpinRite, world's best hard drive recovery and maintenance utility.  Ever hear of that?  Be very handy for you to have, I can promise you that.  There's lot of other great stuff on Steve's site, too:  GRC.com.  He's @SGgrc on Twitter.  That's where you can leave him a direct message if you have a question, a comment, or a suggestion.  Or you can go to GRC.com/feedback, get the same thing done.



We have audio and video at our website, TWiT.tv/sn for Security Now!.  And of course you can always subscribe.  That's probably the best thing to do.  Get yourself a podcast application and just subscribe to Security Now! so you'll get it automatically, every day, the minute it's available.  Steve, thank you.  Have a wonderful evening.  See you next week.



STEVE:  My friend, I can't wait.  Bye.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#719

DATE:		June 18, 2019

TITLE:		Exim Under Siege

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-719.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  There were several significant stories this week.  We have a new DRAM problem called "RAMBleed," news of a Linux server kernel-crashing flaw in TCP, and the occurrence of the expected attacks on Exim email servers - not to mention last week's Patch Tuesday, a Bluetooth surprise, and another useless warning about the BlueKeep vulnerability.  Microsoft missed a 90-day Tavis Ormandy deadline.  We have a good-news GandCrab wrap-up, Yubico's entropy mistake, a bit of post-announce SQRL news, and a favorite iOS security app.  We selected as our title story the attacks on Exim mail servers so that we can talk about the other disasters, which are still pending, next week!



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  RAMBleed, the latest Rowhammer attack, is on the docket along with SACK, a new problem that's been in Linux for years.  And then we'll look at that Exim server flaw he was talking about last week and how it's starting to spread.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 719, recorded June 18th, 2019:  Exim Under Siege.



It's time for Security Now!, the show where we cover your security online with the guy right here, Mr. Steve Gibson of GRC.com, our amanuensis.  No, I'm the amanuensis.  You're the man in charge.  Hi, Steve.



STEVE GIBSON:  Hi, Mom.  I mean, Leo.  Hi, Leo.



LEO:  Is Mom watching from high above?  From heaven?



STEVE:  I wonder what kind of reception you get up there.  I don't know.



LEO:  Well, it should be crystal clear.



STEVE:  Okay.



LEO:  It would be the ultimate in disappointments if you're in heaven and the TV reception stinks.



STEVE:  Wouldn't that be?  Or you only had like the major network...



LEO:  Four channels?



STEVE:  Yeah, like in the old days.



LEO:  God's too cheap to buy cable, yeah, yeah.



STEVE:  And then about 2:00 a.m. this American flag came up, and the jets fly overhead, and then it, like, shuts down until - I don't know why they did that because they had to come back on at, like, 6:00 a.m.; right?  So it's like only...



LEO:  Yeah, what are they doing?  Are the dusting the transmitters?  Why do they shut down?



STEVE:  Probably some FCC regulation or something where they had to...



LEO:  I mean, isn't it worse?  It's harder on the transmitters to shut them all down and turn them back up.



STEVE:  Yeah.  Oh, I'm sure they couldn't turn those things off.  Back then you had tubes; and you had to, like, baby the tubes.



LEO:  You'd have to bring up the plate voltage first.



STEVE:  Do you remember when the TV would go on the fritz?  You and your dad would unplug the TV.  And then, like, hopefully you'd wait a while because the CRT could function as a big capacitor.  And then you'd take the back off, and it had a - what did we used to call it where, if you took the back off, the power cord came with it so that it couldn't be plugged in with the back on.  There was a name for that.  I don't remember.  Anyway, and then there was like all these tubes.



LEO:  A dead man switch.



STEVE:  And so you'd reach in - it was that kind of thing, but there was a specific name.  Anyway, and you'd reach in, and you'd kind of gently, kind of like in a rotating motion, pull each of the tubes out, and you'd carefully put them into a bag, and then you'd go down to the drugstore, where...



LEO:  Oh, yeah.  They had a tube tester.



STEVE:  ...where the tube tester was.



LEO:  I forgot about all that.



STEVE:  Yup.  And you'd stand up on a stool because at that time you were too short...



LEO:  You were short.



STEVE:  ...to reach.  And your dad would hand you the six NVT.  And so then you'd turn the dial to look up the six NVT.  And then there was a whole array of switches.  You just had to slide all the switches into the right positions.  And then you'd plug the tube in, and you'd wait, and you'd see it begin to start glowing.  Then you could kind of see the red coming up through the plate.  And then you'd press the button to test it.  And this meter that had, like, red and then yellow and then green, like the needle would go hopefully into the green.  But if you were having a problem, you were looking for it.  So you'd hope to, like, find it.  Or you'd put the tube in, and you'd wait, and you'd wait, and it was like, hmm.  And when you waited long enough, you'd look closer, and there would be no red glow.  And it's like, the heater burned out, and so...



LEO:  We found the bad tube.



STEVE:  You'd found the bad tube.



LEO:  Oh, my god.  See, this is a little Father's Day memory.  I can see how you got to be a geek.  Your dad did this with you, huh?



STEVE:  He encouraged it.  Like I was tugging on him.  I said, "Come on, Daddy.  Let's go, let's go."  I mean, so he was fulfilling my urge.



LEO:  So awesome.



STEVE:  I don't know where it came from, but, you know.



LEO:  So awesome.



STEVE:  I definitely had some...



LEO:  Well, now we know how this all happened.



STEVE:  He was an enabler, an early enabler of this podcast.



LEO:  Yeah.  Well, that's awesome. 



STEVE:  So we have today an extra, triple-scoop, spiffy good podcast.



LEO:  Another word from Dad, "spiffy."



STEVE:  Spiffy, yes.



LEO:  I don't think anybody's said that in this century yet, so you're the first.  That's good.



STEVE:  That and "tinkle."  Those are...



LEO:  Another one.



STEVE:  That's another one.  So, not surprisingly, this podcast is titled "Exim Under Siege."



LEO:  Uh-oh.



STEVE:  And I thought the word "siege" is the right one because remember from our discussion of, well, last week's podcast was titled "Update Exim Now!."  And we both got a kick out of the fact that it takes a week of holding open a connection and trickling a byte every, what was it, four minutes because the connection's going to time out after five.  But you have to just kind of keep it waiting for more.  And so it takes a week.  So we'll get there.  That'll be the - we'll wrap the podcast up by talking about it.



And I had a hard time deciding what I wanted to talk about because there was so much that's going on.  We have another Heartbleed-ish - or, well, no, actually it's not Heartbleed, sorry - a Rowhammer-ish problem with DRAM, which has come out from the same team again.  And so that was competing with a number of other things that have happened.  But I thought, okay, we really ought to follow up on last week's discussion because we now have a worm very aggressively propagating through Exim email servers.  And as we would expect, its ultimate goal is to mine cryptocurrency because that's what things do now.  It's all about the money.  And if you're not going to be able to extort people and get them to pay you to decrypt their files, then let's just mine some coin.



So lots to talk about.  We've got, as I mentioned, a new DRAM problem, RAMBleed, which is sort of a cross between Heartbleed and Rowhammer, RAMBleed.  We've got a bad Linux server kernel-crashing flaw which, I mean, it's so new, the news just hit yesterday, that it's going to take a little bit of time, a few days, for the bad guys to arrange to craft some packets.  This is another reason why raw sockets are a mixed blessing because that's what they'll use.  And it's going to be able to bring any Linux server in the last 10 years that has not been updated in the last two days, well, it's going to give it a kernel panic and halt it.  So we'll be talking about that next week.  And that's why I thought, okay, let's talk about the Exim siege this week because we'll have a new disaster for next week.



So we've also got Patch Tuesday was last Tuesday, and some news there, including a Bluetooth surprise for a number of Windows 8 and 10 users.  Another useless warning about the BlueKeep vulnerability because, if anybody doesn't know by now, DHS chiming in, that's not going to help.  Microsoft missed a 90-day Tavis Ormandy deadline - whoops! - by, well, we're not sure if it was one day or more, but it was...



LEO:  You don't want to miss those.  Oy.



STEVE:  They missed it, and there is now a new problem.  Doesn't look that bad.  Tavis considers it important, but I'm sure we'll see it fixed next month.  We've got some good news in the GandCrab wrap-up.  We have a mistake in entropy that Yubico found and is in the process of completely flushing out of the  install base of FIPS-compatible Yubi devices.  We've got, oh, a little bit of post-announce SQRL news, a favorite iOS security app that I've been wanting to talk about, and I was prompted to by a tweet that I saw.  And then we're going to talk about what has happened since the Update Exim Now! podcast.  It's been a fateful week.  And of course I do have a fun Picture of the Week.  So lots to talk about.



LEO:  Not to add on, but Facebook announced that they're going to do a cryptocurrency.



STEVE:  What?



LEO:  Which is, yeah, this is a big story because that's 2.5 billion potential users.



STEVE:  Wait, wha-wha-wha-wha - okay.  Okay.  Go ahead, sorry.



LEO:  It's based on a Swiss foundation and consortium called the Libra Network.  It will be the Libra coin.  And I at some point would love for you to look at what they're doing to make it better than or different than bitcoin.  I thought initially it would be pegged, but apparently it won't be pegged on any fiat currency.  They have an interesting proof of stake blockchain that is new.  And there are a number of people, including many in the crypto community, who say this could be a watershed for cryptocurrency.  I know you have some interest in cryptocurrency.



STEVE:  Well, yeah.  It's academic.



LEO:  Yeah.  Besides that erased hard drive.



STEVE:  Ooh.



LEO:  Not to rub it in.



STEVE:  Yeah, thank you. 



LEO:  But it will be a stable cryptocurrency.  That's their hope, anyway.



STEVE:  And so they would be a gateway for moving U.S. dollars into and out of this coin.



LEO:  Not just U.S. dollars.  They're going to use euros, yen, and pounds, as well as U.S. dollars.  It'll be backed by actual assets in those four fiat currencies.



STEVE:  So there'll be an exchange for those.



LEO:  Yeah.  And of course they'll build it into all of their tools, so transactions through WhatsApp, Instagram, and Facebook will probably use - I'm calling them "Zuckbucks," but I think they want to call them "Libra coin."



STEVE:  That's hard to resist, though; you know?



LEO:  It is, it is.



STEVE:  Once you hear it, you can't forget it.



LEO:  Yeah.  But, you know, you did this great, really in-depth piece some years ago on bitcoin and the...



STEVE:  We explained the blockchain, the whole concept.



LEO:  Yeah, the mathematics of it.  This is kind of an up-to-date, two-token system that I don't fully understand, but I'd love to get your thoughts on.  So at some point...



STEVE:  So it exists independently, and there already are Libra coin...



LEO:  No, no.  Well, that's an interesting question.  Facebook says not till 2020.  But they're making this announcement now.  "Libra:  Stable cryptocurrency backed by a basket of financial assets which runs on its own blockchain."  That's in their press release.  So...



STEVE:  I'll know more about it next week.



LEO:  Yeah, it's backed by a not-for-profit organization out of Switzerland called the Libra Association.  I mean, it's fascinating.  And of course they're trying to make it governmentally approved.  But I can imagine a government might see this as an assault.  Certainly the banks will.  And they've got MasterCard, Visa, and PayPal as part of the consortium.



STEVE:  No kidding.  So this is clearly the largest legitimate entity to ever back a cryptocurrency.



LEO:  Yeah.



STEVE:  And of course what are the Winkle Twinkles, now in San Diego...



LEO:  The Winklevi have all - they put their money into bitcoin. 



STEVE:  Right, right.



LEO:  But there's clearly been problems with bitcoin.  And so these are of course to try to address those, particularly the volatility of bitcoin, which...



STEVE:  Well, it suffers from not having enough accelerator resistance in its fundamental algorithm, which allowed ASICs and, well, it just distorted the market.  For example, Monero worked so well because it is hostile to acceleration.  And that tends to just level the playing field more.



LEO:  Yeah.  And of course banking is nervous about it because  it could reshape the payment industry.  Governments might be nervous about it as the...



STEVE:  Well, it's uncontrollable.



LEO:  Yeah, the undollarization of the world, introducing a new unit of account for global trade, a shift in monetary economics from public to private sectors.  It'll be...



STEVE:  I'm surprised they're not calling it Facecoin.  I would think they...



LEO:  Yeah, I think Zuckbucks, honestly.  But I think they're also at great pains not to imply that it's just Facebook behind  this; right?



STEVE:  Right, right, right, right.  Did you have a chance to see the movie that we talked about last week?



LEO:  Not yet.  I told Lisa, I said we have to see this.



STEVE:  Okay, yeah.



LEO:  Before Steve spoils it.



STEVE:  I had a number of people, well, a lot of our listeners wrote...



LEO:  Did they like it?



STEVE:  Loved it.



LEO:  Oh, I can't wait.



STEVE:  In fact, some had already seen it, and they tweeted that the moment they heard me start to mention a new sci-fi movie on Netflix, they knew exactly what it was I was going to talk about.



LEO:  What's the name again, so people can...



STEVE:  "I Am Mother," for those who...



LEO:  "I Am Mother."



STEVE:  Our Picture of the Week, I snapped this off of the web page.  It's something I failed to mention last week.  I just thought our listeners, some of our listeners certainly would get a kick out of it.  One of the early participants over in the web forum took it upon himself - oh, and it came up because of PopSockets.  We were talking about those.  And he had done a SQRL logo on a PopSocket.  And I said, on a what socket?



LEO:  I have it right here.  Right?



STEVE:  Yeah.  And anyway, so we also have T-shirts.  Not we.  I play no part in this.



LEO:  Who's doing this?



STEVE:  It's someone who - I did work with him.  I ordered a couple beta shirts to get the logo the right size and in the right position.  But anyway, they are SQRL T-shirts.



LEO:  Nice.



STEVE:  And they're very nice.  I like a sheer T-shirt because I tend to run my engine a little hot.  And so if you just go to Amazon and google - there's no links anywhere.  I ought to catch up and do that.  I've got a whole lot of catch-up to do.



LEO:  Yeah.  I went to your SQRL page.  You need to, you know, get that merchandise going there, dude.



STEVE:  So if you just go to Amazon and google "SQRL T-shirt" you'll find them in whatever size, and there's a whole variety of colors, and anyway.  So I just thought it would be fun to do that for the Picture of the Week this week, this very busy week.



LEO:  Actually, you've got to be careful because you could get a T-shirt with a squirrel on it.



STEVE:  Yeah, not that one, no.



LEO:  It's not the SQRL you're looking for.



STEVE:  Yes.  Do look at the shirt before you click "buy now."



LEO:  Yeah, SQRL, yeah.  Here we go.  Here we go.  Secure Quick Reliable Login.



STEVE:  There we go, yup.



LEO:  Nice.  Very nice.  All right.



STEVE:  And there's different styles.  There's a hoodie.



LEO:  Is anybody making money on this?  Do you get a little...



STEVE:  Long sleeve.  I think he gets like a small little "ving."  Not much.



LEO:  Because they're pretty affordable.  That's close to the cost, yeah.



STEVE:  Yeah.  Yeah, they're about what you'd expect to pay for a non-SQRL printed T-shirt.  And the back is really nice.  It's got a big SQRL logo.



LEO:  Oh, look at that.



STEVE:  And mine says "Simply Secure."  And we played around with different slogans.



LEO:  I like this one.  "Passwords?  Where we're going we don't need passwords, Marty."  I love it.  I love it.  That's fun.  Okay.



STEVE:  Back to the SQRL.  I mean...



LEO:  Yes, back to the SQRL.  I like that, too, yeah.



STEVE:  So, okay.  Last week, as we know, was Patch Tuesday for this month.  Microsoft, well, Microsoft's and Adobe's Patch Tuesday.  Adobe had another critical vulnerability, once again fixed, in Flash Player, which Microsoft also promulgated for their own OS.  Microsoft fixed 88 vulnerabilities.  Once upon a time, Leo, we would have gone, "Oh, my god!"  But now it's like, oh, okay.  Round number, 88.



LEO:  It's lucky, two eights.  Lucky.



STEVE:  That's right.  More than one quarter of those, 21 in total,  were critical.  Among the remaining 67 noncritical vulnerabilities fixed were the four deemed important which our friend - and I don't really mean that literally - SandboxEscaper found and, as we know, irresponsibly publicly released, as we've previously discussed in some detail.  All four of those were various elevation of privilege hacks.  And last we heard, she still had one more zero-day up her sleeve, which she was promising or threatening to disclose.  So nothing since that disclosure from her.



Of the critical vulnerabilities, 21 in total, eight were in Microsoft's JavaScript Chakra scripting engine, five others in their Edge browser scripting engine that I guess is distinct from Chakra.  Chakra is JavaScript, so I'm not sure, you know, in their enumeration they just said "scripting engine."  But it was browser scripting engine.  All 13 of those scripting problems, both the eight in Chakra and the five that weren't, were memory corruption.  And again, those are critical.  Microsoft considered those critical because, as we know, memory corruption is where remote code execution vulnerabilities are born.  So they need to be taken seriously.



And speaking of remote code execution vulnerabilities, there were three of those fixed in Hyper-V, one in Microsoft's Speech API, another in ActiveX Data Objects, and also that critical Adobe Flash security update.  And then among the remaining 67 important vulnerabilities was pretty much everybody was present and accounted for.  As I said, the scripting engine, IE, Edge, the app platform and frameworks, Windows input and composition, media, shell, server, authentication, cryptography, datacenter networking, storage and file systems, SQL components, Microsoft's JET database engine, Windows virtualization, the kernel, and IIS.



I didn't mention Office.  I guess there's nothing from Office.  Maybe that was done separately, or maybe that's next week.  Anyway.  So another mega Patch Tuesday, which as I said we're now beginning to get kind of used to.  And we'll be keeping an eye out for SandboxEscaper, if she has something a little more substantial to drop than that last one that was just, you know, not really very annoying.



But June also brought us - Leo, you're going to love this one - an update to Windows 10 Bluetooth stack.  And after applying this month's latest security update, as Microsoft puts it, "...the affected platforms will experience the new behavior."  Okay, now, Leo, you'll remember many, many moons ago when Intel published crude conceptual sample source code for what was new at the time Universal Plug and Play functionality.  And though they never intended for this to happen, because it was just sample code, many router vendors copied and pasted that engineering source code sample right into their routers and compiled it.  And the result was a surprisingly widespread vulnerability across router brands since everyone was using something, the same something, that was never intended to actually be put into production.  And I remember we talked about it at the time.  It was like, what?  No.  This was clearly marked "Sample, do not use."  But, oh, look, it works.  Ship it.



So, okay.  Something similar has just happened.  The Bluetooth Low Energy specification provides a sample long-term key, the LTK, which was provided in the specification, the BLE specification, only for illustration purposes, and of course was never intended to actually be used in practice.  Somehow it found its way into Android's Bluetooth.  And as a consequence, Android from 7.0 through 9 built in the long-term key, the same one, the one same long-term key from the Bluetooth Low Energy spec.  And as a consequence, it's not secure.  It's been identified as a security vulnerability, CVE-2019-2102.



Mitre.org says:  "In the Bluetooth Low Energy (BLE) specification, there is a provided example Long Term Key (LTK).  If a BLE device were to use this as a hardcoded LTK, it is theoretically possible for a proximate" - meaning someone close by - "a proximate attacker to remotely inject keystrokes on a paired Android host due to improperly used crypto.  User interaction is not needed for exploitation."  And then it says:  "Product:  Android.  Versions:  7.0, Android 7.1.1, Android 7.1.2, Android 8.0, 8.1, and 9."



Microsoft published their announcement 4507623, saying:  "Some Bluetooth devices may fail to pair or connect after applying June."  And so they said:  "You may experience issues pairing, connecting or using certain Bluetooth devices after installing security updates released June 11, 2019.  These security updates address a security vulnerability by intentionally preventing connections from Windows to unsecure Bluetooth devices.  Any device using well-known keys to encrypt connections may be affected, including certain security fobs."  So in other words...



LEO:  Oh, this is the Titan story come back.



STEVE:  Yes.  Basically Microsoft blacklisted the key which was given as a sample in the spec, which...



LEO:  As it should be blacklisted; right?



STEVE:  Of course, absolutely.



LEO:  Yes.



STEVE:  In fact, I was thinking, you know, the industry should take this as a lesson.  Anyone who's implementing the spec should preemptively blacklist any sample keys that the spec shows, just so that they never work.  The problem is they have worked, and they've worked since Android 7, when they started being used.  And if Windows had always blacklisted the key in the spec, if it had occurred to somebody, then they would have never been used because they would have never worked.  But instead they have always worked until someone said, you know, this is probably not a good idea.



LEO:  So I was wondering if this had to do with - remember Google had a problem with its Titan, Bluetooth LE Titan security key.  I wonder if this is the same issue.



STEVE:  Oh, no.  This is a different issue.



LEO:  This is a different issue, okay.



STEVE:  Yeah, yeah, yeah.  And so this does affect Android devices that will no longer connect to Windows.  And frankly, they should really no longer connect to anything.  Basically, you're using a known long-term key.



LEO:  It's like using "password" for the password.



STEVE:  It's, yeah, it's like every web server in the world used the same certificate.  It's like, uh, no.  It's like, just back away from the terminal.  So anyway, hopefully this mitigation will be made more pervasive, not just Windows 10.  I mean, here's Microsoft that did the right thing, and people are complaining that they can no longer connect their Android device to Windows after applying the June update.  It's like, yes, you should never have been able to connect your Android device to anything with this key from the specification.  So anyway, they fixed it, and this will hopefully force some change through the Android ecosystem, slow as molasses as it is.  Although, to Google's credit, we know they're working in the direction of speeding that up.



Okay.  Now, this will be the topic of next week's podcast.



LEO:  Oh, boy.  There's so much.  So much.



STEVE:  Yes.  Hasn't happened yet because it just hit yesterday.  We were recently talking, Leo, I think it was maybe week before last, about the seemingly nutty idea of China rolling their own Internet-connected desktop OS from scratch.



LEO:  Right.



STEVE:  Which, you know, they should do.  I mean, a homogeneous ecosystem is dangerous because a flaw found in one place affects everybody.  You want a heterogeneous ecosystem.  So it would be great if China had something different.  But I just, you know, again, we were talking about how, well, the example I pulled out of why this is so difficult is how many flaws were found in the early TCP/IP protocol stacks.  All kinds of little things you could do to lock up connections and cause problems and get up to all kinds of mischief.  And so over decades we've, like, finally got a TCP/IP protocol stack that is solid.  Except we don't.  We just found a flaw in Linux's TCP/IP stack which dates back to 2.6.29, released 10 years ago.



LEO:  Ooh.



STEVE:  Every Linux machine now accepting TCP connections on the Internet can be kernel panicked.  It can be halted remotely.



LEO:  Ooh.



STEVE:  Now, tell me that is not going to be next week's topic.



LEO:  Yeah.



STEVE:  Okay.  So there are three CVEs.  It's a combination of something known as a SACK, and of course the Register just went wild with their...



LEO:  Oh, I bet they did.



STEVE:  Having fun with, like, "giving it the sack" and all kinds of things.  Anyway.  Back in 2011 we recorded a series of three podcasts, carefully describing the low-level operation of TCP.  Security Now! 317, which we recorded on September 8th, 2011, was titled "TCP Part 1."  Episode 323 was recorded on October 19th.  That was "TCP Part 2:  Attacking TCP."  And then two weeks later Episode 325, recorded November 2nd, 2011, was "TCP Part 3:  Necessary Refinements."  I mention that because I know that a lot of our listeners have been listening for a year or so, and those were really good.  And, I mean, so if you don't feel like you understand what a SYN packet is, what an ACK packet is, if you go back and listen to those - and back then they were probably shorter.  So we're not talking two-hour podcasts.  They were, like, half an hour.



LEO:  Well, I don't think they were that short.  But okay.



STEVE:  Maybe, yeah.  Anyway, you'll get a really good primer on TCP.  Okay.  So the original ACK packet, as I described it on those podcasts back in 2011, the idea was...



LEO:  Episode 317.



STEVE:  Yes, Episode 317.  The idea was that, when you initiate a TCP...



LEO:  By the way, it's an hour and 30 minutes, so...



STEVE:  Oh, wow, we had already expanded.



LEO:  Yeah, we'd already, yeah, expanded.  That's a good word.



STEVE:  That's right.  We were still doing alternating Q&A episodes.  I can't imagine doing that now because there's just so much news every week.  And that's why they're a little bit spaced apart.  And then some other catastrophe happened between 317 and 323, so we had to cover that.  But then we got back to wrapping up the series, or continuing.  Anyway, the SYN packet - SYN stands for synchronize.  That numbers the first byte that is going to be sent by each endpoint.  And as they come in, the receiving side acknowledges the receipt of the last, the number, the sequentially numbered last byte received.  So it sends back ACKs.  That's the way TCP works.



In RFC 2018, which was dated October 1996, 23 years ago, so not exactly new, this concept of ACKing was improved.  The problem was you might have many packets in flight, especially as speed improves on the Internet.  And if one packet was dropped, but a bunch of succeeding packets made it, or the way the original spec was written, the recipient could only acknowledge the latest consecutive sequential byte received.  That is, even if they had received other later packets, they couldn't ACK those.  And so that was recognized as a problem.



So SACK, standing for Selective Acknowledgement, expanded on the protocol to allow an acknowledgment packet containing a list that was able to enumerate those received and those not received so that, rather than the sending end having to send the packet not received and then resend subsequent packets that had already been received, which would obviously be redundant and would waste bandwidth, this allowed the recipient to specify which packets had not been received, separate from those that had.  So that's Selective Acknowledgment, which we didn't talk about at the time because we were trying to keep things simple.



Well, it turns out that, if you combine a mistake in the code which has been in every Linux kernel for the last decade with another feature of TCP which is MSS, the Maximum Segment Size, the idea there is that the sender is able to specify when they're setting up their connection that there's something wrong at their end, they've got no RAM, so only send me no more than this many bytes at a time.  These packets are known as "segments" in TCP parlance.  And so the idea is you say, okay, I don't have much buffer space.  You can't send me 64K blocks.  You've got to make it a K or something.  Well, it turns out that the smallest size is 48.  When you subtract the overhead of 40 bytes for the packet, that leaves 8 bytes.



A security engineer at Netflix, of all places, found this problem and has reported it.  If you combine in what is arguably a hack of a very small MSS, Maximum Segment Size, with screwing around with selective acknowledgment - and Red Hat has a beautiful, complete expos on this.  I've got a link in the show notes.  And here's the problem is that this is not going to be difficult to reproduce.  This is not going to be difficult for the bad guys to do on any OS that allows raw sockets.  Raw sockets allows you to build your own socket from scratch.  It takes control away from the TCP/IP stack.  And you can put anything you want to out on the wire.



I mean, and what happens is almost instantly the Linux kernel, which accepted a connection innocently, can be caused to panic and halt.  The default behavior of Linux when you get a kernel panic is just it stops.  It flashes the keyboard lights, and it assumes something really bad has happened, so it waits for someone to come along and help it.  You can configure it to reboot itself, but that's not the default state.  There are some workarounds where, if you have a Linux server that you don't want to take down at all, or that you're not ready to update, you are able to disable this SACK support.  It's a sysctl command line.  I've got it in the show notes also.



Ubuntu has a knowledge base article titled "SACK Panic."  They wrote:  "Jonathan Looney discovered several flaws in the way that the Linux kernel's TCP implementation processes Selective Acknowledgement options and handles low Maximum Segment Size (MSS) values.  A remote attacker could use these issues to perform" - and technically this is called "denial of service" because, yes, if your kernel crashes and halts, that's a denial of service to everybody - "perform denial of service attacks on a server.  CVE-2019-11477 is the highest severity issue because a remote attacker can leverage it to immediately crash a system due to an integer overflow when processing TCP SACKs.  It affects all current Ubuntu releases."  This is Ubuntu's knowledge base.  And of course it's Red Hat.  It's AWS.  It's all Linuxes.



"You should update your kernel to the versions specified below in the Updates section and reboot.  Alternatively, Canonical Livepatch updates will be available to mitigate these two issues without the need to reboot.  If neither of those options are possible at this time, you can mitigate the issue by temporarily disabling TCP SACK support."  And I've got the command line.  But if you just google "sack," S-A-C-K, "panic," or sack, yeah, probably "sack panic" or "sack TCP," already Google is returning lots of results.  Oh, and that sysctl modification is not persistent across reboots.



So there is a quick fix.  But as is always the case, there will be lots of servers that are not being well tended, that are in closets.  I mean, and think about it.  Not only mainstream servers, but Microlinux kernels in cameras and in security systems and in DVRs, I mean, anything that is Linux-based for the last 10 years.  In its default configuration, which has this selective acknowledgments enabled because nobody had a reason not to, it was a benefit; right?  It's a feature.  It makes things more better.  You send them a few specially crafted packets, and that thing is dead.  It is over.  So as I said, that will be - that cheery topic will be next week's podcast, I have a feeling.



LEO:  No commitments because there's always something else.



STEVE:  Yes.  Yes.  I can't imagine anything worse, but you never know.



LEO:  Well, a crash isn't the end of the world.  I mean, of course, as we know, crashes are often precursors to hacks.



STEVE:  No, well, this cannot be leveraged in this case.



LEO:  No.  It can't be because it's frozen.  It's too late.



STEVE:  It is the kernel recognizing, oh, my god, I don't know what to do.  I'm halting.



LEO:  Right, halt, yeah.  [Crosstalk].



STEVE:  So it is a programmed fault.



LEO:  Yeah.



STEVE:  But, again, having a server which is expected to stay up - oh, and remember that, if it does reboot, you just send the same little packets again, and it's down again.



LEO:  Right.  It's more of a mischief-making thing than anything else.



STEVE:  Yeah.  But depending upon - but, you know, if it's in control of your power grid or your nuclear reactor...



LEO:  Well, that would be bad, yes.



STEVE:  Yeah.  So there are many places where Linux servers are assumed not to be vulnerable to immediately being crashed.  I mean...



LEO:  Now, if I'm behind a firewall and not accepting TCP packets, it's not a problem.



STEVE:  Right.  The typical home user who is not serving anything, who has no open ports, they're fine.



LEO:  Yeah.



STEVE:  But any, I mean, it doesn't even have to be a web server.  It could be telnet.



LEO:  Well, I have a Minecraft server that's accepting TCP connections.  So it could be...



STEVE:  Yes.



LEO:  That would be vulnerable.



STEVE:  It could be taken right off the 'Net.  Yeah, yeah.  So we'll see.



LEO:  Yeah.  Under siege, Steve.  Under siege.



STEVE:  I happen to be a big fan of wasabi.  I like to mix it with my soy until it kind of forms sort of a dark green mud.



LEO:  How much wasabi?  You use a tiny little ball of wasabi?



STEVE:  Oh, no, no, no, no.



LEO:  Or do you put the whole clump in there?



STEVE:  I thicken my soy sauce with wasabi.  I like to...



LEO:  I thought you might.  I like wasabi, too.



STEVE:  I like to perspire.



LEO:  It's a great name, actually.



STEVE:  Great.  So it was June of 2014, so five years ago, that we got the first whiff of a problem with DRAM memory not being as stable as we all hoped and assumed it was.  And, you know, there'd been the stories about spraying it with Freon, which was surprising to us at the time, and relocating it to a different machine.  It was like, wait.  You mean the bits hold onto their charge that long?  It's like, uh-huh.  Anyway, since then, five years ago, this subtle flaw has been developed into a growing family of attacks which we've gleefully covered over the years - Rowhammer, GLitch, RAMpage, Throwhammer, Nethammer, and more recently DRAMmer.  



LEO:  Oh, yeah.



STEVE:  Now to those we add RAMBleed, which is different.  And we should probably call that prolific team at the University of Michigan and the Graz University of Technology the "Ram Busters."  We have a website, RAMBleed.com, and a logo because, you know, a good exploit needs a good logo.  So we got that.  The official title is "RAMBleed - Reading Bits in Memory Without Accessing Them."  And I'll just read their little description.  They said:  "RAMBleed is a side-channel attack that enables an attacker to read out physical memory belonging to other processes."  Oops.  That's not good.



"The implications of violating arbitrary privilege boundaries are numerous [uh-huh] and vary in severity based on the other software running on the target machine.  As an example, in our paper" - get this - "we demonstrate an attack against OpenSSH in which we use RAMBleed to leak a 2048-bit RSA key.  However, RAMBleed can be used for reading other data, as well.



"RAMBleed is based on a previous side channel called Rowhammer" - which of course we've talked about extensively - "which enables an attacker to flip bits in the memory space of other processes.  We show in our paper that an attacker, by observing Rowhammer-induced bit flips in their own memory, can deduce the values in nearby DRAM rows.  Thus, RAMBleed shifts Rowhammer from being a threat, not only to integrity, but to confidentiality, as well.  Furthermore, unlike Rowhammer, RAMBleed does not require persistent bit flips, and it is thus effective against ECC memory commonly used by server computers."  And of course it was ECC that was stated as the cure for the Rowhammer problem because, if you flipped a bit, the ECC would flip it back.



Okay.  So what did all that mean?  As we've talked about with Rowhammer, remember that the idea was, if you pounded on DRAM, reading DRAM is a destructive process.  Remember that so DRAM itself is a row of little itty-bitty capacitors which store electrostatic charge.  And if they're charged up, they contain a one.  If they're discharged, they contain a zero.  So the act of reading a row of memory - oh, and I should also mention that a DRAM is a big grid, a big rectangular grid of rows and columns of these little cells.  The act of reading a row transfers the charge out of the row into sense amplifiers on the edge, and thus the row needs to be rewritten.  So reading forces a write.



And what was discovered is that, unfortunately, in the push to increase density, the capacitors have been made ever smaller.  And the engineers have said, okay, there's enough margin, error margin, tolerance, noise margin, that this is reliable enough.  Now, if you want more reliability, because you can get an occasional misread, then you add ECC memory.  The idea is there's an extra bunch of bits tacked onto the end of each row to detect and correct a bit if it's read back incorrectly.  The point is that DRAM is not perfect.  And again, unfortunately, they keep making the storage cells tinier in order to cram more of them onto a smaller chip, in order to keep the cost down and to raise the density.  And we end up with a situation where it turns out that, if you make a big ruckus in the neighborhood, then you can cause a misreading, a deliberate misreading in bits nearby, that is, the adjacent row bits.



And then what was discovered, you could even get more bits to flip, or bits to flip more reliably, with what was known as Double Rowhammer, or DRAMmer, where you would be pounding on both rows on either side of your target row, so to kind of get it from both sides and even induce more bit flips.  And, for example, we talked about one instance where - this was so clever - where in a VM environment, the private key which was in use by a server - remember that a private key is deliberately the product of two primes, the point being you can't factor it in order to break it apart again.



Well, these guys flipped a bit in the private key on an adjacent virtual machine that they were sharing, which meant that its private key was no longer the product of two primes.  It was the product of other things that was easy to factor.  And so they factored it and then they were able to spoof their connection, essentially.  So it's just super clever.



Okay.  What these guys have done - okay.  So that was Rowhammer, pounding on adjacent rows on either side of a target to make a bit flip.  Well, the observation had been made then, but it wasn't until now that it was weaponized, that the probability of being able to get a bit flip depended weakly but significantly upon the bit states of adjacent rows.  In other words, if you could arrange to cause a secret to occupy the physical row above and the physical row below your own memory, then you could use Rowhammer on your own memory and, based on the probability, that is, the success rate of flipping bits in your own memory, you could infer the invisible contents of the rows above and below.  Which is just amazing.



They said:  "Specifically, '1' bits tend to flip from 1 to 0 when the bits above and below them are 0, but not when the bits above and below them are 1.  Similarly, '0' bits tend to flip from 0 to 1 when the bits above and below them are 1, but not when the bits above and below them are 0."  In other words, there's a tendency for the adjacent bits to pull the bit in the middle to their same state if you give them the chance.  And these guys weaponized this.  They nailed it to the ground.  And they are able to extract a 2048-bit secret key from an adjacent process that they have no read access to.  They can exfiltrate that key over time.



And it turns out that, if there's ECC memory, that is, if they're trying to do this in the presence of ECC memory, ECC, when it detects that a bit flip has occurred, introduces such a dramatic slowdown in performance that that can be detected.  So they're able to infer that a bit flip occurred, even if they can't see it, because when they attempt to read it to detect a bit flip, ECC will get there first.  It'll see, whoops, we had a problem here.  And it'll fix it.  But that delays the result of the read so much that even though the data comes back apparently not having been flipped, they go, aha, that took too long.  ECC corrected the bit flip that we induced.  So even ECC won't fix this problem.  So there really is no solution to this.



They wrote:  "Users can mitigate their risk by upgrading their memory to DDR4" - because we know that four is better than, you know, more noise-immune than three - with also the feature known as targeted row refresh.  The idea is that, as we know, DRAM, because these are little capacitors storing our data in the charge of a capacitor, it tends to bleed over time.  Again, they've made them so small that they're, like, counting electrons now.  So it's necessary to periodically read through all of DRAM to read every row out before the "1" charges have a chance to get so low that you really can't distinguish them from "0."



So the idea is that the rate at which you refresh affects the integrity of your DRAM.  One of the early mitigations for Rowhammer was increased refresh rate.  That would improve the noise margins of DRAM in order to minimize the chance of Rowhammer attacks.  The alternative is this TRR, which is a new feature, Targeted Row Refresh, the idea being that the memory system looks at the memory access pattern and will over-refresh the areas that might be subject to either inadvertent or deliberate bit flips by selectively, preferentially, refreshing those rows more than others.



So anyway, so fundamentally there is no good solution here.  Just as we have learned that our high-performance CPU architectures are flawed at the fundamental level by being altered by their own execution history, thus Spectre and Meltdown all of last year, we have seen the DRAM is similarly flawed at a fundamental level.  That is, I mean, these are not fixable problems.  These are fundamental flaws in the systems that we've been using.  And we're now - what we've seen for the last, well, really for the last five years, if you consider Rowhammer, but also certainly all of the last year with all of the microarchitectural flaws, is that academic researchers are really looking at things that we've taken for granted closely and successfully poking big holes in what we thought we were able to trust.



So I don't know.  In their Q&A that they have, they've mentioned that they are highly suspicious or skeptical that anyone would have already succeeded in weaponizing this and using this.  But here is a problem that ups the ante.  I mean, it would take - it took a lot of massaging of memory management in order to cause the physical alignment of the secrets they want to exfiltrate with the memory that they have control over.  But this is arguably a more powerful attack, that is, this RAMBleed, than all of the previous Rowhammeresque attacks because, given that you have time and the ability to massage memory allocation, which is what's necessary in order to create physical RAM proximity, then you are able - they said three to four bits per second is their exfiltration bit rate once they have everything set up.



So it really does mean that servers need to be looking seriously at using DDR4 DRAM with targeted row refresh.  Maybe sacrifice some performance by increasing refresh rate, if possible.  And really, stepping back, this doesn't affect typical end users, again, in the same way that Spectre and Meltdown don't, because the real problem is only when you've got reason to believe that something malicious is sharing your hardware with you.  As we've often said, if you've got something malicious on your desktop, sharing your desktop hardware, well, you're already in much bigger trouble.



The real problem is in the cloud.  And the good news, I guess, is that's where they're able to spend some money in order to implement stronger hardware mitigations against this.  But even though this is not good news, it's better to know that we have this problem, I mean, that's why targeted row refresh exists now in DDR4 is thanks to the early work five years ago that DRAM can be exploited successfully with Rowhammer-style attacks.  So we got improved DRAM.  Similarly, we're getting improved microcode now and improved microarchitectures in the future in order to further mitigate microarchitectural information leakage.  So onward.



I just sort of shook my head when I saw another warning about BlueKeep vulnerability.  We're now, what, six weeks from the announcement, which would have been the May Patch Tuesday.  Still no worm.  And as I have said, I doubt that we're really going to see one.  When we talk about the Exim worm here at the end of the podcast, I'll reiterate why Exim made sense to be wormed, and BlueKeep just doesn't.  So as we know, we've had two warnings from Microsoft.  Warnings from lots of other people, too, but two official warnings from Microsoft.  Last week we talked about the warning from the NSA.



And now we have the U.S. Department of Homeland Security, the CISA, the Cybersecurity and Infrastructure Security Agency, which yesterday, Monday, published an alert for Windows users to patch the critical severity remote desktop services RCE, the Remote Code Execution security flaw known as BlueKeep.  Thank you, DHS.  Always keeping an eye on our back.  Again, it's not been patched yet.  I'm skeptical that anybody getting this Department of Homeland Security advisory is going to be patching it that hasn't already.  They did, however, confirm that they had successfully executed a remote code execution on Windows 2000, which was previously - it was speculated to be vulnerable, but it hadn't been proven to be vulnerable.  So they added a little bit of information to the growing pile of information on BlueKeep.



Anyway, as I said, I'm unconvinced that we're going to see a worm, although the world is surely asking for one.  That's what we keep hearing is oh, it's wormable, it's wormable, it's wormable.  It's like, yeah.  Except that it's not difficult to find them.  Anyone can find where all the servers are.  It takes zero time to exploit because basically you just ignore logon credentials, and you can immediately establish a remote desktop session and set up your cryptocurrency miner and then close the door behind you.  So it just seems like as soon as somebody does that, if they haven't already been doing it, they're going to do that.  Doesn't make sense to have a worm.  But as we mentioned, there is one for Exim that we'll be talking about in a minute.



As I mentioned at the top of the show, when Tavis Ormandy tells you that you have 90 days to fix something, get on it.  Of course, we're speaking of Google's illustrious bug finder.  Issue 1804 - I have a link to the bug report, Project Zero, Google's Project Zero.  Issue 1804 titled "cryptoapi:  SymCrypt modular inverse algorithm," reported by taviso@google.com on Tuesday, March 12, 2019.  He wrote, and this was not public, so he put this under lock privately on March 12.



"There's a bug in the SymCrypt" - and that's a component of Windows - "multi-precision arithmetic routines that can cause an infinite loop when calculating the modular inverse on specific bit patterns in" - and then it's a particular function call:  bcryptprimitives is the module; SymCryptFdefModInvGeneric is the function.  So, he said, "I've been able to construct" - and here's the key.  "I've been able to construct an X.509 certificate" - that's a standard identification certificate - "that triggers the bug.  I've found that embedding the certificate in an S/MIME message, Authenticode signature, secure channel connection" - i.e., TLS, HTTPS - "and so on will effectively DoS any Windows server," and then he says, "e.g. IPSec, IIS" - which of course is their web server - "Exchange, et cetera; and, depending on the context, may require the machine to be rebooted.  Obviously, lots of software that processes untrusted content, like AV, call these routines on untrusted data, and this will cause them to deadlock."



He says:  "You can verify it like so, and notice the command that never completes."  And he gives an example.  So he shows C:\, so he's logged into the root directory on a Windows machine.  And the command is "certutil.exe," and he gives it "testcase.crt," which is the certificate that he manually constructed to invoke this flaw, this vulnerability that exists in the Windows symmetric - actually it's in the SymCrypt library, which was previously and traditionally the symmetric cryptography, but naturally a certificate is asymmetric crypto.  So he says:  "I'm filing this as low severity, although you can take down a Windows fleet pretty quickly with it.  This bug is subject to a 90-day disclosure deadline.  After 90 days elapse or a patch has been made broadly available, whichever is earlier, the bug report will become visible to the public."



Okay.  That was on March 12th.  Six days later, March 18, Tavis added an update to that with an MSRC, Microsoft Security Center, Label 50858.  So it had been acknowledged and given a number.  Eight days from then, on Tuesday, March 26th, he added, "Microsoft replied that they would like to issue a bulletin for this issue but need until June 11th."  Okay.  That was last Tuesday; right?  Patch Tuesday.  He says:  "I count that as 91 days, but within the extension period, so it's acceptable."



Then last week, last Tuesday, June 11th, at 8:16 a.m., six days ago, the label of his update submission was Restrict-View-Commit Deadline Exceeded.  "MSRC reached out," he writes, "and noted that the patch won't ship today, and wouldn't be ready until the July release due to issues found in testing."  He says:  "As today is 91 days, derestricting the issue."  And in there, in this disclosure that I have the link to in the show notes and, Leo, you have on the screen, are two files that implement this vulnerability.  Now...



LEO:  It seems a little dickish to me, but okay.



STEVE:  Well, but 90 days. 



LEO:  Yeah, but that's very anal, though.  They said, well, we thought we were going to have it out in time; but we found a problem, so we're going to put it to the next release.  Come on.



STEVE:  Ninety days, Leo.



LEO:  I would say, okay, I'll give you to the next release.



STEVE:  They could have fixed it.



LEO:  Well, okay.



STEVE:  Yeah, I mean, again...



LEO:  What's the point of a 90 day?  Give me the rationale for the 90 days.



STEVE:  So the rationale is that these sort of things could be found by other people.  We don't know they haven't been.  And we don't know that they're not being exploited.



LEO:  And now we know they have.



STEVE:  Yes, now we know they have.



LEO:  So what is the point of this?  To punish them?



STEVE:  To motivate companies to take these things seriously.



LEO:  Well, yeah.  But I think they did, and they had a fix, and then it turned out there were unforeseen issues.  Wouldn't you want to give them...



STEVE:  We don't know, we don't really know.  I mean, 90 days is a long time to fix what is probably something simple in the asymmetric crypto library.  I mean, as far as we know, they dicked around, to coin a term, for 60 days and then thought, oops, whoops, maybe we need to do this.  And so they said, well...



LEO:  Well, what if they didn't?  What if they worked really hard, they got a fix, they didn't - and these things happen; right?  Sometimes the fix causes another bug.  In their testing, right before they released it, they found a bug.  They said, well, okay, we're going to have to wait till next Patch Tuesday because you don't want us to release this with another bug.  They're working hard on it.  What if that's the case?  What if they worked every day between the discovery of this and the release of it, and they just want one more, give us one more Patch Tuesday?  I don't know.  It feels a little dickish to me.  It's not the first time TO has been a little prickly.



STEVE:  You know, we have seen instances where companies have gone six months blowing off reports.



LEO:  And that I understand, absolutely.



STEVE:  And not bothering.



LEO:  But I also don't think it's appropriate to be 100% anal about 90 days.  Come on.  That's a little anal.  What you're looking for is the intent.



STEVE:  He was going to give them an extra day, Leo.



LEO:  A whole 91.



STEVE:  He was going to give them 91.  But I certainly take your point.  The good news is, as we were discussing, GandCrab, which was this weird Ransomware as a Service - remember I had fun reading the, well, I didn't read it, I read my own - I wrote a pitch for people who are unable to create crypto malware of their own to use GandCrab's in return for giving the GandCrab folks a relatively small, apparently, piece of the action.  They announced that they had "earned" all the money - "extorted" is more appropriate - all the money that they needed to.  They had laundered it by investing in legitimate Internet and other businesses, and so they were shutting down.  They urged everyone, all of their subcontractors, to wrap things up, that they were no longer going to be supplying keys.



Well, it turns out that Bitdefender managed to hack their servers and obtain the entire database of keys.  So if anybody was ever infected by version 1, version 4, version 5.0 to 5.2, those keys are available.  Bitdefender has a link to the GandCrab removal tool for versions 1, 4, and 5 of GandCrab.  BleepingComputer covered this news in their coverage and explained that, after being installed, this GandCrab removal tool, it will need to connect to the Internet in order to check in with Bitdefender's servers and obtain keys for that specific machine.



BleepingComputer suggested that you test decrypt sort of a subdirectory that's not crucial to make sure that it's working.  If it proves itself out, then go ahead, turn it loose on your entire machine; and, had you been encrypted, you can get your data back.  So this wasn't quite as cool as GandCrab deliberately turning the keys over.  But the result is the same, and that is that anybody who had been encrypted and who had not yet managed to decrypt themselves, thanks to Bitdefender is able to do so.



Yubico hit an interesting bump and is replacing all of its FIPS-certified keys, that is, versions 4.4.2 or 4.4.4.  There was no 4.4.3.  So anybody listening to this who hasn't already received notification, who has 4.4.2 or 4.4.4, should receive and can receive at no charge a replacement to 4.4.5.  In the middle of March, so just about exactly three months ago, Yubico internally discovered that some of the data left behind by the FIPS firmware power-up self-test was lingering in the device's random bits buffer.  And for the first cryptographic operations which were made after power-up, those bits which were not high entropy were mistakenly being delivered as if they were high-quality entropy.  Once all of those bits were consumed, then the random bits buffer would be replenished with the intended high-quality entropy.



But this meant that the initial post-power-up state of the device was not generating the intended entropy, which they have fixed in 4.4.5.  In their advisory they said:  "An issue exists in YubiKey FIPS Series devices, versions 4.4.2" - actually, I think I've already pretty much covered everything they said.  They found the issue mid-March.  They did a full investigation.



"To safeguard the security of our customers, Yubico has been conducting an active key replacement program for affected FIPS devices since the issue was discovered and recertification was achieved."  They had to get FIPS-recertified for 4.4.5.  "At the time of this advisory, we estimate that the majority of affected YubiKey FIPS Series devices have been replaced, or are in process of replacement with updated, fixed versions of the devices.  However, if you have purchased a YubiKey FIPS Series device or received one from another entity and have not been contacted by a Yubico representative, we ask that you review this advisory to determine if you may be affected and use the replacement portal to receive updated keys."



So, you know, they immediately fixed the problem and have proactively replaced the hardware that was found to have defective firmware.  So props to Yubico.  And if anybody has a 4.4.2 or 4.4.4 FIPS device, it's either that or the C versions.  Anyway, you probably know [crosstalk].



LEO:  The standard YubiKeys are not FIPS devices.



STEVE:  Are not FIPS, yes.



LEO:  In fact, if you have a FIPS key, it should say on the key, if you look at the little dongle that sticks out from it where the QR code is, it'll say "FIPS" on it.



STEVE:  Exactly.



LEO:  When this came out I was all worried, but I realized I just have the regular FIDO U2F key.



STEVE:  Yeah.  And they did note - their explanation goes on at some length.  In trying to assess the severity, they noted that the biggest problem was with ECDSA, Elliptic Curve Signing, because the elliptic curve keys are much shorter, that is, typically 256 bits.  So the percentage of bad entropy, about 80 bits of bad entropy, forms a much larger percentage of the shorter key.  But, for example, a 2048-bit RSA key, because it's so much longer, the percentage of bad entropy 80 bits is a much smaller fraction.  And you could, you know, you would like it to be 100% entropy.  But the actual threat is much less in terms of practical use if you're using a 2048-bit key.  So anyway, they've got it fixed, and they've pulled them all out of the field, and they are replacing them.  So that's all you can ask.



I mentioned last week Sysmon, which was being updated by our friend Mark Russinovich at the Sysinternals branch of Microsoft, or subsidiary, or whatever it is now.  Microsoft bought Sysinternals and then kept it alive.  We were all worried when it happened.  Everybody downloaded all the Sysinternal tools that day because we weren't sure if they were being purchased to destroy them or what was going on.  But no.



We are now - I mentioned it last week because Sysmon was announced to be forthcoming with an update that would allow it to also log DNS queries made by systems to the event log.  And I directed our listeners, typically end users, many of us, to the NirSoft's DNS Query Sniffer.  And I got a lot of positive feedback via Twitter about people thinking it was really cool to be able to watch their machine reach out and look up DNS addresses.  Also, but we have a lot of enterprise listeners, too, who said, "Hey, Steve, just so you know, Sysmon rocks.  Our enterprise uses it like crazy.  And we love the fact that it will be adding DNS logging because this allows us then to pull all of the logs from all of the endpoints in our enterprise and see if anything is doing anything that seems suspicious."



So definitely a useful feature.  And I didn't mean to say that it wasn't, just that for an end user, digging down into the event log, I mean, I have to do it for my own development purposes from time to time, and it's not right there in your face, and not real-time scrolling and all the other good stuff that you'd like to have.  So both purposes have a point.



I have updated the SQRL Explainer.  I forgot to mention in the first, what was it, 17 pages.  It's now at 21 pages because by popular demand I added a three-page glossary at the front of all of the various terms.  One page of standard crypto terminology, and I think about a page and a half of SQRL-specific terminology, terms that had to be invented in order to discuss new features that SQRL brings to the world.  But then I also - someone else mentioned that I had forgotten to mention the "Ask" feature in the first release.



During this protracted five and a half years, almost six, of development of SQRL, we kind of wandered off course a few times.  There were, like, there was discussion.  It's like, well, shouldn't it also fill in forms?  Why not?  While we're doing things, we can do anything we want to.  Form-filling is annoying.  Let's put that in.  And it was like, oh, I mean, and that's maybe an egregious example of a suggestion that's, like, way off of authentication.



But there was some initial design where it was more involved in the sign-in process and in the account management than strictly authentication.  And so a couple times I had to slap myself and say, "Bad Steve.  Let's stick to authentication.  That's what this is.  Let's not make it overly complicated.  Keep it simple."  And so forth.  So it is maybe significant that one non-authentication feature survived.  And it did because it was just too unique.  There wasn't any way to replace it.  There wasn't, I mean, unlike form filling and things, and that problem has been solved already.  And this is the - we call it "Ask" because it allows a limited out-of-band, meaning out-of-browser communication between the web server and the SQRL user.  To allow, for example, a website to confirm something that it really, really, really wants to make sure the user intends to do, like are you sure you want me to transfer $100,000 to this numbered account in the Cayman Islands?



Now, the problem is, if that's in your browser, there's just so many ways that can go wrong.  I mean, so many - our browser has unfortunately become a battleground.  But with this "Ask" feature, the server returns a question and is able to optionally provide the label of neither or up to one or two buttons.  And when the SQRL client sees it, it presents a special dialog to the user with the question as the contents of the dialogue, and prompts them for a response.



So anyway, that's now documented in the Explainer.  And it's the one thing beyond authentication that survived all attempts to restrict this to strict authentication.  We succeeded except here.  And here I think, I mean, who knows?  It may never get used.  But I can see, if this were part - it's the kind of thing where, if it weren't in the spec from the beginning, then it would be difficult for anyone to ever rely on it.  But it's in there.  It's a simple addition.  It was easy to add.  And it potentially could be a real benefit because, again, it allows a web server that knows it's talking to a SQRL user because that's the way they authenticated, to send them a question completely separate from the browser channel that - oh, and I don't know if I - I guess I didn't really explain it, is that there is no potential, there's no possibility of a man-in-the-middle intercept of this, due to the way the SQRL protocol works.  It cannot be intercepted.  So anyway, it made it.



We already talked about T-shirts on Amazon.  We're up now north of 2,000 members in the SQRL web forums as a consequence of big - we got a big bump, not surprisingly, and lots more new SQRL users.  So thank you to all of our listeners who are playing with it and learning it.  A lot of interest has been generated.  People have been having trouble with SQRL on Mac and Linux under Wine.  And so I just wanted to say that I went to lengths to make sure that GRC's Windows client would work under Wine.  But it's not my intention that it really be - that it receive heavy use.



What we need is a native Mac implementation of a SQRL client.  There is some work underway on a native client under Linux.  Far as I know, no one's working on a macOS version.  But we also have browser extensions for Firefox and Chrome and Edge.  And so those work beautifully on a platform independent fashion.  So the problem's been solved.  There were a lot of problems with getting Wine to work, just because it's amazing to me that a Windows app even tries to run on a non-Windows environment, and I'm surprised that it works as well as it does.  But it does.



And also, Leo, you and I talked about what we will do, which will be sort of the official SQRL Explainer professional video production that I'm very excited about doing.  I want to solicit, explicitly solicit, user videos for how to use SQRL.  I think users are better placed, and it makes more sense for end users who are interested in creating videos for how to do things, the different parts of using SQRL, to create videos.  And if they do them, I will be happy to host the best of those on the GRC SQRL web forums and to host the bandwidth.  I'm annoyed by commercials in YouTube, and so I will host the bandwidth.



But if people who listen to the podcast are camera equipped and like the idea of producing some how to use SQRL videos, all different aspects of the experience, I'd love to have them and love to host them.  So I just wanted to make that explicit.  I'm not going to do that.  I will do the technical how it works presentation.  But be really fun to have user-sourced videos.



And just a quick note about SpinRite.  I realized through some other work that I'd been doing that I was underestimating the performance that I think we're going to get under 6.1.  I'd long been talking about half a terabyte per hour.  And it looks like SpinRite will be able to do, on a 7,500 rpm 2TB drive, a Level 2 data recovery scan in closer to three hours, just a few minutes more than three hours.  So that's not bad for 2TB.  And of course what I had failed to take into account was that the data rate on these drives increases as their aerial bit density goes up.  And their aerial bit density has gone nuts in the last five years.  And as a consequence, their raw maximum data rate has been increasing.  And the point is SpinRite will be able to run at whatever the maximum throughput of the drive is.  It will not miss a revolution as it's operating.  So it's going to be screaming.  And as soon as I get the rest of SQRL put to bed, as we know, I will be back to it.



I mentioned an iOS security app.  Andy Pastuszak tweeted about TOTP.  He said:  "I went and enabled TOTP."  That's of course Time-based One-Time Passwords, the increasingly ubiquitous authenticators.  He said:  "I went and enabled TOTP on every site I have an account on that supports it.  And now my Google Authenticator is a complete mess.  Having one long list of TOTP codes can be quite problematic, especially since I use a Firefox extension that wipes all my cookies when I close the browser," meaning that he has to constantly re-log on as his sessions are not sticky across browser sessions.



He says:  "I'd love it if there was a TOTP app that does not sync and allows me to organize my folders, my codes by folder or even tags, or even just had a search feature."  God, I don't know how many he has, but congratulations, Andy.  He says:  "Until SQRL gets here, we're all stuck with two-factor authentication.  Could you maybe recommend the two-factor authentication app that has some organization behind it and has the Steve Gibson seal of approval?  I can't be the only listener with this problem."



Well, so it just so happens, and he didn't mention whether he was an iOS users, turns out he was, and he loved my suggestion.  I heard back from him after I gave him my suggestion.  I have now been for quite a while using an iOS, and it's also available for macOS, app for two-factor authentication which I love.  It's called OTP Auth.  It's got a super high rating, 4.8 on the iTunes store.  And if you look at the little graph of ratings, it's just like the five stars is almost all the way to the end.  I don't know if - because I own it, iTunes won't show me what the price is, so I don't know if I bought it or if it was a free download.  But it is ad free.



LEO:  It's free with in-app purchases.  But I don't see what the in-apps are.



STEVE:  Okay.  So maybe I bought it after I used it or something.  I've never seen an ad.  And the author says it's ad...



LEO:  Well, it's ad-free, yeah.



STEVE:  Yeah.  He says it...



LEO:  It's 3.99 if you want to maybe just pay, like give him some money.  Maybe that's it.



STEVE:  Ah, good.  Then, okay.  So encrypted iCloud sync.  Siri support.  Apple Watch support.  It'll support a notification center widget, and I use one.  So you can very quickly swipe left from your root page, and it'll come up, and you can choose.  It is folder enhanced, and there is a notification folder so you can choose which of your sites you want to show in the notification center.  He says secure application using Face ID, Touch ID, or password.  Create encrypted backups of all accounts.  Import/export encrypted accounts using AirDrop, iCloud, Dropbox, Mail, and so forth.  Works offline.



Anyway, it's done right.  And so I do recommend - oh, he has a Twitter handle, @otpauth, O-T-P-A-U-T-H.  So that's, you know, I recommended it to Andy.  I just wanted to commend it to all of our listeners because I think it was really well done.  And really quickly, My Krol, M-Y space K-R-O-L, tweeting from that, said:  "Hi, Steve.  A lot of podcast episodes I hear you talking about the special tabs in Firefox.  Can you explain me what you are talking about?"  And so very shortly, or very quickly, Tree Style Tab.  It's an add-on for Firefox.  Just search for Tree Style Tab, three words.  It went through a facelift a while ago.  It is now skinnable using CSS.  So you can make a series of customizations to it to make the tabs just the way you want them.  And I love it.  So Tree Style Tab.



LEO:  Okay, Steve.  On we go with the show.



STEVE:  So the question is, can you hear me?



LEO:  Yes.



STEVE:  Okay.  Your audio is doing some bizarre roboto thing.



LEO:  Oh.  All right.



STEVE:  But that's...



LEO:  I'll just shut up.



STEVE:  As long as it's coming in your direction, that's the only thing that matters.



LEO:  You sound great.



STEVE:  Okay, great.  So, okay.  So not a surprise to anybody who's been listening to this podcast so far.  The Exim mail server, which we urged everyone to update, like immediately last week, has in fact come under siege.  The first picture here in the show notes is a little difficult to parse unless you stare at it for a while.  What it shows is, from the announcement day of the vulnerability, the industry's response in the first six days.  And I have to say it is very impressive.  The big green area is non-vulnerable Exim servers.  And so if you just look at the very far left edge, for those of our listeners who are audio only, almost all of the servers on the 'Net, virtually all of them were vulnerable.



One day later - again, this is just very impressive.  In one day the percentage of vulnerable servers drops from essentially 100% down to about 25 percent.  So far fewer vulnerable servers in one day.  The problem is, that was a good day.  Then it sort of wanders, and we're still left with an Internet full of vulnerable servers.  In fact, the second picture is one of our famous Shodan outputs.  And it turns out - remember I asked last week, sort of hypothetically I wondered whether it was possible for us to - whether servers would announce their versions.  Turns out they apparently do.  Or at least it's possible to query them.  I haven't looked at it any further because this is a Shodan search.



The title is "Search for Product Exim Minus 4.92."  Anyway, the idea is it's a search term for versions lower than the one 4.92, which is the lowest one which is vulnerable.  So less than that.  And so we have, a week later, just shy of two million vulnerable Exim servers in the United States - 1,996,569.  In Russia, 192,737.  Canada, almost 143,000.  The Netherlands, 137,000.  Germany, nearly 130,000.  The U.K. 123.  Anyway, you get the idea.  More than two million looks like maybe - in fact there is a number somewhere.  I think it was 3.5 million was the total that I saw.



So CyberReason, an Amit Serper at CyberReason, two days after last Tuesday's podcast, so last Tuesday, June 11th, our podcast was titled "Update Exim Now!"  Two days later, Thursday, June 13th, Amit Serper of CyberReason, his posting was "New pervasive worm exploiting Linux Exim server vulnerability."  For the summary he said:  "There's an active, ongoing campaign exploiting a widespread vulnerability in Linux email servers.  This attack leverages a week-old vulnerability to gain remote command execution on the target machine, search the Internet for other machines to infect, and initiates a cryptominer."



For bullet points he said:  "Currently, more than 3.5 million servers are at risk worldwide.  The attack scours the Internet for a vulnerability discovered last week, CVE-2019-10149, using already infected servers to spread to as many as possible, i.e., worm.  The target of this attack, Exim servers, run almost 57% of the Internet's email servers.  The attack culminates in the downloading of a coin miner payload, which as we have seen previously with WannaMine can have a negative impact on any organization."  Finally:  "These kinds of attacks have big implications for organizations.  The recovery process from this type of attack is costly and time consuming."



So what do we know?  From his posting, he says:  "CVE-2019-10149, which was first discovered on June 5th" - so let's see.  June 5th, and this is on the 13th, so eight days it took - "is now being used as" - well, and also remember that the vulnerability takes a week to do, which says immediately, within a day or two, after the announcement happened, this thing was weaponized and turned loose so that other Exim servers could be commandeered, and this thing could propagate.  He says - so first discovered on June 5th.  Eight days later is now being used "as the vulnerability for a widespread campaign to attack Exim servers and propagate across the Internet."



He says:  "We are aware of an initial wave of attacks as described by Freddie Leeman on June 9th.  The first hacker group began pushing exploits from a C2 server located on the clear web.  A second round of attacks by a different attacker are being analyzed by the Nocturnus team."  That's these guys.  "The campaign uses a private authentication key that is installed on the target machine for root authentication."  That is an SSH key.  I'll get there in a second.  "Once remote command execution is established, it deploys a port scanner to search for additional vulnerable servers to infect."  So classic worm behavior.



"It subsequently removes any existing coin miners on the target, along with any defenses against coin miners, before installing its own."  And he writes:  "Note:  This is a very long script that downloads additional scripts and changes or adds many configurations on Linux servers."  That's where he meant that remediation was tough because it's like really hosed your server.  "This blog has the highlights of what the script is doing to provide a fast reference guide to the attack.  Some of the things that the script is doing are not documented in this blog post.  The hash of the script is available at the end of this article.  It has also been uploaded to VirusTotal."



So the highlights he mentions - I wanted to close a window.  So for highlights he mentions:  "This is a highly pervasive attack that installs cron jobs for persistence and downloads several payloads for different stages of the attack.  In one of those stages, one of the payloads is a port scanner written in Python.  It looks for additional vulnerable servers on the Internet, connects to them, and infects them with the initial script.  In the attack, the attackers add an RSA authentication key for the SSH server, which allows them to connect to the server as root and own it completely."



Okay.  So again, remember that what this thing does is this is not remote code execution.  This is remote command execution.  So what they did was they came up with a script of commands that would cause the server to download an SSH key which would install as an authenticated SSH-RSA key that would then allow somebody to SSH into the server to do whatever else they wanted to.  So he writes:  "If you are running an updated version of the Exim server, or you think that your server is compromised, please look for the following entry in your SSH configurations."  And of course if you're a Linux server user, you know to look in /root/.ssh and in other users' .ssh directories.  And then he provides a textual version of the SSH-RSA key text, which anyone could compare against.  If you find that, it means your server has been taken.



He says:  "In the final stage of the infection, the script downloads what appears to be a Windows icon file (.ico)."  He says:  "Its headers were modified to appear as a .ico file.  However, the icon file is actually a password-protected zip archive under the password 'no-password.'  It's a 64-bit, statically linked, stripped, and UPX-packed ELF file which is then extracted from the archive.  When unpacked, there is another ELF executable that is the coin miner."  He says:  "All in all, there were four scripts downloaded.  We're currently working on identifying more information about the campaign.



"At this point we're still conducting research to dig up more details on the attack, the breadth of the campaign, the payloads being used, et cetera.  Since this campaign has such a broad scope, we felt it would be wise to share as soon as we became aware.  This document will continue to be updated as we dig up more information.  It's clear," he writes, "that the attackers went to great lengths to try to hide the intentions of their newly-created worm.  They used hidden services on the Tor network to host their payloads and created deceiving Windows icon files in an attempt to throw off researchers and even system admins who are looking at logs.  The prevalence of vulnerable Exim servers, 3,683,029 across the globe according to Shodan, allows attackers to compromise many servers in a relatively short time, as well as generate a nice stream of cryptocurrency revenue."



So that was last Thursday.  On Friday, June 14th, the next day, Microsoft's TechNet blog posted:  "Prevent the impact" - this is Microsoft.  "Prevent the impact of a Linux worm by updating Exim."  So Microsoft jumped onboard, saying:  "This week MSRC confirmed the presence of an active Linux worm leveraging a critical Remote Code Execution vulnerability, CVE-2019-10149, in Linux Exim email servers running Exim version from 4.87 to 4.91.  Azure customers running VMs with Exim 4.92 are not affected," okay, meaning not running the vulnerable version of Exim.  However, those who are, are susceptible.



They wrote:  "Azure has controls in place to help limit the spread of this worm from work we've already done to combat spam, but customers using the vulnerable software would still be susceptible to infection."  Meaning if you're on Azure with a Linux instance and Exim 4.87 to 4.91, you want to fix that.  They said:  "There is a partial mitigation for affected systems that can filter or block network traffic via Network Security Groups (NSGs).  The affected systems can mitigate Internet-based wormable malware or advanced malware threats that could exploit the vulnerability.  However, affected systems are still vulnerable to Remote Code Execution exploitation" - and actually it's Remote Command Execution, as we know - "if the attacker's IP address is permitted through Network Security Groups."



So anyway, even Microsoft and Azure are seeing this.  And on Saturday MSRC also confirmed that they have detected this worm targeting Azure customers.  So anyway, what we were expecting did happen.  The reason this is a worm is it takes a week to obtain access.  That's what you want to use a worm for.  Doesn't make any sense for a low number of servers to be trying to infect a large number of servers.  You want exponentiation.  You want servers that are infected to start looking for other servers.



We know that this thing takes seven days, right, because it uses a funky set of edge conditions on email delivery timeouts where you need to keep a connection alive, and then after seven days you then accept a fail.  You then close it, accept a failure message, and then the remote server executes the command which you put in as the account name on an email server that you control.  When all that happens, you are in, but it takes time.  So this is a perfect setup, a perfect scenario for a worm.  And unfortunately it is loose.  It is out there.  It is scanning, and it is installing itself as a cryptominer on, wow, way more than 3.5 million currently vulnerable Exim servers.  It's going to find them all.  We've seen what Internet-scale worms do.  They find them all.  And this thing will.



So a week before this, at the announce time, there was a startling change in version number.  There is a process, there's clearly a system in place for updating these Exim email servers because the percentage of them that are vulnerable dropped from 100% to about 25% overnight, in a day.  But that still leaves plenty that are vulnerable.  And whoever it is who sets up shop in these and is arranging to mint cryptocurrency and then to close the door behind them, they're probably going to be minting for quite some time because a lot of these systems are clearly unattended, and they've been forgotten.



LEO:  Wow.



STEVE:  Yeah.  It happened.



LEO:  There you go.



STEVE:  An Internet-scale worm.  We haven't had one for quite a while.



LEO:  Seems like a really hard thing to implement, though.  Have we seen any exploits?  I mean, you've got to sit there for six days banging on this server.



STEVE:  Well, that's why you want other people's email servers to be doing it for you.



LEO:  Yeah.  Okay, that makes sense.



STEVE:  Yeah, no, I mean, it's spreading.  It's in active use right now.



LEO:  Oh, it is.  Okay, okay.



STEVE:  Yeah, yeah, yeah.



LEO:  Not hypothetical.  



STEVE:  Nope.



LEO:  All right.  There you go.  Steve Gibson, GRC.com.  If you want to get transcripts of this show, 16Kb versions, or the full 64K audio, he's got it there, along with SpinRite, the world's best hard drive maintenance and recovery utility.  Also lots of other stuff, including SQRL information, and soon links to SQRL PopSockets and SQRL T-shirts, I'm sure.



STEVE:  Yes.  I just need to catch up. 



LEO:  Just get those up there, yes.



STEVE:  I've been racing behind the announcement ever since.



LEO:  You can also find audio and video from the show at our website, TWiT.tv/sn.  We do the show every Tuesday, about 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  You can watch us do it live or listen live at TWiT.tv/live.  And if you're live, go in the chatroom.  Join the folks in there who are watching live because there's always a conversation behind the scenes in our chatroom, irc.twit.tv.  We will be back next week.  Maybe you'll talk about Zuckbucks.  I don't know.  Maybe we'll talk about - what's that other thing that's going to happen, bad thing?



STEVE:  Oh, it's going to be the complete collapse of Linux.



LEO:  Oh, yeah, that.  Oh, yeah, Linux just ending.  It's over.  It's all over.



STEVE:  Linux SACK.  All Linux servers will be kernel faulted, yes.  Panicked.



LEO:  Next week.  I'll see you then, Steve.



STEVE:  Maybe.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#720

DATE:		June 25, 2019

TITLE:		Bug Bounty Business

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-720.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we check in on the state of last week's Linux TCP SACK kernel panic, examine two Mozilla zero-days which were being used against Coinbase and others, and note that performing a full factory reset of an IoT device may not be sufficient.  We look at a very clever and elegant solution to OpenSSH key theft via Rowhammer attacks, share an update on the BlueKeep RDP vulnerability, and examine the cause of a three-hour widespread Internet outage yesterday morning.  We discuss NASA's APT, which crawled in via a Raspberry Pi, the cost of paying versus not paying a ransomware ransom, and an update on Microsoft's Chromium-based Edge browser.  Lastly, we handle a bit of listener feedback, then take a closer look at the state of the commercial bug bounty business.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  There is a lot to talk about.  Cyberwarfare, candy drops, and how the entire Internet got routed through Northern Pennsylvania for three hours.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 720, recorded June 25th, 2019:  Bug Bounty Business.



It's time for Security Now!, the show where we cover your security with this guy right here.  We've been doing it for 15 long miserable years, and things aren't getting any better.  Steve Gibson.  How are you today?



STEVE GIBSON:  I would argue they're getting a lot worse, actually.



LEO:  Yeah?  Oy oy oy.



STEVE:  Been a lot of change.  In fact, today's podcast is going to address one of those really interesting developments.  This is Security Now! #720 for June 25th, and I wanted to talk about the bug bounty business from a slightly different perspective than we have before.  We've talked about Zerodium, and we've talked about SandboxEscaper dropping hers, and we've talked about the Pwn2Own competitions.  But there's a really, like 100% white hat hacker business underway that I wanted to shine a little bit of light on today.



This was prompted by some coverage of HackerOne in a recent article on ZDNet.  And I thought, you know, let's talk about this because Zerodium, as you and I both have said, Leo, just sort of makes us a little queasy, you know, the idea that there's something, some entity is purchasing zero-days and paying some substantial coin, and would only be doing this if they were reselling them for profit to we don't know whom.  You know?  State actors, law enforcement, intelligence agencies globally, who knows?



So anyway, a lot happened.  We're going to check in on the state of last week's Linux TCP SACK kernel panic.  Mozilla's Firefox suffered and recovered from two zero-day exploits, which - and I'm a little disappointed because one of them they were informed of in April, and I don't know what they were doing.  I guess they thought, well, by itself it's not a problem so we'll fix it when we feel like it.  But when paired up with the second one, it became a real problem.  So they suddenly went into overdrive and fixed those.  Also there was an interesting story about even if you performed a full factory reset of a NEST cam, that that might not be sufficient before reselling it.



We also have a very clever and elegant solution to OpenSSH's key theft, which of course we talked about extensively when we talked about the RAMBleed Rowhammer attack last week.  There's been a fix for it that I'll explain, which I think is very clean and clever.  Also an update on BlueKeep, the RDP vulnerability.  Also, I mean, as I said, lots happened.  For three hours yesterday morning, well, yeah, morning our time, about 3:00 a.m. to 6:00 a.m. Pacific time, there was a quite widespread outage that affected a whole bunch of major Internet providers.  We'll talk about the cause of that.  This is something that we've touched on a few times in the past.  But negligence actually on the part of Verizon was largely responsible, although they weren't the proximate cause.  Anyway, it's interesting.



Also NASA discovered that they had had an advanced persistent threat in their network at JPL for a year as a consequence of a Raspberry Pi that had been connected to the network.  Also some interesting data on the cost of paying or not paying a ransomware ransom.  Also a quick update on Chromium, a little bit of listener feedback, and then we're going to talk about the way the commercial white hat bug bounty business has been growing sort of quietly behind the scenes, but it's something that definitely deserves a little bit of coverage.  So I think a great podcast for our listeners.



LEO:  Yeah.  You said it was going to be short.  Doesn't sound...



STEVE:  Yeah, that doesn't sound that short, does it.



LEO:  That doesn't sound short at all.  But we are always glad.  The longer you go, the happier most of our audience is.  I've never heard any - I've heard people say "That was too short."  I've never heard anybody say "That was too long."  Steve?



STEVE:  So our Picture of the Week is just simple and fun.  I was just noticing as I was looking at it that...



LEO:  And scary.



STEVE:  That the credit for who created it got cut off.  I didn't cut it off.  It was that way from the person who tweeted it to me, so I apologize for not knowing who did it.  But anyway, the title is "CyberWar," and there's a big U.S. bomber flying through the sky, and a little balloon callout saying "USB key dropped."  And then you can see a dotted line with a little USB key falling out of the bottom of the plane.  And of course that's, as we know, in fact we covered it, that there had been some studies done of whether people would plug in random USB thumb drives found on the road, literally like, you know...



LEO:  And the answer is yes.



STEVE:  Yes.



LEO:  Oh, look.  It's a 64GB drive free.



STEVE:  Ooh, nice.  Yeah, that's right.  Just plug it in and see what's there.  It's like, eh.  Okay.  And just so our listeners know, don't do that.



LEO:  No.



STEVE:  That's not good.



LEO:  But it was used in cyberwarfare.  We don't know by which government, but against Iran; right?



STEVE:  Stuxnet.



LEO:  Stuxnet.



STEVE:  Stuxnet was believed to have been a so-called "sneakernet."  Back before we had networks, sneakernet was floppies that you would carry from machine to machine.  And then of course now we have thumb drives.  And so yes, you're right, it was designed, Stuxnet was designed - because the centrifuges that were being used to concentrate plutonium were deliberately off the 'Net.  They were isolated.  There was no connection between them and the Internet, so there was no way electronically to get to them.  So they said, okay, great, we'll just see if we can hitch a ride on somebody's thumb drive because you still have to have computers at the other side.  Even if they're not dynamically networked, they're sort of virtually networked, thanks to moving data on thumb drives.  So that's all it took.



Okay.  So everyone may have noted, here we are, it's Tuesday.  The Internet is still here.



LEO:  What?  I didn't expect that.



STEVE:  I know.  It's very resilient, Leo, although it did take a bit of beating yesterday morning.  We'll talk about that in a second.



LEO:  Oh, really.  Oh, okay.



STEVE:  Yeah.  Last week's revelation about the multidecade Linux and FreeBSD - FreeBSD to a lesser degree.  The normal configuration wasn't vulnerable.  But there were three CVEs affecting the TCP stack, which as we know allowed any vulnerable listening Linux machine to be remotely attacked and halted with a kernel panic.  And that has not so far resulted in any reports of widespread use and abuse.  And again, the model that's evolving is that the hackers really do seem - this is not surprising, I guess - to be interested in money.  And we've sort of moved past the point where, oh, look, this is fun to do, like which is what we used to have in the early quaint days of viruses; right?  When viruses were propagating, and we were always sort of saying, well, what's the motivation here?  It was just to sort of see if you could.



Now it's about money, which is of course why what we're seeing is cryptocurrency miners are being installed and ransomware is not going away anytime soon.  We'll have a story about that.  So I guess it's not that surprising that there's not any widespread use of this flaw for crashing Linux servers.  I mean, like, okay, that doesn't make anybody any money except maybe in specific targeted instances, and those probably don't make the news.  It's some random single server that crashes, and it's like, oh, okay.  Reboot.  So anyway, nothing much has come of that.  I just sort of thought I would follow up.



Oh, but there was one piece of nice work, a script that was produced by some guys at SentinelOne.  It's up on GitHub for anybody who wants it.  It's a free script, obviously, for Linux systems to both detect and protect the system from this problem.  That is, and you would use it before updating.  If for whatever reason you had a system that was exposed, you were aware of this problem, but it wasn't convenient for you to update the system, to have the server down at all and so forth.  You can run this script.  It makes a backup of the things it's changing so that you are able to revert it, if for any reason you want to.  It makes changes which do survive a reboot.  So unlike the little quick hack we talked about last week, the sysctl command that would transiently but not persistently solve the problem, this does.



So it's GitHub.com/sentinel-one.  And then once you're there you can see their sack-cve-fixer, as they call it.  And it's just a script you can run on Linux.  And it's got separate modes, but there is a check separate from the install.  And so it would certainly be handy.  This does a safe check to see whether your system has been patched, to confirm it, and/or whether it's vulnerable.  So even applying this fix, that is, using the install version, if you then run the check command afterwards, it says, oh, you're not vulnerable.  So it will basically check for vulnerabilities.  So might come in handy for anyone who's got a Linux system exposed to the public Internet.



And my favorite browser stumbled a little bit last week.  The good news is that zero-days in Firefox have been extremely rare.  Maybe that's because it's not as big a target as Chrome and Microsoft's browsers.  But the most recent previous Firefox zero-day was more than three and a half years ago, whereas we're pretty much covering zero-days in other browsers as often as not.  That one back in December of 2016 fixed a flaw that was at the time being abused to expose and deanonymize users of the Firefox-derived Tor browser.



But the bad news in this case, as I said at the top of the show, was that there were two zero-days being paired which were discovered being actively exploited against employees at Coinbase, the cryptocurrency exchange.  And actually they have evidence now of it being used against others, as well.  Philip Martin, who is a member of the Coinbase security team, reported the attacks to Mozilla.  And he said Monday a week ago, so Monday the 17th, he said:  "On Monday Coinbase detected and blocked an attempt by an attacker to leverage the reported zero-day, along with a separate zero-day sandbox escape, to target Coinbase employees."



And as I mentioned, what's a little disturbing is that Mozilla had been privately informed about this a full two months before, on April 18th, when Samuel Gross, a security researcher with Google's Project Zero, said that he reported this first of the two zero-days, and apparently Mozilla didn't do anything about it.  So it's when the Coinbase team reported the same bug being used in the attack against them that Mozilla said whoops and immediately pushed out a fix.  We're at Firefox 67.0.4, and that's after two updates.  They did one for each zero-day because they also only did - they only updated them individually.  So the problem was on 67.0.2 and earlier.  They fixed the first zero-day by going to 67.0.3, and then the second zero-day by going to 67.0.4.



So Philip said:  "We walked back the entire attack" - that is, after they realized what was going on and caught it.  He said that, if successful, the attackers could have gained access to their backend network and used this access to steal funds from the exchange, which is of course an outcome that we've been seeing recently because there have been lots of losses suffered by various cryptocurrency exchanges.  Once again, this kind of comes back to our noting that now it's gotten to be all about money these days.



He said:  "We walked back the entire attack, recovered and reported the zero-day to Firefox, pulled apart the malware and the infrastructure it was using for the attack."  He said:  "We are working with various organizations to continue burning down," he wrote, "the attacker's infrastructure and digging into the attacker involved."  He wrote:  "We've seen no evidence of exploitation targeting customers."  And he also added that other cryptocurrency-linked organizations have been targeted by the group, and they are being notified.



So the outcome was all good.  What's, again, disturbing is that I don't understand why Mozilla didn't jump on the report of a zero-day mid-April, which would presumably have prevented this from happening because this required the combined use of two zero-day exploits.  But for whatever reason, it's fixed now.  And so we are all at 67.0.4 and not subject to those targeted attacks.



LEO:  It sounds like the attack, maybe you know more detail, primarily was able to get the password stored within the browser.



STEVE:  That's correct, yes.  So they were...



LEO:  So that's a great attack, but it's a great reason, I mean, I would, I'm sure you agree, never use a browser as a password manager.



STEVE:  I really do.  In fact, I mentioned, I think it was two podcasts ago, I have a very good friend who's an ex-Microsoft employee who's involved now in the cryptocurrency business, who called me a couple weeks ago asking if I had contacts with the FBI because his Google account was hacked.  That allowed them to log into Chrome as him.  And once they synced, they had all, I mean, his entire username and password list that was all in Chrome.  And so, yeah, it's really handy to have your browser remember this stuff for you.  But as we've seen, in fact I told the story months ago that I was trying to log in on something to help Lorrie log in, and she didn't know what her password was.  I said, oh, let's go find out. 



LEO:  Just look in Chrome.



STEVE:  Exactly.



LEO:  It's not, you know, it's not obscured.  You just - yeah.



STEVE:  No.  It's right there.  And her mouth hung open.  She says, "Oh, my god, there's all my passwords."  I said, "Uh-huh, yeah."



LEO:  Well, you have to be logged in.  I mean, that's Google's explanation is, well, once people are logged into your account, then you're kind of in trouble anyway.  But most password managers will not do that.  Most.  And that's why browsers are not a good place, I think.



STEVE:  Yeah, yeah.



LEO:  I use a YubiKey with my LastPass because I don't want anybody to get into that, you know.



STEVE:  Yeah.  And, you know, I had a problem with LastPass a couple months ago where the authenticator stopped working.



LEO:  Oh.



STEVE:  Yeah.  It would, like, it would prompt me, and then immediately - it wouldn't wait for me to enter anything, would immediately close.  And that has been a problem that, I mean, I immediately, you know, after removing it and starting it up again and so forth, I couldn't get it to fix.  And when I did some googling, it turns out that's a problem that people have had before.



LEO:  I've had that problem with Google, as well, that for some reason - and I think it's just kind of the weird nature of using a USB authenticator with software, that maybe sometimes the software doesn't - because Google, same thing.  I put my key in, pressed the button, said we didn't see it.  What?  That's frustrating.  And maybe we need better hardware authentication methods.



STEVE:  But anyway, you're right.  What they were doing was they were using this to get out of the sandbox, to get into the browser, to get access to all of the stored passwords.  And of course they were hoping that, or maybe they knew, that the people they were targeting were Firefox users.  But that was the nature of the attack was to try to get a hold of passwords stored in the browser, which as we've just said is not a really good place to keep your passwords.



Interesting story, it was in, oh, it's Wirecutter who ran this.  And before I go any further, I'll make it clear that this was a mistake that Google did promptly fix, pushed out an update, and the problem has been quickly resolved.  But for some time before Google was made aware of the problem, and we don't know how long this has been the case, it was true that someone purchasing a previously owned and fully factory reset Nest Cam could still be watched by the camera's previous owner.



Last Wednesday, Wirecutter ran the story with the headline:  "Buyer Beware:  Used Nest Cams Can Let People Spy on You."  They said:  "We've explained before that when you're selling or giving away your old smart home devices, it's critical to do a factory reset on them first in order to protect your data and privacy."  You know, very much like we would wipe our phone or wipe a hard drive when giving a computer away.  They said:  "We've recently learned, however, that even performing a factory reset may not be enough to protect privacy for owners of the popular Nest Cam Indoor. And in a twist, this time the risk is on the side of the person receiving the device, not the person disposing of it."



They said:  "A member of the Facebook Wink Users Group discovered that, after selling his Nest Cam, he was still able to access images from his old camera, except it wasn't a feed of his property.  Instead, he was tapping into the feed of the new owner via his Wink account.  As the original owner, he had connected the Nest Cam to his Wink smart home hub.  And somehow, even after he reset it, the connection continued."



They said:  "We decided to test this ourselves and found that, as it happened for the person on Facebook, images from our decommissioned Nest Cam Indoor were still viewable via a previously linked Wink Hub account; although, instead of a video stream, it was a series of still images snapped every several seconds.  If you buy and set up a used Nest Indoor camera that has been paired with a Wink hub, the previous owner may have unfettered access to images from that camera.  And we currently don't know of any cure for this problem.  We are unsure what further implications there may be regarding Nest's video service, including whether it may be vulnerable to other methods or through other smart home device integrations.  We're also unsure whether this problem affects the entire Nest lineup, including the Nest Cam Outdoor, Nest Cam IQ Outdoor," blah blah blah.



Anyway, they finish, saying:  "According to Google, the issue has now been fixed."  And actually, they also, in an update to their initial reporting, retried all of this and confirmed that it had been immediately fixed and pushed out.  They said:  " When we asked about how the company corrected the error, a representative said:  'We usually don't share how a fix was pushed out for various reasons.  The statement is our update of record for this.'  Although this particular bug has been fixed, we advise anyone interested in smart home gear to be especially cautious when considering buying or selling used items, especially ones that have the potential to interfere with your intimate privacy and security, such as cameras, devices with microphones, and smart locks."



And it's sort of chilling.  I mean, when you think about it, what's going on with the cloud and with connectivity is a black box.  I mean, we get these things.  We follow the instructions.  Everything is kind of done for us; right?  It's automatic.  It's scan the QR code on the base of the device or press a button, spin around three times, click your heels, and do two Hail Marys.  And, oh, look, it's all connected, and things are linking up, and lights are blinking.  I mean, we have no idea what is actually going on.  And so we have no control over these systems, and it's very clear that, in this case even, I mean, the device has probably some, obviously has some factory burned-in serial number that is not being randomized by a full factory reset.  Something somewhere held onto that.



LEO:  Well, but - so I'm thinking, because it's associated with the Wink, right, which is a third-party device, it only did it if you used a Wink Home Hub with your Nest Cam, is my understanding.



STEVE:  Okay.



LEO:  So what I'm thinking is the problem, and of course we don't know because they won't say, is not with the Nest Cam particularly, but maybe with the Wink?  That the Wink authenticated the Nest Cam via, as you say, a hardwired serial number or something that identified it, that a reset of the camera alone wouldn't reset.  But when you then put that same camera on a Wink somewhere else, it would go, oh, I know you.



STEVE:  Yeah.  Although...



LEO:  Do you think it was like that?



STEVE:  Although Google did fix it.  So...



LEO:  So that's true, yeah.



STEVE:  They, like, said, "Oh, crap," and immediately, you know.  So it was something that they were doing that, I mean, probably...



LEO:  Yeah, okay.  But it did require the complicity of a Wink hub.  So I don't know.  You know?



STEVE:  Yeah, yeah.



LEO:  Maybe they called Wink and say, you know, "We're going to change it on this end."



STEVE:  That may be.  Or maybe all of the cameras do feed through Google's cloud and then go back out.  I mean, again, see, and this is my point:  It's magic.



LEO:  It is.  That's the problem.



STEVE:  Once upon a time - yes.  Once upon a time, back when you and I had, well, actually you still have hair. 



LEO:  Yeah, hey.



STEVE:  There was, okay, there was more pigment in our hair.



LEO:  Yes.



STEVE:  The connections between these things were explicit.



LEO:  You understood them, yes.



STEVE:  Yes.  We knew what was going where.  And now it's like, oh, I just pressed a button.  Look, it works.  It's like, yeah.  But what happened?  No one knows.



LEO:  Well, and Stacey Higgenbotham and others have written long articles about how complicated and sometimes frustrating it is to decommission your smart home before a move.  Stacey just moved, and it was a big deal.  There was a lot to do.  I think Georgia Dow is writing the same kind of pieces.  It's just complicated.  It's not as easy as it sounds.



STEVE:  Yeah, I would get one of those - they're not really steamrollers.  I don't know why they call them "steamrollers."  I guess maybe once upon a time they...



LEO:  You and I call them "steamrollers" and "steam shovels."  And they haven't been steam in a while.  Not even in our lifetime.



STEVE:  Put all of your IoT stuff down in front of it and just roll it forward.



LEO:  That's what I think.  Do not resell your IoT stuff.



STEVE:  Because what are you going to get, 30 bucks for something?  Or 20 bucks on eBay?  It's like, not worth it.  Just enjoy the sound of the crunching plastic.  Just...



LEO:  I have one of those Nest Cams right here, right behind me.



STEVE:  Yeah, I wonder, yeah.  Oh, well, okay.



LEO:  So they're accessing the old recordings, obviously, not, I mean, because you don't have the camera anymore, so it's the old stuff that they're finding.



STEVE:  No, no, no.  It was fresh.  It was the buyer's feed taken as a series of stills.



LEO:  From a different camera.  The buyer.  Oh, I get it.  So the older owner could see the new camera's output.



STEVE:  Correct.



LEO:  Got it, got it, got it.



STEVE:  Correct, yup.  And it's funny, too, because Mark Thompson and I were chatting a couple weeks ago, and he just happened to talk about the Vector, the cute little kind of steam shovel robot thing.  And he had talked about...



LEO:  Which also is out of business, by the way.



STEVE:  Which, yes.  These little home robotics things...



LEO:  They go fast.



STEVE:  They're not lasting very long.



LEO:  No.  This was cute, too.  I loved it.



STEVE:  It was.  It was.  And to her credit, Lorrie, when I explained that it had a camera, and actually it was doing facial recognition - because you could say, "Hey, Vector, who am I?"



LEO:  Right.



STEVE:  And it would [sound effects] and say "Lorrie."  Anyway, she was very uncomfortable by the idea that this thing had a camera in it.  And I said, "Well, honey, it's on the kitchen counter.  It's not like it's in the bedroom or anything."



LEO:  It just shows you normal people have good instincts.



STEVE:  Yes, exactly.  It's like, eh, let's give this to somebody else.  So anyway.  So I guess the real takeaway, and I'm glad we talked about this, Leo, because it's worth noting that things that we don't understand - and really, I mean, it's been taken from us, like any sense of what's going on.  It's like, ooh, magic.  And it's like, yeah.  But, boy, are you having to trust all kinds of, well, how can you trust?  I mean, you can hope.  And, you know, we believe that Google would never intentionally do this.



LEO:  Oh, I'm sure not.



STEVE:  But, you know, all those streams are going off to some cloud somewhere, and so, yeah.  I mean, you don't have control over it.



LEO:  And it's hard to do it perfectly, as we've - this show is essentially the illustration of that.



STEVE:  And we bring it to you gleefully every week.  If this doesn't raise your blood pressure...



LEO:  On with the show.



STEVE:  So we talked about, last week, the exfiltration of very sensitive keys, which was done as a proof of concept by the RAMBleed guys.  And so what they did, remember, was that by figuring out how they could pound on their own memory after noticing that the probability of flipping their own bits was influenced by the bits on either side, they figured out how to bring another process's secret into alignment in the DRAM grid so that copies of it were on either side of the row of memory they control.  And then by examining the probability of their own bit flips, they were able to infer the bits of secret data.  So that's not good.



And what they did, as I said, as a proof of concept was they exfiltrated an OpenSSH key from a process they didn't control that was sharing their same server as theirs.  So last Thursday, June 20th, Google security researcher Damien Miller, who's one of the top OpenSSH and OpenBSD developers, added protection against any and all forms of side-channel attacks by leaving the OpenSSH private keys encrypted until they are needed.



And what's weird is that last week, last Tuesday, in the context of talking about RAMBleed, I was just talking about exactly this strategy.  I noted that my own SQRL client for Windows never leaves its keys in the clear in RAM.  The user's password, you know, the one password you use to tell the SQRL client that you are you, that is used to transiently decrypt the keys briefly for the transaction, after which the plaintext decrypted versions are zeroed in RAM so that they are never sitting around available to be exfiltrated.  This is clearly the right way to design code for maximum safety in a hostile environment.



So the good news is the OpenSSH project has received this protection.  According to Miller, OpenSSH will encrypt the secure shell private keys while they are at rest inside a computer's RAM.  If an attacker manages to extract data from a computer or server's RAM, they will only obtain an encrypted version of an SSH private key, rather than the cleartext version.  He indicated that this protection would be able, and I agree, to stop all manner of side channel attacks - Spectre, Meltdown, Rowhammer, RAMBleed, and so forth.  And it's an interesting commentary that we now have so many side channel attacks available. 



But what I thought was so cool, Miller had an additional challenge that I did not have when doing something similar with my own SQRL client design.  The advantage I had was that the knowledge of the key to decrypt the master did not need to exist in the SQRL client since the user would be the one providing that missing data when it was necessary to perform the decryption.  But that wouldn't work for OpenSSH because the system must be able itself to autonomously decrypt the SSH keys on the fly without user input whenever they are needed.



Miller's solution was clever and elegant.  The comments in his code commit read:  "This change encrypts private keys when they are not in use with a symmetric key that is derived from a relatively large prekey consisting of random data, currently 16KB."  So 16KB of data.  I guess it's bytes.  That would make more sense than bits because you want to have a lot.  



He said:  "Attackers must recover the entire prekey" - and I put in brackets "perfectly," and I'll explain why in a second - "before they can attempt to decrypt" what he calls, he uses the term "shielded," as he kind of invented a new term, "the attempt to decrypt the shielded private key.  But," he says, "the current generation of attacks have bit error rates that, when applied cumulatively to the entire prekey, make this unlikely."  And I would say way more than unlikely.



I didn't dwell on the details last week.  But even in their research, the research paper, that successful recovery of the OpenSSH key, which was used to demonstrate a successful RAMBleed attack, it strongly depended upon the use of some very powerful algorithmic post-processing to perform bit guessing among the always somewhat uncertain recovered bits.  In other words, RAMBleed returns bit probabilities, not bit certainties.



So it turns out that there are some clever algorithms that can use the known properties of the relationship between the public and private key components.  Well, for example, we know that a private key is the product of two primes.  So it turns out just knowing that allows you to immediately exclude many bit combinations that break that assumption.  So they were able to use these algorithms to essentially turn the probabilities that they were getting from RAMBleed into enough certainty that they were ultimately able to recover the OpenSSH key.



But none of that would work against a purely random 16KB prekey, where every single bit has to be exact because it's high random.  I mean, it's pure entropy.  There's no assumption of interbit relationships, which you do have with an RSA key used by OpenSSH.  And presumably he takes this - I didn't look at the code, but he probably takes this 16KB prekey and then hashes it in order to get the key, which is then used to statically decrypt the OpenSSH key on the fly.



So it's just - it's brilliant.  Basically, he very cleverly leverages the fact that any of these RAMBleed-style side channel attacks, whatever they are, they're basically getting statistical guesses of bits, and at a relatively low bit rate, and by locking the decryption of the in-RAM OpenSSH key to a large prekey where not a single bit can be off, or when you hash it you're going to get, as we know, something completely different.  And that will not symmetrically decrypt OpenSSH.  And this is all still very fast.  So you can afford to do it on the fly.



Anyway, very cool solution.  And that has been committed into the OpenSSH project as of last Thursday.  So it is available to anyone who updates their builds of OpenSSH.  And I'm sure it'll be moved into various ports and available in Linuxes and Unixes and so forth.  Anyway, he somewhat modestly calls this elegant process "shielding."  And he said:  "Implementation-wise, keys are encrypted shielded when loaded, and then automatically and transparently unshielded when used for signatures or when being saved or exported."  Oh, and he also noted, he says he hopes they'll be able to eventually remove this special protection against side channel attacks in a few years, when computer architecture has become less unsafe.



So he recognizes, and we've talked about this, that we're sort of going through this awkward phase where the cleverness that we were using to accelerate our systems turned out to have lots of little edge cases, as we've been talking about now for about a year and a half, ever since Spectre and Meltdown hit.  Well, actually even before that because the DRAM attacks, the Rowhammer attacks, that goes back to at least 2017 or earlier.  So anyway, just a very cool piece of work.  And it was nice to see a response from the developer community that gave us a really practical solution to these problems.



So a BlueKeep patching status update.  Of course we know that BlueKeep is the very bad authentication bypass attack against RDP.  Despite the fact that everybody has been calling it "wormable," because it is, we've seen no worms.  And I've argued that we probably won't because you don't need a worm.  There is a worm against the Exim email server vulnerability because it takes a week of camping out on an email server to cause the right combination of timeouts and retransmissions in order to execute commands on the vulnerable server.



So it totally makes sense that you'd have a squad of worms out there looking for vulnerable Exim servers, setting up long-term persistent connections, and then waiting a week while they dribble out a byte every four minutes to keep the connection from getting dropped over the course of that time, and then getting their exploit to happen.  And that is in, I mean, we know from last week that that is happening.



So where are we with BlueKeep?  The Patch Tuesday of May, when this happened, was - what was Patch Tuesday?  I don't have the date in front of me.  Anyway, it was Second Tuesday of May.  By the next to last day of May, so 16 days after that Patch Tuesday, Raviv Tamir, who is the group program for Microsoft's threat protection, 16 days after Patch Tuesday he tweeted:  "My dashboard remains bleak, as only 57% of exposed machines I see worldwide have patched for CVE-2019-0708," which is this RDP exploit.  And he said, in all caps:  "GO PATCH."



Okay.  So then the following Wednesday, June 5th of this month, Raviv updated his numbers, tweeting "Numbers are going up, now at 72.4% worldwide.  That's better, but still not good enough.  KEEP PATCHING," in all caps.  And then this past Thursday, June 20th, so another 15 days after the previous one, he updated again with a tweet:  "Another update.  Worldwide update rate for CVE-2019-0708 numbers are up to 83.4%."  And then "KEEEEP PATCHING" was his final.



LEO:  Like the people who aren't patching are reading him.



STEVE:  Exactly.  Exactly.  It's like, okay.  I think, I mean, what we're seeing is probably Microsoft's updates are hitting machines that are not rebooting very often.  And so they're rebooting, and they're doing their updates, and then they're disappearing from his radar. 



LEO:  I'm glad there's progress being made.  That's a good sign.



STEVE:  Yes.  That is a good sign.  BleepingComputer reached out and asked Raviv about the number of computers that continue to be vulnerable to BlueKeep.  And what he's looking at is his Microsoft Defender ATP, the Advanced Threat Protection network that they've got.  And he said, while it is a lot better, there are still several million machines out there.  And as we know, DHS confirmed that even Windows 2000 is vulnerable to this.  Well, it's not getting any updates.  And Microsoft did make that Windows XP update available, but that's not auto updating; right?  You've got to go get that.  So there are, like, old, creaky systems that are not going to get themselves fixed.  And they're going to get themselves owned here, probably, before very long.



Okay.  Yesterday morning, 3:00 a.m. Pacific time to about 6:00 a.m. 



LEO:  This is my favorite story. 



STEVE:  Oh, Leo.



LEO:  Oh, man.



STEVE:  Yeah.  About 2% of the global Internet was mistakenly routed through a Pittsburgh, Pennsylvania steel mill.



LEO:  Say that sentence again.  How much of the Internet?



STEVE:  Yeah, about 2%.  Think about that.  One out of 50.  Two percent of the Internet was mistakenly routed through a steel mill in Pittsburgh, Pennsylvania.  And of course that didn't turn out so well.  Okay.  So first of all, to get a little - okay, wait.  I'm going to share what The Register said because they're always irreverent.



LEO:  Snarky, yeah, yeah.



STEVE:  Yeah, snarky.  I mean, they redefine the term "snark."  They said:  "It all started when new Internet routes" - no, actually I should explain first so that we have a context.  So remember that the way that the global Internet works - and this is not our routers, our little NAT routers that we have at home.  They share the term "router" with the so-called "big iron routers" that are out on the Internet, actually moving all of this traffic around between ISPs.  They've got multiple interfaces on them with packets coming in and going out.  And every one of these big routers is a routing table that basically knows, for any packet that comes in, which interface to forward the packet to, that is, like, towards its destination.



So a router that belongs to Level 3 will receive a packet, and it's connected to a bunch of other high-level ISPs, you know, backbone, Internet backbone Tier 1 providers.  And so it looks at the destination IP.  And we know how Internet routing happens where the most significant bits are the network, and the least significant bits are the machine on the network.  And there's a mask that is used in order to figure out where this should go.  So the routing table does that work.  It looks at the destination IP, consults its table, and then says, okay, the ultimate destination is somewhere down that wire.



And so it puts the packet on the wire and off it goes, and it's done its job.  And so the recipient router does the same thing.  The idea is the packet keeps getting closer and closer - well, actually, hopefully, not in this case, but normally - until it finally gets to its destination.  So the way these tables are managed, because you can't do this by hand, I mean, it's just not possible, is this protocol that we've talked about from time to time.  And it always comes up when something breaks like this:  BGP, Border Gateway Protocol.  That's the communication among the routers which allows them to share their routing tables and to pass updates about the way the Internet is connected essentially among themselves.



And the term that we'll hear as we talk about this is "advertising a route."  It's kind of a weird term.  But the idea is that a router that has a customer connected to it will advertise that block of IPs to its peers, to its peer routers.  And that informs them that it should receive traffic for its customer, for the littler guy that's hooked to it.  So when any of the peer routers receive a packet with that network, bits at the high end of the IP, they should send it to it; right?  So again, it advertises the things it wants to receive, on behalf of its customers, to its peer routers.  And it uses the BGP protocol to do that.  



Okay.  So you could imagine the problem if a router for some reason advertised a whole bunch of networks that it didn't actually have a right to advertise.  That is, it didn't actually have customers connected to it that it could route that to.  That is, if it were advertising networks that belong to other people.  That's really bad.



So The Register writes:  "It all started when new Internet routes for more than 20,000 IP address prefixes" - now, that is to say 20,000 IP networks, right, roughly 2% of the Internet - "were wrongly announced [advertised] by regional U.S. ISP DQE Communications.  This announcement informed the sprawling Internet's backbone equipment to" - this is The Register I'm reading - "to thread netizens' traffic through DQE and one of its clients, steel giant Allegheny Technologies, a redirection that was then, mind-bogglingly, accepted and passed on to the world by Verizon, a trusted major authority on the Internet's," as they write, "highways and byways.



"This happened because Allegheny is also a customer of Verizon.  It, too, announced the route changes to Verizon, which disseminated them further.  And so systems around the planet were automatically updated; and connections destined for Facebook, Cloudflare, and others" - and actually a lot of others, including Amazon - "ended up going through DQE and Allegheny, which buckled under the strain."  No doubt.  No kidding.



LEO:  Because Cloudflare, is it their traffic, their Internet protection traffic, all of that's going through there?  I mean, Cloudflare's not just, like, your average Joe on the street.



STEVE:  Right.  They have 20 million websites.



LEO:  Yeah.



STEVE:  And so many of their customers were advertised as being located at Allegheny Steel Plant in Pittsburgh.  I mean, so that's exactly what happened.  This said to Verizon that Cloudflare's and Amazon's and Facebook's customers, or their networks, were actually located here in Pittsburgh.  So send all the traffic there.



LEO:  It's got to be frustrating because Cloudflare - and I know their CTO, John Graham-Cumming, and I know how committed they are to what they do.



STEVE:  Oh, yes, quality service.



LEO:  And I had chills when I saw Cloudflare is down.  This was before we knew why.  I thought, that, of all the companies in all the world, that's the last company I'd ever expect to have an outage.  Even less so than Google.



STEVE:  And think about it.  There was nothing they could do.



LEO:  Exactly the point, is that's got to be terrifying to them.



STEVE:  Yes.  And in fact he was rather miffed.  John Graham-Cumming was tweeting, but also Matthew Prince, the CEO?



LEO:  CEO, yeah.



STEVE:  Yes.  He tweeted:  "It's networking malpractice that the NOC" - that's the Network Operations Center - "the NOC at Verizon has still not replied to messages from other networking teams they impacted, including ours, hours after they mistakenly leaked a large chunk of the Internet's routing table."



LEO:  Just horrible.  Just horrible.



STEVE:  So here was, what, like, people were frankly astonished that Verizon would not have filters because what came up their wire from their customer, because this Allegheny is dual-homed - meaning that Allegheny was served both by Verizon and DQE.  So DQE was actually the source of the problem.  DQE was running a BGP optimizer, which is some software designed to improve their routing tables.  Well, it went wacky.  It, like, broke.  And so it provided a bunch of these bad routes to the customer they share with Verizon, Allegheny, which then forwarded them to Verizon.



Well, Verizon should and could absolutely know that these were bogus, that there was absolutely no way that Allegheny could own these routes.  And so this is called "route filtering."  And it should have been in place.  And people could not believe that a major Tier 1 entity, backbone Tier 1 provider like Verizon would blindly accept these routes and propagate them out onto the Internet.  And they're inherently trusted, so everyone believed them and sent all the traffic there.



LEO:  This is the ThousandEyes diagram of that happening.  You know, our sponsor, ThousandEyes, this is what they do is they monitor this stuff.  Do you think Cloudflare - user locations on the left side are experiencing high packet loss when trying to reach the Cloudflare CDN.  This is all the packet loss here.  Do you think Cloudflare looks at that and goes, oh, a BGP leak?  Or is it harder to figure it out?



STEVE:  I looked at the timeline from John, and it looked like it took him about maybe 15 to 20 minutes, which is really pretty quick.  Because, I mean, it could be - the problem could be anywhere.  But we would argue that, and we do, always say anybody can make a mistake.  You know?  Stuff happens.  So, yeah.  That could happen.  But what is unconscionable is that there was nobody at Verizon responding.  That is, I mean, if you're Verizon, and you're a trusted Tier 1, you have to have somebody who picks up the phone when Cloudflare calls.



LEO:  Yes.  Yes.



STEVE:  And says, "You know, you're advertising a bunch of routes that are ours.  Knock it off."  And it took quite a while for that to get cleared up.



LEO:  A great piece on ThousandEyes about kind of the whodunit.  And I'm just shocked that this is still possible.  That's the thing that blows my mind.  How can this - we've known about BGP leaks for, I mean, we've been talking about this for a decade.



STEVE:  Yes.  Forever.  It has always been possible.  The problem is we have a bunch of infrastructure in place, and it works kind of like well enough; you know?



LEO:  Yeah, yeah.  Well enough.



STEVE:  It's like when a sinkhole opens in the middle of an intersection in a major city, and cars fall in.  People go, oh, my god, our infrastructure is breaking.  But, you know, they plug up the hole, and everyone keeps driving.  No one actually fixes the underlying problem.  We just sort of go, oh, well.



And so on the Internet we have a problem.  I mean, this is a, you know, and here we are.  Once upon a time when it was email, well, email didn't care.  It would just wait until the connections came back up, and then your mail would get delivered a little bit later.  And you'd click refresh and be thinking, oh, you know, they said they sent me that email.  Then it would come through, oh, look, your email came.  Well, that's not good enough anymore because imagine the serious business use of the Internet and what an outage for three hours does if you're doing real-time stock market trading or managing a nuclear reactor.  Oops.  And, you know, it could actually matter.



LEO:  This is Chernobyl-level incompetence on the part of Verizon, honestly.



STEVE:  It really is.



LEO:  This is the Cloudflare blog.  "A BGP session can be configured with a hard limit of prefixes to be received so the router can decide to shut down if the number goes above that threshold.  Had Verizon such a prefix limit in place" - which is, I gather, a standard practice - "this wouldn't have occurred.  It doesn't cost a provider anything to have such limits in place."  And here's the payoff quote:  "There's no good reason other than sloppiness or laziness that they wouldn't have such limits in place."



STEVE:  Yup.  You absolutely want to put sanity testing on BGP.  And it's easy to do.  So the router says, whoa, I mean, here was 20,000 new network prefixes, bang, that arrived.  And it said, oh, okay.  What?  You know, that never happens.  Yeah, yeah.  So anyway, I mean, the good news is each of these events raises the pain threshold.  And so what we have, I mean, even the idea that we're rate-limiting BGP updates, well, that's clearly a patch.  I mean, that's not the right solution, to rate-limit the updates.  It would have stopped this, but it wouldn't keep, for example, a single malicious update, a deliberate malicious update, from happening.



So we need a system that is able to properly verify the changes that are always occurring as one entity moves from one ISP to another, for example.  And we don't have that yet.  So we'll get there.  I mean, we have a system which works, and it's amazingly resilient when you consider how long ago it was designed and how well it has withstood everything that we've been throwing at it.  But it's not perfect.



LEO:  The Register said normally if a little company in Pennsylvania announced it owned the Internet, you would have some protections in place.  You'd filter that out.  But for, well, how many hours?  Was it eight hours?  It was a while.  This was...



STEVE:  No, no, it was only three hours.



LEO:  Okay, only three hours.



STEVE:  From about 3:00 a.m. to 6:00 a.m.



LEO:  The longest three hours of John Graham-Cummings' life.



STEVE:  Boy, yeah.



LEO:  Oh, god.



STEVE:  NASA, as I mentioned, was infected by an APT for more than a year.  They've tracked it down to an unauthorized Raspberry Pi, which someone at JPL, actually, was where the problem was, connected to their network.



LEO:  It was probably monitoring the coffee pot or something.



STEVE:  Yeah, exactly.  It was probably doing who knows.  Because, you know, JPL is a freewheeling place.  I mean, they do beautiful work, but they're, you know...



LEO:  No, I don't want to have to go down the hall to see if the coffee is fresh.  I'm just going to put a Raspberry Pi in here.



STEVE:  And we'll stick a little webcam on the Raspberry Pi so I can just get a picture of the coffee pot and see what the water level is.



LEO:  And, oh, by the way, we've got a network.  I'll just put in the network.



STEVE:  Why not, Leo?  



LEO:  Why not?  Why not?



STEVE:  Yeah.  So in a report published last week by NASA's OIG, the Office of Inspector General, in that report it revealed that in April of 2019 hackers breached the agency's network and stole - they did, I mean, this was an active threat - stole approximately 500MB of data related to Mars missions.  Because of course that's what JPL does.  The point of entry was a Raspberry Pi connected to the network at, as I mentioned, JPL, without authorization or going through the proper security review.  I mean, they have systems in place where you're supposed to log and have approved anything connected to the network.  But hey, look, there's an RJ45 port.  Let's plug in the Raspberry Pi.



According to this 49-page OIG report, the hackers used this point of entry to move deeper inside the JPL network by hacking a shared network gateway.  The hackers used this network gateway to pivot inside JPL's infrastructure and gained access to the network that was storing information about NASA JPL-managed Mars missions, from where they exfiltrated information.  Quoting from the report:  "The attacker exfiltrated approximately 500MB of data from 23 files, two of which contained International Traffic in Arms Regulations information related to the Mars Science Laboratory mission."  The Mars Science Laboratory is the JPL program that manages the Curiosity rover on Mars, among other projects.



JPL's primary role, as we know, is to build and operate planetary robotic spacecraft such as the Curiosity rover and various satellites that orbit planets in the solar system.  JPL also manages NASA's Deep Space Network, which is the worldwide network of satellite dishes which are used to communicate with NASA spacecraft during their missions.  Investigators said that, besides accessing the JPL's mission network, the April 2018 intruder also accessed the Deep Space Network's IT network.  Upon discovery of the intrusion - I got a kick out of this - several NASA facilities immediately disconnected from JPL and DSN.  It was like, agh!, you know, just pull the plug.  Do not connect to JPL, those crazies out there in California, in Pasadena.  We don't know what they've got crawling around their network.  And they disconnected fearing, of course, that the attacker might pivot into their systems, as well.



NASA's OIG said:  "Classified as an advanced persistent threat, the attack went undetected for nearly a year, and the investigation into this incident is ongoing."  The report blamed, and here it is, JPL's failure to segment its internal network into smaller segments.  And of course, as our longtime listeners know, we've talked about the need for strong network segmentation for several years, even at home and in small offices.  The reason we liked that amazing little Ubiquiti five-port router so much was that for $49 they contained five entirely separate NIC interface adapters, which supported strong network segmentation.  The downside was that they were a beast to configure.  They had a bizarre configuration language.  But many of our listeners were able to get them working and use them in that way.



But of course the reason JPL probably didn't bother with network segmentation is the same reason that most home and small offices don't bother.  It's a pain in the butt to maintain separate networks.  It's so much easier to have everything able to talk and see each other on the network.  But of course that's exactly the problem because, once anything malicious gets onto your network, then it also can talk to and see everything else that it then has access to.  So, not good.



I said I would talk about, and there's some fun data here, about the status of ransomware because what we are seeing is that hackers are now pursuing money.  Riviera Beach, Florida, I think it was - where was it?  I have the data here.  Ah, last Monday evening.  Okay.  Riviera Beach, Florida, is coughing up $600,000, Leo, to hackers after a ransomware attack brought down its computer system.



LEO:  It's a town of 35,000 people.



STEVE:  Yes.  Small town.



LEO:  Like 20 bucks per person.



STEVE:  Small town hit by a ransomware attack at the end of May, on May 29th, after a city employee clicked on a malicious link in an email, according to local reports.  Attackers behind the malware, which then spread throughout the city's network and shut down its computer systems, asked for a ransom of 65 bitcoin.  And it's funny because, well, not funny.  But I meant to go look and see what that would be worth now because I'm not sure when that price was set.  But bitcoin has been enjoying a recent resurgence.  That's like more than $11,000 per coin.  So I'm not sure what, I mean, 65 bitcoins is going to be more than 600,000.  Anyway, in exchange for unlocking computers.



Basically, the systems controlling the water utility were offline.  Government email and phone systems were nonfunctional.  911 emergency calls could no longer be entered into computer databases.  According to local reports, the computer systems controlling city finances and water utility pump stations are now at least partially back online.  So last Monday evening the City Council voted unanimously to authorize its insurer to pay the $600,000 ransom.  So they were insured. 



The security community, for its part, has argued that the city is taking a big gamble.  Remember that we're dealing with criminals here; right?  So the fact that they make the payment doesn't necessarily mean that they're going to get their data back.  On the other hand, it's useful to remember also that future victims' willingness to pay ransom will be a function of this outcome.  So the bad guys who receive $600,000 as a consequence of some city employee clicking on a link, it's in their interest to provide the data to allow the city to recover their computers because these bad guys want to get paid rather than not.



The technical program manager at HackerOne, and we'll be talking about HackerOne at the end of the show, said the Riviera Beach City Council has taken a big gamble by paying the ransom as there are no guarantees the attackers will return any of the data, which could leave the city in an even worse situation.  "By paying the ransom," this person writes, "the council also encourages more of these types of attacks as it makes it more profitable for attackers."  And of course the FBI is formally on record saying do not pay.



Well, that's easy for the FBI to say.  Last year, several Atlanta, Georgia city systems were crippled by a ransomware attack.  A ransom of $51,000 was demanded for recovery.  Atlanta said no thanks, and ended up spending $2.6 million in recovery costs, including incident response and digital forensics, additional staffing, and Microsoft Cloud infrastructure expertise.



Now, certainly, had they said yes to the ransom, it's not like paying $51,000 would have instantly brought all their systems back.  So there would have still been substantial cost over and above paying the ransom.  Hard to say how much.  But the point was they said no to $51,000 ransom, and it cost them $2.6 million to fully recover.



Baltimore, Maryland, another victim, was hit in May.  The attack halted city services like water bills and permits and more.  The ransom demanded was $76,000.  Baltimore also said no, and ended up spending a rather staggering $18.2 million in restoration costs and lost revenue.



LEO:  Whoa.



STEVE:  So, I mean, I guess, you know, putting it into context, it's clear that, as I said, even if you pay the ransom and you get the ability to decrypt your computers, it's still going to - there's a lot of remediation that goes into performing the decryption, getting rid of the gunk, and then how do you know your systems are clear?  I mean, you know, you're still going to be hurt.  But ouch for, you know, 18.2 million in restoration costs.  So many people have said that obviously, I mean, it's easy to say don't click that link in email.  And it really does look like these cities are going to be needing to be in a position where, if something like this hits them, they are able to recover themselves in a more timely fashion.



Also I did want to note real quickly that Microsoft's project to bring their Chromium-based Edge browser has now reached Windows 7.  It is only available in the Canary channel, which is the most bleeding-edge channel, not yet dev and beta.  But if you go to MicrosoftEdgeInsider.com, you can now download the installer that runs under Windows 7 and bring the Edge version of the Chromium-based browser to a Windows 7 system.  I haven't done it yet, but I am sitting in front of a Windows 7 machine, and I will probably do that.  Seems like a good thing.



One quick little bit of closing the loop.  A listener, Hafthor, wrote:  "I think you might be wrong about worming the RDP bug."  He said:  "The advantage would be to gain access to machines on the LAN to have them mine, as well."  And that's, Leo, that's a point you have made a couple times is that using these things to get inside your LAN is an advantage.  I would argue that, if you're trying to use RDP and this bug in the LAN, it's no more necessary than it is on the Internet.  That is, the reason the worm makes sense for Exim, and thus we see one that is out on the Internet now attacking Exim mail servers, is that it takes so long to make this happen.  After you find the server, it takes a week of a persistent connection.  So you want somebody else's computer to be doing that, not your own.



So a worm is perfect.  Here, it's an instantaneous, get it onto the system, with an authentication bypass for RDP.  So in the same way that you can immediately get those Internet-exposed systems, so, too, could you get the internal network-exposed systems just by scanning the LAN that your inside interface is connected to.  So for what it's worth, I mean, I guess I take his point.  But I still don't think that a worm makes sense.



LEO:  Okay, Steve.



STEVE:  So as we know, we've talked about it in several different contexts, bug bounties have become a permanent feature of today's software and system security ecosystem.  We have fun every year talking about the Pwn2Own competitions, a white hacker competition sponsored by large sponsoring companies, some of whom have their products hacked and their defects then responsibly disclosed before they are publicly known, and that way they're able to be repaired.  Then we have the somewhat sketchy outfits such as Zerodium, whose tagline, a little bit chillingly, reads:  "The leading exploit acquisition platform for premium zero-days and advanced cybersecurity capabilities."  And as we know, they pay big bucks for big exploits, which they presumably resell to major player state-level actors, foreign and domestic law enforcement and intelligence services, and we don't really know who.  So there are those categories.



But the other one is white hat bug bounty clearinghouse middlemen, and the premier one is HackerOne.  They sort of serve as a matchmaker between those who put up bounties for the responsible discovery and reporting of bugs in their own products, and the hackers who enjoy finding them and reporting them and are motivated by those bounties.  So in contrast to Zerodium's pitch, HackerOne states:  "More Fortune 500 and Forbes Global 1,000 companies trust HackerOne to test and secure the applications they depend on to run their business."



HackerOne reports that, for organizations that found vulnerabilities before they were exploited using HackerOne, Forrester found benefits of up to $1.6 million, and an ROI, a return on investment, of up to 646%, meaning that the economic benefit to these large companies of having bugs found before they're exploited is, like, way pays back the bounties that they are offering.  And I've got a list of the Top 20 bounty payers that we'll be going through in a second.  But I wanted to mention that HackerOne touts, they say:  "From implementing the basics of a vulnerability disclosure process to supercharging your existing security programs via a bug bounty program, HackerOne has you covered."  And the other thing they say is more security teams use HackerOne to manage vulnerability disclosure and bug bounty programs than any other platform.



So the beauty of this is it's certainly possible for an organization to set up their own bug bounty program.  But I can really see the advantage of certainly HackerOne is a commercial operation.  They're going to get a piece of the action.  But on the other hand, they sort of form a central clearinghouse that makes it much easier for a company to say, okay, we want to establish a bug bounty program.  We're simply going to register ourselves with HackerOne, let them manage it for us.  Essentially outsource that whole process.  Okay.  So in their promotional material they quote GM, General Motors' Vice President of Global Cybersecurity saying:  "Hackers have become an essential part of our security ecosystem."



Okay.  So I mentioned the Top 20.  Interestingly enough, we were just talking about Verizon.  At the head of the list of the Top 20 biggest, fastest moving, and most lucrative - oh, oh, oh, I forgot.  I have a link in the show notes here to a page that is titled "Hactivity" at HackerOne.  And when I brought the page up - and in fact I'll just refresh it now.  Leo, if over in the upper left you click on "New" rather than "Popular," and then "Bug Bounty," for example, about an hour ago there is one of HackerOne's clients, Deliveroo, paid out a $500 bounty.  GitHub, three hours ago, a $2,000 bounty.  Four hours ago, GitHub, $617.  Also four hours ago, $617.



LEO:  This is actually scary.



STEVE:  I know.



LEO:  These are all serious, I presume, bugs.



STEVE:  Yes,  yes.  In fact, if you scroll down a little ways, there's PayPal, eight hours ago, paid $3,200 for a bounty that was found.



LEO:  Wow.



STEVE:  Spotify.



LEO:  There's a lot of bugs out there.



STEVE:  Uh-huh.  But many, many, many fewer because we have good guys finding them and reporting them.



LEO:  Right, right.



STEVE:  Here's GitLab, $1,000, nine hours ago.  Upserve, $2,500, nine hours ago.  Mail.ru, believe it or not, $750, 10 hours ago.  Upserve a bunch, several thousand dollar bounties.



LEO:  Holy camoly.  New Relic.  Automattic.  Uber.  Casper.  I mean, this is unbelievable.



STEVE:  Ooh, look at this one.  $11,600 paid out by PayPal yesterday.



LEO:  So that means it was a pretty severe flaw.



STEVE:  Yes.  GitLab, three grand paid out.



LEO:  Holy cow.



STEVE:  Yup.



LEO:  Wow.  This is amazing.  And this is all through HackerOne; right?  So this is just one of the people who provide these.



STEVE:  Correct.  And, in fact, here's HackerOne paid $2,500 for a problem found in their own system.



LEO:  On HackerOne.



STEVE:  Four days ago.  Yup.



LEO:  Oh, man.  People are making some, I mean, this is a living.



STEVE:  This is - oh, there's Uber, $6,500, four days ago.  $2,600 paid by New Relic.  It is a living.



LEO:  Should I be encouraged?  Here's one for Uber for $14,500 last week.



STEVE:  Oh, baby.



LEO:  Should I be encouraged to see these?  Like, oh, these are bugs that are fixed?  Or discouraged at the vast number of them?



STEVE:  I know.  It is a mixed blessing.



LEO:  Here's a $10,000 bounty by Valve two weeks ago.  Wow.  This is amazing.



STEVE:  Yeah.



LEO:  That's kind of an eye-opener.



STEVE:  Isn't it sobering? 



LEO:  But you've got to praise HackerOne for publishing this.  I guarantee you Zerodium is not publishing this information; right?



STEVE:  No.  They are a black hole.



LEO:  Yeah.  Wow.



STEVE:  So get this.  This gives us some - now I'm glad we took a look at that, and I didn't forget to cover this.  Verizon is number one of the Top 20.  This was a list that was pulled together by Trend.  No, no.  ZDNet, sorry.  ZDNet pulled this together.  Verizon has been a member of HackerOne for nearly five and a half years.  They joined in February of 2014.  Are you sitting down, Leo? 



LEO:  Yeah.



STEVE:  During this time, Verizon has paid out more than $4 million in bounties.



LEO:  What?  Whoa.



STEVE:  The top award was $16,000.  I'm sorry, $6,000.  And a total of 5,269 bug reports resolved across Verizon's products.  The number two position on the list launched two years later, in March of 2016, is Uber.  Since their launch, Uber has shelled out $1.8 million in bounties, paying a top bounty of $15,000 and resolving 1,172 bugs.



Behind Uber in third place is PayPal.  Since just September of 2018, they've paid out more than $1.17 million, $1,170,000, for a total of 430 reports.  Top bounty, they paid out $30,000 for some bug or bugs, which is twice Uber's maximum and five times Verizon's maximum.  So PayPal, they're parting with some serious coin.  But they clearly take security seriously.  They know they have to have it.  And they're willing to have good guys find and report the problems, pay them some money, and not suffer the reputation damage of having a major breach.  What's more expensive to them?  And so this is why it makes sense.



Shopify since April 2015 has paid out more than $1.1 million across 996 reports with a top payout of $25,000.  Twitter is in fifth place, since May of 2014 paying out the same as Shopify, 1.1 million, across almost the same number of reports, 995 reports, paying a top bounty of $15,120.  Intel, in number six place, $800,000 since February of 2018.  Airbnb in seventh place, paying out more than $600,000 since joining in February of 2015.  Oh, and Ubiquiti, who I was just talking about.  I'm glad to see that they are there.  They're participating ever since March of 2015, paying out $600,000 to have bugs privately found and responsibly disclosed, allowing them to fix them rather than bad guys getting them.



And here's Valve.  You mentioned that you saw one they had just paid, Leo.  They're ninth in line, having joined in May of 2018, so almost about a year ago.  And their total payout is lower, $20,000 across 470.  That doesn't sound right.  $20,000?  No, it must be $200,000.  I slipped a zero when I was transcribing this yesterday.  Or maybe more because GitLab is in the middle - oh, that must be a single payout of $20,000, yes, from Valve.  GitLab is right at the middle point, having paid out $570,000 since February of 2018.  And immediately adjacent, one spot below, is GitHub in the 11th location, having joined in April 2016, having paid out a total of half a million, of $520,000 in 348 reports.  And I won't keep going.  But there's Slack is there.



Starbucks is in 15th place, having paid out more than $300,000 since November of 2016.  Mail.ru, Grab, Coinbase, Snapchat, HackerOne themselves.  Dropbox paid a hefty maximum bounty of $23,000.  And finally, in the last place, now number 20, is VK, which bills itself as Europe's largest social network.  So anyway, I wanted to share these numbers, as we saw, to sort of drive home the reality of the fact that there is, you know, we've talked about how possible it is for a hacker to make a career, I mean, this is the world we're in today.  Everybody is online.  Apps have bugs.  Companies are willing to pay some serious money relative to typical annual salaries to have bugs privately reported.  So maybe spend some time in the evenings, people who are interested in bug hunting.  Supplement your income, and maybe ultimately make a career out of it.  It's certainly possible.



LEO:  What does this say about the quality of software?



STEVE:  I know, Leo.  It says this stuff is being shipped on a schedule, not on "it's ready."  And if you talk to developers, they're being, you know, they estimate how long it's going to take them.  It's why I never, ever say when I'm going to have something done.  I learned a long time ago, I don't know.  And fortunately, I don't have a boss who makes me ship something that's not ready.  So yes, it takes a long time for my stuff to go out the door.  But once it does, it's done.  I know that SpinRite is old.  SpinRite 6, I finished it in 2014.  I haven't touched a byte of code since - wait, no.  Not 2014.  2004.  



LEO:  Oh, man.



STEVE:  So it's 15 years old.



LEO:  And have you had any bugs discovered?



STEVE:  There are no bugs.  It's old, yes, and I need to update it.  But there are no bugs in SpinRite because when I'm done, I'm done.  But that's old school.  That's not the way it is these days.  So, yeah, we've got - it is really, really sobering.  So again, to our listeners, HackerOne, H-A-C-K-E-R-O-N-E dot com, then click on the Hactivity link at the top and browse around and see if you wouldn't like to earn some of that because you probably could.



LEO:  Well, that's a, yeah, where do you learn those skills?



STEVE:  In fact, well, in fact they have it.  If you hover over For Hackers, then there is Start Hacking, Hacker 101, Leaderboard, and over on the right...



LEO:  They'll teach you.



STEVE:  ...Hacker 101:  Learn How to Hack and a Get Started button.



LEO:  I'll be diggety-danged.



STEVE:  Yup.



LEO:  Well, that's kind of cool.  You know, you looking for work?  Learn how to hack.  You could be working at home.



STEVE:  Yup.



LEO:  Oh, this is really interesting.  This is really interesting.  Wow.  I wonder if it's really that easy to learn how to do this.  Wow.  That's really - that's amazing.



STEVE:  Yeah.



LEO:  Steve Gibson.  There are no bugs in this man's tree.  If you want to listen to the show, we do it every Tuesday, a little late today, but normally 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  Best place to do it is at TWiT.tv/live.  Live video and audio stream there.  And join us in the chatroom at irc.twit.tv if you're listening live.  Now, if you can't make it live, and I know most of you can't, don't worry because this is a podcast.  That means there's on-demand versions of the show.  Steve has audio available at GRC.com.  And transcripts, Elaine Farris's carefully crafted transcripts of every episode.  And it's free at GRC.com.



In fact, there's only one thing that's not free right now at GRC.com.  That's SpinRite.  That's Steve's bread and butter, so if you need - if you have a hard drive you need a copy.  And if you need a copy, go to GRC.com, the world's best hard drive recovery and maintenance utility.  Everything else there is free, and it's fun to browse around.  It's just a little nerdy treasure trove of fun and interesting stuff.  SQRL is there.  You can learn more about that.  You can get passwords.  You can, I mean, it's just great.  It's just the greatest.  GRC.com.



Steve's on Twitter at @SGgrc.  You can leave him a direct message there if you have a question or a comment.  You can include that in our feedback section, or GRC.com/feedback.  And we have audio and video of the show ourselves at TWiT.tv/sn.  That's the website, TWiT.tv/sn.  And of course you can subscribe.  This is one of the oldest podcasts in the lexicon.  And so every podcast application should have a copy available for you.  If you subscribe, you'll get it the minute it's available every Tuesday afternoon.



Okay, Steve.  I saw "I Am Mother."  Enjoyed that.



STEVE:  Oh, yay.



LEO:  Enjoyed that quite a bit.  Yeah, good show.



STEVE:  Yeah.



LEO:  And you were right.  The trailer.  Don't watch it.  And the worst thing, I'm sitting there, and I'm hovering over it on Netflix because I'm just trying to decide, and it starts playing.  And I forgot that you said don't watch the trailer.  And they played - Lisa will tell you.  I'm going, "Oh, no, no, I don't want to see it."  And she said, "What?"  And I said, "Oh, Steve said not to watch the trailer.  Now I know what's going to happen."  But you don't fully know what's going to happen, I think.



STEVE:  No, no.  But still.  



LEO:  In fact, I have to talk to you off the air because I don't understand the ending.  I want to - I'm trying to figure out, well, you know what I'm trying to figure out.  Did you ever...



STEVE:  I do.  And in fact, I will send you a link to the reviewer who joined IMDB just to post...



LEO:  Just to explain it.



STEVE:  Just to explain it because he nailed it.



LEO:  I think I know.  But I'd like confirmation.  That's the modern thing, by the way.  I can't watch a show anymore without going to the web and seeing what people say about it.  Like, okay, well, why did - who did - why did they have - what did that - and then she - because I'm not as clued in as some of these people.  They watch pretty carefully.



All right.  Have a great evening, Steve.  We'll see you next week.



STEVE:  Okay, my friend.  Bye.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






