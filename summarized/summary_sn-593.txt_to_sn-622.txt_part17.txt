GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#593

DATE:		January 3, 2017

TITLE:		I'm Not a Robot!  (Really)

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-593.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  This week, Leo and I discuss law enforcement and the Internet of Tattling things, a very worrisome new and widespread PHP eMail vulnerability, Paul and Mary Jo score a big concession from Microsoft, a six-year-old "hacker" makes the news, Apple discovers how difficult it is to make developers change, hyperventilation over Russian malware found on a power utility's laptop, the required length of high-entropy passwords, more pain for Netgear, an update on the just finalized v1.3 of TLS, the EFF's growing "Secure" messaging scorecard, a bunch of fun miscellany - and how does that "I'm not a robot" non-CAPTCHA checkbox CAPTCHA work?



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  First show of the new year, and of course there's lots of stuff to talk about, including the hacking of a Vermont power station and that Russian spyware that the Defense Department found. We'll talk about that.  There's a whole lot more coming up.  Security Now! is next.  It's great to be here for the new year.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 593, recorded Tuesday, January 3rd, 2017:  I'm Not a Robot, Really.



It's time for Security Now!, the show where we talk about security, now, right now, with Steve Gibson, the man in charge at GRC.com.  He's the security guru.  We talk a lot about privacy, too, and how things work.  Hey, Steve.  Happy New Year.



STEVE GIBSON:  Leo, great to be with you for 2017.



LEO:  Wow.  Wow.  Wow.



STEVE:  Yes.  Yes, yes, yes.  And I have a feeling it's going to be an interesting year of - actually, the first story is the Internet of Tattling things, so I...



LEO:  Oh, I know what you're going to talk about.  This is the criminal suit, right, or whatever, the prosecution, yeah.



STEVE:  Yes, yeah, from Arkansas.  So we have law enforcement and the Internet of Tattling things.  A very worrisome new and widespread PHP email vulnerability which is present in nine million different websites at the moment.  Paul and Mary Jo scored a big concession from Microsoft just before the end of the year on Windows Weekly.  We have a six-year-old hacker who makes the news over the holidays.  Apple also discovers how difficult it is to make developers change.  Some hyperventilation over Russian malware found on a power utility's laptop and how the press is just going bonkers for, like, no apparent reason.



I found something really nice about the required length of high-entropy passwords from someone who really knows what he's talking about.  More pain for Netgear.  An update on the just-finalized v1.3 of TLS.  Oh, and I realized I changed something here, but I didn't change it in the notes.  The EFF, I found a new page on the EFF site which is an amazing sort of self-serve security reference that I know that our listeners are going to be interested in.  We have a bunch of fun miscellany.  And we're going to do what we couldn't get to last time, two weeks ago, because we just ran out of time.  And that is, I'd also named this week's podcast:  I'm Not a Robot, Really.  How does that checkbox non-CAPTCHA CAPTCHA manage to be so simple and, apparently, effective?  So I think another great podcast to kick off the new year.



LEO:  I was curious if you would talk a little bit about - and I guess there's really not much to say - what the intelligence agencies released as their explanation of the Russian hacking.  Dan Goodin had a great takedown on Ars Technica, and I'm sure you read that.  	



STEVE:  And actually it's our topic for next week.



LEO:  Good.  Good.  Because there was a - I looked at it.  I don't remember the name of the security researchers that went through it.  And it looked as if it was nonsense, basically.



STEVE:  Well, yeah.  It looks like there's, from what I saw, they're describing it as some old and obsolete Ukrainian malware.



LEO:  PHP malware, which is widely available through the web.



STEVE:  Exactly.  But the details of it I thought were interesting, so I thought...



LEO:  Yes, yes.



STEVE:  And there's been a huge amount of interest from our listeners.  So I said, okay, let's, you know, I will share everything that we know from a technical standpoint.  And that's our topic next week.



LEO:  Yeah, we can talk about it then.  Because, you know, there's a meta story about it which is, if this is their evidence, it sucks.



STEVE:  Yes.



LEO:  But there's also the probability that they can't really release their evidence because to do so would impair their operational capabilities.  Maybe.  I don't know.



STEVE:  Yeah.  Well, and of course, as our listeners know, that's what stops me completely because...



LEO:  We don't know.



STEVE:  Anybody can gossip, and you can get that anywhere.



LEO:  Right.



STEVE:  But I'm not interested.  We need details and facts.



LEO:  Right.  We don't know.



STEVE:  And it's inherently, they're inherently not going to be available.  But at least in this one case we know everything about this particular piece of malware which itself looked pretty interesting.



LEO:  Okay, good.  Next week.



STEVE:  So we will do a deep dive next week.



LEO:  All right.



STEVE:  You have 28 people; right?



LEO:  Yeah.



STEVE:  And I had 23 at GRC's height.  Well, or depth, depending upon how you do it.



LEO:  Oh, god.



STEVE:  And one of the things that I noted, which you just alluded to, is it's remarkable how people are constantly having birthdays.



LEO:  A lot of partying.



STEVE:  When you get a certain critical mass of people, it's someone's birthday constantly.



LEO:  Almost all the time, yeah.



STEVE:  And it's like, wait a minute.  Already we have another birthday?  So obviously, if it was evenly distributed, and you had 24 people on a 12-month calendar, you'd have an average of two per month.



LEO:  A month.  That's about right.



STEVE:  So it's like, okay, that's every other week.  It's like, okay, wait a minute.  Again, another birthday?



LEO:  That's a lot of cake.



STEVE:  Stop growing.



LEO:  Now, you could, by the way, use ZipRecruiter to narrow down your candidates by birth date, and have them all have the same birthday, and really simplify things.  But I wouldn't recommend it.  If that's your top priority.



STEVE:  Now, I wonder if that would be considered hiring discrimination?



LEO:  Yeah, I think it would, yeah.



STEVE:  Because, well, it's not something that they have any control over.  So in general, things that are...



LEO:  It has to do with age, so, yeah.



STEVE:  Yeah, yeah, yeah.  Okay.  So I've been thinking about your comment, about this Picture of the Week really is a spoiler for Westworld.  And you're right.  So...



LEO:  Close your eyes.



STEVE:  And here's the problem.



LEO:  Yes.



STEVE:  The thing that makes this such a brilliant picture is why it's such a horrific spoiler.



LEO:  Yeah, right.  That's the humor of it.



STEVE:  Yeah, it's so good because it's not something you know unless you finish the series.  So do not, listeners, look at the first page of the show notes.  Do not even download the show notes.  Forget about everything.



LEO:  And if you [crosstalk] audio, you're safe.



STEVE:  Yes.  And I don't know why you haven't finished Westworld.  What's wrong with you?



LEO:  Right, that's the real question.



STEVE:  Maybe you don't have an HBO subscription.  You're waiting for it to appear on the torrent of your choice.



LEO:  Right.  I think that's true.  No, I think a lot of people are going to - this happens with all the HBO shows.  They sell them later.  So a lot of people say, well, I don't have HBO, but I'll just buy it.  So we don't want to spoil it for you.  So if you've not seen Westworld, close your eyes.  Actually, if you've seen just a few episodes, that's really when you need to close your eyes.



STEVE:  Yeah.



LEO:  This is not revealed till later in the show.



STEVE:  Yeah.



LEO:  But if you have seen it, I think you're going to find this very funny.



STEVE:  It's the best thing ever.



LEO:  Oh, man.  Is this mean.  It's a CAPTCHA.  It's a CAPTCHA with that checkbox that says "I am not a robot," and a picture, and a caption.  That's the part that would be revealing if we said.



STEVE:  Yeah.



LEO:  Somebody is not quite sure if he or she is a robot.



STEVE:  Exactly.  And from what we know of Westworld, there does seem to be some confusion among the ranks.



LEO:  Yes.



STEVE:  And that was one of the really interesting unknowns for a long time was what the robots believed about themselves.  They had a back story which it was explained was put in place to help form, to like anchor their personality.  And so the way they handled clear problems with reality was interesting because of course they weren't in reality, they were in an amusement park.  So anyway.



So a man was found dead, unfortunately, in the hot tub, the backyard patio hot tub of the person who then became the murder suspect.  And his body had cuts and bruises consistent with a fight, and there was blood in the hot tub water.  The suspect has been charged with and will be tried on charges of first-degree murder.  And this happened in Arkansas.  Detectives in Arkansas then wished to obtain data from the suspect's various IoT systems, things that might provide some additional information.  And this is really the anchor of the story that we'll get to in a second, but I want to sort of cover the facts.



So the thing that made the news wasn't what I think is a little more interesting to our audience, although the specifics are, and that is that this person, the suspect in his home had an Amazon Echo.  And Amazon received a warrant, essentially, requesting any information, any audio that the Echo might have picked up and that Amazon had on their servers.  And we'll talk about what Amazon said in a second.  But essentially they've asked Amazon for anything that the suspect's Amazon Echo may have overheard.  So...



LEO:  Wow.



STEVE:  Yes.



LEO:  We've talked about this before.  I mean, it wouldn't have overheard anything unless by accident, as the guy is being murdered, he shouts, "Alexa."  Right?



STEVE:  Right, yeah.



LEO:  Okay.



STEVE:  You know, which edge of the knife do I use?  And I'm not sure what you want to ask Alexa at a moment like that, but yes.



LEO:  Now, I don't know if it's the same case, but there was also a subpoena for recordings of some other device.



STEVE:  Well, what they noted was interesting, was that this person, apparently he was into IoT, for better or for worse.  He had a smart water meter.  And it logged 140 gallons of water used between 1:00 a.m. and 3:00 a.m.



LEO:  Aha.



STEVE:  The night the victim was found dead in the suspect's hot tub.



LEO:  You see we have caught you.  You are guilty.  



STEVE:  And so the investigators assume, and they're alleging, that the water was used to wash away evidence of whatever it was that transpired on the patio.



LEO:  Right, right.



STEVE:  Or maybe the hot tub was dumped and then refilled because it was too bloody?  I mean, who knows?  So the examination of the water meter and the request for stored Echo information raises a bigger question about privacy.  At a time when we have any number of devices now tracking and automating our habits at home, the question arises:  Should that information be available for use against us in criminal cases?  Now, the defense attorney, of course, argues that it should not be, saying one has an expectation of privacy in one's own home.  And, she says, "I have a big problem that law enforcement can use the technology that advances our quality of life against us."  Well, okay.  Oh, boohoo.  I mean, her job is to take that position.  But it does really raise the question.



And from our audience's standpoint, and people who follow the podcast, there's also, as we would well know, a real question of the reliability of information from smart home devices, since accuracy can, as we know, be an issue for any number of readily hackable IoT gadgets.  So someone could plausibly be framed by their IoT devices because, you know, they're being hacked all the time.  So it might not be your light bulb DDoSing some random dotcom site somewhere else.  It could be your IoT device, your light bulb, basically tattling on you, saying, yes, he was home at 5:00 a.m. because I got turned on, when in fact that information could be planted externally.



LEO:  But all of that would be - that's part of the court's, you know, and the testimony, and you have to explain all that stuff.



STEVE:  Well, and we know, I mean, how many stories have been built around the whole chain of evidence issue, you know, the legal formalization of verifying that at no point was the chain broken, was something never subject to tampering, out of control of law enforcement and so forth.



LEO:  That's right, yeah.



STEVE:  So an Amazon spokesperson - first of all, Amazon denied them access to any recorded audio.



LEO:  That's interesting, too; isn't it.



STEVE:  Yes.



LEO:  By the way, if you had access to the suspect's phone, and he had the Amazon app, the Echo app on his phone, all of the recordings are available through that app.



STEVE:  Yes, and we're about to get to that because Amazon does store recordings of what it picks up in the cloud.  So the Amazon spokesperson denied the audio, but did provide purchasing information, that is, non-Alexa - oh, sorry, non-Echo.



LEO:  That's all right, I already screwed it up by shouting.  The jokes, by the way, the jokes here are endless on different commands that you could use that would be incriminating.



STEVE:  Oh, coming over the chatroom?  Oh.



LEO:  Yeah, the chatroom's been having a good time...



STEVE:  Quite prolific.



LEO:  ...with this, yeah, yeah.  I'll - go ahead.  I won't interrupt.



STEVE:  Okay.  So they did provide some information.  Probably the warrant asked for everything.  And they said, well, we'll give you some of what you've asked for, like when did he purchase this new set of knives, but we're not going to give you the audio from the Echo.  So the Amazon spokesperson formally said:  "Amazon will not release customer information without a valid and binding legal demand properly served on us.  Amazon objects to overbroad or otherwise inappropriate demands as a matter of course."



Okay.  So then some facts were made more clear as a consequence of research that followed this.  First, as we have said, the Amazon Echo only captures audio and streams it to the cloud when the device hears the wake word "Alexa."  And as we know, it may actually - it's probably buffering it, looking for that word in the buffer.  So it probably, in the same way that Apple's little animated picture widget captures before and after you actually press the snapshot button, I would imagine that Alexa is picking up some context in case it's important beforehand.



So there is a gated capture and stream to the cloud which surrounds the engagement of the Echo.  And as we know, a blue ring on the top of the device provides a visual indication that audio is being recorded.  So the blue ring is meant to be connected to Alexa is listening actively and streaming to the cloud.  Those clips, or "utterances," as the company calls them, are stored in the cloud until a customer deletes them either individually or all at once.  When that's done, the utterances are permanently deleted.  And just in case someone didn't know, the microphones on an Echo device can be manually turned off at any time.



So I think this represents Amazon having thought this through.  I mean, they couldn't possibly handle the bandwidth of everybody's Echoes and Dots in the world sending, you know, spying on all of their homes.  And nobody would want that happening.  So I think they made a good tradeoff.  They've made it clear what they're doing.  They're no doubt capturing sound samples, both to learn about Echo's master and probably over time to perfect their audio.  And also, for example, if you ask Alexa something that the system, the Echo - oh, okay, I give up. 



LEO:  By now everybody's muted their system.



STEVE:  If you ask her something that she doesn't understand, I'm sure that sends a ping back to the mothership, saying this snippet was not understood.  And probably there's minions somewhere listening to those, going, ooh, what a great idea for, like, what she should be able to understand.  So there's no doubt some serious metrics and analysis going on on the back end to make it even better.  So that's a tradeoff.  And I think Amazon has done a good job.  But the broader question, of course, is not this specific, but more like the power meter, which is a perfect example of information which once upon a time would have been lost, which law enforcement would want to have, if they could.  And now to a much greater degree there is, and we talk about it all the time, the privacy versus tradeoff agreement, essentially.  I mean, the nature of that is, if you've got - what's the thermostat on the wall, the...



LEO:  Nest.



STEVE:  The Nest.



LEO:  There are a lot of them now.



STEVE:  Yes.  Exactly.  If you've got a smart thermostat, if you've got something like a home security system which is doing stuff, I mean, essentially many of these things, not all of them, but many of them are monitoring in nature.  So if your lights are being monitored, your heating and air conditioning, the presence of your motion in rooms and so on, and that information is obtainable retrospectively, then that's just something to remember.  And there may be a very strong defense, which is, "Your Honor, everybody knows these systems are being hacked all the time.  My client was framed.  He wasn't home, even though his home says he was, because somebody arranged, you know, planted that information.  And the prosecution is being fooled by believing that this could be trusted."  So we live in very interesting times.



LEO:  You know, it's going to be everywhere; right?  This is all being gathered at all times.



STEVE:  Yup.



LEO:  And law enforcement's going to want it.  And you know what, if it convicts some murderers, I'm not sure I'm against that.  I just worry that it'll be used against dissidents and others for political reasons as opposed to real crime.



STEVE:  Well, in Southern California we went through a phase, maybe about five years ago, where all intersections got cameras pointing down all four streets.  And it's like, okay.  And in fact it's funny because I remember looking when my particular intersection, because I'm on not a very busy street, got a signal.  It used to just be stop signs, and people were getting killed, unfortunately.  I think it took three, and then they said, okay, that qualifies for a stoplight.



Looking inside this thing, I mean, it looks like a chunk of the Level 3 server farm.  It's an unbelievable amount of technology.  And I'm looking at these lights that go reluctantly green, and red and yellow mostly, and thinking, it takes all this in order to run a stoplight?  As an engineer I'm thinking, there's something wrong with this picture.  But what apparently they have is this countywide incredibly high-bandwidth network that planned to have things like cameras pointing down every street in order to capture this information.  So I don't know if it's being stored locally, or if it's being streamed, or what's going on.  But, yeah, as you say, Leo, there are essentially monitoring posts being set up all over the place in order to provide services, but also to keep the peace.  And as you say, let's hope it isn't abused.



So speaking of abuse, we have an extremely worrisome - PHPMailer is the very popular PHP library - zero day, meaning there is no fix for this, and this is very bad right now.  It's a critical flaw which about nine million PHP-driven websites and many popular open source web applications are using worldwide, including WordPress, Drupal, 1CRM, SugarCRM, Yii, and Joomla, all use this PHPMailer library as their mailing agent interface.  A Polish security researcher, Dawid Golunski, with Legal Hackers is the name of his group, discovered a critical vulnerability some time ago.  And that vulnerability allows an attacker to remotely execute arbitrary code in the context of the web server and compromise the target web applications.



So this is a classic example that answers the question, how do bad guys get in to get up to their evil deeds in a website?  It's exactly this kind of thing.  I have the technical details, that we'll get to in a second, to sort of flesh that out.  But in his posting he wrote:  "To exploit the vulnerability, an attacker could target common website components such as contact or feedback forms, registration forms, password email resets and others that send out email with the help of a vulnerable version of the PHPMailer class."  Okay, and so that's, like, everything today is vulnerable.  There is no fix for this.  "A successful exploitation could let remote hackers gain access to the target server in the context of the web server account, which could lead to a full compromise of the web application."



He responsibly reported the vulnerability to the PHPMailer developers, who then patched the vulnerability in their new release, which was 5.2.18.  And all versions before that release were affected.  Turns out, however, they didn't fix it.  And as a consequence of that patch, attention got focused on this, and the nature of the problem was published in a security mailing list.  So the secret disclosure, essentially, as a consequence of the PHPMailer guys fumbling the fix, their fix was found not to have fixed the problem.  The problem still persists, and now it's public knowledge.  So Golunski immediately posted full details because the cat was out of the bag?  Is that...



LEO:  Cat's out of the bag.



STEVE:  Cat's out of the bag.



LEO:  Why the cat was in the bag, no one knows.



STEVE:  Horses have left the barn...



LEO:  Horses, yes.



STEVE:  ...[crosstalk] jumped out of the hat, whatever.  Anyway, too late.  So we have a situation where websites are vulnerable to deliberate exploitation.  Okay.  So the 2016 original patch was CVE-10033, which added, in version 5.2.17, sanitization of the $Sender variable by applying an escapeshellarg function to essentially protect the arguments before the value is passed to the mail function.  Now, this is, unfortunately, this is a perfect example of sort of the equivalent of setting up a firewall to block only the ports that you know have ever been used for evil, otherwise leaving everything else open.  So it's a blacklisting approach, rather than a whitelisting approach.  Meaning in this context that there's a fundamental problem between the web interaction characters and HTML and the characters used by operating system shells.



And we run across this problem all the time.  In fact, I had it.  That problem I had that we talked about, what, six months ago?  There was no way to exploit it for any mispurpose, but it was the one researcher that found a flaw in one of my forms where I hadn't - essentially exactly this.  I was not perfectly removing special characters from showing them to the web page, which allowed a bad guy to carefully design something that would execute their own code in the page that the user received.  You don't ever want to let that happen.  As it turns out, there was not a way, due to all kinds of other things that I do differently than the rest of the world, to exploit that.  And we talked about it at the time.



So this is one of those.  This is where they just - they missed something for a long time, that there was a way to submit a form to get something that should have been protected from use onto the user's page.  So in technical details it says this unescaped string gets passed to the mail function.  However, what they fixed for that vulnerability does not take into account the clashing of the escapeshellarg function with internal escaping that is performed by the escapeshellcmd, which is performed by mail.  So there it sounds like it's - and I didn't dig into this infinitely because there wasn't any real purpose.  It's going to get fixed quickly.



But on the fifth parameter there is interaction between those two.  So as a result it's possible to inject an extra quote that does not get properly escaped, and break out of the escapeshellarg protection applied by the patch which was made in the first fix at 5.2.17.  So an attacker could pass, and in this case it's the -X parameter of sendmail, to write out a log file with arbitrary PHP code and cause that to be executed.  So what this means is that the current latest versions, both 5.2.19 and 5.2.18, are vulnerable to remote code execution despite the patch.  And Dawid immediately posted all of that information and a proof-of-concept exploit because, why not, we now know how to do it.  I mean, the truth had already escaped because they didn't fix it right the first time.



So if anybody feels they may be vulnerable to this, or know somebody who might, make sure you check to see if PHPMailer has been updated.  It's going to take .20.  That'll be the one which fixes it, 5.2.20.  Anything previous to that has this problem.  And it looks at the moment like it is very widespread.



So just before the holidays, Paul and Mary Jo had the Microsoft Chief Marketing Officer, Chris Capossela...



LEO:  Capossela, yeah.  He was on the show, yeah.



STEVE:  Right.  And they got huge news coverage for Windows Weekly because in this interview, where they were going back and forth...



LEO:  Something must have happened that I missed, because I was listening, and I was right there.



STEVE:  So, yeah, I mean, ExtremeTech, Techdirt, Business Insider, Softpedia, and many more all picked up on the story and referred to Paul and Mary Jo and Windows Weekly.  What he said was, he said, "We know we want people to be running Windows 10 from a security perspective.  But finding the right balance, where you're not stepping over the line of being too aggressive, is something we tried.  And for a lot of the year I think we got it right."  Okay.  Many people would disagree.



He says:  "But there was one particular moment in particular," he says, "you know, the red X in the dialog box..."



LEO:  Yeah.  He admitted that that was a mistake.



STEVE:  Right, "which typically means you can't" - means you, okay, I'm reading what he said, "typically means you cancel, didn't mean cancel."  And he said:  "And within a couple of hours of that hitting the world, with the listening systems we have, we knew that we had gone too far.  And then, of course, it takes some time to roll out the update that changes that behavior."



LEO:  Right.



STEVE:  "And those two weeks were pretty painful..."



LEO:  Yeah.



STEVE:  "...and clearly a lowlight for us.  We learned a lot from it, obviously."  



LEO:  And this is, by the way, why we love having Chris Capossela on because, unlike a lot of executives, he says the truth.



STEVE:  Yes, yes.



LEO:  And you know what, that was valuable to me to hear Microsoft say, yeah, we blew it.  People make mistakes.  And what we don't really know is how long it takes them to fix it.  Takes a couple of weeks, due to the nature of their systems or whatever.



STEVE:  Right.  And the only pushback I would have about that is that the red X was like the last straw.  That was after...



LEO:  Yeah.  No, no.  Yeah.



STEVE:  ...six months of, I mean, even Paul was like, okay, this is, you know, like before the red X, remember when there was no obvious way, like either button you pushed caused the update?  It was like, update now, or update later.  Uh, wait a minute.



LEO:  Right.  That was dopey, too, I agree.



STEVE:  So, yeah.  So their listening systems, I would argue, I mean, if they had some - what happened was they were willing to tolerate a very high level of annoyance, which finally did overcome even their high threshold, which they clearly were intending to tolerate in order to get Windows 10 deployed as far and wide as possible.  And on this occasion, I just checked the Never10 page this morning, seeing 2.262 million downloads.  And we've slowed down now to only 2,236 a day.  So that gave people the option.



LEO:  The other thing I would point out is we knew how loud the pain was because within our circle we heard it.  But I think probably Microsoft's listening systems extend to the entire - remember, there's a billion and a half users of Windows - extend to that larger group, where I don't - I think the upset was probably considerably more muted among normal people; right?



STEVE:  And it's only the instances where the upgrade crashes or kills someone's machine that makes a big cloud, and everybody says, "Oh, my goodness."  Whereas you don't hear from all the people who are like, oh, wow, this is better?  Okay.  And then they just go on.



LEO:  We don't like it.  But we're much more, I think - we're not typical.  We're much more aware of how it was being thrust down our throats.  And I think you also have to acknowledge that they did have - there was some merit in their idea of we'd like to get everybody on the same platform and then offer required but consistent free updates from that point on.  That would really help, not only their business, but the security of the Internet in general.  And so I understand how they were willing to push a little bit to do that.  And I love it that Capossela acknowledged, hey, we pushed too far.



STEVE:  I agree.  I agree.



LEO:  And we realized that immediately.



STEVE:  Yes.  It's nice to have someone from whom you don't get just total spin.



LEO:  He's great.  He was on last year at this time.  And I think he'll be kind of a yearly thing.  And I just - I want to encourage him because it is, it's unusual for a company to be forthright at all about this kind of stuff, you know.



STEVE:  Yup.  Okay.  So in this holiday season's classic demonstration of the security/convenience tradeoff, a six-year-old daughter uses her sleeping mother's thumb to purchase $250 worth of Pokemon toys for herself.  "Ashlynd Howell of Little Rock, Arkansas is a precocious six year old.  While her mom, Bethany, was sleeping on the couch, Ashlynd gently used her mom's thumb to unlock the Amazon app on her mother's phone.  Ashlynd then proceeded to purchase $250 worth of Pokemon presents for herself.  When her parents got 13 confirmation notices about the purchases, they of course thought that they'd either been hacked - well, technically they were, but not by someone remote - or that their daughter had ordered them by mistake."



LEO:  Uh, not a mistake.



STEVE:  "But she proudly explained, 'No, Mommy.  I was shopping.'"



LEO:  I love it.



STEVE:  The Howells were able to return only four of the purchased items.  So anyway, I just got a big kick out of that.  You know, we've talked about the problem of security versus convenience.  Typically we're worried about law enforcement forcing your thumb onto your phone, not your six year old.  So parents beware.



LEO:  So funny.  So funny.



STEVE:  Apple announced last summer, at the 2016 Worldwide Developers Conference, that at the year end, that is, this year end, they would be requiring something known as ATS to be applied to all iOS apps.  And they have learned - this is the story from the forcing people to change is difficult department, which of course is one of the interesting characteristics of IPv4 and SHA-1 certs and stuff we talk about all the time.  It's like, okay, do we really have to change this?  Apple's head of security engineering and architecture, Ivan Krstic, said during a WWDC presentation:  "Today I'm proud to say that, at the end of 2016, App Transport Security" - that's the ATS, App Transport Security - "is becoming a requirement [fanfare] for App Store apps."  He continues:  "This is going to" - I added the fanfare.  "This is going to provide a great deal of real security for our users and the communications that your apps have over the network."



So as our listeners probably already gathered, ATS is a feature  which Apple debuted in iOS 9.  When ATS is enabled, it forces an app to connect to web services over an HTTPS connection rather than HTTP.  So it's very much like the HSTS we often talk about for web browsers.  But of course it's forcing this behind the scenes.  One of the problems, of course, is that we don't know what the apps are doing.  And unless you capture packets and traffic and analyze it, the user is completely oblivious.  At least with a browser and the URL we can get some sense for what's going on, although now that we've got JavaScript, it can be doing its own thing, too.  But there's a lot of control over that.



So I wrote on my show notes:  "Just as the 'S' in IoT stands for 'security,' as we know, the 'S' in HTTPS stands for 'secure.'  But since we have little idea what mobile apps are doing behind the scenes, it can be impossible to determine whether an app's own cloud connections are authenticated and encrypted."  So ATS is enabled by default for iOS 9, but developers have been able to switch ATS off and allow their apps to send data over an HTTP connection.  That allowance was supposed to end at the end of 2016.  And ATS requires the use of TLS v1.2, with a few exceptions for already encrypted bulk data, like media streaming.



But as the deadline approached, and as so many other people have found who've tried to set deadlines, Apple had to change their tune.  On the 21st of December, Apple posted in their developer site:  "App Transport Security, introduced in iOS 9 and OS X v10.11, improves user security and privacy by requiring apps to use secure network connections over HTTPS.  At WWDC 2016 we announced that apps submitted to the App Store will be required to support ATS at the end of the year.  To give you additional time to prepare" - because of course you'd only had six months at that point - "this deadline has been extended, and we will provide another update when a new deadline is confirmed."



So they haven't said when.  They've just said, uh, okay, we're not going to do that.  And no doubt there was a whole lot of backchannel screaming going on from people probably far and wide, saying no, no, no, no, no, we can't, we can't, we can't.  Don't make us.  And so Apple's like, okay, but, you know, we're serious about this.  So get it done.  So again, one more instance of just how difficult it is to get people to change the way they're used to operating.



So through the holidays there's been all of this talk about, as we were talking at the top of the show, Russian malware.  I mean, this is a huge issue in politics at the moment with McCain rumbling about pursuing investigations; our new President-elect saying, oh, computers, nobody knows what's going on in computers because they're too confusing.                                      



LEO:  They're too fast.



STEVE:  His 10 year old spends...



LEO:  Barron knows.



STEVE:  His 10-year-old son is a genius who could do anything with computers.  But nobody really knows what's going on.  But he does, for whatever reason, that's not getting any traction with him.  So amid all of this, and I think because of this, a story got a ridiculous amount of attention, which was blaring headlines:  "Russian Malware Detected in U.S. Electrical Utility."  And in my notes I wrote, "Lots of smoke and noise."  Some malware which has been associated with a known Russian hacking campaign, which has been labeled by the Department of Homeland Security and the FBI as Grizzly Steppe, S-T-E-P-P-E, was found on a single isolated laptop owned by a Burlington, Vermont electric utility.  It wasn't in the grid.  It wasn't crawling around.  It wasn't doing a Stuxnet deal on us.  They did an antimalware scan of all of their assets.  And, oh, look, we've got malware on a laptop.



Well, okay, that's all it was.  No indication that this thing was ever in any way associated with grid hacking or anything.  It was just - it happened to be on a laptop that was owned by this power utility.  And the laptop had and has nothing to do with the electric power grid operation and management.  So everyone take a breath.



On the other hand, speaking of hyperventilation, Vermont's state governor, Peter Shumlin, said in a statement:  "Vermonters and all Americans should be both alarmed and outraged that one of the world's leading thugs, Vladimir Putin, has been attempting to hack our electric grid, which we rely upon to support our quality of life, economy, health, and safety."  And one of the state's U.S. representatives, Peter Welch, a Democrat for Vermont, said Russian hacking was "rampant, systemic, relentless, and predatory."



LEO:  No.  One laptop.



STEVE:  I know.



LEO:  But that does comport with what we'd already heard, which is that - and what we'd expect, probes from time to time, just to look for weaknesses, to see where we could get some spearphishing done.  I mean, nobody - I think we're going to rapidly enter into this age of mutually assured destruction, where we've got hackers; you've got hackers.  You take down our grid; we take down your grid.  So I think what you're going to see is exactly this kind of thing, which is, let's see.  Let's nibble the edges.  Nothing that would be provocative.  Nothing that would be seen as an attack or an act of war.  But let's see where the vulnerabilities lie.  Does that seem sensible?



STEVE:  Yes.  Well, so just to finish, Peter said:  "They will hack everywhere, even Vermont."



LEO:  Yes, especially Vermont.



STEVE:  Yeah, even Vermont has computers, "in pursuit of opportunities to disrupt our country."



LEO:  Right, right.



STEVE:  And so in my notes I said, okay, great.  So we've been talking about the inherent vulnerabilities of our U.S. power grid for, what, at least a decade.  Yet no one appears to have the will to find or raise or commit the money that's going to be required to fix it.  So, I mean, I'm glad for this, if this - I mean, I'm not happy that there was malware on the laptop.  But if this, you know, if these politicians will please bottle their furor and...



LEO:  Do something.



STEVE:  ...send it to Congress, and spend a lot of money on infrastructure that apparently will help the economy - that's what Trump says he's going to do - wonderful.  Let's fix our grid.  Let's not just keep talking about how incredibly vulnerable it is.  And again, people get malware on their laptops, on their computers all the time.  I agree with you, Leo, it is probably not a coincidence that this laptop had malware from Russia.  But malware has to come from somewhere.  And the fact is a lot of it comes from Russia, even if Putin had nothing to do with it, doesn't even know about it. 



LEO:  So you would kind of agree with Trump when he says, "Hey, this stuff, nobody really knows what's going on."



STEVE:  You've heard me saying now, it's been our theme for the last, at least half a year, the degree to which our systems are porous.



LEO:  Right.



STEVE:  I mean, they just are.  And they are too complex.  We run across, like, you know, that bizarre way of getting into the iPhone we talked about, where you've got to touch your nose and click your heels three times and at the same time sneeze in a way that Siri recognizes, I mean, and oh, my, look, suddenly you're in.  It's just because this stuff is just so complicated.



LEO:  Well, and the challenge with the grid is that it's not one company.  There's not one national grid.  There's privately and publicly held companies all over the country.



STEVE:  And Leo, it's still got PDP-11s running it.



LEO:  Right.



STEVE:  Which actually is a good thing.  It turns out that only PDP...



LEO:  Actually, that'd be safer, wouldn't it.



STEVE:  Yes.  Only PDP-11s are qualified to control nuclear reactors because you can't infect them.  So the real problem here is that it's old; you know?  And we have an aging infrastructure.  Our electric grid is, yes, a national asset.  But it predates the Internet.  And so what has happened is, after the Internet had happened, but before today, where a real appreciation of security exists, a lot of connectivity was created between the power grid and the Internet because, oh, look, we can check our meters from home, says the head of the electric utility for Vermont.  Isn't that handy.  I can make sure everything's fine on my iPhone.  Just like you can check your baby monitor, and it can be hacked, too.



So we have a problem, and that is that we are still putting features ahead of security.  And I expect it's going to take an attack on the power grid to bring about a change.  I mean, I hope I'm wrong because widespread power failures are problems.  And of course they could be made part of a larger terrorist plan or something in order to cripple response systems and so forth at the same time.  So it really is bad.  And I hope that, if nothing else, this story generates more concern.  Our podcast listeners recognize that this is taking it a little too far, that this wasn't Russian Stuxnet-esque software that was found spinning on a turbine somewhere.  It was on some guy's laptop that was in charge of the cafeteria.  I mean, who knows?



LEO:  Well, and it's reasonable to, I mean, anything that arouses alarm.  The thing that I worry about is that the President-elect thinks that, well, that just means we should use handwritten notes and couriers as opposed to saying, well, let's try to modernize the infrastructure.



STEVE:  You know?  And we want to avoid pigeons because apparently...



LEO:  They're hackable, too.



STEVE:  ...pizza is now being delivered by drone, and that could be rough on the pigeon industry.



LEO:  But you know what, this is what happens when you've had infrastructure longer than anybody else.  You have piecemeal.



STEVE:  Right.  That's exactly right.  The reason Chinese airports are gorgeous is that they're all new.



LEO:  They just built them.



STEVE:  Yeah, exactly.  So I've been saying for some time that the concern over password length can be overwrought because really good, really, I mean, truly high-entropy passwords - which a user cannot create.  You need software to do that.  We just, you know, you've got to roll the dice, literally, or you've got to ask some software to give me some random gibberish because, if we do it, it'll be our Star Wars name by mistake, and we're in trouble.  So I ran across a tweet from somebody who really knows his stuff.  We've discussed Jeremi in the past.  Jeremi Gosney is one of the, if not THE, leading password-cracking guys.  He's affiliated, well, he's a principal and founder of two companies.  And this first company, he calls it Sagitta, S-A-G-I-T-T-A, HPC.



So their description of themselves is:  "Sagitta HPC is the leader in high-performance password cracking.  We deliver enterprise-grade turnkey solutions that are designed by world-renowned password cracking experts and are tailored for information security, forensics, law enforcement, and litigation support professionals."  So everybody knows how to read between the lines there; right?



"Our modular distributed solution can accommodate clusters of any size and integrates seamlessly with the popular free software you already know and love."  Yeah, because these guys wrote it.  "Whether you need a standalone system with three GPUs or a cluster of 300, you can count on Sagitta HPC to deliver the perfect solution.



"Sagitta HPC is a wholly-owned subsidiary of Stricture Group LLC, founded in January 2013 by Stricture Group founders Jeremi Gosney and Russell Graves, after a large number of inquiries were received asking them to replicate and improve upon their 25-GPU VirtualCL cluster.  Since then, Sagitta has delivered solutions to dozens of government and law enforcement agencies, Fortune 500 companies, security consulting firms, and litigation support firms around the world."  So what we're talking about is world-class, state-of-the-art, we don't care what it costs, we need to crack a password technology.



"At Sagitta's R&D lab," they write, "we perform heavy research into the best possible hardware and software combinations for our own internal use at Stricture Group.  The best of the best solutions then become products that we make available to our customers.  We push the bar higher and higher with each generation, frequently requiring us to write custom code such as od6config to enable the use of next-generation hardware.  We also develop our own in-house code to maximize the performance and enhance the potential of our products.  Sagitta also gives back to the community by frequently contributing and volunteering time to free and open source password-cracking projects, such as Hashcat and John the Ripper."



And I have in the show notes here a picture of Brutalis.  Brutalis is their high-end, 3U-high rack monster.  The description of this piece of equipment - and just looking at the picture I can see one, two, three, four, five, six, seven, eight double-width GPUs with lots of power cabling going back to a back plane; 3EEE redundant 1,000-watt each power supplies.  The caption says:  "Brutalis is an eight-GPU monster, clawing its way through hashes at unprecedented speeds.  Providing up to eight Nvidia GPUs, two Intel Xeon E5-2600V3 CPUs, and up to [wait for it] 768GB of registered ECC memory" - so three quarters of a terabyte of RAM - "the Brutalis is the fastest, meanest, most hardcore system money can buy.  Ships with a three-year warranty."



LEO:  Now, Steve, you remember, somebody dropped by the predecessor to this.



STEVE:  Oh, right.



LEO:  This is an incompleted board that was designed to crack DES.



STEVE:  Right.  And I think only one of the chip spots was populated.



LEO:  Yeah.  There's one chip in here.  But something - probably was a defect or whatever.  But he brought this by, and this was the Cypherpunks out of Berkeley were making this DES-cracking machine.  But I bet you this has one one-millionth of the power.



STEVE:  No, no.  I'd drop the decimal point [crosstalk].



LEO:  Yeah.



STEVE:  Oh.



LEO:  The Brutalis.



STEVE:  This thing would just melt DES.  It wouldn't get past the D.  So with all of this background, now you know who Jeremi is.  This is Jeremi.  He tweeted:  "I've encountered several people lately who use password managers and are generating random passwords, 20-plus characters long, some as long as 200."  That's all he said.  But the message there, the point is he, even this guy knows that, if your password has a large alphabet, meaning you can get upper and lower, special characters, and numerics, and a good source of entropy produced it, you don't have to worry if the website will only give you 16 characters.  Because I see people complaining about 16-character passwords all the time.



Yes, if you're trying to fit your mother's maiden name and your first pet's name into 16 characters, you have a problem.  But if a high-entropy source has generated that, even Brutalis will have a problem because the address space, I mean the attack surface, is so big on a 16-character - so it's, what, 95 is the typical alphabet size when you've got everything engaged.  So that's 95^16.  That is a very big number.  And even a high rate of guessing, which Brutalis would bring to the party, is still going to make it very difficult to crack.  And we hope then that the website did some PBKDF2 work on the backend.  That is, it took your very, you know, your sufficiently long, very high-entropy password, and then made every single guess difficult.  So anyway, I loved the picture of this monster machine.



LEO:  I love the name.



STEVE:  It's like, oh, Brutalis.  Wouldn't it be fun just to have one sitting over there thinking about something.



LEO:  Yeah.



STEVE:  So Netgear's in trouble again.  A security researcher, Pedro Ribeiro, discovered vulnerabilities, multiple unfortunately, in Netgear's WNR2000 routers, including a zeroday flaw that could be exploited remotely.  Now, actually, as I read this now, the headlines I saw said zero-day.  The text says that.  But that's only, as we know, true if it's actually in use, that is, if it was found being exploited.  So I think that's not what I mean to say.  Let's not call it a zero-day flaw, but it is a present flaw.  It can be exploited remotely to take full control of the device if remote administration is enabled.



Now, okay.  No one should have remote administration enabled.  What was somewhat surprising is that his scan found at least 10,000 vulnerable routers, that is, vulnerable Netgear WNR2000 routers, even though the current firmware, v5, has remote administration disabled by default.  And in fact I have to think that remote admin has been disabled by default for some time.  So unfortunately, these are not routers owned by listeners to this podcast because I know no one has turned on remote admin on their routers.



LEO:  I hope not.



STEVE:  So what must be happening is that maybe there are some ISPs that deliberately configure them with remote admin so they can do remote support for their non-technically literate customers.



LEO:  You bet.  My Comcast router, cable modem router, my business-class cable modem router is writable from Comcast.  And they insist on that so that they can, you know, "fix it" if I should break something.



STEVE:  And so that's one reason for the - because I'm a Cox cable subscriber, and it's the same situation.  But of course I have a separate modem and then a little PC running pfSense.  So they're welcome to fuss around with the cable modem all they want, but they're not going any further than the front door, you know, the WAN interface of the pfSense Firewall.



So there's the sad thing.  He attempted to responsibly contact and notify Netgear of his findings, then decided to publish the advisory and release exploit code, which he has done, when Netgear never responded to his emails.  So that's difficult to forgive on Netgear's part.  The vulnerabilities that he found were in the Netgear WNR2000v5, which as I said before does not have remote admin enabled by default on its latest firmware.  So users have to enable remote admin.  Probably that's how this happened.



And what he discovered was that the web service, which in this case is uHTTPd is the daemon that's running in the router that provides web services, it is exposed to the WAN interface if remote admin is enabled.  Probably you connect to port 80, and it shows you a web page where you log in.  And so it turns out that you are able to get to the CGI scripts that are on that router behind the web server and leverage them to do all kinds of things like change the password, reboot the router, and other things that should require authentication which don't.  And he found a stack-based buffer overflow that he was able to leverage into a remote code exploit.  So not good.  He did a scan, found 10,000 of them with this front door wide open to the public Internet, told Netgear.  They never replied.  So after giving them time he said, okay, and he's now gone public with it.



So I don't know what you do in a case like this.  I mean, this is so bad for this kind of user.  Ten thousand routers are no doubt going to be taken over in short order because Netgear can't be bothered.  I almost think in a situation like this a security researcher should just be quiet.  That is, if Netgear's not going to respond, then he's done what he can to notify them, but it's going to take them to fix the problem.  I guess you could argue that going public forces their hand and will get them to fix the problem, which I guess that's better.  But, boy, you know, that one's a real toss-up because, yikes.  I mean, basically the fault is Netgear's for being a provider of widespread networking routers and not being there to handle security.



I imagine at some point we're going to see some legislation.  I mean, I don't know how we're going to get around some sort of requirements about security because we keep running across stories, for example, the Mirai botnet that brought down the East Coast DNS services, that was based on IoT and apparently some routers and cameras and things, and a DVR.  Yikes.



Okay.  So in the future we will do a deep dive into TLS v1.3.  We've covered over the years the progression of SSL.  We've done several podcasts about the secure sockets layer, SSL.  TLS of course stands for Transport Layer Security.  It has been at 1.2 for about eight years now.  And the news is that TLS v1.3, the design was just finalized.  The biggest practical development in crypto, argues the EFF, for 2016, that is all of last year, was Transport Layer Security v1.3.  TLS, as we know, is the most important and widely used cryptographic protocol and is the backbone of secure Internet communication.  I guess I would say that we know that it's built on top of TCP, which is built on top of IP, the Internet Protocol, which then carries the TCP, which then carries, once upon a time SSL, now TLS, in the multilevel or multilayer hierarchy.



So after years of work by hundreds of researchers and engineers, the new TLS design is considered final from a cryptography standpoint.  The protocol is now supported and available in Firefox, Chrome, and Opera. And although the naming, you know, calling it TLS 1.3 makes it seem like a minor version upgrade, it is, and I'm really happy for this, a major redesign from TLS 1.2, which, as I said, is about 10 years old.  In fact, one of the most contentious issues was whether the name should be changed again, much as we changed SSL to TLS, specifically to indicate how much an improvement TLS 1.3 really is.



On the user-facing side, we will probably see, once support is ubiquitous, a noticeable improvement in speed.  TLS 1.3 has been tuned for speed by incorporating a lot of earlier research, reducing the number of network packet roundtrips required before data can be sent, either down to a single roundtrip or, in the case of repeated connections, zero roundtrips.  And these ideas, as I mentioned, have appeared before in experimental form with the QUIC protocol.  And there was another one called False Start for earlier versions.



So those were sort of grafted add-on, experimental add-ons to earlier TLS versions, but now have become part of the default behavior of TLS 1.3.  And they'll become, over time, much more widespread.  And of course this is important because our web pages are getting heavy.  They're increasingly consisting of a huge number of individual assets, each of which requires - increasingly is coming from different locations and so requires different connections to be set up.  So this should reduce latency and improve web page loading times.



And there's also big improvements in security.  It incorporates two important lessons, which is music for me, from decades of experience with TLS.  First, the protocol has been simplified by removing support for a number of old protocol features - yay - and obsolete cryptographic algorithms.  And it was also designed with the benefit of a developmental technology known as model checking, which has been used to find flaws in many older versions of TLS and SSL.  But TLS 1.3 was analyzed extensively, using this model checking, by the cryptographic community before the standardization process, instead of, as has been done until now, waiting until the protocol is widely deployed and thus, as Apple as found, difficult to patch in the field.



So this is great.  You know, it is, I would argue, this is the core protocol of the Internet, and even more so going forward.  As we know, it provides authentication and privacy, those two things we have to have together in order to know who we're talking to and in order to protect the secrecy, the privacy of whatever information is exchanged.  So in the future we will do a detailed walkthrough about what the new features are in 1.3.  I just wanted to mention that it exists, and it's coming soon to all of the technology that surrounds us.



LEO:  Nice.  We've got some teasing, exciting things to come up, including a show that you said I have to watch.



STEVE:  Yeah, and I'm batting a thousand so far in recommending it.  So SSD, easy to remember, ssd.eff.org.  That's the domain name that they gave their page, ssd.eff.org.  I can't recommend this highly enough.  SSD is the abbreviation for Surveillance Self-Defense.  It's in three broad categories, to give you a sense for what's there.  Overviews is a section that contains subtopics:  An Introduction to Threat Modeling; an Animated Overview: How Strong Encryption Can Help Avoid Online Surveillance.  Another one:  How to Make a Super-Secure Password Using Dice.



LEO:  I love it.



STEVE:  Yup.  Another animated one:  Protecting Your Device From Hackers; and Using Password Managers to Stay Safe Online.  Then there's, also in Overviews:  Choosing Your Tools, Creating Strong Passwords, Keeping Your Data Safe, Seven Steps To Digital Security, What Is Encryption?, and Why Metadata Matters.  Then they have a whole series of how-to tutorials:  How to Avoid Phishing Attacks; Circumvent Online Censorship; Delete Your Data Securely on Linux, on Mac OS X, on Windows; Enable Two-Factor Authentication; Encrypt Your iPhone; Install and Use ChatSecure; Use KeepassX; Use Off the Record (OTR) for Mac, for Windows, for Linux; Use PGP for Linux, Mac, Windows; Use Signal for Android, Signal on iOS; Tor for Windows, Tor on Mac; WhatsApp on Android, WhatsApp on iOS.  So really comprehensive.  And then they have what they call Briefings for the third category:  An Introduction to Public Key Cryptography and PGP; Attending Protests (International).



LEO:  Oh, I like that, yeah.



STEVE:  Yeah.  Attending Protests in the U.S.; Choosing the VPN That's Right for You; Communicating with Others, wonder what that's about; How Do I Protect Myself Against Malware?; Key Verification; Protecting Yourself on Social Networks; The Problem with Mobile Phones; and Things to Consider When Crossing the U.S. Border.  So just a really, really neat-looking, comprehensive, across-the-board set of coverage of interesting things from the EFF.



LEO:  Now, I notice this video comes from Al Jazeera Plus.  So it sounds like they've collated material from a variety of sources.



STEVE:  Oh, right, right.



LEO:  As opposed to all original.  But I'm sure it's good.  These guys know what they're doing.



STEVE:  Yeah.



LEO:  Yeah.  Really, really nice stuff.  This is ssd.eff.org.  And if you use it, and you tell your friends, you might consider becoming a donor, as I am, to EFF.  I think they do really good work, and they're one of my monthly charities.  And I feel good about it when I see stuff like this.



STEVE:  Yeah.  So as I said, next week our topic will be a look into this Russian PHP-based malware.  We'll share what we know, and then we'll appraise what we think of what we know.



LEO:  Yeah, that's what I'd like is, I mean, Dan Goodin...



STEVE:  To put it into context.



LEO:  Yeah, in Ars Technica basically mocked the report, but also pointed out that it's possible they couldn't actually give us a substantive report without revealing techniques and so forth.  So I don't know.



STEVE:  Yes.  And this is the problem.



LEO:  It seems like a very sloppy report, to be honest with you.



STEVE:  Yeah.  Well, it looks like the breakdown of the malware was really itself interesting.



LEO:  Yeah.



STEVE:  So I think there's probably deliberate obfuscation.  And the problem is we just, on the outside of the inside security community, we don't know how much we don't know.  We don't know what is known.  And as we often talk, attribution is difficult.  Well, for example, the fact that malware associated with Russia was on that laptop doesn't even mean that it came from Russia.



LEO:  It's from Russia, yeah, of course not.



STEVE:  I mean, it just, you know.  And as I said, a lot of malware is Russian.  A lot is Chinese.  I assume that there's a lot that the U.S. is doing, too.  I mean, say Stuxnet, for example.  I think that qualifies.  If you're on the other side looking at us, that's malware from the U.S.  So, yeah.  Anyway, the technology is what's interesting.  The rest is just unknowable.  And there's really not much we can say one way or the other.  And we know how difficult attribution is.



So I do have some fun miscellany.  And first of all, last week's rerun from years before of the Portable Dog Killer episode was a huge success.  Many people enjoyed hearing it again because it had been so many years since they had.  And, as I expected, since then we have acquired many more listeners, for whom it was the first time through.  And I got a lot of great feedback from that.  One question I saw many times caught me up short, and I thought, isn't that interesting.  And that is, you don't have a picture of the Portable Dog Killer?  And I bring this up because I realized, no.  That was 1970.



LEO:  In those days you had to have a camera with film in it.



STEVE:  Yeah.  And photographers had cameras.  And pretty much there was - you might have a Brownie.



LEO:  Instamatic.



STEVE:  Or an Instamatic, yeah.  And I thought, isn't that an interesting change, that now no one doesn't have a camera.  And photographers were like a profession.  I mean, they still are, obviously.  But you didn't have just everybody with a camera.  And so, yeah, I mean, I've got pictures of what I eat every day, but I didn't have a picture of the Portable Dog Killer because no one took pictures of everything.  I mean, first of all, there was nothing to do with it.  If I had a picture, I could show four people.  But there was no Internet.  And so there was nothing to do with photos.  There was nowhere to put it that it would be seen.  So I thought, wow, isn't that interesting that, I mean, I know in my head I can still see it.  I know exactly what it looked like.  I described it during the podcast.  I still see it clearly.  But it didn't even occur to me to take a picture because why?  I had it.  And there was no Internet that would allow any kind of a broadcast of a picture.



LEO:  Right.  Even if you had a picture, it's in a shoebox somewhere in Mom's house.



STEVE:  Yeah, exactly.  I mean, if I'd been on the news or something, then it would have been broadcast.  But that was literally, when you think about it, look at the change now where everyone has access to broadcast technology.  And back then there was none.



LEO:  Yeah, it's amazing.  No trivia unshared these days.



STEVE:  Yeah.



LEO:  Hey, are you in chat?



STEVE:  Yeah, I have it running in the background.



LEO:  Okay, good.  So people are talking to you in chat.  And I said, "Oh, that's not Steve.  He's never in chat during the show."  But then I looked at who is, and it's somebody coming from Cox in Orange County, so that makes sense.  All right.



STEVE:  Yes.  And I fired it up, and then I put it behind my PDF here.  It doesn't know me, so I can't ever say something quickly, so I just left it running because...



LEO:  We can handle that.  In fact, what you probably - I'll defer to the chat mods because I don't really run this thing.  I don't know how it works.  But you probably want to register that name so that no one else can use it.  And at that point, once you register the name, the mods can make you permanently voiced and opped.



STEVE:  Oh, okay, cool.



LEO:  Yeah.  But you'll need to investigate that.



STEVE:  Okay.  Looking back on 2016, I also wanted to remind people that, without a doubt, The Sequence was the Puzzle of the Year.  As we know, puzzles have become a staple of the podcast.  And I get lots of referrals.  I look at them.  Most of them don't stand up.  But The Sequence, not only did it stand up, no puzzle has generated anywhere near that much positive feedback and satisfaction and happiness and joy.  I sent gift links to my brother-in-law and nephew, his son, after Christmas, just because I thought, you know, I'm sure they don't know about it.  I want them to have copies.  You know, it's 99 cents.



So if anybody missed that, if they're listening to the podcast since then, or they didn't get around to it, I just thought I'd say, you know, it is wonderful.  It is essentially sort of - it's visual programming.  You're solving an animated visual puzzle with little widgets that each do their own thing, and you have to put them together in the right place to move the little hockey puck around.  And it's just - it's a perfect puzzle.  So I just wanted to acknowledge that The Sequence was the puzzle of 2016.  And without question, the most retweeted thing that has ever been said, and definitely the top slogan of 2016, was "The 'S' in IoT stands for 'security.'"  Everybody loves it.



LEO:  And I did not get it at first, so there you go.



STEVE:  Yeah, well.  So thus its subtlety.  Okay.  On December 23rd, two days before Christmas, Netflix dropped a new series.  I was up till 3:30 a.m. Monday morning because I couldn't stop.  It was just, okay, just one more.  Okay, okay, just one more.  "Travelers."  I didn't know why it was so good.  But the name Brad Wright that is prominent on the screen, he produced the show, and he's the writer on many of the episodes.  I thought, why is that name so familiar to me?  Well, he's a Canadian television producer, screenwriter, and actor, best known as the creator and co-creator of the television series "Stargate SG-1," "Stargate Atlantis," and "Stargate Universe."



And I've mentioned "Stargate" before.  It is one of the best, little-known, often-missed, really well-produced and assembled science fiction series.  Whereas Kirk used the Enterprise to fly around and get in trouble, these guys used a discovered alien artifact which allowed wormholes to jump them around.  And so it was very much the same sort of vehicle.  You went somewhere.  You dialed a gate address, and you didn't know what you were going to find.  So they'd send a little probe bot through to see if the air was breathable and look around; and, if so, the team would go through.  Anyway, fabulous series.  So I thought, okay, that's why I know his name.  And that's why "Travelers" on Netflix is amazing.



LEO:  Sci-fi?



STEVE:  Sci-fi.  Now, when I first heard the description, and this is not a spoiler, you know, I don't do those except maybe for this week's picture, unfortunately.  But I just had to, it was so good. But so the non-spoiler background is, in the future, we have really messed up things.  And while...



LEO:  The future?



STEVE:  In the future.



LEO:  Why wait till then?



STEVE:  What could possibly go wrong?



LEO:  The future?  Okay.



STEVE:  So, like, most of humanity has been killed off, for example.



LEO:  I'm hoping we'll have unscrewed up things by the future.



STEVE:  It doesn't look like that's where we're headed.  So, though they cannot send things back, because of course you can't, it turns out they can send personalities back.  They can send people packages, whatever you want to call it, to essentially take over people in the past.  And so when I hear that, I go, oh, no, what a cheesy concept.  I mean, like how could you come up with a less expensive way of doing science fiction than suddenly have an actor go, "Oh, I'm different now.  Now I'm from the future."  It's like, okay.



So if you watch the first one, be warned.  You may not sleep.  There are 12 episodes in the first season.  Yes, there will already be a second one, but we're going to have to wait a year.  Pace yourself if you can.  I was unable to.  I managed to cut myself off after four hours on Sunday night.  And then when I dipped in on Monday I could not stop.  So I went till 3:30 a.m. and finished eight episodes the second night.  It is surprisingly well written.  It's the writing.  It incrementally reveals more information.  What I told you, you figure out almost immediately, so thus it's not a spoiler.



But believe me, I mean, we loved "Stranger Things."  This thing is right up there.  I told Jenny, Jenny and her mom.  I got a text from her this morning saying they loved the first hour.  Three other people I've told just immediately went crazy.  I tweeted it two nights ago at the beginning of Night Two for me.  I said, look, I'm going to be talking about this on the podcast Tuesday, but you may not want to wait.  I got a lot of people who agreed with me.  One person said it started out slow.  And I'm thinking, okay, what show are you watching?  Maybe he had the wrong - "travellers" with two L's will get you something.



LEO:  He was hoping Arnold Schwarzenegger would be in it.



STEVE:  I don't know what's going on.  But anyway - oh, and it's also produced by and stars Eric McCormack, who is, of course, Will from "Will and Grace."



LEO:  Oh, I love him.  He's great.



STEVE:  So, yeah.  He's a great actor.  The acting is good.  Just I can't tell you, just it is so good.  So unreserved recommendation for "Travelers" on Netflix.



I did want to give everybody a heads-up that in two weeks we get, finally, in fact, it had been off my radar for so long I'd forgotten about it, Season 6 of "Homeland," Carrie coming back.  Because "Homeland" is just a great series on Showtime.  And "The Expanse," the second season of "The Expanse," which was a very well-liked sci-fi series.  I read the whole book trilogy, and then they kept on writing more.  The first season we were all remarking was really well done, surprisingly well done.  I was very impressed with it.  And it's a cool concept that we were just about to find out about when the first season ended.  So you have one month, if you want to rewatch the first season to get ready, because Season 2 begins on February 1st, so four weeks from today.



And SpinRite.  I didn't get a chance to talk about this.  This was in my notes from last week.  Yet another thing that in all the times we've talked about SpinRite, I have never mentioned.  It was introduced on December 18th by someone, Jason, who tweeted me a picture of something he says he has never seen before.  Actually, a friend of his.  So he said:  "@SGgrc Friend of mine says he's never seen cabling errors before.  What's going on here?"  And then he sent me a picture, which is in the show notes, which shows very healthy-looking SMART data with ECC corrected and relocated sectors all up at max.  But SpinRite is detecting cabling errors and showing the total of - there were more than 19,000 of them at however far that had gone, with a pretty constant rate of them.



And that's something I've never talked about, but it's another one of the cool things that SpinRite will do because, think about it, and it's something we don't tend to think about, you need a good cable between your drive and your motherboard.  You've got smarts in the drive now; but you need to make sure that, when you're sending 4,096 bits up the cable to create 512 bytes for a sector, that what the motherboard intends to send, the drive receives correctly.  And if you had a flaky cable, you could be recording data incorrectly, even though the drive was recording what it received correctly.  So since that was available, of course I brought it out to the UI.



And so what happens is there is, in the protocol, an error correction.  I'm sorry, it's not error correction.  It is essentially a CRC, a cyclic redundancy check, which the sender appends to the data, and the drive receiving it verifies.  And if it gets a CRC fail, meaning that the data that the motherboard sent had an error in transit, then the drive, thank goodness, will spot that and fail the transfer.  And inside it, it counts, in its own data, that it received the error.  So when SpinRite queries the SMART data for all of these variables, if it sees that the cabling error count is increasing, it immediately locks onto it and then starts tracking it and showing you the number and rate at which those are occurring as SpinRite runs.



So it's another example of something where it only shows up under use, in the same way that error correction or ECC retries or the dynamic relocation of sectors only occurs when it's in use.  Similarly, the cabling errors, you're not going to see it if you just, like, look statically.  You need to put it under load.  So just one more benefit of SpinRite, and something that it's really fun to have surface out on the UI.



LEO:  The topic of the hour really was started some time ago when somebody emailed us, right, about those weird CAPTCHAs, where you click a button and say "I'm not a robot."



STEVE:  So, yes.  I was using the 'Net and encountering this checkbox.  And I'm thinking, wait a minute.



LEO:  Are they secure?



STEVE:  It just asked me if I'm a robot?



LEO:  No, no robots here.



STEVE:  And I just say no?  And every time I click it it goes, okay, and then lets me do what I'm going to do because after all, despite what some people may say, I'm not a robot.  And so I'm thinking, okay, what's going on here?  Because, I mean, we've covered CAPTCHAs.  We did an episode on it, this whole idea of coming up with a way to prove that you're a human because many situations don't want automated bots to do stuff.  And of course we were talking last week, or, sorry, two weeks ago, the last podcast, about the ticket purchasing business and how finally legislators had said, okay, we're just going to make it illegal for bots to bypass anything that is trying to prevent a bot from purchasing so that, if the seller of a ticketing website wants to only sell to people, and they implement any technology to distinguish people from robots, it is now illegal to bypass that.  So how does this CAPTCHA thing work?



Okay, well, first of all, this is a Google property.  So this is a service, this CAPTCHA, reCAPTCHA, I'm not a robot thing, a service from Google.  They're not telling us exactly how they're doing it.



LEO:  Oh, interesting.



STEVE:  Because they don't want, you know, the more you know, yes, this is a little bit of obscurity, but the more you know, the more possible it becomes to defeat it.  So thanks to the fact that it's a server-side thing, there's no way to analyze what's going on on the web page and figure out what's happening.  But there's been some probing done because of course we can probe it.  So one investigator believes that using incognito mode blocks the easy checkbox.  And I think that's probably true because I'll tell you why.  Another investigator has partially spoofed the system by using a B-spline mouse path with randomized way points and destination.  And it's been determined that the user's browser must be able to render the canvas, meaning that you must be able to use JavaScript to draw onto the surface of the web page.  So, and those are like the bullet points from several hours' worth of digging that I did.  And here's what I came up with.  What Google is doing is everything.



LEO:  Of course.



STEVE:  Yes, of course.  Think about what Google actually knows about us.  If this is a "I am a robot" thing, CAPTCHA, that is being displayed on my web page from a Google property, then that server got my Google cookies.  So it knows who I am.  It's got my Gmail.  It knows the IP from which I'm making the request.  It knows where I've been, what devices I'm using.  Think about, like, what Google knows.



So essentially this is comprehensive reputation verification.  And they are also monitoring the mouse pointer and looking at the rate of movement and the path the mouse makes to see if it looks automated, or does it look human-ish?  Do you always stop dead center?  Or as you click from time to time are you stopping in different places?  So essentially they are doing a highly comprehensive analysis of your interaction at the moment, and your entire history that Google has on tap based on your Google identity, which is sent from your browser because this property comes from Google.  So your browser sends a cookie.  Google says, oh, you know, Steve logged into Google Drive to prepare the show notes an hour ago, and he's been apparently homebound over the holidays working on stuff, and this is his IP that hasn't changed since the power outage three months ago when his cable modem came up with a new IP.  So, yeah, we're really pretty sure this is not a robot.



So with a final verification of watching the mouse move and him clicking on the box, we will draw some stuff there, make sure that the surface he's viewing is renderable using the canvas API, all that goes well, yup.  And look at what we get in return for that.  We just get a checkbox.  No more house numbers at an oblique angle at midnight with a poorly lit light.  And, I mean, there are some CAPTCHAs, you look at them and go, I'm really sure I'm not a robot, and I still have no clue what that says.  So that's the story.  And once you see it, it's like, oh, yes, this is what you would want to do.



Only someone like Google could, that is, or Facebook could.  But somebody who has a comprehensive background based on behavior and IP address and constant interactions with their service that occur frequently enough that they're able to develop a comprehensive belief in who you are.  And I'm sure, if you started suddenly to behave like a robot, they'd mark that down in your "I'm not a robot" skepticism column in some database somewhere, and you'd have to be filling out - oh, and by the way, if it's not sure, you still will be prompted.  If you click "I'm not a robot," and it goes, eh, okay, after this question, you can still get a picture and have to fill out what the numbers are, if you need to prove to it.



And so, for example, this is why incognito mode blocks the checkbox.  Incognito mode strips your queries of cookies, so your query is coming in, maybe from a known IP, but that's not enough.  Google needs to know the logged-in user.  And so it'll say, you know, we want to have a reliable "I'm not a robot" detector.  So if you use incognito mode, you've got to fill out this little questionnaire here.  Otherwise, just check the checkbox.  So I thought that was cool.



And again, it's like any, as we originally talked about it, this is a difficult problem to solve if you constrain your solution to just the instantaneous interaction of the user and the web page.  And we could argue it's probably impossible.  It's an intractable problem because, for example, we've seen these things being exported to other countries in sweatshops.  Then all they do is spend all day typing in what the image says and exporting the results back.  So it's spoofable.  But this is not spoofable.  This is your full reputation and past history brought to say, yeah, you know me.  I'm not a robot.



LEO:  Well, it's funny because every once in a while I'll click the button, and it will ask me for more information.  So maybe there are things sometimes where, I think, unless my memory's wrong, but I think I've clicked it, and it's occasionally said, okay, now you have a real CAPTCHA.  So maybe I didn't satisfy its...



STEVE:  Yeah.  And no doubt there's heuristics going on.  Maybe your IP address had just changed.  Maybe you were using, you know, you're constantly setting up new computers.  So a fresh computer wouldn't have Google properties and Google cookies and things registered to it yet.  So it could be lots of stuff.  



LEO:  Yeah.



STEVE:  And it's probably, they're probably sucking in your browser headers and saying, oh, yeah, we've seen that browser before.  That's definitely Steve.



LEO:  Yeah.  It just points up how much Google does know about you.



STEVE:  Yes.



LEO:  Yeah, yeah.  Very nice, Mr. G.  Good way to start 2017.  And end right on time, I might note.



STEVE:  Uh-huh.



LEO:  You are a master.  I don't know how he did that.  Back-timed it.  Steve Gibson's at GRC.com.  That's his website, the Gibson Research Corporation.  That's where you'll find SpinRite, the world's best hard drive maintenance and recovery utility.  You also find all his great cool stuff that he offers.  SpinRite's the only paid thing.  Everything else is free.  And podcasts.  All of these, all 593 of them, audio as well as transcripts at GRC.com, as well as the show notes.



We have audio and video at our website, TWiT.tv/sn.  We are also on YouTube.  You can watch us now on YouTube Live - thank you, YouTube - YouTube.com/twit.  So you can watch live when we do the show, there or on our website, we have many live streams, or on one of those great apps.  There's, like, five Apple TV apps.  There's a Roku app.  A bunch of apps.



You should tune in if you want to see it, though, every Tuesday, 1:30 Pacific, 4:30 Eastern, 21:30 UTC.  I give you the UTC so you can do your own math based on your location.  Join us in the chatroom, too, where Steve has unaccountably appeared, irc.twit.tv.  I said, "Oh, no, that's not Steve.  Steve's never in there."  They said, "No, it is Steve."  Register your nick.  You can also, let's see, did I say everything?  I think I did.  Subscribe, that way you don't miss an episode, to Security Now!.  Thank you, Steve.



STEVE:  I will see you next week to talk about what we know of Russian malware.



LEO:  I can't wait.  And I'll be watching that show all week long.



STEVE:  Oh, boy.  "Travelers."  "Travelers."  "Travelers." "Travelers."  Yes.  And we will do a - we'll talk about it next week.



LEO:  Canceling my weekend plans now. 



STEVE:  Perfect.



LEO:  Thanks, Steve.  We'll see you next time.



STEVE:  Okay, buddy.  Thanks.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#594

DATE:		January 10, 2017

TITLE:		A Look Into PHP Malware

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-594.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  This week, Leo and I discuss the U.S. Federal Trade Commission's step into the IoT and home networking malpractice world, a radio station learning a lesson about what words NOT to repeat, Google's plan to even eliminate the checkbox, a crucial caveat to the "passwords are long enough" argument, more cause to be wary of third-party software downloads, a few follow-ups to last week's topics, a bit of miscellany, a close look at the government's Russian hacking disclosure, and a well-known piece of (related?) PHP malware.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  In a little bit we'll cover that PHP malware that the Defense Department says was used against us by the Russkies.  We'll also talk about the FTC going after D-Link - oh, this is interesting - and why Steve is going to stop using the "A" word.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 594, recorded Tuesday, January 10th, 2017:  A Look Into PHP Malware.



It's time for Security Now!, the show where we cover your security, right now, yeah, with an explanation point.  And that's how right now it is.  Here's the guy in charge, Steve Gibson of GRC.com.  Live long and prosper, Mr. Gibson.



STEVE GIBSON:  Yo, Leo.



LEO:  You need the Vulcan salute.  With thumbs out; right?  That's what we decided?



STEVE:  Thumbs out, yes.  I'll have to get in the habit.  Not that I often do the Vulcan salute, so it's not a big, not a high-demand gesture.  But we did learn from Nimoy himself that that's the way it should be done.  So that's how it shall be done.



LEO:  Nimoy himself has said.



STEVE:  So we said last week that we were going to cover this PHP malware.  And I want to.  But the Daily Beast had such nice coverage of...



LEO:  Oh, it's Kevin Poulsen, that's why.	



STEVE:  Yes, such really good coverage of what happened between reality and the government, that I thought we've got to talk about that, too.  So that'll be the main topic at the end.  And we've got in the meantime a bunch of news.  We have the FTC finally beginning to step into essentially the world of IoT and home networking malpractice.  A fun story about a radio station learning a lesson about what words not to repeat.  Google now has plans to eliminate even the checkbox that we talked about last week with something called an "invisible CAPTCHA."



Also some little quick follow-ups to some things we've been talking about recently.  A crucial caveat to the "passwords are not long enough" argument.  More cause to be wary of third-party software downloads, as if we needed any more cause.  A few follow-ups, additional things to last week's topics; a little bit of miscellany; and then we're going to talk about this whole sort of across the terrain of these Russian hacking allegations and essentially the way the government, for some reason, mishandled the thing as poorly as they did.  So I think another great podcast.



LEO:  Lots to talk about.  All right.  Let's get going here.



STEVE:  So our Picture of the Week.



LEO:  I love this.



STEVE:  While you were telling our listeners about Boll & Branch, I was looking more at it, sort of checking the claim that it was authentic.  And then I realized you can see the person taking the picture...



LEO:  You can, yeah.



STEVE:  ...in the background, on a Samsung phone it looks like because I can sort of see a little bit of that vertical camera silvering that you see, and their thumb.  Anyway, this is from the "this never gets old" department of yet another instance of Windows popping up when you not only least expect it, but really least want it.  The caption that accompanied this tweet was, "I just wanted some water."  And what the picture is, for those who can't see it - it is in the show notes, of course - is it looks like a very high-end, fancy, highly automated refrigerator.  I'm not sure what you would need the screen for, though, because you've got three choices down below:  Water, Crushed, and Cubed, meaning I'm sure ice, crushed ice and cubed ice.  We've got a Lock Controls and a button to turn the light on.  I can't see what the other...



LEO:  It's got plenty of buttons, yeah.



STEVE:  They're sort of cropped off.  Anyway, the point is that dominating this is Upgrading Windows, and it is at this point 32% along.  And down in sort of blurry, out-of-focus print at the bottom it says, "Installing features and drivers, 6%."  So anyway, somebody has embedded Windows in their whatever this is, looks like maybe a refrigerator.



LEO:  Maybe.  It could just be an image somebody put on the refrigerator.



STEVE:  They went to some lengths, though, if they wanted to spoof this, because we're picking up a reflection from the Upgrading Windows in the upper chrome bezel.



LEO:  Well, somebody in the chatroom says, you know, it could just be they put a wallpaper on there, they put an image on the screen.



STEVE:  If they had access to the OS in the refrigerator. 



LEO:  Right, right.



STEVE:  I mean, that would almost be less believable than...



LEO:  Good point.



STEVE:  ...that Windows actually is upgrading itself.  I mean, this is what this picture suffers from is nobody any longer doubts its authenticity.



LEO:  Exactly.



STEVE:  I'm getting pictures constantly from people in foreign countries, walking by kiosks that are saying, oh, sorry, we're in the middle of upgrading.  And many times they're also sideways for some reason.  The screen itself is oriented in a portrait version, but the boot time and the upgrade is still landscape, so it's all twisted sideways.  But anyway.



So I want to take our listeners through this first piece because the details are what's interesting.  And that is the FTC on the 5th, so last Thursday, in San Francisco federal court, filed a lawsuit against Taiwan-based D-Link Corporation and its U.S. subsidiary, D-Link Systems, Inc., for its failure to take steps to secure their devices, thus leaving them vulnerable to hackers.  And I got the complaint and read through it.  And what's interesting is it made sense as I was thinking about this, I mean, this is the branch of the federal government that is responsible for protecting consumers and dealing with things like fraudulent claims and fraudulent advertising.  And, ooh, they really smack it to D-Link in this.



So this begins saying:  "Plaintiff, the Federal Trade Commission (FTC), for its Complaint, brings this action under Section 13(b) of the Federal Trade Commission Act, 15 U.S.C.  53(b), to obtain permanent injunctive relief and other equitable relief against Defendants [meaning D-Link] for engaging in unfair or deceptive acts or practices in violation of Section 5(a) of the FTC Act, 15 U.S.C.  45(a), in connection with Defendants' failure to take reasonable steps to secure the routers and Internet-protocol cameras they designed for, marketed, and sold to United States consumers."



Then there's a bunch of definitions of who's who that I'm skipping.  So then under "Defendant's Security Failures" it reads:  "Defendants have failed to take reasonable steps to protect their routers and IP cameras from widely known and reasonably foreseeable risks of unauthorized access, including by failing to protect against flaws which the Open Web Application Security Project has ranked among the most critical and widespread web application vulnerabilities since at least 2007."  So this sort of gives us a snapshot into their thinking.



Among other things:  "Defendants repeatedly have failed to take reasonable software testing and remediation measures to protect their routers and IP cameras against well-known and easily preventable software security flaws, such as 'hard-coded'" - it actually says that, 'hard-coded' in quotes - "user credentials and other backdoors, and command injection flaws, which would allow remote attackers to gain control of consumers' devices.



"Defendant D-Link," it reads, "has failed to take reasonable steps to maintain the confidentiality" - oh, and get a load of this in the details we'll get to in a second - "of the private key that Defendant D-Link used to sign Defendants' software, including by failing to adequately restrict, monitor, and oversee handling of the key, resulting in the exposure of the private key to a public website for approximately six months.  And Defendants have failed to use free software, available since at least 2008, to secure users' mobile app login credentials" - because they also include an app that allows you to view their IP camera stuff - "and instead have stored those credentials in clear, readable text on a user's mobile device."  Meaning no attempt to secure these important credentials.



Then, under the section "Thousands of consumers at risk," the FTC alleges - and this is Paragraph 16.  I've jumped way down.  "As a result of Defendants' failures, thousands of Defendants' routers and cameras have been vulnerable to attacks that subject consumers' sensitive personal information and local networks to a significant risk of unauthorized access.  In fact, the press has reported that Defendants' routers and cameras have been vulnerable to a range of such attacks and have been compromised by attackers, including by being made part of large-scale networks of computers infected by malicious software known as 'botnets.'



"The risk that attackers would exploit these vulnerabilities to harm consumers was significant.  In many instances, remote attackers could take simple steps, using widely available tools, to locate and exploit Defendants' devices, which were widely known to be vulnerable.  For example, remote attackers could search for vulnerable devices over the Internet and obtain their IP addresses using readily available tools, such as a popular search engine" - and of course we know what that is, Shodan - "that can locate devices running particular software versions or operating in particular locations.



"Alternatively, attackers could use readily accessible scanning tools to identify vulnerable devices operating in particular areas or on particular networks. In many instances, an attacker could then take simple steps to exploit vulnerabilities in Defendants' routers and IP cameras, impacting not only consumers who purchased these devices, but also other consumers who access the Internet in public or private locations served by the routers, or who visit locations under the IP cameras' surveillance."



And, finally:  "By creating these vulnerabilities, Defendants put consumers at significant risk of harm in a variety of ways.  An attacker could compromise a consumer's router, thereby obtaining unauthorized access to consumers' sensitive personal information.  For example, using a compromised router, an attacker could redirect consumers seeking a legitimate financial site to a spoofed website" - we know that's all too true - "where they would unwittingly provide the attacker with sensitive financial account information.  Alternatively, using a compromised router, an attacker could obtain consumers' tax returns or other files stored on the router's attached storage device, or could use a router to attack other devices on the local network such as computers, smartphones, IP cameras, or connected appliances.



"Similarly, by exploiting the vulnerabilities described in Paragraph 15, an attacker could compromise a consumer's IP camera, thereby monitoring consumers' whereabouts to target them for theft or other criminal activity or to observe and record over the Internet their personal activities and conversations, or those of their young children.  In many instances, attackers could carry out such exploits covertly, such that consumers would have no reason to know that an attack was ongoing.  Finally, during the time Defendant D-Link's private key was available on a public website, consumers seeking to download legitimate software from Defendants were at significant risk of downloading malware, signed by malicious actors using D-Link's private key."



So, wow.  I mean, this is everything we would want to be aired in court, at where we are at this point in time.  This is the government saying, hold on a second.  Here is an egregious blaring instance of somebody selling equipment with hard-to-defend irresponsibility.  To this point we've just sort of been shrugging.  We've talked about all of these issues in the past and talked about D-Link and all of these problems.  But it's just been, so, well, you know, at least it won't affect our listeners because our listeners know better.  But it's affecting everybody else.



So this goes on to then really take D-Link to task.  And I won't go through the details.  But basically it enumerates D-Link's explicit statements of security in their promotional claims, talking about how their products are easy to secure, have advanced network security.  They even talk about 128-bit security encryption, "protects your network with 128-bit AES security encryption, the same technology used in ecommerce or online banking."  And so they basically demonstrate that there are all these claims being made on the promotional side, none of which are borne out by years of actual experience, and well after D-Link absolutely had plenty of time to respond.  They were informed of their private key used to sign their firmware being publicly available for months and took no action to remove it or change it.



Anyway, this thing just basically shreds them.  D-Link's attorneys responded, saying that this was without any sort of value, there was nothing to these claims, and they would be generating a formal legal response shortly.  The second two thirds of this document, and I have a link to the PDF in the show notes, is all exhibits attached, showing snippets from the press, exhibits just from one end to the other, basically substantiating what the FTC is alleging.  And this is the only way I can see that this kind of problem gets resolved, is if the FTC, which does seem to be the right body, looks at the claims being made, does the research to pull together the facts that all of the listeners of this podcast now just sort of take for granted, and says, okay, wait a minute, we're not taking this for granted.  This is wrong.



And I think we're going to see that, 10 years from now, this terrain has changed.  Companies won't simply be able to say anything they want to and sell knowingly insecure products into this marketplace.  So this is the first step that we've seen of this.  I think it's a great way to start off 2017.  Wow.



So I've heard you, Leo, talking about - I think I first heard you on Sunday saying, okay, we're going to make a pledge on this network...



LEO:  Not to use the "A" word.



STEVE:  Not to use the "A" word.



LEO:  Or the "S" word.



STEVE:  Exactly.



LEO:  Or the "G" word or the "C" word.



STEVE:  Exactly.  And really, I only really tend to talk about the "A" word.



LEO:  Okay.



STEVE:  And even in my show notes I said:  "A radio station learns to be careful when saying the..."



LEO:  I think it was TV; wasn't it?  CW Channel 6.



STEVE:  Oh, you're right, right, right, yes, "...A-L-E-X-A."   So of course we know that the perils of automated purchasing are not new.  I mean, especially Amazon, who's got those little Dash buttons all over the place.  The good news is they figured out not to accept another order from a Dash button until the previous one arrives.  But still, I have a feeling that some households are just going to have toilet paper piling up because it just, you know, it's there.  It wants to be pushed.  Which of course is the brilliance of the whole concept.



Anyway, another such instance occurred last week in Dallas, Texas, when a six year old asked her family's new Amazon Echo, she said, "Can you play dollhouse with me and get me a dollhouse?"  It's the household bot.  Why not ask it for what Santa somehow failed to make happen two weeks before?  So "The device, of course, readily complied, ordered a KidKraft Sparkle mansion dollhouse, in addition to" - and I'm not sure how this was connected - "four pounds of sugar cookies."



LEO:  They were probably an additional interaction at another time, I'm guessing.



STEVE:  Exactly.  Something else.  She's like, "Well, you know, while you're at it..."



LEO:  Oh, yeah, long as we're at it...



STEVE:  Yeah.  "While I'm playing with the dollhouse, I might work up an appetite."  So the parents, of course, quickly realized what had happened.  And I guess they weren't able to return them because they donate - I don't know what happened to the cookies, but the dollhouse was donated to a local children's hospital.  So that has a nice ending.  However, the story was picked up and covered by a San Diego, California news station, CW6.  At the end of the story, the anchor, Jim Patton, remarked, he said, quote:  "I love the little girl saying" - and then he used the "A" word - "ordered me a dollhouse."  According to CW6 news, Echo owners who were watching the broadcast found the remark triggered orders on their own devices across San Diego.



Now, the anchor, Jim Patton, didn't think that any of the devices went through with their purchases, although he would want not to.  He told reporters that the station had received reports of viewer devices attempting to order a dollhouse after hearing his remarks.  And of course we note from a security and technology standpoint that the Echo's settings can be adjusted through the device's app.  Users can either turn off voice ordering altogether or add a passcode to prevent accidental purchases.



So I thought this fit perfectly, Leo, with our New Year's Resolution on the TWiT Network not to be glib about using those reserved words because you can actually get in trouble when people are playing these out loud, and we're within her hearing range.



LEO:  I'm a little skeptical about all these claims because at least on mine it then asks me for a PIN code, which I have to enter to confirm it.  And that is the default setting.  So the only way that this story can really be accurate...



STEVE:  Oh I guess they turned them off.



LEO:  ...is if they turn it off.



STEVE:  Right.



LEO:  Which, if you have a kid, is a very bad idea.  And then did all the people watching the show also turn it off?  I mean, what I suspect happened is in many cases, and this is what happens if you order something...



STEVE:  It woke up.



LEO:  It woke up.  It responds and then puts it on the shopping list, but doesn't actually buy it.



STEVE:  Ah, okay. 



LEO:  So the passcode is on by default.  We checked that.



STEVE:  Good.



LEO:  I mean, I guess the parents could have turned it off.  I don't know.



STEVE:  Yeah, and, well, you can imagine that there are - we know that.  There will be some people who will get annoyed by having to give it a passcode every time.  And when they see the option, they think, oh, yeah.



LEO:  There's a good reason to do that.  But even, you know, an ad can trigger it.  Or accidentally we could be having a conversation.  You don't want to trigger sales accidentally.  The passcode is a very good idea.



STEVE:  And I had one of those devices.



LEO:  It's an Echo.  It's always been an Echo.



STEVE:  Okay.  I had an Echo in my living room, where I watch television.



LEO:  Yes, and it would [crosstalk].



STEVE:  And with the volume turned up, it will invariably fire her off every week or so.  She'd suddenly wake up, and the little blue ring would sparkle a bit, and I'd be like, oh, isn't that interesting.



LEO:  But it didn't order anything.



STEVE:  No.



LEO:  Well, the funny thing is on the one in my office I've changed the trigger word to Echo.  So during this whole piece you've been doing it's triggered twice.  So you just can't win.  You just can't win.  There's three possible trigger words:  the "A" word, but also "Amazon" or "Echo."  And, you know, what are you going to do?  There's got to be a solution to this.



STEVE:  I imagine, well, what'll happen - so the reason there are only three is that, as we know, they are pre-burned in the firmware.  And so there is wide and well-tuned generic voice recognition.  In the same way that anybody could walk down the street and say one of those words, and anyone would be able to understand them because we have really good speech recognition built into our brains, those devices have been carefully tuned to pick up those words without training.  The fancy recognition is up in the cloud.  And so as we discussed last week, that word triggers the streaming of a buffer which then sends it to the cloud for detailed, maybe speaker-independent, but also trainable recognition.



Anyway, so the point is that my guess is in the future you'll have much more latitude, maybe complete latitude, where in the same way that we train our fingerprint readers on our smart phones by giving them lots of samples of our thumb many times, and it builds up a composite image, you could train whatever you wanted as the trigger word by saying it over and over and over a number of times, in a number of different situations, different distances, different volumes and pitches.



LEO:  It's not the future, it's the past, unfortunately.  The Moto X allowed you to do that.



STEVE:  Oh, good.



LEO:  You could have any arbitrary trigger word.  And so I actually, I forgot what mine was, something like "Open the pod bay doors, Hal."  But I, you know, made it long and complex and  not likely to happen by accident.



STEVE:  Right.



LEO:  But they did it, and they don't do it anymore, and no one else has done it.  So I don't know.  It's more challenging, obviously.  You have to train it.



STEVE:  Yes.  And I remember seeing you sometimes not getting it to respond.



LEO:  Right, right.



STEVE:  And so that's what you don't want.  You don't want false negative responses.  Neither do you want false positives.  So I think it's that situation where we didn't yet - we didn't quite have the horsepower needed to pull it off well enough for it not to be more trouble than it's worth.  So with today's horsepower, they're able to build very good recognition for a few words so that it will very reliably trigger.  My guess is that downstream, once we're on the A27 chip from - I guess that's Apple, not Echo, but the equivalent - then we'll have more horsepower, and you'll probably be able to just say, "Hey, this is what I want; this is the phrase I want to use in order to wake you up."  And then we'll be in much better shape.  It'd be like everybody having the same password right now.  It's just not a good idea.



So speaking of passwords, some listener - thank you - sent this to me.  I wouldn't have stumbled on it myself.  You can google "invisible reCAPTCHA," and you will find it.  It is in beta.  It is not yet released.  The show notes have a nice picture of the screen you get when you bring up invisible CAPTCHA or reCAPTCHA.  And following from what we talked about, it was the end of our podcast last week, this notion of how can just checking a checkbox differentiate you between human and a bot?  And what we understood was that the original concept of a self-contained test was what was at fault in the very first CAPTCHAs because, unfortunately, computers learned to read.  And if they couldn't, then you could outsource this to sweatshops in other countries, and you'd have human beings reading for you in order to get these little problems solved.



So the proper solution, which we discussed last week, is if you are a company in a position to have a huge amount of background associated with a user, and Google if nothing else is that company, then you're not just bringing, like, solve this little puzzle to bear.  You're bringing, oh, you're logged into Gmail in the background, or any Google property.  We've got your cookie.  We see your IP from which the query is being made.  We know everything you've been doing all morning, all of the previous day, and so forth.  Oh, and we know that this is a site that you tend to visit.  You've done so 26.5 times, because once you were in a hurry and you didn't bother scrolling, in the last year.



So the point is, with all of that knowledge, why even bother with a checkbox?  And they've been incrementally moving in that direction.  And it's in beta now.  Developers can use it.  We'll be seeing it before long.  They simply remove the CAPTCHA completely from the screen.  And it's called the "invisible reCAPTCHA."



They have three ways to invoke this.  You can have a web page that loads automatically bind the "challenge," as they call it, to your own button, or you can programmatically bind the challenge to a button, both of which just mean that you can, when you're designing your page, you can have your own "press here to confirm you're not a bot."  And so it's your own button, not the fancy rendered reCAPTCHA logoed thing, just your own.  And behind the screens that button is invoking a snippet of JavaScript, which is sourced from Google, and sends the query to Google that allows it to do its work, to determine if it believes this assertion you've just made.



The other thing you can do is you can programmatically invoke the challenge, meaning that just loading the page can run the JavaScript, perform the query, and return to code on the page a go-no-go assertion about whether the person who is viewing this page is or is not a bot without the user doing anything.  Which is the ultimate of what we want.  So essentially this is a perfect example of us having sort of passed through a problem that over time, over the course of, what, maybe six or seven years, continued to evolve until other, well, essentially network externalities were able to be brought to bear so that the whole problem just went away.



Now, it's, again, not everyone can do this with the reliability that Google is bringing because not all remote properties have the opportunity to create this kind of knowledge.  Arguably, Facebook probably also does.  But you want somebody that's going to have enough depth that they're going to be able to do this.  And let's remember also that there is something Google is getting in return for this.  For a while, when we were solving images for them, especially when we were doing the one known word and the one unknown word, that's the first time we talked about CAPTCHAs, back in the day, was we were actually digitizing eBooks for Google.



Remember that the idea would be they would verify that we were doing a not-a-nonsense problem solve by presenting a word they did know the answer to, and then also would present one they did not know the answer to.  And we typed in both words and, in the process, performed a little bit of OCR for this big project that was running behind the scenes.  So anyway, essentially this problem is gone now.



Oh, but I was going to say that remember that our browser is telling Google where we are and that it is we who are there.  So in the same way that Facebook's Like button is, while it's convenient, it's also an explicit tracking beacon, so is the ubiquitous presence of this very nice reCAPTCHA technology.  It's all going back as essentially a tracking flag, although lots of pages already have Google Analytics on them anyway.  So they're getting that information that way, even if they weren't performing this nice service for us.  So I expect that, in a while, we'll just stop seeing explicit requirements to declare or prove that we're human, and somehow the system will just know.  And of course now we know how that's happening.



I found an interesting post that reminded me that I needed to clarify something from our discussion last week about brute-forcing passwords.  Remember that we talked about Brutalis, which was this monster multiple-GPU triple-redundant power supply, 3U-high, rack-mounted, industrial-strength cracking machine that this company was selling throughout the world to governments and law firms that wanted access, basically anybody who wanted to be able to brute-force passwords for whatever purpose.  And the argument there was that the guy who had designed this, who had very deep understanding of the difficulty of password cracking of high-entropy passwords, was sort of bemoaning the fact that he was seeing people complaining that 16 characters wasn't enough.  And in fact, by the way, he was aware of our discussion and sent me a tweet saying, hey, you know, that's for the shout-out about our work.



So this post asks the question, and answers it, why are more than, or he says 12-plus, 12 or more character passwords vulnerable?  And he writes:  "Practically speaking, people who manually create passwords above 10 characters for the most part use common words or phrases.  Why do they do this?  Because remembering the password 'horsebattery123' is way easier than 'GFj27ef8%k$39.'"  Now, I read that on purpose because also notice how much more difficult it is even for me to say it than "horsebattery123."  The fact that I can say "horsebattery123" as quickly as I can demonstrates its lack of entropy.  It's the fact that horse is a thing.



LEO:  It's organized.  It's organized.



STEVE:  And battery - yes, exactly.  Battery is a thing.  And 123...



LEO:  I love him, but I wish he'd never done that cartoon.



STEVE:  Xkcd?  Yes.



LEO:  Becomes it comes up all the time.



STEVE:  I know, I know.



LEO:  Everybody's convinced because he's usually right.  Randall, I forget, is his name Randall?  I can't remember.  He's usually right, but this one's not.



STEVE:  So he says, the author of this post:  "It's just simple human behavior exhibiting path of least resistance that will always exist; and, until auto-generating password managers gain mass adoption, this vulnerability will always be around."  He says:  "I agree that xkcd's password strength cartoon for four random words is sound, but only for non-fast hashing algorithms like bcrypt."  And then he finishes:  "In this article we will demonstrate combo and hybrid attacks using Hashcat that will expand your cracking knowledge toolkit.  These examples will show how an attacker can efficiently attack this larger keyspace, with modern hardware, and make these so-called 'strong passwords' succumb to his cracking methodology," "his" meaning the attacker's cracking methodology.



And I won't go any further.  I have the link to this in the show notes.  But what he does is he goes through exactly these examples, like "horsebattery123."  And he uses Hashcat with parameters because it includes a dictionary that has, for example, the word "horse" and the word "battery" in it.



And so you can parameterize the invocation of Hashcat to tell it how you want it to guess.  And he does this with all manner of the typical passwords that people are using, which, even though they may be 12 or more characters long, his demo, he shows you screenshots with the timer.  In five hours, bang, got that one.  In 3.5 minutes, bang, got that one.  In a few hours, bang, got that one.



So what I wanted to make sure people understood was that that second example, that took me 30 seconds just to utter, as opposed to "horsebattery123," that one, that's part of the requirement of 12 characters, or certainly 16, being enough.  That is, and I did say it, but I wanted to make sure it was heard, it has to be a high-entropy password.  That is, it has to be something that either you roll dice, literally, in order to choose, or you went to GRC's passwords.htm page, or you used your password manager's random number generator to produce a string.  People just can't do it reliably.  We have an inherent bias.  There will be a pattern.  Even if we try to generate 10 in a row, they will get linked.



So anyway, I just wanted to make sure that, in discussing why passwords didn't have to be 128 characters long to be good, they could be shorter, but they absolutely need to be high-entropy.  And I also did not mention that the use of, on the server end, on the backend, a password-based key derivation function is crucial.  That's the thing that is deliberately time-consuming.  It takes what the user provides and doesn't just do an MD5 or an SHA-1 or an SHA-256.  Even though SHA-256 is very nice, we now, thanks to so many crypto currencies now using that, we've got hardware that just cuts through SHA-256.  Which isn't to say 256 bits isn't a lot.  That's still a lot.  But it does allow you to do brute-force guessing at incredibly high speeds.



So the point is you can't just do a single iteration of SHA-256.  You have to iterate it.  You have to arrange to make it difficult to accelerate.  And of course that's what password-based key derivation functions are all about doing.  So high entropy, doesn't have to be super long, and we want the server on the backend to protect us.  The problem, of course, is we don't know, we have no control as users on what's going on over on the backend of this.  So all we can do is choose something that's pure gibberish and have it be as long as we're comfortable with and as the website will accept, and hope for the best.



For a couple weeks now I've been seeing a mention of something that I hadn't picked up on.  But we had time this week, so I wanted to note it, a chat protocol which is in many ways the granddaddy of chat protocols, XMPP.  It's been around for almost 15 years.  I believe the author began working on it in the late 1990s.  It was originally known as Jabber and then got renamed XMPP, which stands for eXtensible Messaging and Presence Protocol.  The reason it's on Security Now!'s radar is it has recently received an extension called OMEMO, which gives it end-to-end encryption and support for encrypted group messaging.  So for those who don't know, it's an XML-based, so text-based, messaging platform.



That's sort of a mixed blessing.  The text-basedness means that it is easy to extend, and in fact that it's been extended crazily.  There's a whole bunch of standards.  There's, like, four anchoring RFCs that define the protocol, which because of its age have been replaced and extended several times by successive RFCs.  It enables near real-time exchange of structured, yet extensible data between any two or more network entities.  It's not strong on binary sharing because it is a text-based protocol, which would require that you base64-encode any binaries that you want to send from point to point, which of course makes them about 50% larger, which means that you're going to have - it's going to be slower than a protocol that incorporates an understanding of native binary attachments or content as part of it.



XMPP/Jabber doesn't have that.  But it was designed to be extensible.  And as I mentioned, the extensions are numbered.  And you almost - it's hard to count them.  There are just so many of them.  I mean, which is really a nice aspect of this for something that we want to be able to evolve over time.  And in fact OMEMO is an extension added to the existing framework without the underlying protocol needing to be changed at all.  And so it's in active use.  There are about 10 million users of this as of 2003.  So I don't know if it's gone up or down since then.



LEO:  Down, because unfortunately it was a protocol for Google Talk.



STEVE:  And they abandoned it, yes.



LEO:  And they abandoned it.  So I was really disappointed.  I had very high hopes for XMPP.  We wanted to use XMPP for all sorts of things, for our chat and so forth.  But when Google deprecated it, it was the end of the line.  I mean, Jabber is still used from time to time, but it's just - it's sad because that would have been a universal protocol.



STEVE:  Yes.



LEO:  And now we just have all these silos that are incompatible with one another.



STEVE:  Exactly.



LEO:  In fact, I'm really sad because now that they've added encryption, it would have been a really good choice for chat and stuff.



STEVE:  Yeah.  Well, and it does exist.  There is a nice client called Conversations for Android.  And, I mean, there are clients for all platforms.  OMEMO is a recent - essentially OMEMO took OTR, which is the Off The Record protocol that we've discussed in the past, and added to that multi-user chat and multi-device support, meaning devices owned by a single user.  And OMEMO is inspired and based upon Open Whisper Systems - I practiced pronouncing this before, and now here at the moment I can't, the Axolotl...



LEO:  Yeah, Axolotl, yeah.



STEVE:  ...Axolotl protocol, which we've talked about extensively, which was developed for Signal.  So it's based on the Axolotl Ratchet, which is the way you do this kind of protocol securely.  It's not backwards-compatible with OTR, but it is, that is, OMEMO is being standardized as the new end-to-end encryption mechanism on XMPP.  So that's what everyone's going to be using.  You need no server-side support.  That is, the XMPP server just sort of serves as the hub for Conversations.  And it's only the clients that need to know how to understand this extension.  It's XEP-0280 is the number.  So it needs no server-side support.  And there are other chat protocols, other secure chat protocols in development, OTRv4 and one called n1sec.  But it looks like this one is here now and has a lot of client-side support.  There's Conversations that I mentioned on Android.  There's, is it Gajim, G-A-J-I-M?



LEO:  I don't know.  Sounds like gag 'em.



STEVE:  Gag 'em, probably.  Cryptocat, of course, we've talked about, on iOS, Android, Mac, Linux, and Windows; ChatSecure on iOS and Android.  These all support XMPP.  These are XMPP clients that now support OMEMO.  Monal, or monal, on iOS and Mac; the Tor Messenger on Linux, Windows, and Mac; Instantbird; Jitsi, which is a Java-based platform; Let's Chat; and Pidgin on Windows and Linux.  So anyway, I just sort of wanted to put it on our listeners' radar.  I know that within our group there are people who still refuse to be on Twitter and would probably like the idea.   You can set up your own XMPP server.  Jabber.org exists.  There are both free and very inexpensive commercial services where you can get an account.



And then, if you have a reason for doing group chat, XMPP is there.  And now it can be secure.  It's got the same problems that we're always going to have with authentication.  So you need to go through that step of making sure that the key of someone you think you're talking to is actually their key, but it has provisions for that.  And once you get it up and going, it's apparently very easy to use, and solid.  So anyway, just for if anyone wants to roll their own, essentially, and also use an RFC-based, standards-based, open - everything's there to inspect.  Everything's on GitHub.  There are links to GitHub in many of the clients that I was talking about.  So it's another way to go and still have security.



LEO:  Yeah.  The guys who did, I think, XChat, two of the chat clients, IRC chat clients are also talking about doing their own encryption scheme.  Everybody realizes the importance of encryption now, and they want to put it everywhere.



STEVE:  Well, I just think we're in early days, Leo.



LEO:  That's right.



STEVE:  This will end up getting sorted out.



LEO:  Yeah.



STEVE:  And it'd be like somebody trying to come along and do HTTPQ or something.



LEO:  Right, right.  Not now.



STEVE:  No.



LEO:  Not now.



STEVE:  Sorry.



LEO:  Let's not.  Can you believe $28,000?



STEVE:  Mom will be 90 in March.



LEO:  Don't send her your CDs.



STEVE:  No.  She would put them in the DVD player and try to watch them.



LEO:  You've got good genes.  She's 90 next March?  That's awesome.



STEVE:  Yeah, still going strong.



LEO:  That is awesome.



STEVE:  In fact, she was complaining that she's had a little bit of a problem with macular degeneration.



LEO:  Oh, it's hard to read.



STEVE:  And she's been using Kindles that I provided her with years ago.  So at Christmas I said, "Mom, I want to show you an alternative."  And so I brought up the Kindle software on my iPad.  She says, "Oh, honey, I don't know what Warp Drive is, but I can read that."



LEO:  Oh, nice.



STEVE:  So anyway, that's going to be her birthday present is I'm going to go up and get her set up with an iPad for reading because basically she just loves to read.  She does that.



LEO:  I set up Mom with two Echoes.  And she now listens to audiobooks, which she loves.



STEVE:  Nice.



LEO:  Yeah, yeah.



STEVE:  So I got a couple tweets that I wanted to share.  Our friend of the show, Simon Zerafa, shared a tweet, both with Paul Thurrott and me, from someone named Kevin Beaumont, who said - he's laughing.  He says, "Ahahaha, just found a Windows 10 install ISO on BitTorrent..."



LEO:  Oh, that'll be good.



STEVE:  Uh-huh, "...which includes a scheduled task to download and run a ransomware EXE after 90 days."



LEO:  Nice.  Let's get some data on there and then encrypt it.  Smart.



STEVE:  Exactly.  Exactly.  Wait three months and then lock it up.



LEO:  Wow.



STEVE:  Ugh, yes.  So, and again, Microsoft will pay you to download their Windows 10 install ISO, so don't go get it somewhere else.  And actually the same thing is true, I have a little comment later about GRC's DNS Bench because somebody apparently found, was trying to figure out what was going on and did some googling, and there was a malware site that showed the file data, ostensibly from DNS Bench, that had all the fields empty, like file version, filename, original name, publisher, and so forth.  Well, of course my software has all the fields filled in and is digitally signed.



So the problem is that when something becomes popular, bad guys take the opportunity to bundle malware as that thing and get people to run it without knowing any better.  So be just, you know, especially with the death of download sites.  They've just pretty much become worthless.  Again, it's a nice era we went through, but it's died.



Another tweet raised a good point.  I just wanted to follow up also from last week.  James P. tweeted:  "When I see password length limits on websites, it makes me wonder if they're not hashing the password on the backend."  And I did forget to mention that last week.  Again, people are complaining when websites say "Enter your password, no more than X characters."  And so on one hand I immediately jumped on that as, oh, well, they're upset because it's not allowing them longer passwords, when in fact a very reasonable concern that James raises is that, well, the fact that they even care about the length does worry us that they have a fixed amount of space in their database record for the user's password in the clear.  So they're not doing anything with it except storing it.



Which reminds me of a little giggle I had when I was up with Yubico, demonstrating the SQRL, doing the whole run-through of the SQRL technology and of course showing them a demo, because of course I had to have a password length limit of 256 characters.  And it says that on the field, you know, username - because I support traditional login and then side-by-side SQRL login.  Anyway, the point is that they noticed, in a light gray type below the field, I said, "Password up to 256 characters."  And one of them said, "Who has a password of 256 characters?"  I said, "Well, nobody.  But the point is it can be anything."



LEO:  It's like encouraging to make it longer; right?  Yeah.



STEVE:  Yeah, exactly.  And I of all people have to have a virtually unlimited password length because everyone should.



LEO:  So the point being, once something's hashed, the result of the hash is always the same length.



STEVE:  Yes.



LEO:  That's short.



STEVE:  Yes.



LEO:  Or relatively short.  How long is it?



STEVE:  Well, yeah, 256 bits.



LEO:  Oh, okay.



STEVE:  But if you divide that by eight, what do you get, 32?  So it's only 32 characters.



LEO:  It's not bad, yeah.



STEVE:  Yeah.  Okay.  And lastly, Aaron Bishop provided some interesting addition to our dialogue about Apple's ATS security issue.  Remember we talked last week how Apple had announced at the WDC 2016 last summer that there would be a sunset on the ATS workaround, where with iOS9 it was there, and everybody should use it, but developers could turn it off.  And they said we're going to stop allowing you to submit new iOS apps that turn it off at the beginning of 2017.  They backed away from that.



Well, Aaron wrote:  "I'm an app developer who links to third-party websites and have had issues with ATS on some sites because they don't allow any ECDHE cipher suites."  Okay, that's Elliptic Curve Diffie-Hellman Ephemeral.  And we've talked about that a long time ago.  Elliptic Curve, of course we know, is the faster, shorter keys, strong security.  Diffie-Hellman is the so-called Diffie-Hellman Key Agreement, which is the way you negotiate in public a private key where a bad guy can see a whole dialogue going back and forth but still can't figure out what key you guys agreed to, even though he saw all of the conversation.



And then Ephemeral is really the key.  That's why Apple is enforcing this.  And that is that it is not a persistent, long-lived key.  It is ephemeral, as the word says.  It is being negotiated on the fly on a per-connection basis.  Which gives you perfect forward secrecy, meaning that if anyone were ever to get any one key, it wouldn't help you either with future or previous, to decrypt future or previous conversation.



So he goes on:  "ATS only allows for ECDHE, and my guess is that the sites don't allow it, either because they haven't updated and modified their cipher suites, or they're afraid of lawsuits for using elliptic curve crypto."  And we covered that about a year ago, that there was a patent troll that had sued a whole bunch of large sites, saying that they had a patent on elliptic curve crypto.  It's like, okay, well, we haven't had any - I've been looking for more news of that, but I haven't seen anything.



Anyway, he finishes:  "Just thought you might like to know since I haven't heard anything about ATS requiring Elliptic Curve Diffie-Hellman Ephemeral with regards to patent lawsuits."  Right.  "I don't know if ECDHE falls under that patent, but the key words are enough for a chilling effect."  So I certainly agree with Aaron on that point.  And I agree - I didn't realize, and this is why I wanted to share it and bring it up, is that ATS enforces ECDHE.  Well, that is relatively new.  And it is the case that not all sites are supporting it, even when they do support TLS 1.2 or 1.3 and lots of the other very recent cipher suites.



So I would argue that that was a little aggressive on Apple's part.  It would have been nice maybe if they'd staged that, if they'd said you've got to have TLS, and we'll give you another year for it to be ECDHE.  Because after all, most of the world is not on ECDHE yet.  We're still using earlier cipher suites which are universally available.  So anyway, Aaron, thank you for providing that from a developer's standpoint.



And two last points.  One is that I've been meaning to mention this for a couple weeks because this is a few weeks old now.  Many people have noted this; and I finally, when I was putting this together, I said, okay, I'm not going to forget again.  This was via a Twitter DM.  A listener said:  "Hi, Steve.  Just got an email from Cox."   He said, "Seems to be from them, based on the header."  So he was skeptical.  Good.  He says:  "As I write this, it might be your DNS Benchmark triggered this."  And in fact I'm sure of it.  Cox has an email they're sending out to people who have used GRC's DNS Benchmark.  It reads:  "Cox has identified that one or more of the computers behind your cable modem are likely infected with the Zeus Trojan bot, also known as Zbot."  And it goes on, but that's the headline.



So he says:  "My first question is how can Cox see what's behind our modem?  We're running NAT routers.  Second, the only thing that changed over the past day is that I replaced our previous routers with Linksys running dd-wrt," and he says, "a.k.a. Tomato.  Could dnsmasq be causing this false positive?"  And he says, "Oh.  I also ran your DNS Nameserver Benchmark the other night and rebuilt our list of nameservers.  Perhaps that unusual traffic was it.  Third, when did Cox start caring if a key logger is installed on one of our computers?  Again, how could Cox know?  Is Cox seeing a bunch of data coming out of our router all of a sudden?"  And then he ends saying, parens, "(DNS Benchmark would do that.)"  And he's absolutely right.



A number of people have reported that they've received such an email from Cox.  Clearly there is a one-to-one correspondence between using the DNS Benchmark and receiving this note.  So what has apparently happened is about a month ago, just judging, or maybe at the beginning of the year - no, it's longer than that, I think I got some at the end of 2016 - Cox decided to get proactive, which is a good thing, in looking at their subscribers' traffic.  And again, I just think that's all for the better, that we wish more ISPs would be more proactive.



Unfortunately, running the Benchmark does look like, if you didn't look too carefully, like you are generating a DNS reflection attack because a DNS reflection attack sends a bunch of little queries off to a whole bunch of different DNS servers, and with a spoofed source IP.  Now, of course the source IP is not spoofed.  So they could be a little smarter about this and see that in fact what's actually happening is a bunch of valid DNS queries spewing out from a given client.  So the only one they would ever be DoSing would be themselves.  And of course the Benchmark is very careful about metering those so that you don't saturate your own bandwidth because one of the things that the Benchmark does is check the reliability of the DNS servers.  So I wouldn't want to saturate the connection, or we'd cause packets to be dropped and get false positives on low reliability, that I was careful not to do.



So anyway, for anybody else who has received this letter from Cox and has shot me a note, but I never responded, I'm absolutely sure that there is a correlation.  And it doesn't seem that anything goes further.  Nobody's had their traffic cut off.  Cox is just providing them with a warning.  And that does demonstrate that, on a per-subscriber basis, Cox has deployed technology to notice if the behavior of their subscribers indicates that they may be infected.  So although it's a false positive that occurs when you run the DNS Benchmark, it does say that we're seeing some positive movement in this direction from a major Internet connectivity provider.



And then this is just random, but I thought this was interesting.  The first country is beginning to phase out FM radio.  This is from the "The Way Things Were" department.  Leo, you're probably - you're a couple years younger than me, but you probably remember when FM was new.



LEO:  Yeah, oh, yeah.  Big deal, yeah.



STEVE:  Yeah.  And it was...



LEO:  And it sounded so good.



STEVE:  Oh, so much better.



LEO:  Yeah, it was stereo.



STEVE:  So, yes, exactly.  And so of course we have AM, which stood for Amplitude Modulation, where a high-frequency carrier has its amplitude modulated, that is, the amplitude fluctuates, and that fluctuation is the audio signal that rides on the carrier.  The problem with that is that all kinds of other things could cause - like interference could cause the amplitude to be modulated for other than the audio.  That's why AM is not as good as FM.



FM is Frequency Modulation, where instead of changing the amplitude of this high-frequency carrier, you subtly tweak the frequency, which actually amounts to the instantaneous phase of the sine wave that is being broadcast.  And although you can have problems with reflections, frequency modulating is far more noise-resistant than amplitude modulation between the transmitter and the receiver.  Thus FM was, like, way better.



Two years ago Norway announced they were going to formally phase out FM radio.  And at this point about 70% of the population has the replacement already, DAB, Digital Audio Broadcasting.  And I guess for whatever reason FM, I mean, like for reason of the terrain, actually, in Norway, FM is just - it's difficult to get coverage.



LEO:  Oh, that's interesting.  So that would make sense, yeah.



STEVE:  Yes.  Apparently there are only five FM stations as is, and DAB allows them, within the same bandwidth, to get 20.  So there are 20 DAB broadcasts and only five terrestrial FM.



LEO:  It's very different here.  I mean, this is a very different terrain, yeah.



STEVE:  Yes.  And so the Norwegian government estimated then, and has updated their estimate, saying they expect that, I guess, I don't know how many stations, how many radio station corporate entities there are.  But they expect $23.5 million to be saved annually by the stations switching over.  People are, of course, reluctant to do this.  Sixty percent of the people recently polled said, no, we'd rather just keep what we have.  But, sorry.  So this week the first FM station went dark.  They're going to be blanking them out geographically over the course of the year.  And they will be FM radio-free by the end of 2017.



And really the only losers are - there are about two million autos on the road in Norway that still have FM radios and don't yet have DAB, this Digital Audio Broadcasting.  So there's a bit of concern that lack of broadcast communications for highway-traveling vehicles could pose a safety hazard.  Cars should have some way of receiving notification of problems.  But everybody's going to have phones and things now, too.  So I don't know that that's a big problem.



Radio.no, if anyone is curious, is a Norwegian web page for the association and the country.  And it's all in Norwegian until you get about three quarters of the way down, and then there's sort of like a read English, finally.  I can't tell most of what its saying.  But this is happening.  And the U.K. has talked about doing this, too.  And so it does look like in the long term we may be seeing FM go the way of the dodo bird.  But probably not soon.



LEO:  You know most phones sold outside the U.S. have FM radios built into them.  And the same hardware in the U.S. could do it, but it's disabled by the carriers, who would far prefer you used data and pay them than listen to free radio.



STEVE:  Free music?  What a minute.



LEO:  Yeah.  There is a movement from the National Association of Broadcasters in the United States, trying to force these companies to turn those FM radios on for safety reasons, if nothing else.



STEVE:  Right.



LEO:  Internet goes down, your cell network goes down, you'd still be able to receive a radio broadcast.  I'd hate to see - I don't think that that's - I think it's a nonstarter in the U.S.  Although it reminds me a lot of how the FCC forced television stations to go from analog broadcasts to digital...



STEVE:  Precisely.



LEO:  ...to conserve spectrum.



STEVE:  Right.



LEO:  And resell that really precious spectrum they owned and make billions of dollars.



STEVE:  Right.  In fact, I sort of assumed that's what was going on.  It was like, you know, why force existing infrastructure to go dark when it's already there, and it's been amortized, I'm sure, and paid for, except maybe to recover the bandwidth.  But it looks like they're repurposing it with a digital technology that will give them...



LEO:  More on the same frequencies.



STEVE:  Exactly, more for the same bandwidth, yes.



LEO:  Analog spectrum is limited, obviously.



STEVE:  Yeah.



LEO:  But digital use of a spectrum probably is not unlimited, but it's effectively much, much - it's almost unlimited.  Much better.



STEVE:  Right.  Well, look, think about what comes over a cable.



LEO:  Right.



STEVE:  It's just astonishing.



LEO:  Right, right.



STEVE:  It's just astonishing.



LEO:  You could see why they would want to do that.



STEVE:  Oh, yeah.  So I got a nice note from a follower, Glasair Pilot, with an interesting - and something yet again we haven't ever talked about, about SpinRite.  He said:  "Hey, Steve.  A SpinRite story for you.  I built a RAID 10 recently."  Okay, now, RAID 10, that's a combination of a one and a zero.  So a RAID 0 is where you span a volume across two drives.  A RAID 1 is where you mirror a volume across two drives for redundancy.  And of course spanning gives you size.  So a RAID 10 is both.  It is essentially two drives are spanned in RAID 0, and then duplicated to another two drives to give you RAID 1 for each drive, resulting in RAID 10, you know, RAID one zero, sort of both.



So he said:  "I built a RAID 10 recently using four identical brand new Western Digital Black Caviars.  To my surprise, the RAID went critical twice in two weeks shortly after."  And then he said:  "Interestingly, the drives didn't have a problem rebuilding."  He says, "Therein lays a clue.  Naturally," he says, "I was disappointed since the drives were new.  Since I thought I might have to fight Western Digital over RMA's" - return merchandise authorizations - "I decided to run SpinRite so I could document bad sectors or any other problems.  I started out running Level 3, but SpinRite reported that would take three days at that level.  So I ran Level 2 to cut it down to one.



"Surprisingly, no bad sectors reported.  And SMART data was good, too.  But more importantly, after running the drives through SpinRite at Level 2, no more critical RAID errors.  I think what's happening is similar," he writes, "with what SSD guys are finding.  If I have it right, a RAID Controller waits a certain amount of time for a drive to acknowledge a write is complete.  If the drive takes too long, say, due to a sector relocation, then the controller assumes the drive has failed, and takes it offline, and marks the RAID critical."



He says:  "Ostensibly, the WD Red NAS drives mitigate this in their firmware.  By running a drive through SpinRite at Level 2, any questionable sectors are exercised out.  And voila.  The RAID has not failed since."  So yet another happy and somewhat inexplicable way that SpinRite makes hard drives better.  From my standpoint, you can imagine I'm a little frustrated because drives have become so sophisticated that they have become black boxes.  The manufacturers consider the details proprietary, with good reason, I think, because they are magic in the amount of storage that you're able to get in such a small space.



But as the SpinRite developer, I would dearly love to have visibility into exactly what's going on.  I could do so much more, as I did once upon a time, back when there were no controllers in drives, when the controller was separate, and it had to be documented in order for software to talk to it.  So essentially what's happened is SpinRite has evolved into a very intolerant exerciser of the lowest common denominator.  That is, okay, if all we can rely on is this is a black box, and we're going to give it data and get it back, we're going to be really, really, really picky about that.



And so of course SpinRite does put the drive into a bunch of special maintenance modes to turn off retries, to shut down error correction, and to force the drive to operate without a lot of the bells and whistles, then makes it do that and helps to show the drive it has problems that it was rather kind of preferring to ignore.  Once you run it through that process, then turn all that stuff back on again.  You're back to a drive with all the bells and whistles, but also with some of the fluff and debris and dust bunnies brushed off to the side.  And it works.



LEO:  All right, Steve.  Let's talk malware.



STEVE:  Okay.  So, yeah.  Kevin's article in the Daily Beast started with a whole bunch of sort of background stuff that we pretty much know.  So I didn't want to drag everyone through it because I wanted to focus on the second portion, where he really sort of pulls this down to Earth and shares some details.  So, for example, he writes - oh, and I should say the title is "How the U.S. Hobbled Its Hacking Case Against Russia and Enabled Truthers."  And so he writes, jumping down to the middle:



"The department released" - and he's talking about the Department of Homeland Security and the FBI, this was their joint creation - "released 876 Internet IP addresses it says are linked to Grizzly Steppe" - which we talked about last week - "hacking, and urged network administrators everywhere to add the list" - okay, 876 IP addresses - "to their networking monitoring.  Lists of IP addresses," Kevin writes, "used by hackers can be useful 'indicators of compromise' in network security.  Admins can check the list against access logs, or program an intrusion detection system to sound the alarm when it sees traffic from a suspect address.  But that assumes that the list is good, carefully culled, and surrounded with enough context that administrators know what to do when they get a hit."  Meaning not just here's a list of 876 bad IP addresses.



Kevin writes:  "The DHS list is none of these things, as Lee, founder of the cybersecurity firm Dragos, discovered when he ran the list against a stored cache of known clean traffic his company keeps around for testing.  The results stunned him.  He said:  'We had thousands of hits.  We had an extraordinary high amount of false positives on this dataset.  Six of them were Yahoo email servers.'  It turns out that some, perhaps most, of the watch-listed addresses have a decidedly weak connection to the Kremlin, if any at all."  Kevin writes:  "In addition to the Yahoo servers, about 44% of the addresses are exit nodes in the Tor anonymity network."



Now, I have seen a different number quoted elsewhere.  I saw a number of 15%.  But still it's ridiculous to think that Tor exit nodes mean anything.  I mean, we know what they are.  They're general-purpose servers where the traffic is finally decrypted after its last hop through the Tor network and emerges onto the public Internet.  So, yeah, bad guys use it, but so do all kinds of good people who just would like to have the privilege of anonymity on the Internet.  So what I got a kick out of was that, thanks to Kevin's work, we did get a bit more information about what triggered that Vermont electric utility concern that we discussed last week.



He writes:  "The consequences of the over-inclusive list became apparent last week, when a Vermont utility company, Burlington Electric Department, followed DHS's advice and added the addresses to its network monitoring setup," as DHS said to.  "It got an alert within a day.  The utility called the feds.  The Washington Post soon broke the distressing news that 'Russian hackers penetrated the U.S. electricity grid through a utility in Vermont."  Well, of course we debunked that already last week because it was clearly not the case.



And then Kevin says:  "The story was wrong.  Not only was the laptop in question isolated from the utility's control systems, the IP address that triggered the alert wasn't dangerous at all.  It was one of the Yahoo servers" - the Yahoo email servers - "on the DHS list, and the alert had been generated by a Burlington Electric employee checking his email.  The Post article was later corrected, but not before Vermont Senator Patrick Leahy issued a statement condemning the putative Russian attack."  Oh, good lord.



So anyway, the good news is there's some meat here.  Kevin says:  "But to analysts in the computer security industry, the hackers are old, familiar adversaries."  I skipped a big bunch of stuff that we sort of already understand, where they were just talking about how there was way more smoke than substance in this.  But so he says these hackers are old, familiar adversaries that they, meaning the security industry, have been watching under a microscope for the better part of a decade.



"The first group, called Fancy Bear or APT28, has been active since at least mid-2007.  The group typically begins its attacks with targeted spearphishing emails" - oh, my god, Leo, I forgot to mention.  On Sunday I heard Reince Priebus.  And I didn't verify it, but he did say on national television that John Podesta's email account's password was "password."



LEO:  Yeah, I don't think that's true.



STEVE:  Okay.



LEO:  I think that's more fake news.



STEVE:  Okay.  Good.  Because...



LEO:  I'd like to see the verifying document on that.



STEVE:  Yeah.  So we know what spearphishing is.  "Then the group installs backdoors controlled through a cloud of command-and-control servers deployed around the world.  Its targets have included NATO, several U.S. defense contractors, the German Parliament and, after Russia's doping scandal began, the World Anti-Doping Agency.  One of the command-and-control servers used in the DNC hack was reportedly also used in the Bundestag intrusion."



LEO:  Bundestag.



STEVE:  Thank you.



LEO:  That's the German Parliament.



STEVE:  "The other group, commonly called the Dukes or APT29, was first spotted operating in Chechnya in 2008.  Stealthier and more cautious than Fancy Bear, the Dukes have nonetheless been detected infiltrating the White House, the State Department, and the Joint Chiefs of Staff.  Known for innovation - one attack campaign used Twitter as a command-and-control channel - they have their own fleet of customizable malware, including a program called Seaduke [S-E-A-D-U-K-E] that they only bring out for the really important targets, and which was found again on the DNC's network.



"Security companies," Kevin writes, "can tell you much more about these groups, their code, their infrastructures, and their methods.  F-Secure has an excellent 34-page write-up of the Dukes, and FireEye has a deep dive into Fancy Bear, among many other reports from different companies on the 'Net.  From analysis of the dozens of malware packages used exclusively by these hackers, researchers can tell you that they're usually compiled on machines with the language set to Russian.  Both groups operate during working hours in Russia and take Russian holidays off."  And these are of course attributes that we've talked about in the past as being signals that are used to give some sense for what's actually going on.



"Their targets are radically different from those of for-profit criminal hackers in Eastern Europe or anywhere else - no banks, no retailers with credit card numbers to steal, always governments, companies, journalists, NGOs, and other targets that the Russian government would be interested in.  In other words, these hackers don't operate like 14 year olds" - or the 400-pound person on the bed.  "They sometimes use off-the-shelf hacking tools, but more often they deploy industrial-scale malware no teenagers have access to.  They hit targets of interest to spies, not kids.  And virtually all the public analysis of these two groups concluded - well before it became a political issue with the DNC hack - that they are likely controlled by the Russian government.



"The evidence, then, that Russia interfered with the election is already solid" - well, okay, to whatever degree that it mattered.  Well, of course DNC is part of the election, but okay - "and is supported by years of work by the security industry."  Lee again, from Dragos, notes:  "If you've been following along, all the evidence that matters is already public.  This is one case out of hundreds that they've investigated involving the same hackers.  It's all very, very consistent.  It all makes sense.  It's all very, very solid," he says.  "It's just that the government is now confusing everyone."



And so in my notes I wrote, you know, how are we to understand what happened here?  And all I can think is perhaps it's a mixture of Internet-illiterate politicians, coupled with the demands of bureaucratic management.  It must be that we have, although this would shake anyone's confidence, probably deep within the NSA, skilled professional hackers who know all of this, who are no more happy than the security industry and informed Internet-savvy people are with the nature of this disclosure, which has just mucked everything up.  And this must have been, I don't know, like calling most companies and speaking to technical support.  We know you're not talking to an expert when you talk to the frontline tech support person.  You wind up talking to someone who thinks IP is something that happens when you wake up in the middle of the night, rather than an Internet Protocol address. 



LEO:  That's a good line.  I like it.  I'm going to steal it.



STEVE:  That's original.  So anyway, so that's the story with the report.  Now, part of that was this, oh, we found some malware.  Okay, well, get a load of this.  First of all, it's publicly available.  The version that they reported is known as "PAS."  And the DHS talked about v3.1.0, which is relatively old.  There is 3.1.7 is freely available, and they're already at a v4.  And it's sort of generic PHP-based malware.



So, okay.  So let's step back for a second.  We've talked around a lot about PHP and other server-side technologies.  Essentially there's, just to give you a little bit of background, there are sort of three ways to skin the cat of enhancing a web page.  Without any of that, you end up with a Tim Berners-Lee, click a link, there's a document.  And you scroll through it, and it has other hypertext links, which you click, and then it takes you to other linked documents.  That was the original web.



Of course then we said okay, we've got to make it do more.  So there's the server-side approach, which has two different flavors.  There's native code, which is my style.  For example, ShieldsUP!, GRC's ecommerce system, the Perfect Passwords page, the SSL Fingerprints, the Cookie Forensics, the DNS Spoofability, all of those things, those services that I've created over the years, are enhancements to the web server.  So they're extensions to the web server.



In fact, many people have seen ne.dll in the URL.  That's my own DLL.  NE stands for Net Engine.  And so that's that thing.  It's my own extension to Microsoft's IIS server, which has been growing.  And the idea is that it can intercept things coming in.  So, for example, it sees passwords.htm.  And on the way to the server it converts the URL into an ne.dll something in order to access that DLL service on the backend, which then uses a very high-quality pseudorandom number generator, and then presents a static page - and this is the key - a static page customized for that particular instance.  So the code running on the backend on the server is involved in dynamically creating the page.  It's not just dumping an HTML file from the server's drive out onto the user.   So it's involved in doing it on the fly.



Then the second approach is to move the dynamism to the client side.  And that's, of course, what scripting, client-side scripting gives us.  And of course I've done that where necessary.  The Off The Grid tool that generated Latin squares locally, on the user's browser, and the Password Haystacks is a perfect example where, as you're typing in, the haystack calculator is showing the alphabet size and the length of time that password, current to that keystroke, would be updated.  So that's JavaScript running in the browser, which is the way you create the greatest level of interactivity.  I could have done that, but I would have had to have a roundtrip to GRC and back and be like constantly updating the whole page, which just isn't the way to do it. 



The third approach is sort of a compromise.  And it's an interesting solution.  It's code on the server side which is interpreted sort of on the way to the user.  So there's a static page that the server delivers to this middle layer, which it could be Perl or PHP, Ruby, Java, Python, or even JavaScript on the server side.  And so the idea is that something like PHP, it scans the page, looking for something that it has to do.  There will be a specific escape character sequence that is the key that says, "The following is PHP code."  So when the PHP interpreter scanning the page sees that, it reads that as code and then executes the code and typically replaces that code with the result of the code execution.  So, for example, it might make a sequel database backend query, or a bunch of them, or do any amount of work.



And so the idea is, as the page is leaving the website, this scripting language, essentially, an interpreted scripting language on the server takes a static page and executes the code in the page.  And then what the user gets is the result of that.  There's no more PHP anymore on the page.  It's been replaced with whatever that PHP code resolved to.  So those are sort of the - those are the different ways you bring pages alive today.



So the server-side scripting is a very powerful architecture, and it's clever.  I mean, I've had to write native code, well, because I want to, in my case in assembly language.  Apache modules are another example of native server-side extensions.  And in fact those interpreters are implemented as server-side modules of the various servers, if you want to include them in your server.  So it's a powerful architecture and, again, a nice compromise because it allows you to have a nice purpose-built language made for, in the case for example of PHP, made for expressing web page content, rather than a more general purpose language that would provide less help to the author.  But it comes with a great responsibility because now the web page content is being scanned by this extension to the web server for things to do, which it then does on the fly.



So this is exactly the same as with all the problems we've talked about with SQL databases over the years, where, for example, web pages would, well, intended to embed commands to retrieve and display data from backend SQL databases.  But if attackers were ever able to get their own submitted SQL commands to be displayed and interpreted by the web server, commands like "drop table" can inflict significant damage.  So you see what's happening here.  The idea is that the way the scripting interpretation works is that the page that's being served contains invocations of the interpreter which evoke the interpreter to do something.



But the danger is, the responsibility is that it's just web content.  So sites that aren't really careful about how they display the web content, if a site blindly takes a blob of text, for example, a forum submission, and displays it to the user in front of them, and that display hasn't been filtered for the escape characters to invoke a PHP interpreter, then the PHP interpreter will be invoked and run the code that the bad guy submitted to the forum posting.  So while it's powerful, as we always see, with that great power comes great responsibility.



And so this is what's tricky about PHP.  Now, it's also why it's an attack vector, because if any mistakes are made somewhere, then that provides an opportunity for a bad guy to run their code.  And PHP is a powerful language.  There is a working implementation of SQRL in PHP, the server-side SQRL.  So you could do a lot of things with it.



Okay, so specifically digging into this PAS 3.1.0, having already said there's not that much special about it.  What it is, is a PHP function which contains an encrypted blob of text which is decrypted when the attacker supplies the decryption password.  After the PHP blob is uploaded to a web server, the attacker accesses the file somehow - and we'll get to that in a second - through a web browser, provides the password, which is then stored as a cookie so that it no longer needs to be reentered because PHP is able to look at the site's own cookies as they come and go, and so that allows the user to put the password in only once and then use this freely as essentially a function-enhanced web page.  So the blob is decrypted with a password and then executed on the web server.  That is, remember, it's executed by the PHP extension, which has a lot of power on the server.



So in the case of this PAS 3.1.0, it is what's known as a "web shell," which is a multipurpose, well-known type of toolkit that is often found in forensic examinations.  It contains in this case a file browser and explorer, allowing a remote attacker to just bring up the contents of the drives that the system has access to and browse around.  It's got a file search function, a database client to download the contents of the site's database, network tools including a port scanner and the ability to bind a service to a port.  So essentially it's able to create a server on the fly.  Hooking a service, it will then accept incoming traffic, assuming that it's able to get to the server.



A tool to brute-force attack passwords on FTP and POP3 email, a command line client to run arbitrary operating system commands, I mean, you could do anything you want with this thing.  And a utility to view the server's configuration info in order to glean more information about the site and the way it's configured.  And it is freely available on the Internet, with an optional bitcoin support donation.  The site that offers it has a form you fill out.  You put in the password you would like the blob encrypted with, and it then says, okay, here's your download link.  You download that, and it is preencrypted for you.  You arrange somehow to get a site to display it.  And when it prompts you for the password, you enter that, and you then have all of those tools at your fingertips.



So the remaining question is how does such malware infect a PHP site, like WordPress, which of course is PHP-driven.  So that's unclear.  Nothing in the document that the government provided, provided that information.  And, I would argue, maybe they know.  Maybe what we're looking at is the result of massive redaction so that, like, all of the good stuff was taken out.  Because there was, as we know, a private meeting that both President Obama and President-elect Trump had with the security agencies, which was not made public, and maybe more, like more compelling information was available.



But the point that Kevin was making was that we already know all of this.  That is, this was just not an attack by these people only on the DNC.  It has been an ongoing campaign for a long time with known groups where there is a strong reason to believe that they are based in Russia.  We have to draw the connection to who supports them and gives them their marching orders.  But all of those things, all of the facts line up.



So how does it get in?  The guys who have looked at this a lot say that, well, WordPress website owners might have other malware installed on their workstations, and that malware attempts to install PAS, P-A-S, this malware, on their WordPress websites.  Maybe a related cross-site request forgery, we've talked about those before, CSRF vulnerability is used to install the malware.  Maybe unwitting users are voluntarily installing this on their own websites after downloading it from a malicious website, thinking it's safe.  That is, like, oh, look, it's password-protected.  I can do whatever I want to with it.  It'll allow me to do things remotely on my own server.  Bad idea.  Or attackers could be compromising websites through some other means, and then using compromised credentials obtained to manually sign in and install PAS with a standard browser.  And those sign-ins could be partially or fully automated.



So anyway, the bottom line is there is nothing whatsoever particularly impressive or unique about this particular piece of malware.  Anybody who wants it could download it by the end of the podcast.  It's not clear why DHS and FBI provided it.  Maybe they felt they had to provide something.



LEO:  Well, it was in hexadecimal.  It looked really good.



STEVE:  It's gibberish.  Leo, it was encrypted.  It must be, you know, encryption, all that encrypted stuff.



LEO:  Yeah.



STEVE:  Yeah.  So that's what we have.  I think we have bureaucracy.  Maybe the intelligence community is protecting its sources and methods, and we want them to do that.  The presumption is, from what we learned from Snowden, there's a lot of stuff with crazy code names that is like, you know, freaky technology that actually exists.  We saw pictures of it, and we saw slides.  So none of that is represented by this ridiculous report portion that was made public.  And it was unimpressive.  But what I think this really says is that, yeah, there is ongoing cyber, I don't want to say warfare, but cyber intrusion.  We are probably - we the U.S. are probably doing it every bit as much and as successfully as teams in Russia and teams in China and teams in the U.K. and wherever else.  I mean, that just seems to be something going on.



And, you know, in the press, I've been thinking about this, in the press there's been a lot of, oh, what does this mean?  Is this going to be an escalation?  Is this the new arms race?  And I was thinking, well, you know, the nuclear powers all have nuclear missiles aimed at each other.  And no one has fired them because there just sort of seems to be this mutual deterrent effect.  And meanwhile, we've become globally, thanks to the Internet, really closely knit together.  I mean, there is strong financial ties now.  And even though Russia may not need us, well, China buys a lot of Russian energy products, and we transact a lot with China.  So even if it's one step removed, we're connected.  Nobody wants the economic side to come tumbling down.



So I think countries are establishing this technology.  They probably, you know, we call them "implants."  We probably have implants throughout the critical infrastructures of these other countries.  We're doing our best to thwart theirs, but they probably have them, too.  And this is just sort of the way it's going to be.  And meanwhile the script kiddies do their script kiddie stuff on an entirely different level than what the state actors are doing.  And we saw sort of a snapshot of it, but it certainly wasn't very impressive.



LEO:  Well, there you have it.  Thank you, Steve Gibson.  GRC.com.  Everybody should go there and get the latest copy of SpinRite, the world's finest hard drive maintenance and recovery utility.  That's Steve's bread and butter.



STEVE:  A couple people did while I was talking.



LEO:  I heard the yabba dabba doos.



STEVE:  So thank you, whoever you were.



LEO:  Keep those yabba dabbas coming.  You can also get the podcast there.  He has audio and transcripts at GRC.com, and lots of free stuff.  I mean, it's a great site for, I mean, it's just a treasure trove.  You can just browse your little heart out there:  GRC.com.



We have audio and video at our site, too, of course, TWiT.tv/sn.  And we put it on YouTube.  By the way, we're on YouTube Live now.



STEVE:  Nice.



LEO:  If you go to YouTube.com/twit, you can watch the live stream there as well as on our website, as well as using those apps.  There are so many apps.  There's like five or six apps on Apple TV, but all of them have live streaming.  Or get the show after the fact, watch on YouTube or download.  I think downloading is the best thing to do.  Whatever.  You know, we don't care how you get it, just get it.  You don't want to miss a single episode of this show.



We record on the air, so you can watch it if you want live, every Tuesday, Wednesday, Tuesday at 1:30 - I have to think, what day is this?  Tuesday at 1:30 Pacific, 4:30 p.m. Eastern time, 21:30 UTC.  You just go watch the live stream, and you'll see a little bit different version of the show than the one we put out, but it's pretty much the same thing.  You can also, let's see, what else?  Get the app and subscribe.  You'll have a collection.  There's lots of ways to play.  Steve, back next week.  And we're going to do questions; right?



STEVE:  I'm not sure.  I've got a bunch of topics to talk about.  I'm sort of - a couple of people have suggested I sort of fold some in, as I did this time.



LEO:  Oh, we could do that.



STEVE:  Some feedback from our listeners.  So when it's topical and it makes sense.  But maybe we'll do a Q&A.  I'll check the  mailbag.  And if it's compelling, that'll sell me.



LEO:  Okay.  The mailbag really isn't a mail or a bag.  It's you go to GRC.com/feedback, and you can leave a comment there.  Or really I think most people now use Twitter.  Steve's Twitter handle:  @SGgrc.  @SGgrc.  Thanks, Steve.  We'll see you next time on Security Now!.



STEVE:  Thanks, Leo.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#595

DATE:		January 17, 2017

TITLE:		What's Up With WhatsApp?

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-595.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  This week, Leo and I discuss a classic bug at GoDaddy which bypassed domain validation for 8,850 issued certificates.  Could flashing a peace sign compromise your biometric data?  It's not only new IoT devices that may tattle - many autos have been able to for the past 15 years.  McDonald's gets caught in a web security bypass; more famous hackers have been hacked; Google uses AI to increase image resolution; more on the value or danger of password tricks; and, finally, does the WhatsApp messenger incorporate a deliberate crypto backdoor?



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We're going to talk about WhatsApp.  The Guardian says it's insecure.  Moxie Marlinspike says no, no, it's secure, it's an implementation issue.  Anyway, you know Steve will get to the bottom of that, plus all the security news.  It's all next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 595, recorded Tuesday, January 17th, 2017:  What's Up With WhatsApp?



It's time for Security Now!, the show where we cover your security, your privacy, your safety online and in the world at large with Mr. Spock himself, James Tiberius Gibson.  He's got the thumb, he's got the "Y" thing.  I just saw - what did I see? I just saw a video where a guy did that.  I think it was a wedding video.



STEVE GIBSON:  I did, too.  And I was thinking, okay, thumbs correct.



LEO:  We are now to the point where we are passing judgment on people's Vulcan hand gestures.



STEVE:  That's right.  It only took us 12 years to get to this point, but that's [audio anomaly] something.



LEO:  This is where nerds gather.  I just want to, before you get going, it just broke, the story broke three minutes ago.  President Obama has commuted Chelsea Manning's sentence - not a pardon, but a commutation.  Chelsea Manning, of course, was she in the army?  I think she was.  I actually have a link that I can...



STEVE:  Didn't she have some intelligence role?



LEO:  She was an intelligence agent and leaked to WikiLeaks a considerable amount of material, including, and I think the most important one, and the one that she really provided a public service, the video of a drone strike that showed really kind of an irresponsible use of a drone strike.



STEVE:  Wasn't that also - I have a hard time saying "she," but I guess we're supposed to.



LEO:  She's a she.  He was a he when that happened, and he's transgender now.



STEVE:  But wasn't that also the Abu Ghraib revelation?



LEO:  It was, yeah, I believe so, yeah.  So certainly - but, now, what's interesting is, of course, now you're going to wonder, Obama's got three more days in office, if he will also pardon - now, he would have to pardon Snowden.  Snowden has never been on trial or sentenced.  And I think that might be one of the big differences.  Manning has served time on this.



STEVE:  So commutation means time served?  It means you're now free?



LEO:  Almost.  She won't be free till July.  So effectively she's shortened - her sentence has been shortened.  Remember, she's committed or attempted suicide - she never committed it.  But she attempted suicide several times in custody.  And so I think some of this is humanitarian as much as anything else.



STEVE:  Boy, we really are in an interesting time of moral dilemmas, where, I mean, you know, like the question of what's right.



LEO:  She was an army intelligence analyst, leaked classified material to WikiLeaks and was sentenced to 35 years.  She would have been released in 2045.  She attempted suicide twice and went on a hunger strike because she wanted gender reassignment surgery, which was granted to her.  She was Bradley Manning, and she's now Chelsea Manning.  She was imprisoned in 2010.  So she's served six years, more than six years.  700,000 military files, diplomatic cables.  You know, the same rhetoric was used about her as Snowden - traitor, endangering U.S. assets abroad and so forth.  She pleaded guilty and apologized.  And I think that may also have something to do with the clemency.  In any event, that's breaking news, just happened.  Actually, I'm sorry, it's not July.  BBC says May 17th.  So what is that, four more months of her term, which is considerably less than her overall sentence.  She was sentenced to 35 years in 2013.



STEVE:  Yeah.



LEO:  Edward Snowden himself had tweeted that he asked, he said, "Mr. President, if you grant only one act of clemency as you exit the White House, please free Chelsea Manning.  You alone can save her life."  So there's the news, just came out moments ago.  And now, on with the show.



STEVE:  So our title today is "What's Up With WhatsApp?"  The Guardian broke the news late last week, I think it was Friday.  And it just caused a huge storm of concern because WhatsApp, due to its connection to Facebook, is the most heavily used, supposedly private and secure messaging application on the planet; Facebook says 15 trillion messages handled per year.  So it's a busy application.  The problem is that there has been the allegation made that it contains a backdoor.  So our main topic for the week will be taking a look at that.  What does that mean?  What's actually happening?  And I know there's a great interest among our listeners because I got a flood of, oh, you know, what's this, is this right, incoming.  So that's our title for this week.  Otherwise we would have done maybe a Q&A.  But I think this is an important thing for us to discuss.



So we've got a classic bug.  Oh, my god, this one is just one for the history books, a classic bug at GoDaddy that bypassed domain validation for 8,550 certificates which were issued.  Then the question of could flashing a peace sign compromise your biometric data.  It's not only new IoT devices that may tattle on us.  It turns out that many cars have been able to do that for the past 15 years, but it really wasn't ever on anyone's radar.  McDonald's gets caught in a web security bypass the likes of which we have danced around and discussed even recently.  And I heard you mention on MacBreak Weekly that the hackers have been hacked, namely Cellebrite.  We'll talk about that.  And one that I thought would really catch your interest, Leo, is that Google has come up with a technology to use AI to increase image resolution.  That is without losing sharpness, actually putting information in that didn't exist in the original image, which you can't do.  And a little bit more discussion, because I'm seeing a lot of questions about this still, on various spins of like, Diceware and using multiple English words and things.



So I thought, okay, we're to the point now where we've established enough foundation to lay down a formal definition of, I mean, sort of with almost mathematical formality, a formal definition of what it means to have a maximally secure password, and the test you can apply.  And then we have a little bit of miscellany, not much; and then we're going to talk about the details of a design decision that WhatsApp made that has become controversial.  So I think a lot of great things to talk about.  And, oh, the picture this week, this was - someone named Steven Minnick tweeted this to me.  This is a picture he took of the center console - you recognized it instantly.



LEO:  Oh, yeah.  Oh, yeah.



STEVE:  You knew what it was, because you have one, of this Tesla.  I assume it was his because it's from the driver's angle.  The tweet says:  "@SGgrc Steve, you look fantastic."



LEO:  He's listening to Security Now!, "A Look Into PHP Malware."  But he's got an album...



STEVE:  Last week's episode.



LEO:  ...from somebody called Mashonda, who is wearing quite a low-cut negligee.



STEVE:  Ooh, goodness.



LEO:  But I have to tell you, this has been going on for ages.  Tesla does a terrible job of figuring out album art for podcasts.  In fact, I have a picture I took just the other day.  I'm listening to "The People's History of the United States" by Howard Zinn.  And for some reason the album art it pulls up is Hamster Theatre with a guy with a Pinocchio-length nose.  



STEVE:  I thought the podcast contained the album art.



LEO:  It does.  But the Tesla is not receiving it, I gather, from - remember, it's streaming it.  It's not storing it.  So it's streaming it from TuneIn.  And so TuneIn doesn't send the album art, apparently.



STEVE:  Yeah.  Anyway, if I looked like that, I would have a different relationship with myself than I do.



LEO:  You wish you looked that good.  You wish you looked that good, yeah.  That's funny.



STEVE:  So, okay.  As I said here in the show notes, the righteous bug of the New Year bites GoDaddy and allows the issuance of 8,850 certificates.  Actually it turns out it ended up being a little more because of some interesting caching issues that they later found, without proper domain validation.  Now, the first note here is that GoDaddy handled the entire mess perfectly and responsibly.  Which doesn't make the bug any less wonderful.  But unlike Woebegone, or WoSign that we talked about, that just completely tripped over themselves, this is also a textbook case of a responsible certificate authority discovering a problem, immediately responding to it, and then coming completely clean and doing all the remediation that you would want.



So Wayne Thayer is the senior Internet product and technology leader at GoDaddy.  He posted in a Google Group, the mozilla.dev.security policy group, where this sort of stuff goes, on the 10th.  So exactly one week ago today, during last Tuesday's podcast, this was happening, an incident report titled "Certificates issued without proper domain validation."  So, first of all, that's like the worst thing a certificate authority ever wants to have to post somewhere.  It's not something strange crawled out of the wall, it's "We issued certificates that didn't get validated."  And their whole job, I mean, that's the whole point of a certificate authority.  So again, props.



So he writes:  "On Friday, January 6th" - okay, so that was the previous Friday - "GoDaddy became aware of a bug affecting our domain validation processing system.  The bug that caused the issue was fixed late Friday, the same day."  So instant response in terms of, like, knowing about it and immediately fixing it.



"At 10:00 p.m. PST on Monday," he writes, so three days later, January 9th, "we completed our review" - which they were probably doing all weekend - "to determine the scope of the problem and identified 8,850 certificates that were issued without proper domain validation as a result of the bug.  The impacted certificates will be revoked by 10:00 p.m. PST on Tuesday" - that is, 24 hours later - "Tuesday, January 10th, and will also be logged to the Google Pilot CT log."



That's Google's certificate transparency system, which is on my list of things to get to for the podcast because it's a terrific idea.  Basically it's a publicly postable and viewable log that the concept would be that all certificate authorities would publish the certificates they issue as they do, and other transactions about it, the idea being that then there is a - it's called "certificate transparency" - a single place where everything can be found to both create visibility and accountability and allow much better, you know, basically to sort of close an aspect of this open and supposedly public process, but which still isn't getting enough light shined in it.  So that'll be a topic that we'll be getting to.



So under "detailed description," Wayne continues.  He says the week before that, so on Tuesday, January 3rd, "one of our resellers, Microsoft, sent an email to a GoDaddy notification account" - which I guess from the context was the wrong place to send it - "and two GoDaddy employees.  Due to holiday vacations and the fact that the issue was not properly reported per our guidelines, we did not become aware of the issue until one of the employees opened the email on Friday, January 6th, and promptly alerted management.  The issue was originally reported to Microsoft by one of their own customers and was described as only affecting certificate requests" - get this, this is strange - "when the DNS A record of the domain was set to 127.0.0.1."



Which is, okay, now, I guess you could do that by mistake.  So the reason you would do that is if you wanted to get a certificate for some local internal purpose.  But that's not being issued anymore.  That is, one of the changes that we've seen in 2017, and we've talked about this before, is that certificate authorities are going to stop issuing certificates for private domains, only doing it for public domains.  So that's clearly not what GoDaddy intends.  Because I should just  mention that 127.0.0.1 is of course the localhost IP, meaning it's an IP that always resolves to the system that's asking the question.  So it would be, well, for example, I have one that I issued myself for my own purposes.  It's www.steve.  And that refers to...



LEO:  That's a self-signed certificate; right?



STEVE:  Precisely.



LEO:  So anybody could do that.  There's no...



STEVE:  Yes, yes.  And so what's different here is that they're getting one - and so, for example, I've arranged for my browsers and other clients to trust that certificate so that I'm able to do things locally to test the security that I then deploy  publicly at GRC.  That allows me to operate, to have a staging area.  Which is probably what this person was wanting.



LEO:  It's a fairly routine thing.



STEVE:  Right, right.



LEO:  So you wouldn't normally go to GoDaddy to do it.  That's what's weird.



STEVE:  Correct.



LEO:  You just do it yourself.



STEVE:  Correct.  And now the CAB guidelines say no more issuance of certificates for nonpublic IPs.  So they continue:  "An investigation was initiated immediately, and within a few hours we determined that the problem was broader in scope."  Oh, boy.  "The root cause of the problem was fixed via a code change at approximately 10:00 p.m. on Friday, January 6th."  So same day they found out about it, they plowed into it, found the problem, and immediately fixed it.



On the next day, on Saturday - so yes indeed, they were working the weekend - January 7th, "we determined that the bug was first introduced on July 29th, 2016, as part of a routine code change intended to improve our certificate issuance process.  The bug is related to our use of practical demonstration of control to validate authority to receive a certificate for a given fully-qualified domain name.  In the problematic case, we provide a" - and so here's part of the problem.  "We provide a random code to a customer and ask them to place it in a specific location on their website.  Our system automatically checks for the presence of that code via an HTTP and/or HTTPS request to the website."



And at first I was thinking, wow, how could HTTP be secure?  But on the other hand, I'm sure they're taking that into account.  "If the code" - that is, this random nonce, essentially - "is found, the domain control check is successfully completed.  Prior to the introduction of the bug, the library used to query the website and check for the code was configured to return a failure if the HTTP status code was not 200."  We've talked about this before.  When the browser makes a query to a remote server and says blahblahblah.html, in the response headers there is an HTTP "200 OK" is the common phrase.  And there are different things that can be returned.  For example, it might say "301 redirect," in which case another header will tell the browser that this asset has moved to a different URL, and the browser should then ask again at this new location.



So those little return codes are very handy.  And of course the famous one is the 404, which people typically encounter.  We used to more in the past.  Now, of course, when a domain is abandoned, you generally get a squatter who sticks some junk there, just so you get something.  But if you just put, like, if you put in a completely erroneous long URL yourself, then you can get a 404 because the page you've asked for is not available.  So that 404 code comes back, that HTTP 404, and then it typically says "not found" afterwards, meaning we looked for that page, but couldn't find it.



"A configuration change to the library caused it to return results even when the HTTP status code was not 200."  Now, here it is.  "Since many web servers are configured to include the URL of the request in the body of a 404 'not found' response, and the URL in question also contained the random code, any web server configured this way caused GoDaddy's domain control verification to complete successfully."  This is just such a sublime bug.



So what that meant was that, if the domain being checked wasn't owned by the person who asked for a certificate - excuse me, I've got a little scratch in my throat.  If the domain for which a certificate was being requested was not owned by the requestor, GoDaddy would send back to the requestor saying, "Here is a special code.  Put this on the following page on your site and let us know when you have."  So whether the person did or not, they would say okay.  And then GoDaddy would ask for that page containing that code, but the request also contained the code.  So when the site that wasn't owned by the user said, huh, I don't have any page like that, if in trying to be helpful that web server's 404 "not found" said, oh, you know, sorry, we could not find this URL you just asked for, and displayed that in the error page, the modified library that GoDaddy was using since July would say, oh, and find the code in the error page rather than in a legitimate page, and pass the domain validation test.  Whoops.  Wonderful bug.



So Wayne continues:  "We are currently unaware of any malicious exploitation of this bug to procure a certificate for a domain that was not authorized."  So in other words, they don't know that it was abused.  So it wasn't found...



LEO:  Probably wasn't.  I mean - right?



STEVE:  Right.



LEO:  Because nobody knew it was there.



STEVE:  Exactly.  Exactly.  "The customer who discovered the bug revoked the certificate they obtained" - oh, so maybe there was one test.  And, he said, "and subsequent certificates issued as the result of requests used for testing by Microsoft and GoDaddy have been revoked.  Further, any certificate requests made for domains we flag as high risk" - for example, Google.com - "were also subjected to manual review."  So if during that time any high-value, high-risk domains happened to be reissued, those they immediately, you know, they read through the whole list and immediately were responsible.



"We have re-verified domain control on every certificate issued using this method of validation in the period from when the bug was introduced until it was fixed.  A list of 8,850 potentially unverified certificates" - it says "potentially unverified" - "representing less than 2% of the total issued during the period" - because I should mention that most certificate authorities, and we've discussed the various means of validation in the past, they'll give you multiple ways of doing it.  It's like take a file, stick it on the root, do this, do that.  Make this appear in a text record in your DNS records.  So they typically give you a range of things to do.



And you'll remember that, back when I was playing with trying to get a third party to scan GRC.com when I had received a report of there being some sort of a cross-site scripting vulnerability, I had trouble doing that because - I don't remember now why.  Oh, it's because I don't use a default home page.  I always redirect anyone coming in to just the root, you know, backslash, I always redirect them - or rather, forward slash.  I always redirect them to intro.htm, which I've long used.  And their systems wouldn't follow a redirect.  Which maybe was good for security, but it wasn't something I could easily fix.  So anyway, so as a consequence, there were other ways I was able to do it.



So then they conclude, saying:  "Additional code changes were deployed on Monday and Tuesday to prevent re-issuance of certificates using cached and potentially unverified domain validation information.  However, prior to identifying and shutting down this path, an additional 101 certificates were reissued using such cached and potentially unverified domain validation information, resulting in an overall total of 8,951 certificates that were issued without proper domain validation as a result of this bug."



So as we know, anyone can make a mistake.  GoDaddy's response is textbook perfect.  And finally, under "Next Steps," he concludes, saying:  "While we are confident that we have completely resolved the problem, we are watching our system closely to ensure that no more certificates are issued without proper domain validation, and we will take immediate action and report any further issues if found.  A full post-mortem review of this incident will occur, and steps will be taken to prevent a recurrence, including the addition of automated tests designed to detect this type of scenario in the future.  If more information about the cause or impact of this incident becomes available, we will publish updates to this report.  Wayne Thayer, GoDaddy."



And again, it doesn't get any better than that.  I mean, you know, talk about a different response from WoSign, who said, "Oh, okay, yeah, we revoked that one."  And it's like, "Okay, but this was a big problem you had.  What else was affected?"  "Oh, well, I guess we could look."  It's like, no.  Don't use them.  And for what it's worth, I still love Hover.  They are my goto certificate authority.  Or, I mean, not certificate authority, domain host.  And of course DigiCert is my certificate authority of choice.  Was I saying CA?  I was saying CA this whole time, wasn't. 



LEO:  I hope so.  Oh, instead of domain registrar.



STEVE:  Yeah.



LEO:  Domain registrar.  Please s/ca/domain registrar/g.  All fixed.  Oh, wait a minute, this isn't text.  Just a little grep.  Yes.



STEVE:  So anyway, just beautiful, beautiful work for handling domain registration.  And again, I just want to - I do want to give Hover props.  In fact, I just allowed three domains to expire on GoDaddy.  I was trying them for a while, and they're just not my cup of tea.  Also they're sort of hypercommercial, and I really just want somebody that does, you know, that's got good coverage, good policies, and a long track record.  And of course Hover is based on Tucows.  It's a service of Tucows.  And so I'm happy there.  But again, couldn't ask for better performance from those guys.



I don't know what she's doing with her tongue here, Leo.  It's rather...



LEO:  Okay.  That's a bad way to start.



STEVE:  It's rather distracting.



LEO:  She has a very hyperactive tongue.



STEVE:  Should I know her?



LEO:  Oh, yeah, it's Miley Cyrus.  Don't you know Miley Cyrus?



STEVE:  No, Leo.



LEO:  Oh, come on, Steve.



STEVE:  I don't know her or her tongue.



LEO:  She's very famous for, well, you know Billy Ray Cyrus, her dad.



STEVE:  Okay, first of all, I know the name.  I absolutely know the name.



LEO:  Yeah, she's a pop star.



STEVE:  But I couldn't pick her out of a crowd.



LEO:  Yeah.  She's also on "The Voice" right now.  She's one of the judges.  And she has really a - there must be a name, I would love to know what it is, for an agile and lengthy tongue.



STEVE:  It's freaky.



LEO:  Yeah.



STEVE:  She's rather inked up, also.



LEO:  Yeah.  Well, she's under 30.



STEVE:  Has she got, like, crib notes on her left hand?  What does that say there?



LEO:  Those probably are.  Yeah, that's probably something she wrote, and then - it's hard to distinguish those from the tats.  But the point of this photo is she's flashing the peace sign.



STEVE:  Yes.  And what I have noticed is, well, we've all noticed camera resolutions are skyrocketing.  And I am sometimes still taken aback when I zoom well into a photo and the image remains super sharp.  Because, I mean, in the old-school days you're used to zooming in, and it gets all blurry.  Except that the resolutions are so crazy now that we are picking up a lot of additional information.  So the question is raised, could flashing a peace sign - or a Vulcan hand grip.  That's now a problem.



LEO:  I can see your fingerprint right now.  Right now.



STEVE:  Could that lead to biometric theft?  And I would have to argue, with sufficient resolution, certainly if somebody had a good lens and image stabilization, and the lighting was correct, and meaning that you were able to pick up the 3D nature of the ridges, it's probably possible.  We have seen an instance where a distant photo of an old-school mechanical lock key was taken with a state-of-the-art camera.  It was modeled.  It was rotated to remove all the perspective distortion.  And then a key was cut from that, and it worked the lock.



LEO:  It's how the TSA master key was cracked.  Pictures.



STEVE:  Yup.



LEO:  Now, if somebody had your fingerprint, though, I mean, most fingerprint readers you can't just put the - I think you'd need more than just an image of the fingerprint, wouldn't you?  Maybe not.



STEVE:  Well, yeah.  And we've talked about that a little bit.  And so the hope was that, by using - technically the term is "projective capacitance."  That is, when you put your finger down, for example, on the Apple capacitive touch sensor, we heard that it wasn't reading just that surface layer, that there was some penetration.  And so it was reading the meat a little more than just the ridges that are ruffled.  And but on the other hand, we also have seen successful spoofs using gummy technology, using like conductive rubber that has ridges on it in the pattern of a fingerprint.  So I don't think we can rely enough, unfortunately, on the technology, for example, verifying there's a pulse and maybe checking your pulse ox to say, okay, yes, this person is alive, instead of just being a gummy bear that's had a fingerprint etched on top.



LEO:  Yeah, I think there's infrared.  They read the heat and, I mean, which is not to say you couldn't create a prosthetic that would work.  But Steve, this is a part of a much larger picture, which is essentially that basically your soul can be stolen now.  And if somebody has an image of you, they can make it do anything they want.  I mean, you can no longer trust photographs or video.  You can, I mean, it's...



STEVE:  Well, we saw a young Princess Leia.



LEO:  Right, right.



STEVE:  And, you know, she never said what she said.



LEO:  Right, right.  Commander, what is it, Commander Tarkin was reproduced in the newest one?  I haven't see it yet.  But I hear that's - so, yeah, exactly.  We're in that era.



STEVE:  Yeah, yeah.



LEO:  And it's people like you and me, who have thousands of hours of video widely accessible, I think, yeah, I think your fingerprints are the least of it.  You could be recreated.



STEVE:  Well, and that takes us into this next article perfectly because Forbes got a lot of attention this week from their Thomas-Fox Brewster, the guy who covers crime, privacy, and security in digital and physical forms for Forbes.  And he did some research that revealed something we may have sort of been sort of dimly aware of, but not focused on.  And that is that, while our IoT devices that we've just been talking about are all in the news, for example, we just recently were talking about the Amazon Echo and how it has been subject to a subpoena, and in fact it was that that brought this to Thomas's awareness.



He writes:  "The rapid spread of connected devices that can listen and locate has been a boon for law enforcement.  Any new technology hooked up to the web has the potential to become a surveillance device, even if its original purpose was benign, as shown in a 2016 Arkansas murder investigation where Amazon was asked to hand over audio from a suspect's Echo."  And so he broadens this to note that "such information and much more, I've learned," he writes, "has long been retrievable from cars."



Now, of course, this is also fodder for investigative movies and all kinds of things, where you have GPS in a car that is building a log of where you go.  But he did some court document research and revealed a 15-year history of what's been dubbed "cartapping," where almost real-time audio and location data can be retrieved when law enforcement order vehicle tech providers to hand it over.  And he notes that, for example:  "One of the more recent examples can be found in a 2014 warrant that allowed New York police to trace a vehicle by demanding satellite radio and telematics provider SiriusXM" - and first when I saw it I thought, wait a minute.  SiriusXM is one way.  It's downward traveling satellite.  You're not sending anything up to the Sirius satellite.



But, "The warrant, originally filed in 2014, but only recently unsealed" - and in the article he publishes it in full - "asked SiriusXM to activate and monitor as a tracking device the SiriusXM satellite radio installed on the target vehicle for a period of 10 days."  The target was a Toyota 4Runner wrapped up in an illegal gambling enterprise.  And as Thomas dug into this, he found that, well, he talked to Sirius, who told Forbes it "complied with the order and did so by switching on the stolen vehicle recovery feature of its connected vehicle services technology, which is only available in a subset of cars it supplies."  And he notes that the satellite radios alone cannot be tracked as the telematics services can.  And so on.  In fact, his article goes to great length to enumerate many of these sorts of things.



So it's just sort of, again, another heads-up that, when law enforcement discovers a means for acquiring data, we can no longer think, oh, maybe they're not going to pursue it, or they're not going to try to get it.  What we're seeing is instances - and in fact this brings us later to the WhatsApp question.  What we're seeing is that anything available, someone's gas meter, which is an IoT device, or of course the Amazon Echo, or law enforcement will examine the technology in the cars believed to be used by suspects, obtain a court order, and use whatever technology is available there in order to pull data from it.  So Leo, to your point, this is the world we are in today. 



LEO:  Yeah.



STEVE:  And I'm doing nothing very interesting in my car, so I'm not that concerned.  Of course my car has the advantage of being 15 or 16 years old, and so it's completely disconnected.  But I'm sure...



LEO:  Yeah, I'm the exact opposite, of course.



STEVE:  Correct.  Well, and because of its age and mine, I know I'm going to outlive mine.  And so at some point I will be upgrading, and I no doubt will be, you know, I mean, I see these things when I travel somewhere and rent a car, or am a passenger in somebody else's, all this stuff happening.  And so I'll have that eventually.  I guess the only takeaway would be, if somebody is really concerned about privacy, modern car technology creates a problem because, well, especially now we're going to have cars, as you have mentioned, as we talked about on this podcast and I heard you talking about elsewhere, the National Highway Traffic Safety Board will eventually be moving inter-vehicle communication from optional to mandatory.  So we're going to be driving connected computers around.  And so another aspect of privacy disappears.  And of course we've talked about license plate scanners that are increasingly present, so that this information is just - it's out there.



LEO:  I feel like we're all living in a Philip K. Dick novel at this point.  I mean, really, science fiction is coming true.



STEVE:  I know.  That's exactly right.



LEO:  Yeah.



STEVE:  Yeah, it really is.  I mean, some of the things we used to talk about on this podcast, it would have been science fiction 10 years ago, while we were doing the podcast.  Now it's like, oh, yeah.  Yeah, the guy's power meter finked on him.



LEO:  Moving fast, it's moving fast.



STEVE:  Because he apparently washed down the patio, and we know how much water he used at 2:00 a.m.



LEO:  Yeah.



STEVE:  What were you doing in the middle of the night?



LEO:  I had to fill the hot tub, man.



STEVE:  So McDonald's website was found to have a problem which they didn't acknowledge.  And after the person gave them plenty of time, I think six months or more, to respond, when he couldn't get a hold of anybody who cared, he said, okay.  And so he laid it all out.  And our listeners will quickly understand the problem.  His post was titled, the blog post, "Stealing Passwords from McDonald's Users."  And essentially he worked out all the details required to do that, where he can go to anyone's computer who has ever logged onto McDonald's and display their password in cleartext.  This is a consequence of a series of mistakes.  In order for that to be true, you have to really mess things up in terms of your web services.



But there's one place where this starts.  And the title was "Reflected XSS" - which of course is cross-site scripting - "through Angular JS" - which is a well-known JavaScript Library - "sandbox bypass causes password exposure of McDonald's users."  What he first saw was - and this is a mistake kind of related to what happened with GoDaddy, but this is similar to what caught me.  Because remember that I, when I was asking people to enter their transaction code in order to look up a previous purchase of SpinRite, if they mis-entered it, I would return an error and say, oh, sorry, we couldn't find that transaction code.  And to be helpful I was returning it filled into the form so they could look at it and check again and just fix the typo.  Well, just that, that helpfulness, created a potential vulnerability because what that allowed was a user to control what the web browser displayed.



And just last week we were talking about the dangerous power of PHP, where if an attacker could somehow arrange to cause a victim or target web server to display a page containing PHP code that the attacker provided, then in the display of the page, as it went from the web server and was intercepted by the PHP parsing module, if it encountered PHP, it would execute it.  And if it was the attacker's code, that's not good.



Well, the closely related problem is the same kind of thing at the client side, whereas if you don't have PHP, pretty much everybody is running JavaScript.  I mean, even I gave up blocking JavaScript some time ago, as we all know.  I dropped using NoScript because it was just becoming too much of a pain and switched to uBlock Origin.  So similarly, if your browser gets a web page, it scans it for any JavaScript embedded in the page's content that it should execute.  So if the attacker can arrange to put their own script tags in, then you are in trouble.



So what I had not done in my case was properly "escape," as it's called, escape the special characters, like the less-than sign and the greater-than sign, which are used to bracket the keyword "script," because, for example, if I had simply - and I do now - convert the less-than sign to an &lt; that's the HTML code for less-than, and similarly the greater-than sign to an &gt;, then it'll still display as if it were the brackets, but it doesn't parse.  They aren't actually brackets.  And so JavaScript never gets invoked.



Anyway, those things are missing from McDonald's corporation website in the search results.  When you search the McDonald's site, and whether or not it finds anything, it comes back saying "X number of search results were found for," and then, yes, it feeds back on the page what you gave it to search for.  Then it even fills it into a form field to be helpful, and then asks do you want to search, and then it gives it to you again in the news section.  So that's the beginning of a detailed dive that I won't go through in infinite detail here because we've pretty much covered the high points.



But Angular JS is supposed to be sandboxed.  And it turns out the sandboxing isn't done right, and it can be bypassed to break out of a sandbox.  And remember the extra danger of code running that appears to be from the site.  Remember there is the same-origin policy, which is strictly and rigorously enforced by our web browsers.  And that is that code from a domain, from a so-called "origin," is only able to communicate with its own origin, that is, where it came from.  That's what prevents, for example, code in ads from being able to attack the sites that serve the ads.  The code in ads that came from an ad server can only make queries back to that same ad server.  So there's a strict compartmentalization.



But in this case, where you put something into a search term, and it's displayed by the server back to you, that code came from the server.  Which means that the attacker, the site is not protected by the same-origin policy because it is from the same origin.  It came from that server via the form being responded to.  And so anybody who wants to who's clever can sit there poking at it, doing tests, and work their way through the security.  And there are many things wrong.  For example, they store - oh, I can't even believe I'm going to say this.  They store the user's password encrypted in a cookie on the user's browser.  It's like, why?  Why would you do that?



I mean, so we're looking also at an amateur who implemented the logon process and authentication process.  You would never encrypt the user's cookie, I mean, encrypt the user's password and store it in their cookie.  It's like, no.  Accept the password, hash it the same way - well, there's no hashing going on, obviously, this is so screwed up - and then check it against the previous hash from the time that the user created the account.  We all know how this is to be done properly.  None of that happens here.



So there's crypto present.  The initialization vector and the key are static across all users.  It's not per user as it absolutely should be.  I mean, it's just mistake after mistake.  I was just reading this thing and going, oh, you know, this is not something you repair.  You just flush this and get somebody who actually knows how to build a website.  So unfortunately they hired somebody who was unable to implement security in a useful fashion, and then didn't listen to a responsible individual jumping and down and waving his arms, saying, "Hey, you've got a real problem here."  And of course, if this is all that was found, we know there's, I mean, this is sort of probably the tip of the iceberg.



So you don't want a situation where somebody can drop a magic incantation into your browser and up pops the JavaScript alert box with your password in the clear, which it decrypted from your cookie, which for some reason that site stored in your browser.  It's like, no.  Where do you begin with what's wrong with this?  So anyway, again, these guys are doing it wrong.



And this you talked about a little bit on MacBreak Weekly.  Or I guess, no, I think Rene said that you were aware of it, but you hadn't talked about it.  And that is that, although I did see the amount of data, which was rather substantial, Leo, we've talked about Cellebrite before.  They're the Israeli, well-known, very popular in their circle...



LEO:  Among law enforcement.



STEVE:  Exactly, law enforcement.



LEO:  Authoritarian regimes everywhere.



STEVE:  Exactly, provider of cell phone hacking technology.  So Motherboard wrote:  "Motherboard has obtained 900GB" - okay, so that's not some usernames and passwords, nearly a terabyte - "of data related to Cellebrite."  Motherboard was directly contacted, by the way, by the attacker who extracted the data from Cellebrite.  Motherboard continues:  "...one of the most popular companies in the mobile phone hacking industry.  The cache includes customer information, databases, and a vast amount of technical data regarding Cellebrite's products.  The breach is the latest chapter in a growing trend," writes Motherboard, "of hackers taking matters into their own hands, stealing information from companies that specialize in surveillance or hacking technologies.  



"Cellebrite is an Israeli company whose main product, a typically laptop-sized device called the" - get this - "Universal Forensic Extraction Device (UFED), can rip data from thousands of different models of mobile phones.  That data can include SMS messages, email, call logs, and much more, as long as the UFED user is in physical possession of the phone."



And in fact there was a really sort of interesting photo that went along with this, that showed sort of a filing cabinet with just hundreds of phones shown, like in their repository.  And I've seen such things in hard drive data recovery facilities, where they maintain a massive store of spares for drives that die because their main board fries or something.  And they go, oh, yeah, we have one of those, and they swap the main board in, and the customer's back in business.  This is like that.  It's like, let's see.  And of course the phone may be 10 years old that law enforcement wants to crack.  And so it's like, oh, do we have one of those?  Yep, it's over here down Aisle D.  So, yeah, they've got quite a collection.



So they write:  "Cellebrite is popular with U.S. federal and state law enforcement; and, according to the hacked data, possibly also with authoritarian regimes such as Russia, the United Arab Emirates, and Turkey.  The data appears to have been taken, at least in part, from servers related to Cellebrite's website.  The cache includes alleged usernames and passwords for logging into Cellebrite databases connected to the company's my.cellebrite domain.  This section of the site is used by customers to, among other things, access new software versions.



"Motherboard verified the email addresses" - I thought this was kind of clever - "in the cache" - meaning this 900GB of data they received - "by attempting to create accounts on Cellebrite's customer login portal.  In the majority of cases, this was not possible because the email address was already in use."  Meaning there's already an account there with that email address.  So that's sort of a nice negative feedback means of verifying that, yes, this data appears to be authentic.



"A customer included in the data confirmed some of their details."  So Motherboard found someone they knew and said, "Is this you?  Does this sound right?"  It's like, ooh, yeah.  So the [audio anomaly].  "The dump also contains what appears to be evidence files from seized mobile phones, and logs from Cellebrite devices."  So the hackers got hacked.



And Leo, get a load of this one.  Google calls this RAISR - as in sharp and edge - R-A-I-S-R, an acronym for Rapid and Accurate Image Super Resolution.  This is a research paper.  A couple interns at Google in research and a few others teamed up.  The abstract of this paper reads:  "Given an image, we wish to produce an image of larger size with significantly more pixels and higher image quality.  This is generally known as the SISR, [S-I-S-R] problem."  Stands for Single Image Super-Resolution, meaning you don't get multiple images where you could then infer from multiple images a single higher image.  You get one low-resolution image, and you are being asked to bring it to super-resolution.



"The idea is" - and get this - "is that, with sufficient training data" - and this is where I was thinking, when you talked about we're pretty much into science fiction and some other world with AI now - "with sufficient training data, meaning corresponding pairs of low and high resolution images,  we can learn" - in the training and AI sense of "learn" - "a set of filters (in other words, a mapping) that, when applied to a given image that is not part of the training set, will nevertheless produce a higher resolution version of it, where the learning is preferably low complexity.



"In our proposed approach," they write, "the run-time is more than one or two orders of magnitude faster than the best competing methods currently available, while producing results comparable or better than state of the art.  A closely related topic is image sharpening and contrast enhancement, in other words, improving the visual quality of a blurry image by amplifying the underlying details.



"Our approach additionally includes an extremely efficient way to produce an image that is significantly sharper than the input blurry one" - so not only could it increase the resolution without getting blurry, but it could also just, like, using AI, essentially what they have done is they have trained something in what blurry pictures look like, and it's good enough to remove the blur, to learn what the original sharp picture was.  And if it knows that, then it could fix a blurry one, or scale it up to whatever size you ask.  So it's just - I'm thinking, okay, I really am beginning to feel my age when this sort of stuff is just happening.



And so finally they conclude, saying:  "We illustrate how this effective sharpening algorithm, in addition to being of independent interest, can be used as a pre-processing step to induce the learning of more effective upscaling filters with built-in sharpening and contrast enhancement."  So this is sort of the beginning of this next generation of image processing where one of the things that always bugs all of us who listen to this podcast is where a cheesy movie takes a satellite image of basically a furry blob in the desert, and then someone says, "Can you enhance that image?"  "Oh, yes, sir."  And then, you know, types a few buttons, and you see this wonderful thing scanning back and forth, and cubes are zipping around, and things are scaling in, and it goes [sound effect], and with each one of those [sound effect], it's like getting sharper and sharper, and magically emerges the license plate that you can read from a keyhole satellite at 50,000 feet, who knows.



Anyway, it's looking like that's not so farfetched because, when you think about it, different original images will have slightly different blurs.  And so there are things you're going to miss.  That is, if there was a defect in the image that the blur masked, well, the AI would have no way of knowing that, that is, I mean, if the information was really removed.  But there's been a lot of work on anti-blurring filters.  And now the point is they're going beyond an algorithm to something that is actually a knowledge base which has been trained by looking at blurry and not blurry, or a different size, and learning how to go from one of lower quality to higher.



And this all, by the way, was couched in various articles that picked up the story, talking about how Google might be, and this is a long way away, of course, but it was sold as a means of reducing the communications bandwidth.  That is, you send thumbnails, and this magic brain running in your browser, you know, Chrome Triple-Plus, just expands them to the proper size, and it looks just fine.  And even if it is Myrus, who, Smiley?  Sniley?  What is her name?  I forgot.



LEO:  Sniley Myrus.  No, no.  Miley Cyrus.



STEVE:  Miley Cyrus, yes.



LEO:  She was TV's Hannah Montana.  Come on, Steve, you must have watched that.



STEVE:  Eh, no.  No, I was busy with "Stargate SG-1" when she was dancing around or whatever she was doing.  So I got a question, I just picked this one out of the bag of many because our discussion that we've been having recently of passwords sort of brought this to the fore.  Rodrigo Barrouin said:  "You said that a random character password was better than one with random words.  Is this true of the Diceware system, even if longer?"



So the Diceware system is one which is basically sort of an easy and fun-to-use system for choosing words at random in order to build a larger composite password.  And many people ask this or related questions.  And remember it was last week we were talking about how, whatever it was, I can't remember the example I used.  Something or other 123.



LEO:  Horse, stable, cat...



STEVE:  Yes, thank you, horsestaple123.



LEO:  Something like that, yeah.



STEVE:  Yes.  And the fact that I could say it that quickly was a clue that we had a problem here because horse and staple are objects which are known.  And so their actual - the entropy they contain is dramatically less than the entropy that number of characters could hold, which is the problem.  And then I pronounced a 12-character piece of gobbledy-gook that took me 15 seconds just to get it all said, which similarly demonstrated that it actually had more entropy.  There was no way I could express that to our listeners any faster, which was, again, a clue.



So I thought, okay, let's formalize this.  And here it is.  If there exists ANY - and I put this in caps in the show notes.  If there exists ANY strategy for in any way short-circuiting a brute-force attack, then, while the system may still offer sufficient security, it does not offer maximum possible security.



So again, if there exists any strategy for in any way short-circuiting a full brute-force, basically try everything attack, then even though the result may still offer sufficient security, it does not offer maximum possible security.  Even my own Password Haystacks method has been validly attacked by those who said that haystack patterns could be checked for.  And those people are absolutely correct.  Checking for haystack patterns would be a strategy that would be better than sheer, blind, brute force.  So in terms of composing a truly maximally difficult to crack password of a given length, only if every character composing a string's password is equally probable to occur, randomly chosen, and without any interdependence among or between other characters, do we have both maximum entropy for the length and, by definition, no possible strategy for reducing the search space below what's required for a full brute-force attack.



So that's the formalization.  Any strategy, if you can look at it and invent a way of making it quicker, you're in trouble.  So clearly horsestaple123, well, dictionaries exist.  And we know that people tend to like actual words.  They may change the capitalization around, and that will slow down the attack.  But so imagine a brute-force system that was really well designed.  The last thing it would try was to assume a super-competent password chooser.  It would hope that a human used horsestaple123.  And so it would try those.  It's got its dictionary.  It would use the words in the dictionary and put them in different orders and intersperse different numbers.  And then, if it's really up to speed, it would think, huh, I wonder if this guy might listen to Security Now! and did the Haystacks approach to lengthening the password.



And so, again, before trying, before dropping to just, okay, I give up, I'm going to try everything, it would do things like append dashes to the front and the back, a bunch of them, and then asterisks, and then stick them in the middle and, you know, do haystack-y things.  The point is, if you look at it, and you can invent a way, and an algorithm could check for that, then that's not maximum entropy because a committed cracker will check for that.  That's what we're learning.  The cracking technology does do that increasingly.  And so the only safe place to hide is with maximum entropy.  Anything you do, even a bunch of words chosen at random, sorry, we've got a dictionary.  Those are words.  We're going to try those before we go off into the blue and just assume the worst case for the cracker, which is a maximum-entropy password.  So that's of course what we want to use.



LEO:  Moving on.



STEVE:  Yes.  So just one little bit of miscellany.  The question was raised, and we discussed what I had heard Reince Priebus say Sunday before last when he was criticizing the DNC's handling of email and the DNC hacks and so forth, claiming that John Podesta's email password was "password."  Well, it turns out that we sort of know where that came from.  The good news is it wasn't exactly "password," but it fails the high-entropy test by a long shot.  It's p@ssw0rd.



If we assume the validity of this piece of information from WikiLeaks, this was Eryn Sepp, who I did track down.  She is an administrator with the DNC.  And this email that WikiLeaks has is from her to John Podesta, saying, with the subject "2 things," says:  "Though CAP" - C-A-P, whatever that is - "is still having issues with my email and computer, yours is good to go."  And then it says "jpodesta" on the next line, and this "p@ssw0rd" with an "@" sign and a zero, which I think is her giving John Podesta his email account login.  Now, presumably somebody who was security conscious would change their password.



LEO:  That was his login for his computer, though, not his Gmail.



STEVE:  Oh, really?  



LEO:  Yeah.  Read it closely.  And it's, by the way, we do it, too.  In fact, the default we use is "changeme."  And then you're expected to take that and change it.  So again, we do know what the password was.  It wasn't particularly good.  The Gmail password is apparently the combination of the word "runner" and four digits.



STEVE:  Ah, so it was not p@ssw0rd.



LEO:  No.



STEVE:  Okay.



LEO:  This is just - and by the way, so what if it was?  That's blaming the victim.  Doesn't mean he wasn't hacked by Russia.  And anyway, do we know if it was a spearphishing attack?  I mean, was it just somebody brute-forcing his account, or was it a spearphishing attack?



STEVE:  Yeah.  And, see, this is the problem with any of this kind of coverage is that we just don't know.



LEO:  We don't know.



STEVE:  Yeah.  And so it's just not - it is, in fact, it's unknowable, unfortunately.  I did have a...



LEO:  I thought it was a phishing email.  I was pretty sure.



STEVE:  I definitely heard that, Leo.  But so that's what was said.  And in fact, it might even have been somebody over on the  DNC side who said that, not somebody who may have had an axe to bear, to grind, or do something with.



LEO:  IT guys routinely will do that.  Here's your password.  And it's some loopy password like "changeme."  That's what we use.  And you should change it.



STEVE:  Right, right, yeah.  And so that's just enough to get you to log in, in order to make the password change, yes.  



LEO:  Anyway.  It says, if you read it closely, it says "Windows 8 password."  He's telling him how to log into his Windows 8 system.  "Your computer is good to go."  And then he warns you, the Windows 8 system is very different from what we had back at the White House.



STEVE:  I did see all that, yes.



LEO:  Yeah.  So, you know, this whole story came from, well, I don't even want to go into it.  And I don't think it's even germane.  If the Russians hacked his email, whether they did it easily or with difficulty doesn't change the importance of a foreign government attempting to change the results of our election.



STEVE:  I was just following up because I had heard that, and there was some question about it.  And then when I saw this, I thought, oh, maybe it was actually "password."



LEO:  Now they're saying "runner," the chatroom's saying "runner" wasn't even for his Gmail.  That was his iTunes, his Apple account.  So I don't know.



STEVE:  It's so sad that we know all of this, though.  



LEO:  Well, because his email was leaked.  But I have to say, if you read the email, besides being embarrassing, it wasn't particularly revelatory.



STEVE:  No, and I think in general that...



LEO:  I never saw a smoking gun in there.



STEVE:  Right, right, right.  So I got email from Tim D., who said he's near Detroit, Michigan.  And this was following up to last week's discussion, and this is a tip that I had never really thought about.  But when he said it, it's like, it makes so much sense.  He said - so the subject was SN-594, which was last week's SpinRite story, which was about the RAID 10 that was having problems until the user ran SpinRite on his drives.  He said:  "I just wanted to note something I learned from an older and wiser geek."



And then Tim writes:  "He told me that about half of the very substantial price difference between a commercial (Compaq at the time) server and something with similar specs that you built out with consumer parts was that Compaq has a large supply of hard drives that are not managed in a first-in/first-out manner."  Meaning that Compaq maintains their own warehouse, essentially, of drives.  And so he says:  "Instead your server" - and he means here a Compaq server or any server - "would be assembled so that you did not have two drives from the same manufacturing run in your array because infant mortality in hard drives would tend to cluster around certain manufacturing runs."  Which is known.  "If one drive failed early, and all drives were from the same manufacturing run, it's more likely that a second drive would fail before a new drive could be swapped in and the array rebuilt."  And he finishes, saying:  "This is the reason I have always purchased drives for my Drobos one at a time."



And I thought that was really interesting because there are famous examples.  I remember, like, where a major drive manufacturer will go through a rough period, where they just sort of, you know, sometimes in the semiconductor fab industry it's called "losing the process," where something gets contaminated, or something happens, and they'll produce a big batch of drives, or in some cases processors, which pass QC, get through test, but there's a little whisper of a problem or something that only reveals itself out in the field.  And those tend, because everything is done in a big production batch, and then the assembly line is switched over to make a bunch of something else, and then later it's switched back, there is a temporal and a sort of a batch-based connection.



And so what I liked about this was if you really did want to assure yourself of the most robust result, you would rather have a heterogeneous collection of drives than a homogeneous collection.  And really you would not want to populate a large RAID with just all drives from a single batch because - if there was a problem.  Now, the flipside is they might all be really, really good, rather than to have a problem.  But in a RAID the whole point is to protect yourself from a low-probability but high-value failure.  And spreading your drives around makes a lot of sense.  So anyway, Tim, thank you for that tip.  That's one that made total sense when I heard it, but it hadn't ever occurred to me independently.



So, What's Up With WhatsApp?  Someday, hopefully, we'll have a widely recognized single understanding of the term "backdoor."  We don't have that yet.  The confusion arises partly because the term is good for baiting clicks on websites; and a backdoor is a scary term, partly because it seems sinister, and also because it's a brilliant term.  Everyone can readily visualize a backdoor.  As we saw during the FBI's request for a "we're not asking for a backdoor, we want a golden key to the front door" debacle last year, the proper use of the term requires an understanding of the intent.



A UC Berkeley security researcher, Tobias Boelter, waited nine months after informing Facebook and WhatsApp of what he believed to be a security vulnerability in WhatsApp's implementation of the Signal protocol.  When he was ignored and obtained no satisfaction, he went public, and the Guardian dropped the bomb using the heavily freighted term "backdoor."  The trouble was, and is, it was neither a mistake, which Tobias found, nor a secret.  It's a deliberate and carefully considered design decision.  This doesn't mean necessarily that it's the correct decision, but it does definitely mean that it's not a backdoor in the usual way that we still somewhat softly understand the term.



A backdoor is, I think, if we were to define it, by definition an unknown and secret password or something like that, a secret something, embedded for example into router firmware that allows anyone who knows the secret incantation, whatever it is, to obtain unofficial and unauthorized access to the device, whatever is being protected.  That's a backdoor.  But figuring out how a deliberate design decision feature can be abused, while it may not be a good thing, it's not a backdoor.



So shame on The Guardian for succumbing to the temptation to use that term.  It achieved its goal, but I would presume at some small increment of reputation cost to them because there's been a lot of back push on this.  Moxie posted, and he was really unhappy at Open Whisper Systems, I mean, and it caused a ruckus.  Well, and no kidding because Facebook bought WhatsApp and its, as I said at the top of the show, trillion messages a year.  It is the most heavily used, supposedly secure, end-to-end secure instant messaging system, and even delayed messaging system, which actually is where the story goes, that exists, just because of the size of Facebook.



So what's this all about?  It's another classic instance of a tradeoff between true security and security transparency.  Users say they want encryption privacy.  We know that privacy requires authentication to be meaningful.  That is, you have to know who you're talking to.  It's not enough just to have arranged a private communication.  If you're talking to the man in the middle, then you don't have privacy.  But edge cases arise that force a decision about whether to involve the user.  As we discussed on Episode 555, and I remember at the time people saying, "Are you going to make this a special episode about the classic timer chip, the NE555?"  I said, uh, no.  Instead we talked about WhatsApp.  That was the title of podcast #555, where we explained that WhatsApp is an implementation of Open Whisper Systems Signal protocol.



In its introduction back then, the introduction to the podcast, I summarized the news of the week and concluded:  "And we will discuss the result of my deep dive into the Open Whisper Systems Signal communications protocol that's finally been fully integrated into the world's number one multiplatform messaging system, WhatsApp, along with" - and I said this at the time - "two things" - and what was it?  That was 40 weeks ago.  So if we're at Episode 595, that was 555.  And back then I said:  "...along with two things that must be done to get true security."  And it's not entirely a consequence of the fact that they're not, but that would at least mitigate this and provide notification.



So what's going on?  Performing real-time online messaging, where both endpoints are guaranteed to be online for the conversation, such as with any VPN, it turns out that's an easier problem to solve securely because, as we've discussed before, we have robust protocols, such as the Diffie-Hellman Key Agreement, which allow the two ends to securely negotiate a new shared secret, on the fly, in full view, with their traffic being monitored.  Yet nobody in the middle, nobody passively examining the traffic can figure out what key it is that the two agreed upon, even seeing them talk.  It's a perfect piece of crypto technology.



So long as reliable authentication is tied into the negotiation, meaning that you are sure who you're negotiating the new key with, to prevent tampering up to and including a man-in-the-middle attack, the endpoints are free to generate shared encryption keys at the start of every dialog and even to regenerate them periodically for long-lived communications.  You wouldn't want to, for example, use that key forever if you had a VPN that was just up statically connecting two offices because that would mean you don't have what we call "perfect forward secrecy."  If you'd always been using that key for a long period of time, and it became compromised in the future, then that key could be - and all of the historical traffic had been captured, as we believe the NSA is doing with their massive drive farm in Nevada - no, not Nevada, in Utah - then the problem is the future compromise of the key would allow all past communication to be decrypted.



So a more secure system is periodically expiring the key.  During the ongoing traffic, one end will send to the other, hey, let's do another key agreement.  And they'll both take new random numbers, while everything is going on agree on a new key, and then synchronize their change over the new key.  So they're constantly expiring and switching to new keys so that at any point that a key got exposed somehow, the key probably in use, well, the key that was in use would only compromise the piece of traffic, the piece of conversation that was in place at the time.  Okay, but that's, by definition, everything I just said assumes the endpoints are online together, and they can do this in real-time.  That's effectively synchronous communications, where they're able to dynamically interact.



But the plot thickens a great deal, actually, when we ask for the same sorts of guarantees of authentication and secrecy, that is, privacy, from an asynchronous communication system like messaging.  In the asynchronous scenario, we want to send a secure message to the other endpoint, even if it's not currently online.  And we discussed 40 weeks ago how the Signal protocol arranges for this.  And that's, of course, what WhatsApp inherits from Signal.  Signal and WhatsApp ask clients to pregenerate a substantial block of 100 public key instances which will be escrowed on the central messaging server.  So these keys are then distributed on an as-needed basis to anyone who wishes to obtain a key that is going to be a public key for the message's recipient.  And that key would then be used to encrypt the message which is going to be sent to that recipient.



So if endpoints are offline, not only does the messaging server hold essentially prekeys, these keys available for distribution while a client is unavailable, to allow people to asynchronously encrypt messages; but, if that endpoint is offline, the messaging server must and does retain all unsent messages in encrypted form.  It can't read them, so it's just storing blobs.  But it's waiting for the recipient to come back, to log back into WhatsApp or Signal.  And then it says, ah, and then it flushes those messages to it, and that client is then able to decrypt those messages because it's got the master key from which these ephemeral public keys were created.



So here's the problem.  What happens when that other endpoint, that recipient endpoint, changes phones, or SIM cards, as is more frequently done outside the U.S., or when the application is reinstalled, while there are still pending messages  encrypted for its previously keyed identity?  So again, the scenario is we have an asynchronous messaging platform.  All clients keep the central server loaded with keys to use in the future by other people who wish to send them an encrypted message that the middleman, the messaging server, nor anybody else listening, is able to decrypt.



So they sort of made a promise.  They've set up a bunch of keys and said, "Use these to send me stuff."  And then they go offline.  Then a bunch of their contacts do send them things, but because they're offline the server holds them.  Now, while staying offline, they change keys.  Something causes them to change the key.  WhatsApp gets reinstalled, they get a new phone, they change SIM cards, whatever, or just reinstalling WhatsApp.  So now we have a problem.  The root key which created these ephemeral public keys is different and will not be able to decrypt the messages that come back to it when it next connects.



So this is where Signal and WhatsApp diverged in their policies.  Signal decided we're going to go for privacy first.  We're not going to compromise that.  So what happens is they block any additional communications and notify the sender that the recipient's key has changed and require affirmative acknowledgement and agreement.  Now, and understand that this could be a malicious key change.  So the key changing is one of the tricks in this system, and that's one of the things that we talked about at the time.



And I said to people, the thing that annoys me is that is off by default.  That is, the notification of a contact's key changing is not on.  So I told everybody listening, if you're going to use WhatsApp, and I don't remember what the case was with Signal, but certainly with WhatsApp, if you're going to use it, go into the security settings and turn that on because you absolutely want to know if a key changes.  Not being notified...



LEO:  Signal always tells you.



STEVE:  Okay.  So there's no ability to disable that.



LEO:  No.



STEVE:  Okay, good.  And so, of course, that was one of the two things was make sure you had that turned on.  The other is you could take it on faith that the key you're being displayed or that you've been given belongs to the person you think you're talking to.  But authentication is incomplete unless - and there's really no way to automate that.  We want to keep not taking responsibility for authentication, but it's ultimately up to us, if we really care.  I'm not telling everyone they have to care.



But if you really do care, you need to authenticate, meaning that there is that block of signature digits that represents the other end's key, and they've got yours, and you need to somehow, hopefully out of channel, that is, don't use the messaging app itself to send that back and forth because that would be self-defeating.  You need to use an out-of-channel means like talking to each other or emailing some of the digits or something.  It's just a hash of your key, so it's not sensitive information, but it can be used to verify the key.



So Signal decided they're going to alert the user to a contact's key changing.  Not only did WhatsApp decide not to alert, but they also decided, you know, we need to make this more transparent to the user.  We get 15 trillion messages a year.  This thing is too popular.  We just don't want to, like...



LEO:  Rock the boat?



STEVE:  Exactly, rub people's face around in something messy because they won't like it.  They won't understand it.  They won't know what is happening.  So what WhatsApp does is the way the protocol works, the sender of the message that posted the message onto the server holds it until the server sends back a notification that that held message has been received successfully by its recipient.  And as I was reading this, there's apparently a one check and a double check.  And so the one check is the server got it, and the double check showing is the recipient got it.  So while there's a one check on the sent messages, your client still has them.



And so again, purely for the sake of convenience, WhatsApp chose to make the key change absolutely transparent.  The way they do that is that when the recipient reconnects with a new key, WhatsApp realizes, the WhatsApp server realizes that and says, oh, shoot.  I've got a bunch of messages for you, pending, encrypted under your old set of public keys that you can no longer access because you've just changed your master key.  So hold on a second.



The messaging server obtains a new blob of 100 ephemeral keys for that newly rekeyed endpoint, redistributes them to all the clients whose messages are pending and says, we had a problem in transmission.  Would you please reencrypt those messages you sent before, but haven't deleted because we told you not to, because we hadn't confirmed their delivery.  We need you to reencrypt them for us under this new key.  Without any fanfare, no user notification, nothing happens, that all happens behind the scenes.



So the messages that had only kind of been half sent get resent under the new recipient's key.  And they go back to the messaging server.  Now they can be sent to the newly rekeyed recipient, who can decrypt them.  And that notification goes back to the messaging server that sends it back to the senders, and they get the double check on the messages, and everybody's happy.  Well, not everybody because it's clear this - it's not a backdoor, it's a problem, if you absolutely really care about security.  The problem is now that everybody knows about this.  I mean, maybe some people knew about it.  This is why I believe that disclosure is always the best policy.



But all law enforcement in the world now knows that it is possible to cause the most secure messaging platform on the planet - the most popular, not the most secure, the most popular supposedly secure messaging platform on the planet - to not notify endpoints of message receipt, that is, not give them the second check and permission to delete it locally, and to give them any key they want and cause the messages which have not yet been sent to be resent under a key that a third party could decrypt.  And it turns out that it doesn't - the argument has been, oh, that only applies to the most recent message.  No.  If the protocol is obeyed, that's true.  But nothing enforces the protocol.



Tobias demonstrated that it is possible to have a conversation back and forth between endpoints where the messages are never being confirmed.  The conversations happen, but the final message is not sent from the messaging server, preventing the endpoint from dispensing the messages.  And in real-time there can be constant rekeying going on which is completely transparent to the user.  So we don't know this has ever happened.  We don't have any reason to believe it would be.  But this is a consequence of the decision that WhatsApp made not to give people a dialogue box they won't understand.  Just like, wait, wait, what do you mean, my contact's key changed?  I mean, what?  What are you going to do with that information?



And so they said, no, we're just, you know, not only - oh, and even if they did turn on the security switch, saying I want to be notified of any change to my contact keys, it turns out all pending messages are rekeyed and re-sent before the dialogue is displayed.  Well, figure - explain.  Figure that, well, you know, why?  But that's what happens.  That's the way the system is designed.  So that's what this was.  I love this because it's a perfect story for this podcast.  Moxie and Open Whisper Systems understood the problem, I mean, and solved this difficult asynchronous messaging system in a clever way, with this notion of a blob of prekeys that are used ephemerally in order to allow offline encrypted messages to be stored by a central server that cannot decrypt them.



But then you have the problem - if you're in real-time, there's no problem with a key being changed when you're offline because you're online.  But if you're asynchronous, either end's key could change and does change for any number of reasons.  So they had to figure out how to do that.  They did that by causing the endpoints to hold unconfirmed received messages until they had been confirmed received, and the ability to have them rekeyed and resent.  And with that goes notifying the user.



But, and as you said, Signal does that.  Unfortunately, as you start chipping away at the details and take away notification and then make it completely transparent, and even if you notify, you do it after the fact?  Okay, well, then you've really got broken privacy.  Again, we don't know that it's going to happen.  But now what we're seeing is law enforcement is being very aggressive about what they're asking for, and they're getting courts to ask for what they want, and they're compelling companies to comply.



So this is an interesting tradeoff.  I read Moxie's complete rebuttal.  He was standing up for Signal and WhatsApp.  And he argues that, okay, yes, the way The Guardian portrayed it as a backdoor was clearly wrong, and I completely agree; that WhatsApp had a hard decision to make.  Were they going to offer privacy, but set the bar to a point that it would be a problem for users, which might drive them to, as Moxie put it, some other less secure solution?  That is, this is good, but unfortunately it can be manipulated because of the details of the way it was designed.



So I'm still with Threema.  I don't actually use Threema because I don't have anything I'm sending my friends except, oh, did you know that Season 6 of Homeland has restarted?  It's like, okay.  But here's my message to anybody who is paying more than [audio anomaly].  If you actually truly really do need security, then you're going to have to sacrifice some of this convenience.  And I would argue most people on Facebook, they're also, you know, retexting that Season 6 of Homeland has started.  They're not doing anything more that requires real protection from law enforcement or authoritarian governments and so forth.  If you do, then what I love about Threema is that it doesn't have any of this complexity.  It is dirt simple.  Which is why it's trustable.  It's now been audited, too.



And remember, with Threema, it does nothing for you.  It's 100% up to you to verify the key of the person at the other end.  And they provide means to do that.  But it's up to you to do it.  And if the other end changes their key, you've got a broken link.  But wouldn't you rather have a broken link than have an unknown link to some foreign government, or your own government?  So that's the tradeoff.



Anyway, I love this for the podcast because it's a perfect instance of the press mischaracterizing this, but there being a true dilemma because we're, despite everything Moxie and company did, this is the best they could come up with.  And it's very good.  But you then say, oh, yeah, it's very good, but we can't be, like, popping up some scary thing every time a key changes.  Okay?  But part of the security model was that we would do that.  And so if you start chipping away at it, then you do open this kind of, it's not a backdoor, but it is an opportunity for exploitation or for someone to be compelled to use the system in a way the protocol was not designed to be used.  And it will do that.  You need no modification to the clients, only the central server.  And you can create a third-party tap.



LEO:  So action items would be use Signal.



STEVE:  Yes.



LEO:  Or turn on the warning in WhatsApp, in the settings.



STEVE:  Yes.



LEO:  Then it would be roughly equivalent to Signal, or would it be exactly equivalent to Signal?



STEVE:  Not exactly, because unfortunately it notifies after the horses have left the barn.



LEO:  After, right, okay.



STEVE:  So when you were sending your secret location of the rendezvous to meet with the other bad people, that message got out and could get decrypted.  And unfortunately, only then it pops it up.  But by that point, unfortunately, the decryption is done.



LEO:  If you're really up to no good, I would think that the best thing to do would be to install PGP, generate long keys, 4,096-bit keys, be very careful about protecting your private key, and then just using - because it's easy to encrypt with PGP anything; right?  Or are you saying a one-time pad?  Or just a pen and paper?  That's what President Trump says.



STEVE:  Unfortunately, I did see that, as a matter of fact.



LEO:  Yeah, write a message and send it with a courier.



STEVE:  It is hard to argue.  And in fact we've talked about it.  We've said, "Go into the middle of a park and whisper to each other."



LEO:  That's better than paper, yeah.



STEVE:  "After you throw a coat over both of your heads so there's no audio leakage and no lip reading being done."  I mean, it really is a problem.  And remember that we still have the problem that, if some malware, or we now have a word for that, it's called an "implant," that's the official NSA term, an implant in the phone will still be sending stuff out before it gets encrypted.  So, yeah.



LEO:  You might have noted the use of Telegram in the dossier.  And in fact the dossier implied that Telegram had been hacked.  And Telegram was at great pains saying, no, no, we weren't hacked.  Of course we don't know if the dossier is real or not, or if all the items are real, or some are real, whatever.  But I have to say, you know, I remember I told you there was a dog whistle in the blog post, Telegram blog post a few months ago, where they had Pepe in there, Pepe the Frog.  And remember it's written by a Russian, Pavel Durov, who claims to hate Russia because they stole, you know, he was the Zuckerberg of Russia.  They stole his Facebook clone.  Putin's government forced him to sell [crosstalk] fire sale, and so he's fled Russia.  Yeah, but maybe not.  And I don't know.  I wouldn't - we've talked about the fact that Telegram uses its own roll-your-own crypto that isn't really necessarily safe.  We certainly don't know how safe it is.  So don't use Telegram.  Use Signal.  Or Threema.  Now, Threema's not open source, but you say it's been audited.



STEVE:  Yes.



LEO:  So that's as good.  Okay.  Just, you know, in case you're up to no good.



STEVE:  I use iMessage.  I use iMessage, and I drive my car around, and I don't worry about any [crosstalk].



LEO:  iMessage is no better than a lot of other systems where the key is held by the company.



STEVE:  Correct.  Key management, remote key management is the bugaboo.



LEO:  Yeah.  Because that means Apple has access to the messages.



STEVE:  Yeah.  If you're not authenticating yourself, then you're not secure.  I mean, you probably are.  But you're not guaranteed.  There's no way, we have not found a way of subcontracting authentication.



LEO:  Yeah.  Yeah, it's an interesting, interesting world.



STEVE:  It's an interesting theoretical problem.



LEO:  It is, it's more interesting from a theoretical point of view, exactly.



STEVE:  Yeah.



LEO:  Yeah.  Steve does this show every Tuesday at about 1:30 Pacific, 4:30 Eastern, 21:30 UTC.  If you'd like to stop by and watch live, join us in the chatroom at irc.twit.tv, we'd love to have you.  But you don't have to because we have on-demand versions of the show.  Steve's got them at his site, GRC.com.  That's one place you can get them, GRC.com.  He also has transcripts.  That's the only place you can get that.  He also has SpinRite, the world's best hard drive recovery and maintenance utility, his bread and butter.  GRC.com.  Lots of other good stuff there, as well.



We store audio and video at our website, TWiT.tv/sn.  You can watch live there or watch live on YouTube, YouTube.com/twit.  There's also a Security Now! channel on YouTube.  You can watch the video there if you want.  And you can download copies of audio and video from anywhere you get your podcasts, including iTunes and Google.  Google Play Music has our show.  Spotify has our show.  Stitcher.  Slacker has our show.  TuneIn has our show.  It's everywhere.  Just subscribe.  That way you won't miss an episode.  Thanks for joining us.  Next week you want to do questions?



STEVE:  Let's try, yes.



LEO:  Leave your questions for Steve.  He's @SGgrc on the Twitter.  Or you can go to GRC.com/feedback and fill out the feedback form there.



STEVE:  Yup.



LEO:  And we will get to as many as we can next week, good lord willing and the creeks don't rise.  Thank you, Steve.



STEVE:  Thank you, my friend.  Talk to you next week.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#596

DATE:		January 24, 2017

TITLE:		Password Complexity Calculations

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-596.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  This week, Leo and I discuss how, while still on probation, Symantec issues additional invalid certificates; how  Tavis Ormandy found a very troubling problem in Cisco's web conferencing extension for Chrome; yesterday's more-important-than-usual update to iOS; and renewed concerns about LastPass metadata leakage.  The SEC looks askance at what's left of Yahoo.  We talk about a troubling browser form autofill information leakage.  Tor further hides its hidden services, and China orbits a source of entangled photons.  Then Heartbleed three years later, a new take on compelling fingerprints, approaching the biggest Pwn2Own ever, some miscellany, and, finally, some tricks for computing password digit and bit complexity equivalence.



SHOW TEASE:  It's time once again for Security Now!, your favorite show.  Every week, Steve Gibson joins us and talks about the latest security news.  He's found, well, somebody found and he's going to talk about, a really horrible flaw that exists in Chrome and some other browsers.  You won't believe this one, and it's been around for a while.  He'll also talk about passwords once again, the easy way to determine if your password is really good.  Well, easy if you love logarithms.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 596, recorded Tuesday, January 24th, 2017:  Password Complexity.



It's time for Security Now!, the show where we cover your security and privacy online, all over the globe.  And it's going to be more and more important.  This guy here, he's doing it, Steve Gibson of the Gibson Research Corporation, creator of SpinRite, discoverer of spyware, author of the very first antispyware program, and our guru.  We sit at his feet and learn each and every Tuesday.



STEVE GIBSON:  And sweeper of cobwebs.



LEO:  Well, that we've got to do, too, a little bit, get those cobwebs out of the brain.  We had yesterday, I did Triangulation with my old friend from ZDTV, Alex Wellen.  And in 1999 he and his partner Luke Reiter pitched ZDTV with a show called "Cybercrime."  And it was, I mean, here we are, 17 years later.  It was the earliest days of malware and hacking.  In fact, we showed a little clip of it from YouTube that had - what was the - was it Melissa?  It was one of the early viruses.



STEVE:  It was so quaint back then.



LEO:  It was quaint.  It was half an hour a week.



STEVE:  It had a macro in your email.  Aw, ain't that adorable.



LEO:  Exactly.  And little did we know, I mean, how advanced this would all get and how much it would change.  Yeah.



STEVE:  Yeah.  So we have - there was just a ton of news.  And a number of our listeners have become very engaged, a large number actually, in this whole question of password complexity, password formation.  And, for example, I saw so many times over the last week, well, okay, Steve, you said you don't like to use dictionary words.  But if you had enough of them, wouldn't that be strong?  And I thought, okay.  I've sort of glibly danced past some math which I use all the time for answering those questions.  And I've never explained it.



And I thought, what I should do is give our listeners, who are obviously interested and capable, the tricks that I use for answering those questions so they can perform their own tests.  They can look at the number of words in the dictionary and determine how many bits of entropy that number of words, if randomly chosen, would have.  And then, by multiplying by the number of those words, they can determine the total bit strength of a password generator and so forth.  So the topic for this week is Password Complexity Calculations.  And it sounds kind of freaky and scary, but it's actually amazingly simple, thanks to the incredible beauty of logarithms, which of course make slide rules popular.  Now, I realize children may not know what a slide rule is.



LEO:  I don't think slide rules are popular anymore, but okay.  It's how they work.



STEVE:  Yes.  The idea that you can add lengths to multiply numbers is like, what?  Because of the special shape of this curve.  So it turns out there's, like, a couple simple things that we're going to explain that allow us to determine some really cool things about passwords.  But first, there was so much news this week.  While still on probation, Symantec has stepped in it again with being caught issuing invalid certificates and not acknowledging them, so it was caught by a third party.  Tavis Ormandy, our friend at Google, has found a very troubling problem in the 20-plus million widely distributed Cisco WebEx extension to Chrome, which allows any site you visit to install any malware that it wants on your computer. 



LEO:  Oh.



STEVE:  Not good.  We have yesterday's more-important-than-usual update to iOS.  There was also a flurry of concern about LastPass metadata leakage in the last week due to a posting that someone put up about their discovery.  And so we're going to explain about that.  News of the SEC looking askance at what's left of Yahoo after the Verizon acquisition.  A very troubling browser form autofill information leakage that actually works, and a way our listeners can frighten themselves and then think about whether they want to leave it the way it is or what they want to do.  An improvement coming later this year in Tor's Hidden Services feature to make them less discoverable.  China, orbiting a - I can't even believe we're there, but maybe we are - a satellite which distributes entangled photons, as in quantum entanglement, to distant locations on the globe to allow instantaneous undetectable communications.



We're also going to review Heartbleed, which is now coming up on three years old.  An interesting new take on the compulsion to produce fingerprints.  We're approaching the biggest Pwn2Own competition ever.  And we now know how that's going to get laid out.  And after all that, a little bit of miscellany, and then we'll talk about computing password complexity.  So I don't think anyone's going to find this podcast comes up short.



LEO:  Lots of good stuff.  Anyway, on we go.



STEVE:  So our Photo of the Week is just - it's fun, a snapshot of things to come.  It shows, for people who can't see it, a guy with one of our mobile coffee carriers in his hand, apparently in the process of leaving the house.  The front door is open.  He's turning back to look at this black cylinder which is speaking to him near the front door.  And we know that that's probably the Amazon Echo.  And it says, "Your phone told your Fitbit that told your Nest that told your Sonos that told me that you owe your wife an apology.  Two dozen red roses are only $29.99 for a limited time."  And he sort of looks at it with a little worried look on his face, like, okay, you know, is that the case?  So, ah, yes.



LEO:  Love that.



STEVE:  It's all connected, and who knows what's going to happen?  And actually this ties into a couple notes we have about that Amazon technology later on in our miscellany.



LEO:  Oh, good, yeah.



STEVE:  So, okay.  As we covered at the time, back in 2015, Symantec was caught having issued some certificates that they shouldn't have, which is of course a big no-no because essentially, as we've often discussed, the PKI, this Public Key Infrastructure for browser certificate trust, the whole thing relies on essentially something impossible, we could argue, and thus its weakness.  That is, the perfect performance and behavior of a huge, and I would say ever-growing, except some are dying because they misbehave too badly that they stop being trusted, but hundreds of individual entities all trusted in parallel to never make a mistake in issuing a certificate which asserts the identity of the entity to whom they issue the certificate.



So unfortunately, for the system to work functionally, all of the browsers everywhere must trust all of the certificates issued by all of the certificate authorities everywhere, in this many-to-many mapping where a single mistake can have significant consequences.  Well, basically it violates the integrity of the system for the entity that is named in that certificate.  So everyone tries to do the best job they can.  But the nature of it requires monitoring and policing.



So one of the things that's on my list of in-depth coverage is a system that Google has designed and has brought up called the Certificate Transparency Log, which is essentially a publicly accessible database of all issued extended validation certificates.  At this point they are asking all certificate authorities to post to this certificate transparency log any extended validation certificates they issue.



And when you think about it, this is a nice additional layer.  It doesn't, like, lock down anything.  But if we didn't have a central visible clearinghouse, then there's no visibility at all into what certificates certificate authorities issue because they have a one-to-one transaction with the person asking for a certificate, and that person then puts it on their web server.  And only when people visit do browsers then see the certificate that they were issued.  There's sort of no oversight globally available.  So this is a really worthwhile first step.



So looking at this Certificate Transparency Log, a security researcher named Andrew Ayer, who is also the founder of a certificate authority reseller known as SSLMate, he was just browsing through the Google Certificate Transparency Log that's maintained by the Google Certificate Transparency Project.  And he noticed some oddities.  And then when he looked closer, he probably did some data analysis through the log, he found 108 invalidly issued certificates by three Symantec-owned certificate authorities.  Nine of those 108 were issued without the permission or knowledge of the affected domain owners.  So that's a big no-no.  That is, certificate authorities aren't supposed to just make certificates for random domains that they feel like.  I mean, they have a role in this larger scheme.  And it's crucial that they not step outside of that.



Of the remaining 99 certificates, they were issued without proper validation of the information contained in the certificate, the company information.  Now, the problem is that, as we know, these certificates would have enabled their holders to spoof the identities of sites that were protected with HTTPS or TLS communications.  And the whole idea, we talk about this often, privacy through encryption is nice.  But unless you have authentication, unless you absolutely know as robustly as possible the identity of the other end of the connection, then without that, you have actually no guarantee whatsoever.  You  may be scrambling your bits, but you might be scrambling it to somebody other than who you expect to.



So authentication is, you know, that's really what the certificate provides, thus all of this drama and why it's so important.  So it is the case that many of the certificates issued contained the string "test" in various places.  And they were revoked shortly after being issued, in as little as an hour.  But that's nevertheless a significant violation of the CAB rules, the Certificate Authority Browser Consortium rules.  And as we know, because I went off on Google and Chrome about this years ago, Chrome in particular, this whole CRL set, Certificate Revocation List is utterly broken.  It doesn't work at all.  And to the degree that it kind of works, it turns out that it's only extended validation certificates that they're concerning themselves with, despite the fact that that's a minuscule fraction of the total certificates in the industry.  So the fact that they were revoked really provides no security, I mean, no practical security.



And as we also discussed back then, when a browser tries to verify through the online technology whether a certificate is still valid, unless it affirmatively is told that it is invalid, it fails open.  That is, because the whole validation system is still not as robust as it should be, it's just sort of been limping along coming forward over time, the browsers don't want to inconvenience their users with false blocks of access.  So unless it's told it's invalid, it assumes it's not.  So the point is revocation isn't any excuse.  And no one argues that this wasn't a mistake.



And what's interesting is these were not EV certificates because, as a consequence of Symantec's misbehavior in 2015, Google required that Symantec post all issued certificates into the Certificate Transparency Log, not just extended validation certificates.  So this has just happened.  It's not clear what's going to come of this.  The three CAs that Symantec now owns are the Symantec Trust Network, GeoTrust, and Thawte, spelled T-H-A-W-T-E.  So those are all Symantec properties, and they were up to some mischief, which as a consequence of this Certificate Transparency Log we now know.  If it weren't for that, and if it weren't for the fact that Symantec were on probation and had to post the brief creation and in some cases miscreation, and certainly creation in violation of the CAB guidelines, if they didn't have to post that, this would have never been seen.  It would have just been under the radar.



So here is props to Google for coming up with this concept of making high-value certificates visible in a way that helps us deal with the fundamental problems that we have of the system being fragile by nature.  And then also in requiring Symantec's behavior to change in response to some earlier missteps, which did catch, as a consequence, later missteps.  And thanks to Andrew - for whatever reason, he was poking around in the log - for finding this and bringing it to the industry's attention.  So, and what'll happen is two years ago a bunch of people were fired immediately after this was discovered.  I'm sure that Symantec will be, I mean, they're wanting to be responsible.  Now that they know this happened, and now that they know we know this happened, I imagine some additional heads will roll.  And hopefully the industry at large, the CA industry, is observing this and thinking, oh, you know, that's a doghouse we don't want to be in.



And more props to Google, in this case Tavis Ormandy, who is a bug hunter whom we often are referring to.  He found something very distressing buried inside of Cisco's WebEx, which is their web-based teleconferencing system, the WebEx extension for Chrome.  And reading Tavis's log of what he did, you just sort of think, I mean, he did some things that were a reach at best, and then they shocked him when they worked.  It's like, oh, my lord, this worked?



So what happened was he found what we call a "magic cookie" inside of the Cisco WebEx browser extension.  And it's those GUIDs, G-U-I-D, Globally Unique Identifier, which are sort of groups of hex, eight characters in the first group, and then four sets of four, and then eight in the last group.  So there's one of those following "cwcsf-nativemsg-iframe" and then hyphen and this GUID.  So he thought, okay, that's interesting.  So it turns out that magic cookie was an enabling token for Cisco's access to a very powerful DLL which they had installed as part of this browser extension in the user system.  This was essentially - I'm not going to use the term "backdoor."  We'll call this an escape hatch.  This allowed the extension to reach down into API functions in this DLL, which is down in the system, and get it to do things that Cisco's WebEx browser extension for doing teleconferencing needed.



The problem is what Tavis found was a lack of control.  That is, this really wasn't that hidden.  It was part of the manifest in the browser, so it was discoverable.  And anyone else could do what Tavis did.  Now, what Tavis arranged was simply, you know, the standard test of can I run calc.exe?  And sure enough, without any permissions from a non-Cisco domain, Tavis was able to cause the Cisco WebEx browser extension to run code of his choice, in this case calc.exe.  And it looks like it's very possible to cause this to download whatever they want, that is, whatever the user of this API wants, and cause it to run.



So essentially this would have allowed a no-user-interaction privileged remote code execution on any of the 20-plus million machines where this WebEx extension was present in the Chrome browser, and the Chrome browser was being used.  So any malicious website or an advertisement, because this also works in iFrames, could silently install malware in the computer of anyone visiting with Chrome who had the WebEx extension also installed and enabled at the time, as it typically is.



So to Cisco's credit, Tavis posted this secretly under Google's 90-day, if this isn't fixed within 90 days, whether it is or not, it goes public, at the very end of last week.  And Tavis was very impressed.  They responded within that day with a patch.  So there's an update to this Cisco WebEx extension, 1.0.3.  That's what you want to make sure you have.  So you can use the Chrome UI to browse the extensions you have installed, check the version numbers, verify you've got 1.0.3.  So Cisco responded immediately.  However, Tavis, while he was impressed with the response speed, he was less impressed with the nature of the response.  They asked if it would be okay if they simply restricted the domain from which the magic URL could be used to *.webex.com.



Now, until now it didn't matter where it was being run from, so you had no same-origin protection at all in this, which is what caused this serious concern.  So locking the use of that to any subdomain or machine name of WebEx.com does control it to a greater degree.  Tavis accepted the update, but hopes - and I got a kick out of this - that the Cisco WebEx.com site doesn't, as he put it, have lots of cross-site scripting problems since, as we know, now the only protection is the same-origin lockdown.  But as we've been discussing a lot recently, cross-site scripting allows an attacker to potentially inject their own script that runs as if it was coming from the vulnerable website, which would in this case bypass that somewhat weak protection afforded by this URL same-origin to anything.webex.com.



So, yeah.  Basically problem kind of solved.  There is a better solution for anyone who is concerned by this because, again, now we're trusting that there isn't a mistake anywhere, and I mean anywhere, on the WebEx.com site that anyone could find that would then allow this to be bypassed.  And now the whole world knows about this problem.  This was all responsibly disclosed.  It was fixed before it went public.  But still, now we know that there's a potential exploit if a way could be found to invoke this through WebEx.com.



So there is a better solution.  In Chrome, you can use user profiles.  So for users that, for example, need to have the Cisco WebEx extension available, they could uninstall it from their primary default profile, create another user profile, switch to that, install it there, and use that user profile when they're explicitly wanting access to the WebEx extension, which would essentially make it unavailable to their normal surfing around the 'Net and daily operations.  So anyway, I have a link in the show notes to a page that discusses this in greater detail.  There was another researcher who took much greater umbrage at this glitch and detailed exactly how to use the Chrome UI in order to solve the problem, what to do, and also how to use multiple profiles in order to sequester this WebEx extension for those who have to have it installed.



By the way, if it's installed, and you put it in two years ago because you thought you were going to need it and you don't, now, very much like Java, this would be a good time to remove it.  Because if you're not actually using it, as a general rule, anything that isn't in constant use does increase the attack surface.  And here's a perfect example of a big chunk of attack surface that was just discovered.  And in any event, verify that you do have v1.0.3.  And if you don't, then by all means update because the earlier versions don't provide even this weak level of protection.



And normally we don't talk about iOS updates.  I run around whenever they are coming out, update all of my various devices.  Yesterday's update was a little more important than usual because it contained a raft of WebKit vulnerability fixes.  So kernel problems, there were two of those, buffer overflows that would allow arbitrary code execution with kernel privileges.  But that would typically, for example, have to be from an app that got through Apple's scrutiny.  And certainly when they find a problem like this they go back and check for any apps they  might have missed because they didn't know there was something to look for.  So that normally doesn't really raise to the level of concern.



There was also a problem in libarchive such that unpacking a malicious crafted archive could lead to arbitrary code execution.  And once again, that's something that you want to fix.  But once fixed, it's probably not a big problem.  The WebKit vulnerabilities, of course, don't require or don't submit themselves to Apple's normal application submission scrutiny and even after-the-fact verification.  This is anybody with a version of iOS earlier than yesterday's release, which was 10.2.1, which is what everyone wants because we got 10.2 not long ago, but now we're at 10.2.1.



There were a whole bunch of potential problems found in WebKit, from a number of security people, sort of all over the place, but among them about half of them were from Google's Project Zero.  So again, props to Google.  They're succeeding in making us more safe.  So, for example, there was - and remember that Apple never is hugely forthcoming about this.  Their approach is to say, "Oh, this just improves security and fixes some things and adds some features, so please install it," without giving us lots of juicy detail.



But, for example, there was a maliciously crafted web content, able to exfiltrate data cross-origin, meaning that it was a breach of the same-origin policy for data exfiltration.  And we know how important it is that we not allow non-source origins to have access to anything outside of their own domain.  There was an arbitrary code execution problem as a consequence of a memory initialization issue, which has been addressed.  Another arbitrary code execution problem, another cross-origin problem, the ability for a malicious website to open pop-ups - which I guess that's not such a huge problem, but an annoyance if nothing else.  Because remember the days when pop-ups, as you and I, Leo, were talking at the beginning of the show about the quaint days 17 years ago of security vulnerabilities, or even 10 years ago.  And pop-ups used to be a problem until browsers said, okay, we just can't let anyone who runs a script to pop up junk on the screen.



So just wanted to say there were a bunch of things that potentially have significant impact.  So this is one that you want to act on sooner rather than later.  And as I've commented before, I proactively tell my various iOS devices to go get themselves updated.  It seems to take, I don't know if I'm at the end of a list somewhere, but I don't know why I would be because I've got so many of these things everywhere.  But I don't get actually a proactive alert of an update for about a week.  So I didn't want to let a week go by if other listeners are in the same category.



Okay, there was a huge amount of attention to a blog posting at HackerNoon.com.  And the title was "LastPass Does Not Encrypt Everything in Your Vault."  And it was like, oh, my god.  Well, now, the good news is it caused a huge furor because it matters.  LastPass matters.  It is the leading password manager in the industry now.  So if there's a problem, then a lot of people care.  I care.  It turns out, first of all, this is one of those things that happens from time to time that is somebody looking around rediscovers something that is already well known.  So if this sounds familiar, it's because we talked about it previously.  It's a feature.  It's not a bug.



And in fact, as I said, it's a well-known and previously well-examined feature.  Some metadata required for the use of bookmarklets and favicons must be accessible and are exchanged with the LastPass servers outside of the user's encrypted storage.  And it is specifically because LastPass cannot see into the users' encrypted storage that, if we want the features that - and I don't want to use the term "compromise" because our privacy is not being compromised except to the fact that metadata does represent some leakage between us and LastPass.  Not globally, because it is all over TLS encrypted communications.  So it's not something that anybody who's sniffing traffic at Starbucks can see.



But what this person discovered was some obfuscated URLs.  They were hex encoded just to allow them to be transmitted over an ASCII channel.  And they're required for, among other things, the history tracking feature which the user can disable if they don't want that tradeoff.  But for example, in the previous coverage there are other things that we know are not encrypted, and we've always known are not.  The email address that we use to log in is not encrypted.  They have to have that in order to send us notifications and email.  The IP address of logins to LastPass is not encrypted because, if we want features like the country restriction, which is very handy, I mean, like, I don't travel out of the U.S. almost ever, so I've got that turned on.  I don't want LastPass to even consider non-U.S. domestic logins to my LastPass account.



So that's strong.  I would argue that's a tradeoff that's easy to make, just to have my IP address not encrypted.  And the IP address of previous website logins.  That's what LastPass uses if we have the tracking history enabled, which allows us to monitor for our own purposes, our own monitoring, the IPs from which logins have happened to our account.  And under the LastPass Vault > Account Settings > Advanced Settings > Privacy, there is Track History, which you can turn off, if you would rather shut that whole process down. And then that information will not be provided to LastPass.  But on balance, it's the kind of thing that, if you're suddenly worried something strange might be going on, to have that history available, I think, again, that's a worthwhile tradeoff.



Okay.  So this is just a little quickie.  We were talking about - extensively, actually, in recent months - about the problems that Yahoo has had.  I mean, yes, massive in scale; but also very questionable in the handling of, or I was going to say the handling of the disclosure.  But it would be more appropriate to say the lack of disclosure.  Years go by, and no one is told publicly of a breach that they apparently know about, which is really not what you consider responsible behavior.



So the news this week is that the SEC is launching an investigation into Yahoo's improper handling of past cyber attacks.  And I noted in some of the - I was digging around to understand if there was anything more here.  There was some comment that most of Yahoo's assets were sold, of course, recently to Verizon.  So it's not clear what's left.  Verizon probably said, "We'll take all the good stuff.  You keep the problems with the SEC.  Those are yours."



LEO:  Yeah, I wonder who's responsible for breaches once a company's sold.  Certainly not the new owners.



STEVE:  No.  It's not, no, exactly.  So they just got the goodies and left sort of a damaged shell.



LEO:  A mess, yeah.



STEVE:  A damaged shell.  But what I did find interesting were a couple of little nuggets.  We, the nation, the U.S., still don't have strong uniform legislation in place.  And following that very widely publicized, and we certainly discussed it on the podcast, the big Target breach, there was some legislation drawn up called the Personal Data and Notification Act, which would have required companies to notify the public of data breaches within 30 days of discovery.  It was introduced in 2015, so about two years ago.  But it never made it through the House of Representatives subcommittee review.  So it never emerged as legislation that could be voted on.  And apparently there was frantic industry lobbying that caused the representatives on that subcommittee to say, "We have better things to do."



But it had some teeth.  Which is, again, why the lobbying was so heavy.  It would have carried - get this - a shockingly serious penalty of up to $1,000 per day per person affected.  And it would have superseded the 47 state laws, including the District of Columbia, which have varying standards of when notification must be made and what types of information breach qualifies.  So if I read this right, as long as a company notified within a month, within 30 days, they would be okay.  But, boy, if they took longer than that, at $1,000 per day per affected person...



LEO:  Oh, wow.



STEVE:  Yes.



LEO:  That's, let's see, a thousand, times half a billion, times, oh, what did it take them, three years?



STEVE:  Uh-huh.



LEO:  Hmm.  Maybe Verizon will cancel that acquisition.  That's quite a fine.  Holy cow.



STEVE:  So, never happened, but it does - I think it indicates, if nothing else, that there was some intent.  And I'm watching the actions of this new administration.  No coherent pattern yet.  No coherent philosophy seems to be emerging that I've seen yet.  But I'm interested to see, like, on things like this, where will the new administration come down?  So it'll be interesting to see.  Certainly, at the moment, having federalized this, and allowing all, you know, individual states to have their own rules is a problem for services like Yahoo, which are, well, I was going to say they're certainly countrywide, but they're also global.



So at some point we're going to need some better handling.  And I think, clearly, if there were this sort of statute in place, then we would not have had to wait two years to find out.  Yahoo!, as a consequence of the lack of any strong enforceable legislation, just chose not to tell us.  Well, if you were being charged $1,000 per day per person whose information was escaped from your database, and you knew it, then you'd be motivated to just say, by the way, we have suffered a breach.  We're still investigating it.  We'll let you know more.  You know, that'd be better than nothing for a couple years.



Okay.  Here's one that's not fluffy.  A Finnish web developer and hacker discovered, and I have verified, that several web browsers, which at least include Chrome, Safari, and Opera, but interestingly not Firefox - actually, in this case because Firefox is lagging a little bit in this feature, which now they're glad for - as well as some plugins and utilities such as LastPass, can be tricked into giving away, as in divulging, a user's personal information through their profile-based autofill, auto form fill systems.  And the attack is surprisingly simple.  Leo, if you go to the link in the show notes here, this GitHub.io link, it'll bring up a tiny little form.  You do not want to do this on camera.  That is, don't submit the form on camera.  But view the source because this is trivially simple to do.



So the attack is surprisingly simple.  When a user begins to fill in information in a web-based form containing just simple text boxes, such as their name and email address, the browser's autofill system, when enabled, which is intended to help you to avoid tedious repetition of standard information, such as your address and et cetera, but also including credit card  information and lots of other stuff, will populate other profile-based information into any of the other standard identifiable text boxes, based on the text box label.



LEO:  Ah.  So this has hidden fields.



STEVE:  Yes.  What the view source shows is that in the CSS - scroll down a bit - they set the offset to, I think...



LEO:  Fortunately, I don't have any browser-based information in there.  You see that all of that stuff is missing in mine.



STEVE:  Correct.



LEO:  So I'm wondering if it has to do with you allowing the browser to save that information.  Because I use LastPass for it.



STEVE:  Correct.  Exactly.  And so for me, Firefox doesn't have this autofill facility.  It's on the drawing board for them.  And hopefully now they'll be aware of this and will take some action.  So normally all of the form fields come up on the screen.  But with CSS you're able to set an absolute offset.  So all that page does is set the horizontal position, I think that's what it was, to -550 pixels, pushing them off the screen to the left so you don't see them.  But the browser doesn't check to see whether it's visible or not.  So you fill in the first one, which is your name, which is normally, you know, it'll give you a dropdown list, and you go, oh, yeah, Leo Laporte.  Then, without you seeing it, all these other fields are populated.



And it did it for me, that is, under Chrome.  I don't use Chrome for much.  But it had my full street address - street, city, state, country, domain, zip code, everything.  And if I were a Chrome user, and I had said, yeah, hold onto this other information for me, like credit card number, the CVV, the expiration date, all of that can be captured.  And it is a trivial attack.



Now, I should also note, the reason not only did I not want you to, if there was anything sensitive for users to see it because it just comes right up, but there is a site which this GitHub.io page uses to display this.  It's a benign site.  It's httpbin, B-I-N, dot org.  Which is a well-known, very cool, longstanding site which provides an array of tools which echo back browser query parameters.  So, for example, if you were doing HTML, and you wanted to see what the POST data was that your browser sent, you could post it to httpbin.org/post.  And all that does is it takes the information your browser sent and then sends it back to you in a nice JSON-formatted display, which is what you put up on the screen a second ago.  But the point is that it is https://httpbin.org.  But it is going to a third-party site.



So I did verify it works.  There's no reason not to trust these people.  But they're getting any information that your browser happens to fill the forms in with.  It would be nice if they had a non-obfuscated version, that is, without the -550 pixel shift, so that you could see for yourself what information was being filled in.  That ought to be convincing enough.  It's a little more dramatic as it is this way.  So maybe one of our listeners will do that because you could see the source.  Anyway, it would be trivial to do.  So if somebody does that and sends me a tweet or a link, I'll share it.



LEO:  But you could probably do it just in real-time, using the developer tools.  I bet you I can.  Let me just see.



STEVE:  Yeah.  I wonder if you could see the form populated without sending it somewhere.



LEO:  Well, I think I can change - can I change these numbers?  Yeah, I can change these numbers.



STEVE:  Oh, I see, you mean edit the page text, yes, yes, yes.



LEO:  On the fly.  Yeah, edit on the fly.  Not a difficult thing to do, it's just...



STEVE:  In order to bring them back.



LEO:  ...painstaking.  Because there's quite a few of them.  I'll do that.  You keep on talking.



STEVE:  Okay.  So, good, perfect.  So again, it worked for me in Chrome.  I didn't see anything scary because I've never given Chrome my credit card number.



LEO:  Well, I know that some of these fields have the designation "LastPass."  I wonder if it calls LastPass to pull that information, as well.  Or maybe that's just a choice on there.



STEVE:  It doesn't.  But LastPass is monitoring the page itself.  



LEO:  Ah.



STEVE:  So when it sees those form fields, it says, oh, I'm going to be helpful.



LEO:  So any page that has a form could do this to you.



STEVE:  Yes.  I mean, this is really big.



LEO:  It's horrible.



STEVE:  Yes.



LEO:  Wow.  All right.  I've modified those.  Let's see.  Oh, yeah, here you go.  Look at that.



STEVE:  Yup.



LEO:  Look at that.  So your phone, your organization, your address.  All I did was take these fields and move them onscreen.



STEVE:  Right.



LEO:  They were off to the left.



STEVE:  Literally right.



LEO:  Yeah, well, that moved them right.  Your address, your postal code, your city.  Here's the credit card, or the country, the credit card field.  Wow.  Wow, it's all there, baby.



STEVE:  Yeah.  And, I mean, so here's an example of another feature that was put there for convenience, but has been, well, should we say repurposed?  I mean, basically the developers didn't consider that they should provide security for invisible or in any way unseen fields because - so in practice, you would go to a site that simply wants your name.  You know, create - you're going to add a post to a blog.



LEO:  You see this, but you give them this.



STEVE:  Tell us your name.  Exactly.



LEO:  Wow.



STEVE:  Just trivially.



LEO:  That's horrible.



STEVE:  It really is.  It's shockingly simple.



LEO:  I guess you would go into the settings and turn off form fill.



STEVE:  Correct.  So users can protect themselves from this kind of phishing attack by disabling the autofill system within their browser or their extension settings.  So that's the way to do this.



LEO:  That's why it didn't work.  I have that disabled already. 



STEVE:  Ah.  Yes.  



LEO:  I must have been smart in earlier life.



STEVE:  Well, and when I was using Chrome, and I typed in, like, "S"...



LEO:  I'm sure it's on by default.



STEVE:  Yes.  But the next item in the dropdown list was autofill settings.  So Chrome was, like, presenting them to me, so it should be easy for a user to decide, I don't think I need autofill until we get this -



LEO:  Well, and then you can also go deeper into it and see what information Chrome has.  I have autofill turned off.  But it's been saving stuff.  And actually this is all pretty old stuff.  So, you know what, Chrome the first time says, do you want me to remember forms so I can fill these in?  And I'm sure what I said is no.  But I didn't say it.  So I'm going to delete all of these addresses so it doesn't by accident - or maybe just put in innocuous stuff; right?  Don't have any credit card info on there, which is good.



STEVE:  Yeah, that was, I mean, I was like, oh, wow.



LEO:  Oh, wait a minute, I do have credit card info.  Holy cow.  Holy moly.  Okay, we should probably take all that stuff out; right?  Holy moly.  Oy oy oy.  Okay.  Golly.  So turn off autofill, and maybe as a second level of defense remove all that form information, if it's stored there. 



STEVE:  Yeah, yeah.  So later this year we have an upgrade coming to Tor's so-called "dark web" technology.  That's what the popular press calls it.  We know of it as Tor Hidden Services.  This change, which is forthcoming, although it's going to require an upheaval, as major changes do, to the existing system, and probably will be able to be incrementally adopted, will require an upgrade to Services' servers.



So just to step back a little bit.  As we know, the way that - the Internet was never designed for secrecy or privacy.  It was designed for robustness.  It was designed to work against all odds.  And that it achieves brilliantly and with amazing scalability over time.  The fact that we're still using the Internet that we showed in that diagram that had five machines on it, and now it's got - now we no longer have enough IPv4 at 4.3 billion endpoints, IP address endpoints, and it still works.  Same system that started off in the beginning because they just did that part right.



What they know they didn't do right, but they weren't trying to because it was a miracle that it worked at all, and as we've often said, is privacy and secrecy.  It wasn't intended for that.  And so it's always been having a problem with that since.  We know that we can make our data private by encrypting it and being sure about who we're encrypting it to.  But the fact of the data moving across the Internet is still in plain sight.  And we can almost consider this sort of a metadata, that is, okay, there went a packet from point A to point B.  Everyone can see that happen.  Even though they can't see what's in the packet, the fact of the packet, the metadata of its existence, is known.



So Tor was a system designed to robustly thwart, I won't say "prevent" because, as we know, you can't prevent what's impossible to prevent.  But it really upped the ante.  The idea with Tor was you would have a large collection of Tor nodes.  And Tor, T-O-R, started off as being an abbreviation for The Onion Router, you know, T-O-R, The Onion Router, the idea being that there would be a subset of the Internet that was participating in this Tor blob.  And traffic that would go into all kinds of entry points in this big blob and be untrackable and undecryptable as it hopped around through the blob.  And then it would come out of some other random place.  And so even though it ended up emerging in public, if a whole bunch of traffic went into this black box, and a whole lot of other traffic came out of different points and different times, it would be very difficult to determine which went in, which specific packet went in, and when and where it came out, thus creating a big mystery in the middle of this otherwise public routing system.



Now, of course, there have been lots of attacks of that because, if you bring sufficient resources to bear, as I said, since it's not actually possible to do that perfectly, then there are ways to defeat it.  Another service, though, was added.  So note that what Tor originally did was create a black box in the middle of the Internet so that things went in, and they came out.  But the point was they were coming out and then going to their destination public service.  So the fact that you were connecting to that service was - it did a good job of obscuring that connection and your interaction.  Good, but not perfect.  But the service itself was still public.



So later Tor added this concept of hidden services so that the services themselves, like the websites, could exist in the black box, could be part of this Tor mystery, so that then your traffic goes in, it still bounces around to scramble anybody who's trying to track it, but then it ends up at a service which is not public, that is, it is part of the black box.  So that further increased the security guarantees that Tor was providing.



The problem was that, again, because the Internet wasn't designed for this, and arguably the original first-generation hidden services, Tor Hidden Services system didn't do everything it possibly could.  It turns out that at last year's Black Hat a paper was published that demonstrated there were, of about 300 directory servers, which is sort of the equivalent of DNS for Tor's Hidden Services.  These are hidden directory servers.  Of the 3,000 that exist, a little over 100 of them were found to be suspicious, that is, they appeared to be using technology to unmask the existence of hidden services, which the system was explicitly designed to protect.  So there have been famous hidden services, and they've got funky-looking domains.  They all end in dot onion.



The WikiLeaks anonymous upload system is at the well-known Tor Hidden Services dot onion domain of "wl," as in WikiLeaks, "upld" as in upload, and then "3ptjvsgwqw," which is just gibberish, in order to make the entire domain name longer, 16 characters.  Silk Road, that we talked about back in its day, was "silkroadvb5piz3r.onion."  So that's what onion addresses look like.  And Tor users were able to use those within the Tor network to get to a server where their traffic never emerged back out onto the public 'Net in the same fashion.



So it turns out the idea, the original concept was only if you knew that crazy, wacky, unobvious, difficult-to-discover domain name, or kind of URL, 16 characters, maybe with something on the beginning, but then with gibberish attached to it, dot onion, only if you knew that could you find the services because they were supposed to be hidden.  Well, it turns out that, even without knowing a hidden services address, it has been possible for hackers, law enforcement, security firms, snoops, and others to discover the existence of these services independently.



So the developers involved with Tor don't like that.  They have said the only people who should know about your hidden service are the people you tell about it.  And while that's a pretty simple concept, they admit, it's currently not true.  So the next generation of hidden services will use a new method to protect the secrecy of those addresses.  Rather than declaring their dot onion address to a hidden service directory within the Tor system, next-generation hidden services will instead derive a cryptographic key from the known dot onion address.  That is, they know their dot onion address, but that's the secret they will keep.



They then use that to produce a cryptographic key, and that key will be placed in the next-generation hidden services directories, rather than the dot onion address itself.  What that means is that any Tor user who in advance knows the name of the hidden service they want, can perform the same derivation to check the key and then route themselves to the correct hidden service.  So essentially it's creating another level of cryptographic indirection which prevents the kind of discovery that researchers now know has been going on.  Since the hidden service directories cannot derive the dot onion address from the keys which they hold, only those who know the hidden services keys can discover the hidden services address.



So the original Tor project cofounder, Nick Mathewson, was quoted saying:  "The Tor network is not going to give you any way to learn about an onion address you don't already know."  And then, finally, this next generation of hidden services will also be switching from using 1,024-bit RSA encryption keys to shorter, but tougher-to-crack, and of course my favorite, the ED25519 elliptic curve, the behavior of which is what enabled the whole SQRL concept in the beginning.  And of course we're seeing this thing really taking over the industry in the last few years because it offers so many benefits.



So these changes, of course, mean that the hidden service URLs will change, too.  They're expanded from 16 characters, as they are currently, to 50, five zero.  But Nick argues that that change doesn't affect the dark web addresses' usability since 16-character gibberish is already too long to memorize.  So they will be 50 characters - substantially longer, but substantially more secure - and use a technology that prevents, essentially, any form of discovery of the existence of these hidden services.



And the plan is that - oh, and in fact I think it was Nick himself who maintains an open family wiki and an open family calendar as a Tor hidden service.  And all of the members of his family know the secret address and are able to easily and freely publicly share it.  There's no password on that server because only members of his family are able to get to it.  So an interesting idea.  It'll be fun to see this happen.  And I'm sure we'll be talking about this later in the year as this thing gets deployed.



Okay.  Now, there isn't much to say in detail, but just I was caught off guard by the fact that such a thing exists.  China has actually put into orbit, it exists, it's called the Mozi, I think it is, M-O-Z-I.  It is an operating proof-of-concept quantum satellite, which is now operational.  So quantum entanglement is interesting, and I'm not going to get into the details because I'm not qualified.  I'm not a quantum physicist.  And there's more that I could say.  But the concept is it is possible to produce a pair of photons.  And it actually applies to things other than photons, as well.  But photons are easy because they're particles of light.  So you can produce a pair of photons which are entangled, as the term is, at the quantum level.



And, for example, if a single photon had a property that was equally likely to be true or not, and another photon had a property, the same property, equally probable to be true or not, then taken together there are four possible combinations for that property of two photons; right?  They could both not have it.  They could both have it.  One could have it; the other wouldn't.  Or the other could have it, and the other wouldn't.  Four possibilities.  Well, normally, if they're not entangled, we would expect those properties - and the properties individually had a 50/50 probability - then the combination of them would have 25% probability, all equal one quarter probability.  When you entangle them, what happens is you disturb that equal distribution of probabilities such that they are interconnected, and now there's 0% probability of two of the four shared states, and 50/50 of the other.



Okay.  So what this means is - and there was an experiment done a few years ago that was mindboggling, where two entangled photons were sent in opposite directions and polarized.  And at some time later, that is, downstream, the polarization of one of them was rotated, and the polarization of the other entangled photon spontaneously changed.  And here's where it gets really spooky.  Not at light speed.  Instantaneously.  So there was some spooky connection between them that existed, that was created and then demonstrated.



So what China has in orbit now, and I guess experiments will start, is they have put into orbit a photon entanglement device which, if this is credible, is supposed to be beaming down to the Earth, to specific points, pre-entangled photons such that the recipients at distant locations will be receiving photons that in principle allow FTL, that is, Faster Than Light, transmission of information by, just as was done with this tweak the polarization of one photon and the other one responds instantly.  Although lots of problems to be solved.  Bandwidth is probably low.  You're not going to have a video conference over this thing anytime soon.  I mean, we're still way in the weeds.  But just the idea that this, like there's something up there now that's going to be sending entangled photons down to multiple locations, that's mindboggling.  So I had to share that.



Okay.  It's been nearly three years.  It was April of 2014 that the big news of Heartbleed hit.  And we talked about it a lot.  To remind people, this was a bug found in the highly popular OpenSSL library, which was found through code inspection, and there was, I mean, so it was clearly a bug.  The question was, was it exploitable?  And for quite a while people thought, eh, well, okay, it's bad.  We're going to fix it.  But we don't really think you could actually do anything with it.



So a number of organizations, and CloudFlare, I think it was CloudFlare was one of the most notable, put up a test, a contest, said here is a Heartbleed-vulnerable server on a domain we just created.  Can you get its private key?  And of course we know that's the keys to the kingdom.  The private key is what you never want to allow anyone to exfiltrate from your server because that then allows them to defeat the guarantees that the certificate that the private key was made from, or that the certificate was made from the private key, from being honored.  So as we know, yes, it turns out there was a low statistical probability.  This essentially allowed an exfiltration of large blocks of the server's private memory.



So the question was, okay, that's not good.  But is there anything in that private memory that can be accessed by this bug that we care about?  And sure enough, keys, cookies, passwords, all kinds of debris, but some that was valuable, and debris you didn't want, you never intended to have escape was exfiltratable.  So many people, I mean, this came to everyone's attention.  The EFF and Bruce Schneier all considered Heartbleed to be a catastrophic flaw.  We were all shocked that it had been around for so long.  And there was an immediate move to fix this. 



The curve, however, of rate of fixing has not been kind.  About a month and a half later, toward the end of May of 2014, we went down from around half a million of the Internet's secure servers at the time of the first revelation, about six weeks later we were down to 1.5% of the 800,000 most popular secure, TLS-enabled, secured websites still vulnerable to Heartbleed.  Shodan published a report, I think it was yesterday, that showed that here we are today, nearly three years later.  If this report is correct, and I'm a little skeptical because the numbers are still so horrifying, this shows Shodan identified at least 199,500 websites still vulnerable.  Most are in the U.S.A.  I've got a link in the show notes.



In fact, I can say it, if anyone's curious:  www.shodan.io/report/ and then all - no, I was to say "all caps," but it's not - DCPO7BkV.  That's their report.  Really interesting graphics.  It's very easy to scan and look at.  But I think it was 43,000 of this just shy of 200,000 servers are in the U.S.  I think the second most prevalent was in South Korea.  The largest holder of vulnerable servers, 6,376, was SK Broadband, South Korea Broadband.  But what troubles me, and the reason I'm a little skeptical, is that this shows Amazon.com in the number two position with their AmazonAWS.com.



LEO:  So lots of people host on AWS.  So it's not - well, maybe not because it's AWS.com.  I don't know.



STEVE:  Yeah, see, so it's Amazon servers.



LEO:  Yeah, but we're on Amazon servers.  I mean, a lot of people are on Amazon servers.



STEVE:  Okay.  So what I'm saying is what they may be doing is working, for example, from version number.  They have to be using some sort of a priori information from the server because the nature of Shodan, I'm sorry, the nature of Heartbleed...



LEO:  Well, they reverse DNS, so - right?



STEVE:  No.  The nature of Heartbleed doesn't allow you to easily determine the actual presence of the vulnerability.  So it's probably based on the version that the site is publishing of OpenSSL.  And we also know that Amazon AWS, because we talked about this before, they've already spawned their own SSL or TLS stack because OpenSSL had so many features they weren't using.  So my strong suspicion is that this is not an accurate report.  It got a lot of press coverage about 200,000 websites still vulnerable.  I doubt any of Amazon's vulnerable, despite the fact that this says 5,163 are.  It almost certainly is the case that Shodan is just doing a scan of SSL or TLS version numbering, like what is the library behind it.  And Amazon probably has their own independent versioning that happens to collide with what OpenSSL was.  At least that's my guess.  Again, I could be wrong.



If this is true, this is horrifying.  But it does sort of, I mean, certainly we know from all our experience that there will still be problems, that something like Heartbleed never completely goes away.  The servers in the closet aren't going to be updated.  On the other hand, the server in the closet probably has low value.  And it's still, even in the case of a high-value target, it was very difficult, but not impossible, we found, to exfiltrate data using the Heartbleed vulnerability, which is now three years old.



So, okay.  Here's a couple fun things.  A Minnesota court on the Fifth Amendment and compelling fingerprints to unlock a phone.  We've been having fun talking about biometric or not.  You can't be told to divulge a password that you have memorized because that's clearly testimony.  But a fingerprint, it's felt, is not testimonial.  And so as we know, courts have recently been using the argument that forcing a person to use their finger to unlock a phone is not afforded Fifth Amendment protection against self-incrimination because using a finger does not qualify as being testimonial.



So a recent case, the State v. Diamond, who was the defendant, was decided just yesterday by this court in Minnesota.  Actually it went to appeal.  In the original case, the defendant Diamond initially refused to comply.  He said, no, I'm not going to let you make me use my finger because I don't want to.  The judge...



LEO:  No, you don't have to.



STEVE:  Exactly.  The judge explained that using his finger was nontestimonial, so he could not plead the Fifth, and he would be held in contempt of court, probably means put in a cell until he changed his mind, if he refused.  So he complied.  But then we presume he got a fancier attorney, who came up with another angle, and they appealed that decision.  On appeal, Diamond argued that the government had violated his Fifth Amendment rights because the government made Diamond select which finger to use.



LEO:  Oh, wow.  That's not going to - that's interesting.



STEVE:  Specifically, Diamond argued...



LEO:  So he's testifying by choosing the right finger.



STEVE:  Correct.  He had some information that they didn't have.



LEO:  That's really good.  I like it.



STEVE:  Isn't that great?  I love that.  He said, quote, he "was required to identify for the police which of his fingerprints would open the phone," and that, quote, "this requirement compelled a testimonial communication."  So everybody, just keep that in mind.  I thought that was very clever.  It's like, okay.  Yes, I do have fingerprints.  But I don't want to tell you which one.  And so we're back to something you know.



LEO:  Interesting.



STEVE:  Very cool.  Okay.  In March we have the CanSecWest in Vancouver 2017, which is the 17th annual security conference.  And part of that is the Pwn2Own competition which is now going to have its 10th year.  And, boy, a lot has changed in 10 years.  That first year, and we've been covering it because it's always fun and interesting, if nothing else to see how quickly and sadly our browsers that we wish were more secure or, well, we wish they were perfectly secure, how quickly they're just cut through.  The first year the prizes were a laptop and $10,000.  Whoo, 10 years ago; right?  Last year more than $450,000 in cash and prizes were awarded over multiple categories.  So a lot of evolution in nine years.



And very much as we were talking about software and security seven years ago when, "Oh, look, an email macro, how quaint," 10 years ago, for example, a single bug was used, or a vulnerability was used to exploit QuickTime.  Last year a significant chain of interconnected vulnerabilities was required to complete a compromise and fully win a category.  So everything's been getting more secure, but we've got a lot more people looking at it.  And with increasing prize awards and prestige comes a lot more attention.



So this year we now know the so-called Zero Day Initiative, who are the people who put on the Pwn2Own and organize this, this year the Zero Day Initiative will be offering more than $1 million, spread across five categories:  Virtual Machine Escape, obviously from guest to host; Web Browser and Plugins; Local Escalation of Privilege; Enterprise Applications; and Server Side.  And the first two are the most interesting.  So in VM Escapes, Virtual Machine Escapes, in their laying out the rules they explained that VM Escapes were first added last year with VMware.  And remember that we've talked about some fun ones, like I think it was the long-unused floppy disk driver, the virtual floppy driver was sitting there in VMware, and turns out it wasn't secure.  And so it was possible to leverage something you absolutely didn't even need.  Again, minimize attack surface wherever possible.  But it was there, and that was a way to get out of, to break out of the VM containment and get down into the containing hypervisor because that's where the actual driver was residing.  So if you could arrange to execute code in it, you broke out of a virtual machine containment.



This year, in addition to VMware, it's being expanded to include Microsoft's Hyper-V.  An attempt in this category must be launched from within the guest OS from a non-admin account and execute arbitrary code on the host, that is, the underlying operating system.  Both the guest and the host operating system will be running the 64-bit versions of Windows 10.  A successful exploit in either of those VM products, VMware or Microsoft's Hyper-V, will net $100,000 for the contestant, plus what they described as a "lucky 13 Master of Pwn points."  They have another thing in addition to dollars.  You get these Pwn points.



The second category was Web Browser and Plugins Exploits.  And a demonstrated exploit in Microsoft Edge gets you $80,000 and 10 Master of Pwn points; an exploit of Google Chrome, the same $80,000 and 10 Master of Pwn points; Apple Safari, $50,000 and 8 Master of Pwn points; Adobe Flash in Microsoft Edge, the same, $50,000, 8 Master of Pwn points; and Mozilla's Firefox, $30,000 and 5 Master of Pwn Points.  And this is interesting because, as we know, the award is regarded as proportional to the difficulty.  So here's Microsoft Edge and Google Chrome at parity as the two highest awards at $80,000 and, sadly, Mozilla Firefox at the other end at $30,000 because it's regarded as today not as difficult to find a problem with as Edge and Chrome.  And Safari is stuck there in the middle at $50,000, along with Flash in Edge, also at $50,000.



And then in addition they say:  "Moreover, contestants may earn an additional $30,000 if their entry achieves system-level code execution on Windows-based targets, or will receive an additional $20,000 if their entry achieves root-level code execution on macOS-based targets.  And the Windows-based targets will be running in a VMware Workstation virtual machine.  If the contestant escapes the browser or plugin AND the containing VMware Workstation virtual machine to achieve code execution on the host operating system, the contestant will receive an additional $100,000."  So this is going to be fun to see - some serious money and lots of motivation.



LEO:  Can you use the amount offered as a rough gauge of how easy to hack these platforms are?



STEVE:  Yes.



LEO:  So the more you get, like if you hack Apache, wow, you're good because that's the biggest prize.  Or Hypervisor or...



STEVE:  Well, if you hack Edge and Chrome at $80,000 each, those are regarded as the two hardest to hack.  And unfortunately, Firefox is regarded as the least difficult to hack.  That only gets you $30,000.



LEO:  Right.



STEVE:  Yeah, so that's interesting.  And I think we're going to have a fun competition.  I think it's in March.



A couple little bits of miscellany.  Oh, I got a request or a tweet from a Pete Stringer, who asked if I was able to put details of the router that we've been talking about so much in GRC's Linkfarm, and I did.  It's there now.  And that is, of course, the Ubiquiti EdgeRouter X.  So remember the Linkfarm, GRC.com/linkfarm.  That's become a repository of these sorts of high-value things.  Somebody also, I saw a tweet go by, I don't think I responded to him, what was the site that contained that broad spectrum of security information.  And that was PrivacyTools.io.  But, for example, it, too, is there on the Linkfarm.  So if there's something that's like one of our core things that this podcast gets excited about, check GRC.com/linkfarm, and you may be able to find a direct connection to it.



And speaking of our cartoon at the top of the show, I did want to note the news, for those who hadn't already heard, that the Amazon - I guess I have to say "Echo"; right?  Is that still okay?  I won't say the "A" word.



LEO:  Yeah.  Now you can say "Computer."



STEVE:  Yeah, well, yeah, but now that's a problem.  I tend to use the term "machine," so we're not in danger.  But it turns out that, the point is, they have added to - so now there is the "E," you can wake up the Amazon Echo device with that word, the "E" word; the "A" word, by Amazon's own name, Amazon; and Computer.  So for those of us who appreciate Scotty talking to the mouse of the Macintosh on "The Voyage Home," "Computer" - and people are looking at him like, okay, is this guy nuts - we can now wake up our Amazon Echo by addressing it by "Computer."  Apparently that's - and of course that applies to Dots, as well.  The Dot is v48.12.  And I don't know if the Dot and the Echo are synchronized because I just saw some postings about that.  So if anyone's interested, that works.



And then, in a related note, someone tweeted me, and I think he wasn't aware of previous dialogue about this.  His Twitter handle is Brad Online, or his name Brad Online.  He said:  "@SGgrc, I've noticed Amazon ads for the Echo do not trigger the device."



LEO:  Yeah.  They used to, but of late they don't.



STEVE:  Yes.  It turns out - and so he said:  "I hypothesize there is a tone that plays to kill the response."  Well, as it happens, I ran across some dialogue that appeared to be authentic.  I haven't verified it myself.  But there is, there appears to be, a deliberate notch at 5kHz in the spectrum of those commercials.  And so it used to be a big problem.  It caused lots of ruckus.  I looked back in doing some googling, there was, like, tons of stories about how the early Echoes were being fired off by the Amazon commercials all the time.  Amazon responded.  And I think what they did is they, in a firmware upgrade, I mean, the firmware is having to do a spectrum analysis anyway.



The way you do this kind of speech recognition is you chop the spectrum up into pieces and analyze the varying power contained in different little slices of band within the spectrum.  And so it would have been trivial for them to remove a small notch, which would not affect the audio we hear at all, but it would stand out glaringly in a spectral analysis.  And there are people online who have done this and found a missing chunk of audio spectrum in the commercials for this device that allow it to speak the words and not have the device respond.  So just a cool little bit of trivia.



And believe it or not, I found yet another interesting application for SpinRite as a consequence of somebody who listened to last week's podcast.  He didn't give me his name, and he sent it as "SM," but looks like his first name is Sergey because that was in his email.  And he's in Lake Stevens, Washington.  He was wondering about cabling errors, which is what we discussed last week, or recently.  He said:  "Hey Steve, I just finished" - oh, yeah, it wasn't, it was recent, it was 593.  "I just finished Episode 593, and I think you're wrong on cabling errors.  I'm a listener since Episode 1 and a SpinRite user.  I played with SpinRite v6 a lot.  I even" - get this.  "I even bought a large batch of 'completely dead' drives from eBay, and I was able to use SpinRite to restore most of them to full error-free operation."  So that had never occurred to me that eBay would sell a big box of dead drives, and you could use SpinRite's magic to bring some subset of them back to life.  They probably didn't cost very much, if they were dead.  But now they're alive.  Then you just stick them in your Drobo, and off you go.



Anyway, he says:  "But while SpinRiting a few of the" - oh, he got 36 drives - "a few of the 36 drives I was getting cabling errors.  So I would swap cable and try again.  I even bought two different brands of SATA cables so I could use them for swapping.  And despite everything I could, a couple of those drives were still showing cabling errors.  So it looks like it's not always a cable's fault for cabling errors.  What do you think?  I think it must be something inside the drive.  Please explain if you can.  Can't wait till the next episode.  Thanks."



Okay.  So, correct.  I called it "cabling errors" because "cyclic redundancy check" wouldn't have much actual meaning for the audience that I've aimed SpinRite at, which is "I want you to fix my drive, please."  And I'd say, "Yes, just press this button," and off it goes.  So almost always, but almost always, the cause for a CRC, a communications error, is the cable.  But it doesn't have to be.  And if you've got a really dead drive that lightning struck once, and there's a big welt in the cover, and black soot and a little crater, and you still think that's a good place to store your data, and you're getting cabling errors, well, yes, it was probably the lightning strike that didn't do the drive any good.



The point is something is causing there to be a fault in the data transmission from between the endpoints.  It's going to almost always be the cable or the connection.  So you just pull the cable off and reseat it, plug it back in again, problem solved, 99.999% of the time.  But it doesn't have to be that.  It could certainly be that something just killed some aspect of the drive, that maybe a bit is stuck.  So when that bits needs to be other than in its stuck state somewhere, it gets an error.  So I did come up with something that was much more meaningful than saying that there was a cyclic redundancy check problem, which is mostly accurate.  But there are certainly other possible causes for receiving that.



And, finally, a couple tricks for password complexity.  This was a heavily tweeted and emailed in the mailbag question.  I just found two representative tweets.  James Brooks tweeted:  "If I use a pseudorandom word generator to string together five-plus words for a passphrase, is it still weak?"  And he's referring to Episode 594.  Ben Maughan said:  "Love SN, listened for years.  Character-based passwords just use a different dictionary from word-based passwords.  Words can be as good."  And then in a follow-up tweet he said:  "Actually, Oxford dictionary estimate about 100,000 nouns, so a random five-noun password is better than 12 random characters, I think."  So I thought, okay.



LEO:  Wow, boy, that just doesn't sound right.



STEVE:  I know.  And so...



LEO:  I think some of this is coming from the website, what is the name of it, for these passwords?



STEVE:  Diceware?



LEO:  Diceware.



STEVE:  Yup.



LEO:  So I'm really glad you're going to address this because it doesn't - but I want to hear what you say.



STEVE:  Okay.  So here's what we want to do.  We want to understand, first of all, we agree that the individual items of the dictionary - whether they're characters and the dictionary is the alphabet, or they're words and the dictionary is an actual dictionary - that they are chosen with equal probability and no association, that is, remember, if you start making a phrase, if you start making a sentence, now they're no longer independent.  And as I did say a lot last week, it is crucial that there not be linkage between them.



So anything, I mean, even one of many people's favorite idea is to use the first character of a well-known phrase.  Well, the problem is well-known may be well-known not just to you.  And so it's certainly possible that a block of things to test are the first letters that people tend to choose, of phrases that people tend to choose.  And we know that there's some weird astral connection between people because too many people chose "monkey."  Like, why "monkey"?  So there's never been an adequate explanation for the fact that "monkey" was number five on the password lists for so long.  Just it's bizarre.



So, okay.  So the choice has to be, of the individual items, has to be random.  So the question is, how many, or what's the means or the approach for determining how many possibilities there are?  Well, we know that, if you take the number of words, like 100,000, and there's only one of them, then there's a one in 100,000 chance of guessing it.  If there's two of those 100,000 nouns, now it could be any 100,000 for the first one; and, because they're not interlinked, there's no reduction in entropy for the second.  So it's another 100,000.  So you multiply those two 100,000s.



And if you have a third noun, now, again - and again, no linkage between them so the third one is every bit as likely as the earlier ones, now that we have all of those two 100,000s multiplied another 100,000 times.  So again, we multiply.  In other words, it is the total number of combinations is the dictionary size raised to the power of the number of objects - the number of nouns, the number of characters, and whatever.  So, for example, in the case of an alphabet, where our dictionary is the set of available typeable characters, we know that there are 95 of those - lowercase alpha, uppercase alpha, the digits zero through nine, and then all, I think there's 33 special characters that can also be typed.  That sums to 95.  So there you've got 95 times 95 times 95, that is to say, 95 to the power of however many characters in your string.



But there are a couple simple math tricks that can be used because, for example, we'd like to know what is the - because we think in terms of 128-bit encryption key, or 96-bit, or 64-bit, or 256-bit.  The term "bit" is a binary digit.  That's what "bit" stands for.  And so how many binary digits or bits is a password made up of objects from a dictionary of a certain length equal to?  Well, it turns out there is a cool trick that involves logarithms.  As I mentioned at the top of the show, a slide rule has a logarithmic scale.  And the way the slide rule works is you are adding two distances to get the sum of the distances.  But you're adding them based on the scale, which is logarithmic.



And it turns out that an interesting property of logs, I mean, THE interesting property of logs is that the logarithm of a quantity A, added to the logarithm of a quantity B, is equal to the logarithm of their product.  And that's the secret of a slide rule.  That's how you can linearly add two things, two quantities that you find on the scale.  And in doing that you are summing the logs of each, and the result is the product of them.  Which is how addition gets turned into multiplication.  And so you can multiply and divide.  Division is just the reverse.  It's subtracting logs.  So there's that.



But the other cool trick is that you can change the base of a logarithm by dividing by the log of the base you want to change to.  I'll give an example.  So say that we have our alphabet of 95 typeable characters.  How many bits is one of those characters equal to?  Using any calculator, and it doesn't matter whether you use the natural log, which is often LN on a calculator, or the log base 10, which is typically LOG.  You have to be consistent, always use the same one.  But if you take the log of the size of the alphabet, 95, divided by the log of 2, what you get, just instantly, is the equivalent number of binary bits required to enumerate the items in that alphabet, that is, those 95 items.



So, for example, in this case the natural log of 95 is 4.554.  The natural log of 2 is 0.6931.  When you divide 4.554 by 0.6931, you get 6.57.  That's the exact number of binary bits represented by an alphabet of 95 characters.  So if you have 12 of those, you simply take 6.57 times 12.  And that gives you the equivalent bit strength, in the terms we're often talking about, of 12 characters, where each character can be any one of 95.  And you could always take 95 to the power of how many characters.  You could do that.  Or notice what I did was I took the log of the size of the alphabet over log of 2 to get the equivalent in bits.



Well, you could take the log of the size of the alphabet divided by the log of 10, and what you get is the equivalent number of decimal digits.  So you don't even have to do a raise it to the power.  You simply divide by log of 10, rather than log of 2, and it converts it, essentially, to an alphabet of decimal size or, in the case of log of 2, an alphabet of bit size to get the equivalent.  So what this simple little trick allows you to do is to easily, yourself, just with any calculator, and of course if you didn't have a physical calculator or one in your phone, of course I love 42 PCalc on iOS.  And do you know if it's available on Android, Leo?



LEO:  No, it's not.  It's iOS only, yeah.



STEVE:  Oh, shoot.  Yeah, it's...



LEO:  But there are good calculators on Android, as well.



STEVE:  Yeah, yeah, exactly.  And you can certainly find an online calculator.  If anyone is curious, anyway, that's how this works.  So you would take 100,000 and take the log of that and then divide that by the log of 2.  That gives you how many bits per noun.  Then you decide how many nouns you need.  But your intuition is right, Leo.  People think that 100,000 is a lot.  But the problem is, as anyone who plays with this math will now discover, to get an equivalent strength from words, compared to high-entropy large alphabet character set, requires just too many words because there isn't that much entropy per word.



But again, just dividing by the log of 2, dividing the log of the size of the alphabet by log of 2, that'll tell you - and I've been talking about combinations.  But that's entropy.  It's the number of equivalent bits of entropy, given that you have a well-chosen, equally probable choice of objects from your set.  That gives you the measurable entropy of a single one of those.  And so if you have 10 of them, you just multiply that by 10 because entropy scales linearly that way.  And so people can play with this themselves and gauge whether they think words are as good as they would like them to be.  My passwords are all relatively short, but high character set in terms of values per location, no intercharacter coupling, no tricks, just obtained randomly.  And of course the problem is people say, oh, well, yeah, but I could use lots of words.  Except then you hit a website that says, oh, sorry, you're limited to 20 characters.  Well, now you can't use 15 words and 200 characters.



LEO:  So Wolfram Alpha has a password strength calculator.  So you can just go there and do it.



STEVE:  Does it actually do the math?  Does it show you what the...



LEO:  I presume it does.  Let me see.  What is it, horsestaplemonkey.  I don't know.  Let me just - I'll show you, yeah, I mean, all of Wolfram Alpha's stuff does that kind of stuff.  Let me see if I can...



STEVE:  I don't think that's really what we want.  Or, I mean, that could be useful and valuable also.  But what we really want is just a simple logarithm, just to be able to determine, to know what it is that we're getting.



LEO:  Yeah.  I don't know how they calculate it.  I'm sure they'll say.  And I presume that they're smart enough to do the right thing.  I don't know.  I don't know what the - "very weak."  I would guess that they're using something similar to that, your system.  But anyway, there's another way to do it.



STEVE:  Yes.  There are many.



LEO:  I like logs.  I like logs.



STEVE:  Oh, they're just fabulous.



LEO:  So I shouldn't believe the FAQ at Diceware or - because they imply that this stuff is very, very secure; right?  They're just underestimating the computational power of an enemy.



STEVE:  Well, and I think the problem is practicality.  We do know that we are often hit with maximum password size limitations.  So if you're limited in the size of the password you can use, there isn't room to have as many, I mean, words are low entropy.  No question about it.  You could say "gopher," and everybody knows from that little bit of sounds what those characters are to make up the word "gopher."  So the problem is individually they're low entropy.  So the only way to make up for that is to use a lot of them.  Well, but if the website you're trying to use it at restricts you in total length, you don't have a choice.  You cannot use low entropy if you don't have much size.  Which is why I just go for maximum entropy, smallest size.  And this little trick allows you to determine how much entropy is in different things that you want to try.  And so that's just what I wanted to share.



LEO:  Good.  And that wraps this sucker up; right?



STEVE:  Yes.



LEO:  Nice and tidy with a bow.  We do this show every Tuesday, just so you know.  And if you want to watch us do it, you can because we stream it live on YouTube, YouTube.com/twit; and on our website, TWiT.tv/live; and, you know, all those places.  So you can watch it live.  The time to tune in would be roughly 1:30 Pacific, 4:30 Eastern, 21:30 UTC, thereabouts, on Tuesday afternoons.  Of course you can always get it on demand and have it at your leisure, whenever you wish, from the website TWiT.tv or from Steve's website, where he puts it all up on there, along with transcripts, too, at GRC.com.  You can also go there to get SpinRite, the world's best hard drive maintenance and recovery utility, Steve's bread and butter, and lots of stuff like passwords.  Steve's password generator is awesome and truly random. 



STEVE:  Yes.



LEO:  That's the key.  We want to remind you we're doing a survey of our audience.  We'd like to hear from you so we can give you more of what you want, less of what you don't want.  And, yes, our advertisers are curious.  Are you men?  Women?  Aliens?  Who's listening?  It's very short this year.  We decided to make it as easy as possible, just a few basic questions at TWiT.tv/survey.



If the question doesn't apply to you, or you don't find an answer in the options that you like, just skip it.  That's fine.  TWiT.tv - people say, "Well, I can't answer this."  One guy said, "You asked me what I do for a living, but I'm retired.  That's not an option."  Okay, just skip it, then.  That's fine.  You don't have to answer every one of them.  TWiT.tv/survey.



I think we ask - I guess "retired" doesn't show up.  I don't know.  I didn't design it, so I'm not going to speculate on why we ask the questions we do.  But I think that it was questions that we need to know, primarily for advertising, I would guess.  And certainly I use it for programming.  I think that's of use.  But mostly, if you just listen to the show, then I'll know you like it.  That's really all it requires.  Thank you, Steve.



STEVE:  Okay, my friend.



LEO:  Hope you feel better, and we'll see you next week.



STEVE:  Perfect.  Will do.  Talk to you then, buddy.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#597

DATE:		January 31, 2017

TITLE:		Traitors in Our Midst

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-597.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  This week, Leo and I discuss the best "I'm not a robot" video ever; Cisco's WebEx problem being far more pervasive than first believed; more bad news (and maybe some good news) for Netgear; Gmail adds .js to the no-no list; a hotel finally decides to abandon electronic room keying; more arguments against the use of modern AV; another clever exploitable CSS browser hack; some (hopefully final) password complexity follow-ups; a bit of errata and miscellany; a SQRL status update; a "Luke - trust the SpinRite" story; and a very nice analysis of a little-suspected threat hiding among us.



SHOW TEASE:  Time for Security Now!.  Steve Gibson is here.  The latest security news, some fun videos, and the "Traitors in Our Midst" - something we all use that actually could be a foothold, a toehold for bad guys.  It's coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 597, recorded Tuesday, January 31st, 2017:  Traitors in Our Midst.



It's time for Security Now!, the show where we talk about the latest security news now - now! - with this guy here, Mr. Security in Chief, Steven "Tiberius" Gibson.  Hello.  Live long and secure.



STEVE GIBSON:  Oh, it's a thumb out.



LEO:  Thumb out.



STEVE:  The officially approved Spock Vulcan whatever it is, live long and prosper.



LEO:  I was leaving the gym this morning, and the manager gave me the live long and prosper sign.  And I thought, well, that's quite appropriate, actually.



STEVE:  Very nice.



LEO:  Yeah.  So how are you, Steve?	



STEVE:  I am great.  And we have another terrific-looking podcast.  I ran across a really interesting - it started as a master's thesis and evolved into a formal security analysis, which led to my titling this podcast "Traitors in Our Midst."



LEO:  Oh, my.



STEVE:  Yes.  Something we've sort of touched on over the years, but we haven't, and the industry hasn't, to nearly enough degree.  So that's our main focus, which we'll get to toward the end.  But fully half of the tweets that I received over the last seven days were referrals to this hysterical video, the "I'm not a robot" arm, sort of defeating the purpose, or so everyone thinks.  So on one hand, it's really funny.  On the other, it actually, I mean, in some of the coverage, I think they've missed the point.  But we'll get to that.



It turns out that Cisco's WebEx vulnerability that we talked about last week in the Chrome extension is far more pervasive than we first believed.  More bad news, boy, it just keeps piling up for Netgear.  But there's maybe a little light at the end of the tunnel.  Gmail is restricting .js attachments as of middle of February, I think it's February 13th, which I assumed they were before, but they're going to start.  There was some misreporting of a hotel, an old-school, 111-year-old Austrian hotel that was attacked by malware and locked the guests into their rooms.  Well, that was never true, but...



LEO:  Yes, I got bit by that.  That was a fake news story, yeah.



STEVE:  Yeah.  But there is some interesting news there.  We've got some additional arguments against the use of modern AV from some people who really do know.  Another, oh, you're going to love this one, Leo, a very clever exploitable CSS browser hack.  Remember last week or the week before we showed how form fields that were being auto-filled could be hidden so that the user didn't see that they were being filled and sent back.  There's one where you copy a command line that a web page is showing you  into - you copy and paste.  So you copy it from the browser and drop it into your terminal window, and it does much more than it looks like it does.  In fact, I would tell people not to do that, but instead to drop it into Notepad.  I did, and it's like, whoa, look what it actually is, that is, inside this very innocuous-looking little command.  So we have that.



Some final follow-up, hopefully it's final, to the whole password complexity discussion we've been having the last few weeks.  A bit of errata.  Some miscellany.  A SQRL status update that a lot of people have been asking for.  A "Luke, trust the SpinRite" story.  And a very nice analysis of a little-suspected threat hiding among us.  So I think a great podcast.



LEO:  Oh, how exciting.  Hiding among us.  Hmm.  You've got a lot of spooky stuff in this one.  Leo Laporte, Steve Gibson.



STEVE:  So our Picture of the Week is just - it's fun.  It's crazy.  It's not meant to be taken seriously.  But it shows your typical wiring closet 19-inch rack, just crammed with cross-connect cables.  So it's like three square feet of RJ jacks, all plugged together with a big sign in the front, like on the glass on the outside:  "In case of cyberattack, break glass and pull cables."  Meaning just run away.



LEO:  That would work, actually, wouldn't it?  You just disconnect all those cables?



STEVE:  It would, yeah, although they don't look like they're labeled.  And so you'd have a serious nightmare...



LEO:  Just pull them all.



STEVE:  ...figuring out where the cables went, yes.



LEO:  Where they go back afterwards, yeah.



STEVE:  It's like, wait, you took that seriously?  No, we were just kidding.



LEO:  Yeah. 



STEVE:  And it's like, oh, I got a virus in my computer, so I pulled all the cables out.  Oh.



LEO:  Oh.



STEVE:  Okay.  So for our listeners who won't be able to see this, while you were telling us about FreshBooks, I wanted to see whether googling "I am not a robot arm" would bring up anything so that we could tell our listeners who aren't seeing the video how to go see this for themselves.  And, well, boy, does it - 4,100,000 hits on that phrase.



And the entire page of Google results:  "Robot arm beats 'I am not a robot' CAPTCHA test," says Geekologie.  Daily Mail:  "Googly-eyed robot beats 'I'm not a robot' security system."  The Enquirer:  "Robot declares 'I'm not a robot.'  World implodes."  UPI:  "Googly-eyed robot beats 'I'm not a robot,'" and on and on and on.  "A robot arm successfully beats an 'I am not a robot' CAPTCHA," says LaughingSquid.com.  And DailyMail.co.uk and on and on and no.  So as I said, this thing, the reason it's here is that, first of all, there's been a global misconception about what this means, but also because it is just hysterical.  So with that said, Leo, we need to share this video.



[Video plays]



LEO:  Here you go.  So should I describe it?  There's a robot arm with a stylus doing a trackpad on a MacBook with one of those CAPTCHAS that says, you know, we've talked about it before, it's a Google thing, "I am not a robot," and you just click the checkbox.  And the robot clicked it.  I don't know how it found it.



STEVE:  Well, because the operator is, like, servoing it.



LEO:  Oh, it's not autonomous.



STEVE:  No.



LEO:  And it says, "Deal with it."



STEVE:  Okay.  So, and the audio back there, I mean, it sounds like it's grinding gears.  It's got two D cells or something, and it's just, you know, it's a toy.  But very cleverly, the person who put this together got a capacitive stylus and used it to servo the cursor of the laptop over into the checkbox, and then lifts it up and then pushes it down.  And as some have observed, it even does a mic drop at the end.  It opens its jaws, drops the pen, and then takes a bow with its googly-eyes.



LEO:  Okay.



STEVE:  So the reason, I mean, so what everyone has said is, oh, look.  It fooled Google.



LEO:  No.



STEVE:  It fooled the "I'm not a robot."



LEO:  No.  The laptop knows, it actually knows the operator.



STEVE:  Precisely.  The only reason that option was presented is there is probably a multiyear-long preexisting relationship between that laptop and Google and the Internet.  And so there's a massive history behind it which, as we've explained, is how this thing works, is that it's deep reputation based.  And so, yeah, I mean, it's funny and clever, but all the press thinks this actually means that a robot beat the "I'm not a robot." 



And so I wanted to make sure people understand, no, an Internet bot would have never received that option.  It's because there's a person basically there.  And even though he used a cheesy noisy arm to do what he could have easily done with his own finger, well, that doesn't really defeat the purpose.  So I got a kick out of it. And, boy, it was the most shared thing with me of the week.  So I wanted to share it with our listeners.



We talked last week about Tavis Ormandy's discovery of a mistake.  You have to call it that because it is such a critical mistake that Cisco responded, to their credit, over the weekend.  I think Tavis notified them on Friday and started his 90-day countdown, and he got to 89, and it was fixed.  I mean, just immediately fixed.  However, Cisco keeps coming back to the disclosure page, their security advisory page.  And it was updated as recently as today because it turns out it's a little more pervasive than originally believed.



They said, quote:  "A vulnerability in Cisco WebEx browser extensions [plural] could allow an unauthenticated, remote attacker to execute arbitrary code with the privileges of the affected browser on an affected system.  This vulnerability" - and here's where it gets much broader - "affects the browser extensions for Cisco WebEx Meetings Server and Cisco WebEx Centers [which is Meeting Center, Event Center, Training Center, and Support Center] when they are running on Microsoft Windows.  The vulnerability," they explained, "is due to a design defect in an application programming interface (API) response parser within the plugin."  Which is a nice way of saying whoever wrote this, whoever coded this, wasn't thinking about the way it could be abused.  They were just thinking about getting it to work.



"An attacker that can convince an affected user to visit an attacker-controlled web page or follow an attacker-supplied link with an affected browser could exploit the vulnerability.  If successful, the attacker could execute arbitrary code with the privileges of the affected browser.  Cisco has released software updates for Google Chrome, Firefox, and Internet Explorer."  So all three major browsers.  All we knew last week was from Tavis's position because he was looking at Chrome, his baby that he's more responsible for.  We didn't hear anything then about the other browser extensions.  It turns out there's common code that Cisco reused across their third-party extensions, and all of the browsers are similarly affected, that is, Chrome, Firefox, and IE, except for Edge on Windows 10, which is effective.



So I heard, I received in response to our discussion last week a number of people whose companies used to use WebEx but did no longer, yet of course no one had gone in to retroactively remove it because it was there and didn't seem to be hurting anything.  Well, now we know that that was an unused extension presenting an expanded attack surface.  So standard best security practice, as with, for example, Java, is if you don't know you need it, if you're not actively using it, thereby justifying the unavoidable exposure that anything extra brings, it's just better to remove it.



So Cisco has updated all of their extensions.  If you know that you need WebEx, and you're using either, as we said last week, Chrome, but also now we know Firefox or IE, since this has been getting a lot of attention in the industry, and it turns out it's not difficult to exploit, you want to make sure that you're up to speed.



Cisco did provide a workaround, suggesting that if something prevented updating the extensions, or for example in a large organization, IT could provide some filters at the border to prevent WebEx activity on non-WebEx sites, which is where the bulk of the problem comes from.  Remember that Tavis said he  hoped that the Cisco site didn't have any cross-site scripting vulnerabilities that would allow someone to loop their attack through that site, which would then bypass this protection.



So updating the extensions, and if you're an IT individual with responsibility for a network that maybe you don't have any per-machine control over, or your company needs to use WebEx, and you're not sure everybody's going to update, you can restrict the access so that the WebEx API can only be used on the sites where you expect it to be used, and not as an unintended attack vulnerability.  And they said under "Workarounds": "There are no workarounds that address this vulnerability.  However, administrators and users of Windows 10 systems may utilize Microsoft Edge to join and participate in WebEx sessions as Microsoft Edge is not affected by this vulnerability."  So that's good news for any corporations that have moved to 10.



Netgear.  Boy.  You know, I think it's probably easily the top of our list of the most recently troubled router firmware.  This is a vulnerability which affects at least tens of thousands, and perhaps hundreds of thousands of routers.  The guy who wrote this up, Simon Kenin, posted to the SpiderLabs Blog at Trustwave.  It was well written, sort of in a first-person narrative, and kind of fun, so I'm just going to share it as he sort of discovered this.



He said:  "It was a cold and rainy winter night, almost a year ago, when my lovely Netgear VEGN2610 modem/router lost connection to the Internet."  He writes:  "I was tucked in bed, cozy and warm.  There was no way I was going downstairs to reset the modem.  'I will just reboot it through the web panel,' I thought to myself.  Unfortunately, I couldn't remember the password, and it was too late at night to check whether my roommates had it.  So I considered my options:  Get out of bed, go downstairs and freeze as I reboot the router" - which itself I guess had frozen.  "Or be lazy, stay in bed and, since I am a security researcher, try to hack it.  Needless to say, I chose the latter.



"So where do I start, I thought to myself.  Well, it has a web interface, and I need to bypass the authentication somehow, so the web server is a good start.  I started manually fuzzing the web server with different parameters.  I tried the '../..' classic directory traversal and so forth.  And after about a minute of fuzzing, I tried '...' and got this response."  And then in his explanation he shows us essentially a web page response with <html> section, then <head>, then <meta>, then <title>, then <body>, and then the close tags.  And essentially it's a response, a 401 Unauthorized response, meaning that there's a problem with authorization or authentication, you know, password.  It says:  "Access to this resource is denied.  Your client has not supplied the correct authentication."  And then there's a form with a post, so you post something.  And the action is "unauth.cgi?" and then an ID with a decimal number, 1211868232.



And so he says:  "Hmm, what is that unauth.cgi thingy?  And what does that ID number mean?  Luckily for me," he writes, "the Internet connection had come back on its own, but I was now a man on a mission.  So I started to look around to see if there were any known vulnerabilities for my VEGN2610.  It turned out there were none.  I started looking up what that 'unauth.cgi' page could be, and I found two publicly disclosed exploits from 2014 for different models that manage to do unauthenticated password disclosure.  Booyah," he writes, "exactly what I need."  And then he provides two links in his discussion of those two reports.  "Those two guys found out that the number we get from unauth.cgi can be used with passwordrecovered.cgi to retrieve the credentials.  I tested the method described in both and, voila, I have my password.  Now I can go to sleep happy and satisfied.



"I woke up the next morning excited by the discovery.  I thought to myself, three routers with same issue?"  Meaning his and those two others.  "Coincidence?  I thought not.  Luckily, I had another, older NETGEAR router laying around.  I tested it and, bam, exploited.  I started asking people I knew if they had Netgear equipment so I could test further to see the scope of the issue.  In order to make life easier for non-technical people, I wrote a Python script to test for this issue.



"Now, I'm not a great programmer.  I'm aware of that, and that's why I don't work as a full-time programmer.  As it turned out, I had an error in my code where it didn't correctly take the number provided from the unauth.cgi, and it passed gibberish to passwordrecovered.cgi instead.  But somehow it still managed to get the credentials."  Meaning that you didn't even have to give passwordrecovered.cgi whatever that thing was, that ID string from unauth.cgi.



So he says:  "Wait.  What's going on here?  After a few trials and errors trying to reproduce the issue, I found that the very first call to passwordrecovered.cgi will give out the credentials" - meaning the username and password - "no matter what the parameter you send.  This is a totally new bug that I haven't seen anywhere else.  When I tested both bugs on different Netgear models, I found that my second bug works on a much wider range of models.  A full description of these findings, as well as the Python script used for testing, is available online" - now, again, remember, this was almost a year ago - "and the vulnerabilities have been assigned CVE designations."



Then he says, with the subtopic "The Responsible Disclosure Process," which was a bit of a mixed bag, unfortunately, with Netgear, he says:  "This is where the story of discovery ends and the story of disclosure begins.  Following our responsible disclosure policy, we sent both findings to Netgear in the beginning of April 2016."  So, what, nine months ago.  "In our initial contact, the first advisory had 18 models" - 18 Netgear models - "listed as vulnerable, although six of them did not have the vulnerability in the latest firmware.  Perhaps," he wonders, "it was fixed as part of a different patch cycle.  The second advisory included 25 models, all of which were vulnerable in their latest firmware version."



So that was April.  So two months later:  "In June, Netgear published a notice that provided a fix for a small subset of vulnerable routers and a workaround for the rest.  They also made the commitment to working toward 100% coverage for all affected routers.  The notice has been updated several times since then and currently contains 31 vulnerable models, 18 of which are patched now, and two models that they previously listed as vulnerable, but are now listed as not vulnerable.  In fact, our tests show that one of the models listed as not vulnerable" - which was a DGN2200v4 - "is, in fact, vulnerable, and this can easily be reproduced with the proof-of-concept provided in our advisory.



"Over the past nine months we attempted to contact Netgear multiple times for clarification and to allow them time to patch more models.  Over that time we have found more vulnerable models that were not listed in the initial notice, although they were added later.  We also discovered that the Lenovo R3220 router is powered by Netgear firmware and is vulnerable, as well.  Luckily, Netgear did eventually get back to us right before we were set to disclose these vulnerabilities publicly.  We were a little skeptical since our experience to date matched that of other third-party vulnerability researchers who have tried to responsibly disclose to Netgear, only to be met with frustration."  And of course that's some of the stories we've been discussing the last few months.



"Two changes helped sway our opinion.  The first was that Netgear committed to pushing out firmware to the currently unpatched models on an aggressive timeline.  The second change made us more confident that Netgear was not just serious about patching these vulnerabilities, but serious about changing how they handle third-party disclosure in general.  That change was their commitment to Bugcrowd" - and then he provides a URL, bugcrowd.com/netgear - "a popular," he writes, "third-party vendor that helps to vet research, provides oversight for the patching process, and provides bug bounty rewards to help to motivate third-party researchers.  We fully expect this move will not only smooth the relationship between third-party researchers and Netgear, but in the end will result in a more secure line of products and services."



Anyway, and then he concludes, asking himself rhetorically why is this vulnerability so critical.  He says:  "For starters, it affects a large number of models.  We found more than 10,000 vulnerable devices that are remotely accessible.  The real number of affected devices is probably in the hundreds of thousands, if not over a million.  The vulnerability can be used by a remote attacker if remote administration is set to be Internet facing.  By default, this is not turned on.  However, anyone with physical access to a network with a vulnerable router can exploit it locally."  And of course that would also mean they could turn it on to then make remote exploitation possible.



Ah, and "This would include public WiFi spaces like cafs and libraries using vulnerable Netgear equipment."  Because by definition all of those people are on the inside of the router.  Normally you would depend upon the router's admin username and password to protect it from unknown users on the LAN side.  This bypasses that, making this an important thing to fix.  And he says, "As many people reuse their password, having the admin password" - this doesn't just bypass it, this gives it to you.  So you know what the password is.



"Having the admin password of the router gives us an initial foothold into the network.  We can see all the devices connected to the network and try to access them, perhaps with the same password.  With malware such as the Mirai botnet being out there, it's also possible that some of the vulnerable routers could be infected and ultimately used as bots, as well.  If running a bot is not possible, the DNS can be easily changed to a rogue server, as described" - and of course we've talked about DNS exploits before.



So he finishes:  "We recommend" - and certainly I do, too - "that all users of Netgear equipment check the Knowledge Base Article for instructions to test if they're vulnerable and/or how to apply patched firmware if they are."  So we've talked about Netgear enough that hopefully any users are staying on top of their firmware.  Again, you'd have to deliberately turn on remote admin, but you may be depending upon the router's authentication, username and password, to protect you.  That is bypassable trivially in the case of these vulnerabilities.



And I know I ran through this quickly and threw a lot out there, but as I read this, there are still to this day a large population of routers vulnerable.  So at this point you'd have to say that remote admin is unsafe on this family of routers, and come up with some other way to get the job done without remote admin, unless you're able to confirm that you're not vulnerable.  And of course the problem is there's been now such a history of vulnerability in this family of routers that I just don't think you can consider it safe to set up a Netgear with remote admin.  Who knows what other problems are already there, may already be known.  Yikes.



I was surprised that Gmail was still allowing .js attachments, that is, JavaScript attachments.  But they announced that, and they gave some warning, that as of February 13th, Gmail will start blocking and bouncing any incoming mail with JavaScript attached.  They currently restrict a wide array of extensions, like .bat, .chm, .com, .exe - the ones you would expect - .msc, .vbe, Windows scripting host, .sys, a whole bunch, .lib.  But they had not had .js on the list before.  Oh, and .scr, because as we know that's a Windows screensaver which is actually an EXE which is just renamed .scr to differentiate it.



And of course also we know that they will look inside of different forms of compression, and both .gz and .bz2, .zip and .tgz files.  And if they see files with those extensions in the zips, they block that.  And if they see a password-protected archive that they're not able to look into, they block that, too.  So I'm glad that Google is raising the barrier.  I just wanted to give people a heads-up that - I don't know why anyone would attach a JavaScript to email.  I guess you might stick it in a .zip file and get it to somebody if you had some cause for doing that.  But you won't be able to.



And they said, if you still need to send .js files for legitimate reasons, and obviously the reason they're blocking it is not because they're grumpy, but because you just get up to too much mischief if you attach JavaScript to email.  They say you can use Google Drive and Google Cloud Storage or other storage solutions, and of course just forward a link to somebody who can then grab the copy that you posted somewhere.



Okay, this hotel story.  The reporting was fun because it said people were getting locked in their rooms, which never happened.  And so it may have been some confusion about language.  It's an Austrian hotel that's been around for 111 years.  And the problem is it has been plagued with cryptomalware which has three times in the past infected the hotel's networks, including the door keying system, which has prevented people from getting into their rooms.



So there is some sort of - the way their system works, obviously, is that you don't need the computer just to create the key, but the computer is somehow involved, some room management computer is involved in the unlocking process such that, if that computer goes down, all the rooms are locked.  People can still leave.  You just can't get back in.



So the managing director of this hotel - it's a four-star hotel in Austria, very high-end - his name is Christoph Brandstaetter.  And he was quoted in the coverage saying:  "The house was totally booked with" - now, this is in the most recent attack - "with 180 guests.  We had no other choice.  Neither police nor insurance help you in this case.  The restoration of our system after the first attack in summer has cost us several thousand Euros.  We did not get any money from the insurance so far because none of those to blame could be found."  He said it was cheaper and faster for the hotel to just pay the Bitcoin.



Brandstaetter said:  "Every euro that is paid to blackmailers hurts us.  We know that, and we know that other colleagues have been attacked who have done similarly."  He said:  "When the attackers got the money, they unlocked the key registry system and all other computers, making them run normally again."  So the conclusion of this is sort of sad.  He said:  "We are planning at the next room refurbishment for old-fashioned door locks with real keys, just like 111 years ago at the time of our great-grandfathers."



LEO:  Can't hack them.



STEVE:  So here's a situation where they thought they would move up with the times and stay current and do the right thing.  But they just keep getting hacked.  Four times.  And so there have been three previous hacks, and I guess there was a fourth attempt that was thwarted.  But they said, okay, look, that's enough of this.  We cannot be going through this and have our guests inconvenienced.



So, yep, going back to old-school keys.  It'll be kind of quaint, but it's also sad.  And you have to think, too, that this is certainly going to hurt the vendor who sells the system because they're going to get a reputation among the hotelier community of having a system that is attackable and that people have to stop using because it creates an actual problem for guests.  And it'd be a little frustrating if you couldn't get into your room.  I mean, what do you do, if you've switched everything over to electronic keying and that system breaks?  So maybe a sign of the times.



LEO:  So the only thing that was made up was that people were stuck in the room.  The hack happened.  They couldn't create new card keys.  The old ones didn't work.



STEVE:  Correct.



LEO:  But nobody was stuck in the room, which would make sense.



STEVE:  Nobody was trapped, exactly.



LEO:  Yeah, yeah.  You'd have to have a pretty - it'd be more like a jail cell than a hotel room, if you couldn't...



STEVE:  Yeah, and I'm sure that it would even violate all kinds of codes.



LEO:  Safety codes, yeah.



STEVE:  Yeah, there's no way that ever able to [crosstalk].



LEO:  I should have thought about that, yeah.  What if the power went out?  People wouldn't be able to get out of their rooms?  No, that wouldn't be good, yeah.



STEVE:  Right.  So an interesting story appeared on Friday in Ars Technica, written by Sebastian Anthony in the U.K., when a former Firefox developer, Robert O'Callahan, who is now a free agent and safe from, as Sebastian put it, the PR tentacles of his corporate overlord, said that antivirus software is terrible, AV vendors are terrible, and you should uninstall your antivirus software immediately - unless you use Microsoft's Windows Defender, which is apparently okay.  And this has been a growing trend.  A couple of months back, Justin Schuh, Google Chrome's security chief, and indeed one of the world's top "infosec bods," as Sebastian put it, said that antivirus software is - okay.  This is Google Chrome's security chief quoted saying:  "My single biggest impediment to shipping a secure browser."



LEO:  Wow.



STEVE:  Yeah.  Further down the thread he explains that meddling AV software delayed Win32 Flash sandboxing for over a year, and that further sandboxing efforts are still on hold due to antivirus.  The man-in-the-middle nature of antivirus also causes a stream of TLS errors, says Schuh, which in turn breaks some elements of HTTPS/HSTS.  And of course we've talked about that, how a number of the AV, in order to get visibility into HTTPS streams, they're now installing their own certificates in their users' machines and essentially spoofing the certs of sites you go to so that, if you go to a secure site and look at the certificate in your browser, it will say - it won't be from the site itself.  It'll have been created by that AV software that you installed on your machine.  So it's doing on-the-fly man-in-the-middle.  And the point is it's not just kind of worrisome, but it's actively causing software vendors major trouble.



"These are just two recent instances," writes Sebastian, "of browser makers being increasingly upset with antivirus software.  Back in 2012, Nicholas Nethercote, another Mozillian working on Firefox's MemShrink project, said that 'McAfee is killing us.'"  Yeah.  "In that case, Nethercote was trying to reduce the memory footprint of Firefox and found that gnarly browser add-ons like McAfee were consuming a huge amount of memory, among other things.  If you venture into the browser mailing lists, anti-antivirus sentiment has bubbled away just below the surface for a very long time.



"The problem," writes Sebastian, "from the perspective of the browser makers, is that antivirus software is incredibly invasive."  And of course that's why Windows Defender is an exception, because it's built into the OS.  It's not a third-party add-on that is forced to do things that are unauthorized in order to hook itself in to the depth that it needs to.  And then, just to finish what Sebastian wrote:  "Antivirus, in an attempt to catch viruses before they can infect your system, forcibly hooks itself into other pieces of software on your computer, such as your browser, word processor, or even the OS kernel.  O'Callahan gives one particularly egregious example."



He says:  "Back when we first made sure ASLR" - remember Address Space Layout Randomization - "was working for Firefox on Windows, many AV vendors broke it by injecting their own ASLR-disabled DLLs into our processes."  So we've discussed here that not only all of that, but remember that modern AV is also introducing a larger and more fertile attack surface.  We've discussed, I think it was Symantec whose AV itself had buffer overrun errors.  So you could take a system that had been carefully vetted and tweaked and developed over time, where there weren't any problems, there wasn't anything for the virus to attack, and add antivirus to it and make it vulnerable because of the AV, which was itself vulnerable.



So anyway, I thought that was very good coverage and some nice insight sort of behind the scenes.  That's not a view we normally have.  And there is some problem.  The reason these developers aren't being vocal about this is that they recognize how popular AV is.  They're trying to coexist.  But it's very difficult.  The AV people have, I mean, they're like an endangered niche in the market, which I think, certainly in the case of Windows, doesn't make as much sense as it did originally because, just as Microsoft finally added an actual firewall, and then had it there but it was turned off, then finally turned it on, now, I mean, you don't see third parties doing firewalls the way they used to.  That finally went away.



Well, the AV market is hanging on because people see it as insurance.  It's like, well, you know, I don't want to get infected, so I'll add this.  But it turns out that it can make your system more vulnerable, rather than less vulnerable.  And we've seen instances.



LEO:  Yeah, wow.



STEVE:  Okay, now, Leo.  This link, this look-before-you-paste.  You've got to go to that page, and then you'll see this nice little command, "ls -lat," which obviously is Linux's or Unix's directory list command.  Copy that, you know, mark and copy, and then go to a Notepad and paste what you've copied.  That cute little, what is it, one, two, three, four, five, six, seven characters, "ls -lat."



LEO:  I'm just going to copy that right off this page right here.



STEVE:  Copy it right off the page.



LEO:  Copy that there.



STEVE:  Copy that.



LEO:  Yeah.  Now it's opened a terminal here because I think that's - I'd like to paste that command because apparently I don't know how to do an ls.  But that's okay.  Let's just paste this command here.  Whoops.  Paste that command.  Whoa.  I just gave my access - what the - hey, what happened?  A lot of stuff happened there.  What was all that that just zipped by?  One of the problems with paste is it also pastes a return, so you can paste in and execute a command at the same time.



STEVE:  Right.



LEO:  Let me just open the StickyPad or something else to look at that one because...



STEVE:  Yes, take a look at what is in...



LEO:  That was a bad idea.



STEVE:  Take a look at what is in your...



LEO:  Nothing bad happened.  You wouldn't do that to me; would you?



STEVE:  No, I would not do that to you.



LEO:  But fortunately...



STEVE:  And this is just meant to be a proof-of-concept.  But it's very effective.



LEO:  Wow.



STEVE:  So essentially this uses CSS, the technology for controlling the formatting of web pages, to hide a large block of code, which you can't see.  And I don't know if anybody has experienced this, where you copy something out of a web page, like a high-end advertising-laden website.  And when you paste it, you get a little ad stuck at the end of your paste, which is like, "If you are interested in additional information."  And it's like, wait a minute, I didn't copy that.  But it's, no, unfortunately, we've got scripting running, and we've got all kinds of technology that's able to override the normal behavior that we're used to with copy and paste.  And so that's what's happening here is that cascading style sheets is used to take a big blob and just hide it from you.



LEO:  Yikes.



STEVE:  Yes.  And we've talked, for example, about how dangerous curl can be, where you just curl some URL, and you suck it in.  Well, that could be used with this so that you see the URL that you're going to curl.  You first go look at it and go, oh, yeah, okay, that looks fine.  Then you copy the curl command and the URL off of the page, which is actually different from what you see, and then run something coming from a different server and still get yourself blasted.



LEO:  Wow.



STEVE:  So, boy, I tell you.  What we're seeing is we've developed a very complex, very sophisticated set of interacting pieces.  And unfortunately there's more and more attention being given to answering the question, what kind of mischief can we get up to by clever use, re-use of benign technology in mischievous ways.  And of course, sometimes more than mischievous.



So I had a couple of follow-ups from last week.  Someone sent via Twitter:  "How strong do pronounceable passwords calculate?"  And he says, "LastPass feature?"  And he says:  "I use for a few I must remember."  And that sort of goes along with Max, who tweeted, he said:  "You go on about logarithms for half an hour, but you fail to actually answer the question.  God, that is so frustrating to listen to."



Okay.  So, correct.  The topic of last week was Password Complexity Calculation.  That's what I delivered.  I didn't give you password complexity conclusion, I gave you calculation.  Because I wanted - my goal was to equip our listeners with a simple means for wrestling the questions of alphabet size, how many words in Diceware, how large is the alphabet, do I have upper and lowercase?  How many characters, and so forth.  How do you turn that into a meaningful number?



And so that's what we did last week.  I stopped short on purpose because there isn't - I mean, okay.  While there is a definitive means of saying passwords formed in this fashion will have this many equivalent bits of entropy, what people want is, well, how much should I have?  How much is enough?  And that's what there's no answer to.  More is better.  Less is worse.  But the real problem is that we are ultimately in the hands of what the website or the system that we're providing that password to does with it.



If, for example, they do an MD5, you know, a lame old 128-bit hash for which all kinds of tables already exist, in fact it's like they just unsalted MD5.  You have to have a really long, I mean, really, really high-entropy password in order for it not to be brute-forced because that hash doesn't provide enough brute-forcing strength.  And tables can be applied because every user would be using the same hash.  And so they would be able to essentially do a parallel attack.  If, however, a more modern hash like an SHA-256 were used, that's good, except that unfortunately the crypto currencies use SHA-256, so now that's all been reduced to silicon, and it's screamingly fast.



But hopefully a forward-looking site, one that actually has good security, would be using a per-user salt, meaning that when you create your account, they just, out of the air, they pluck a random salt, and they put it into your account data.  That is added to your password as salt to the hash in order to make every user's hash unique.  So even two users with the same password would end up with different hashed results so that you wouldn't see that collision.  And that prevents any kind of precomputation attack.  And then, if we wanted to go really to where we should be, we use a password-based key derivation function, a PBKDF, where it's an iterative process that is deliberately intended to take 10,000, we've talked about this before, like 10,000 iterations.  So you put this in, and you crank on it for seconds until you have an answer.  So that dramatically reduces the ability to brute force.



But the problem is users have no control over that.  We don't know what random website is going to do when we give it our password.  So that's why I've been pushing back against this idea that there are easy solutions.  "Easy" means low entropy.  That's what's easy about it.  If it's a word, that's low entropy.  If it's an abbreviation for your favorite phrase, well, low entropy.  We have no control.  We don't know what they're going to do.  So our best solution is to use the highest entropy raw input, with a reasonable length.  Sometimes the website says no, no, no, that's too long.



Okay, well, so give it the highest entropy, longest password it lets us, and then store that somewhere.  And, for example, this first person who tweeted, saying he uses a few of these pronounceable passwords that he must remember.  Well, again, pronounceable means lower entropy.  What LastPass had to do in order to make it pronounceable, and we sort of discussed this a few weeks back, where we were talking about that - what was that, oh, it was LessPass.  And, boy, was it Less.



LEO:  Oh, yeah. 



STEVE:  Where, remember, each location in the password had a preassigned class of alphabet so that it was really lower entropy.  So again, if it's pronounceable, less random.  The point is convenience means lower entropy.  The fact that you can remember it means it has lower entropy.  So to this first person who said he uses a few, I'm not saying they're bad.  I'm saying they're not as good.  I'm not saying Diceware is bad.  It's not as good.  And then people say, oh, yeah, well, I use 27 Diceware words.  It's like, okay.  If you can find a website that will let you put 27 Diceware words in...



LEO:  That'd be pretty good.



STEVE:  ...then go for it.  I would never argue that that had low entropy, as long as you had a lot of words as candidates, and they were chosen at random.  The problem is you can't find a website that will let you do that.  So given that we generally have a length limit on our passwords, within that length I would always opt for as much entropy as possible.  You're erring on the side of caution.  And to the person who needs them to be pronounceable, so he can remember them, I say write them down.  As Bruce Schneier said, we understand how to manage bits of paper.  We have wallets.  And leave off a couple of the leading or trailing characters so that it doesn't disclose the entire thing, in case somebody discovers it.



But you can write down a big long blob and then maybe customize it per site or something.  There are some tricks you can do where you start with a root of high entropy, and anything you add to high entropy never reduces its entropy.  So that's something important to remember.  Anything you add to something with high entropy never reduces the entropy that you originally had.  It can only add to it.  It can only make it better.  So anyway, it's been really useful to get some pushback and some feedback from people who, unfortunately, whose favorite practices I have disparaged a little bit, not intending to say you shouldn't do it, but just saying, okay, I want you to make an informed decision.



LEO:  Right.  If you're going to choose it, know how good it is.



STEVE:  Yes, yeah.



LEO:  Now, let me ask you, because of course this is always the problem is the balance between memorable passwords and effective passwords.  Memorable in general means it's less effective because it's less entropy.  I try to balance it, and I wonder if this is a bad strategy, by using an algorithm to generate the password that I can recreate.  For instance, I've mentioned in the past you could - this isn't what I use, and it's probably not long enough, but you could use the first initial of the last name of the last 10 presidents, capitalizing the Republicans.  And if you wanted to add a digit to that, you could add the number of years they served.  You could, in your head, go through that and regenerate it.  But is that password inherently less reliable because there's an algorithm generating it?



STEVE:  Well...



LEO:  I mean, it's memorable, so I guess it means less entropy.



STEVE:  Yeah, exactly.  Although probably still high.  Remember that the threat is brute-forcing.



LEO:  So if somebody could figure out my algorithm or look at, you know, the worst thing would be they could look at it and go, oh, I see what he's doing.



STEVE:  Right.  Exactly.  And if someone didn't hash your password, and that escaped, and someone really was targeting Leo Laporte...



LEO:  Well, I don't - reusing them is bad no matter what.



STEVE:  Well, no.  But depending upon the algorithm, looking at it might give someone a clue to what it was you were doing.  And, for example, they might notice, well, we don't know how he got these letters and numbers, but look at the pattern that's always the same.



LEO:  Yeah, yeah.  So it's a letter followed by a number, right.



STEVE:  Right, right.



LEO:  So actually that's why I don't use it.  But here's a good one.  You could take page 10 of "War and Peace," and third paragraph in, and then just use the first letters of that paragraph plus punctuation.



STEVE:  And the problem is what that then does is encourage you to reuse that one phrase.



LEO:  Well, no.  Let's say I'm using it for my LastPass password.  We all have one - if you're doing it right, you have one password that you have to remember, your Password Vault password.



STEVE:  Right.



LEO:  Best just to make up a random one and memorize it; right?  Force yourself to memorize it.



STEVE:  I really think that today there is no alternative than using a database, a password manager, to collect your passwords and just roll that way.  I have all kinds of stuff stored in mine.  And I just don't think there's a useful alternative, unfortunately.  I mean, I know people want shortcuts and tricks.  But the nature of that means that you're not going to get the security that you could have.



LEO:  Yeah.



STEVE:  Two little bits of errata.  We have listeners of the podcast among Google's security team.  After last week they reached out...



LEO:  That always makes me nervous when I hear stuff like that.



STEVE:  They reached out to add a bit of additional information about that troubling auto-form-fill hack, the one that we talked about where the fields were off the screen, and it was being populated.  One of them wrote and said the CC number field, the credit card number field, will not be populated until and unless the user responds with the card's correct CVV.  That's the card verification value.  And so once again, tip of the hat to Google.  They were thinking ahead.



So even with all the fields on the screen, Chrome will not populate the credit card number, even if it knows it.  It waits for you to put in your three- or four-digit CVV to prove that you're you.  And then it'll go, okay, and then, bing, populate the credit card field.  So Google security team, thank you for the clarification.  I'm glad to know that there's that extra safeguard in there.  Of course, as we know, there's also street address and all kinds of other information that doesn't have the same protection.



And a good friend of the podcast and listener, Taylor Hornby at Defuse.ca, he shot me a note saying:  "Quantum entanglement does not let you talk FTL" - as in faster than light - "but you can use it to get one-time-pad keys."  So I just wanted to share that.  There were a number of people who were sticking by Einstein's information cannot travel faster than the speed of light.  That's an absolute limiting characteristic of the universe.  Which China seems to think that's no longer a limitation.  So we'll see because their system is definitely designed to allow people to experiment with FTL communications.  But Taylor's note that you could use entangled photons to essentially securely feed two parties one-time-pad keys, that's a really good point, too.  So Taylor, thanks for that.



Two bits of miscellany.  Addam Tait wrote, he says:  "Curious about your opinion regarding software delivery teams.  Better to have security person per team overseeing, or all devs taking ownership of security?"  Which I thought was interesting.  It's like, okay, so we're talking about security all the time, about what a problem it is, about how it doesn't seem to get the attention that it needs.  So how do you structure a group of developers so that the product of their work is secure?  And I  think the answer is all of the above.  That is, you make security an issue for all of them, that is, every single developer spends some time in meetings or in classes or somehow you just make sure this is on their radar.  That is, that they not believe this is somebody else's problem.



And then also I don't think you can - you cannot avoid one person whose entire responsibility is security.  That is the "buck stops there" person.  And it's not that he's in the hot seat, he or she is in the hot seat.  It's that it's so easy to get distracted when you're trying to get something to work that it's, okay, I'll check this to make sure I didn't make any security mistakes afterwards.  Well, and then you get distracted by some other distraction, and you forget.



So I think, first of all, you need everyone on the team to be conscious of the importance of security.  But you need it to absolutely be one person's sole focus so that everything that is produced is challenged from that perspective.  That's that person's job.  They're not distracted by anything else because that is, you know, that's all they have to do is make sure that the code being produced, the systems being produced are as secure as they know how to make them.  I don't think - it just can't be part-time.  Somebody has to have full-time oversight while everybody else understands the need. 



And then, finally, Christian Loris said something that I thought was interesting, Leo, following up on my comment last week and our observation that "monkey" seemed bizarrely popular as a password.  And he proposed something that I thought was kind of interesting.  He said:  "Heard your ponder on password monkey's prevalence.  Maybe French because 'mon key' is French for 'my key.'"



LEO:  Except it's not.  But other than that, okay.



STEVE:  Oh.



LEO:  It's a good guess.  Key is "cl."  "Mon cl" would by "my key."



STEVE:  Mon cl.



LEO:  Mon cl.



STEVE:  Nice.  Sorry, Christian.



LEO:  Nice guess, though.  No, I think people like monkeys.  I don't know.  It is a good question.



STEVE:  I think you're right.  I think they're kind of cute and funny.  And if you're not really caring back in the old days about security, it's, oh, monkey.



LEO:  Monkey123 if you're really secure.



STEVE:  So I've had a lot of people saying, hey, Steve, you haven't been talking about SQRL lately.  I confess that, as you know, Leo, I had gotten myself distracted for the last month.



LEO:  The top-secret project.



STEVE:  But a couple weeks ago I announced to the SQRL newsgroup that I was back, that I'd done all I could for the time being on the distraction that I had been nursing.  And I am back and active, and we're going to get this thing finished so I can get back to SpinRite.



But where I am at this instant I think everyone will find interesting.  We're trying to make a practical system.  And one of the things that SQRL does is it more tightly binds an identity to you.  That is, as everyone knows, you set up one SQRL identity.  That's sort of your surrogate on the Internet.  And as you go to different websites, it presents your identity for that site, which is unique for everyone who uses that site.  But the problem is, what about a site where Mom and Dad both want to do their banking?



Well, traditional username and password, we would call that "weakly bound," or "loosely bound," meaning that the only way the site knows them, knows anyone, is the username and password.  So whichever side of the marriage sets up the banking account simply says to husband or wife, here's our username and password for banking.  In other words, it's easy to share the identity that you're using because it is weakly bound.  It is just a secret that you're trying to control.



Well, SQRL changes that.  SQRL's identities are tightly bound.  And not only does SQRL change it, but that's where we're headed.  We're headed toward a world with more tightly bound identity.  Biometrics does that.  It's not easy to give your thumbprint to someone else the way you can give them your username and password.  And if we get retina scans eventually, that'll be a problem.  And arguably, the "relying parties," as they're called in the identity system, the relying parties want tighter binding.  They want to know who it actually is and not just somebody who got told somebody else's username and password.



So although this isn't technically SQRL's problem, and it doesn't affect the SQRL protocol because the SQRL protocol is about asserting an identity, and this is about using those identity assertions, it still is a problem that needs to get solved.  And anyway, so what we've developed is a solution we call "managed shared access," which I'm in the process of coding and will add it to the SQRL demo at GRC, mostly as a proof of concept, to demonstrate how in an environment where you have tightly bound identities, it's no longer practical to say, well, just share your username and password.



Now, without having this, we have a solution.  That is, we had to come up with a kludge which is kind of ugly.  And it's because I've never liked it that I thought, okay, there's got to be a better way.  And of course the kludge is Mom and Dad each have their own SQRL identity, and then they also have a shared SQRL identity.  So the banking site that doesn't support managed shared access, well, if it only allows one identity to log on, then it needs to be a shared identity.  It can't be either one of their SQRL identities.



So they create a third, which they share.  The reason this is messy is that quickly it gets out of hand because then your corporation wants you to use SQRL to access the Intranet.  IT controls it and, like, what, gives everyone a SQRL identity that that site recognizes.  That's clearly not the right way to go, either.  So the point is that there's a general problem with tightly bound identity not working well with the current world of loosely bound identity, which is really to say no particular assertion of identity at all.  It's just, oh, yeah, here's the username and password.



So anyway, so we're in the process of - actually, I think we've pretty much figured out how we want this to work.  And so I am adding that to GRC, not to say that anyone using SQRL has to do this, but I think by demonstrating an elegant, simple, low-overhead solution, which essentially allows an account owner to share access in a managed fashion with as many people as they need to for the application in a corporate environment, it might be everybody who's using SQRL with their own identities who needs access to a single relationship.  So the point is, rather than creating a one-to-one binding, this allows a many-to-one mapping with management.



So that's where we are.  So I'm working on that.  And then it's just finish the installer, and I've got a little bit of UI stuff to wrap up.  I mean, the system is running.  Oh, there are a couple little issues involving the rekeying of identities that we haven't reached consensus on yet.  But I expect we will soon.  So I'm back in the saddle, and I'm going to push this thing across the finish line so I can get back to SpinRite.



And speaking of which, we have from Pete Kokkinis - K-O-K-K-I-N-I-S.  He sent a note to Greg with the subject, "Please forward to Steve - testimonial."  And this I referred to a little tongue-in-cheek as "Luke, trust the SpinRite."  Because he said:  "Hey, Steve.  I'm a long-time SN listener and user of SpinRite."  He writes:  "You're an inspiration to me, and I look forward to your weekly podcasts.  But for approximately 14 minutes I doubted SpinRite, and for that I'd like to apologize.



"I had a PC running at a client site acting as a syslogger for many years.  Unbeknownst to me, my client closed up shop and moved when I went to visit their shared space with another tenant client of mine.  So I picked up some things I had left behind, including the syslog PC.  I fired it up back at my office only to find it hanging at boot-up with a message about hal.dll missing or corrupt."  And, yes, many Windows users have, at least I've heard...



LEO:  That's pretty much a showstopper right there.



STEVE:  Exactly.  Yes, that's the hardware abstraction layer dot dll.  And if you don't have that, you don't have Windows.  So he says:  "Being curious at this point, I ran SpinRite.  Its forward progress stalled at 4%.  Since I was impatient, I canceled it and tried booting as any running of SpinRite often seems to get enough of the disk back.  This time it didn't.  So I reran SpinRite, starting at 5%."  Basically, so he skipped over the area that SpinRite was starting to really work on and work on recovering.  He started at 5%, and it completed an hour later.  "I tried booting again, nothing."  Because of course he skipped over the work.



Anyway, "So I plugged the drive into a USB/SATA adapter to see if my syslog folder could simply be copied off.  No dice.  Even though I could see the folder, I couldn't even open it, as it would just hang.  So I right-clicked the drive and ran a CHKDSK with both options checked."  That's "Automatically fix file system errors" and "Scan for and attempt recovery of bad sectors."



He writes:  "The drive was making a normal scanning noise" - and I kind of thought, okay, what is that?  But anyway, scanning...



LEO:  [Vocalizing]



STEVE:  Yeah, "as CHKDSK was running, and the progress bar was steadily moving.  At this point I questioned the usefulness of SpinRite if CHKDSK can do this recovery more successfully.  The progress bar finally reached the end after 14 short minutes, and I clicked Close.  I browsed to the external drive letter, opened Program Files, and, bam," he says.  "Half the folders in the system were completely gone, including the syslog folder I needed, which I had seen earlier.



"I googled if CHKDSK can delete files; and, wow, did I see some upset users out there.  It never occurred to me that if CHKDSK can't repair, it will wipe data if corrupt, granted that you check both option boxes.  Anyway, keep up the great work, and I will never doubt you again."  So, and we do hear this often.  People come to us having done this and saying, oh, oh, I didn't realize.  Will SpinRite help me?  And it's like, no.



LEO:  Too late now, yeah.



STEVE:  No,  Because what happens is CHKDSK doesn't do recovery.  It just sort of says, okay.



LEO:  It's all cleaned up.



STEVE:  This has gone into oblivion.  



LEO:  You don't want that.



STEVE:  Snip this off right here.



LEO:  You didn't want that file, did you?



STEVE:  So it's like, it's just gone.  And it's like, oh, sorry about that.



LEO:  Sorry.



STEVE:  Okay.  So are our ubiquitous printers traitors in our midst?



LEO:  Oh, they are.



STEVE:  Yes, they are.



LEO:  They are.



STEVE:  Yup.  This began as a master's thesis titled "Exploiting Network Printers - A Survey of Security Flaws in Laser Printers and Multi-Function Devices."  And this computer science grad who was working on his master's in computer science and network security took a very close look at printers.  In the introduction to the paper he created, he wrote:  "The paperless office has been a dream for more than three decades.  However, nowadays printers are still one of the most essential devices for daily work and common Internet users.  Instead of getting rid of them, printers evolved from simple printing devices to complex network computer systems installed directly in company networks and carrying lots of confidential data in their print queues.  This makes them an attractive attack target."



And in this paper this person and two others basically take a very careful forensic look at 20 mainstream printers:  HP LaserJet 1200, 4200N, 4250N, and on.  There's, like, seven HPs, two Brothers, three Lexmarks, three Dells, a Kyocera, two Samsungs, a Konica, and an OKI.  Every single one has a varying number, in some cases one, two, three, four, five, six, seven different classes of vulnerability.  And so what we have, I  mean, and this shouldn't surprise any listeners to the podcast, I mean, the details are interesting.  Oh, in fact, the last column in the chart in the show notes is number of printer vulnerabilities they found.  And in some cases, the two very popular HP LaserJets, the 4200N and the 4250N, both have 12 separate vulnerabilities each.  Other ones have seven or 10, and seven, nine, 10, 10, five, 11.  I mean, so rife with problems.



The problem is that security has not been traditionally a focus of our printers.  And, I mean, and Leo, you obviously know this because you immediately knew the answer to the question, do we have traitors in our midst.  I remember when Apple first produced the LaserWriter.  You know, we were all tolerating Epson printers going bzzz, bzzz, bzzz, bzzz, bzzz, you know, back and forth.  And suddenly, with no sound at all, out emerges this, like, photo-ready, shockingly attractive typeset piece of paper.  It was astonishing.



And what I remember being really interested in at the time was recognizing that there was far more processing power in that LaserWriter itself in order to render the PostScript into bitmap than was in the machines that the printer was connected to.  The machines could barely get DOS booted, yet they were able to, with the proper drivers, print a lovely page, thanks to the work that Apple did with HP.  Or was it Canon back then?  I don't remember.  I think it was Canon initially.



So what we have is we have decades-old technology that has been quietly simmering and evolving with features being added and no one ever taking a close look at the security implications.  And, for example, someone said, hey, let's do wireless printing.  Oh, great.  So then they add a wireless access point to this thing that is already layered with multiple languages and interpreters to turn the code into the printed page.  And they give this thing an access point so that people with AirPrint can connect to it.  What could possibly go wrong?



So they say in their paper:  "In this paper we conduct a large-scale analysis of printer attacks and systematize our knowledge."  And I ought to say this is the first time this has been done.  The work is heavily referenced with all of the previous work, which has been scattered and, distressingly, is in some cases two decades old.  There was some awareness of problems back in 1996 that still exist today, never got fixed.  So they said:  "Based on our methodology, we implemented an open-source tool called PRinter Exploitation Toolkit (PRET).  We used PRET to evaluate 20 printer models from different vendors and found all of them to be vulnerable to at least one of the tested attacks."  And as I said, in some cases to 12 of the tested attacks.



"These attacks included, for example, simple denial-of-service (DoS) attacks or skilled attacks extracting print jobs and system files," meaning exfiltrating what the printer is printing.  And think about that for a minute.  I mean, think about what confidential information a printer has passed through it when anything that it prints could be exfiltrated from the network if something evil has managed to crawl into it.



So they say:  "On top of our systematic analysis we reveal novel insights that enable attacks from the Internet" - from the Internet - "by using advanced" - get this, there is something called "cross-site printing techniques combined with printer CORS spoofing.  Finally, we show how to apply our attacks to systems beyond typical printers like Google Cloud Print or document processing websites."  For example, those that convert a PostScript into a PDF, for example.  "We hope that novel aspects from our work will become the foundation for future researches, for example, for the analysis of IoT security."



So, okay.  So the number one problem is what we would expect.  People who've been following the podcast closely know that, in our current era, anytime you are interpreting something, you have a security challenge.  When you have user-provided input to an interpreter, it is so difficult for an interpreter which essentially is reading what you provide as a form of code, it is so difficult to make sure there isn't some way to subvert that.  And we've talked many times, for example, all of the Stagefright problems with the media library, those were problems in processing the media files because interpreters are used to read the tokenized formats of these files.  They're convenient and efficient, but they're dangerous because they require the interpreter to be perfectly coded so that it cannot be subverted.



And so what we have with printers is, I mean, they are loaded with interpreters.  That's what they do.  By their nature they do not take a raw raster from the computer.  They take a higher level description of the page.  And that's what PostScript is.  That's what PDF is.  That's what PCL is, is a high level description.  They read that, and then in their own internal RAM they build a bitmap which is then sent out to the printer imaging hardware as the page runs through it.



So, I mean, it is interpreter city in any of our contemporary printers.  And they've got, in order to be maximally compatible, they all support - they're just bristling with protocols.  There's the Internet Printing Protocol, IPP; the LPD, which is the Line Printer Daemon.  There's SMB.  A lot of them support Server Message Blocks.  SNMP, which we've talked about, that's the generic Simple Network Management Protocol.  Turns out all the printers support that, too.  So you can query the printer.  You can find out its make and model and all that remotely.  And that of course tells you how to attack it.  And then they also support, on port 9100, that's known as the JetDirect or AppSocket port, raw printing through port 9100.



So there's a wide array of protocols.  These guys did not attack the protocols.  Which is not to say they're not attackable, but they went for where the meat clearly was, which was the interpreters.  There's a top layer which are the job or the printer control languages.  Those are sort of the supervisors of the lower level page description languages.  And so there's something called PJL, which is the Printer Job Language.  PJL was originally introduced by HP, but soon became a de facto standard just due to HP's early strength in the printer market, in the laser printer market.  It resides above the other printer languages and can be used to do things like change settings, paper size, and the tray from which the paper will be pulled and so forth, sort of a meta level above page description.  So PML, which is the Printer Management Language, which is a proprietary language also used to control HP printers, combines sort of PJL, the Job Language, with SNMP as its transport layer, in order to get that done.



Then page description languages are, you know, everybody can do their own if they want to because it only needs to be in agreement between the printer driver in the client machines and the printer itself.  So consequently there's been an explosion of those.  Kyocera has one called PRESCRIBE.  Samsung has one that they call the Samsung Printer Language, SPL.  Xerox has something called the Xerox Escape Sequence, XES.  Canon has their own Printing System Language, CaPSL.  Ricoh has the Refined Printing Command Stream, RPCS.  Epson has the Standard Code for Printers, ESC.  HP of course has HP-GL and HP-GL/2, which are primarily focused back in the days of plotters.  So there's all of those.  Then there's of course the PDF format, and most recently the XML Paper Specification, XML Paper Spec, or as we know it, XPS, which is often now found on newer printers.  And we find support for that natively in Windows.



But among all of those, the most common standard page description language is PCL and then PostScript.  PostScript is incredibly vulnerable, and that was one of the - actually most of the printers had problems with PostScript because it is a very complex language.  It is a full stack, PostScript itself, a full, stack-based, Turing-complete programming language which consists of about 400 operators for math, stack manipulation, and graphic manipulation, and managing various types of data in arrays and dictionaries.



So, I mean, it's a language.  And access to the PostScript interpreter, which is what all printers ultimately provide because that's how you provide, if you're using PostScript, that's how you provide them with a page description for them to interpret, access to a PostScript interpreter is classified as code execution because any algorithmic function can be implemented in PostScript.  And PostScript has access to the file system because there have been extensions over time.  Of course there have been.  It's like, oh, wouldn't it be convenient if we could store things locally in the printer, and if we could get them back later?



So of course, yes, it's convenient, but it also means then that an attacker who has access to PostScript can use PostScript and the interpret either the way it was intended or by subverting it in order to gain access to the internals of the printer and in some cases set up long-term resident code, meaning that a printer in a network could be an advanced persistent threat, an APT, which as we know is one of the greatest concerns that corporations have.  That's what brought Sony down, famously, was that something got into their system and lived there for a long time and allowed attackers to look around and gain a lot of knowledge.



So there were three attack models that these guys looked at.  They looked at the local attacker, meaning physically local, that is, in physical proximity to the machine, who they characterize as the strongest attack possible, having physical access to the printer for even a limited time because that means that an attacker could plug in external storage media like memory cards or USB sticks; could temporarily connect the printer to a USB or parallel cable; and could change control panel settings.  Essentially, that gives them a lot of power.  On the other hand, it is realistic because printers are generally not seen as a security node in a network.  They're typically, it's typically a shared resource in a public location.



They write:  "It's a realistic attack for most institutions and companies.  Gaining physical access to a printer can generally be considered easier than for other network components like servers or workstations because they're usually explicitly shared by and accessible to a whole department.  So sneaking into an unlocked copy room and launching a malicious print job from a USB stick takes only a matter of seconds."



Their second type of attacker doesn't have physical proximity, but is on the LAN.  So a LAN-based attack can connect to the printer device via TCP/IP network, that is, the local LAN network, over any of these protocols - FTP, SMB, SNMP, LPD, and IPP, and even the raw port 9100.  So there's lots of connectivity possibility in order to establish a connection.  And they could do whatever they want to do over a longer period because their connection to the printer is part of what their own computer typically does during the day.  So having that connection wouldn't even pop up in any kind of typical traffic analysis.



And as we mentioned before, many newer printers bring their own wireless access points to allow for easier printing and flexibility.  AirPrint-compatible mobile apps are able to do that.  And so while connecting to a printer through WiFi requires the attacker to stay physically close to the device, we know that it's possible to use focused antennas and that WiFi reaches a good distance these days, too.  So you don't even have to be physically on the network in order to get a connection to an attacker.



And then, finally one of the more chilling attacks is - I mentioned this before - essentially a cross-site attack where a web-based attacker not in the network, not even connected to the printer, is able to gain access.  And they demonstrate this in their work.  It's actually possible to perform cross-site printing attacks.



So this so-called cross-site printing technique would be where a victim in a corporation unwittingly goes to a website and pulls a page from a malicious server that looks benign.  Their web browser brings down a bunch of JavaScript and then uses Ajax requests to port 9100 of the victim's Intranet printer in order to talk to it, which is completely feasible.  It is not, in this mode of attack, it is not possible to get results back because the same-origin policy in the browser will prevent an interaction with port 9100, but it is possible to send data to the printer.  And so depending upon what's known about the attack, it's even possible for a remote web server to leverage JavaScript running in an Intranet user's browser to access the printer.



And I'm really glad these guys did this, pulled this all together, and that it's getting some attention.  They did responsibly disclose everything they found to all of these manufacturers.  I mean, in the case of HP, a score, scores of vulnerabilities were disclosed.  It's not clear whether they have been fixed or will be fixed.  History has not been kind to printer security.  We haven't seen it exploited extensively.  But as we know, more and more, if something can be done, it will be done.  Google was kind enough to reward this group's findings with $3,133.70, which of course we know is LEE1E, essentially, in LEET.



LEO:  Oh, it's upside down.



STEVE:  Exactly.



LEO:  I was trying to think, it's like Euler's number?  What is that number, 31337?  Because 3 is E and 7 is L.



STEVE:  Right.



LEO:  And there's one T?



STEVE:  So I guess, yeah, it's trying to be ELEET.



LEO:  ELEET or something.  Oh, I get it.



STEVE:  They're elite.



LEO:  That's funny.



STEVE:  So, yes, just another little thing to worry about on a cold winter's day, whether anything has crawled into your printer and might not have your best interests at heart.



LEO:  Wow.  That's just...



STEVE:  Yeah.



LEO:  Somebody in the chatroom said, and I completely concur, "I would hate to be a corporate IT guy these days."  Just the stuff you'd have to keep track of.



STEVE:  Oh, yeah.  And you're then - you've got the target on you.  When something does happen, your boss is like, well, how did this happen?  I thought you were in charge of this.  I thought you were supposed to be some great security genius.  It's like, uh, yeah, but our printer just got us.  You know, who thought that was going to happen?



LEO:  I'm not that big a genius.  On the other hand, if you're the guy who's securing this stuff, and you do know it, you've got a job for life.



STEVE:  That's very true.  Years ago, earlier in the podcast, people used to say, you know, I've been listening to this podcast for a while, and I'm thinking there might be some career opportunities here.



LEO:  Yeah.



STEVE:  And it's like, I'm not going to have any hair left, and we're still going to be doing this podcast.  Actually, I don't have that far to go.



LEO:  I think it happened, Steve.



STEVE:  Maybe you, Leo, will not have any hair left.



LEO:  I think it happened.  Steve Gibson's at GRC.com.  That's where SQRL lives.  That's where SpinRite, the world's best hard drive recovery and maintenance utility lives.  Lots of free stuff.



STEVE:  It doesn't give up.  



LEO:  It doesn't give up.



STEVE:  A SQRL that doesn't give up.



LEO:  SQRL will never die.  You'll also find Perfect Paper Passwords, a password generator if you want good, long, 64-character passwords, true random.  You said it, but how do you know it's - you're not using pseudorandom number generator, I mean, for those passwords.



STEVE:  Yes, I am.



LEO:  Oh, you are.



STEVE:  I am.  Although it is running through a cryptographic cipher, with a secret key.  So it's completely unpredictable and...



LEO:  It's getting entangled.



STEVE:  Yes, and guaranteed not to repeat.  So it cannot repeat, and it's unpredictable.  That's the two things you want.  It's never use the same one again, and unpredictable.



LEO:  We've been meaning to do a hardware random number generator.  What do you do, a diode?  I can't remember what you use?



STEVE:  Yeah, yeah.  You basically push a diode into breakdown, and the actual passage of electrons force back through the diode.  The diode doesn't want to let them through.  But if you put enough voltage on it, but limit the current, it creates noise.  And it is quantum-level noise.  So if you then take that and digitize it and then whiten it, because it can be - it's noisy, but it's not exactly uniform.  So what you can do is you can take pairs and XOR them in order to whiten it and then end up with true hardware randomness.



LEO:  I presume that LastPass and other passwords vaults, when they generate their random passwords, are doing something like you do, right, somehow?  It's got to be a pseudorandom number generator.  That's all computers can do.



STEVE:  Yeah.  They could take some seed entropy from the server and then mix their own local entropy with it.  And again, remember, as I said earlier, when you add entropy to entropy, you never get less.  You only get more.  And so it makes sense to sort of like start with some and then add some so that you can trust the source.



LEO:  Got it.  That's why I like this show.  You learn so much.  You can also find the podcast at GRC.com, audio and full transcriptions.  Steve takes money out of his own pocket to pay to do that, and I think that's so great.  I really appreciate your doing that.  It gives you a searchable database.  So if you're looking for any particular phrase or term, you can literally google, do site:GRC.com and google a term, and you'll find that podcast, and you can listen to it or read the transcript.



STEVE:  Yeah.  And as this library gets increasingly deep...



LEO:  Oh, it's amazing.



STEVE:  ...I'm seeing lots of interaction in Twitter of people saying, oh, just go over to GRC, and you'll find the podcast where we talked about that.



LEO:  It's becoming kind of a compendium of everything you need to know about computing in one show.  We have audio and video, if you want to watch Steve gesticulate.  He's a fabulous gesticulator.  And the video of that, the proof, is at TWiT.tv/sn.  Or you can always watch live.  We do the show Tuesday afternoon at 1:30 Pacific, 4:30 Eastern, 21:30 UTC.  So you can watch it live.  We are now on YouTube.com.  The YouTube Live is at YouTube.com/twit.  But you'll also find a link there to the show after the fact, so you can watch all the previous shows on video there at YouTube.



You can watch live.twit.tv/live or live.twit.tv or use one of the many apps.  There's five Apple TV apps.  There's Roku.  There's a Roku app.  We commissioned that from Craig Mullaney at ShiftKeySoftware.  Lots of places you could watch live.  If you are live, go in the chatroom, irc.twit.tv.  If you want to watch after the fact, same thing.  We've got podcasts, but there's also a podcatcher for your particular platform everywhere.  Just search for Security Now! in that, and you'll find it and subscribe, and that way you get it every week.  Thanks, Steve.  We'll see you next week.



STEVE:  Okay, my friend.  See you in February.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.










GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#598

DATE:		February 7, 2017

TITLE:		Two-Armed Bandits

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-598.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week, Leo and I discuss printers around the world getting hacked!  Vizio's TVs really were watching their watchers, Windows has a new zero-day problem, Android has an easy-to-hack pattern lock, and an arsonist's pacemaker rats him out.  A survey finds that many iOS apps are not checking TLS certificates.  Courts create continuing confusion over email search warrants.



In a blast from the past, SQL Slammer appears to return.  Cellebrite's stolen cell phone cracking data begins to surface.  We discuss some worrisome events in the Encrypted Web Extensions debate and that non-Windows 10 users are not alone.  We answer a couple of questions, report on a terrific sci-fi series, offer a bit of other miscellany, and end with a fun story about one-armed bandits being hacked by two-armed bandits.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson's here.  Boy, does he have a lot to talk about.  A lot of tech news, including that hack on the one-armed bandits, on the slot machines.  What's going on with the iPhone apps?  Thousands of them, apparently, or at least hundreds that are hackable.  Steve will give you the story and the mitigation and of course all the security news.  Coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 598, recorded Tuesday, February 7th, 2017:  Two-Armed Bandits.



It's time for Security Now!, the show where we cover the latest security and privacy news with this guy right here, Steven Gibson of the Gibson Research Corporation.  He's armed and ready with his super triple quadruple venti latte.  



STEVE GIBSON:  You betcha.  Hey, Leo.  Great to be with you again.



LEO:  Good to see you.



STEVE:  Episode where we're closing in on number 600.  This one is 598.  And I named this one "Two-Armed Bandits."  And in fact we're kicking off here on time, and I just got the - I forgot even to set the intensity and camera settings.  So people are going to be annoyed that I didn't tweet the show note link beforehand.  If anyone is wondering where it is, it's the same format as always, but it's 598 rather than last week's 597.  So I'm sorry for not putting it into my Twitter stream.  I'll do that in two hours, once we're done.  But anyway, there was a really fun story that was perfect for the podcast about a problem with one-armed bandits, which of course is the fun term for slot machines, not being random enough.



LEO:  Uh-oh.



STEVE:  And the consequences of that, which is how we'll wrap this up.  But we had a lot of news, as we have been for the last few weeks.  Speaking of the devil, because of course last week was "Traitors in Our Midst," 150,000 printers around the world got hacked.



LEO:  Oh, that wasn't what precipitated the show?  That was after the show?	



STEVE:  Yes.



LEO:  Oh.  Wow, you were prescient.



STEVE:  So a little bit of prediction there.  We also have Vizio's TVs that have been - the corporation has been slapped by the Federal Trade Commission when it was discovered what they were doing, that they really were watching their watchers.  Microsoft has an unpatched zero-day problem as a consequence of not responding to the security researcher who found it within 90 days.  Hopefully - we're going to have to wait a week because this is one of those months where the second Tuesday, Patch Tuesday, is as late as it could possibly be because they missed the first Tuesday by one day.  Last one, of course, was January 31st was our last Tuesday.  So this is the first Tuesday of the month, meaning that maybe they're going to get this fixed.



And then I just loved this story.  Some researchers examined camera footage of people using the Android pattern lock and realized that's really not hard to guess what their pattern is.  So they demonstrated that in a research paper that we'll discuss.  An arsonist's pacemaker ratted him out.  We have a survey finding that many iOS apps are still not checking TLS certificates and the consequences of that.  The courts have just made a ruling on Friday which has created new confusion in the email search warrant mess.  I imagine you and Jeff may want to talk about this because it involves Google.  We'll talk about that ourselves here in a second.  We have a bizarre blast from the past that actually predates the podcast.  There were actually security events, Leo, before the Security Now! podcast.



LEO:  What?  No.



STEVE:  Even though this seems like it's been going forever.  Yes, this was the SQL Slammer worm has returned.



LEO:  I remember Slammer.  I remember Slammer, yeah.



STEVE:  Yeah, yeah.  Cellebrite's stolen data has started to surface.  We talked a couple weeks ago about how 900GB had been exfiltrated from them.  Now we have some sense for what it is, and it's not actually very impressive.  There's some worrisome movement in the encrypted web extensions debate, another topic that you and Jeff and company may want to discuss.  I think it was Business Insider, no, no, shoot, Financial Times?  I can't remember who.  Anyway, we'll get to it when we get to it, about just reminding us that non-Windows 10 users are still not alone.



I'm going to answer a couple quick questions.  I have a report and recommendation without reservation of a new sci-fi author and series that I finished.  I was up till 2:30 a.m. and then 3:30 a.m. on the final two nights last week, Wednesday and Thursday.  I just could not make myself stop reading it, and I ended up finishing the fourth book in the series, and it was really fun.



LEO:  Wow.



STEVE:  A bit of miscellany, and then we'll talk about two-armed bandits ripping off one-armed bandits.  So the Picture of the Week I got a kick out of.  This was actually - this was taken off-angle from the large presentation screen in a security presentation.  And just for the heck of it, I removed the perspective distortion, but it still has some parallelogram distortion that I didn't get rid of.  It's like, okay, well, the message is coming through anyway.  Anyway, so I got a kick out of it because this was in the context of the problem that we continue to have, I mean, arguably will always have, with well-meaning notifications that users are supposed to pay attention to because this is important.



I mean, of course, the classic example was remember how my own operations manager and bookkeeper and accountant and Girl Friday who's, like, made my life possible for the last - she's been with me now for 23 years - how I set her up with a mirrored RAID disk array, and it started complaining that one of the drives was dead, and press Enter to continue.  And so she did, for about another couple years, until the second one finally died.  And then when I said, "Well, Sue, the whole point of having two drives is that if one dies, then you can limp along on the other one."



LEO:  But fix it.



STEVE:  "But you should, like, let me know."  And as it happens, that turned out to be a SpinRite recovery story I was able to share because, even though both drives had completely gone belly-up, I ran SpinRite on the most recently dead one because of course it had the most recent information, and SpinRite was able to recover it, and we lost nothing.  But I said to Sue, "Okay, listen.  Anytime something like this happens again, the point is there's an early warning system here.  So don't just say, oh, look, I can get my work done if I just say okay."



So this dialogue, in the title it says:  "Website Certified by an Unknown Authority."  Okay, whoops, wait a minute.  That would be an indication, our listeners know, of a man-in-the-middle attack.  Something has intercepted a connection such that the identity of the other end was not verifiable.  So of course what the user sees is the dialogue.  It just reads - it's got the yellow triangle warning - "Something happened and you need to click OK to get on with things."  Which, you know, that's how everyone just sort of says, okay, how do I just get rid of this annoyance?  And in the details it says:  "Certificate mismatch" - oops, I just hit the spacebar and paged down by mistake.  "Certificate mismatch security identification administration communication intercept liliputian snotweasel foxtrot omegaforce."



LEO:  So this is a real error message?



STEVE:  No, no, no.  They're just sort of saying nobody reads this crap.



LEO:  Yeah, yeah, yeah.  But, I mean, it was - some program does this.



STEVE:  Oh, I think somebody made it up for the presentation.



LEO:  Oh, okay, yeah.  My favorite part is the buttons.



STEVE:  Exactly.  The button, and we've all seen this, this button is labeled "Technical Crap," dot dot dot.



LEO:  So if you want that, you can get it, yeah.



STEVE:  So if you haven't had enough yet because "snotweasel foxtrot omegaforce" wasn't sufficiently confusing.  And then there's three radio buttons:  More technical crap, Hoyvin - what is this?



LEO:  Hoyvin-Glayvin.  It's Jerry Lewis.  Hoyvin-Glayvin.



STEVE:  And then the third one is "Launch photon torpedoes."  Meaning nobody reads this.



LEO:  Hoyvin-Glayvin.



STEVE:  All they get is, wait a minute, I want - where's my document?  What do I - you know, okay, fine.  Do I push OK or Cancel?  Just get this out of my way.  And so we have - this is a sort of a fun reminder that we have a fundamental problem with the breach, the gap that exists between our systems, which are designed by people who know what is important.  And basically we've made the dialogues look pretty because users want 3D and shadowing and so forth.  But we still haven't managed to solve the underlying problem, which is essentially users don't care, and they're just going to click through.



And so ultimately this stuff has to be designed, I mean, if you literally take the word or the expression "foolproof," break it down into two words.  Unfortunately, that's what we need is somehow to make this stuff just foolproof, that is, don't try to tell them.  Don't teach them.  That was always my tech-y friend Bob's biggest complaint.  Or actually it was the complaint of the people he was working with because he was always trying to teach them what this stuff means.  And they're like, no, just what button do I push?  Just I don't want to know this.



LEO:  Don't explain it, just tell me what to do.



STEVE:  I don't want to go to college or school of Bob.  I just want to know how to make the lights turn green.  And he'd just shake his head and say, well, then, you're on your own.



LEO:  I give up.



STEVE:  So last week the topic of the podcast was "Traitors in Our Midst."  And independent of that, and it had to have been going on already, it couldn't have been inspired by the podcast because I looked at some of the work this person did, and it's some nice scripting.  A security researcher who calls himself "stackoverflowin," with no "G," stackoverflowin, wrote a script to scan the Internet for printers publicly exposing their IPP, which we discussed that week, Internet Printing Protocol; LPD, the Line Printer Daemon port; or port 9100.  Just scan the 'Net.  Now, there was, again, misreporting as a consequence of people taking literally sort of his joke, like what he had printers print.  But in the last week I've received a bunch of tweets from people.  There's also a fun one, Leo.  There's a link - I didn't turn it into a link.  It's a Twitter picture below that first picture from Priscilla.  She tweeted:  "The printer for our POS" - and I'm assuming that in this case she means point of sale.



LEO:  Yes, not piece of... 



STEVE:  Yes, "systems at work got hacked."  And she says, "LMAO," and then she posted a tweet which actually shows a strip of point-of-sale paper.



LEO:  Yes, carbon paper, too.  I mean, it's, yeah.



STEVE:  Yeah, with this thing.  And so it says:  "Read.  Stackoverflowin the hacker god has returned.  Your printer is part of a flaming botnet operating on Putin's forehead utilizing BTI's (break the Internet) complex infrastructure."  Okay, none of that is true, but he was just having fun.  And so one of them shows, I don't know, a terminal and then a computer.  And the screen says "Hacked, hacked, LOL, just kidding."  And that was one of the messages that he had 150,000 printers around the globe print.  And then on the next page of the show notes is a different message, where he did an ASCII art bot and basically said the same sort of thing.



So, okay.  So first of all, there was no botnet.  Nothing was technically pwned or owned.  But what was surprising, although it shouldn't have been from our reporting last week, is that 150,000 printers are publicly exposed, including this POS system.  That's like, why?  Why would you have your port exposed?  And I thought about it, and I thought, okay, there's a takeaway for our listeners because our advice, Leo, yours and mine, for more than a decade, we can say that now because we're in year 12 of the podcast:  Disable Universal Plug and Play.  That has been our mantra because that Universal Plug and Play is now a widely used interface on the LAN side of routers which is enabled by default because they want to be helpful, and they don't want tech support calls, and everybody else does it, so why shouldn't they?  It allows any device on your LAN to make itself public.  That's arguably what it was for.



Now, Microsoft gets away with this, this was their standard, because they've also got a firewall in the PC.  So they're able to be a little more careful in using Universal Plug and Play.  But so people understand, you can log into your router, and you can deliberately map ports inside.  That is, you're able to essentially bypass the router's Network Address Translation stateful firewall.  Meaning, and we've talked about this often, that unsolicited packets are ignored.  It's only packets returning from an initial outbound connection which are automatically allowed back in and are then routed to the original sending machine.



But there are some things like Skype, for example, back in the peer-to-peer days of Skype, and gaming, where in order to be on a gaming network, they're greedy with the ports, saying there's like four or five ports you need to have opened.  And so because that was too hard, I mean, and arguably, I'm not saying it isn't, it's a problem where convenience and security are colliding because on one hand your Xbox wants to have some ports exposed that we're trusting it to handle responsibly.  So it uses Universal Plug and Play to open those ports behind our back.  That is, the router has it.  It's enabled by default.  Any device, there's no security.  That was the controversial feature of Universal Plug and Play because, again, they opted for ease.  There is no security on it.



So a device on your network needs no permission to talk to your router and say - and in this case we're talking about a printer.  This has to be how 150,000 ports are open.  Printers are trying to be helpful.  They're saying, oh, what if you're roaming, and you want to print to me from out on the public Internet?  Well, if you're going to do that, I have to make a port open.  So when you connect your printer in your LAN, it reaches out to the router and opens ports behind your back, exposing it to the Internet, which is where 150,000 of these ports came from, most of them probably unintended.  So here's, again, so we've had an example of the number of printers that are probably exposed and the mechanism by which printers are able to make themselves public without their owners knowing it.



One thing you can do, because we know what these ports are, the port for the Internet Printing Protocol, IPP, is TCP/631.  For LPD it is TCP/515.  And for the RAW protocol it's port 9100.  Now, because those are not - and as I'm saying this, I'm thinking, well, it's time maybe to revisit this.  Because these are not your typical widely used service ports like FTP and SSH and web and so forth, they're not in GRC's ShieldsUP! small port list.  They are on the service port list, except for 9100, because I go up to a little bit above 1024, but not to 9100.  So if you did a full service port scan, look to see if 631 and 515 are showing as open, in which case something in your network, probably without your permission or knowledge, has opened those ports and is listening to - of course maybe you already got the robot ASCII art a couple days ago, and so you know that somebody somehow got into your printer.



The point is that this was just - they just used it as a printer this time.  But this is a nice heads-up saying, okay, you've got your warning shot.  Your printer is exposed.  And we know from what we talked about last week from the research that was done that because, for example, PostScript is a full Turing-qualified language that is able to execute code, you just don't want your printer exposed.  It creates an opportunity for someone to get into your network, onto your LAN, with a computer, because of course printers are computers.



LEO:  Is it sufficient to turn off UPnP?



STEVE:  I would say turn off UPnP and reboot.  You could check from - the advantage that any port scanner like ShieldsUP! provides is it's got a public-facing view.  You really can't see what's going on inside your network because there's just so much going on.  What matters is what's exposed to the public.  And so ShieldsUP! will, by looking back at your IP address, will give you a warning if anything that you are not expecting is exposed.  And so it's worth doing a little ShieldsUP! scan every few months, just to make sure that none of the IoT devices has gotten up to any mischief.



But unfortunately, if we had a quiet LAN, if it was the way it once was, where all we had was some probably well-behaving PCs from conscientiously responsible security-oriented companies, then it's like, okay, well, that's probably okay.  But now we've got Vizio TVs and light bulbs and the whole IoT thing.  And we know that many of these companies are not being responsible about security.  So if we give them access to Universal Plug and Play, it's just game over.  They'll open ports for, they will argue, to provide functionality that their users want.  But the problem is, in doing so, it creates entries into our network.



So the problem is, if you disable Universal Plug and Play, some stuff might break.  Something might stop working.  Now, that alone is a useful test to verify that that device was requiring Universal Plug and Play.  Normally it's possible to create your own static mapping.  That's, you know, the security-conscious gamers don't just turn on Universal Plug and Play in order to get their Xbox to be connected into the big network.  They look at the five or six ports that are necessary, and they manually map those to their Xbox, and they get the same functionality without just letting it be open for the whole network.



And really, the fact that there's no security, that's the concern.  But it was a choice, a tradeoff that was made.  And arguably it was made, as I said, at a time when there was less potential vulnerability in the LAN because it was just going to be some Windows and Mac PCs, not refrigerators.  Someone sent me a tweet of a Walmart slow cooker that had WiFi access.



LEO:  Oh, geez.



STEVE:  And it's like...



LEO:  I just got a WiFi-connected oven, you know.



STEVE:  Right.  I think I watched you, yeah, you were talking to Stacey about that last Wednesday. 



LEO:  Yeah, the June Oven.  And actually it's funny because I'm ready to cook last night and had salmon steaks all out, and the oven wasn't working.  It was in a boot loop.  So I went online, and they have chat support.  And I said, "It's in a boot loop."  They said, "Oh, yeah, you need to reset your oven.  Unplug it, hold down the button, and reboot."  And I see the Android screen for refresh, you know, reloading the bootloader and refreshing the ROM.  So I refreshed the ROM, and it all worked again.  So we, I mean, we really - it's amazing, you know.  And it won't work unless you connect it to the WiFi.  It gets recipes and stuff. 



STEVE:  Ah, what could possibly go wrong?  



LEO:  What could possibly go wrong?



STEVE:  So speaking of Vizio and IoT, the Federal Trade Commission has slapped them.  I consider this a slap because it was $2.2 million.  Actually, there were two types of...



LEO:  FTC's action happened several years ago.  Vizio settled today.  That's the news story.



STEVE:  Okay.  That was the news.  Okay.  Good.



LEO:  So we've known about this for a couple of years.



STEVE:  Yes.



LEO:  But what's interesting is Vizio agreed not to do it anymore.  I wonder if they kept doing it until they settled.  I don't know.



STEVE:  Yeah.  I'm not sure about the timing here.



LEO:  I'll tell you what, I'll look it up, and I'll give you a recap.



STEVE:  So the FTC said:  "Consumers have bought more than 11 million Internet-connected Vizio televisions since 2010.  But according to a complaint filed by the FTC and the New Jersey Attorney General, consumers didn't know that, while they were watching their TVs, Vizio was watching them.  The lawsuit challenges the company's tracking practices and offers insights into how established consumer protection principles apply to smart technology."



So then it says:  "Starting in 2014, Vizio made TVs that automatically tracked what consumers were watching and transmitted that data back to its servers."  So I think you're right, Leo.  It does mention that they retrofitted older models.  But this sounds like probably shortly after that behavior was found, the FTC said, okay, hold on, folks.



LEO:  Yeah.  And they agreed to delete everything collected before March 1st of last year.



STEVE:  Correct, correct.  What I liked was a little bit of interesting information here, and that was that on a second-by-second basis, the Vizio TV was collecting a selection of pixels from the screen that it matched to a database of known television, movie, and commercial content.



LEO:  Otherwise how would they know what you're watching?



STEVE:  Yeah.  I thought that was just like, oh, wow.



LEO:  Yeah.



STEVE:  So, well, so they went to some serious effort in order to track people.  And of course the set itself might, if it was the tuner, it would know what you were tuned to.  But if it was just the screen, like for example playing it...



LEO:  Right, in most cases, right.



STEVE:  Right, exactly.  So if you had another piece of equipment doing the channel selection and tuning, or a DVD player, it would be saying, okay, what movie is this?  And so the idea of actually capturing video off of its own screen and then sending it back for identification is just like, okay.  Well, again, they put some effort into this.



LEO:  This is a longstanding practice, I've got to point out.  Fifteen years ago...



STEVE:  This level of monitoring?



LEO:  Yeah.  Fifteen years ago we were offered at TechTV data from people's TiVos that had second-by-second viewing, graphs of what they'd rewound.  And TiVo at the time, and I imagine still does, was selling that to networks for programming.  It's like ratings.



STEVE:  Yeah.



LEO:  And so that's - now, it's a little interesting that the monitor is doing that.  And obviously they have to jump through more hoops.  The TiVo knows exactly what you're watching; right?



STEVE:  Right.



LEO:  I bet you anything Roku has that information.  Apple TV has that information.



STEVE:  Yeah.  I think they're monetizing it.  You're right.



LEO:  Of course they are, yeah.  It's like ratings.  It's better than ratings.  It's gold.



STEVE:  Right.  And the FTC's argument was that consumers didn't understand.  And they called it, Vizio called it "Smart Interactivity" feature, although it didn't actually provide smart interactivity.  And, I mean, we've been talking about recent FTC actions, for example, like in going after, who was it, the router maker, D-Link, I think it was.  The FTC was stomping on D-Link.  And I think that's going to be the arm of enforcement that we rely on for beginning to hold companies accountable for their behavior because that's what we've got here in the U.S.



LEO:  You know, it's interesting that, as the FCC rolls back all of its regulations under the new administration, one of the things that they seem to want to do is to put all of the burden of this on the FTC, the Federal Trade Commission.  And former Commissioner Tom Wheeler in a really good interview with Susan Crawford yesterday on Backchannel said that that's been the goal all along because the FTC is swamped because they have to do everything from dishwashers to microwave ovens.  And they have an enforcement arm that's deceptive practices, but it's not going to be sufficient to protect Net Neutrality or any of these other things.  But so all these companies like Comcast would just love the FTC to be in charge of enforcement because they know that they'd be - never hear from them.



STEVE:  Because they know nothing will be able to happen; right.



LEO:  Yeah.  Well, look at this action.  This is stuff Vizio started doing in, what, 2014.



STEVE:  Yes.  Yes.



LEO:  So, yeah.



STEVE:  So a security researcher, Laurent Gaffie, gave Microsoft a 90-day heads-up to patch a flaw he found in - and this is just now - a Windows client processing of the Server Message Block, the SMB protocol, which is better known as Windows File and Printer Sharing.  And of course this is a long, you know, this is a core Windows Microsoft protocol that's been around forever and has also been a constant source of problems.  There were some stories a few years back of, because Internet Explorer would actually honor smb:// URLs, it was possible for websites that wanted to fingerprint their visitors to put an smb:// URL in the web page, and that would cause a Server Message Block connection from the user's computer back to the website.  And through that you've got all kinds of information.



So anyway, so Laurent gave Microsoft 90 days, Microsoft did not respond in 90 days, and so has just posted a Python proof-of-concept which allows a malicious server to crash a Windows client.  And that's effective through Windows 10.  I don't know how far back it goes.  In the coverage I only saw Windows 10 and Windows 8.1.  So I don't know one way or the other whether Windows 7 or, for example, XP would also be affected.



However, US-CERT has further warned that the vulnerability could do more than just crash a system, which of course is not surprising to us.  As we know, these things start as crashes.  And then when hackers look at them very carefully, they're able typically to extend that from just sending the program off into the wilderness - which is what we call a crash.  It's extended into having the program execute code which they provide.  And in this case US-CERT has warned that it can also execute arbitrary code with Windows kernel privileges.  And the reason is the SMB protocol is deeply wired into the OS.  So it is the kernel which is making this connection.



And so if you can do some sort of a buffer overrun-style vulnerability, then you're executing code with the permission of that service, which in this case is SMB.  I'm hoping, because this, I mean, this is not showstopping, end of the world, because it does require that a client establish a connection to a server.  The problem is there appear to be many ways to make that happen.  Because it is the core protocol, the core networking and communications protocol in Windows, it's always there, and we have a long history of exploits against it.



So I'm hoping that the 90 days, essentially that Microsoft needed 97 days, that is to say, that next Tuesday's Patch Tuesday will fix this because there is a - it's on GitHub now, Win10.py, a very nicely written Python script which pretends to be a Server Message Block server and is able to crash any Windows machine that reaches out to it.  And as we know, and as US-CERT said, today it crashes it; tomorrow it could be executing code on it.



The question, I think the big question is, is there still a way to get a website to cause Windows to make an SMB connection?  I immediately researched that when I saw this story, and it wasn't clear, that most of the coverage is many years old where this was happening, and I'm hoping that that's not the case.  There is a mitigation.  And that is we know what SMB protocol uses.  It uses TCP protocol over ports 139, and 445 and UDP over ports 137 and 138.  So if you're in an enterprise, and your responsibility is the security of your enterprise, and you don't have a reason for machines in your Intranet, in your LAN, to be connecting to remote SMB servers - and if you do, you probably need to rethink your architecture because that's just a scary thing because SMB has been such a trouble-prone protocol.



My point is, if you don't know you need to permit outgoing connections to remote shares, remote SMB protocol, then blocking TCP 139 and 445 and UDP 137 and 138, that would be prudent.  And that would prevent this until Microsoft has it fixed.  But again, it'd just be better not to permit that unless you have a definite use case because then you're ahead of the game, not waiting for vulnerabilities to surface, but being protected from them beforehand.



LEO:  I think this is when we met was NETBIOS vulnerabilities.



STEVE:  Yes.  Yes. 



LEO:  Fifteen years ago.



STEVE:  And those quaint email scripts.  You know, it's like, oh, that's so cute, running scripting.  And again, I was railing, I mean, I was a broken record at the time.  Who needs scripting in email?  Turn that off.  And it took five years, and finally Microsoft said, oh, maybe that's not a good idea.  It's like, oh, you think?



Okay.  So I love this, Leo.  You will, too.  I remember the first time I saw Android's grid sequence lock.  I thought, that's kind of cute.  You know, you put your finger down, it's a 3x3 grid of dots, and you trace out some interconnection pattern among the dots.  And it's like, oh, you know, that's kind of cool.  That seems - it's different than, at the time, iOS's four-digit passcode, which they've extended.  It's easier than typing in a long passphrase.  You just sort of do a little swipe-y thing, and your phone is unlocked.



The problem is, I mean, the moment I saw this research, I thought, oh, that's just so cool.  Because, if you think about it, looking at someone do it, even if you didn't see the screen, they're just - there aren't, there's not sufficient ambiguity in their actions.  And it turns out that the more complicated the sequence is, the easier it is to crack.  And what the research did was these guys showed that, from 30 feet away, just by looking at video in standard static security cameras in Starbucks or wherever, somebody who's holding the phone in front of them, where you're just able to see the motion of their hand, with 95% accuracy it's possible to deduce the sequence.



And of course the reason is that, as you swipe around, if you go up, then you know that you had to have been down in order to go up.  And if you go up twice, then you know that the down where you were was all the way down because there's only three levels.  And the same thing for left and right.  So, for example, if someone did a little tiny square in one corner, well, okay, there are, what, four possible locations on a 3x3 grid where a 1x1 square could be drawn.  And you typically have five chances before you're locked out from guessing wrong.  So it's very clear that there just isn't enough ambiguity if anyone can see the relative motions of your hand.



So in the abstract of this research - and, by the way, the paper is on - I got the link in the show notes - is on Google Drive.  It's just beautiful.  They show a whole bunch of little 3x3 grids with lines drawn on them to demonstrate how unambiguous these things are.  In the abstract they said:  "Pattern lock is widely used as a mechanism for authentication and authorization on Android devices.  This paper presents a novel video-based attack to reconstruct Android lock patterns from video footage filmed using a mobile phone camera.  Unlike prior attacks on pattern lock, our approach does not require the video to capture any content displayed on the screen.  Instead, we employ a computer vision algorithm to track the fingertip movements to infer the pattern.



"Using the geometry information extracted from the tracked fingerprint motions, our approach is able to accurately identify a small number, often one, candidate pattern to be tested by an adversary.  We thoroughly evaluated our approach using 120 unique patterns collected from 215 independent users, by applying it to reconstruct patterns from video footage filmed using smartphone cameras.  Experimental results show that our approach can break over 95% of the patterns in five attempts before the device is automatically locked by the Android OS.



"We discovered that, in contrast to many people's belief, complex patterns do not offer stronger protection under our attacking scenarios.  This is demonstrated by the fact that we're able to break all but one complex pattern with a 97.5% success rate, as opposed to 60% of the simpler patterns in the first attempt.  Since our threat model is common in day-to-day life, this paper calls for the community to revisit the risks of using Android pattern lock to protect sensitive information."



Anyway, I just thought that was, you know, it's the kind of thing where, again, in retrospect it's like, oh, yeah, that would not be that difficult to do.  But it's sort of - so somebody who wasn't thinking about the attack angle, so to speak, said, oh, this is interesting and different and fun.  Unfortunately, it's just not very secure.  If anyone can see you even from a distance do it, there just isn't much guesswork involved in figuring out what you did.



LEO:  They kind of acknowledge that.  When you choose your screen lock, they rank them from least secure, which is the pattern lock, to the most secure, which is a strong password.  So people do get a little bit of advice, anyway, from Android when they set that up.



STEVE:  And probably people, again, as we know, people are, A, not actually that concerned about their security and will opt for convenience.



LEO:  They want something you can do with one hand.  The nice thing about pattern lock, you do it with one hand.



STEVE:  Yup.



LEO:  So if somebody is in the situation, they're a strap hanger, whatever, and they need to - on the subway, you know, and they need to use one hand, they almost always will choose pattern lock.  And usually they'll choose very easy-to-deduce patterns.  John C. Dvorak was always trying to figure it out from the grease on my phone.  Because if you do it enough, it will kind of form a pattern on the phone.



STEVE:  Yeah, yeah.  So Network World had some reporting on this.  And I paraphrased it a bit.  And their title was "Cops use pacemaker data to charge homeowner with arson and insurance fraud."  They said:  "If you're dependent upon an embedded medical device, should the device that helps keep you alive also be allowed to incriminate you in a crime?  After all, the Fifth Amendment of the U.S. Constitution protects a person from being forced to incriminate themselves.  Nonetheless, that's what happened after a house fire in Middletown, Ohio.



"WCPO Cincinnati" - which is a TV station - "caught video of the actual fire, as well as delivered news that the owner's cat died in the fire."  And then they editorialize.  "As a pet owner, it would be hard to believe that a person would set a fire and leave their pet to perish that fire.  The fire in question occurred in September 2016," so a few months ago.  "The fire department was just starting an investigation to determine the cause of the blaze.  A month later, 59-year-old homeowner Ross Compton was arrested and charged with felony aggravated arson and insurance fraud.  The cause of the fire was still undetermined, but it had resulted in $400,000 in damages to the house and contents of the 2,000-square-foot home.



"Fire investigators knew there had been," quote, and this was suspicious - "'multiple points of origin of the fire from the outside of the residence.'  At the time, the police cited inconsistencies in Compton's statements when compared with the evidence from the fire.  There were additional conflicting statements given to the 911 operator.  Compton had said 'everyone' was out of the house, yet the 911 operator also heard him tell someone to 'get out of here now.'  In the 911 call published by WLWT5, an out-of-breath Compton claimed he had 'grabbed a bunch of stuff, threw it out the window.'  He claimed to have packed his suitcases, broken the glass out of a bedroom window with his walking stick, and tossed the suitcases outside.  Compton also told the dispatcher he had 'an artificial heart,'" which he didn't have.



"After this, things really get interesting because police investigators used data from Compton's" - well, maybe it was artificial because I had - well, no, the term is "pacemaker" elsewhere.  So "...Compton's electronic heart device against him."  And the question is, "Isn't that self-incrimination?  Can a person plead the Fifth when it comes to self-incriminating data collected from their medical device?"



LEO:  I don't think so.  I'm no judge, but...



STEVE:  I agree.



LEO:  They can collect your fingerprint.  I don't know why they couldn't collect pacemaker information.



STEVE:  Yeah, I agree.  "Police set out to disprove Compton's story about the fire by obtaining a search warrant to collect data from Compton's pacemaker.  WLWT5 reported that the cops wanted to know 'Compton's heart rate, pacer demand and cardiac rhythms before, during, and after the fire.'"  Then on January 27, just two weeks ago, the Journal-News reported that court documents stated:  "A cardiologist who reviewed the data determined 'it is highly improbable Mr. Compton would have been able to collect, pack, and remove the number of items from the house, exit his bedroom window, and carry numerous large and heavy items to the front of his residence during the short period of time he has indicated due to his medical conditions.'"



"Middletown Police said this was the first time it had used data from a heart device to make an arrest, but the pacemaker data proved to be an 'excellent investigative tool.'  The data from the pacemaker didn't correspond with Compton's version of what happened.  The retrieved data therefore helped to make the indictment."  So, yes, another instance of, I mean, we've talked about malicious attacks against things like insulin pumps and pacemakers in the past.  And then we were just recently talking about somebody, there was a murder in a hot tub, and between 1:00 a.m. and 3:00 a.m. some huge amount of water was monitored and recorded from the person's IoT water meter.  And so we're seeing increasing instances where police are being creative in gathering forensic evidence from all of the electronic things that surround us.  Yikes.



So Will Strafach, S-T-R-A-F-A-C-H, Strafach, maybe?



LEO:  I guess.  I was talking about him, too, earlier, yeah.



STEVE:  Yeah.  The president of the Sudo, I love that, the Sudo Security Group, as in S-U-D-O...



LEO:  And by the way, Rene told us on MacBreak Weekly this morning, well-known guy, highly respected.  So listen.  What he's saying is right on.



STEVE:  Yup.  He posted, well, and he's got an interesting service that we'll talk about that they're just in the process of launching.  So he posted yesterday on Medium, he said:  "During the development of our web-based mobile app analysis service" - and that's what we'll talk about in a second, it's called verify.ly, V-E-R-I-F-Y dot L-Y, so verify.ly, which now it's got a page out so listeners can go check it - "verify.ly," he says, "it was essential to have a clear understanding of the most common security issues which plague mobile applications today.  Automatically scanning the binary code of applications within the Apple App Store en masse allowed us to get a vast amount of information about these security issues."



Okay.  So what they've got is they've got a static binary analyzer that scans Apple iOS applications, looking for problems that it's able to identify just by doing like a first-degree automated reverse-engineering of what the code apparently is.  He says:  "I will present some findings within this post which I believe to be in the public interest, related specifically to iOS applications which are vulnerable to silent interception of normally TLS-protected data while in use.  Our system flagged hundreds of applications as having a high likelihood of vulnerability to data interception, but at this time I will be posting details of the connections and data which I was able to fully confirm as vulnerable using a live iPhone running iOS 10" - and he has in quotes "malicious," meaning his own test proxy server - "to insert an invalid TLS certificate into the connection for testing."



So to take a break here for a second, so what he's talking about is that, as I have been, it's like been my most oft-repeated fundamental, is that without authentication all you have is the illusion of security.  If you do not know to whom you are speaking, then the fact that you're encrypting your data means nothing because modern encryption negotiates keys on the fly for security.  But if the person you're negotiating that on-the-fly key with is a bad guy, and you don't know it, then it's self-defeating.  So what this static analysis found was 76 popular iOS applications that do not verify the identity of the certificate to which they are connecting.  Which means an illusion of security, but no actual provision of security.  So he lists some highlights from what he found.



"During the testing process," he writes, "I was able to confirm 76 popular iOS applications allow a silent man-in-the-middle attack to be performed on connections which should be protected by TLS" - and in this case HTTPS - "allowing interception and/or manipulation of data [in flight] in motion."  According to Apptopia estimates:  "There has been a combined total of more than 18 million downloads of app versions of those iOS applications which are confirmed to be affected by this vulnerability."



For 33 of the iOS applications, this vulnerability was deemed to be low risk, meaning that all data confirmed to be vulnerable to interception was only partially sensitive analytics data about the device.  Maybe partially sensitive personal data such as email addresses and/or login credentials, which would only be entered on a non-hostile network.  So about half of them, a little less than half, while they weren't vulnerable from not verifying the identity of the certificate, it wasn't a horrible problem.



For 24 other of the iOS applications, this vulnerability was deemed to be medium risk so that they were able to confirm the ability to intercept service login credentials and/or session authentication tokens for logged in users, which of course we know would allow an impersonation attack, very much like in the old days of Firesheep.



For 19 of the iOS applications, this vulnerability was deemed to be high risk, where they confirmed the ability to intercept financial or medical service login credentials and/or session authentication tokens for logged-in users.  And we've been speaking the last couple weeks about Apple's App Transport Security feature, ATS.  It turns out that that does not contain and help block this vulnerability from working.



He posted in detail, and I'm not repeating here, the low risk, those 33 low-risk applications; but he's withholding the names of the 24 medium and 19 high-risk applications for a responsible disclosure process.  So the publishers of those applications have been notified and are being given time to fix this discovered problem before that's made public.



LEO:  And they're only low risk because they don't hand over important information.  It's the same mechanism all around; right?



STEVE:  Correct.  Correct.  



LEO:  It's [crosstalk] information can be extracted.



STEVE:  Right.  So in digging into this, I was wondering, you know, why can't Apple and their App Transport Security, ATS, and iOS simply enforce certificates?  For example, we're used to platforms that do that for us.  Like down at the OS level, if you have said to the OS, make me a connection over to here, then  that comes back failed.  If the handshake and the verification doesn't work, you get back an error saying "invalid certificate," and you're not talking to those people.



So he explains, he says:  "This class of vulnerability poses a complex problem, as application developers are the only ones" - this is the key.  "Application developers are the only ones who can fully mitigate it.  It's derived from networking-related code within iOS applications being misconfigured in a," as he puts it, "a highly unfortunate manner.  Due to this, Apple's 'App Transport Security' mechanism will see the connection as a valid TLS connection, as it must allow the application to judge the certificate validity if it chooses to do so.



"There's no possible fix to be made on Apple's side because, if they were to override this functionality" - that is, if the OS were to enforce it, as I was suggesting - "in attempt to block this security issue, it would actually make some iOS applications less secure as they would not be able to utilize certificate pinning for their connections," meaning that certificate pinning allows an application the freedom to obtain the certificate and get the fingerprint of the certificate, which is unforgeable, so that they're able to lock themselves to a given certificate remotely.



He says:  "And they could not trust otherwise untrusted certificates which may be required for Intranet connections within an enterprise using an in-house PKI [Public Key Infrastructure].  Therefore, the onus rests solely on app developers themselves to ensure their apps are not vulnerable."  So there's, like, no solution except what, I mean, and this is why what's being done here by the Sudo Group is so good is they've proactively scanned apps, developed candidate potential vulnerabilities, then verified them manually.  So their automated system can raise red flags.  But then they need to verify that it wasn't a false positive.  They do that, then they notify the application developers and say, look, you need to lock this down.  You've got X amount of time to do so because we need to, as a responsible security research firm who's discovered this, we need to let people know if you don't get it fixed.



So the only workaround he offered in the interim, and of course we don't know which applications are vulnerable at the medium or the high level of vulnerability, so we don't really know what we should be protecting against.  But he said, as a workaround, disable WiFi during sensitive transactions.  He said:  "There's a short-term trick which can be used to mitigate this type of vulnerability.  The vulnerability is very likely only to be exploited if your connection is flowing over WiFi, whether you've joined a public WiFi network or a determined attacker has force-joined your mobile device to a rogue network without your knowledge.



"Therefore, if you are in a public location and need to perform a sensitive action on your mobile device, such as opening your bank app and checking your account balance, you can work around the issue by essentially opening Settings and disabling WiFi prior to the sensitive action.  While on a cellular connection the vulnerability does still exist, cellular interception is much more difficult, requires expensive hardware, is far more noticeable, and is quite illegal within the United States.  Therefore, it is much less plausible for a casual attacker to risk attempting to intercept a cellular connection."



So, and then to companies he addressed, he says:  "If you offer an application in the iOS App Store, consider analyzing builds prior to App Store submission using our verify.ly service.  This class of vulnerability and all other possible low-hanging fruits, meaning vulnerabilities discoverable to a determined attacker who commits 24 hours of total analysis time" - not deep analysis - "can be fully detected by performing an automated scan of the binary code and giving you an easy-to-read report outlining any and all flagged issues, ensuring your customer data is safe."



And then at the end of his post he talks a little bit about verify.ly.  In fact, I went over to see what this was all about.  And their site explains that verify.ly allows you - and by "you" they mean an app developer, this is not for the end-user - to scan the binary code of an iOS application to produce a human-readable report detailing all detected common security issues and a breakdown of all useful security-related information pertaining to the app.  The app scan is performed in seconds using our proprietary automated static analysis engine, yielding actionable information regarding the security of the scanned mobile application.  No source code is required.



And then, for example, under features, they have:  Detects all common CWEs and mobile OWASP Top 10 issues within an app.  Detects hardcoded sensitive content that is easy to reconstruct - API keys and secrets, encryption keys, passwords, and more.  Detects any use of malicious, private, or risky APIs.  Detects issues related to SSL and TLS validation, like was the subject of this posting.  Detects issues with sensitive data at rest stored non-securely.  Detects code hashes against databases of known malicious code.  And generates easy-to-read report explaining issues and displaying relevant data in a high-level manner to ensure readability by the widest and potentially nontechnical audience.



And it sounds like it's in the process of coming online.  They offer a professional plan for $100 a month, which is a single user account.  A small business plan for $500 a month is coming that allows 10 users per account.  And then for an enterprise class plan they say contact them.  So a nice-looking service.  And this is - I'm glad to see this, Leo.  It's the kind of broad-based, automatable, binary-based testing that allows a company like this to provide a service of saying, you know, your app doesn't look to us like it's as secure as it needs to be.



LEO:  Yeah.  I like the name, too, verify.ly.



STEVE:  Yeah.  So again, somewhere, maybe it was there, I saw something about it being a problem that, like one of the ways this is happening to developers is there is this drag-and-drop approach which is becoming increasingly prevalent, unfortunately.  And that is that developers who are at deadline, under the gun, in a hurry, they will grab a blob of code off of GitHub or somewhere that has some functionality that they don't fully understand.  They didn't write it.



And that's the point is they just grabbed it because they needed this chunk of function.  And they drop it into their app, just assuming things about it which is not in evidence.  And they get themselves in trouble as a consequence of code having behavior different than they expect.  For example, not verifying certificates.  They just assume it's being done and don't know one way or the other.  So they get themselves and their app in trouble.  



LEO:  This is really common.  Remember the flaws that were exposed, I can't remember, was it OpenSSL where they cut and paste Intel reference code with baseband radios because why rewrite it?  Somebody wrote it.  There's the code.  I can get it from Stack Overflow or somewhere.  Why would you need it?



STEVE:  Yeah.  I think it might have been a whole bunch of routers that all had...



LEO:  That was it, the routers, yeah.



STEVE:  They just took a chunk of code that was clearly marked by Intel as "this is just to demonstrate how to talk to the registers of our hardware chip.  This is not for production."



LEO:  Don't do this.



STEVE:  But it was like, oh, it works.  We compiled it, and it worked, so we shipped it.



LEO:  We shipped it.



STEVE:  Yeah.  So last Friday, what, five days ago, confusing things further, muddying the waters, a U.S. judge ruled that Google, unlike the decision that had been reached for Microsoft, must turn over foreign emails.  And this is from - I edited this down from Reuters reporting.  And, boy, the logic is tortured here.



A U.S. judge has ordered Google to comply with search warrants seeking customer emails stored outside the United States, diverging from a federal appeals court that reached the opposite conclusion in a similar case involving Microsoft, that of course we all discussed at the time.  U.S. Magistrate Judge Thomas - and his name is Rueter, R-U-E-T-E-R - in Philadelphia ruled on Friday that transferring emails from a foreign server so - here's the tortured logic - from a foreign server so FBI agents could review them locally as part of a domestic fraud probe did not qualify as a seizure.



The judge said this was because there was "no meaningful interference" with the accountholder's "possessory interest" in the data sought.  "Though the retrieval of the electronic data by Google from its multiple data centers abroad has the potential for an invasion of privacy, the actual infringement of privacy occurs at the time of disclosure in the United States," this judge wrote.  Google replied in a statement Saturday, that is, over the weekend:  "The magistrate in this case departed from precedent, and we plan to appeal the decision.  We'll continue to push back on overbroad warrants."



And this ruling came less than seven months after the Second U.S. Circuit Court of Appeals in New York, which is what we discussed at the time, said Microsoft could not be forced to turn over emails stored on a server in Dublin, Ireland that U.S. investigators sought in a narcotics case.  That decision last July 14 was welcomed by dozens of technology and media companies, privacy advocates, and both the ACLU and U.S. Chamber of Commerce.



A few weeks ago, on January 24, 2017, the same appeals court voted not to revisit the decision.  So they were staying by it, and it had already gone to appeal; and they said, no, we're sticking by it.  The four dissenting judges, however, called on the U.S. Supreme Court or Congress to reverse it, saying the decision hurt law enforcement and raised national security concerns.  Both cases involved warrants issued under the Stored Communications Act, a 1986 federal law that many technology companies and privacy advocates consider outdated.



In court papers, Google said it sometimes breaks up emails into pieces to improve its network's performance, and did not necessarily know where particular emails might be stored.  That's going to be a hard one to substantiate, of course, because if you want to retrieve your email, you can.  So obviously Google knows how to find it.  Relying on the Microsoft decision, Google said it believed it had complied with the warrants it received by turning over data it knew were stored in the United States.  And in background, Google receives more than 25,000 requests per year from U.S. authorities for disclosures of user data in criminal matters, according to the judge's, to Judge Rueter's ruling.



And the takeaway for our listeners is email is not secure and arguably not securable.  If you want to exchange messages securely, then TNO is the only solution.  Write the message in a simple editor.  Encrypt them yourself using a high-entropy key that you arrange to share with the other party out of band, and then send that encrypted blob however you want.  If you write your message, you encrypt it yourself, then it doesn't matter what you do with it.  You could send it by Pony Express, by donkey, by unencrypted email.  It's just a blob of binary that nobody can ever access except the person who has the matching key.  And so that's the way to do it.



People, I saw a lot of people all excited about Ladar Levison's email service coming back online.  And it's like, okay.  None of this ever moves me.  In fact, Google even has a hosted S/MIME service that they're putting together to make it, again, to increase the security of email in transit by using TLS-style authenticated connections.  The problem is, as soon as it moves out of that protected environment, and if it's stored encrypted, and Google's announcement of this hosted S/MIME service says yes, you know, we store it under our own encryption, well, that means that a judge can say, okay, and we'd like you to decrypt it now, please.



So TNO applies to messages in the same way that it applies, as we've been discussing now for years, to user data stored in the cloud.  At the moment, it's more of a manual process.  But I would argue, if you need to send something to someone, and you actually do care about it being secure, then Trust No One.  Encrypt it yourself, and then it doesn't matter how you send the blob.  Nobody will ever be able to see what's inside there.



LEO:  It could be pretty much automated.  I mean, if you have GPG, Gnu Privacy Guard, it pretty much automates the process for you.  So it's about as easy as can be.  Once you've got somebody's public key, you can even say, "Every communication I have with them should be encrypted with this key."



STEVE:  Right, right.  So if both endpoints are going to be communicating, and of course that's...



LEO:  That's the only challenge.



STEVE:  Yes.  And famously, that was the whole Edward Snowden and Greenwald...



LEO:  And Greenwald couldn't figure it out, yeah.



STEVE:  Right.  Right.  So from the Blast from the Past Department, and it actually is a bit of a blast, we have the return of SQL Slammer, of all things.  Check Point's blog noted that, starting on November 28th of last year and running through December 4th, so just until a month before last, Check Point observed a huge spike in the long-dead SQL Slammer networking traffic.



Okay, so what was SQL Slammer?  This predates, as I was saying at the top of the podcast, it actually predates the podcast.  Fourteen years ago, back in 2003, many web servers using SQL database were misconfigured to expose the standard SQL query port, which is 1434, to the public Internet.  So a web server does want to expose port 80 and 443 for doing web services.  It probably has processes running on the server which want to connect locally to the SQL database using the localhost 1434 port, but never intended for that SQL database to have the port 1434 made public.



Well, it was.  Microsoft SQL Server 2000 and the Microsoft Developer Edition Tools 2000 both exposed that port and had a buffer overrun vulnerability.  And the reason it was so bad was it would work over UDP, which allows IP spoofing and single-packet transactions.  You don't have to do the whole TCP connection setup and everything.  You're able just to spray UDP packets with abandon.  And so what happened was the receipt of a single UDP packet containing this SQL Slammer worm at port 1434 would go into the server, blow the buffer, cause the execution of the packet, and, statically, it would essentially run a subprocess in the server that then itself began spewing that packet back out at random IP addresses all over the Internet.



So at the time there was a huge number of exposed SQL database servers.  And they all got found by - and the reason we called this a worm is exactly that behavior.  A worm is by definition a self-propagating thing on the Internet.  And so someone somewhere sent one of these UDP packets, they launched this worm at one SQL server, infected it, and it started spewing these UDP packets at random that eventually found other SQL servers that were exposed, which in turn began spewing at random.



And before long, I mean, like almost immediately, the Internet was filled with all this SQL Slammer traffic.  And all of the exposed SQL servers were found and infected and participating.  And so, and if any servers were, like, offline for the weekend, and then came back up online on Monday, wham.  They were getting slammed, literally, from all the other SQL servers when they happened to choose that server's public IP by chance.  So it was quite a mess.



Now, here we are, 13 years, 14 years later, and Check Point noticed a return of SQL Slammer and, you know, scratched their head.  They don't know where it came from or why.  What must have happened is that, because old code never really dies, someone forgot about this being a problem and for some reason, somehow, established a bunch of new Microsoft SQL Server 2000s on the Internet.  And somewhere there was probably an SQL Server in a closet that never stopped scanning.  Remember, we refer to this often as Internet Background Radiation, IBR.  These worms will never die.  So somewhere there was one or two that didn't die.  They just kept sending out their SQL Slammer UDP packet at random.



Something else caused a population of Microsoft old - what now, SQL Server 2000? - so 17-year-old SQL Server to be brought up on the Internet, and that closeted SQL Server ended up hitting them, and the worm began to live again.  And as the population of infected machines grew, the rate of probes being sent out by all of those grew.  And so it accelerated the rate of finding all the rest.  And before long there was a whole bunch of SQL Slammer traffic on the 'Net.  So this is the way these things actually happen.



LEO:  Steve Gibson, we are talking security on Security Now!, as always.



STEVE:  We are indeed.  So we talked a couple weeks ago about this big hack of Cellebrite, which is the well-used by law enforcement mobile hacking service. 



LEO:  I saw a number of articles that said this was the one the FBI had used to get the San Bernardino iPhone, but I don't think we know that.  I don't think we ever found that out.



STEVE:  We don't, right, I don't think we know for sure.  But we do know from the records that have been leaked that it's heavily used by law enforcement.



LEO:  Oh, they have a lot.  They make that device that can suck the data out of any cell phone in seconds.



STEVE:  So nearly a terabyte, 900GB of phone-hacking tools and databases and information of all kinds, we reported some weeks ago when the industry was covering this, were allegedly stolen from them.  We're beginning to see some of this data surfaced.  The hackers' ReadMe in the public release of some of this noted that much of the iOS-related code is very similar to that used in the jailbreaking scene populated by a community of iPhone hackers that typically breaks into iOS devices and releases its code publicly for free.



Jonathan Zdziarski, I always trip over his name, Jonathan Zdziarski, and we've talked about him often, who's a well-known forensic scientist, agreed that some of the iOS files were nearly identical to tools created and used by the jailbreaking community, including patched versions of Apple's firmware designed to break security mechanisms on older iPhones.  A number of the configuration files also referenced 



"limerain," the name of a piece of jailbreaking software created by infamous iPhone hacker Geohot.  He said he wouldn't call the released files "exploits," however, meaning Jonathan said.  Why can't I say that?



Zdziarski also said that other parts of the code were similar to a jailbreaking project called QuickPWN, but that the code had seemingly been adapted for forensic purposes.  For example, some of the code in the dump was designed to brute force PIN numbers, which may be unusual for a normal jailbreaking piece of software.



Zdziarski noted:  "If, and it's a big if, they used this in UFED or other products" - that's the Cellebrite flagship product - "it would indicate they ripped off software verbatim from the jailbreak community and used forensically unsound and experimental software in their supposedly scientific and forensically validated products."  So this is really not a surprise.  Essentially, this Cellebrite Group are just vacuuming up all of the hacking and cracking work being done by the open hacking jailbreaking community, pulling it together, sticking their own label on it, and then reselling it as a package to law enforcement, apparently not really making any changes to it.  So it's like, oh, well, you know.  Not a big surprise, but interesting to know that basically they're just recycling what's available publicly with a little more work.



There was also, Techdirt reported, some worrisome movement in the Encrypted Web Extensions standardization process.  We've talked about this a couple times.  This emerging, it's called EME, the Encrypted Media Extensions.  I was going to say, wait, not encrypted web extensions, EME, Encrypted Media Extensions.  So there's an ongoing fight and dispute, essentially, as a W3C, the World Wide Web Consortium, over encrypted media extensions, which is the HTML5 digital rights management scheme that several companies want incorporated, embedded in the web standards.  And that fight took two worrying turns recently, reports Techdirt.



First, Google slipped an important change into the latest Chrome update that removed the ability to disable its implementation of EME, the encrypted media extensions, which further neutered the already weak argument of supporters that DRM is optional.  So  it's becoming non-optional.  And in the latest version of Chrome, no longer optional.



But the other development is even more interesting and concerning.  Dozens of W3C members and hundreds of security professionals have asked the W3C to amend its policies so that its members cannot use the encrypted media extensions to silence security researchers and whistleblowers who want to warn web users that they're in danger from security vulnerabilities discovered in browsers.  So far, the W3C has stonewalled on this point.  This weekend the W3C executive announced that it would not make such an agreement part of the EME work, and endorsed the idea that the W3C should participate in creating new legal rights for companies to decide which true facts about browser defects can be disclosed, and under what circumstances.



So that's a problem.  One of the major objections to EME has been the fact that, due to the anti-circumvention copyright laws of several countries, including ours, the U.S., it would quickly become a tool for companies to censor or punish security researchers who find vulnerabilities in their software.  The director of the standards body called for a new consensus solution to this problem; but, not surprisingly, "The team was unable to find such a resolution."



So there's headbutting going on.  The new approach will be a forced compromise of sorts in which, instead of attempting to carve out clear and broad protections for security research, they will work to establish narrower protections only for those who follow a set of best practices for reporting vulnerabilities.  In the words of one supporter of the plan, it won't make the world perfect - now, this is a supporter; right?  It won't make the world perfect, but we believe it's an achievable and worthwhile goal.



So they're trying to come up to a compromise.  But the argument is this is not a real compromise because, by working to determine where the line should be drawn, it creates an important presumption, or an implicit presumption, that there is a line to be drawn.  Having a policy is a tacit endorsement of the use of DRM for censoring security researchers, to whatever degree, because the argument is not about to what degree such use is acceptable, but whether such use is appropriate at all.



So another battle here.  And maybe we're going to end up coming out, we the industry, coming out on the right side of this.  But, boy, there is a lot of pushback from the content holders that want DRM to get pushed out to the browser.  And if we have that, the question then is when vulnerabilities are found, what happens?  My take is that, if responsible disclosure is criminalized, that is, I believe researchers are still going to find, going to be digging into this and finding problems.



The question then is, what do they do with the knowledge that they gain because having done that is a criminal activity, which doesn't mean it won't happen, but it will mean that it gets forced underground.  So I think disclosures will still be made.  But unfortunately, they won't be able to be made responsibly.  They'll be made anonymously and publicly, creating reputational damage for the companies who could have fixed those problems under responsible disclosure prior to public disclosure, and also endangering end users, who'll be using products with known public and unpatched problems.  So it's a mess.  And unfortunately, it really sounds like there is a serious dispute that's not getting resolved at the moment in the W3C.  



I'm going to skip a few things that are sort of minor.  And I did have, I found an interesting question I wanted to cover from a tweet from Passeride.  He said:  "Thoughts" - he was asking for my thoughts - "on salting password hashes with characters in the username randomly and deterministically selected based on unhashed password?"  So this was interesting.  We've talked about salting passwords and the need for a per-account salt.  We were talking about it just last week, that if all passwords were salted, well, first of all, if all passwords were unsalted, that was a big problem.  But if it was a global salt that was used, that was mixed with the password, then the problem would be, once that global salt was known, then you essentially had the equivalent of an unsalted password.  The only safe thing to do was to obtain a random per-user salt, which you store along with the user's account, and use that when the user puts their password in the first time, and then also when they put it in again.



So Passeride is saying, okay, what if the salt is not random?  What if it's derived from something the user puts in, like their name and the unsalted password?  And I'm sure that close listeners to the podcast will realize that that doesn't give us any protection.  Now, using it, incorporating characters from the unhashed password complicates things.  But it doesn't make it completely random.  And so I do think it's clearly superior if the salt is unrelated in any way to what the user puts in.  It's a completely random nonce which is unique per user and isn't tied, deterministically tied, to information that the user provides.



And then Matt Clare also asks, he says:  "Why do botnets prefer IRC?"  And he suggests:  "Tor hidden too much work?  XML too fancy?  Hiding in plain sight safer?"  And he notes that he's been a listener since Episode 1.  And I remember looking at this back in my early days of looking at DDoS attacks.  And I realize that IRC is used for a couple of reasons.  It is a very simple protocol.  It's so old that it's been implemented in virtually every language.  There's Perl script and Python and, I mean, you can find IRC chat clients everywhere.  And IRC stands for Internet Relay Chat.



So that's a convenience for botnets.  What that allows is that allows botnets to all convene in an IRC chatroom on a specific server or servers, yet to be controlled by the botmaster somewhere entirely else.  And the Internet Relay Chat, because it relays chat over the Internet, will automatically propagate the commands that somebody issues anywhere in the world in IRC to the server that is listening for incoming commands in that chatroom.  And so botnets prefer it because it is, as Matt suggested, simple to use, but it also has some strong benefits in terms of the botmaster hiding themselves.  They never have to connect to the server where the bots are aggregating.  They're able to use IRC itself to propagate their commands to the botnet.  Which makes it handy.



And some final things.  I did want to note that Season 2 of "The Expanse" series has returned.  A huge percentage of our listeners really liked the first season.  It's based on a series of books that I read a couple years ago when I knew that Syfy channel was going to be producing it.  And the series does stand out as remarkably high quality compared to most of the stuff that's there.  I wanted to recommend a series of four books called the "Intrepid Saga," from an author M.D. Cooper.  He has a site called Aeon14.com, spelled with an "A."  So A-E-O-N-1-4 dot com.  That's all you need to know to find it.  Aeon14.com will take you there.



Four books in the first set.  There is a fifth one that I'm now anxiously waiting for that's - I don't have to wait very long.  It's due out in April.  But the fourth book ends that first set, so you don't have to worry about being left at a cliffhanger.  On the other hand, number five will probably be ready by the time you are.  I really enjoyed them.  And especially the fourth one.  It's good character development.  It is sort of military sci-fi, set in the far future, in the year 4000 something.  So the solar system has been fully populated.  We've terraformed some worlds.  A lot of the planets have elevators going up to rings that are circling them.  Really nice, rich, well fleshed out future vision.



And at this point Sol is getting a little crowded, and so we're sending off world ships, first to terraform, and then colony ships on multi long missions with the colonists in stasis to go and spread humanity around.  And so this is set in that sort of a backdrop.  And it's just - it's very enjoyable, a very strong female protagonist, Tanis Richards.  And I think people will like it.  And, boy, the fourth book, it just really comes together.  It's almost like someone else wrote it.  I don't know.  Maybe he came back from vacation or something before he wrote it.  But it just grabbed me.  So I can't say enough good things about it.



The Healthy Sleep Formula that we've talked about briefly and that has become extremely popular and helping a lot of people, is in the process of being made into a packaged product so it's simpler for people to take.  HealthySleepNow.com, for anybody who's interested.  There's an Indiegogo project that's being put together.  I'm not involved in it in any way, so I'm not making any representations about it, although the person who's doing it is an advocate of it and is soliciting people's feedback and input.  So HealthySleepNow.com.



And what else?  Oh, and I did get a tweet from someone named Morgan Speck, who copied me on a tweet to Hover.  He said:  "You did everything for me automatically," to Hover.  "I see now why @SGgrc was raving about how great you all are.  Thanks!"  And that gave me an opportunity to note that I just moved another domain.  I have a domain BeyondRecall.com, which I'll be using in the future as a spinoff, a high-speed secure wiping facility for hard drives and SSDs.  I moved it from Network Solutions.  I'm migrating all my stuff from Network Solutions over to Hover.  And I did that just last week.  And again, Hover, I'm so happy with the job that they're doing.



I did get actually two tweets on the first of February, so the day after the last podcast, last Wednesday.  It sort of held me in suspense.  The first one is from a guy whose Twitter handle is @arcanecode.  First one came in at 6:09 a.m., and he said:  "Not much longer for #SpinRite to run on this laptop.  Crossing my fingers @SGgrc."  So he wrote to me.  And he enclosed, he attached a Twitter pic that showed the box counting down the amount of time remaining.  And then about 3.5 hours later, at 9:29 a.m., he again tweeted:  "YAY!" in all caps.  "#SpinRite fixed my hard drive.  My laptop now works without the hard drive constantly churning.  Thanks @SGgrc."  So that was nice to see, happening sort of in real-time.



And a nice note to remind people that it's not only for data recovery and when you're worried that all might be lost.  But when the laptop isn't working the way you think it used to, or the desktop seems to pause - that's actually a really scary sign.  If your desktop seems to sort of freeze or pause for a while, there's no visual indication that your hard drive is having a problem at that time.  But that's often what is going on.  We know that because people run SpinRite, and then that all stops.  So it was the drive behind the scenes, or the OS, trying, hitting a bad spot and working with the drive to deal with it.  Running SpinRite makes the fix permanent.



And lastly, two-armed bandits.  I got a kick out of this story because I knew our Security Now! listeners would appreciate it.  Wired.com provided the coverage for this.  And Leo, you had already, I guess you said that you and Lisa had seen this.



LEO:  Yeah.  Lisa is a slot-machine player, so she had a serious interest in this overall.



STEVE:  So I'm just going to summarize this because we're just about out of time.  Essentially what happened is Putin outlawed gambling because he thought that that would reduce the amount of underworld activity in Russia.  And that forced the resale of a large number of electronic slot machines out onto the market.  That allowed the examination and reverse-engineering of this large population of slot machines.  And it was discovered that they were not using high-quality, hardware-based, like quantum-level noise in a reverse-bias diode that we were talking about just last week.  They're using pseudorandom number generators.  And they're not very good.



So as we talked about the problem with pseudorandom number generation as why, for example, I don't use them in SQRL, because if you are ever able to obtain a snapshot of the state of a pseudorandom number generator, then you can predict all of the future because it's just a software algorithm.  And so I did a podcast, we called it "Harvesting Entropy," a few years back, where I explained in detail the way I'm avoiding that trap in SQRL.  SQRL doesn't have a high need for randomness.  Generally it's just deterministic working from what the website generates.  But when you create an identity, you want it to be an absolutely random 256-bits.  So I arranged to do that with something that generates very, very high-quality entropy and not anything where taking a snapshot of it would allow you to predict the future.  It specifically protects against that.



Unfortunately, it looks like from the coverage of how widespread this problem is, none of the slot machines that are in use are using high-quality true random numbers.  They are using some indeterministic.  And so here's how this works.  Of course I called this "Two-Armed Bandits" because the bandits themselves have two arms, but they are exploiting one-armed bandits, the common designation for slot machines.  By watching slot machines operate, and in the later attacks using Skype to stream the video back to the exploiters, who happen to be in - I think they were in St. Petersburg, I think they're in Russia - the current state of the pseudorandom number generator could be determined just by watching the outside of the slot machine.



And first of all, I think that's unconscionable that this design would be so poor.  But that's the story.  So by essentially playing the slot machine for some length of time, while capturing what it does, it is possible from looking at the series of outputs to reverse-engineer fully the software state of the pseudorandom number generator and then predict the future.  So the way this was done was that - the way it was first caught was the bad guys would hold their smartphone in their hand, and one quarter of a second before they were supposed to press the button to stop the spinning of the wheels, their phone would vibrate.



So the phone was set to vibrate a quarter second before pushing a button would cause a payout.  And even though it was, you know, your timing might be a little bit off, so you wouldn't get a payout, but that was enough of an advantage, having the phone vibrate.  And the person, as quickly as they could, immediately pushed the button on the slot machine, biased it enough that these guys were generating 10s and 100s of thousand dollars a week in payout across the U.S.  And later, because they were caught in the video surveillance of the casinos, they were caught holding the phone, they started wearing it in a shirt pocket and with an abrasive grill so that they would be able to feel it easily vibrate in order to make it less obvious what they were doing.



But it turns out that, because the randomness of the slot machines wasn't sufficiently random, and you could, by looking at the outside of it, you could determine the current state of its pseudorandom number generator inside, you could then predict the future.  And that would give you a bias to substantially overcome the house odds and end up getting a lot of money out of the one-armed bandits. 



LEO:  Unless you're playing nickel slots, in which case...



STEVE:  Yeah, it's not going to be much money.



LEO:  Not going to be much worth it.  Well, it is a great story.  It just shows you the flaws in random number generators.



STEVE:  And these things generate so much money, Leo, they could afford a reverse-bias diode in there.  



LEO:  Yeah, put a diode in.



STEVE:  As I said, they must just be repurposed pong machines or something from the 1970s.  I don't understand.  I mean, they are so expensive.



LEO:  It's old Russian technology.  It's probably Soviet technology.  Here is, I thought, for you, your benefit, I thought you would enjoy this from Stack Overflow.  They do an analysis from time to time on popular programming languages based on the number of questions they get, and the differential between languages used during the weekday and during the weekend.



STEVE:  Interesting.



LEO:  People unfortunately during the weekday ask a lot about SharePoint and PowerShell and VBA.  But on the weekends it's Haskell and assembly language, baby.



STEVE:  Nice.



LEO:  Number one and number two.



STEVE:  Yeah, you know, assembly's making a bit of a resurgence, thanks to IoT, because these things have to be super cheap.  And money matters, and cost matters.  And there just isn't much room in those little things.



LEO:  You know, Forth is also getting a little bit of a resurgence because these little chips, these little Arduino and Stamps and things, Forth is perfect for that.



STEVE:  Absolutely.



LEO:  Hey, great fun.  Thank you, Steve, as always.  You can catch Steve's act on his website, GRC.com.  That's where you should go to get SpinRite, the world's best hard drive maintenance and recovery utility and Steve's bread and butter.  Make that yabba-dabba-doo happen, baby.  He also - some places you make it rain, other places you make it yabba-dabba-doo.  You can also find the podcast there.  He has audio, 64Kb audio and, uniquely, transcripts, so you could read along, side by side.  That's often the best way to consume the show.



GRC.com.  While you're there, there's a feedback form, GRC.com/feedback.  There's all sorts of great information.  I can just go on and on.  But really it's a fun site just to browse around, explore.  GRC.com.  We have audio and video of the show on our web page, TWiT.tv/sn for Security Now!.  And of course you can watch us live every Tuesday, 1:30 p.m. Pacific, 4:30 Eastern, 21:30 UTC at live.twit.tv, TWiT.tv/live, or now on YouTube live at YouTube.com/twit, however you like to watch it.  You can also get it on demand.  And you know that.  Just go to our website or find yourself a place to subscribe, a podcatcher, and you'll get every episode.  Two episodes away from No. 600.



STEVE:  Woohoo!



LEO:  Celebrating - what did you say? - 12 years.  That's about right.



STEVE:  Yeah.



LEO:  Wow.  Steve, thanks so much, and we'll see you next time.



STEVE:  Okay, my friend.  See you next week.



LEO:  Bye-bye.



STEVE:  Bye.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#599

DATE:		February 14, 2017

TITLE:		TLS Interception INsecurity

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-599.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  This week, Leo and I discuss the delay in this month's Patch Tuesday (we may know why!).  Our favorite adblocker embraces the last major browser.  A university gets attacked by its own vending machines.  PHP leaps into the future.  We cover a slick high-end Linux hack, the rise of fileless malware, some good advice for tax time, that it's not only Android's pattern lock that's vulnerable to visual eavesdropping, and what happens when you store a huge pile of Samsung Note 7's in one place.  We've got some fun miscellany; a must-not-miss science fiction TV series; and, finally, a look at the growing worrisome security implications of uncontrolled TLS interception.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Our Valentine's Day special.  That's why I'm wearing red.  Steve's valentine to you, he says, the best picture he's ever shown on any episode.  If you're not watching video, we'll describe it.  We'll also get you all the news.  For instance, it's Patch Tuesday.  Where's the patch?  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 599, recorded Valentine's Day, Tuesday, February 14th, 2017:  TLS Interception INsecurity.



It's time for Security Now!, the show where we cover your security online.  Steve Gibson is here of the GRC, Gibson Research Corporation.  Steve, before we begin, we should mention a couple of milestones.  This is Episode 599.  Next week is our 600th episode.



STEVE GIBSON:  Woohoo!  I did also notice that TWiT was 601.  So I'm catching up.



LEO:  We're two ahead.  But TWiT takes - actually, we used to take time off.  I haven't done that in a long time.  And you never take time off.  So I don't know.  This may be a neck-and-neck race for some time.



STEVE:  I'm going to hang in there.



LEO:  However, I'm also happy to say, I was just taking a look at the numbers, and this is now, after TWiT, the highest rated show on the network.  It's TWiT No. 1, Security Now! No. 2.



STEVE:  Cool.



LEO:  So congratulations.  This show has been growing like crazy over the last two years.  Something like 30% every year.  So, and I think that that's because, well, it's a great show, but also people are very interested in this topic now, aren't they.	



STEVE:  Well, it's certainly one that just keeps on giving.



LEO:  Yes.



STEVE:  We have a bunch of fun stuff.  The title for today's podcast is "TLS Interception Insecurity."  A fabulous paper was produced after a bunch of very interesting research into, it turns out, the surprising number, larger by an order of magnitude than these experts in the industry expected, in the number of HTTPS connections which are being intercepted.  It wasn't known to be that big.  And unfortunately they found just a huge number of problems with this.  So all listeners to this podcast know that this has been a source of angst for me for a long time.  For a long time my position was corporations should absolutely not be doing this.  And then I kind of came around, saying, well, okay, but how can they monitor their traffic if they're not able to look inside the connections which are increasingly TLS tunneled, not plaintext.  And then of course we've also discussed the worrisome problem with the AV software, which is now doing this to end-users, typically without their having any awareness of it.  So this paper is wonderful, and we'll wrap up the podcast with that.



But we have, interestingly, a delayed Patch Tuesday.  And we may know why.  We were talking about this last week.  Our favorite adblocker has, well, it's more than an adblocker.  It's not fair to call it that.  In fact, Gorhill gets annoyed when people call uBlock Origin an adblocker.  But it has embraced the last major browser that was a long-term holdout.  An unnamed university has gotten attacked by its own vending machines.  PHP leads the future in upgrading its built-in crypto library.  There's a very slick high-end Linux hack that I'm not going to promote, but I did want to bring it to the attention of our major Linux people because it's very cool.  We have the rise of fileless malware.  Some good advice for tax time, which gave me an opportunity to revise a recommendation I had recently made in error.  An observation that it's not only Android's pattern lock that's vulnerable to casual visual eavesdropping.  Also a cautionary note of what happens when you store a huge pile of Samsung Note 7's in one place.



LEO:  Uh-oh.



STEVE:  A little bit - uh-huh.



LEO:  Oh, boy.



STEVE:  A little bit of fun miscellany.  A must-not-miss science fiction TV series we've discussed before.  I failed to bring it up last week, well, because I hadn't started into Season 2.  And, oh, my goodness.  It's just amazing.  And then we will wrap up talking about some surprising revelations about just how bad TLS interception has become, not only because of its prevalence; but also, as always, the devil is in the details, and the devil's been busy.



LEO:  Oh, boy.  That'd be a good title for a show sometime:  "The Devil's Been Busy."  He's been busy.  Okay.  You've got a good Picture of the Week.  You want me to show that?



STEVE:  The best picture of all time. 



LEO:  Okay.  



STEVE:  This is the funniest thing I have ever seen in my life.



LEO:  I'm looking at it.  Oh, I get it.  Oh, my god.  Shall we see if people get it just by looking at it?



STEVE:  Oh, yes.  It's just too wonderful.



LEO:  What's happening here?  So it's a motor or a generator, looks like.



STEVE:  Yeah, we have a gas, it looks like a gasoline-powered generator that's sitting on an elevated grid.  And then there's this green solid copper wire, I mean green insulated, but you can tell it's solid copper, that comes out and then wraps several times around a pole, like a long rod, which is shoved into a half-filled pail, a plastic pail of dirt.



LEO:  Do you get it, Burke?  Do you get what that is, Burke?  Burke's shaking his head, yeah, I know what that is.  Maybe it'll help you if, instead of calling it dirt, you call that a pail filled with "ground."



STEVE:  Uh-huh.



LEO:  Oh, man.  A grounding wire?  I don't think so.



STEVE:  Oh, I just love this.  Now, I mean, as with anything on the Internet, you're tempted to think, okay, this is just, you know, this is a setup.  But it's just so adorable.



LEO:  John says we do that with all our racks.  We've got buckets of dirt all around.



STEVE:  That's it.  You need to hook your ground wire to a stake pounded into the ground.  And here's a half-filled plastic bucket of dirt.  Oh.



LEO:  This is hysterical.



STEVE:  It's just wonderful.  Anyway, so whoever - I wish I could remember who tweeted this to me.  I saw it yesterday, I said, okay.



LEO:  So funny.  So funny.



STEVE:  This is the best thing I've ever seen.



LEO:  Oh, my god.



STEVE:  It needs no explanation.  I tweeted it late last night when I was putting this all together.  And I got a great bunch of response from people, a bunch of humorous responses from people who got it.  And it's just, oh.



LEO:  Well, this is a great little test to see if people understand electrical power.  If they're geeks, I think a geek would get this; right?  If they look at it, and they go, what, what do you mean, what's wrong with that?



STEVE:  Yeah.  You've got to parse it a little bit visually and go, wait, wait, what's going on here?  And then you sort of follow the green wire over to the rod that's stuck into a plastic pail and say, okay, well, this is not what we really had in mind when we were saying you need to ground the generator.  Oh, wonderful.



Okay.  So Microsoft announced today, Microsoft Security dated February 2017, their formal announcement on Valentine's Day, today, 2/14, Microsoft Research Security Team said:  "Our top priority is to provide the best possible experience for customers in maintaining and protecting their systems.  This month, we discovered a last-minute issue that could impact some customers..."



LEO:  Showstopper.  That's a showstopper.



STEVE:  Uh-huh, "...and was not resolved in time for our planned updates today."



LEO:  Wow.  Wow wow wow.



STEVE:  "After considering all options, we made the decision to delay this month's updates.  We apologize for any inconvenience caused by this change to the existing plan."  Now, we don't know, okay, Ars believes this is the SMB.  I mean, they're stating, and so maybe they have some other channel reporting because I haven't seen...



LEO:  That was the big bug we talked about last week.



STEVE:  Yes.  This is the Server Message Block problem, which is a zero-day because Microsoft, after being given 90 days to patch the flaw, never responded to the security researcher, Laurent Gaffie, who told them about this more than three months ago.  And so he went public with a proof-of-concept written in Python, very nicely written, actually.  You need to essentially pretend to be an SMB - that is, a Server Message Block - server.  So he wrote an SMB server in Python.  And the idea is that, at the minimum, you can crash a Windows system.  This is any 8.1 or 10, also Server 2012 and 2016.  So you need to arrange to get one of those to reach out to an SMB server.



The problem is historically there have been many ways to get a browser to do that, and that's all it would take to bring your system down.  And US-CERT has been concerned that because the SMB client runs in the kernel, that would mean, if this could be matured from a crash into a remote code execution, then it would be possible to run a remote attacker's code in the kernel, which is as bad as it gets.  So we don't know this is why it's delayed.  And I'm a little suspicious because this should not be a hard problem to solve, unless - and I never took the time to dig into the nature of this exploit.  But if it's just something that's easily patchable, it should have been easily patched months ago because this is a bad problem.  So maybe the delay is for some other reason.  We were expecting this, however, to certainly be fixed on this Tuesday's, today's, update.  And we don't have an update today.  So we don't know.



LEO:  That's why, yup.  Yup.



STEVE:  It certainly follows.  Safari on macOS gets uBlock Origin.



LEO:  I was just talking about that this morning.



STEVE:  Yes, I heard.



LEO:  I wasn't sure if it did.



STEVE:  Yup.  It doesn't...



LEO:  Oh, interesting.



STEVE:  And I also heard you mentioning that your Chromebook immediately, when it synchronized...



LEO:  Oh, yeah, baby.  Oh, yeah.



STEVE:  ...and got its updates, that came in.  So for our listeners who don't know, we did a podcast, #523.  So here we are at, what, 599.  So that was more than a year ago.  I think it was like maybe September of 2015?  Something like that.  Anyway, it's podcast #523, for those who don't know and want to go back and bring themselves fully up to speed.  I was a holdout for NoScript, which is what I'd been using for years.  But it was just becoming too big a problem to have scripting disabled by default.  So the big change for me was dropping NoScript and switching to uBlock Origin.  We have full coverage in 523.



But I'll just mention that it has now,  for a long time, been supported by the Chromium-based browsers, meaning Chrome and Opera; by the Firefox-based browsers, so Firefox, Firefox on Android, SeaMonkey, Pale Moon, and anything else derived from Firefox.  Firefox does support an extra feature in uBlock Origin which is not available in the Chromium browsers, just because they lack the required feature in the API, which is script tag filtering, which is, as Gorhill says, "is of great help to foil attempts by many websites to circumvent adblockers."  So this prevents circumvention.  There is a Debian apt-get build that someone named Sean Whitton has produced for Debian 9 and later and Ubuntu 16.04 and later, which makes it very easy to add that to Linux.  It's also supported under Microsoft's Edge; until now has not been available for Safari on macOS.  It is now.  It's available for download on GitHub.  You might want to give it a few weeks to settle and stabilize because it's very new.



But essentially Gorhill takes exception to us calling it an "adblocker."  He says it is not an adblocker:  "It is a wide-spectrum blocker which happens to be able to function as," as he puts it, "a mere adblocker," as well.  The default behavior of uBlock Origin when newly installed is to block ads, trackers, and malware sites.  And it strongly leverages the publicly curated lists - EasyList, EasyPrivacy, Peter Lowe's ad/tracking/malware servers, various lists of malware sites, and its own filter lists.  And he deliberately does not take donations.  But he says, if you feel compelled to support this, then support the teams that are doing this list building because that's a public social service, essentially, which his engine uses to drive its selections.



So anyway, it's great to see it come to Safari.  And it's funny, I've been running for a while.  I've just sort of not noticed that I didn't have any blocker on my iOS Safari until I hit a site a couple days ago that was just intolerably obnoxious with ads.  And I thought, don't I have one blocker turned on?  And it turns out for some reason I'd turned it off.  Probably some site was saying, oh, please don't block us, and I thought, okay, fine.  Anyway, I turned it back on because it was just like it was impossible to use this site.  There were, like, bouncing windows jumping around in front of me.  I said, okay, this is just ridiculous.  So uBlock Origin is what we now recommend.  And obviously, Leo, you have remained a fan of it.



LEO:  I still use it, yeah.



STEVE:  Yeah.



LEO:  Mixed feelings about using an adblocker, but at least on the air I don't think it's inappropriate for me to keep that on.



STEVE:  Well, and unfortunately, malvertising has become a thing.



LEO:  Yeah, yeah.



STEVE:  And so it's almost a defensive deal.  I mean, I'm a supporter of the Google Contributor system.  And we reported that they were going to be taking it down and then bringing it back up again.  And, I mean, I'm happy to do that.  When I browse a site, and I see - I have mine set to show those little pastel circles kind of moving around.  And I see a page covered with rectangles that look like that, I think, okay, good.  I've given some money to this site.  I'm visiting it.  And that seems like a reasonable tradeoff.  So it would be nice if that sort of thing...



LEO:  Google stopped it, by the way.  I don't know if you noticed.



STEVE:  Right.



LEO:  The Google Contributor thing is down.  But I think that that's preparatory to a bigger rollout, I would guess.



STEVE:  Yeah.



LEO:  Got to do more; right?



STEVE:  Correct.  We're not exactly sure how they're going to change it or what they're going to do.  And they haven't said when.  But they said we just want to let you know we're shutting it down.  Any unused balance will be returned to you, and we'll be coming back in the future.  So it seems like a strange thing to do, like okay, why go through this dead period?  But I'm sure they have their...



LEO:  I'll tell you why.  Google.



STEVE:  Yeah.



LEO:  It's weird.



STEVE:  Exactly.  Exactly.  So this is an interesting story.  And it's just so apropos to everything we've been talking about recently.  We don't know what university this happened to.  But this appeared in a Verizon four-page brag sheet about their involvement in this.  But it was really interesting.  And it was written in a first-person narrative format that I'll just share as is, although I've edited it a little bit.



So it reads:  "Senior members of my university's IT Security Team rotate weekly as on-call Incident Commanders" - as they're called at the university - "in the event that a response is needed.  This week was my turn; and as I sat at home, my phone lit up with a call from the help desk.  They had been receiving an increasing number of complaints from students all across campus about slow or inaccessible network connectivity.  As always seemed to happen, the help desk had written off earlier complaints, and it was well after 9:00 p.m. when I was finally pulled in.



"I joined the conference bridge and began triaging the information.  Even with limited access, the help desk had found a number of concerns.  The name servers, responsible for Domain Name Service lookups, were producing high-volume alerts and showed an abnormal number of subdomains related to seafood.  As the servers..."



LEO:  What?



STEVE:  Yeah, seafood subdomains, you know, sushi dot something dot something.



LEO:  Okay, mm-hmm.



STEVE:  "As the servers struggled to keep up, legitimate lookups were being dropped" - so this was a classic bandwidth flood or server flood denial of service where the servers were overwhelmed with seafood subdomains, and so legitimate queries couldn't get their IPs resolved, so this was preventing access to the majority of the Internet.  "While this explained the slow network issues, it raised much more concerning questions.  From where were all these unusual DNS lookups coming?  And why were there so many of them?  Were students suddenly interested in seafood dinners?  Seemed unlikely.  Suspecting the worst, I," he writes, "put on a coffee pot and got to work.



"Now that I had a handle on the incident in general, I began collecting and examining network and firewall logs.  The firewall analysis identified over 5,000 discrete systems making hundreds of DNS lookups" - that is, within their internal campus network.  "Of these, nearly all systems were found to be living on the segment of the network dedicated to our IoT infrastructure."  Okay, now, so there's a little bit of good news.  The good news is they have a segment of their network dedicated to their IoT infrastructure, meaning they have a segmented network, which of course is what we've also been talking about on this podcast now for some months, is the need to give your IoT devices their own network segment so when this happens, you have some control.  And also so that when they get taken over - not if, when - then they'll be blind to the more important portion of your network.



So he continues:  "With a massive campus to monitor and manage, everything from walkway light bulbs to vending machines had been connected to the network for ease of management and improved efficiencies.  While these IoT systems were supposed to be isolated from the rest of the network, it was clear that they were configured to use DNS servers in a different subnet."  So that's interesting.  So they were on their own segment, but DNS had not been isolated.  So this little mistake in isolation allowed the problem on the IoT segment to essentially bring down the rest of the campus.  So there's an interesting data point.



"Of the thousands of domains requested, only 15 distinct IP addresses were returned.  Four of these IP addresses and close to 100 of the domains appeared in recent indicator lists for an emerging IoT botnet.  So what they found was they'd been infected by an emerging IoT botnet.  This botnet was known to spread from device to device by brute-forcing default and weak passwords.  Once the password was known, the malware had full control of the device and would check in with command infrastructure for updates and change the device's password, then locking us out of those 5,000 devices.



"This was a mess," he writes.  "Short of replacing every soda machine and lamp post, I was at a loss for how to remediate the situation."  And really, think about it.  They have 5,000 devices that have been commandeered, taken over, and had their passwords replaced.  So this is not a small, I mean, like, this is the kind of thing that is actually happening now.  And, I mean, as he says, how do you remediate this?  How do you fix this?



He continues:  "We had known repeatable processes and procedures for replacing infrastructure and application services, but nothing for an IoT outbreak.  Fortunately, a less drastic option existed than replacing all the IoT devices on campus.  Analysis of previous malware samples had shown that the control password, used to issue commands to infected systems, also was used as the newly updated device password.  These commands were typically received via HTTP and in many cases did not rely on SSL to encrypt the transmissions.



"If this was the case for our compromise, a full packet capture could be used to inspect the network traffic and identify the new devices" - plural, times 5,000 - "password.  The plan was to intercept the cleartext password for a compromised IoT device over the wire and then use that information to perform a password change before the next malware update.  If conducted properly and quickly, we could regain control of our IoT devices."  At this point I'd be holding my breath and crossing everything, all my body parts, fingers and toes and everything else, that this might work.



"While we waited for the full packet capture solution, I instructed the network operations team to prepare to shut down all network access for our IoT segments once we had intercepted the malware password.  Short lived as it was, the impact from severing all of our IoT devices from the Internet during that brief period of time was noticeable across the campus, and we were determined to never have a repeat incident.



With the packet device underway, it was only a matter of hours before we had a complete listing of new passwords assigned to newly compromised devices.  With these passwords, one of our developers was able to write a script which allowed us to log in and update the password and remove the infection across all devices at once."  Whereupon I'd be going, "Whew!"  "The whole process took a matter of minutes, and I made a mental note to save that script for later, although I prayed that we would never need it again.  Now that the incident had been contained, we looked towards ways to prevent it from happening again."



So, lessons learned:  Don't keep all your eggs in one basket; create separate network zones for IoT systems.  Air-gap them from other critical networks where possible.  Don't allow direct ingress and egress connectivity to the Internet; don't forget the importance of an in-line proxy or content filtering system.  So essentially, retrospectively, they're wishing that they had already had a device in place that was already filtering all of their IoT traffic.  They ended up installing that in order to obtain visibility into this IoT segment that then allowed them to capture the password and perform the remediation and also to take it off the Internet so that they were able to fix all of the compromised devices in isolation.



And then also change default credentials on devices; use strong and unique passwords for device accounts and WiFi networks.  Regularly monitor - and this is where most people fall down.  Regularly monitor events and logs.  Hunt for threats at endpoints, as well as the network level.  Scan for open remote access protocols to your network and disable commonly unused and unsecured features and services - that's of course broadly good advice - that aren't being required.  And include IoT devices - and this is difficult - in IT asset inventory because there are going to be so many of them.  This university had 5,000 of these things.  They had all of their vending machines plugged into the Internet in order to support and improvement management.



So an interesting and, I thought, really fascinating story.  And of course it applies on a much smaller degree to end-users, to listeners of this podcast because, as I've said, we really do need to go to the effort to isolate the IoT devices.  Clearly, we're headed towards a world where we've got devices that, for their functionality, need to be on the Internet.  You just don't want their compromise to be able to then compromise the rest of your network.



LEO:  You're still recommending the Ubiquiti EdgeRouter X as a way to do that?



STEVE:  Yes.



LEO:  Okay, good.  Somebody called the radio show this weekend and asked, and I said, yeah, as far as I know, this is still a very good way to go.



STEVE:  Yes.



LEO:  You have to know what you're doing.  You get on the web interface, and you have to know how to set up the VLAN stuff.  But at least it lets you do it, with a 50, $60 router.



STEVE:  Forty, $49.



LEO:  Forty, yeah.



STEVE:  Yes.  And it's four or five ports and true isolation, where you're able - so it is a router where, instead of being a router connected to a switch, it is a router where every one of those ports is a separate Ethernet interface that can be given its own network.  So you can have 192.0.x on one port, 192.1.x on another port, 192.2.x on a third port, and those networks cannot see each other.  They are isolated.  So it's just - it's a fabulous and very cost-effective solution.



But as you say, Leo, at this point you really do need to sit down and figure out networking.  There are a lot of how-tos on the network.  And I'm sure that the drive toward IoT and the problem with its security is going to end up with people coming up with a drop-in solution.  If I didn't already have my plate overfull, I'd have some online configuration stuff for the Ubiquiti router.  But I'll just let - because other people can do that, too.  Those things are being created on the Internet.



So PHP 7.2:  It just got voted to add libsodium to the core standard library.



LEO:  Woohoo.  Wow.



STEVE:  Yes.



LEO:  That's interesting.



STEVE:  Yes.  Last week the voting phase closed on an RFC to add libsodium to PHP 7.2.  The result was unanimous:  37 in favor, zero against.  Now, there was a much more split vote.  In looking at it visually, I couldn't visually see, it looked to me like it was about 50-50, that is, the question was whether to keep the existing API namespace.  It's like /sodium/something, or with this change to bind it into the root namespace, and they decided to do that.  That did win.  So it'll be - and I don't remember.  I remember seeing it, but I don't remember what it was.



So it won't be down a hierarchy, it'll be in the root namespace.  And so it's going to be built in.  Libmcrypt is what PHP has had since 2007, so for, what, nearly 10 years.  And that left only OpenSSL as a viable option for PHP 5.x and 7.0 up to this point.  So of course by comparison libsodium is, as we know, it's my favorite library.  It's the one that I used functions from for SQRL.  We're seeing it being adopted pervasively across the industry.  It's a modern, state-of-the-art crypto library offering authenticated encryption, high-speed elliptic curve crypto, and all the features that you want.  So unlike other crypto standards, which basically are a potluck of various crypto primitives, for example, like WebCrypto, libsodium is comprised of carefully selected algorithms implemented by true security experts to avoid side channel vulnerabilities to offer short keys with high security.



And as a toolkit there was effort made in its design to prevent people from doing the wrong things.  There aren't, unlike OpenSSL, where there are about 25 different ways to do something, there's one way to do something in libsodium, and it's the right way.  So don't make it up, just call the function, and it'll keep you from hurting yourself.  So bravo for the PHP team for doing this.  And we're not going to see it immediately.  It's going to be later, like toward the end of this year.  But 7.2, when available, will have it built in.  And that's just a great step forward.



Okay.  So this is not for the faint of heart, but I wanted to put it on the radar of our Linux users because I know we have a  huge Linux following in the podcast.  This is a script called takeover.sh.  It's up on GitHub.  I've got the link to it.  It's GitHub.com/marcan, M-A-R-C-A-N, the guy who did this, /takeover.sh.  He describes it as:  takeover.sh is a shell script to completely take over a running Linux system remotely via SSH, without rebooting, allowing you to log into an in-memory rescue environment, unmount the original root filesystem, and do anything you want, all without rebooting.  You can even replace one distro with another without touching a physical console.



And so he warns people.  He says:  "This is experimental.  Do not use this script if you don't understand exactly how it works.  Do not use this script on any system you care about.  Do not use this script on any system you expect to be up.  Do not run this script unless you can afford to get physical access to fix a botched takeover.  If anything goes wrong, your system will most likely panic.  That said, this script will not itself," he writes, "make any permanent changes to your existing root filesystem, assuming you run it from the temp file system.  So as long as you can remotely reboot your box using an out-of-band mechanism, you should be okay.



"But don't blame me," he writes, "if it eats your dog.  This script does not have any provisions for exiting out of the new environment back into something sane.  You will have to reboot when you're done.  If you get anything wrong, your machine won't boot.  Tough luck.  This is not a guide for newbies."  He writes, "I'm deliberately not giving you commands you can copy and paste."  There's no curl where you click the link on a web page.  "If you can't figure out," he writes, "what to do exactly without handholding, this script is not for you."



But anyway, I thought it was cool.  This would allow somebody who knows what they're doing to SSH into a system and essentially, on the fly, switch from the mounted boot file system into an in-RAM temporary file system, and so much so that you are then able to unmount your normal operating boot file system, for whatever reason you may have, and perform surgery at a much deeper level than you normally could and then, finally, reboot the system and, if you've done everything right, have it come up under its new configuration.  Anyway, I just thought it was very cool.  Scary, but for people who are seriously into Linux, I could see where there could be an application for it.  So I wanted to put it on our high-end Linux users' radar.



Dan Goodin, writing for Ars Technica, covered a story that comes out of Kaspersky that I thought was interesting.  And it indirectly affects us because we've been talking about, we've been seeing the Mirai botnet where just rebooting devices gets control back.  And this sort of shows us an interesting trend as a consequence of the fact that there are more and more Internet-connected things that do not routinely reboot, like your refrigerator or the Coke machine or whatever.



So Dan writes:  "Two years ago, researchers at Moscow-based Kaspersky Lab discovered their corporate network" - their corporate network, Kaspersky - "was infected with malware that was unlike anything they had ever seen.  Virtually all the malware resided solely in the memory of the compromised computers" - and, by the way, we did cover this at the time, but since then we're seeing a trend - "a feat [on the side of this malware] that allowed the infection to remain undetected for six months or more.  Kaspersky eventually unearthed evidence that Duqu 2.0, as the never-before-seen malware was dubbed, was derived from Stuxnet, the highly sophisticated computer worm reportedly created by the U.S. and Israel to sabotage Iran's nuclear program."  Which of course we covered extensively at the time, to spin their centrifuges up to a speed that would cause them to self-destruct.



"Now, fileless malware is going mainstream, as financially motivated criminal hackers mimic their nation-sponsored counterparts.  According to research Kaspersky Lab plans to publish Wednesday [tomorrow], networks belonging to at least 140 banks and other enterprises have been infected by malware that relies on the same in-memory design to remain nearly invisible."  And think about it.  We detect malware by looking for it on mass storage.  This is deliberately not storing itself where it can be scanned.  It's in RAM, and we're not good yet at finding that.



"Because infections are so hard to spot, the actual number is likely much higher.  Another trait that makes the infections hard to detect is the use of legitimate and widely used system administrative and security tools - including PowerShell, Metasploit, and Mimikatz - to inject the malware into computer memory."  And in fact turns out that it was actually PowerShell.  I'm sure we'll cover this because I'll have more details about this next week after having a chance to take a look at what Kaspersky has provided.



"What's interesting here," writes Kaspersky's expert Kurt Baumgartner, "is that these attacks are ongoing globally against banks themselves.  The banks have not been adequately prepared in many cases to deal with this."  He went on to say that people behind the attacks are "pushing money out of the banks from within the banks" by targeting computers that run ATMs, automatic teller machines.



"The 140 unnamed organizations that have been infected reside in 40 different countries, with the U.S., France, Ecuador, Kenya, and the U.K. being the top five most affected nations.  The Kaspersky Lab researchers still don't know if a single group of individuals is behind the attacks, or if they're being carried out by competing hacker gangs.  The use of the fileless malware and command-server domains that aren't associated with any whois data makes the already difficult task of attribution nearly impossible."



So in general we're seeing a rise in this kind of in-memory compromise.  It may be in many cases that firmware may not be safely writable on these machines, or can't even contain the malware.  It's like, you know, as we know, our IoT devices tend to be memory lean, only having enough firmware space to contain their own OS.  So there just, you know, there may be no space for that.  So the malware lives opportunistically in RAM, hoping to stay there for as long as it can.



And many of these things, these IoT devices are appliances that are rarely, if ever, power cycled.  And of course servers typically remain up between their patch cycles, if they don't crash.  So malware that is scanning and is able to infect on the fly is able to take up residence until the system is updated and then rebooted after a patch cycle.  And as we know, some routers, whether big iron or consumer little blue box routers, they may be up for years at a time with something living in RAM.  So it's interesting because, as we know, scanning the device's file systems will reveal nothing because this stuff is no longer in files.  It's arriving over the connectedness of the device and just taking up residence.  Interesting.



And here's where I wanted to correct myself.  This was a really nice blog posting that brought to my attention that I had referred to AxCrypt recently when I meant AES Crypt as the recommended encryption utility.  Remember I was talking about email last week and saying, if you really care about the security of something you're sending to someone, TNO.  You need to use the same protocol that we use for storing data, our data, in the cloud, or PIE, as we've also called it, Pre-Internet Encryption, where you encrypt it before you let the Internet have it.  So anyway, this nice blog posting was titled "Encrypt Your Tax Documents Before You Send Them."  



"It's that time.  We're all doing our taxes online now.  Some of us need some help, and that usually ends up with digitally sending tax documents.  Do not ever," they write, "under any circumstance, email a document that has your Social Security number, home address, phone number, bank information, et cetera, unencrypted.  That's a good way to lose all of your money and even your identity."



He writes:  "I'm going to use two free services to send my encrypted documents out:  AES Crypt and Dropbox.  Step 1: Install AES Crypt.  Get AES Crypt installed on your computer.  Go here and install your platform."  And I'll follow up with details here in a second.  "Step 2:  Zip all of those files.  You want to send one encrypted blob of data."  And I'm happy to see him use the word "blob."  I wonder if he's a listener of Security Now!.  "Put all your files into a single folder and zip that up.  On the Mac, select the file and go to File >> Compress 'Sensitive Docs.'  Step 3:  Encrypt.  Follow the directions for your platform to encrypt your zip file.  On the Mac, drag the file onto the dock icon and enter a strong password when prompted.  I suggest," he writes, "using a secure password generator" - oh, I think he is a listener - "like the one over at GRC.com."



LEO:  Well, there you go.



STEVE:  "You can also use LastPass or whatever you use to manage your passwords.  Make a note of that password, though.  You can keep a copy of the password in TextEdit or nvALT - the documents are already on your computer.  Step 4:  Drop it in Dropbox.  Using Dropbox makes sharing files easy.  Put the encrypted file into your Dropbox folder and click Share Link.  You can send them the file directly through the Dropbox website or get the link and send it separately.



"Step 5:  Send the Password via a Separate Method."  Yes.  "If you send an email with a link to your encrypted document, it doesn't make sense to put the password into that email.  Additionally, you might not want to send the password from your account to that same account.  Try to send the password over iMessage, Signal, or another instant messenger, or to a secondary email address the person has.  You could also" - and this is what I would do - "make the password something you could read to them over the phone."



Now, this person suggests "uppercase J, lowercase z, six, five" and so forth.  I would prefer a long numeric password, grouping the numbers in pairs, because it's very easy to say 35274256.  That's acoustically unambiguous.  You get a pair of digits per utterance.  And so if you just take a long numbers-only password - and of course you can have LastPass do that for you.  You just tell it that you want numbers only and it'll say, okay.  And then put spaces, and that's easy then to read over the phone to the other person.  And then Step 6, this blogger writes:  "The Takedown.  Once the recipient has confirmed that they have the documents and have decrypted them on their own machine, remove the encrypted documents from Dropbox.  Sure, they're safe," he writes, "but there's no reason to keep them online."



So about AES Crypt.  I recently referred to AxCrypt, actually just last week, in this context, when I meant to say AES Crypt.  We had spoken about it before, and I just got my tongue tied.  AxCrypt was what I used to recommend, but it has gone commercial.  And against all reason, they have a subscription plan, a few dollars a month or more per year.  It makes no sense to me at all that you'd have a subscription plan for an encryption utility.



So bye-bye, AxCrypt.  Hello, AES Crypt.  The website is AEScrypt.com.  It is free, open source, multiplatform - available on Windows, Mac, Linux, iOS, Android, and also for PHP and Java.  So that's what you want to use.  It is drop-dead simple.  And as we know, AES is a good strong cipher.  So you simply give it a good key.  It'll use that key to encrypt your document.  And then send it any way you want to.  You could certainly do this.



And I presume this blogger is suggesting to use Dropbox because it can handle large things easier than email.  But if it's not too large, then you could also just mail it to someone.  But again, you do want to arrange to pass the shared secret key through some secure means.  And again, I like making it very long, numeric only, chop them up into digit pairs, and then just dictate it over the phone, which is very easy to do.



LEO:  This post kind of makes me sad, I'll be honest with you.  PGP does all of this, and it does it better, and it does it right, and there's PGP key exchange servers that work very well and very reliably.  It's public key crypto, not symmetric crypto, which you're using here.  I just - and nothing wrong with this solution.  But here's an opportunity to get more people to use PGP or GPG, which is in the long run a far better solution.  My opinion.  Just throwing that in.



AES works fine.  But why not use GPG?  You don't have to worry about the symmetric key exchange because you have public key crypto.  The problem is, of course, it's difficult to set up.  But that's my issue, is I think fragmenting the market is not the right solution.  The right solution is get everybody using PGP.  People at Keybase are doing a great job making PGP very accessible, very easy.  They have a key server.  And I would say it's arguably more secure because you don't have the symmetric key exchange involved.



STEVE:  They are abandoning it, though.



LEO:  Who's abandoning what?



STEVE:  Keybase is abandoning PGP.



LEO:  No, that's not true.  They're doing a chat - well, you know more about this than I do, probably.  And I'm using their encrypted chat.  But that's not using a GPG key.  But you absolutely use your GPG key there.  Keybase is still a GPG or a PGP key server.  And that's the main purpose of it.  They're not abandoning it.  They're just not using it in their chat solution.



STEVE:  Well, they're abandoning it in the future.  They are.



LEO:  Are they?  Is that what they said?



STEVE:  Yes.



LEO:  Well, that's disappointing.



STEVE:  Well, I don't think PGP ever worked.  I mean, you and I differ on this.  I just think the hurdle is too big, and we need to reduce the hurdle.



LEO:  Well, is there another better way to do public key?



STEVE:  It's on its way, yeah.  Libsodium is all public key.



LEO:  Right.  And somebody's pointing out in the chatroom that you probably should be working with a CPA that supports HTTPS file uploads in the first place.



STEVE:  Very good point.



LEO:  If your CPA is asking you to email your tax records, that's probably not the CPA to use.



STEVE:  Well, and of course I like this more as a generic solution because it's not just your CPA.  It's anybody you want to send something to.



LEO:  It's good for people to know about AES Crypt.  Yeah, I agree.  But I don't think a symmetric key solution is - I think that adds some fundamental problems.  I think, we've got public key crypto, let's make it easier to use.  Let's figure out a way to make it easier to use.



STEVE:  Yeah, yeah.



LEO:  Just my two cents.  You're the expert.  I'm just throwing that in.



STEVE:  No, I don't disagree.  So Sam Cox tweeted me an observation that just hadn't occurred to me when we were talking about the Android pattern lock.  He said:  "Numeric passcodes are also entered on a 3x3 grid."  I guess technically 3x4.  So, he said, "So aren't they also vulnerable to the Android pattern lock attack?"  And it's like, yes, come to think of it, they are.  Because the whole point of that was that you just - you have limited spatial resolution on a large grid.  And what you're looking at is relative motion.  And so the pattern lock  attack exactly attacks a low-resolution keypad grid in exactly the same way.  So bravo, Sam, for that observation.  I thought that was great.



And Leo, oh, the second picture that we have here in the show notes, and it's just sort of sad.  Snarky as always, theregister.co.uk called this a "Touching Note 7 Tribute."  Which was the Samsung battery factory, which burst, spontaneously burst into flame.



LEO:  Oh, lord.



STEVE:  Now, the good news is nobody was hurt.  And my takeaway was just sort of shaking my head.  It was:  When you are recalling a huge number of batteries because they are prone to spontaneous combustion, it would be best not to store them in what amounts to a large - and this is my term - Fukushima pile.



LEO:  Yeah.  This is actually where they're recycling the Note 7 batteries; right?



STEVE:  Yes, right, right.  So they're, like, all being returned, and this was the recycling facility.  And unfortunately, I mean, maybe they just couldn't get to them all?  I mean, clearly, I did a lot of travel, when was it, I guess over the holidays.



LEO:  Oh, yeah.



STEVE:  And every, without fail, every single gate that you're at, the attendants are reminding us, now, if you have a Note 7, you need to just not bring it on the plane because you can't do that.  But think about it, Leo.  I mean, think about how many of them there were, that none of them are trusted, yet the battery has to be taken apart.  I mean, it needs to be - the device needs to be opened, and the battery needs to be unwrapped because essentially it was wrapped over itself, and the corners were bent, and it was just manufacturing problems.  But I never really thought about the problem of disposing of these.



LEO:  Well, we had talked about this on The New Screen Savers with Kyle Wiens of iFixit.  He says he doesn't know of a recycling factory that hasn't had a lithium-ion battery fire.



STEVE:  Wow.  Wow.



LEO:  This is very common because, if you puncture a lithium-ion battery in air...



STEVE:  If you breach it, yup.



LEO:  ...it will burst into flame.  And he says this is the real problem with these batteries that everybody's using now that are not in a protective shield.  They're very hard to recycle.  And he said everybody has fires.  This is universal now.



STEVE:  Yeah.



LEO:  So I guess you're prepared about that; right?



STEVE:  Yeah.



LEO:  On we go.  You've had your coffee.  You've been caffeinated.



STEVE:  Recaffeinated.



LEO:  Recaffeinated.



STEVE:  As opposed to decaffeinated.  So a listener of ours shot me a tweet that I appreciated.  He said:  "pfSense allows one to activate UPnP and allow only user-specified IP addresses..."



LEO:  Oh, I like that, oh.



STEVE:  Yes, "...on the LAN to open ports."



LEO:  pfSense FTW.



STEVE:  Yup.  And so that's a nice feature of pfSense.  And I logged into my pfSense router, because that's what I'm using; and right there on the UPnP page, the second section says "UPnP Access Control Lists."  And it reads:  "These entries control access to the UPnP service.  Client systems may be granted or denied access based on several criteria."  And so you then have a standard router-style or firewall-style access control list where you're able to specify which devices can or cannot use UPnP.



So this is, again, for a higher end network environment, this is what you want, where you would assign a static IP, for example, to your Xbox based on its MAC address.  That way it's always going to get the same IP because it's always going to - its interface, its Ethernet interface will always have the same MAC address.  So you bind those together.  And then you give it alone access to Universal Plug and Play so it can handle, it has permission to handle its own mapping, but nothing else in your network is able to see that.  And alternatively you could use the network segmentation we were talking about because you're able to give access to a range of ports.  So, for example, you could make your IoT segment able to use Universal Plug and Play, I mean, if you wanted to.  Yet the other devices on your network could not.  So a lot of flexibility there.



And there is, I meant to mention a couple months ago, there is a very nice little pfSense box.  It's red, and it only has two ports.  I wish it had more ports because you'd like to also be able to do network segmentation, although it does support VLANs.  So if you had a VLAN-capable switch, which they're inexpensive, then you could have the little cute red two-port pfSense as your router and firewall, and then connect that to a VLAN-aware, inexpensive switch in order to then get multiple port segmentation in order to create a more powerful network.  Anyway, we've got so many toys now to play with in our home networks, and none of them are very expensive.  So anyway, thank you for reminding me that pfSense supports ACLs.



Okay, now, I have the Awful Pun of the Week, Leo.  It's just...



LEO:  All puns are awful, Steve.



STEVE:  Yes.  I need to apologize profusely in advance.  Now, in my defense, I grew up with a grandfather who lived to 103, who was the master punster of the world.  And I've mentioned some of his favorites in the past.  I think probably my all-time is "I opened the window, and influenza."



LEO:  Yeah.  On Valentine's Day, the old Marx Brothers pun:  "When love comes in the door" - no, let's see.  Oh, shoot.  "When I come in the door, love goes innuendo."  Something like that.



STEVE:  And he also, my grandfather, used to say, "If the rain keeps up, it won't come down."  So anyway, with that understanding, many people saw this and sent it to me.  And I don't know where it originated.  But so this one is - and of course it's perfect for the podcast:  "You can't use 'beefstew' as a password."



LEO:  What?



STEVE:  "It isn't stroganoff."



LEO:  That's good.  That is a good - no, that's good.  For a pun, that ain't bad.



STEVE:  You can't use "beefstew" as a password.



LEO:  Why not, Steve?



STEVE:  It isn't stroganoff.



LEO:  Yeah, that transcends pun into wordplay, as Logan5 says, yeah.



STEVE:  And I do like wordplay.  Now for our next one.  We have metaphysics and Schrodinger's cat meets cryptography.  Now, this is a little along the lines of the sound of one hand clapping, or if a tree falls in the woods.  So here it is, metaphysics:  If you encrypt data with a key that is so strong that it would take more than all of the energy in the universe to crack, and the key is then destroyed, does the data continue to exist?



LEO:  Oh, that's the old black hole conundrum.



STEVE:  Isn't that good?  I like that, yeah.



LEO:  That actually is kind of deep.  That's what Stephen Hawking was debating, whether information can exist inside a black hole.



STEVE:  Right.  And so in this case we have data which is encrypted using a key that would take more than all of the energy in the universe to decrypt.  Then the key is destroyed.  The question is, so does the data still exist?



LEO:  Mmm.



STEVE:  Don't know.



LEO:  So Groucho's quote - and I stand corrected.  One of our chatters gave it to me.  He said - oh, crap, I just lost it.  Madame Swempski says:  "I don't like this innuendo."  Groucho says:  "That's what I always say.  Love flies out the door when money comes innuendo."  I like mine better.  Go ahead.  Sorry.



STEVE:  Okay.  And, finally, another follower and frequent tweeter friend of mine on Twitter said of the second season of "The Expanse":  "Very realistic space combat.  Makes 'Star Trek' look very silly and childish."  And I was thinking exactly the same thing.  I didn't mention it last Tuesday because I hadn't yet started to watch the second season.  I was two episodes behind, or maybe one, and then the other one came in on Wednesday.  Oh, my god, Leo.  What I found myself thinking as I was watching this is I have only ever seen this in my mind when I am reading the best military sci-fi.  There was some combat stuff in the first couple hours of the second season of "The Expanse" that is among the best I have ever - I've never seen it on the screen.  I've only visualized it from reading it.



So, and we talked about this last year.  "The Expanse" is a stunning Syfy series.  It is now - the first season, for those who didn't already see the first season, is now on Amazon Prime.  So if you are an Amazon Prime subscriber, you can catch up with the first season, which was excellent.  It is just - it's the best science fiction on television without question.  And the second - and I remembered that we knew this was coming.  I read all four of the first books because the books are always better.  But, boy, this move to the screen does not disappoint.  So I just wanted to say to anybody else who thought, holy wow, I mean, this is just amazing television, yes, I completely agree.  And for anybody who's not watching it, first season on Amazon Prime, and you're only two weeks into the second season. Definitely worthwhile.



And I did get a nice note from David Goldenberg.  He said:  "Hi, Steve.  I've been a happy owner of SpinRite for a few years now.  It's my secret weapon in the technology trenches.  I am the family tech guy, I help out at my kids' school, and I have my own part-time business fixing PCs, training, and networking."  So, yes, he's in the crossfire.  He says:  "SpinRite is always within reach, and never lets me down.



"Last week I'd been preparing a laptop for a presentation for my ARES Amateur Radio group.  I volunteered to get a new program running to send text messages and email-type communications over the radio.  After several days, I had everything working great, and spent several hours getting screenshots for the PowerPoint I was to prepare.  I was getting together with another ham to go over what I had and to get his machine working.



"So I started up my laptop and got the dreaded BSOD" - which of course we know is Blue Screen of Death - "and an unmountable boot volume.  I didn't break a sweat, or even worry, as I knew from experience that SpinRite would save the day.  And needless to say, two hours later the drive scanned, several sectors were repaired, the laptop booted perfectly, and everything I needed was ready to go.  You're great.  Thanks!  David Goldenberg, KJ6MCQ."



LEO:  Nice.



STEVE:  And David, you're great for sharing your experience.  I really appreciate that.



LEO:  I like it.



STEVE:  And a ham, yes.  A fellow ham.



LEO:  Are you a ham?  I keep - are you a ham?



STEVE:  I'm not.



LEO:  You're not.



STEVE:  Never got into it.



LEO:  Too analog for you, huh?



STEVE:  Well, yes, yeah.



LEO:  Well, you do some analog stuff.  Your light pen was analog.



STEVE:  Yes, indeed it was. 



LEO:  Early days, yeah.



STEVE:  Yup, yup.



LEO:  But nowadays ham has become as much digital as analog, really.



STEVE:  Which I find really interesting, yes, the fact...



LEO:  Me, too, yeah.



STEVE:  Yeah.  Okay.  So...



LEO:  Let's go.  TLS.



STEVE:  A fabulous piece of work.  This was a big team with members from the University of Michigan, University of Illinois, Mozilla, Cloudflare, Google, UC Berkeley, and the International Computer Science Institute.  And when I just scan their names, there are lots of people that I recognize there.  This was a beautifully assembled paper.  And I started to go through it, trying to sort of like pull excerpts from it.  And I just couldn't stop finding stuff that was all relevant.  So I'm going to share the abstract and the introduction, both of which are just perfect.



The abstract reads:  "As HTTPS deployment grows, corporate middlebox [as they term] and anti" - and so corporate middleboxes are these things, these appliances which corporations are using to intercept their Intranet's HTTPS communications to the outside.  "And antivirus products," they write, "are increasingly intercepting TLS connections to retain visibility into network traffic.  In this work, we present a comprehensive study on the prevalence and impact of HTTPS interception.  First, we show that web servers can detect interception by identifying a mismatch between the HTTP user-agent header and TLS client behavior."  And I inserted here my own observation:  In other words, current TLS interceptors are not bothering to mask their presence, though they certainly could.  Okay.  So then they continue.



"We characterize the TLS handshakes of major browsers and popular interception products, which we use to build a set of heuristics to detect interception and identify the responsible product.  We deploy heuristics at three large network providers:  the Mozilla Firefox update servers, a set of popular ecommerce sites, and the Cloudflare content distribution network.  We find more than an order of magnitude more interception than previously estimated and with dramatic impact on connection security.



"To understand why security suffers, we investigate popular middleboxes and client-side security software, finding that nearly all reduce connection security, and many introduce severe vulnerabilities.  Drawing on our measurements, we conclude with a discussion on recent proposals to safely monitor HTTPS and recommendations for the security community."  That's the abstract.  So I'll just now share just the introduction because essentially this wraps up all of the meat of it.  And then the rest of the paper, I think it was 14 pages, is just details.  And actually it's all stuff that we've talked about in various contexts previously.



So they write:  "When it comes to HTTPS, the security community is working at cross purposes."  And we know this is true.  "On the one hand," they write, "we are striving to harden and ubiquitously deploy HTTPS in order to provide strong end-to-end connection security.  At the same time, middlebox and antivirus products increasingly intercept (i.e., terminate and re-initiate) HTTPS connections in an attempt to detect and block malicious content that uses the protocol to avoid inspection. Previous work has found that some specific HTTPS interception products dramatically reduce connection security; however, the broader security impact of such interception remains unclear.  In this paper, we conduct the first comprehensive study of HTTPS interception in the wild, quantifying both its prevalence in traffic to major services and its effects on real-world security.



"We begin by introducing a novel technique for passively detecting HTTPS interception based on handshake characteristics. HTTPS interception products typically function as transparent proxies.  They terminate the browser's TLS connection themselves, inspect the HTTP plaintext, and relay the HTTP data over a new TLS connection to the destination server.  We show that web servers can detect such interception by identifying a mismatch between the HTTP user-agent header and the behavior of the TLS client.  TLS implementations display varied support (and preference order) for cipher suites, extensions, elliptic curves, compression methods, and signature algorithms."  In other words, sort of the metadata of the TLS connection varies from implementation to implementation that allows the target servers that are instrumented to detect those signatures.



So they say:  "We characterize these variations for major browsers and popular interception products in order to construct heuristics for detecting interception and identifying the responsible product.  Next, we assess the prevalence and impact of HTTPS interception by applying our heuristics to nearly eight billion connection handshakes."  So this was a comprehensive study, eight billion connections.  "In order to avoid the bias inherent in any single network vantage point, we analyzed connections for one week at three major different Internet services:  the Mozilla Firefox update servers, a set of popular ecommerce websites, and the Cloudflare content distribution network.



"These providers serve different types of content and different populations of users, and we find differing rates of interception:  4% of Firefox update connections, 6.2% of ecommerce connections, and 10.9% of U.S. Cloudflare connections were intercepted.  While these rates vary by vantage point, all are more than an order of magnitude higher than previous estimates."  So think about that.  Nearly 11% of U.S. Cloudflare connections - it's surprising - were being intercepted.



"To quantify the real-world security impact of the observed interception, we establish a grading scale based on the TLS features advertised by each client.  By applying the metric to unmodified browser handshakes and to the intercepted connections seen at each vantage point, we calculate the change in security for intercepted connections.  While some older clients' proxies increased connection security," meaning if you had an older client that was inherently low security, the running it through a TLS transparent proxy might actually increase its security because, for example, the client might be using RC4 encryption and not offering anything stronger, whereas the proxy does.  So the weak security connection would be kept local, and the public connection would be stronger.



So they say:  "While some older clients' proxies increased connection security, these improvements were modest compared to the new vulnerabilities introduced:  97% of Firefox, 32% of ecommerce, and 54% of Cloudflare connections that were intercepted were made less secure."  Again, 97% of the connections to the Firefox update servers, where you would like to have security because you're getting a new copy of Firefox that could be compromised.  And more than half, 54% of the connections to Cloudflare, of which 11%, 10.9%, were intercepted, more than half of those had their security compromised by that interception.



"Alarmingly, not only did intercepted connections use weaker cryptographic algorithms, but between 10 and 40% advertised support for known broken ciphers that would allow an active man-in-the-middle attacker to later intercept, downgrade, and decrypt the connection data.  A large number of these severely broken connections were due to network-based middleboxes rather than client-side" - which is to say typically AV - "security software:  62% of middlebox connections were less secure, and an astounding 58% had severe vulnerabilities enabling later interception.



"Finally, we attempt to understand why such a large number of intercepted connections are vulnerable by testing the security of a range of popular corporate middleboxes, antivirus products, and other software known to intercept TLS.  The default settings for 11 of the 12 corporate middlebox appliances we evaluated expose connections to known attacks; and five of those 12 introduce severe new vulnerabilities, for example, incorrectly validated certificates.  Similarly, 24 of the 26 client-side security AV products we tested reduce connection security, and two thirds introduce severe vulnerabilities."  That's 24 of the 26 client-side AV products.



"In some cases, manufacturers attempted to customize libraries or reimplement TLS, introducing negligent vulnerabilities; in other cases, products shipped with libraries that were years out of date.  Across the board, companies are struggling to correctly deploy the base TLS protocol, let alone implement modern HTTPS security features.  Our results indicate that HTTPS interception has become startlingly widespread, and that interception products as a class have a dramatically negative impact on connection security.  We hope that shedding light on this state of affairs will motivate improvements to existing products, advance work on recent proposals for safely intercepting HTTPS, and prompt discussion on long-term solutions."



I should mention that they also had a responsible disclosure, where all of the vendors of these sweepingly insecure products were privately notified.  The document, this 14-page PDF has charts and tables showing devices which presumably have since been remediated, hopefully.  But this really raises an interesting dilemma.  And it's one for which there is not a clear solution.  I mean, as the paper starts out, and as we said, the industry is at odds with itself because we are moving from plaintext to secure tunnels, yet there is a legitimate need to protect from the ability for malicious content of all kinds to hide within those tunnels.  Even, for example, if you're doing nothing but looking for credit card numbers and the exfiltration of sensitive corporate data, that you could argue any corporation has almost an obligation to do.  But if all the connections are secure, it can't.



So they had a couple takeaways.  One was, they said:  "We need community consensus.  There is little consensus within the security community on whether HTTPS interception is acceptable.  Discussions over protocol features that facilitate safer interception have been met with great hostility within standards groups.  These communities need to reach consensus on whether interception is appropriate in order to develop sustainable long-term solutions."



And then they also said:  "Antivirus vendors should reconsider intercepting HTTPS.  Antivirus software operates locally and already has access to the local filesystem, browser memory, and any content loaded over HTTPS.  Given their history of both TLS misconfigurations and remote code execution vulnerabilities," you know, we've talked about that the AV has become, has started introducing a larger attack surface than the OS it's trying to protect.  Then they continue:  "We strongly encourage antivirus providers to reconsider whether intercepting HTTPS is responsible."



And so the point they're making here is that, instead of trying to get the data on the fly, wait till it arrives and put your hooks in there, where you do have visibility.  And I would say, as we did, I think, just last week or the week before, get rid of third-party AV now, at least on the Windows platform, because Microsoft, just as they did with the firewall and the previous firewall vendors, they've done this with AV and the hopefully soon-to-be previous AV vendors.  There's a built-in solution that solves this in a secure way from a company that is on the ball, except for a little delayed Patch Tuesday today, on Valentine's Day, is keeping things current and updating monthly.  Just go with the one that's built in.



And so, finally, I conclude by reminding everyone that, so long as the intercepting proxy, whether it is middlebox appliance or AV, as long as it does not have the ability to on-the-fly synthesize certificates which are trusted by the global public trusted root store - and no certificate authority will willingly, or at least publicly, allow this or they would become untrusted.  This interception can only be done with the knowledge of the user's browser client which has chosen to trust the public key installed into it by the proxy's vendor.



So an interesting possibility here would be the explicit notification to the user when a non-globally trusted certificate is being used for any connection.  In other words, for the browser, which certainly knows whether the certificate that has been used to sign this connection is part of the global trust store or was added on afterwards, that would be nice to display to the user.  Not that a user in a corporation has any choice.  But at least being informed that your connection is being intercepted, even if it's benign interception, I think that's what we should have.



And we can have it.  That is an unspoofable, robust compromise that at least tells the user something between here and there cracked this connection open in order to look into it.  Maybe you have no choice, if you're in a corporate Intranet.  But being informed, I think, is the least we can do.  And until then, as an industry, we have a conundrum because we want our data to be secure, but we also want it to be safe.  So anyway, great, great piece of work, and great research.  And, wow, nice to know that 11% of connections are today being cracked open.  And most of them have their security damaged as a consequence.  So it's not even benign.



LEO:  Anything as an end-user you can do about that?  Not really.



STEVE:  No, not really.  Now, most AV products give you an option to turn that off.  That's certainly something you would want to do.  It defaults on because they're thinking that's a benefit.  I would turn that off.  I would just say, eh.  But then, of course, I mean, if you're a person who wants a third-party AV, you want it to scan the stuff coming in over HTTPS.



LEO:  Right.



STEVE:  So it's like, well, you don't want to turn that off, because you want the benefit of it being able to be looking into your secure connections on your behalf.  



LEO:  Right.



STEVE:  The problem is, 24 out of 26 of them do it badly, so that it actually hurts your security.



LEO:  Wow.



STEVE:  Yeah.



LEO:  Kind of amazing.



STEVE:  We're just in a tough time right now.  We need to get past this somehow.  And we will.



LEO:  Well, yeah.  And in the meantime, you're here. 



STEVE:  That's right.



LEO:  Thank goodness.



STEVE:  And we'll be back next week for Episode 600.



LEO:  Wow.  Wow wow wow wow wow.  Ladies and gentlemen, we do this show every week for 600 darn weeks.  That's an awful lot.  And we plan to keep on doing it, as long as you keep on listening.  You can catch it live every Tuesday at 1:30 Pacific, 4:30 Eastern time, 21:30 UTC.  Join us in the chatroom at irc.twit.tv.  But if you don't want to do it live - and by the way, if you want to watch live, you can watch on our website, TWiT.tv/live or on the YouTube Live we have now, YouTube.com/twit, or at Ustream, or Twitch, Twitch.tv/twit.  There are a lot of places to watch.  Actually, if you go to our website, you'll have a choice.  You can also watch or listen on demand.  Steve has audio at his website, GRC.com, as well as transcripts.  It takes about, what, a day or two after the show for...



STEVE:  Yeah.  I normally get it either - so today is Tuesday.  Normally Thursday morning I find that Elaine will have mailed them, and I get them posted as quickly as I can.  



LEO:  Show notes are also at GRC.com, and that's helpful if you want to see the pictures Steve talks about, or get the links to the research he's talking about.  Where do you keep those?  All in the same place as the shows; right?  It's all in the same place.



STEVE:  Yup, yup.  GRC.com/sn.  



LEO:  Okay.  We don't have that, but we do have audio and video of the show at TWiT.tv/sn.  And actually the way most people do it is they find their favorite podcast application, and they subscribe.  That way they get it each and every week.  You don't want to miss an episode.  Is there anybody listening right now, show of hands, who has listened to all 599 episodes?  Besides you and me, Steve.  Oh, I haven't even listened to all 599 episodes.  I've missed more than a few on vacations.  I bet you half the audience has listened to every show.  And the rest are trying to catch up.  Bill in Michigan raises his hand.  Good job.  Strengths raises his hand.  Yeah, all of them, all of them.  Wait.  Jay is saying, all?  Yeah, all, 599 episodes.  Thank you for listening.  We appreciate it.  And we will see you next Tuesday on Security Now!.



STEVE:  Thanks, Leo.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#600

DATE:		February 21, 2017

TITLE:		The MMU Side-Channel Attack

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-600.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week, Leo and I discuss the completely cancelled (amid a flurry of serious problems) February Patch Tuesday; that it's not only laptop webcams that we need to worry about; the perils of purchasing a previously-owned Internet-connected auto; Chrome changing its UI making certificate inspection trickier; the future of Firefox add-ons; that Win10's lock screen is leaking the system's clipboard; a collection of new problems for Windows; an amazing free crypto book online from Stanford and New York University; pfSense and Ubiquiti follow-ups; a bit of geek humor and miscellany - and a deep dive into yet another sublime hack from our ever-clever friends led by Professor Herbert Bos at the University of Amsterdam.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here with a couple of fun photos, two Pictures of the Week - well, a cartoon and a picture.  How processors are made.  Oh, that's exciting.  We also have a look at a new exploit that is unfixable on modern hardware.  It's the side-channel MMU attack.  Well, we'll find out what that is.  Get your propellers and your propeller hats ready.  It's time for a geek-out episode of Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 600, recorded Tuesday, February 21st, 2017:  The MMU Side-Channel Attack.



It's time for Security Now!, the show where we cover your security and privacy online with the Explainer in Chief himself.  Look, I can hold his head in my hand.  Steve Gibson is here from the Gibson Research Corporation.  I'm actually virtually holding your head in my hand.  Steve joins us via Skype each week to talk the latest security news.  And today it's an exploit.  You like to talk about exploits; right?



STEVE GIBSON:  Well, yes.  So first of all, Episode 600.



LEO:  Oh, wow, I forgot.



STEVE:  Yeah.



LEO:  Wow.



STEVE:  Yeah, yeah, yeah.  We've hit 600.



LEO:  We're doing almost exactly 52 a year, with a few exceptions.  



STEVE:  Yeah.  Yeah.



LEO:  So this is our - that can't be right - 12th year?



STEVE:  You know, it's going to really foul me up if we have to go to four digits.  So I think we ought to just plan on 999, if that's where we're...



LEO:  We'll stop in eight years?  We'll stop?  That'll be it?  It'll be over?



STEVE:  I think we'll be done.  We'll be fried by that point.



LEO:  We'll be in our late 60s.  There'll be a new president eight years from now.



STEVE:  That's probably the case, well, in all likelihood that's true.



LEO:  Life will be very changed.  I mean, imagine what we've seen in eight years of technology evolution.



STEVE:  Oh, yeah, yeah, yeah.  I mean, during the 12 years of this podcast we've seen, you know, we've gone from the quaint little email macros...



LEO:  Honey monkeys, or what was that, yeah.



STEVE:  Right.



LEO:  Melissa.



STEVE:  To what we're going to discuss today is yet another piece of sublime engineering from our friends who are led by Professor Herbert Bos at the University of Amsterdam.  They're the people who gave us Flip Feng Shui and Drammer.  And they've figured out how to, in JavaScript in a browser, leverage the innate functioning of the memory management hardware which is present in all contemporary CPU architectures in order to break, to robustly break address space layout randomization.



LEO:  ASLR.  Oh, wow.



STEVE:  Yeah.  So anyway, so this is, okay, you may be able to walk and chew gum at the same time, but you're not going to be able to listen to this podcast, the latter half of this podcast, AND walk and chew gum.  So choose two out of those three.



LEO:  This is what we call a propeller hat episode.



STEVE:  This is going to be, yes, we're going to lose some people.  I don't want to.  And I'll take this easy.



LEO:  Take it as a challenge, my friends.  Can you continue to listen to this show?



STEVE:  Yeah, so stop chewing or stop walking.  Maybe this is where you put your car on auto drive because, boy, this is going to take - this is going to need everything you've got.



LEO:  Holy moly.



STEVE:  But I think everyone's going to enjoy it.  And we have a bunch of interesting news.  We've got Microsoft's February Patch Tuesday changed from "delayed" to "entirely cancelled," amid a flurry of serious problems.  They updated, saying, oh, we're not even going to do it this month.  Wait till March.



LEO:  What?  They canceled it?



STEVE:  They completely canceled it.  So, I mean, and it's not because there wasn't anything in need of fixing.  There's like some serious problems.  We've got the SMB zero-day flaw still out there flapping in the breeze, and a couple other things that we'll talk about today.



It turns out that it's not only laptop webcams that we need to worry about.  Oh, and a really interesting story, actually from an IBMer through the RSA Conference, about the perils of purchasing a previously owned, Internet-connected auto.



LEO:  Whoa.



STEVE:  Yes.  Chrome 56 changed its UI, making certificate inspection more difficult.  We've got a workaround for that, at least for Windows.  I don't know if it works on other platforms.  Some news about the future of Firefox add-ons, which is a definitely mixed blessing.  It turns out that Windows 10's and also 8.1's lock screen is leaking the system's clipboard data, and Microsoft says they don't care.



LEO:  Oy.



STEVE:  There's the collection of new problems for Windows.  An amazing free crypto book online from Stanford and New York University professors that we'll talk about.  Some quick follow-ups to pfSense and Ubiquiti that we've been talking about recently.  A fun bit of geek humor, a little bit of miscellany, and then a deep dive into yet another sublime hack from our ever-clever friends from the University of Amsterdam, who are  that team that Herbert Bos leads.



This just came out of - actually it came out of, well, it went public last Tuesday and just totally ate up my Twitter feed.  All of the press got it wrong of course because, as I said, this is a wind-up your propeller.  So we'll talk about that.  And so many people were tweeting, saying, do you know about this?  Do you know about this?  And it's like, yeah, I haven't been able to say anything about it because it was embargoed until they made news.  And so I've been waiting to talk about it because, again, it's just, wow, another beautiful piece of work.  So I think we've got a great two hours for our listeners.



LEO:  Wow.  All right.  Let's hit the week's news.  I guess that's next.



STEVE:  Well, we do have our Picture of the Week.



LEO:  Oh, of course we do.



STEVE:  Which is a fun little cartoon.  It shows a married couple, husband and wife.  He's in front of his laptop, and she's sort of looking over his shoulder.  And he says, in the caption, "Of course this website is safe.  As an extra measure of security, they make you sign in with your Social Security number, mother's name, your bank account, home address, phone number, and date of birth."



LEO:  What could go wrong?



STEVE:  How could a bad guy ever find out all those things?  So of course it's secure.



LEO:  Yeah.



STEVE:  That's right.



LEO:  And then they store it.



STEVE:  Yeah.



LEO:  For future reference.



STEVE:  That's right.  Just to make sure you don't change anything.  So as we mentioned at the top of the show, the February 2017 Security Update was formally canceled.  In an update that Microsoft Security posted later that Tuesday, when we referred to it a week ago, they formally said "Update on 2/15," so it was Wednesday, the day after, we will deliver updates as part of the planned March Update Tuesday, March 14, 2017.  So don't be holding your breath, anybody.  I had had it on my mind because I know that there are so many significant pending issues.  And the presumption was that, well, in fact, they originally said that, you know, they were a little obscure.  It was like, uh, something has just recently come to our attention that we need to deal with, so we're going to be delaying the update.  Well, yeah, a month.



And actually it may have been just the fact that they've established this monthly cycle, that there's too much blowback, even at the expense of delaying a fix for something important, too much blowback about an out-of-cycle patch that they said, well, you know, it's going to upset too much of corporate customers for us to just drop this thing a week or two late.  So we're just going to stay with the plan, stay with the normal Patch Tuesday cycle.  I mean, because I imagine they must have these problems fixed by now, but they just - they must have decided it's too much of a problem for us to do it out of cycle.



LEO:  I mean, we don't know, obviously.



STEVE:  We don't know, yeah.



LEO:  They haven't said.



STEVE:  Oh, yes.  I mean, some of these problems, they're just - they're not difficult to fix.  I can't figure out what's going on.  So there's a U.S.-based industrial cybersecurity firm named CyberX Labs in Framingham, Massachusetts.  Their blog disclosed the results of some of their research, which was interesting.  Their posting was titled "CyberX Discovers Large-Scale Cyber-Reconnaissance Operation Targeting Ukrainian Organizations."  And in their blog they wrote:  "CyberX has discovered a new, large-scale cyber-reconnaissance operation targeting a broad range of targets in the Ukraine."  And it's funny, too, because I think I heard you, Leo, talking about Ukraine versus The Ukraine?



LEO:  Yeah.



STEVE:  And, like, why does...



LEO:  They don't like "The Ukraine" anymore.



STEVE:  Yeah.  And I thought, okay, so I'm trying to say Ukraine, not The Ukraine.



LEO:  It's hard.  We've been trained for years to say The Ukraine.



STEVE:  Yes.



LEO:  You don't say The Italy or The France.



STEVE:  Precisely.  Or The America.  But we do say The United States.



LEO:  Oh.



STEVE:  So I was sort of wondering if somehow its history of The Ukraine is connected to The United States.



LEO:  I think it's connected to The Soviet Union and not wanting to be seen as a region of the Soviet Union, but instead a whole country.



STEVE:  Right.  So anyway, I was self-conscious about the use of "the" throughout their blog posting.  And I thought first, before I realized they were in the U.S., I thought, oh, well, maybe this is a definitive non-U.S.-biased source of where "the" should be used.



LEO:  I have been told in no uncertain terms, there's no "the" there.



STEVE:  Interesting.



LEO:  It's Ukraine.



STEVE:  Interesting.  So range of targets in Ukraine is what it should say, not the Ukraine.



LEO:  That's correct.



STEVE:  In Ukraine.  Okay, well, so forgive me for any "the's" that are in their blog posting, but I didn't put them there.



LEO:  We'll send them a letter, an angry letter.



STEVE:  It's good to know that, definitively, no more "the."



LEO:  Right.



STEVE:  So they say:  "Because it eavesdrops on sensitive conversations" - and here's the point, or one of the most scary factors - "by remotely controlling PC microphones in order to surreptitiously 'bug' its targets, and uses Dropbox to store exfiltrated data, CyberX has named it 'Operation BugDrop.'  CyberX has confirmed," they write, "at least 70 victims successfully targeted by the operation in a range of sectors including critical infrastructure, media, and scientific research.  The operation seeks to capture a range of sensitive information from its targets, including audio recordings of conversations, screenshots, documents, and passwords."  So this is a comprehensive trojan of some sort.



"Unlike video recordings, which are often blocked by users simply placing tape over the camera lens, it is virtually impossible to block your computer's microphone without physically accessing and disabling the PC hardware."  Which, as we know, is true.  Maybe disable the media driver, the hardware-level driver to access the microphone.  But that's not obvious.  And covering the microphone hole, if you could even find it, with a piece of tape, well, that might muffle the audio a bit, but it's not like blocking it the way you can sticking tape over the camera.



So they said:  "Most of the targets are located in the Ukraine, but there are also targets in Russia" - not The Russia - "and a smaller number of targets in Saudi Arabia and Austria.  Many targets are located in the self-declared separatist states of Donetsk" - I didn't practice pronouncing this before.



LEO:  Don't look at me for help, buddy.



STEVE:  "...Donetsk and Luhansk, which have been classified as terrorist organizations..."



LEO:  I believe that's "the Donetsk and Luhansk."



STEVE:  I think so, "...by the Ukrainian government.  CyberX is keeping the identities of the affected organizations private, but give us a high-level peek, saying:  'Examples of Operation BugDrop targets identified by CyberX so far include:  a company that designs remote monitoring systems for oil and gas pipeline infrastructures; an international organization that monitors human rights, counterterrorism, and cyberattacks on critical infrastructure in the Ukraine; a scientific research institute; an engineering company that designs electrical substations, gas distribution pipelines, and water supply plants; and editors of Ukrainian newspapers."  So, you know, this really does sound - we don't have any reason to believe it's Russian backed.  But it sure seems like it might be.



"Operation BugDrop," they write, "is a well-organized operation that employs sophisticated malware and appears to be backed by an organization with substantial resources.  In particular, the operation requires a massive backend infrastructure to store, decrypt, and analyze several gigabytes per day of unstructured data that is being captured from its targets.  A large team of human analysts is also required to manually sort through captured data" - like, listen to all of that - "and process it manually and/or with Big Data-like analytics.



"Initially, CyberX saw similarities between Operation BugDrop and a previous cyber-surveillance operation discovered by ESET in May 2016 called Operation Groundbait.  However, despite some similarities in the Tactics, Techniques, and Procedures" - we have a new acronym, TTPs, Tactics, Techniques, and Procedures - "used by the hackers in both operations, Operation BugDrop's TTPs are significantly more sophisticated than those used in the earlier operation."



And they go into some additional details that leads us, I mean, they're like, there are connections to Duqu and Stuxnet and high-level work.  So no clear attribution for this.  But a serious, persistent campaign that was discovered by these guys.  So props to them.  And it's just nice, even though it's not affecting us, and there's nothing we can do about it, it's sobering to recognize that this kind of high-budget, large data-focused campaign is present, probably all over the place in the world.  This is just an example of that.



Graham Cluley, who is a well-known security blogger, provided some nice coverage for a story that many outlets picked up.  CNN he quotes at one point.  And this is from a presentation that Charles Henderson of IBM gave at the RSA Conference recently.  He's the global head of X-Force Red at IBM.



And so Graham wrote:  "Not too long from now it will be pretty much impossible to purchase a new car which is not connected to the Internet in some fashion.  Many modern purchasers are more swayed by the gadgetry bells and whistles their car includes than its performance, and within a world where everything seems  to have an associated smartphone app, why should vehicles be any different?  If most new cars," he writes, "are going to be Internet-enabled, then you know what that means?  Yup.  Second-hand cars are going to be increasingly 'smart' as well as vehicles are sold on after a few years."  I don't know how that parses.



He says:  "Oh, and yes, as I'm writing on We Live Security, it should go without saying that it also means security threats.  This point was brought home last week at the RSA Conference in San Francisco, where IBM's Charles Henderson described how - over two years after he had traded it back in to the original authorized dealer - he was still able to access his old car via a smartphone app.  Despite deauthorizing" - and again, this is not a newbie.  This is head of X Force Red at IBM, speaking at the RSA conference.  "Despite deauthorizing all associated accounts, satellite radio, garage door openers, resetting the Bluetooth, as well as surrendering all the keys at the time of sale" - so this guy Charles Henderson completely did everything he knew that he could to divest himself of access to the car - "he discovered that his mobile app never forgot his old car.



"The app allowed Henderson to track the geolocation of the car, adjust its climate control, send its SatNav systems new directions, and even trigger its horn.  But perhaps most alarmingly of all, the app also gave Henderson the ability to remotely unlock the vehicle."  And this is two years after it was gone from him.



"Fortunately," writes Graham, "the IBM researcher isn't one of the bad guys.  But it's easy to imagine how a car thief or stalker would exploit such a feature.  As Henderson explained to CNN:  'The car's new owners would have no clue that they were potentially at risk.'"  Quoting him in his interview with CNN:  "The car," he said, "is really smart, but it's not smart enough to know who its owner is, so it's not smart enough to know it's been resold.  There's nothing on the dashboard that tells you 'the following people have access to the car.'"



"It turns out that, although Henderson took more effort than probably most people in ensuring that he had wiped the car's knowledge of him and associated accounts before trading it in, that wasn't enough.  As the researcher explains, that's because a full factory reset of the unnamed vehicle does not revoke access by the smartphone app.  The information still lurks in the cloud and can only be wiped by a factory-authorized dealer.  One has to wonder," writes Graham, "how often that occurs.  Henderson's own investigation discovered four major vehicle manufacturers were allowing previous owners to access cars from a mobile app.



"This is the Internet of (insecure) Things at work again.  In the rush to add bells and whistles," writes Graham, "features are not being properly thought through" - as we well know on this podcast - "and security is not uppermost in manufacturers' minds.  Until more effort is made by vendors to integrate the Internet in a safe way into the myriad of devices that surround us, we are going to hear more and more stories of security breaking down like this."



And it's like something that had never occurred to me.  I mean, I guess it's - I don't know how you flush that.  Maybe you delete the app and  then reinstall it?  But a lot of these apps, you know, they use the cloud to keep themselves synchronized.  So a really interesting oversight on the fact of, apparently, four major car manufacturers are still allowing this to be possible.



So I mentioned also that Chrome 56 has a UI update that took out a feature that I imagine many of our listeners would have appreciated, or had appreciated in the past.  It used to be that, when you were at a secure site, and you wanted to check the certificate, for example, you'd like to know if there's a man-in-the-middle attack or a middlebox is filtering your connection, that is to say, is the certificate actually from eBay or Gmail or your bank?  Or is it from some manufacturer of a TLS-intercepting proxy that's in your connection?  The only way to know is to look at the certificate and see what it says.



It used to be that you could click on the padlock in Chrome, and you would get a little pop-up where it would give you the domain.  And I did it to myself this morning, www.grc.com.  Then underneath it says, "Your connection to this site is private."  And then it says "Details," which is a hyperlink.  You click on that.  And in all versions of Chrome prior to 56, that is, up to 55, that then presents you with the certificate information.  And that certificate information is the Windows UI under Windows, and it's the Mac UI on the Mac because Chrome has traditionally used the existing secure infrastructure in the underlying OS to perform its TLS stuff.



Well, the ability to do that is gone in Chrome 56.  If you click on the padlock, you no longer have an option to view the certificate.  Which, again, they probably simplified it for most users.  Maybe this is a consequence of some rearchitecting of the way they handle security.  I didn't take time to dig more deeply into this.  You can still find it on all versions of Chrome 56 under the three dots or three bars menu.  You go there and look under More Tools, and then Developer Tools, and then click on the Security tab, which you may not even be able to find because it's way off to the right, and it gets truncated with the little chevron.  So you may have to click the chevron.  Then you'll find Security Overview.  And then there, there is a View Certificate button.  So they made it a lot more involved, at least for Windows users.



There's a shortcut, however, which is not documented anywhere that I could find.  And that's Ctrl-Shift-I.  So for our listeners, who tend to be certificate-aware, if you are using Chrome 56 and later, Ctrl-Shift-I, it still won't pop up the original convenient certificate display.  But that jumps you immediately to the proper page, the Security Overview page in the Developer Tools area, and does allow you to pretty quickly get a hold of the certificate.  So Ctrl-Shift-I for some reason.



So Firefox has been moving forward with their plan to evolve the browser.  And over the year of 2017 they have a succession of updates planned which take us from 51, where I am today, up to 57.  And unfortunately there are deliberate and side effect consequences to some of the things that they're going to be doing.  They'll be putting the finishing touches during 2017 up through November on their plan, which they put in motion in 2015, to replace their original add-ons API.  Theirs was an ad hoc, you know, their own system, which had the advantage of more flexibility than the Chrome approach.  Chrome's system and the Chromium-based browsers - Chrome, Vivaldi, Opera, and others - they're based on a WebExtensions API.



So the mixed blessing is Firefox has been moving to the WebExtensions API.  But the existing Firefox extensions are not WebExtensions APIs.  So as they move through this year to WebExtensions, as a consequence of other things they're doing, the ultimate upshot, by the time we get to Firefox 57 in November, is the entire existing Firefox add-ons ecosystem dies.  Which makes me nervous because, for example, there are not yet equivalent extensions in Chrome that I depend upon for Firefox.  There are various workarounds.  And largely they're the same.



So what Firefox gains from this, of course, is a single WebExtensions API that it will then share with the other Chromium-based browsers.  So that's cool.  That means that - and it's great for developers that then can only need to write one extension which will then run, not only on Chrome, but also on Firefox.  So Firefox inherits the extension API of what has now become with Chrome the world's number one browser, which long-term is probably good for them.  It does create some pain for we Firefox users who in the meantime, our days are numbered, maybe, with some of our favorite extensions.  We know that things like uBlock Origin, that's already on Chrome.  LastPass is already on Chrome.  Many things are.



I really like my hierarchical tabs.  And I know that there are a few tab solutions.  Nothing that I've seen yet replaces what I've got on Firefox.  So I can hope that maybe some of the existing add-on developers will choose to move their work over to WebExtensions, which would also give them access to the number one browser in the industry, which would be useful.  So on Firefox 53, which is slated for release on April 18, there will be no new legacy add-ons, as they call them, will be accepted any longer.



So they'll support existing ones, but no new ones will be accepted at addons.mozilla.org.  However, developers and users will be able to update preexisting legacy add-ons, and Firefox will start running with multiprocess support for most users.  I tried to turn mine on, thinking, well, maybe that's a good thing.  But it's been disabled by one or more of the legacy add-ons that I have.  So that's one of the problems is that there has been some conflict with the behavior that Firefox brings with multiprocessor support and its incompatibility with legacy add-ons, which is one of the other reasons that they're beginning to move away.  It's like, okay, they're just going to have to force the issue.



So any Firefox add-ons that are confirmed as multiprocess-incompatible will be disabled in Firefox with Firefox 53 on April 18th.  So this incompatibility is being flipped.  Right now the incompatible add-ons are able to disable the multiprocess feature.  With Firefox 53, it goes the other direction.  Multiprocess feature stays, and incompatible add-ons, they're disabled.  So that's an important takeaway.  Then over Firefox 54, 55, and 56, Firefox will be expanding its sandbox security features to incorporate multiprocessor support and also expanding.  Right now that's only two process.  You have sort of a UI process and then everything else.  And that will be expanding from two to three and more processes in order to create more robustness and to deliver more performance.



And then on Firefox 55, although there's no firm date set for that, the legacy add-ons will stop working due to this migrating change.  And then, finally, with Firefox 57, which is due to release on November 14th, that's the end of the line for old Firefox add-ons.  If developers don't migrate their code from the old add-ons API to the new WebExtensions API by November 14th, their add-ons will stop working for good.  So that means that, with Firefox 57, it will only run add-ons built to the new WebExtensions API.  For some reason, addons.mozilla.org will continue to list the legacy add-ons.  But if you've been updating your Firefox and kept it current, it won't run them any longer.  And they have not said at what point they will stop providing the legacy.



Oh, there is one possible reason, and that is there are some of the Firefox derivatives, derivative browsers, like Firefox ESR and Pale Moon.  They may still provide a safe harbor for the legacy add-ons, and so you'd be able to get them, for that reason, from there.  So Mozilla says that the reason old legacy add-ons will stop working in Firefox 57 is because they plan to remove from Firefox 57 a workaround that allowed non-multiprocess code to run on the new Firefox engine.  So it's just that's over at that point.  And removing the workaround means that no code will run in Firefox if it hasn't been optimized for multiprocessor support.



And at this point it would be crazy for a legacy add-on developer to update their add-on.  They might as well just recode for the WebExtensions API, which, again, it's somewhat more limited.  There are some things that it cannot do which may end up being restrictive for the sake of security and so not a bad thing anyway.  So anyway, a little bit of a bumpy road for the next year.  I don't, at this point, my problem with Chrome is that it's just so memory hungry.  And I'm still operating within a 32-bit, 4GB, actually 3GB under Windows, constrained address space.  So with Chrome burning up as much memory as it does, I'm still using Firefox, and happy to do so.  But I know that we have a lot of Firefox users among us, and so I wanted to let everybody know what the future looks like for 2017 in Firefox.



LEO:  Steve Gibson, Leo Laporte.  We're talking security and the Windows Clipboard.



STEVE:  So, okay.  Earlier this year a Norwegian MVP, Oddvar Moe, made a rather shocking discovery that went mostly under the radar.  He found that on Windows 10 there is a way to easily read clipboard contents, that is, the system's global clipboard, from the screen of a locked machine without requiring any form of authentication.  So this is a problem.  In any enterprise environment, any coworker could easily go through a bunch of PCs at lunchtime, harvesting potentially juicy information such as passwords, without leaving any traces.  So how do you do this?  It's pretty simple.  The Win+L locks the workstation.  So you can imagine employees locking their workstation with Win+L and then going off to lunch.  Well, from the lock screen, Win+ENTER starts the Narrator.  Then CapsLock+F1 opens Narrator Help, and Ctrl-V pastes the clipboard into the narrator and exposes its contents.



LEO:  It starts reading you the clipboard.



STEVE:  Yes.



LEO:  Wow.  All of this without unlocking.



STEVE:  All of this without unlocking.  And in the write-up it said this can also be achieved through the WiFi selector UI on the lock screen.  So there are a couple ways to do this.  And it not only affects Windows 10, but it's confirmed to be effective on 8.1  According to Oddvar, Microsoft does not consider this to be a security issue because it requires physical access.  Okay.  So there are some mitigations, and I have a solution for our listeners.  Possible mitigations include disabling these features through the appropriate Group Policy settings, or using a cute little utility called ClipTTL.



And as we know from our discussion of the Internet and packets, TTL often stands for Time To Live, as is the case here.  It's a small utility, written by Firas Salem, to protect against this and other cases of accidental clipboard pasting.  And since it is the case often that the clipboard can contain some sensitive information because we use it as a little transport carrier from one place to another, ClipTTL is an automatic clipboard wiper which prevents accidental or malicious pasting.



Firas is a Brussels, Belgium-based security consultant whose work we've covered before.  He also wrote that RCC utility which many of our listeners were interested in because it finds potentially rogue certificates in Windows and Firefox.  It looks for any certificates that are not part of the baseline installation.  So he's at TrustProbe.com.  I've got the links in the show notes.  The utility that automatically wipes your clipboard after a set about of time is ClipTTL.  But there has been some additional people saying, what was that utility that lets me audit my trust store?  And that's RCC, also by the same guy.  And in fact he's got a whole bunch of interesting bits of freeware that are sort of related and in this environment that I would recommend people check out.  He's a good guy.



LEO:  That's why LastPass somehow, I don't know how it does this, it's kind of magical...



STEVE:  Yes.



LEO:  ...allows one, and one only, paste; right?



STEVE:  Correct.  And...



LEO:  It clears the clipboard after you paste.



STEVE:  Yes, it does.



LEO:  How does it do that?



STEVE:  And it's caught me out a couple times, where it's like, I was assuming it would - like I needed it still to be there.



LEO:  And paste it several times, right.



STEVE:  Yeah, like for example I would have it generate a password, put it on the clipboard, then I would paste it into a web app, but I also then wanted to make my own private copy.



LEO:  Right.



STEVE:  And so I'd go over and open Notepad and try to paste it, and it was gone.  It's like, ooh.



LEO:  There must be a command or some sort of pasteboard command that does that.



STEVE:  Oh, yeah, yeah.  Yeah, there is.  Well, for example, if you've ever exited an app, and you'll get a popup from Windows saying the app that you've just...



LEO:  We're going to clear your clipboard, yeah, yeah, yeah.



STEVE:  Yes, the app that you've closed just left a whole bunch of stuff there.



LEO:  Yeah, that's embarrassing.



STEVE:  Like, uh, no, I don't want that.



LEO:  No, no, no, no.



STEVE:  Get rid of that, please.



LEO:  Yeah.  



STEVE:  So on November 16th of 2016, a little over 90 days ago, a problem was reported to Microsoft.  And again, I don't know what's going on up in Redmond.  But this was Project Zero gave Microsoft, and confirmed receipt, a formal notice of a worrisome problem that had been found, actually a number of worrisome problems, in a commonly used API known as SetDIBitsToDevice.  Actually, that's a friend of mine, that API.  I used it way back in the days of 16-bit Windows when I wrote ChromaZone, that crazy screensaver toolkit app that's what I used to teach myself the Windows API.



And I was using it just recently.  SetDIBits, DI stands for Device Independent Bits.  So it's a way of supplying bitmap information to a device and, for example, like a way of programmatically drawing a bitmap.  I use it in SQRL, for example, in order to display the QR codes for transfer between devices.  So it's a way for me to take an algorithmically generated binary QR code and display it.  You SETDIBITSTODEVICE.



Well, it turns out that the ever-present metafiles, speaking of Honey Monkeys and the very early episodes of this podcast, we covered the controversial escape, which I'm convinced Microsoft originally had in the Windows metafile on purpose.  But then a decade later it didn't look nearly as benign as it originally did when security wasn't such an issue.  Well, it turns out that after metafiles we got enhanced metafiles.  And metafiles are interpreted, essentially interpreted API calls.  So when you process a metafile, you are actually reading the API and its parameters.



So what the Project Zero guys discovered was that there were some mistakes in the verification of parameters to the SETDIBITSTODEVICE API, and actually a number of others.  So that was on November 16th, 2016.  Microsoft was informed.  Well, last Tuesday, February 14th, which was supposed to be Patch Tuesday, but as we know wasn't for whatever reason, that was the 90-day expiration on the Project Zero timer.  And all the details of this went public last Tuesday.  So what went public is a full documentation of the problem, including a proof of concept, which demonstrates...



LEO:  I can just see them at Microsoft, ready to push the button on the update.  This comes out, and they say, "Did you know about this?  I think he told us, didn't he?"



STEVE:  I mean, you almost - it almost feels like a communication screw-up of some...



LEO:  It must be.  They ignored it for three months.



STEVE:  Yes.  Yes.  And it's not a hard problem.  So reading from this, "The proof-of-concept file attached here," they say in their disclosure, "consists of a single EMR_SETDIBITSTODEVICE record" - so excluding the enhanced metafile header and end-of-file records - "which originally contained a 1x1 bitmap.  The dimensions of the DIB, the device-independent bitmap, were then manually altered to 16x16, without adding any more image data.  As a consequence, the 16x16, 24 bits per pixel" - which is to say 3 bytes per pixel - "bitmap is now described by just 4 bytes," which of course is no longer necessary.  It needs, what is it, 16x16 is going to be 256.  Three bytes, that's 768 bytes.  But the API only gives it four, which they say is "good for only a single pixel.  The remaining 255 pixels will be drawn based on data from the heap, which may include sensitive information, such as private user data or information about the virtual address space."



The author of this said:  "I have confirmed that the vulnerability reproduces both locally in Internet Explorer and remotely in Office Online via a .docx document containing the specially crafted EMF file."  So what this means is that the abuse of this API allows private data from the heap to be turned into an image which can then be captured and exported.  So this is not good.  And it's been public for a week.  And Microsoft will be updating this and hopefully fixing this in March.  So until then, good luck to us.  And it's all public now.



LEO:  Yeah.



STEVE:  Project Zero also having been busy, spent some time looking at the Windows NVIDIA driver.  Oliver Chang posted a nice little summary, saying:  "Modern graphic drivers are complicated [yeah, indeed] and provide a large promising attack surface for [what he writes] EoP [which is] elevation of privilege, execution opportunities, and sandbox escapes from processes that have access to the GPU [such as] the Chrome GPU process.  In this blog post," he writes, "we'll take a look at attacking the NVIDIA kernel mode Windows drivers, and a few of the bugs that I found."  Yeah, 16.  He says:  "I did this research as part of a 20% project with Project Zero, during which a total of 16 vulnerabilities were discovered."



And I won't go through them all because they've been fixed.  And in fact they were fixed impressively.  Describing NVIDIA's response, he wrote:  "The nature of the bugs found showed that NVIDIA has a lot of work to do.  Their drivers contained a lot of code which probably should not be in the kernel, and most of the bugs discovered were very basic mistakes.  One of their drivers, NvStreamKms.sys, also lacks very basic mitigations, [for example, like] stack cookies, even today."  So a lack of even automated security mitigations that could have easily been turned on with a proper compilation.



"However, their response," he writes, "was mostly quick and positive.  Most bugs were fixed well under the deadline, and it seems" - unlike Microsoft's - "and it seems that they've been finding some bugs on their own internally.  They also indicated that they've been working on re-architecting their kernel drivers for security, but were not yet ready to share any concrete details."



So he concludes, saying:  "Given the large attack surface exposed by graphics drivers in the kernel and the generally lower quality of third-party code, it appears to be a very rich target for finding sandbox escapes and elevation of privilege vulnerabilities.  GPU vendors should try to limit this by moving as much attack surface as they can out of the kernel."



And of course this has been a constant refrain of ours.  One of the arguably, in retrospect, worst things Microsoft did was to move GDI, the Graphics Device Interface, from userland, from being a DLL in user mode, where it was subject to the same restrictions as the programs which invoked it.  Unfortunately, back at a time when processors were having a hard time keeping up, and Microsoft graphics performance was lackluster, Microsoft decided, okay, there's just too much ring transition between GDI and the kernel.  So they moved GDI into the kernel.



And this is - that's the reason that, for example, we then had all of these problems with graphics vulnerabilities, where a JPEG can take over Windows.  It's because the JPEG is being interpreted in GDI, which is now in the kernel, which makes any escape from GDI subject to having ring zero root privileges, essentially.  So what they should have done is kept it out in userland, in which case, even if you did find a problem, it would be like a bug in the user's software, rather than a bug in the kernel.  So anyway, it would be great if - again, it's just sort of - the problem is it's easier to keep it all as one lump and just stick it all in the kernel.



The problem then is all of that code has to be perfect.  If instead you go to the effort of bifurcating this job so that the only code in the kernel is what has to be there, then you keep most of the bulk of the work in the user space, where it's under ring three and subject to all of the protections that our state-of-the-art hardware provides.  So anyway, we'll hope that this is growing pains.  But I was impressed with NVIDIA's response.  They jumped right on it, and they fixed all 16 problems.  I did see a summary of them and scan them, and they were just like, fixed, fixed, fixed, fixed, fixed, fixed, fixed, right down the line, and did so quickly.  And they said that they're working to make things better.



So I have a treat for our listeners:  toc.cryptobook.us.  That'll take you to the table of contents of a 580-page tour de force, freely downloadable PDF, graduate course in applied cryptography.  We've talked about this Victor Shoup in the past.  He's a professor of security and cryptography at New York University.  And Dan Boneh is a professor of computer science and electrical engineering at Stanford.  He's also the co-director of the Stanford Computer Security Lab.  These two profs are authoring this book.  It is at 0.3 version, not yet finished, but already really comprehensive.



Describing it in its preface, they said:  "Cryptography is an indispensable tool used to protect information in computing systems.  It's used everywhere and by billions of people worldwide on a daily basis.  It's used to protect data at rest and data in motion.  Cryptographic systems are an integral part of standard protocols, most notably the Transport Layer Security (TLS) protocol, making it relatively easy to incorporate strong encryption into a wide range of applications.



"While extremely useful, cryptography is also highly brittle.  The most secure cryptographic system can be rendered completely insecure by a single specification or programming error.  No amount of unit testing will uncover a security vulnerability in a cryptosystem.  Instead, to argue that a cryptosystem is secure, we rely on mathematical modeling and proofs to show that a particular system satisfies the security properties attributed to it.  We often need to introduce certain plausible assumptions to push our security arguments through.



"This book is about exactly that:  constructing practical cryptosystems for which we can argue security under plausible assumptions.  The book contains many constructions for different tasks in cryptography.  For each task we define a precise security goal that we aim to achieve and then present constructions that achieve the required goal.  To analyze the constructions, we develop a unified framework for doing cryptographic proofs.  A reader who masters this framework will be capable of applying it to new constructions that may not be covered in the book.



"Throughout the book we present many case studies to survey how deployed systems operate.  We describe common mistakes to avoid, as well as attacks on real-world systems that illustrate the importance of rigor in cryptography.  We end every chapter with a fun application that applies the ideas in the chapter in some unexpected way."  And it goes on.  Then the book goes on for 580 pages.  I browsed through it.  Everything is there:  symmetric crypto, public key, hashes, you name it.  Over the years, many of our listeners have asked for recommendations of books.  And of course Schneier's "Applied Cryptography" is still the bible, but it's not free.  This one is.



They're not finished.  I think they said they have three major parts, and they have all of part one and half of part two finished.  But what's there already is very comprehensive and worthwhile.  So again, TOC dot - it's got its own domain:  toc.cryptobook.us.  And there's a link there that will take you to crypto.stanford.edu, where you can find it.  So links are in the show notes.  The show notes are already posted publicly at GRC.com/sn.  I've got the header for this Episode 600 is already there with the link to the show notes.  So everybody should be able to find it without any trouble.  And it looks great.



So if anyone is looking for something to just browse through casually or to really roll up your sleeves, I know that from the past a lot of our listeners are interested.  This looks like the place to go.  And this thing is only going to get better with time as the profs continue to flesh it out.  And this one, this is - it had a 0.1, 0.2, and 0.3.  This 0.3 was just released in December of 2016, so a few months ago.  And they're working on finishing it.  Looks like it's going to be a nice piece of work.



And they did note, I didn't describe it, but that there is a deliberate hierarchical nature, that is, you can skip the proofs.  You can skip a lot of the formalism and still get a lot out of it, just by reading sort of the introductions to each section and the goals of the various algorithms, without getting down into the weeds.  The weeds are there, but you don't need to bother with them.  And they explicitly have written it this way so that there is a softer, gentler side.  It's not just for people who want to do formal proofs for crypto.  There is sort of an upper layer that will still give you a lot.



And I wanted a couple pfSense and Ubiquiti follow-ups.  I got a tweet from a Dan Moutal, whose Twitter handle is @scruffydan.  He said:  "Listening to this week's Security Now!," he said, "and the EdgeRouter also allows you to restrict UPnP by IP address."  Now, that's something I didn't know, so I wanted to share that.  We've talked about the Ubiquiti EdgeRouter X, which we like so much because it is a true router with independent ports where each port can be its own subnet, which allows you to, for example, create an IoT region.  We were talking about, last week, I talked about how pfSense, part of the UPnP UI on pfSense absolutely has UPnP ACLs, Access Control Lists, to allow you to explicitly restrict what ranges of IP or specific devices on your LAN have access to Universal Plug and Play, because you really don't want that to give blanket permission.  You'd rather only provide it where you have to.



In fact, back when we were having connection problems, not it turns out due to anything we were doing, but because the relay server at Microsoft was bogged down that afternoon, one of the things that I did was thinking, well, maybe Skype now requires UPnP.  It certainly would like to have it.  Even though I had manually statically mapped a port and given Skype permission to use that statically mapped port, I thought, okay, well, maybe it just - it's not using that anymore, and it needs to use Universal Plug and Play.  So I for that reason enabled Universal Plug and Play, but only permitted the machine that I use Skype on for this podcast to have access to it.  So again, I'm practicing what I'm preaching here.



The point is that Dan brings up is that, if you do have a Ubiquiti EdgeRouter, as many of our listeners now do because it's $49 and a great bargain for that price, you do need to use the command-line interface.  It's not surfaced in the web UI, but the capability is there.  He says:  "You need to use the CLI and the UPnP2 service to create the UPnP ACL."  And the router is so popular, and there's so much stuff on the 'Net, that I'm sure, users, if you google for UPnP2 service, UPnP ACL, Ubiquiti EdgeRouter, you'll probably be able to find some how-tos and guides and things to help you with the command-line interface.  But the point is, it's in there.  And that would allow you  without any additional hardware to just use the Ubiquiti EdgeRouter X and enable UPnP for those devices on your network that need it.



Somebody else tweeted, he asked:  "What was the name of the new pfSense box that you mentioned on SN-599." last week.  And he said:  "I don't think you mentioned the name, just that it had two ports."  And so that is the SG, I'm sure no relation, -1000.  So if you go to pfSense under Products, you'll find the cute little two-port box, the two-port router that is running pfSense.  It's $149.  What I don't know, and I didn't dig in, is what its bulk throughput is.  That is, it does have a pair of gig interfaces, so a gig link to your upstream DSL or cable modem or wherever you're getting your bandwidth from; and a gig link then, probably to a switch, which would then fan out to go to and access other things.  The beauty is that this cute little thing, I mean, it's like the size of a matchbook, I mean, it's just big enough to hold two 10-base-T ethernet connectors.  So it's very cute and tiny, and very low power.



The beauty of it is it's running pfSense, which, as we know, I mean, that's the universal toolkit for this.  I mean, you can run proxies, you can run OpenVPN VPNs - I couldn't figure out if I was missing another word there - VPNs for point-to-point links.  And just you know, any other kind of security - full firewalls, traffic shaping, bandwidth limiting, rate limiting, I mean, it's everything you could want.  The question is, will this little box actually run your data at a gig?  Now, it doesn't really have to unless you have a gig upstream bandwidth.  But will it run it at 300Mb?  I just don't know.  So it's one thing to have the links capable of a gig.  But the question is, what is the packet rate throughput through it?  That I don't know.  But that's what it is, it's the SG-1000 for $149 from Netgate.com, which is where the pfSense.org page for their products takes you when you click on it.  And a beautiful little machine.



And finally someone tweeted:  "Hi, Steve.  I'm listening to SN-599.  You can use the Ubiquiti EdgeRouter X with the small pfSense box for network segmentation."  And so that's another point that I hadn't made.  I was talking about using that with a VLAN-aware switch because the pfSense box has full support for  VLANs, virtual LANs.  And what that essentially means is that different ranges of IPs could be tagged as members of VLAN 1, VLAN 2, VLAN 3.  Then when that goes to a VLAN-aware switch - you want to make sure that a little dumb switch is VLAN-aware - then it would perform the network segmentation.  So this person was tweeting to me, he just observed that the Ubiquiti EdgeRouter X will also support network segmentation.  So I wanted to make sure everyone knew that, too.  So we're ending up with a nice little not-very-expensive toolkit of components that allow us to structure a very robust and safe network.



A number of listeners reminded me of miniLock.io.  And this followed up from our conversation, Leo, last week about PGP and...



LEO:  AES Crypt.



STEVE:  And GPG and Keybase. 



LEO:  And Keybase [crosstalk], yeah.



STEVE:  Right.  And they reminded me that I had talked about miniLock some time ago because it was originated by our friend Nadim Kobeissi, who did Cryptocat and then did miniLock.io.  It's audited, peer reviewed.  And following on from what you said, Leo, and that is the notion that public key crypto makes much more sense for sharing encrypted content than symmetric key crypto.  And you're right, this provides a simple means to do that.  It is available as a Chrome add-on, so anyone with the most popular browser in the world, Chrome, can easily use this.  It uses your email and a secret passphrase to generate what they call a "miniLock ID."



MiniLock IDs are small because it also uses the Bernstein curve, the Curve25519, which allows us to use much shorter and more convenient keys than normal public key, traditional RSA public key crypto allows at the same level of security.  So it makes them easy to share online.  So anyone can use your ID, which again you generate with your email address and secret passphrase, to encrypt files to you, that is, encrypt them for your eyes only; and only you who have the matching private key to your miniLock ID, which is essentially your matching public key, then are able to decrypt them.  So it's been audited.  Matt, Matthew Green - in Maryland; right?  That's where Matt Green is?



LEO:  Yeah, he's at Johns Hopkins in Maryland.



STEVE:  Johns Hopkins, right.  He's gone over it and was involved and helped with this.  It's been audited independently and peer reviewed, developed by experts and so on.  So, for example, the user flow would be - we have the typical roles in crypto, Alice and Bob.  Alice wants to send a scan of her passport to Bob.  Sending it over email would compromise her personal information.



So Alice decides to first encrypt the scan using miniLock.  Bob opens miniLock and enters his email address and his own personal secret passphrase.  miniLock displays his miniLock ID, which is tied to and actually generated from his passphrase and is persistent.  He sends Alice his miniLock ID, which is a relatively short ASCII string, and he can tweet it to her.  He can email it to her.  I mean, it can be sent over a public channel, does not need to be kept secret because all that is his public encryption key, essentially.



Alice drags and drops her passport scan into miniLock, which again is a Chrome extension, and enters Bob's miniLock ID as the recipient.  She clicks the encrypt button and sends the resulting .miniLock file to Bob.  Once Bob drags the encrypted file into miniLock, it automatically detects it as a miniLock-encrypted file destined to Bob, and decrypts and saves the passport scan on his computer.  So no third-party software, just uses Chrome, uses Curve25519 for short and tweetable keys, and is a superior way to send encrypted content between two people who both have Chrome.  So thank you to our listeners for reminding me that we talked about this once before, and I just forgot to bring it up again last week.



LEO:  I just did it.  It's great, yeah.



STEVE:  Cool, cool.  Also last week our title was HTTPS Insecurity.  And the link that I tweeted, and I don't think I had it in the show notes, or maybe I did, I think I had it in the show notes, but it turns out that the document disappeared, and a whole bunch of people - actually I was very impressed with how many of our listeners wanted to dig deeper and follow up.  And they weren't able to because the document was gone.  So it's back.  When I saw that, I grabbed it, as I still had a copy from my own reading of it last week.  So I stick it up on GRC and tweeted it a few days ago.  It is also now in the show notes. So anybody who isn't in Twitter or missed my tweet can, in this week's show notes, down here in the notes is the link to grab that paper, if you're still interested.



[https://media.grc.com/misc/HTTPS-Interception.pdf].



And then we have two little bits of lighter side.  So we have a riddle:  Why are assembly programmers always wet?



LEO:  Why, Steve?



STEVE:  Because they work below C level.



LEO:  Oh, geez.  With the letter "C," obviously, yeah.



STEVE:  That's right.  Yes, we do.  And then the follow-on funny Picture of the Week, if you're a little geeky, is a photo that shows how computers are made in our show notes.



LEO:  I guess I should show that, huh?



STEVE:  I think you should, just so that it...



LEO:  Oh, because you can't really describe it, can you.



STEVE:  No, you really...



LEO:  Yeah, you just have to - you've got to see it to be there.  And if you're listening on audio, just download the show notes.  Steve tweets the URL out every week at @SGgrc, or you can go to  GRC.com/securitynow and get the show notes, and you'll see the image.



STEVE:  Right.  And I did tweet this in my feed a few hours ago.  I got a lot of people thought it was fun and funny.  



LEO:  It's a small IC and some resistors with squiggly tails, that's all.



STEVE:  That's right.  Use your imagination.  And under the picture is a link to the full size original image, which is really large.  So anybody who wants a high-resolution copy, you are welcome to it.



LEO:  Funny.  Resistance is fertile.



STEVE:  Oh, that's really good.



LEO:  Chickenhead21 gets credit for that one in the chatroom.



STEVE:  That's wonderful.  Wonderful.  And I have a quick note to share from a Bob McKinstry in St. Louis, Missouri.  He wrote:  "Hey, Steve.  I'm a listener of Security Now! since year one" - so 12 years - "and a SpinRite owner.  A few days ago, one of the residents in our training program asked how he could get help with his Dell laptop, as our IT department does not support machines they don't own.  The story is classic:  After a power outage, his laptop would not boot into Windows.  It would blue screen, reboot, rinse and repeat.



"Without hesitation, I told him about SpinRite.  I lent him my trusty SpinRite CD, told him he'd have to purchase SpinRite if it did fix his machine.  I told him he had to be patient and let the program do its work.  So he fired up SpinRite and went to bed.  When he woke in the morning, he was good to go.  Just like that.  SpinRite," he writes, "is a great product.  Another drive saved.  Another happy customer.  Thanks for all that you do with GRC, and thanks for all the great podcasts that you've done with Leo on the TWIT network."  And Bob, thanks for sharing your success.



LEO:  All right, Steve Gibson.  Time to talk - what is MMU?



STEVE:  That's the Memory Management Unit.



LEO:  Ah, okay.



STEVE:  Yes.  So we do have - I got a tweet while you were talking about ITProTV.



LEO:  Yes?



STEVE:  From Mike Roberts, who also had something clever to add.  He said:  "Who said resistors are passive?"



LEO:  All right, all right.



STEVE:  Because, of course, unlike a transistor, resistors and capacitors are considered passive components.



LEO:  Oh, man, that's geeky.



STEVE:  No, these resistors are - they're getting busy, as they say.  Okay.  So the press coverage of this.  I won't embarrass the specific magazines or the specific websites by calling them out.  But one's headline was "A Chip Flaw Strips Away a Key Hacking Safeguard for Millions of Devices."  Uh, no.  It has nothing to do with a chip flaw.  These guys have figured out how to very cleverly leverage a fundamental operational characteristic of all modern processor architectures to penetrate the randomization of all address space layout, ASLR.  Another one wrote "New ASLR-Busting JavaScript Is About to Make Drive-by Exploits Much Nastier."  Uh, no.  It doesn't make them nastier, it makes them significantly more possible and likely.



Someone else, one of the other major coverages was "JavaScript Breaks ASLR on 22 CPU Architectures."  Uh, no.  It breaks it on all contemporary CPU architectures.  In one of the papers they did, they applied some of their research to 22 different CPU architectures, but that's all the ones they had around.  So until something changes, this thing just busts it.  And, finally, "A Simple JavaScript Exploit Bypasses ASLR Protections on 22 CPU Architectures."  Well, okay, it says a "simple JavaScript exploit."  There's nothing whatsoever simple about this.



Now, I hope I didn't scare everyone away with my admonitions at the top of the show to not try to walk and chew gum while listening to this.  It's not that crazy.  But these guys are.  As I mentioned, this is the group at the University of Amsterdam who gave us, and we covered in Episode 576, Flip Feng Shui.  And our listeners since then will remember that they came up with just a "sublime," is the word I used at the time, a sublime hack, where they were able to leverage the fact that virtual machine environments consolidated identical pages of virtual memory in order to share pages that were the same.  They figured out a way to use the bit flipping of Rowhammer in order to get a shared page with sensitive information from an adjacent running VM.  And that was Flip Feng Shui that we described.



Then later, in Episode 583, they gave us Drammer, which was a new attack exploiting Rowhammer on Android devices, and essentially cut through all of the Android devices that had been looked at.  There were some newer ones had some mitigations against it.  But in general it was devastating at the time.  This one, there are some potential mitigations we'll talk about.  But again, this is just sublimely clever.  So in the show notes I've got links to the two research papers that back this up.  I don't think these links will be ephemeral as the one from last week was.  So they're linked on their site.  They have a history of keeping these things present into the future.  If they do go away, I can provide them, but I'd rather that people went there.  So their site is www.vusec.net/projects/anc.  That's the short name of this attack, ANC.



Okay.  So what they've got, and I'll explain exactly how it works, is a working means to allow arguably the sloppiest, least low-level code that we use, namely JavaScript in a browser, to determine the virtual address of important regions of memory in the operating system, which is exactly what address space layout randomization is designed to hide, to obscure, and to prevent.



Okay.  So there isn't a harder problem to solve than doing this in JavaScript.  They have native code that can do it.  And unfortunately, malware could take advantage of this if it wanted to, also.  So, I mean, this is a big deal.  So the press coverage was wrong in the details but right in the overall concept.  And the bad news is, as we'll see, what this leverages is a low-level fundamental fact of the way our processes are designed on purpose, which leaks information in an exploitable way about where things are in the operating system's address space.  And I don't know what's going to happen.  There are mitigations that can be applied to the browser to make this less feasible.  But there's nothing you can do at the low level.  There's nothing you can do for native code running on the system.  And I'll explain why.



So backing up a little bit, address space layout randomization.  One of the early mitigations for buffer overruns, which are such a problem, the idea with a buffer overrun was that an Internet-connected client or even a JPEG, anything that where an attacker had some way of providing data, whether as a fake image or as a packet to a server, if the hacker could provide data, that data would be accepted in some kind of buffer that you open the image, and so that JPEG file gets read into a buffer.  Or it's received over the Internet.  It goes into a communications buffer.  What the early attacks were, they leveraged some means of executing that data, executing the data in the buffer.



And so the first bright idea that occurred to people is, wait a minute, let's add a feature to the processors where segments of memory can be marked as "no execute," that is, you can put anything you want to in there, but you cannot set the processor's instruction pointer to an address in there.  There will be a flag on that page of memory saying execution is prohibited.  And so it just - it simply can't be executed.  So that was called DEP, Data Execution Prevention.  And that was good.  And Microsoft gently moved us into it because we wanted to make sure that applications were DEP compatible.  And so initially the OS used it, and then only parts of the OS, and then users could turn it on for specific applications.  And we sort of, you know, we gradually adopted data execution prevention.  And it was a good thing.



So now attackers are prevented from providing their own data to somehow jump to and execute.  But they're clever blokes, as we know.  So they said, hey, you know, we can't provide our own code, but there's already a lot of code in the system.  So what if we jump to the end of a subroutine that does a little bit of work and then comes back to us.  The way subroutines work, you normally jump in at the top of a subroutine.  And where you came from is put on the stack.  And when the subroutine exits, it pops the stack and returns to the instruction after the one that caused you to execute the subroutine.  So that's why it called a subroutine.  It's you're doing something, you say, go do this, and then come back to me when you're done.



Well, what they realized was they may not want all that the subroutine does.  But toward the very end, there might be something useful going on.  So nothing makes you only jump in at the top.  You can typically jump in anywhere.  And we've talked about mitigations to prevent that, to allow only jumping to the top of subroutines, but there's work going on about that, too.  But so that's the way Data Execution Prevention was worked around.  It's like, okay, if you won't let us provide our own code, we'll just go jumping around in the operating system, using code that's already there, to get our work done.



So then in this cat-and-mouse game the next flash of mitigation was, ah, the problem is the same code for a given OS is always loaded in the same place.  Like when the OS boots, it loaded itself in at the bottom, and everything else went on top.  But we realized it didn't have to go at the bottom.  There's nothing, because of the way memory management works, we could scatter the code around within the OS's virtual address space, and we could do it randomly so that every boot of the OS, every instance of booting all over the planet, would end up just scattering, scatter-shotting the various modules of the operating system into random addresses.  So now the hacker has a problem because there's code they may want to execute, but they don't where it is. 



Now, there have been some interesting mistakes made by older APIs, well, not even older APIs, but APIs that weren't written with this hide-their-own-address problem.  For example, there are APIs, like functions you could call in the operating system, where you say give me a buffer containing something.  And that function allocates in itself some buffer space and returns a pointer to the buffer.  And in doing so it gives up its own address.  So there were APIs, there were functions that were leaking, that were found to be leaking their own addresses.  And that was used for a while in order to, again, to get around the fact that ASLR randomizes where everything is.



Okay.  So that's where we were.  We had data execution prevention, and then we added address space layout randomization so that code that was essentially blind, it came in and is running in the OS, and there's that boundary between it and the operating system where all it can ask is the operating system to do things for it.  But it can't see where they're being done.  So that was good news.  Okay.  These guys figured out a way around that, and thus the problem.



Modern Memory Management Units, all modern architectures have it.  This is this whole virtual memory versus physical memory thing, the idea being that there is a mapping between the virtual memory addresses and physical addresses.  Meaning that the instructions and the programs and the jumping and accessing and reading and writing what looks like physical memory may be physical memory, but the address you're using is a sort of a pseudo address, a virtual address which the processor's hardware, the Memory Management Unit, maps into physical memory.  So, for example, 64-bit processors, contemporary 64-bit processors like the Intel x64 family, they have a 64-bit virtual address space.  Now, okay, because that is an insane memory space.



Okay, remember, 64 bits - 32 bits is the Internet; right?  So that's 4.3 billion.  So 64 bit is, we already know that's going to be 4.3 squared billion billion, which is something like 17 billion billion.  Well, okay, that's more memory than anything's going to have for a long time.  So Intel said, okay, we're not even going to bother with all 64 bits.  We'll use 48.  So on 64-bit Intel processors the virtual address space is 48 bits.  There is what's known as page-level granularity, meaning that memory is allocated in pages, which happen to be 4K for various reasons.  It's convenient.  That's the typical cluster size on a hard drive.  So when you need to load a page, you load a cluster.  So 4K sort of was a nice size.  4K is 12 bits.  That is, it takes 12 bits to address 4K.



So in a 48-bit address, the lower 12 bits is the offset in the page.  That is, 12 bits gives you 4K bytes, so it's the location, the byte location within the 4K page.  So that's 12 bits out of 48, leaving 36 higher order bits.  Those are broken into four sets of nine bits.  Nine bits, since eight bits gives us 256 combinations, nine bits gives us 512.  So each of those four sets of nine bits is considered a pointer into a system global page table.  So the highest order nine bits of the 48-bit address, that actually is a pointer to a 512-entry table which is the first page table.  And its location, its absolute location in physical memory is pointed to by a register in the processor called CR3, Control Register 3.



So the highest nine bits is the offset into that table, which contains a pointer to the second-level table, which the second most significant nine bits provides the offset into.  That entry points to the third-level table, where the third most significant nine bits provide the offset into that table; which points to a fourth-level table, which the fourth and least most significant nine bits of the upper 36 provides the offset to; which believe it or not finally points to the actual page in physical memory.



LEO:  Does some of this go back to the memory segmentation of the early x86?



STEVE:  Yeah.  It's like that on steroids to the power of infinity.



LEO:  Because I remember when I was doing assembly language on the Motorola 68000 it was such a joy because the memory space was flat, had big enough registers you could directly address anywhere in the memory.  You didn't have to do this segmentation thing.



STEVE:  Yes.  And as a programmer of 16-bit code, I was, I mean, a lot of SpinRite still is 16 bits because everything I've been writing for the last couple decades has been 32 bits.  So there's more and more 32-bit code coming into SpinRite.  But the original stuff had segments, and I was always having to, like, be loading the segment...  



LEO:  It's so annoying.



STEVE:  Oh, it is.  And so for me...



LEO:  So why do they still do it?  The registers are 64-bit now.  They could address all that memory easily, directly.



STEVE:  Ah, yes.  It's because you give the processes the illusion of 3GB, or however much memory you want to.  But many different processes are sharing it, and it doesn't actually exist.  That's what a page fault is.  A page fault is an attempt to map a page which is not currently in physical memory.  And so that's where swapping comes from.  So what happens is, if that page is marked as swapped out, then the process thinks it has the memory, and it's completely hidden from it.  It says, "Give me this byte at this address."



Well, so the processor looks up through this crazy four-level hierarchy of page tables and goes, whoops, we swapped that out last week, and no one's asked for it recently.  So it has a pointer to where it is.  It reads it into physical memory and then adjusts the page tables to point to that.  So the code has no idea any of that happened.  Oh, and the virtual memory manager, which is where that's going on, it's scrambling stuff up all over the place.  Stuff's being swapped out, brought back in.  Processors are being put to sleep and woken back up.  So this is all, I mean, it's incredible that this all works.  And it all works because it was developed carefully over time, and it's sort of a process of evolution, sort of like a human body.  It's amazing that it all works as well as it does.



So all of these levels of indirection, this four stages of hierarchy provides a huge amount of flexibility to the operating system.  But imagine the overhead.  I mean, we're talking every single instruction fetch, every single data write or read has to be mapped through this system.  Well, you couldn't get off the ground if there wasn't some way to speed that up.  So the first thing there is, is something called TLBs, Translation Lookaside Buffers.  The TLB, it's got a funky name, Translation Lookaside Buffer.  But that's like the cache for all of that work.



So the Translation Lookaside Buffers provide a shortcut that maps virtual memory to physical memory so that you don't have to follow this four-layer linked list, essentially, every single time you want to do anything with physical memory, whether it's execute an instruction or read or write data.  The problem is there's only so much TLB space available.  Processes are busy.  And the TLB starts out empty in the beginning when you boot up.  So it's only as a consequence of doing all of that fetching through all these crazy page tables that you end up maturing the TLB so that it's got a useful working set of mappings in order to convert virtual addresses to physical addresses.



Okay.  So if the TLB does not have the mapping, there is an LRU flush, a Least Recently Used replacement policy which - so if an entry hasn't been used for a while, it gets pushed out of the TLB by what's been going on more recently.  So that's standard caching logic.  So the processor looks through the TLB, hoping to find a match for the virtual memory range that it's having to do something with, anything with - execute an instruction, read or write data.  If it doesn't find it, now it's like, oh, crap.  I need to do the lookup.



So, now, these page tables exist in main memory.  And if there's anything we know about DRAM it's not only can you hammer it, but it is slow, it is excruciatingly slow compared to the contemporary speed of today's processors, which have just shot ahead, I mean, with dizzying speed.  What, 4.3 GHz, 4.3 billion cycles per second.  Meanwhile, DRAM's technology has kept it back.  So it's way faster than a hard drive, but it is agonizingly slow compared to the processor's appetite.



So what's been developed is Level 1, Level 2, and Level 3 caches.  The Level 1 cache is the closest to the execution and data units.  And it's actually split into Level 1 instruction cache and Level 1 data cache, meaning that instructions have their own Level 1 cache, and data reads and writes have their own.  That's to prevent either one from knocking the other one's data out of the cache.  Then the Level 2 cache is below the Level 1 cache and feeds the Level 1 cache.  And that's shared between instructions and data.  But both Level 1 and Level 2 caches are per core.  So if you have a quad core chip, each of the four cores has its own Level 1 data and instruction cache and Level 2 cache.



However, the entire chip of multiple cores shares a much larger, like on the scale of 8MB, Level 3 cache on a large chip; whereas, for example, the Level 2 are more like 32K.  They're big and very fast and accessible to the core.  But Level 3 cache, still on the chip, is more like 8MB.  And then that's - if it's not in the Level 1 cache or the Level 2 cache or the Level 3 cache, then the system has to go out and ask for it from DRAM, which in pulling it into the chip, it leaves it in the caches, where it can then be found from there on.



Okay.  So if a mistake was made, a fundamental architectural mistake, it's that it never occurred to anyone.  And I don't mean just Intel because AMD has the same problem, call it a vulnerability, as does the ARM architecture.  They all do this.  The Memory Management Unit with this crazy multiple hierarchy of page tables, in order to get the speed it needs, if the Translation Lookaside Buffer has had its entry knocked out, uses the caches.  It uses the same set of caches as the program.  And that's the problem.  What this means is that Memory Management Unit activity, which is by design hidden from the code, that's all down in the OS.  You could argue it's even below the OS.  It's in the hardware.



But the point is, programs do not need to know their physical addresses.  They see their virtual addresses.  And they should not see them.  It's hidden by design.  But the mistake that every processor makes is that this Memory Management Unit, for the sake of performance, is sharing the caches with executing processes.  And we know how an executing process can determine if something is cached or not.  If something is cached, and it's able to determine the length of time it took to get it, the cache is so much faster that it will get an almost instant answer.  In fact, it is instant, if the cache is close to the core.  If it's not in the cache, it forces a DRAM read which is measurably slower.  So this allows operating code to probe what's in the cache and is not.



And what these guys so cleverly figured out is that, if something as crude and coarse and granular and high-level and interpreted, well, JavaScript is not as interpreted as it always is because we're trying to speed it up so much, but still, given the limited resources of JavaScript running in the browser, these guys figured out how to essentially use the fact that memory management was sharing the data cache of the code that was running in the browser to probe and reverse-engineer the contents, the recent history of the Memory Management Unit's use; and, for example, to turn in the virtual address of the heap for the process, which is memory which should otherwise have no known value.



So we talked about timing and the vulnerability of our systems to timing attacks in the past.  And as a consequence, browsers - well, for example, there were several attacks where the time required to decompress data was used by adding data to headers and making lots of queries, JavaScript was able, by accurately measuring the time it took to perform this, was able to reverse-engineer the contents of the unknown data because the compression would compress the added data, if it was the same as the unknown data, and that would change the length of the compressed result.  And so very clever scripting was able, with enough queries, to figure out the unknown data compared to what was known.



Browsers years ago in response to those attacks which we covered on the podcast, they decided, okay, we're not going to give JavaScript super granular timing information.  The browsers, both Firefox and Chrome, reduced the resolution of the API, which captures the current instant in time from infinity.  It used to be the resolution of the clock.  So it was nanoseconds.  They reduced it to five microseconds.  So they deliberately removed resolution so that JavaScript could not determine as accurately as it originally could what time it was.  So once again, not to be daunted, our clever engineers said, okay, fine.  We don't have accurate timing.  But is the five microseconds itself accurate?



And it turns out that Chrome and Firefox differ a little bit.  Chrome is a little trickier.  They fudge the actual timing event.  It still has five microseconds' resolution.  But they delay it a couple different amounts in order to, like, further obscure the actual moment of that five microseconds event.  Firefox didn't do that.  So, for example, in Firefox the way you solve this problem is you query the API until the timer ticks.  So that gives you tick alignment.  Then you perform the operation that you want to time, in this case a read or a write of a certain location because you're wanting to probe the cache.  And as soon as you get control back from that operation, you start doing something, you start counting how many times you can go in a loop until the next timer tick.



And so what happens is essentially the five microsecond resolution puts fenceposts.  And you align yourself with one fencepost at five microseconds, do something, and the moment you get back, you spin until the next fencepost, until the next value, the next five microsecond tick occurs.  And so the number of spins that you are able to accomplish, that allows you to infer, by comparing cached and uncached counts, spin counts, essentially, whether the unknown timing was short or long.  So that's one of the clever things these guys did.  There's another approach using WebWorkers, which essentially extends the JavaScript API to a multithreaded environment, where you're able to say, you go off and do this.  Oh, and you also - the workers, in order to coordinate, they have a region of shared memory, memory that they can both see.



So the WebWorker, you give it the task of simply spinning up a counter in the shared memory as fast as it can.  Now you have your own timer, which you're able to use, which is, again, much higher resolution than the deliberately blunted resolution of the JavaScript timer in order to determine how long something took to occur.  So these guys worked around the deliberate timing blunting that the browsers added to JavaScript.  They realized that, because all the current processor architectures share a common cache, which is available to operating code, with the Memory Management Unit - which they have to cache, otherwise the whole system would just come to a grinding halt.  If the page tables only were non-cached, nothing would ever get off the ground.



So but the problem is they could have been given their own cache.  Instead, they shared that caching, which made good sense in terms of resource utilization for the processor, but it was a critical architectural mistake in this context.  If you absolutely do not want programs to know where stuff is, you can't let the programs have any visibility into the memory management.  And as a consequence of the fact that page tables are stored in main memory and are cached in a common cache which can be probed by timing-sensitive code, unfortunately we no longer have the protection that ASLR was designed to provide.  So bravo to these guys.  And I don't know what this means.  I mean, the browsers could further work to fudge things.  But as I said, there's nothing you can do to prevent this at the native code level.



So, I mean, and that's really where the big problem is with malware that people download by mistake and are relying on system-level protections to keep that code from being able to elevate its privilege to root or kernel level.  This presents bypass opportunities.  This is all in the public domain.  It's open source.  There's a bunch of utilities these guys created, both JavaScript and native code that demonstrates it.  One of the papers is this 22-architecture demo that they provide, where essentially they did a generic memory mapping unit reverse-engineering code because these inner levels of detail don't need to be known by the software.  So they're undocumented across these various chip families.  The high level, how you talk to the Memory Management Unit, everybody knows that.  But the inner details are - it's not that they're such a big secret.  But it's like, well, it's just the hardware architecture.  Nobody really needs to know how it's all wired together.  To pull this attack off, you have to know.



So these guys have a tool that allows them to reverse engineer, under automation, to reverse engineer and determine the exact sizing of every level of caching in the hierarchy and the nature of the cache replacement policies within the chips.  Just a phenomenal, beautiful piece of engineering, and something that will probably drive some changes in the architectures of our chips moving forward.  The bad news is there's nothing we can do about the entire install base of chips, of chip hardware that we have now.  We're in for an interesting ride.



LEO:  Not that, once you get the memory table, the rest is cake.



STEVE:  Correct.



LEO:  I mean, you have to inject code somewhere in that memory table.  You have to figure out where the next jump is, that kind of thing.



STEVE:  Correct, yes.  And that's why I was critical, for example, in Ars Technica, who said new ASLR-busting JavaScript is about to make drive-by exploits much nastier.  No, no, it's not.  It makes them significantly more possible, and perhaps more likely, because this has the potential for defeating address space layout randomization, which we really are depending upon.



LEO:  Well, I mean, there's been other issues with ASLR.  It often breaks things.  Does everything use it now?



STEVE:  Yeah, it's becoming very widespread.  The most...



LEO:  Because back in Windows 7 it was optional.



STEVE:  Yeah, they attacked Linux because Linux's ASLR is 28 bits of entropy, whereas Windows is only, I want to say - 28 bits in Linux.  I think it's 24 bits in Windows.  And so they tackled the harder of the two platforms.



LEO:  Good for them.  Good for them.



STEVE:  And they made this work both in Firefox and Chrome on Linux. 



LEO:  Wow.



STEVE:  Yeah, beautiful piece of work.



LEO:  Yeah.  It's quite elegant, that little timing thing.



STEVE:  Yup.



LEO:  Very clever.  Well, there you have it.  Don't know what the upshot is.



STEVE:  Okay.  Everybody can start chewing gum again.



LEO:  Take your propeller hats off.  Start chewing and walking.  It wasn't as bad as you said.  Or maybe just because I've had to already wrap my head around address segmentation for so long, which is a terrible thing.



STEVE:  Well, and remember, these things, as we know, exploits only get worse.  They never get better.  And so this is a major wedge in a technology that, to a large degree, a mitigation we've been depending upon.



LEO:  Right, right.



STEVE:  Beautiful piece of work.



LEO:  And to be fair, we didn't have ASLR till recently.  Right?



STEVE:  Correct, yes.



LEO:  So it's not like - it's going to be back to the old days.  It's not like it's, oh, everything's, you know.  This is a relatively new technique to make you more secure.  And there are other things protecting you, I would hope.



Steve Gibson is at GRC.com.  And someday we're going to answer your questions, maybe even next week, so get them out and put them there at GRC.com/feedback, or tweet him at @SGgrc.  When you get to GRC.com, you'll note many fine and fabulous things.  Don't get too distracted by those until you buy a copy of SpinRite, the world's finest hard drive maintenance and recovery utility.  It's right there.  Then after that you can try ShieldsUP! and Perfect Paper Passwords, the Password Generator, read about SQRL and the Healthy Sleep Formula.  There's so much stuff there:  GRC.com.



Also the show, of course, 64Kb audio, human-written transcripts by Elaine Farris, all at that site.  We have audio and video at our site, TWiT.tv/sn.  We also have a survey we'd love you to take.  Just a week left in the survey, so if you can get to it before the end of the month, take a couple of minutes of your time, TWiT.tv/survey.  It lets us know a little bit more about you in aggregate.  We don't take any, you know, we don't want your personal information.  We're not going to try to figure out who you are.  We just want to know what percentage is men, what percentage is women, what percentage have college educations, what's the income.  And some of that's for us for programming purposes, but I think we know you pretty well for that.  Mostly it's for us to tell advertisers because they always want to know that stuff.



STEVE:  Right, right.



LEO:  "Well, who's listening?"  I don't know.  They're good people.  Everybody, all the other advertisers like them.  They want that stuff.  And so it helps us a lot to monetize the network.  And if we don't do that, well, we got no network.  So TWiT.tv/survey.  If you would, it really helps out.



We have caps now, by the way.  We have TWiT logo caps and TWiT.tv caps in the TWiT Store, TWiT.tv/store.  We don't make a lot of money on it.  It's not really a profit thing.  It's more because, well, people have asked us.  "We want T-shirts.  We want mugs.  We want hats."  And now we've got really nice hats, actually.  I'll wear one next time.  That's at the TWiT Store, TWiT.tv/store.



Steve, let's do this again, maybe, I don't know, I'm thinking next Tuesday, about 1:30 Pacific, 4:30 Eastern?



STEVE:  How about for Episode No. 601?



LEO:  601.



STEVE:  Yeah.



LEO:  It was a good 600.  Kind of a nice show, as always.  Congratulations on 600.  Well done.  Twelve years.



STEVE:  Yes, in our 12th year.  Okay, my friend.  I will talk to you next week.



LEO:  Thanks, Steve.



STEVE:  Thanks.  



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#601

DATE:		February 28, 2017

TITLE:		The First SHA-1 Collision

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-601.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week, Leo and I discuss the "Cloudbleed" adventure; another Project Zero 90-day timer expiring for Microsoft; this week's IoT headshaker; a New York airport exposing critical server data for a year; another danger created by inline third-party TLS-intercepting middleboxes; more judicial thrashing over fingerprint warrants; Amazon saying no to Echo data warrant; a fun and widely misunderstood drone-enabled proof of concept; another example of A/V attack surface expansion; some additional crypto education pointers and miscellany; and, finally, what does Google's deliberate creation of two SHA-1-colliding files actually mean?



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  What a week in security this has been.  He's going to explain what the SHA-1 collision that Google announced means and whether it's time to set your hair on fire.  And speaking of hair on fire, what about Cloudbleed, the huge security incident at Cloudflare?  Steve explains all and a lot more, coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 601, recorded Tuesday, February 28th, 2017:  The First SHA-1 Collision.



It's time for Security Now!, the show where we cover the latest in security and privacy.  And there's no better person to do that than Mr. Steve Gibson here from the GRC Corporation.



STEVE GIBSON:  As you turn around and look at me on the screen.



LEO:  Just to make sure you're there.



STEVE:  I'm there.  I'm here.



LEO:  Twelve years we've done this show, 601 episodes, and something happened at 600 because for the first time ever we didn't have an Internet connection.  We actually are starting about an hour and a half late because of that.



STEVE:  Yeah, I was watching the traffic this morning because I have all kinds of monitors because I've been watching my connection to GRC, and I'm able to look at incoming and outgoing bandwidth and all this stuff.  And I started seeing some glitching in the early morning.  And this has been a problem in the past.  And sort of the characteristic I've noticed is that it gets really bad, and then it seems to get fixed.  Like it got bad enough for somebody at Cox to get enough complaints that they thought, okay, I guess there actually is a problem, and so they go and shake some wire somewhere and fix it.



And I've always been nervous because the only thing that really is, like, real-time, can't be put off, I mean, it's a beautiful day at the beach down here in Southern California.  I could have been there.  But it's the podcast.  And so I've been nervous in the past that exactly this might happen, that there might be one of these cable problems at 1:30 Pacific on a Tuesday.  And it finally did happen.



LEO:  But you haven't had problems at other times, have you?  Or have you?	



STEVE:  Oh, yeah.  This is consumer-grade Internet.  And I also watch mine much more carefully than most people do.  I mean, I notice if packets are being dropped.  Whereas people are like, oh, why did Netflix just freeze?  Oh, maybe backspace and try it again.  So, you know, people put up with this, but because I have a lot of instrumentation I see what's going on.  And so this morning just happened to collide with the podcast.  Normally I just don't worry about it.  I just do some other work that's offline.



LEO:  We've got a lot to talk about.



STEVE:  Oh, my.



LEO:  I'm actually really glad we're doing the show. 



STEVE:  Good, yeah.



LEO:  There's quite a bit in the news.



STEVE:  So this is Episode 601.  And so we've got basically anchors at each end.  In my little summary of this week on Security Now!, I started off saying, "If it leads, it Cloudbleeds."  So we have another Project Zero - aside from the Cloudbleed problem.  And it's so funny, too, because Tavis deliberately said, "I'm going to do everything I can not to call this problem 'Cloudbleed,'" because of the obvious connection to Heartbleed was there.  So we have that we'll talk about.



We're going to end up with the title of the podcast, which is "The First SHA-1 Collision."  And I want to, again, provide somewhat better context.  You did a great job on Saturday with The New Screen Savers, Leo, sort of bringing down the temperature to what is more reasonable, I think.  But we've also got another Project Zero 90-day timer has expired on Microsoft, revealing another problem with IE and Edge that we thought was going to get fixed, and Microsoft probably hoped so, too.  We've got this week's IoT headshaker.  A New York airport was found to have exposed critical data from its servers for almost a year, starting last April.  Another danger created by inline third-party TLS intercepting middleboxes.  More judicial thrashing over fingerprint warrants.



Amazon says no to an Echo warrant.  A fun drone-enabled proof-of-concept which was widely misunderstood.  And I think I also heard you talking about that and also getting it right.  And another example of antivirus attack surface expansion.  We've got some additional crypto education pointers which actually came to me from our listeners, saying oh, and this and this and this.  And we'll then wrap up by talking about what Google's creation of a deliberate SHA-1 collision actually means.



LEO:  Oh, good, good, good.



STEVE:  So I think another great podcast.



LEO:  Good.  And I'm very interested in hearing your explanation on Cloudbleed.  I think you're less sanguine than I am.  And apparently Tavis Ormandy just tweeted about 30 minutes ago, the Google engineer who discovered it and, I think, well, I'd love to hear what you think, inappropriately in my opinion tweeted it before he contacted Cloudflare.



STEVE:  I saw his tweet last week, and I thought, uh-oh.



LEO:  Yeah, that's like, hmm.



STEVE:  That can't be good.



LEO:  But maybe he thought it was serious enough that everybody should know before anybody knew.  But he's apparently very dismissive of Cloudflare's explanation.  He says they're basically being deceptive.  So I'm very curious what you think about all of this.



STEVE:  And there's a meta view to this, too, that we'll talk about.  Because remember I once, about a year ago, actually, said under no circumstances would I give GRC's private keys to a third party.



LEO:  That's right, that's right.



STEVE:  And this is what everybody else has done.  And as a consequence, plaintext from privileged communications was going to other people, is what that meant.



LEO:  Right, right.  Which is serious on the face of it.



STEVE:  Oh, yes.  That's really bad.



LEO:  Can't get much worse, yeah.  All right, Steverino.  Fire away.



STEVE:  So the tick-tock on this, what we're calling - okay.  So the official name...



LEO:  Let's call it Cloudbleed.  We might as well; right?



STEVE:  And so this is our friend, Tavis Ormandy, who did come in on the weekend, last weekend, because this was enough for him to have to cancel Sunday plans.  And he was at Google, trying to figure out what was going on.



LEO:  Well, they needed to clear the caches because they had cached this information; right?  They had to get...



STEVE:  Yeah.  Okay, so the official title was "Cloudflare reverse proxies are dumping uninitialized memory."  Now, technically that's correct.  What it really means is they're dumping whatever happens to be in the memory that is dumped.  And if all of the users in the world are using the same proxy - and that's not the case.  There are many different proxies, and people are jumping around between them.  But the point is that nothing scrubs the data buffers when someone stops using it.  The buffer is just released back to the system, and the presumption is that no one will assume there's anything valuable in that memory.  So, okay.



LEO:  Before you get too close to this, I should mention that we have been negotiating with Cloudflare for sponsorship, and I do believe they're going to start advertising on TWiT in the next month or so.  So that's a disclaimer.  We always have to do that.  It's a potential advertiser.



STEVE:  Yes.  And for what it's worth, in my show notes, which I haven't yet been able to publish because my workflow is Google Docs, and I couldn't get to Google Docs, I said somewhere here, I'm looking for it - oh, it's way down.  I said, if you do need to use a third-party TLS proxying service, don't use some random service no one has ever heard of.  Use Cloudflare.  The takeaway is they absolutely responded as best they possibly could.  And this is like where we've seen a problem with LastPass, yet Joe was on it immediately, fixed it in minutes, and then pushed the fix out.  So, I mean, and these guys responded, the Cloudflare people responded as quickly.  So none of this should be read as being criticism of Cloudflare.  And in fact my feeling is what you want is a track record of robust response to the inevitable problems which will arise, which is what they demonstrated.



So, okay.  So turn the clock back to February 19th, where Tavis, as part of Project Zero, he was working on something unrelated.  He called it a "corpus distillation" project.  And so he writes:  "On February 17th I was working on a corpus distillation project when I encountered some data that didn't match what I had been expecting.  It's not unusual to find garbage, corrupt data, mislabeled data, or just crazy nonconforming data; but the format of the data this time was confusing enough that I spent some time trying to debug what had gone wrong, wondering if it was a bug in my code.



"In fact, the data was bizarre enough that some colleagues around the Project Zero office even got intrigued.  It became clear after a while we were looking at chunks of uninitialized memory interspersed with valid data.  The program that this uninitialized data was coming from just happened to have the data I wanted in memory at the time.  That solved the mystery, but some of the nearby memory had strings and objects that really seemed like they could be from a reverse proxy operated by Cloudflare, a major CDN service.  A while later, we figured out how to reproduce the problem.  It looked like if an HTML page hosted behind Cloudflare had a specific combination of unbalanced HTML tags, the Cloudflare proxy would intersperse pages of uninitialized memory into the output."



And again, let me make clear what that means.  That means memory that doesn't belong to this conversation, but was almost certainly part of some previous conversation.  And so one of the takeaways for our listeners is that this is a fundamental responsibility and a source of probably great angst for Cloudflare because they are, to do the job, to offer the services they're offering, they're terminating their customers sites' TLS connections.  That is, we've been talking in the last few weeks about these middleboxes.



Well, essentially, Cloudflare is a big CDN middlebox.  And our listeners will probably remember a year ago when GRC was suffering those DDoS attacks, and people were saying, oh, you know, you should go behind Cloudflare or some other service like that.  The problem is that the only way to do that is to give essentially that service which is going to front for your servers your private keys.  And there's no way GRC.com is giving anyone its private keys.  I get it that everybody else does, and that's fine.



But what this means is that Cloudflare had plaintext content of all of the HTTPS conversations that they had been proxying on behalf of their customers, the customers behind Cloudflare.  And so this decrypted, very often sensitive data, which we're all now encrypting over TLS and HTTPS on purpose, in order for Cloudflare to offer the services that they are offering, they have to have visibility into it.  They have to be a decrypting middlebox, essentially.  And that means they have plaintext of all of the traffic going to all of the sites behind them.



LEO:  And they have a higher responsibility as a result.



STEVE:  Correct.



LEO:  I think your point is well taken in that regard.



STEVE:  Correct.  So what Tavis saw was other people's data in  their data.  That is, data that, like extraneous data, and like passwords and session cookies.  And OkCupid is one of the Cloudflare people, and there was OkCupid data.  And so, I mean, potentially the kind of the stuff we deliberately protect with HTTPS connections was leaking across different users' sessions.  So he writes:  "A while later, we figured out how to reproduce the problem.  It looked like if an HTML page hosted behind Cloudflare had a specific combination of unbalanced HTML tags, the Cloudflare proxy would intersperse pages of uninitialized memory into the output," and he says, "(kinda like Heartbleed, but Cloudflare-specific, and worse for reasons I'll explain later)."



He writes:  "My working theory was that this was related to their ScrapeShield feature which parses and obfuscates HTML."  And I heard on The New Screen Savers you talking with Nick.  And, for example, they are doing something automatically that GRC does.  I have, for example, sales and support email links on my site.  But the GRC server, whenever they change, it replaces them with images so that users can see the image, but that bots can't easily scrape the email off the site.  And so, for example, that's one of the things that Cloudflare is doing on behalf of the customers behind it.



LEO:  That's their email obfuscation feature; right?



STEVE:  Exactly.  And you can just say, yeah, I would like that.  But if you say, yeah, I would like that, that means they have to see the page their customers are sending out.  Because, for example, bots are roaming those sites and trying to harvest, for spam purposes, email addresses.  Well, instead, they transparently swap that email address with an image of it which is not ASCII text anymore, and so the bot can't see it.  But of course this is how Google comes in because Google is spidering all of these sites.



And so the nature of this vulnerability is that anyone who happened to pull a page that had broken HTML in this particular breakage fashion would receive a bunch more than they asked for.  And so, as a spider, Google's job is to go and ask for every page there is.  And so as a web cache, they became an unwitting repository of this leaked plaintext of other people's plaintext communication data.  They didn't want it.  And it's sort of reminiscent of the problems Google got into when they were doing the street view and sucking in all of the WiFi.  They didn't want the unencrypted WiFi, but it was there.  So they ended up with it on a hard drive somewhere.



LEO:  Any web host would also have that situation.  I mean, I have SSL with my webhost, but the data that I'm storing there is in the clear on their servers.  And if they did it, you know, so they have access to all my data anyway; right?



STEVE:  Yes.  That's a great point.  The problem here is that, to offer the services, essentially Cloudflare is a concentrator.  That is, all these customers go through it to get to all of these sites behind it.



LEO:  But you could, I'm just saying, you could say the same thing about, let's say, WordPress.  It's a different service that WordPress is providing.  But any site that's hosted by WordPress.com or Squarespace or anybody has that same thing going on; right?



STEVE:  Correct.



LEO:  They see everything in the clear.



STEVE:  Correct.



LEO:  Even if it's SSL.



STEVE:  Exactly, because they're on their side of the encryption.



LEO:  Right.  So when WordPress says, for instance - another sponsor, I should mention - we host 27% of the web, it's a big concentrator, too.  It's just a different kind of service than Cloudflare is providing.



STEVE:  Correct.



LEO:  Right.



STEVE:  Correct.



LEO:  You host your own content, so you don't come up against this.



STEVE:  Correct.



LEO:  You know what's in your content.



STEVE:  So, yeah.  Tavis writes:  "It became clear after a while we were looking at chunks of uninitialized memory interspersed with valid data.  The program that this uninitialized memory was coming from just happened to have the data I wanted in memory at the time."  So he says:  "That solved the mystery."  He says:  "We fetched a few live samples; and we observed encryption keys, cookies, passwords, chunks of POST data, and even HTTPS requests for other major Cloudflare-hosted sites from other users.  Once we understood what we were seeing, and the implications, we immediately stopped and contacted Cloudflare security."  And, yes, somewhat controversially, by tweeting, "Would someone from Cloudflare please contact me."



So he says:  "This situation was unusual.  Personally Identifiable Information (PII) was actively being downloaded by crawlers and users during normal usage.  They didn't understand what they were seeing.  Seconds mattered here.  Emails to support on a Friday evening were not going to cut it.  I don't have any Cloudflare contacts," he said, "so reached out for an urgent contact on Twitter and quickly reached the right people."  So bottom line is this immediately came to Cloudflare's attention.  Nicholas got busy.  John Graham-Cumming got involved.  And, I mean, this obviously got everyone's attention.



And they have some configuration, what they call "kill bits," that allowed them to, within in some cases minutes, and in some cases hours, but again, only a few, to shut down these problematic services once they understood the nature of what was going on.  Then the problem was that Yahoo and Bing and Google, I mean, any web crawler that had crawled over the last few weeks - because this was introduced relatively recently, earlier in February - it got sort of worse.  And I'll explain the nature of this, and you talked about it on Saturday on The New Screen Savers, is any of these - I've completely lost my train of thought.



LEO:  The search engines that cache the content.



STEVE:  Yes.



LEO:  They have stored those contents in their search indexes.



STEVE:  Exactly.  As the pages they thought they were retrieving actually had a bonus that they didn't want.  So Cloudflare then worked to spread the word quietly to get them all to clean up their caches as much as they possibly could before this got any more public coverage.



So, okay.  So Tavis went back and forth with the Cloudflare people.  He was a little annoyed because he felt this was really an important problem.  They were scrambling, Cloudflare was scrambling as quickly as they could to pull together full disclosure and a complete write-up.  And in fact it was on last Wednesday or Thursday that went public with what everyone agrees is a complete, fully responsible, beautiful disclosure, where they explained exactly how it was that this happened.



And I got a kick out of you commenting, Leo, about this loop termination condition because essentially what happened is they were using a toolkit.  And even though the toolkit could have been written better, Cloudflare took responsibility for misusing it because, in typical C power, there's a pointer which is auto-incrementing and being checked against the end of the buffer value with an equality.  And so this pointer is moving through the buffer.  And unfortunately, the Cloudflare code advanced that pointer itself, rather than always letting the library that they were using advance the pointer.  And since the Cloudflare code advanced the pointer, then the library advanced it.  And so it did a double increment and skipped past the equality, skipped over that test for the end of buffer.  And your comment was, and you were completely correct, rather than testing for equals, test for greater than or equals.



LEO:  Yeah, I mean, really it's basic, I mean, Programming 101.



STEVE:  Yes.  And it's funny because I'm sort of superstitious that way.  All of my code does that.  I use inequalities to test for end of loops, even though it should never happen.



LEO:  It should always be automatic.  There's no reason not to.



STEVE:  Right.



LEO:  And I think this is as much, could as easily be a typo because then, I mean, of course any programmer does that; right?  You don't consider, well, will this ever happen?  You just check for the range.  Now, was this a library they used?  Or was this code that came from Cloudflare?  I'm not clear on that.



STEVE:  It was a library.  And I can't remember the name of it.



LEO:  They didn't write it.



STEVE:  No, they did not write it.  And it is, it's a beautiful library.  I was unfamiliar with it before.  It's a system that compiles regular expressions into C, C++, or assembly.  So it's very cool.  It's exactly what you want for high-speed pattern matching.



LEO:  It's in something called Ragel, R-A-G-E-L.



STEVE:  That's it, yes.  And so, yeah, the Ragel authors, technically they didn't make a mistake.  But their code could have been more robust by using a "greater than or equal to" loop termination, rather than just "equals."  But again, to their credit, Cloudflare says, "We misused the library."  And they recognized how, if you're incrementing something, testing for equality should be enough because you're going to get there eventually.  But that does assume that nothing would ever cause you to skip over the end, in which case you would just keep on going.



LEO:  So the Ragel code wasn't written improperly, but it probably should have been a better range check.  But the reason it was triggered is because the code that Cloudflare wrote for Ragel, that Ragel then parsed...



STEVE:  Invoked, correct.



LEO:  ...invoked, contained a bug that caused the pointer to jump over the end of the buffer, and then the lack of the proper check didn't stop it.



STEVE:  Exactly.



LEO:  Okay.



STEVE:  Yes.  And again, Cloudflare wasn't blaming anyone other than themselves.  They said, "This is on us."  And again, I have no reduced confidence in Cloudflare as a result of this.  I mean, people who want the service they're providing have to make this tradeoff that a third party will have access to their communications.  I mean, that's what you have to do if you want this service.  But, for example, if today I were to choose such a service - and I wrote this in the notes even before you said they might be a future sponsor, so I don't want anyone to confuse this.  I would rather go with someone who has a proven track record of this kind of responsible management of problems because we know, if we know anything from this podcast, it's that code is incredibly difficult.



LEO:  Stuff happens.



STEVE:  Stuff happens.  It's incredibly difficult.



LEO:  And it's how you respond to this stuff.



STEVE:  Yes.



LEO:  But as you also say, and to be fair on the other side, when you're doing what Cloudflare is doing, you have a higher responsibility because there is such a risk involved.



STEVE:  And my point is that they're not alone.  Remember that all of these middleboxes are doing it, and there are other similar services that are doing it.  And if I were to choose a service, I'd go for a service that had a lot to lose.  I mean, and you asked Nick, "Have you slept recently?"  And he said, "Uh, no."



LEO:  Nobody had.  The reason we didn't - we wanted to talk to John because he's been on the show, and we know John Graham-Cumming very well.  But it was midnight U.K. time, and he was, I think, catching his first sleep in close to a week.  He's the CTO over there.



STEVE:  Yeah, that's all you could ever ask.



LEO:  Yeah.  But this is, nonetheless, serious.



STEVE:  Yes.  So we have some new pages are on the Internet now.  Cloudbleed already has a Wikipedia page to formalize its name and existence.  And then we have the site doesitusecloudflare.com, which is a site that allows you to determine if some site is behind Cloudflare.  And one of the things that I...



LEO:  But is that - now, this was a question I had.  Is that fair?  Because not every site would be impacted by this; right?  Only sites that were using some of Cloudflare's service, some particular subset of their services.



STEVE:  No, that's the problem, is that...



LEO:  Okay.  I got that wrong, then, because that was...



STEVE:  The flaw is triggered by a site offering technically broken HTML.  But the trigger would dump the contents of whatever was in RAM, even if it was a good site that had flawless HTML.  So essentially...



LEO:  But, I mean, not all Cloudflare subscribers with malformed HTML would necessarily get bit by this.  You'd have to be using their email obfuscation service, one of the services.  Or would every single Cloudflare customer potentially be at risk?



STEVE:  Every single Cloudflare customer because, even though they made no mistake...



LEO:  The dump could have been of anybody and their database.



STEVE:  Yes.



LEO:  Got it.



STEVE:  Anybody, any of their clients could have had their data [crosstalk].



LEO:  I get it.  So you don't have to be at a site that's using one of the services or with malformed HTML, but some other site could trigger a dump that might include your data.



STEVE:  Just because you happen to have been left in RAM.



LEO:  Got it.



STEVE:  The contents of a previous dialogue with one of your visitors was still there, sitting in RAM.



LEO:  Got it.  Right.



STEVE:  Yup.  That's it exactly.  So anyway...



LEO:  Those lists of Cloudflare customers, those are legit.  Now, what Nick told us, and he said John felt the same way, is they're not changing passwords because, as far as they could tell, little was leaked.  But on the other side, I'm looking at Tavis Ormandy's tweets, and he's pretty angry at Cloudflare.  He said:  "Cloudflare is having a busy day misleading and misdirecting."  He tweeted that today, an hour ago.  He also retweeted Pinboard's author, whose name I'm not going to mangle - well, I will, Maciej Ceglowski, sorry, Maciej - who says:  "Cloudflare has learned one lesson from Trump:  Tell lies about the thing you want to distract from until the story becomes about your lies."  So that's what I'm curious about.  What is Cloudflare doing that's so upsetting Tavis and Maciej?



STEVE:  Yeah, without knowing what it was.



LEO:  Yeah.



STEVE:  For example, so on February 23rd, John Graham-Cumming said:  "Last Friday, Tavis Ormandy of Google's Project Zero contacted Cloudflare to report a security problem with our Edge servers.  He was seeing corrupted web pages being returned by some HTTP requests run through Cloudflare.  It turned out that in some unusual circumstances, which I'll detail below, our Edge servers were running past the end of a buffer and returning memory that contained private information such as HTTP cookies, authentication tokens, HTTP POST bodies, and other sensitive data.  And some of that data had been cached by search engines.  For the avoidance of doubt, Cloudflare customer SSL private keys were not leaked."  So he wants to separate that.



He says:  "We quickly identified the problem and turned off three minor Cloudflare features - email obfuscation, server-side excludes, and automatic HTTPS rewrites - that were all using the same HTML parser chain that was causing the leakage.  At that point it was no longer possible for memory to be returned in an HTTP response.  Because of the seriousness of such a bug, a cross-functional team of software engineering, infosec, and operations formed in San Francisco and London to fully understand the underlying cause, to understand the effect of the memory leakage, and to work with Google and other search engines to remove any cached HTTP responses."



And I'm skipping a bit, and then he says:  "The bug was serious because the leaked memory could contain private information, and because it had been cached by search engines.  We have also not discovered any evidence of malicious exploits of the bug or other reports of its existence."  So, okay.  So I don't know what may have been said more recently that Tavis and others are specifically responding to.  As we know, I've had my own Internet outage this morning.  But John was completely forthcoming about this.



Now, I guess maybe the only thing I could say is that they're trying to put the best face on it as they can.  He says:  "The greatest period of impact was from February 13th and February 18th, with around one in every 3.3 million HTTP requests through Cloudflare potentially resulting in memory leakage.  That's about" - and then he does the math for us.



LEO:  Lots of zeroes.



STEVE:  Yes, 0.00003%.  So, you know...



LEO:  That's the real problem, in a way, with this, though, is we don't know what was leaked or what sites were affected, and we can't know; right?  I mean, we...



STEVE:  Correct.



LEO:  I guess Tavis and people who have access to the cached data would have some knowledge.



STEVE:  The takeaway - and I'm doing the same thing as Nick and John.  I haven't changed any passwords.  The only thing - and again, so I would - the way to characterize this for our listeners, I think, is an incredibly, I mean, you can't even - I would have more zeroes in front of, you know, like after the decimal even than John, probability of any of us actually being hurt in any way from this.



LEO:  Well, that's the other thing, is what is in RAM is completely random, as it was with Heartbleed.  So you don't know what's there.  It could be half a password, could be no password.



STEVE:  Right.  It would be less than lightning strike probability.  And so if you want to do anything, I would say for the next month increase your level of vigilance.



LEO:  And turn on two-factor, which would mitigate this, as well; right?



STEVE:  Correct.  Because even if a password or a session cookie, I mean, for example, if it was a session cookie, someone could use that to immediately become logged on as you.  But again, it's like we presume - and Tavis wrote, "We don't know that nobody else knew about this."  Again, you can't - there's no way to prove a negative.  So we don't know what we don't know.  But it did take somebody fetching a page from one of the broken sites that interacted with this parser.  And once again, here's another interpreter which has bitten us - interpreters are hard - that would have leaked contents that opportunistically happened to be in the cache from somebody else using Cloudflare beforehand.



So again, I don't know what it is that these guys are upset about.  From my perspective, Cloudflare has been very forthcoming and immediately mitigated the problem, found it, fixed it.  And the fact that you've got archiving technology crawling the web all the time, again, huge boon for us.  Can you imagine if we just put anything that came into our head into a search engine and immediately find the pages that are relevant.



LEO:  Right.



STEVE:  That was the world 15 years ago.  So but with that comes - so even the caching spiders have a responsibility to, when notified, work to flush their caches of data that they wished they hadn't acquired, perhaps by mistake.  So again, hats off to Google.  Hats off to Cloudflare.  This is an example of - what would it be?  Low impact, a very serious, very low-impact flaw.



LEO:  Well, and also, for it to be exploited, somebody would have had to know about it before it went public, before the caches were cleared, and had been paying attention; right?



STEVE:  Yes.



LEO:  Because at this point there's no way you can exploit it, hoping the caches have been successfully cleared.



STEVE:  Yeah.  So say that somebody had a time machine.



LEO:  You could go back in time and do it.



STEVE:  Yes.  So that they were able to go back in time.  What they would do is sit there making, just pounding these broken services, these pages that have the broken HTML.  And they would be sucking in - they would be deliberately sucking in what Google and other spiders were inadvertently sucking in, and then looking to see if they got any treasure.  Again, you can't target anybody.  It's just whatever happens to be left behind from previous use of that same hardware by any other random person in the entire world going to any other random Cloudflare site in the entire world.  And it was funny, Tavis did say, in his note he said, "I had no idea so much of the Internet was behind Cloudflare."



LEO:  Yeah.



STEVE:  It was a wakeup call.



LEO:  Yeah, they're huge, yeah.



STEVE:  For Google it's like, wow.



LEO:  And also I apologize if Tavis used backchannels to contact Cloudflare.  I had read the initial posts from Cloudflare that implied that they became aware of it, I think Nick even said this, when Tavis tweeted publicly Cloudflare had been leaking customer sessions for months.  That's how I thought they learned.  If Tavis did some backchannel attempt to contact them first, then I apologize.  But I was a little critical of Tavis.  If that's the way he announced it, that might have been - because that would have been - that was February 23rd.  That would have been before he'd cleared all the - or maybe not.



STEVE:  Well, I mean, he may have reached out through email, not had an immediate response, and because he was panicking, he thought, okay, they're at dinner.  I'm just going to use Twitter to get an emergency message out.



LEO:  And we're pretty sure that everybody, like Bing and everybody else, cleared their caches?



STEVE:  I did read that the Cloudflare guys looked at their logs, found all of the spidering that had been done, and then reached out proactively to the other caching organizations and said, "We have had a huge problem.  We fixed it, but you may have something historical."



LEO:  Here's the tweet.  Okay, I do apologize.  This is the first tweet, from February 17th:  "Could someone from Cloudflare Security urgently contact me?"  There's Tavis's initial post.  So that's completely fair.  If you can't get hold of somebody, what else?  You use Twitter.



STEVE:  Yeah.



LEO:  It worked, by the way.  They did, they contacted him.



STEVE:  Yeah.  And as I mentioned, I saw that go by, and I thought, oh, I wonder what that's about?



LEO:  Yeah.



STEVE:  And now we know.



LEO:  Well, it's an unfortunate circumstance all around.



STEVE:  Well, yes.  It is, but it's also the consequence of the way our system is evolving.  And I don't want to say devolving.  But Cloudflare exists to perform a service that many people find vital.  So to do what they do, they have to, as you said, Leo, they have to take the responsibility.  I wouldn't want that responsibility.  But they said, okay, we're going to do this, to offer the service.



LEO:  Well, if you're going to be a DDoS service, don't you have to do that?  I mean, don't you have to give - we don't use Cloudflare.  We use CloudFront, which is Amazon's.  And we use it for load balancing as much as DDoS protection, although it's effective DDoS protection.  Wouldn't they have to have your keys and have to be a man in the middle?



STEVE:  No.



LEO:  No.  Okay.  So there'd be other ways to do it.



STEVE:  They could proxy the TLS connection.



LEO:  Got it.



STEVE:  Just at the TCP level, not at the protocol level.



LEO:  They don't need to crack it, yeah.



STEVE:  But, for example...



LEO:  But the rewrites do, if you want to do a...



STEVE:  Correct.



LEO:  Yeah.



STEVE:  Correct.  And I've gone to DDoS protected by Cloudflare sites where I get this weird sort of intercept page with some bouncing balls.



LEO:  Yes.  It says:  "We're protecting from DDoS."



STEVE:  Correct.



LEO:  And you get that when the site is actually under attack, I believe.



STEVE:  Right.  So what they're doing is they're using their ability to intercept the connection and injecting their own JavaScript into the page as part of the active technology to discriminate attackers from non-attackers.  So again, I mean, props for all of the technology that they have brought to bear.  And again, they're providing a super useful service.  But mistakes happen.  And again, I never fault anybody for a mistake.  Anybody can make them.  I'm banking a lot of karma of my own for the day that I royally screw up somehow because, you know, it could happen.



LEO:  Well, and Cloudflare even said, "We've been in the process of replacing the Ragel parser code because we thought it was difficult to use, and we've been writing our own replacement."  Just not in time, I guess.



STEVE:  Right.  Well, in fact, I think that Nick said that it was actually the process of replacement...



LEO:  Oh, that's right, yes.



STEVE:  ...that caused this to get a lot worse, and that brought it to Tavis's attention.  So again, it's so, I mean, if Tavis hadn't seen this - look, I mean, imagine that this had continued.  Because it was sort of a fluke that he even saw this, looking at data collection on a broad scale and going, uh, what is this?  Did I make a mistake in my code?  No, Tavis.



LEO:  In a way, this is an example of everything working as it should.



STEVE:  Yes, yes. 



LEO:  I mean, you know.



STEVE:  If we're going to have a system of brittle technology - and like it or not, that's what we have.  This world that we're in, that we have created, is brittle.  So if you're going to have brittle technology, the best you can do is monitor it, is keep an eye on it; and, when problems arise, fix them as quickly as you can.  And again, this is the way to do it.



Which is actually a perfect segue into the next story I had here, which is Google's report of another high-severity bug, this time in Edge and IE, with no patch available.  Microsoft was notified more than 90 days ago, on November 25th.  The clock ticked, and Project Zero gave them 90 days.  Presumably this is something that would have been fixed in Patch Tuesday three weeks ago, which we know didn't happen.



So Google's Ivan Fratric discovered and posted details.  I won't go into them because it's just mumbo jumbo, mostly.  But essentially, he stumbled upon a mistake in the way columns are parsed, either in HTML tables or in CSS, such that, when he looked, he dug into the code that Edge and IE share, a user controlling - get this - the width and the spacing details of a table could inject their own code through just HTML properties and end up getting it executed.  And he demonstrated that you could push this thing all the way through.  So there is, unfortunately, the 90-day expiration happened.  Unlike Cloudflare, that had this whole thing wrapped up in four days and shut down in a few hours, Microsoft is 90 days out.  And again, we still have the SMB problem unfixed.  And now we have this that also came out of its nondisclosure timeout.



So it has been - the U.S. National Vulnerability Database gave it a CVE of 2017-0037, saying that it, quote, "allows remote attackers to execute arbitrary code via vectors involving a crafted Cascading Style Sheet token sequence and crafted JavaScript code that operates on a table-header element."  So I'm sure we'll get a fix, I hope we'll get a fix for this in March.  And Leo, didn't someone say, I don't remember which podcast it was, but I heard someone tell you that they - I think it might have been Paul and Mary Jo last Wednesday - that Microsoft's internal patching system had collapsed, or had a glitch or a fault or something like that?



LEO:  I think I was joking about that.



STEVE:  Oh.



LEO:  I certainly, no, I don't think we know what happened at all.  We can only speculate.  I don't think Microsoft ever said why they killed the patches.



STEVE:  No, no.  They've said nothing.  They just sort of said, well, we were going to delay it.  Now we're going to cancel it altogether.  Meanwhile...



LEO:  I want to correct it, my pronunciation of the creator of Pinboard, Maciej Ceglowski:  Mah-chay Chi-glou-ski.  Sorry, Maciej.  Go ahead.  I've been schooled in Polish.  Sorry.



STEVE:  So in this week's IoT headshaker we answer the question, what are CloudPets?  So get this.  The idea is that friends or relatives can use the CloudPets smartphone app to record an audio message and send it to an app on the parent's phone.  The app then uploads the audio to the plush toy, a teddy bear.  Apparently you can get a unicorn.  One of the security researchers said, "Oh, I decided I would buy a unicorn in order to look into this further" - using Bluetooth LE.  When the child presses the animal's right paw, it will play back the message, you know, from Grandma or whomever.  They can then record their own reply message by pressing the teddy bear's left paw.  The iPhone app then retrieves the audio via Bluetooth from the plush toy and sends it back to the friend or relative.  What could possibly go wrong?



Well, yes.  Motherboard reports:  "A company that sells Internet-connected teddy bears that allow kids and their far-away parents to exchange heartfelt messages left more than 800,000 customer credentials" - that's a popular plush toy - "as well as two million message recordings, totally exposed online for anyone to see and listen to.



"Since Christmas day of last year and at least until the first week of January, Spiral Toys" - and I won't make any jokes about spiral, where they're spiraling - "left customer data of its CloudPets brand on a database that wasn't behind a firewall or password-protected.  The MongoDB was easy to find using Shodan, a search engine that makes it easy to find unprotected websites and servers, according to several security researchers who found and inspected the data.



"The exposed data included more than 800,000 emails and passwords, which are secured with the strong, and thus supposedly harder to crack, hashing function bcrypt."  That's a good PBKDF, password-based key derivation function, which makes it more difficult to brute-force.  "Unfortunately, however, a large number of these passwords were so weak that it's possible to crack them anyway, according to Troy Hunt, a security researcher who maintains 'Have I Been Pwned' and has analyzed the CloudPets data."



A different researcher, just this morning, Tuesday morning, posted - his name is Paul Stone.  He's with U.K.-based security firm Context.  His post was "Hacking Unicorns with Web Bluetooth."  And I won't go into the details, but suffice to say for an unknown reason these pets do not use any pairing.  So anyone within Bluetooth LE range, or longer if you have a directional or higher gain antenna, is able to connect to a child's CloudPet and upload their own content.  Yes, and that video's pretty funny.  He's got the Dalek saying one of their expressions in a menacing fashion.



[Clip]



LEO:  That's coming from Cayla the Bear.



STEVE:  Not what you want your infant to be exposed to necessarily.



LEO:  Destroy, destroy.  Wow.



STEVE:  Yeah.  So once again, another, as I said, a headshaker brought to us by the Internet of Things.  We also have a - oh, boy - sort of off-the-map, but still significant, airport 60 miles north of Manhattan.  The Stewart International Airport, which apparently serves hundreds of thousands of passengers a year, is regularly used by the military.  It's known for accommodating charter flights of high-profile guests, including foreign dignitaries.  Unfortunately, the IT person, who was a contractor, who was set up to create a backup for their servers, left the server and its backups publicly exposed on the Internet since April, okay, not April Fools, but April of last year.



This backup, there were 11 full disk image backups with hundreds of gigabytes of files and folders, including dozens of airport staff email account data, sensitive HR files, interoffice memos, payroll data, and what appears to be a large financial tracking database.  Many of the files that security researchers reviewed included confidential internal airport documents, including schematics and details of the core infrastructure.



Others, belonging to Homeland Security agencies, were marked "sensitive," but not "classified," including comprehensive security plans, screening protocols, and arrival procedures for private jet passengers.  And then there was a file containing a list of usernames and passwords for various devices and systems, allowing unfettered access to the airport's internal network, according to two researchers who took a look at it.  So just another example of too much data loose and unsecured on the Internet.  But it's very convenient to have your backups available.



We have news of a problem that Google encountered which caught them by surprise as they began to deploy Chrome v56.  We've been talking about 56.  That's the one a couple months ago we were talking about it, anticipating it, where they were going to start being a little more proactive and cautioning users when connections were not secured.  Not saying they were insecure.  But instead of saying nothing, if you don't get the happy green padlock, they would up the ante a bit and say, uh, this is a non-secured connection.  That was Chrome 56.



Well, it turns out the other thing they rolled out as part of their process was support for the latest version of TLS, Transport Layer Security, which is v1.3.  And then the problems began.  Suddenly, reports started coming in of, like, people all over the place having no Internet connectivity.  They were unable to get to all kinds of sites.  And so Google got on it and figured out what was going on.



It turns out that many people are using the Blue Coat v6.5 proxy.  Blue Coat is one that came to my attention a couple years ago.  In fact, on the HTTPS fingerprinting page I use Blue Coat as an example of one of these middlebox proxies which is intercepting all TLS connections.  And we can already guess what the problem is.  Blue Coat doesn't support TLS v1.3.  But the TLS protocol elegantly handles version downgrade.  And in fact, that's been a source of attacks in the past, where you're able to - a man in the middle can downgrade a connection in order to force the use of weaker security.  So there have been all kinds of mitigations and protections against that over time.



But here we have an instance where something which is trying to be transparent got itself exposed because its code had a problem with this latest version of TLS 1.3, which Chrome was the first major browser to bring out into the public.  And then we had a worse problem because first of all they had to figure out what was going on.  Then they said, okay, well, after much back and forth, and I've read the whole thread, they just decided, okay, we're just going to roll back TLS 1.3 support for Chrome 56.  We'll take that out.



The problem is, once people had already upgraded their Chrome to the v56 that had TLS 1.3, and if they were behind a Blue Coat proxy, as a surprising number of people are - and again, this is another surprise that Google found was how many customers suddenly had a problem because they ran across this.  But now the problem is your browser's broken, so you can't get it fixed.  It can't update itself because there's no way around the proxy.



So the researcher was a guy named Jay H. Lee.  And the thread goes on.  I won't go into details because we're a little bit short on today's podcast.  But I wrote in my notes, "CATCH-22:  Once you've received a Chrome 56 which starts using TLS v1.3, and you're behind a Blue Coat, distressingly nontransparent TLS proxy, you can no longer connect to Google to receive the update."



So Jay did end up posting some workarounds.  He said, first, to anyone who is affected by this:  "On your internal DNS server" - that is, if you're corporate IT, and you're running your own DNS which your Intranet users refer to - "create a temporary address record, a DNS A record, that points" - clients4, numeral 4, C-L-I-E-N-T-S-4 dot google dot com, at - and then he gives an IP address, 64.233.186.102.



He says: "Once that's in place, restart Chrome, reboot Chrome devices a few times."  He says: "It may take up to 30 minutes" - that's because Chrome doesn't aggressively go out to get new updates.  It only checks periodically to see if there's anything new.  So it will take it a while to go and discover something new.  He says:  "It may take 30 minutes and a few restarts, but devices should get the update to stop using TLS" - okay, now, he says to stop using 1.2.  I think that's just a typo.  He must mean to stop using 1.3 and to use 1.2.  And then he says:  "Important:  Be sure to remove the DNS A record once this has been fixed.  Leaving the record in place WILL, [all caps], BREAK THINGS DOWN THE LINE."



Second possible workaround:  "Have the user visit" - and then he has a URL here  "chrome://flags/#ssl-version-max, and set it to TLS 1.2."  So you're taking, at your client side, you're backing it down, saying we're revoking permission for you to use TLS 1.3.  Use 1.2.  He says:  "This works for Chrome users, but not if the problem is occurring on Chrome OS login screen."  And then he says, again:  "Important:  Be sure users turn this setting back to Default after leaving it on for 1-2 hours."  Meaning you would need to do that in order to get your Chrome updated.  Then you'd want to re-permit, you'd want to remove that limitation so that, once Blue Coat fixes their problem, this thing will get fixed.



And then, finally, the third option, the workaround:  "Allow Chrome to connect directly to the Internet for connections to clients4.google.com."  That is, if the Blue Coat system has a whitelisting provision, whitelist clients for .google.com.  "Then clients within your Internet will be able to get themselves updated to the fixed Chrome 56.  And then, after that's been resolved, remove that whitelist from Blue Coat."  So again, a brittle system that we're all riding on top of.  And every so often things break.



This was an interesting wrinkle that I just wanted to put on our listeners' radar, and that is that this question of using fingerprints to unlock a phone, still unresolved.  And so ultimately what this is going to end up being, I think, is the right case needs to be found, and then we have to get this up to the Supreme Court.  But in this particular case a federal judge in Chicago issued an opinion on February 16th that was only made public a couple days ago, that would deny the government's attempt to force Apple device owners from providing a fingerprint to unlock their device.  Of course the listeners of the podcast know that there's been sort of this interesting compromise where the argument has been that using a fingerprint is not testimonial, whereas requiring the divulgence of a password is.  So there was that kind of awkward compromise.



The 14-page opinion that this judge rendered as part of a child pornography case adds to the growing debate around individual rights to privacy and the needs of law enforcement to get past encryption techniques and technologies to further their investigations.  It boils down to Fourth Amendment protections against unreasonable search and seizure, as well as the Fifth Amendment right to avoid self-incrimination.



"In recent similar forced-fingerprinting cases, prosecutors have argued that providing a fingerprint does not threaten an individual's Fifth Amendment right to not implicate oneself.  A fingerprint provided as a means to identify an individual has, of course, been allowed in court."  This is, you know, Sherlock Holmes would be out of business if you couldn't use fingerprints.  "However, a fingerprint pressed into service as a means to unlock a user's smartphone is unfolding in courtrooms as an entirely different matter.  While Judge M. David Weisman stated in the court document, which is still sealed, that law enforcement did have probable cause to search a particular home, he drew the line when it came to 'compelling individuals to provide their fingerprints to unlock an Apple electronic device.'



"The case in Chicago also riles privacy advocates, who argue that requiring those swept up in an investigation to provide fingerprints to unlock a device raises Fourth Amendment issues against unreasonable searches and seizures."  Which is to say there isn't probable cause for investigators to believe there's something.  They're just looking for opportunity to look everywhere.  "Speaking with Ars Technica about the case, Abraham Rein, a Philadelphia-based tech lawyer, pointed out that 'there is a big difference between using a fingerprint to identify a person and using one to gain access to a potentially vast trove of data about them and possibly about innocent third parties, as well.'"



LEO:  That's where this is going to come down to.



STEVE:  Yes.



LEO:  You can't be compelled to testify against yourself.  Your phone has so much in there that I think it's de facto a form of testimony.



STEVE:  Right, right.  So anyway, I finished this up saying my take is that judges are coming down all over the place due to a current lack of clear statutory law.  Even appeals courts are splitting on this.  So we're going to need ultimate clarification from the Supreme Court.  And these issues are so important to the way our future unfolds that we need to hope that any decision is carefully considered and widely debated.  I'd expect to see a huge number of amicus briefs presented by both government and industry, representing all of the interests.  And at this point we just need the right case to be argued and presented to the Supreme Court in order to get some law.  Because right now, again, the way our judicial system works is it's up to the judges to decide what is and is not correct.



Oh.  And in coming back to a fun topic that we talked about in November, Amazon is continuing to refuse to hand over data which may - and again, no reason to know it, but may have overhead details of a murder.  So remember we discussed this, that suspicious and suspected murder case in Bentonville, Arkansas late last November, where somebody was found deceased in a hot tub.  The local police investigators have asked for all available evidence from the connected home's owner's IoT devices.



And as we discussed at the time, they had already obtained a data dump from the IoT water meter, which indicated a huge and suspicious amount of water had been used in the wee hours of the morning, with no explanation given by the suspect owner of the home.  But they want to know whether the owner's Amazon device, to keep from triggering it, may have overheard anything illuminating.  Ars reports that Amazon is balking at a search warrant seeking cloud-stored data from its system.  Arkansas authorities want to examine the recorded voice and transcription data as part of the murder investigation.  Among other things, the Seattle company claims that the recorded data from an Amazon Echo near a murder scene is protected by the First Amendment, as are - the First Amendment.



LEO:  The First Amendment.



STEVE:  As are - that's what they're - yeah.  As are the responses from the voice-assistant itself.  "Amazon said that the Bentonville Police Department is essentially going on a fishing expedition with a warrant that could chill speech and even the market for those devices..."



LEO:  Well, yeah.  That's a good point.  That's a good point.



STEVE:  Yup, "and competing products.  In a motion to quash the subpoena, the company said that, because of the constitutional concerns at issue, the authorities need to demonstrate a 'compelling need' for the information and must exhaust all other avenues to acquire that data."  And I didn't have it in the notes here because I wanted to keep this from getting too long.  But, for example, Amazon pointed out that, if this user had the Alexa - oops, I said it, sorry - the "A" word app on their phone, then that app would have a record.



LEO:  Yeah.  You could unlock the phone with the finger - oh, he's dead.



STEVE:  Right.



LEO:  Oh, no, the suspect [crosstalk].



STEVE:  Right.  And, I mean, if this guy's got his water meter hooked into his home with IoT, no doubt he's got the Alexa loaded on his phone.



LEO:  Sure, sure.



STEVE:  And so the point is, rather than just doing this broad sweep, they need to keep this from becoming overly broad.  And anyway, so Amazon's doing the right thing, I think, for us, and certainly for themselves, by saying, you know, by pushing back against this.  And there's no reason to believe that there's any data there.  They're just saying, you know, what if?



LEO:  We would like to see, yeah.



STEVE:  Yeah.  I doubt anyone screamed out the name of the...



LEO:  The murderer is [incoherent death throes sounds].



STEVE:  Call 911.  Call 911.  So the case has generated a huge amount of publicity, putting heightened pressure on everyone on both sides of the struggle.  Amazon needs to assure all of its customers that these devices are not surreptitious spy-bots, and the detectives need to recover and analyze all possibly relevant evidence.  The question is, where do we draw the line?  And, boy, essentially this is a classic case of a whole bunch of new problems being presented by new technology.  Just wait till the self-driving cars start causing damage.  We're going to be back in court again.



Okay.  So this was a proof of concept.  That's all it was.  But because it was well executed, it had a video and a clever name, the press got a little carried away.  Even UPI had the headline:  "A computer's LED light can smuggle out data from the hard drive."  And Wired.com covered this, saying, "Watch Malware Steal Data From Air-Gapped PC With Blinking Lights and a Drone."



LEO:  Oh, lord.



STEVE:  So in this dramatic video, we have the first-person perspective of the drone, with this industrial building in the background at night.  And we take off, and we somewhat hesitantly fly up to the building and look in the window with our drone, hovering there, looking sinister, in the middle of the night.  And it finds this flashing light on the hard drive.  Now, the key here is that this is an un-Internet-attached machine, a so-called "air-gapped PC," deliberately running the other half of this experiment's software, meaning deliberately flashing the light in order to send the data out in a meaningful and high-data-rate fashion.  So, and we talked about this a long time ago, Leo.  Remember the network lights behind us that were flickering away on the servers?



LEO:  Yeah.



STEVE:  And it was like, oh, my god, there's our data being exfiltrated.



LEO:  It's leaking data.



STEVE:  No, that's a packet that went by, and who cares?  Because it's not showing you the contents.  So this is worse, inasmuch as what they found is that they could get about six "kiloblinks" per second.  And they did, they set up a four kilobit link, optical link from this machine rapidly flashing its hard drive light, which the malware, I mean, their proof-of-concept code running in the machine was deliberately encoding with the data.  And the drone was out there hovering, picking it up.



So we've had a lot of fun on the podcast over the years, looking at all the different ways you can arrange to exfiltrate data with the sound of the hard drive, the subsonic or ultrasonic sounds coming from the speaker, just anything you can imagine that could be controlled, that could be sensed at a distance, whether sound or light.  And so here's just another one.  But this is not to say that anyone can fly a drone up to someone's window and, I mean, you don't want any drones looking in your windows anyway.  But having it look at your hard drive light on your computer is going to tell it nothing, especially if it's running Windows, because the drive never stops flashing, and no one has ever figured out why.  So anyway, not a big problem there.



A quick note just on this whole topic of third-party AV having the potential to expand the attack surface, unfortunately.  The securitylist.org has reported just a couple days ago a remote-code execution as root via ESET's Endpoint Antivirus 6 on macOS.  And of course ESET's marketing material proudly boasts:  "ESET Endpoint Antivirus for OS X delivers award-winning cross-platform protection for multiplatform environments.  It protects against malware and spyware and shields end users from fake website phishing for sensitive information such as usernames, passwords, or credit card details."  Of course, as we know, the only way it can do that is looking inside all of your connections.



"Unauthorized devices," they write, "can be blocked from the system entirely.  The solution's highly intuitive interface allows for quick navigation.  Unfortunately, however, vulnerable versions of ESET Endpoint Antivirus 6 are statically linked with an outdated XML parsing library and do not perform proper server authentication, allowing for remote unauthenticated attackers to perform arbitrary code execution as root on vulnerable clients."  So that means if your Mac is running ESET Endpoint Antivirus 6, and somebody wanted to target you, I mean, this isn't just something that anybody can find you through Shodan or something.  You would need to be targeted.  They would need to intercept the periodic connection which the ESET daemon makes to check for its licensing.



And it goes to edf.eset.com, which it uses periodically to verify that it is currently licensed.  If that query is intercepted, then because there's no validation of the certificate, it's trivial for a man in the middle to leverage this XML parsing library flaw and run their own code with root privileges on your Mac.  So none of this would be possible if you didn't have this thing in your system trying to help you.  But again, because it has to be perfect, and it's difficult to make things perfect, it ends up creating more vulnerabilities, one might argue.  Or at least a very well known vulnerability.  And I should have followed up, but I didn't, to see whether this had been fixed by the time it was disclosed.  It looks like this CVE is 2016-9892, a big CVE number, so toward the end of 2016.  My guess is that it wasn't until this got fixed that these guys disclosed it, since that's at least several months since then.



A couple bits of miscellany.  I got a tweet relevant to this.  AspiringLockpicker is his Twitter name.  He says:  "@SGgrc, is it okay to install an offline-only antivirus like ClamWin?  Or is it still issue-prone regarding OS hooks, delays, et cetera?"  And so I just wanted to address the fact because I saw other people asking, too, in the email bag.  And that is that the biggest concern is something running all the time because it always creates the vulnerability.  So the idea of using an offline, on-demand AV scanner, that's a much different problem, and I would argue much lesser concern than something that has persistently hooked itself deep into your OS in order to look into your encrypted connections to inspect them to make sure nothing bad is going on.  And since it's doing that, it does present a much bigger persistent attack surface.  An offline scanner that you run on demand doesn't create that problem.



I also mentioned at the top of the show that a number of people had sent other crypto education links.  There's Crypto101.io, which is another very nice, I think it's a 252-page PDF.  Also Ed Felten at Princeton has an amazing five-page encryption primer meant for, like, policymakers, like politicians who absolutely understand nothing.  And I've seen it referred to as "arguably the most clear, well-written piece of technical documentation in the history of man."  Some simple diagrams.  It's only five pages.



And I wanted to note that I have created a new section on GRC's Linkfarm page for free crypto education resources, and all of these links are there now.  And I also noticed, I just googled "linkfarm," and I was curious.  GRC's Linkfarm page is now, after the Wikipedia definition of the term, the first hit that Google returns.  It might be biased because Google knows I am me.  But it looks also like that page is becoming popular, and Google has noticed it.  So you can just put "linkfarm" into Google, and you can probably find the one that you mean, which is GRC's Linkfarm page.



A quick tweet from Ryan McGinnis, who said:  "@SGgrc Good God #theexpanse is killing it.  When did Syfy" - meaning S-Y-F-Y, the channel - "go from B-movie schlock to HBO-quality space drama?"  Anyway, I agree, and many of our listeners are enjoying "The Expanse."  So I just wanted to remind people about that.



Also, many people have been encountering this long-forgotten Portable Sound Blaster, or Portable Dog Killer, I called it "The Quiet Canine Page," at GRC.  I had a whole bunch of pages that I never got around to fixing.  But it's a constant source of interrupt for me because people say, hey, whatever happened?  So anyway, I fixed it all.  For what it's worth, if you just put in GRC.com/tqc, for The Quiet Canine, there's now one page has the schematic, the bill of materials, and it explains what we learned, which is you cannot get a dog to stop barking three doors down.  But if you need a personal defense device, if you're a postal worker who delivers mail on foot, or you're a jogger and dogs are nipping at your heels, this will convince them that they don't want to do that anymore. 



Lastly, I got a note, a follow-up from a couple months before I shared with our listeners from someone who calls himself Glasair Pilot.  He was the guy who ran SpinRite at Level 2 on some drives in his RAID which were constantly timing out and causing problems.  And after doing that, even though SpinRite didn't say there were any problems found, no more timeouts.  So he was kind enough yesterday to follow up and say, "FYI, it's been almost two months since I did SR L2 on the two spinning drives (out of four) on my RAID 10.  Zero warnings since then, and RAID has not gone critical since."  So it's nice to know that whatever SpinRite did, it fixed it well.



And I did want to share - I know you know, Leo, we had major storms in Southern, well, in California, both in Northern California and Southern California.  A little over two weeks ago we had major winds that knocked out Sue's power.  It was two years ago May that we had that experience that I shared with our listeners where Sue called me and said, "My computer won't boot."  And I said, oh.  And she said, "Yeah, a while ago it started complaining every time I turned it on about the RAID being critical, but said I could press Escape to continue, so I have been."  So one drive went down.  Everything was still working until the second drive went down.  And I've talked often since then, and this never occurred to me, but a RAID ought to say, "Okay, call IT."  It should not say, "Click here to ignore this warning." 



LEO:  To ignore this, yeah.



STEVE:  So it happened again.  However, Sue learned her lesson.  This time, after the big storm, when she brought her machine back up, she got the big boot halt, saying "RAID critical, press Escape to continue."  This time she phoned me.



LEO:  Good.



STEVE:  She said, "Steve, I got this error message, and you told me not to ignore it this time."  I said, "Oh, bless your heart."  So I said, "For now, press Escape."  I said, "As soon as I can" - I think this was like late the week before last.  I ended up, with our schedules, not getting down to her until last Friday.  And it was textbook perfect.  I identified the one of the two drives that was the problem, removed it, put in a blank spare, rebooted.  It noticed that the RAID was broken, offered to fix it.



Oh, I should say that, before even taking the system down, I used my favorite tool, which is Image for Windows, to make an image of the running system on an external drive so that nothing that then happened could cause me to lose from that point forward.  Then we repaired the RAID, rebooted.  We're all up and good again.  So that's the way it's supposed to work.  So SpinRite saved the day a couple years ago as a consequence of Sue just saying, oh, well, okay, I don't want to bother Steve, so I'm just going to hit Escape because I'm not sure what this error message means.



And that brings us to the "The First SHA-1 Collision."  We're out of time.  But there's enough time.  Essentially, you know, we've talked about hashes.  We know I'm bullish on hashes.  I love, as a piece of a toolkit, what a hash allows you to do because it takes a file of any size and reduces it to a unique set of bits that represent that file, such that, even if you change one bit of the entire file or communication stream or whatever it is that you have hashed, then the cryptographically strong nature of the design of the hash means that, on average, half of the bits will flip in the output, if you just change any one bit in the input file.  So just a cool technology.



But a hash has several properties that make it worthwhile.  And I thought, I didn't want to bias this for this story, so I would read it right out of Wikipedia, that says:  "[A hash] is deterministic, so the same message always results in the same hash.  [A hash] is quick to compute for any given message.  It is infeasible to generate a message from its hash" - meaning to go backwards - "except by trying all possible messages.  A small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value."  And the fourth principle of a hash:  "It is infeasible to find two different messages with the same hash value."  That's what Google did.



Now, to say they did it doesn't mean it was easy.  And I know that you've covered this, Leo.  In Google's explanation of this, it took them years.  Nine quintillion, that's 9,223,372,036,854,775,808 SHA-1 computations in total.  That would be 6,500 years of GPU computation to complete the first phase of the attack, and a 110 additional years for the second phase.  So what this represents, though, is the first instance where we have had enough detailed understanding of, that is, our appreciation for how SHA-1 has matured, that we understood how there was a potential flaw.  And I'm looking in my notes to see if there's anything else I wanted to cover here.



Google wrote:  "In 2013, Marc Stevens published a paper that outlined a theoretical approach to create a SHA-1 collision."  Meaning where two different texts can be deliberately designed so that they hash, individually hash to the same thing.  That's what is supposed to be impossible.  So Google wrote:  "We started by creating a PDF prefix specifically crafted to allow us to generate two documents with arbitrary visual content, but that would hash to the same SHA-1 digest.



"In building this theoretical attack in practice, we had to overcome some new challenges.  We then leveraged Google's technical expertise and cloud infrastructure" - meaning lots of brute-force computing - "to compute the collision," meaning that they computed custom data that would be embedded in each of these PDFs that would not be seen, but would allow them to deliberately cause both different PDFs...



LEO:  Oh.  That makes sense.



STEVE:  ...to resolve to the same thing.



LEO:  So they did the PDFs first without regard to the hash, and then started loading them up with data to make the hashes match.



STEVE:  And then embedded, exactly, embedded different data in the two of them.  So, okay.  So what does this mean?  This is, you know, we've been talking about SHA-1 being sunsetted.  This doesn't mean that suddenly it's broken.  It doesn't mean that those things that are still using SHA-1 are suddenly vulnerable.  But this is the way cryptography works is that we chip away at this.  And in fact, Leo, there's a fabulous link in my show notes.  Oh, you don't have my show notes.



LEO:  I don't have, yeah.



STEVE:  Aw.



LEO:  Just tell me what it is, I'll Google it.  That's what I've been doing all along.



STEVE:  Yeah.  It was a...



LEO:  It took them, what was it, the equivalent of 100,000 years in computing time to create these, this collision.  So it's merely that computing is getting faster and faster.



STEVE:  Okay, check this out.  It's ValerieAurora, V-A-L-E-R-I-E-A-U-R-O-R-A.



LEO:  You don't have to spell it.  Google doesn't care.



STEVE:  Oh, sorry, ValerieAurora.org/hash.html. 



LEO:  Okay, good.



STEVE:  And what that shows, it's a wonderful chart that shows hashes that have been created over time vertically, and then a horizontal timeline of how they have been weakened, first problems found, vulnerabilities, and then total breakage.  So a really, really nice chart that will be in the show notes.  As soon as we're through recording here, I will get them converted to DOC and a PDF, and get them posted, and tweet.



LEO:  And it's exactly the curve you'd expect.



STEVE:  Yes.



LEO:  As computing power increases.  And, I mean, eventually computer power will increase to the point where a 2048-bit RSA key won't be sufficient.



STEVE:  Well, and of course the big scary bugaboo is quantum.



LEO:  Right.



STEVE:  Does that suddenly, like, can you suddenly solve all the problems at once somehow.



LEO:  Right, right.



STEVE:  And at this moment you can, as long as there's only 16 of them.  That is, if you need to find which pattern of four bits, that we can do with a qubit, a quantum four-state thing, somehow.  But we don't have four bits.  We have 256.  And the problem doesn't scale linearly, it scales exponentially.  So we're safe for the time being.  But again, this is the way these things happen.  They get chipped away.



LEO:  And Aurora's hierarchy goes, assumed to be weak, but no one bothers to break; then collisions generated by hand; then meaningful collisions generated on home computer.  We're not there yet with SHA-1.



STEVE:  No.



LEO:  First collision found, serious weakness discovered, minor weakness discovered, general acceptance, peer review, initial proposal.  So we're still at first collision found.  We've got, you know, meaningful collisions on home computer, we're probably a way off from that, I would think.



STEVE:  Well, right, you're exactly right.  And what we have seen, if there's another major lesson that our listeners have now seen play out over and over and over, is that legacy crypto is hard to kill.  It is just, you know, when they tried to just stop using SHA-1 for TLS, it was like, no, no, no.



LEO:  Was I wrong?  Google still uses SHA-1 in its certs, but they expire them in three months.



STEVE:  Well, although that's a different case.



LEO:  Oh, okay.



STEVE:  Because a self-signed cert can be used.  You don't have a collision problem there.



LEO:  Ah, okay.



STEVE:  So root certs can be signed by SHA-1, just not endpoint certs.



LEO:  Oh, I got it.  Okay.



STEVE:  So anyway, so it's not the end of the world.  This is the way this happens.  Again, hats off to Google for, boy, investing in nine quintillion-plus SHA-1 cycles.  And can you imagine how you'd feel?  It's like, oh, my goodness, did it really work?  And then you'd check it again by hand, and double-check it and make sure.  It's like, we just found, we just created...



LEO:  We have a collision, yeah.



STEVE:  Yes, the first.  So again, this is not something that is going to hurt us.  And everybody is moving away from SHA-1 as quickly as possible.  Nobody today would ever build a new system using SHA-1.



LEO:  Right.



STEVE:  So eventually those dinosaurs that are still around will end up dying off, and we'll all be at SHA-256, which has a bright future.



LEO:  Right.  Steve, as always, I think you have a bright future in this security thing.  You might want to consider doing a podcast or something.  Lot of fun.



STEVE:  How about next week?



LEO:  Next week?  Okay.  Every Wednesday.  I'm sorry, Tuesday.  You know, how long ago did we change it to Tuesday?  One of these days it'll sink in.



STEVE:  Oh, and I'm liking it on Tuesday because I notice that Mondays get skipped on President's Day and things.



LEO:  That's right.



STEVE:  I'm really happy with my Tuesday.



LEO:  Every other employee is praying for a day off, a three-day weekend.  But you, no, you say, I don't want to miss an episode.  And that's why we love you.  Steve Gibson, GRC.com.  If you go there, what will you find?  Well, you will find some wonderful stuff, including SpinRite, Steve's bread and butter, the world's finest hard drive and recovery utility.  You'll also find the podcast itself, and handwritten transcriptions, and show notes.  In this case, it's the only place you can find show notes until Steve gets them up.  I'm sure you'll get them up in minutes  after the show's over.



STEVE:  Yup.



LEO:  What else can you find there?  You can find Perfect Paper Passwords and the Healthy Sleep Formula and all those great things Steve does for us.  And those are all free.  GRC.com.  If you come here you'll get video and audio, as well as a lovely feeling when you see the beautiful TWiT website.  I got nothing.  I got nothing.  Steve's got all the good stuff.  I just have podcasts over here.  And you can also watch live, as I said, every Tuesday, 1:30 Pacific, 4:30 Eastern.  That's 21:30 UTC.  And we're usually done by this time, but thank you to Megan and Jason...



STEVE:  For their patience.



LEO:  For their patience.  They're going to start TNT in just a second.  And of course you should subscribe because really you want a complete set of Security Now! podcasts.  You can put them on your - get them leather bound and put them on your bookshelf.  You'll be the pride of your commune.  Thank you, Steve.  Go watch the State of the Union Address.  That's what I'm going to be doing.



STEVE:  Oh, I've got it, yes, absolutely.  Well, or the Address to Congress, actually.



LEO:  I guess it's not the State of the Union, it's just the address; right.



STEVE:  Correct.  Yup.  Thank you, my friend.  And we'll do it 1:30 sharp next week, come hell or high water. 



LEO:  Yes, yes.  Well, maybe, no, unless Cox screws up again.



STEVE:  Well, I was going to say, props to Cox.  Once it came back up, not a single dropped packet the entire time.



LEO:  Maybe they were fixing, you know, maybe they were fixing the network for you, so that it'll be perfect from now on.



STEVE:  Let's think that.  Let's think that.



LEO:  Yes.  That's [crosstalk].



STEVE:  And actually even Cox reached out and tweeted me, asking if I needed any assistance.



LEO:  Oh, good.  Oh, good.



STEVE:  So this got elevated to their attention.



LEO:  Well, yeah.  In fact, when I went to the outage page, they have a little section on the side of tweets.  It was all about Security Now!.  So I'm sure they were aware of the issue.  Thanks, Steve.  We'll see you next time on Security Now!.



STEVE:  Thanks, Leo.  



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#602

DATE:		March 7, 2017

TITLE:		Let's Spoof!

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-602.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  This week, Leo and I discuss the countdown to March's Patch Tuesday.  What was behind Amazon's S3 outage?  Why don't I have a cellular connectivity backup?  We share some additional Cloudflare perspective.  Amazon will fight another day over their Voice Assistant's privacy.  An examination of the top nine Android password managers uncovers problems.  We'll cover another fileless malware campaign found in the wild; security improvements in Chrome and Firefox; a proof of concept for BIOS ransomware; a how-to walk-through for return-oriented programming; a nifty new site-scanning service.



Matthew Green compares desktop and mobile security.  We've got a bunch of feedback quickies, an incredibly wonderful waste-of-time accomplishment, the future threat of deliberately fooling AI, and the dark side of automated domain validation certificate issuance.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  He's got all the security news.  He's got a lot of other stuff, too, including the weirdest scientific diversion ever, and a look at some new certs from Let's Encrypt that could be problematic.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 602, recorded Tuesday, March 7, 2017:  Let's Spoof!



It's time for Security Now!, the show where we get together and talk about security with the one who knows, Mr. Steven "Tiberius" Gibson of GRC.  Hi, Steve.



STEVE GIBSON:  Yo, Leo.  Great to be with you.  This is the first Tuesday of March because March 1st was last Wednesday.  So this is, once again, this is the lead-up to the probably big March second Tuesday, which of course - so I guess this would be the, what, the penultimate Tuesday...



LEO:  Use your words.



STEVE:  ...before the March Patch Tuesday, which we expect to be huge, well, huge in the sense of it fixing a lot of things.  But as we know, now that Windows has just a single blob that is a rollup of everything that they fixed, it's just going to be one thing, but it ought to be a big relief for people who are concerned.  I've even seen some independent companies releasing short-term fixes, their own Windows patches for some of these things that Microsoft just didn't get around to fixing in time.



And we've got a ton of news.  This is Episode 602.  We now know what was behind Amazon's S3 outage.  I wanted to just quickly respond to why I don't have cellular connectivity backups, backup connectivity, which a number of our listeners commented on due to last week's delayed recording.  I wanted also to share some additional Cloudflare perspective, following up also on last week.  Amazon turns out is going to be fighting another day over their Voice Assistant system's privacy questions, so we'll touch on that briefly.  An examination of the top nine Android password managers, which uncovered problems in them all, including LastPass, the one that I use and you use and is sort of the show favorite.



There's another wireless malware campaign which has been found in the wild using a disturbing new technology - well, actually a disturbing old technology in a new way.  We've got security improvements in both Chrome and Firefox that are both interesting in different ways.  A proof-of-concept BIOS ransomware, which was demoed at the RSA conference and will be fully disclosed at this upcoming Black Hat Asia conference at the end of the month.  A brief how-to walkthrough for return-oriented programming.  I'm not going to go into it in depth, but it's interesting, and I know that some of our listeners will want the details.  I found a nifty new site-scanning service.  Matthew Green in his most recent blog compares the fundamental security of desktop versus mobile in a very interesting way that I wanted to share because I knew that would be of interest.



We've got a bunch of feedback quickies from our listeners.  The most wonderful, amazing waste-of-time accomplishment I've ever seen.  I mean, like, okay, I mean, it's just a feat that I think you and our listeners and I also really got a big kick out of.  Then the title of this podcast is how we'll wrap up, after we talk about something that just hit me between the eyes, and that is the future threat of deliberately fooling AI.  Then we'll wrap up with the reason I titled this "Let's Spoof!" - the dark side of automated domain validation certification issuance.



LEO:  Oh.



STEVE:  That is, we know about Let's Encrypt.



LEO:  Let's Encrypt, now Let's Spoof.



STEVE:  And what's happened, Let's Spoof!.  Turns out it's being horribly abused.



LEO:  Of course it is.	



STEVE:  And, you know, of course it is.



LEO:  This is why we can't have nice things.



STEVE:  Exactly.



LEO:  You're not going to talk about the Vault 7 - it's probably too early to talk about the WikiLeaks Vault 7 release of CIA tools.



STEVE:  I just didn't have any time to come up to speed.  



LEO:  Yeah.  And we'll talk, I'm sure, next week about it.  And it's one of those things where it's 7,000 pages.  It's a lot to look at, millions of lines of code.  I do think it's interesting the names.  You remember we were always fascinated by the naming scheme that the NSA had used in the Snowden dump.



STEVE:  Right.



LEO:  But that was, I guess, a randomized two-word generator.



STEVE:  Right.



LEO:  Apparently the CIA doesn't have such an interest - or the GCHQ because I think of these came from Britain - in obfuscating the purposes and the functions of its tools, though they have a lot of clever names that do in fact give you a hint as to what the thing does.  I'm sure we'll be talking about that a lot over the next few weeks.  All right.



STEVE:  I forgot to ask you how many breaks we had in this podcast.



LEO:  One more after this.



STEVE:  Okay.  So our Picture of the Week is xkcd having some fun with a topic we've been covering a lot.  This is xkcd.com/1807.  And the cartoon shows his little stick figure people that he typically uses, with a couple entering the house.  The people who live there say, "Hello, welcome to our house."  And then the couple entering say, "Thanks for inviting us," and then the A-word.



LEO:  Echo, we'll say Echo.



STEVE:  Echo.



LEO:  Yeah.



STEVE:  So they say, "Thanks for inviting us.  Echo, order two tons of creamed corn.  Echo, confirm purchase."  And then the caption says:  "When visiting a new house, it's good to check whether they have an always-on device transmitting your conversation somewhere."



LEO:  Wow.  Come on, Randall.  Really?  Aw.



STEVE:  And of course we know that's not the way it works.



LEO:  Somebody made a point, though, the other day that you could, Amazon could program it to listen for other keywords.  Right?  I mean, sure, there's not a lot of processing or memory in that, but maybe a handful of other keywords that it would silently transmit up to the cloud.



STEVE:  Well, Leo, they could change...



LEO:  They could just turn it on.



STEVE:  Yes.  The problem is they can't have all of the devices in the world simultaneously streaming audio to them.  That would probably even tax them.



LEO:  Yeah.  And nobody's got time for that.



STEVE:  But they could certainly turn on a microphone here and there, if they had some cause to.



LEO:  I suspect that's why they fought the law enforcement subpoena, because they don't want the next step to be law enforcement saying we want a FISA letter, a national security letter that says "Turn on the mic for Leo.  Don't tell anybody."



STEVE:  Right, right, right.  Okay.  So as we said at the top of the show, holding our breath.  Seven days from now, March Patch Tuesday.  And so we'll probably give that a little more attention than we have because there are a number of zero-days that are being fixed, and who knows what else?  And I don't know if we'll ever understand how February got missed, but it did.



We did get some clarification from Amazon about what happened last week.  As we know, you guys were talking about it on MacBreak Weekly before last week's Security Now! podcast.  And Amazon had a problem with S3.  The details, I think, are really interesting.  Amazon in their formal post-action report said:  "We'd like to give you some additional information about the service disruption that occurred in the Northern Virginia Region on the morning of February 28th.  The Amazon Simple Storage Service (S3) team was debugging an issue causing the S3 billing system to progress more slowly than expected."  Okay, so they were doing some maintenance.



"At 9:37 a.m. Pacific time, an authorized S3 team member, using an established playbook, executed a command which was intended to remove a small number of servers for one of the S3 subsystems that is used by the S3 billing process.  Unfortunately, one of the inputs to the command was entered incorrectly..."



LEO:  Pseudo rm-rf* - something like that; right?



STEVE:  Yeah, well, a typo, "...and a larger set of servers" - and apparently it was very much larger - "...set of servers was removed than intended."  So they have a command-based sort of meta management system that manages this vast infrastructure.  And so, like, rather than going over to servers and talking to them individually, they just do something.  And, for example, I've seen that when GRC has been under attack, and it's just like, okay, we're being blasted with ridiculous amounts of traffic.  I talk to Level 3, and I say, okay, just blackhole the IP.  And so someone at a single console types a command, and all of Level 3's routers at all of their borders update their routing tables to remove that IP.



LEO:  That's the power of BGP; right?  I mean, we've seen people scrub BGP before.



STEVE:  Right.  But it also typically, and certainly this is the case with Level 3, they have their own, you know, they can't have that happen by mistake.  There's got to be an audit trail.  You have to have authorization to do that.  So there's like a management layer on top.



LEO:  They should never have hired that guy from PricewaterhouseCoopers.  That was - just because he was out of work, come on.



STEVE:  So Amazon says:  "The servers that were inadvertently removed supported two other S3 subsystems.  One of these subsystems, the index subsystem" - whoops, that sounds important - "manages the metadata and location information of all S3 objects in the region.  This subsystem is necessary to serve all GET, LIST, PUT, and DELETE" - which is, you know, pretty much everything, all the critical verbs for getting and putting, listing and deleting stuff.  "The second subsystem, the placement subsystem, manages allocation of new storage" - like where should we put stuff - "and requires the index subsystem to be functioning properly to correctly operate.  The placement subsystem is used during PUT requests to allocate storage for new objects.



"Removing," they write, "a significant portion of the capacity caused each of these systems to require a full restart.  While these subsystems were being restarted, S3 was unable to service any requests.  Other AWS services in the US-EAST Region that rely on S3 for storage - including the S3 console, Amazon Elastic Compute Cloud (EC2) new instance launches, Amazon Elastic Block Store (EBS) volumes when data was needed from a S3 snapshot, and AWS Lambda - were also impacted while the S3 APIs were unavailable."



So Amazon's been building this system over time, and they've been building it carefully, and it's sort of their new services are aggregated and use the older ones.  And it happens that S3 was where this all began.  This is like the root core on top of which all these other things are built and depend.



So Amazon says:  "S3 subsystems are designed to support the removal or failure of significant capacity with little or no customer impact.  We build our systems with the assumption that things will occasionally fail, and we rely on the ability to remove and replace capacity as one of our core operational processes.  While this is an operation that we have relied on to maintain our systems since the launch of S3, we have not completely restarted the index subsystem or the placement subsystem in our larger regions for many years.  S3 has experienced massive growth over the last several years, and the process of restarting these services and running the necessary safety checks to validate the integrity of the metadata took longer than expected."



So essentially this is something that they knew worked, but they hadn't done for a long time.  And it sounds like they were surprised that it didn't - that this was an area that wasn't scaling linearly; it was scaling more exponentially.  So that they went "oops" when they realized that they'd inadvertently shut down much more of this than they expected.  And they're not explaining why, but essentially it required a full restart.  And then they realized with some chagrin that, oh, is it restarting?  It's like, well, yeah, it's trying.  I mean, it is.  But it turns out it, like, took way longer than they had expected.  And it's something they hadn't done for a long time.



So they said:  "The index subsystem was the first of the two affected subsystems that needed to be restarted.  By 12:26 p.m. Pacific time, the index subsystem had activated enough capacity to begin servicing S3 GET, LIST, and DELETE requests."  But remember not PUT because that's the other thing.  "By 1:18 p.m. Pacific Standard Time, the index subsystem was fully recovered, and GET, LIST, and DELETE APIs were functioning normally.  The S3 PUT API also required the placement subsystem."  And remember that that relies on the index subsystem, so that had to come later.  "The placement subsystem began recovery when the index subsystem," they write, "was functional and finished recovery at 1:54 p.m. Pacific Standard Time.  At this point, S3 was operating normally.  Other AWS services that were impacted by this event began their recovery.  Some of these services had accumulated a backlog of work during the S3 disruption and required additional time to fully recover."



Anyway, I just think this is a fascinating look.  And it goes on, for anyone who wants more details.  I'm not going to bother going into it here, but I have it all in the show notes.  Just sort of an interesting look into, like, the scale of this kind of system, where first of all the design is cool, the idea that - and we know how redundant it is because, for example, you pay more for more redundancy, and you pay less for lower redundancy.  And even so, you don't pay much.



I use S3.  It is my, as I said before, it is my primary cloud-based backup system.  And every month I get a bill.  I just keep pouring stuff into it.  All the podcasts are archived there, lots of images of critical subsystems and systems are archived there.  I get a bill for two bucks.  It's incredible.  I mean, I'm not doing massive bandwidth transit because that's where you start to pay the bills.  But $2 a month.  And I think about all the huge amount of storage that I use.  And it's incredibly robust.  As we've seen, not absolutely unable to fail.



But I should just follow up and say that this was a lesson for them.  And as this post continues, they assure us that many steps have been taken as a consequence of what they learned.  It is no longer possible to enter that typo.  There's much more oversight.  There's another system that watches what the impact will be and says, whoa, wait a minute, is this what you meant?  And so there's, like, much more accident prevention so that the tool, which arguably had become overly powerful - and this is another one of those things where, over time, the very, like the birthing assumptions no longer really held; but no one had revisited them because there were other things to do, forward progress and work.  So, anyway, just an interesting case study, I think, for this kind of massive, super-enterprise scale and how it operates, how it's resilient, and how it gets managed.



LEO:  I thought it underscores, just as Cloudbleed did last week, the problems with a monoculture.



STEVE:  Mm-hmm.



LEO:  It wouldn't have been such a big deal except that everybody uses S3 and Amazon Web Services.



STEVE:  Yes, exactly, a very good point.  So suddenly, wham, you know.  And in fact, I just saw that because we'll be talking about an interesting site viewer tool later.  And someone sent me a link to it.  Actually it was our friend Simon Zerafa sent me a link that he had run GRC against it, which is kind of a very boring response because GRC is a small site.  And I looked at their list of recently run scans and clicked on CNN, and it was just, like, full of AWS stuff.  And I thought, ooh, CNN would have been impacted if there wasn't redundancy.



Now, the flip side of that story is us delaying the podcast last week because my Internet connection was down for a couple hours.  And I got a lot of people who were like, what?  You mean you don't have an alternative emergency Internet connectivity backup of some sort?  And I said, okay.  My response was a little defensive, I guess.  But 601 podcasts.  So in 600 previous podcasts we have never had a single problem.  So to me, this represents the proper degree of concern, which is almost none because...



LEO:  It's just a podcast, folks.



STEVE:  Exactly.  It's a prerecorded podcast.



LEO:  People ask us the same thing.  "What would happen if the power went out at TWiT?"  We'd be down.  "What would happen if the Internet went out at TWiT?"  We'd be down.  "Well, don't you have backup?"  And my answer is always the same.  It's just a podcast.  This isn't a hospital.  It's not even network television.  You can wait a few hours, not a big deal.



STEVE:  And I do have things I consider mission critical.  GRC has never been down.  GRC has redundancy like crazy, I mean, where I don't even - I even have separately physical servers, and I've got hardware firewalls isolating because I just don't trust anything.  I mean, I have RAID 6 on SSDs with battery backup.  I mean, you cannot take it down.  So, I mean, a DDoS will.  But it's like, okay, again, people were like, "Oh, my god, you're under attack?  Move to Cloudflare."  And it's like...



LEO:  Agh, my hair's on fire, my hair's on fire.



STEVE:  Yeah.  So it's like, okay, let's just keep a sense of perspective.  So, yeah.  And when my bandwidth has a problem I, you know, like I said, hit the beach.  It's some nice-looking beach weather right now.



LEO:  Incidentally, we had a great conversation yesterday on Triangulation.  I encourage everybody who listens to Security Now!, I think you'll enjoy it.  Marc Rogers joined us.  He's a legendary hacker.  But also SecOps, he's in charge of security operations at DEFCON, has been for 15 years.



STEVE:  Nice.



LEO:  That's a pretty good street cred.  And he's the security guy at Cloudflare.  So he came on.  We talked a little bit about Cloudflare, Cloudbleed, and how they handled that.  But we also talked about his hacking history.  And it was a great conversation.  I think you'd enjoy it, too, Steve.  One of the things he said, we were talking about NSA surveillance, and I said, "Well, wouldn't the NSA have every outbound call to Russia that anybody made from anywhere during the election?"  And he said, "Well, they might have collected it, but they don't have the capacity to store everything."



He said there's no doubt they collect everything.  He used to work at Vodafone, at British Telecom, for 10 years.  So he kind of has a knowledge of what is going into the government databases.  He said, you've got to understand, there's so much data, even with this new Utah datacenter, he said, they might have a few weeks or a month, but they can't store stuff forever.  They have to pick and choose and say, well, we'll save this stuff, and this stuff's going to get overwritten.



STEVE:  Right.  And in fact we know from the early Snowden revelations, where we were looking at the nature of the equipment installed in those closets in the major Internet interchange nodes, that they are, in the case of text, they are keyword search sorts of things.  So you put filters on certain traffic and say, you know, capture streams that contain the following.  So I wouldn't be at all surprised if there is speech recognition stuff at some level where it's like, okay, just look for these words in a conversation.  And if you see them, then that we want to store.  Otherwise, we don't have infinite storage.



LEO:  Yeah.  He was good.  He also is a car hacker and talked about Tesla and hacking the Tesla.  This guy is, I think, pretty good at what he does.  So recommend that, yesterday's Triangulation episode.



STEVE:  Nice.  So, okay.  And then some other people, responding to our discussion of Cloudflare last week, one in particular, I just found one, I just pulled one tweet out of many from someone who called himself Amateur, who said:  "@SGgrc Did you read the same response I did?  Cloudflare," he writes, "totally blamed what they called 'ancient software' and acted like this didn't matter."  And I got a number of responses like that, which caused me to think, okay, did I get that wrong?  And then I realized that what I was relying upon much more was the clear responsibility-taking discussion that Nick Sullivan had with you, Leo, on The New Screen Savers, where he was completely forthcoming.



And what I saw of Cloudflare's written disclosure, which was written also by our friend John Graham-Cumming, it appeared to be consistent with what Nick had said.  But I imagine that someone reading only a more carefully written corporate post might come away with a different feeling than someone, as I did, watching a completely candid, off-the-cuff interview where he was responding to your questions and saying, yeah, you know, this is what happened.  So if anyone still has any doubts, let me encourage you to watch Nick at the beginning of The New Screen Savers from Saturday before last, that was February 25th, Episode 93.  I put a link to it in the show notes.  Your interview with Nick, Leo, is right at the top of the show so people won't have to dig down into it to find it, if that's the only thing they want to watch.  He's right there at the beginning.



And anyone, I think, would come away with the feeling I did, which was they responded immediately.  They responded responsibly and really did take responsibility for their use of this software which was actually - and where the problem was triggered by their on-the-fly replacement of that so-called "ancient software."  So anyway, many people were like, wait a minute, is that the proper characterization?  And the only thing I could think is that I got it directly from Nick from watching his interview with you.



LEO:  And I think you'd say the same thing if you saw Marc's conversation yesterday with me.  The one question I had, and it's still an open question, is, well, but do we know, was any damage done?  How much damage was done?  Because that's the thing that's kind of unknowable.  We know that data is leaked.



STEVE:  Correct.



LEO:  But we don't know what data, for who, for what sites.  And he said, "We would expect certain kinds of traffic based on somebody getting a hold of any information that was useful.  We haven't seen any evidence of that."  So while you can't say everything's fine, we're obviously still watching.  But so far, so good.



STEVE:  Right.  And some listeners did point out that there are other spidery things than just the arguably responsible search engines.



LEO:  Right.  Well, that's true.



STEVE:  Which is true.  Again, they would have to have known to look or have discovered this and then gone on a mission for trying to find information.  And it isn't something where you're attacking a given site.  It's opportunistic, completely out of your control what it is you may get.  Low probability of getting anything useful.  And then you would have to understand what it was.



LEO:  Which is why a government is not going to be as interested as you might think because they have targets.



STEVE:  Right. 



LEO:  And so it's useless for that.  The chances of you or me having information revealed is infinitesimally small.



STEVE:  Right.  And in fact we also do know that there's been a lot of dialogue in the last few days after our President tweeted on Saturday morning that Barack Obama was wiretapping Trump Tower.  The fact is that, even if there were conversations involving non-U.S. citizens, as soon as any wiretap realizes there is a U.S. citizen on the line, the rules change.  So, yeah, I just don't think it's nearly - my point is that, if our government were to find information that they can't have, well, they're not able to look at it, either.



LEO:  And you can't really use it in court, either.



STEVE:  Well, exactly, exactly.  So we've discussed - we revisited last week the pending question of Amazon's response to the police warrant in the suspected murder, well, somebody was at least killed, we don't know who did it, but of this person in the hot tub, remember, from November of 2015.  I thought it was 2016.  It may have been...



LEO:  No, no, it was two years ago, yeah.



STEVE:  Okay.  So what happened was we're not going to get any judicial opinion about search warrants covering past audio obtained by and queries served by Amazon's Voice Assistant technology.  In documents which were filed last Monday, which we didn't know about for last Tuesday's podcast, but they just came to light, the defendant in that case, James Bates, said that he was willing to allow law enforcement officials to review information contained on his Amazon Echo speaker before the company handed the data over on Friday.  So he's pleaded non-guilty.  He said, "Fine, I have nothing to hide.  Amazon has my  permission to turn over anything that they have."



So it would have been interesting because Amazon, of course, is big enough to fight this with enough strength that this might have ended up being decided by a court, maybe ultimately by the Supreme Court.  We don't know.  But it's not going to be this case that determines that because in this case they just essentially agreed to say, yeah, okay, fine, let them have what they want.



A group of security researchers who are with the Fraunhofer Institute for Secure Information Technology - their handle is TeamSIK, S-I-K, which sounds a little hacker-ish, but it's actually SIK stands for Security Is Key.  And these are obviously the real deal.  They wrote:  "We meet up regularly in our spare time.  Our main motivation is to work on interesting security-related projects for fun, with the goal of exposing security issues.  Currently," they write, "our main issues of interest are Android applications."



Well, this was in the news last week because they examined carefully and closely, in order to obtain the level of detail they did, the top nine password managers on Android.  And those were:  MyPasswords, Informaticore Password Manager, LastPass Password Manager, Keeper Password Manager, the F-Secure KEY Password Manager, Dashlane, Hide Pictures Keep Safe Vault, Avast Passwords, and 1Password.  So, and the way they described this, like what they came away with was interesting.



They said:  "There are different policies for the generation of secure passwords."  They give us a little bit of background on password managers.  "However, one of the biggest challenges is to memorize all these complex passwords.  Password manager applications are a promising way of storing all sensitive passwords cryptographically secure.  Accessing these passwords is only possible if the user enters a secret master password.



"At first sight, the requirements for a password manager application seem simple:  storing the passwords of a user centralized in a secure and confidential way.  However, how is the reality on mobile password manager applications, especially on Android?  Applications vendors advertise their password manager applications as 'bank-level' or 'military-grade' secure.  However, can users be sure that their secrets are actually stored securely?  Despite the vendors' claims, is it nevertheless possible to obtain access to the stored credentials?



"In order to answer these questions, we performed a security analysis on the most popular Android password manager applications from the Google Play Store based on download count.  The overall results were extremely worrying and revealed that password manager applications, despite their claims, do not provide enough protection mechanisms for the stored passwords and credentials.  Instead, they abuse the users' confidence and expose them to high risks.



"We found several implementation flaws resulting in serious security vulnerabilities.  Some applications stored the entered master password in plaintext, or implemented hard-coded crypto keys in the program code.  Consequently, attackers can easily" - now, that's a little questionable because I looked at this carefully also.  But they say "attackers can easily circumvent the crypto algorithm" - so I would say "can" circumvent the crypto algorithm - "altogether and thereby gain access to all of the user's data.  In other cases, we could simply access all 'securely protected passwords and credentials' with the help of an additional app.  Once installed on the device, this malicious app extracts all passwords and credentials in plaintext and sends them to the attacker.



"In yet another case, we could use a so-called data residue attack to access the master key of an application.  In most of the cases, no root permissions were required for a successful attack that gave us access to sensitive information such as the aforementioned master password.  Furthermore, many of the apps completely ignore the problem of clipboard sniffing, meaning that there is no cleanup of the clipboard after credentials have been copied into it.



"While this shows that even the most basic functions of a password manager are often vulnerable, these apps also provide additional features which can, again, affect security.  We found that, for example, auto-fill functions for applications could be abused to steal the stored secrets of the password manager" - and we've been talking about form auto-fill hacks and attacks in the last couple podcasts - "using hidden phishing attacks."



So they end up in nine password managers, finding 26 different vulnerabilities.  All of the ones they disclosed have been fixed beforehand.  Avast is the only one that had, first of all, many problems, many more than any of the others.  And about half of them remained unfixed.  And I should mention that this all happened back in August and September of last year.  So there's been plenty of time for these to get fixed.  All of the other password managers, including Last Pass, have had those problems which - so this was all responsibly disclosed and responsibly handled.



In the case of LastPass, which I was most curious about, they write:  "The Android app of LastPass comes with an access control mechanism which prohibits arbitrary usage of its functionality.  By default, the user is asked to enter his master password in order to gain access to the application.  Due to its enforced complex requirements" - meaning LastPass Android requires a complex master password - "a user can easily get frustrated entering the master password over and over again.  Therefore, LastPass offers to substitute the master password with a PIN mechanism."  So here, again, we're trading security for convenience.  "Thereby the user can agree to save the master password on the device" - because remember, the master password is what decrypts the store, that is, the storage blob, so you still need the master password somewhere.



So LastPass offers to substitute the master password for a PIN.  "Thereby the user can agree to save the master password on the device and shift all access control to a PIN, which can be from 4 to 14 digits long.  The master key and the PIN are symmetrically encrypted and stored in a shared preferences file in the local app folder.  The key and PIN are stored encrypted.  The key for encrypting and decrypting the credentials is hard-coded into the application's source code."  Thus the problem, and that's what LastPass - that's what these guys found.  And LastPass said, okay, you're right, we could do better.  So the key for encrypting and decrypting the credentials was found to be hard-coded into the application's source code.



However, even so, for stealing the encrypted master key or PIN, we assume, and that is they required, that the attacker gained physical access to the device, which we're now calling the Evil Maid attack, and in which case that approach required reading out the LPAndroid.xml content, which did not require a root exploit.  But not on each Android version.  So only on some was this possible.  The second approach did require a root exploit, which exists for various types of Androids, depending upon version.



So it wasn't, again, so I wouldn't call that easy.  That required either physical access or a root exploit that then allowed you to get it.  But even so, LastPass could have done better.  And before this was made public, actually quite some time ago, this got fixed.  And so I only looked at that level of detail at LastPass.  But I've got all the links in the show notes for anyone who might be using - for example, I know 1Password was also popular.  They had a similar - there were three problems that all sort of circled around this for LastPass, three different discrete vulnerabilities.  I think 1Password had the same.  And again, they were all fixed.



So I just say props to these guys for taking a careful close look at these.  And it really looks like what we're seeing is examples where the manufacturers do what they think is, you know, they're trying to implement the best security they can, while also keeping their devices sufficiently easy to use.  And even so, a third party is still super useful to, essentially, to challenge those assumptions and say, okay, this could be better over here; and then for the original developer to say, uh, yeah, you're right.



And of course I have the perspective of being working on SQRL.  And so whenever I see any of these things, I challenge myself, too.  I go, okay, wait a minute.  How have I handled that?  And because I have some of the same sorts of tradeoffs that, for example, in my implementation of SQRL, although this is not part of the protocol, this is part of the how you should do it right, I have handled all of this in a way both with entropy, harvesting, and solving the problem of needing something very complex as a get-out-of-jail-free card that we call a "rescue code," yet needing something interactively simple; yet at the same time not leaving debris around where, if someone got a snapshot of it, they would get any substantial benefit.  So, I mean, these are hard problems to solve, and all you can do is the best possible.  In some cases, what is being seen is that manufacturers or software developers have not done everything they possible could.



We discussed a few months ago a file-free malware that was - in fact, it was Kaspersky who actually discovered this in their servers, which their scanners weren't finding because they were scanning the hard drive.  They were not looking in RAM.  And so they discovered a RAM-resident malware, which was the first time that we had seen that.  Well, we've now found another one.  Cisco's Talos security research group discovered a new and relatively sophisticated attack which launches from an email-phishing malicious Word document and successfully executes a Windows PowerShell backdoor by communicating with command-and-control servers through a series of - get this - DNS requests.



So we know about DNS.  You ask for a machine dot domain name, and you receive whatever is in this DNS record.  But you're able to ask, for example, you could do a forward lookup, that is, give me the IP address of this machine and domain.  Or you could do a reverse-lookup.  You can also ask for other kinds of records, rather than just an address record.  And in this case the system uses text records, TXT records, which is another type of thing you can store in the database.  And they're increasingly used for various things.  The SPF, Sender Provider Framework, which is used as an antispam system, uses DNS TXT records.  So whereas originally they were rare, they're becoming quite common now.



So Cisco's dump of this work that they did was called "Covert Channels and Poor Decisions:  The Tale of [what they're calling] DNSMessenger."  And they go through it in detail.  I won't take us all through it.  I do have a lot more detail in the show notes.  But essentially they're using a VB script that runs from Word, which a user is tricked into opening, in order to use PowerShell, which Microsoft has continued to make more and more powerful and capable.  Basically, it's full scripting of Windows admin tasks.  Anything you could imagine wanting to do, you can now do in PowerShell.  It's multilevel, hierarchical, object-oriented, insanely powerful.



And what's tricky about this is that this avoids antivirus.  It gets past currently the majority of AV because they're not looking for this.  And it uses DNS, which has generally been regarded as safe.  And the malicious content is obtained over the 'Net, through DNS TXT records.  So there's nothing malicious in the code that just sort of benignly says, oh, go fetch me a TXT record at this domain.  So that's something you can easily do from PowerShell.  Unfortunately, what it receives is a blob which, when decrypted, does become malicious.  And it turns out that there is a way, then, using PowerShell, to create persistence through the registry because you're able to create a backdoor using Windows Management Instrumentation, the WMI database.  So that allows you to create something persistent which can survive across reboots.  So no actual files written by the malware; yet, once this thing gets into your system, if you make the mistake of opening the document, your system can be infected just over DNS.  Wow.  The bad guys are clever.



LEO:  But reboot would fix it; right?



STEVE:  No, it survives reboot.



LEO:  Well, that's not nice.



STEVE:  No.  No.  No, and it's...



LEO:  So it must write something out.  How does it survive reboot?



STEVE:  It's in the registry.  It uses the Windows Management Instrumentation. 



LEO:  Ah.  So it doesn't have a file, but it does store itself on the file system.



STEVE:  Correct.  Ultimately it does, right.



LEO:  Well, they're just getting so clever.  I've seen - but that's what Mirai was.  It was RAM-based only; right?  Because if you rebooted the router, Mirai was wiped.



STEVE:  Yes.  So Chrome has further tightened up its security on macOS, actually bringing it to parity with some features that already existed in Chrome for Windows.  Chrome has their safe browsing initiative.  In this case it's broadening its protection of macOS devices, enabling safer browsing experiences by removing defenses against unwanted software and malware which targets macOS.  So as a result, now and in future, macOS users using Chrome will start to see more warnings as they navigate to dangerous sites or download dangerous files.



As part of this next step towards reducing macOS-specific malware and unwanted software, which we've begun to see a little bit of, safe browsing is focusing on two common abuses of browsing experiences:  unwanted ad injection, and manipulation of Chrome's user settings, specifically the start page, the home page, and the default search engine.  Of course, Windows users have fought with changes to their browser for a long time.  Google feels and believes users deserve full control of their browsing experience, and unwanted software policy violations hurt that experience.



So what they've done is they've created an API to give extensions control of the Chrome settings, yet to not allow this to happen behind the scenes.  So confirmation boxes have been added that users will start seeing to essentially confirm and prompt to allow changes to be made where those things previously could just be made behind the scenes.  And that will - actually, it's not already.  I misspoke.  It's starting with March 31st.  So the end of this month, Chrome and safe browsing will warn users about software that attempts to modify Chrome settings without permission of the user.  So that's, again, sort of marching forward a little bit to make sure that not too much changes at once, and people are not inconvenienced.



And similarly, from Mozilla on the Firefox front, they have moved their Containers from their very early testing over to the Test Pilot versions.  About a year ago, last summer, Tanvi Vyas, who is a security engineer, posted about contextual identities on the web.  And what she wrote was perfect as sort of to give us an understanding of what they're talking about, what these Containers are.



She wrote:  "The Containers feature in Firefox Nightly enables users to log into multiple accounts" - okay.  So users log in to multiple, like, Internet web accounts - "on the same site simultaneously and gives users the ability to segregate site data for improved privacy and security."



She writes:  "We all portray different characteristics of ourselves in different situations.  The way I speak with my son is much different than the way I communicate with my coworkers.  The things I tell my friends are different from what I tell my parents.  I'm much more guarded when withdrawing money from the bank than I am when shopping at the grocery store.  I have the ability to use multiple identities in multiple contexts.  But when I use the web, I can't do that very well.  There is no easy way to segregate my identities such that my browsing behavior while shopping for toddler clothes doesn't cross over to my browser behavior while working.  The Containers feature I'm about to describe attempts to solve this problem:  empowering Firefox to help segregate online identities in the same way I can segregate my real life identities.



"With Containers, users can open tabs in multiple different contexts:  the personal context, the work context, the banking context, and the shopping context.  Each context has a fully segregated cookie jar, meaning that the cookies, indexedDB, localStorage, and cache that sites have access to in the Work Container are completely different" - disjoint, separate - "than they are in the Personal Container.  That means that the user can log into their work Twitter account on Twitter.com in their Work Container, and also log into their personal Twitter on Twitter.com in their Personal Container.  The user can use both accounts side-by-side in tabs, simultaneously.  The user won't need to use multiple browsers, an account switcher, or constantly log in and out to switch between accounts on the same domain.



"Note that the inability to efficiently use contextual identities on the web has been discussed for many years.  The hard part about this problem is figuring out the right user experience and answering questions like:  How will users know what context they are operating in?  What if the user makes a mistake and uses the wrong context?  Can the user recover?  Can the browser assist by automatically assigning websites to Containers so that users don't have to manually manage their identities by themselves?  What heuristics should the browser use for such assignments?



"We don't have answers to all of these questions yet, but hope to start uncovering some of them with user research and feedback.  The Containers implementation in Nightly Firefox is a basic implementation that allows the user to manage identities with a minimal user interface."



So, and here is where I loved what they did.  "What is the security model provided by Containers?"  And this is actually in a recent post, in the last couple days, where this was part of the Test Pilot.  They wrote:  "The security enhancements of Containers in Nightly and Test Pilot is common across both versions and are based on a modification to the browser's Same-Origin Policy (SOP)."  Okay, now, as we know on the podcast, the Same-Origin Policy, which we talk about often because it's such, I mean, you could argue it's the most crucial security operational guarantee that browsers enforce.  Which ensures that documents and data, and that includes queries made by script running on a page from distinct origins, meaning like domains, are isolated from each other.  It is a critical browser security mechanism, as we know, that prevents content from one site from being read or altered by another, potentially malicious, site.  



So getting a little more specific:  A page's origin is defined by the tuple composed of the scheme - that's typically http or https, but could also be smb:// or ftp: or whatever.  So that's the scheme.  That's the first part of the tuple.  The host, which is the name www.grc.com, for example, and the port, so all three of those - the scheme, the host, and the port.  In the case of https it's typically 443.  But as we know, the URL can have a colon and a port override, so those are all bound together.  And that collection, that tuple, defines the origin.  So a page has that origin made from that tuple, which means that only the things on the page - the things on a page are only able to access content at the same origin, thus the same-origin policy.



So what Mozilla has cleverly done is to add an additional parameter, which they call the userContextID integer, to the normal scheme, host, and port.  So now it's userContextID, scheme, host, and port.  So what's so clever about this is they get to reuse all of the existing containment enforcement that is core in the browser in order to create essentially contextual instances.  When a user visits Gmail in a Work Container tab, the browser performs the same-origin policy check against, for example, 2 - two as in the userContextID - https, mail.google.com, 443.  That's the tuple that defines the same origin, or that defines the policy and the origin for that page.  Then when the user visits Gmail in a Personal Container tab, that tuple would be 1, https, and then mail.google.com and 443.  So even though it's technically the same site, the addition of this integer to the tuple allows the browser to segregate those sessions, essentially.



So those containers which are implemented with this additional integer index, separate all of the things that matter - the cookies, the local storage, the indexedDB, and the cache data - from each other and from the default container in Firefox.  So that is now going to be - that's now in the Test Pilot version.  And it's been in Nightly now for a while.  It's instrumented.  They're watching to see how people use it.  So it's sort of an advanced and persistent version of Incognito, where rather than not - where Incognito mode typically works by creating a "never write anything to disk, keep everything in RAM," so that when we flush, it gets completely flushed.  This essentially solves the problem that many of us have solved by using different browsers because at this point everything in a browser is global to the browser.  Which is convenient when you want to have, like multiple tabs to the same site, and you stay logged on.  It's inconvenient when you, for whatever reason, do want to appear as someone different.



And I'll note also that this is an issue that we've looked at, again, in the SQRL context.  We've had extensive similar discussions because one of the things that SQRL does is give you a fixed identity for a website.  But then the question is, okay, but what if I, for whatever reason, don't want to be me at that website at the moment?  And so we've incorporated that into the design, and we call them "alternative IDs," where it's trivial to create an alternative ID which has all of the properties SQRL is known for - no trackability, no way to, I mean, essentially,  you look like an absolutely unconnected, completely different person form the standpoint of the SQRL identity.



And so if you coupled that, for example, with this kind of container management, it solves the whole problem.  You could be logged in as two different people, both through your single SQRL identity, yet be using an alternative, and just jump around with containers.  The way they've implemented this is under the file tab there is, like, Open New Window, Open New Tab, Open Incognito Session.  They've added the ability to access containers, to define or open containers.  And they show you which one you're in by color-coding the container's name and right-justifying it in the URL region so you can always see if it's your work, your personal, or whatever.  So nice movement for Firefox.  I'm glad to see that.  And looks like it's going to be convenient.



At last month's RSA 2017 conference, the team of security researchers from Cylance gave a live demo of Cylance's UEFI Ransomware proof of concept.  So that's, like, as bad as it gets, the idea that something that you could get over the Internet could break through all of the multiple levels of security we have and alter the firmware on your motherboard to create persistent malware.  In the demo that they showed, they successfully exploited a system with an Intel Skylake CPU running Microsoft Windows 10 Enterprise, that's build 1607, with all updates installed.  They enabled all available security features in the system, including Secure Boot; Virtual Secure Mode, that's VSM; and the Device Guard under its default policy.  They, like, turned everything on.



Despite all of those protections and mitigations, based on the various virtualization technologies in Windows 10, specifically designed to prevent this from happening, the systems remained vulnerable to UEFI-based or UEFI-targeted attacks through system management mode.  We talked about this about a year ago, how it was possible to use system management mode in order to bypass some of the protections.  Because system management mode allows direct access to physical memory, it's possible to bypass the virtualization layer of isolation which has been put in there for protection, which allows an unprivileged application to install persistent malware like ransomware, which they have done in their demo, invisibly into a motherboard's firmware.



So at the end of this month, at the upcoming Black Hat in Asia, which will be occurring in Singapore, they will be going - what they did last month was give a full live presentation demo.  At the end of this month, they'll be disclosing all of the details of the vulnerabilities they exploited in their talk last month.  So the bad news is, I mean, again, here's another perfect example of attacks never get weaker, they only get better and stronger.  And so they have taken the concept and brought it to a full live working demo that manages to cut through all of the mitigations and protections which have been put in place to prevent that.



We've talked, and were just talking last week, in fact, about return-oriented programming, where it's possible to use snippets from the end of, or typically the tails, of existing kernel-privileged code to make things work.  The details are, well, we've talked about it enough that there's really no need to go into any deeper level of detail.  But one of our listeners sent me a link and said, hey, you know, what do you think of this?  I've got it in the show notes.  It is on GitHub.io, called Gargoyle, G-A-R-G-O-Y-L-E, a memory scanning evasion technique.  And what's nice about this is it is a beautiful, extremely clear, yet very technical, I mean, like to the level we don't need to do on this podcast.  But I imagine some of our listeners would be interested.



So it's a very nicely written up, detailed example of a practical working ROP, Return-Oriented Programming, exploit on Windows that demonstrates how an attacker can arrange to execute pure non-executable data.  And so this person, again, we've already in the past years discussed this in many different instances and contexts.  So there's no need to dig in any deeper.  But I thought, hey, this is a perfect example, if somebody really wants to go real-world.  I mean, there is proof-of-concept code in this posting.  You can do it yourself.  You can make a dialog box pop up from something that is just data.  So the person who put this together pushed it all the way to here's how this ROP, Return-Oriented Programming, works in practice.  So if anyone's interested, I refer them to this Gargoyle on GitHub.io, and the link is in the show notes.  



So an interesting site.  And Leo, you probably want to put TWiT into this.



LEO:  Okay.



STEVE:  It's URLScan.io.  It's in beta.  U-R-L-S-C-A-N dot I-O.  And when you just go there, you will see, as many of these sites now do, SSL Labs does the same thing, showing you the most recent domains that have been scanned.  This will show you the most recent things it's seen.  And what it does is it's very convenient because you can give it a URL, and it will go, pull the page, and then summarize in aggregate what the page did - how many domains it reached out to, how many requests it made, how many ads were blocked, if there was any malicious content that it tried to fetch, what percentage of it was secure versus insecure.  Was any IPv6 pulled?  What about subdomains?  What about discrete IPs?  What about what countries were fetched from, the size of the content, how many cookies and so forth.



So it really, I mean, it formats it beautifully.  You can drill down.  You can get additional detail.  Those words across the top in green, those are tabs.  So you can look at the various tabs to drill into various details.  So I imagine it's something that our users will get a big kick out of:  URLScan.io.  And I did it for CNN, and also for GRC.  As I said, GRC was, just because it's a small site that really doesn't refer to any other third parties, it's pretty small.  And there you've got yours for TWiT up on the screen, Leo.



LEO:  Yeah, but I didn't realize we were doing so much stuff.



STEVE:  Yeah, it's very revealing.



LEO:  Yeah.  A lot of this is Amazon load balancing.  So it says, for instance, we're pulling from three different countries.  Well, I don't know why.  Canada, the U.S., and it looks like the U.K.  But that's probably Amazon; right?  I don't, you know, I don't - geez.



STEVE:  Yeah.



LEO:  Fifteen ads blocked is pretty amazing since there's only two ad blocks on the whole page.  I don't know what they're talking about there.  Interesting.



STEVE:  Can you click on it?



LEO:  No, it's not live.



STEVE:  How about the word...



LEO:  No, that's HTTP.  I don't know - you can't click on this.  I'm not sure what they're getting about ads blocked.  But I'm sure it makes sense.



STEVE:  Another cool site.



LEO:  Yeah, really neat.  URLScan.io.



STEVE:  So Matthew Green answered a question which followed from a tweet that BuzzFeed's editor Miriam Elder posted.  She tweeted, on March 3rd:  "Possibly stupid question.  Is the Signal desktop client as secure as the mobile app?"  And so Matthew wrote:  "No, this is not a stupid question.  Actually, it's an extremely important question; and, judging by some of the responses to this tweet" - that is, to her tweet - "there are a lot of people who are confused about the answer."



He writes:  "Since I couldn't find a perfect layman's reference anywhere else, I'm going to devote this post to providing the world's simplest explanation of why, in the threat model of your typical journalist, your desktop machine is not very safe; and, specifically, why you're safer using a modern mobile device  and particularly an iOS device  than just about any other platform."



He writes:  "A brief caveat:  I'm a cryptographer, not a software security researcher.  However, I've spent the past several years interacting with many folks who live in this space.  I'm pretty confident that they would agree with this advice."



"So what's wrong with my laptop/desktop machine?  Sadly," he writes, "most of the problem is you.  If you're like most journalists  and really most professionals  you spend less than 100% of your time thinking about security.  You need to get work done.  When you're procrastinating from work, you visit funny sites your friends link you to on Facebook.  Then you check your email.  If you're a normal and productive user, you probably do a combination of all these things every few minutes, all of which culminates in your downloading some email attachment and (shudder) opening it in Word."  Which of course were just talking about, how that could easily contain an exploit that would infect your system.



And then he cites an older post from someone named Eva from September 12, 2016, saying:  "You can't tell journalists 'just don't open attachments.'  They will ignore you.  Journos open attachments from strangers for a living."



And so he continues:  "Now, I'm not trying to shame you for this.  It's perfectly normal, and indeed it's necessary if you want to get things done.  But in the parlance of security professionals, it also means you have a huge attack surface.  In English, this means that, from the perspective of an attacker, there are many different avenues through which to compromise your machine.  Many of these aren't even that sophisticated.  Often it's just a matter of catching you during an unguarded moment and convincing you to download an executable file or an infected Office document.  A compromised machine means that every piece of software on that machine is also vulnerable."  Okay, that was key:  "A compromised machine means that every piece of software on that machine is also vulnerable.



"If you don't believe this works, head over to Google and search for 'Remote Access Trojans.'  There's an entire commercial market for these products, each of which allows you to remotely control someone else's computer.  These off-the-shelf products aren't very sophisticated.  Indeed, most require you to trick your victim into downloading and running some executable attachment.  Sadly, this works on most people just fine.  And this is just the retail stuff.  Imagine what a modestly sophisticated attacker can do."



So then he asks himself the question, or has this hypothetical journalist saying, "I do some of those things on my phone, as well.  Why is a phone better?"  He writes:  "Classical desktop and laptop operating systems" - and I love what he wrote here.  I thought this was very insightful - "were designed primarily to support application developers.  This means they offer a lot of power to your applications.  An application like Microsoft Word can typically read and write all the files available to your account.  If Word becomes compromised, this is usually enough to pwn you, in practice.  And in many cases, these applications have components with root or admin access, which makes them even more dangerous.



"Modern phone operating systems like Android and iOS were built on a different principle.  Rather than trusting apps with much power, each app runs in a sandbox that mainly limits it to accessing its own files.  If the sandbox works, even a malicious application should not be able to reach out to touch other apps' files or permanently modify your system.  This approach, combined with other protections such as in-memory code signing, hardware secret storage, and routine use of anti-exploitation measures, makes your system" - meaning your mobile operating system - "vastly harder to compromise."  He wrote "vastly harder to compromise."



"Of course, sandboxing isn't perfect.  A compromised or malicious app can always access its own files.  More sophisticated exploits can break out of the sandbox, typically by exploiting a vulnerability in the operating system.  Such vulnerabilities are routinely discovered and occasionally exploited.  The defense to this is twofold:  First, run a modern, up-to-date OS that receives security patches quickly; and, two, avoid downloading malicious apps.  Which brings me to the main point of this post:  Why use iOS?"



He writes:  "The fact of the matter is that, when it comes to addressing these remaining issues, Apple phone operating systems on iPhones and iPads simply have a better track record.  Since Apple is the only manufacturer of iOS devices, there is no middleman when it comes to monitoring for iOS issues and deploying iOS security updates.  This means that the buck stops at Apple, rather than with some third-party equipment manufacturer.  Indeed, Apple routinely patches its operating systems and pushes the patches to all supported users, sometimes within hours of learning of a vulnerability, something," he says, "that is relatively rare at this point in any case,"  meaning vulnerabilities.



"Of course, to be fair, Google has also become fairly decent at supporting its own Android devices.  However, to get assurance from this process you need to be running a relatively brand new device, and it needs to be manufactured by Google.  Otherwise, you're liable to be several days or weeks behind the time when a security issue is discovered and patched  if you ever get it. And Google still does not support all of the features Apple does, including in-memory code signing and strong encryption.



"Apple also seems to do a relatively decent job at curating its App Store, at least as compared to Google.  And because those apps support a more modern base of phones, they tend to have access to better security features; whereas Android apps more routinely get caught doing dumb stuff for backward-compatibility reasons.



"Finally, every recent Apple device, starting with the iPhone 5S and up, also includes a specialized chip known as a Secure Enclave Processor.  This hardened processor assists in securing the boot chain, ensuring that nobody can tamper with your operating system.  It can also protect sensitive values like your passwords, ensuring that only a password or fingerprint can access them.  A few Android phones also offer similar features, as well.  However, it's unclear how these devices are implemented in contrast to Apple's Secure Enclave Processor.  It's not a bet I would choose to take."



And finally he says:  "So does using iOS mean I'm perfectly safe?"   He says:  "Of course not.  Unfortunately, computer security today is about resisting attacks.  We simply don't quite know how to prevent them altogether.  Indeed, well-funded attackers like governments are still capable of compromising your iOS device, and your Android, and your PC or Mac.  Literally the only question is how much they'll have to spend doing it.



"Here's one data point.  Last year a human rights activist in the UAE was targeted via a powerful zero-day exploit, likely by his government.  However, he was careful."  And we covered it on this podcast.  "Instead of clicking the link he was sent, the activist sent it to the engineers at Citizen Lab, who reverse-engineered the exploit.  The resulting 35-page technical report by Lookout Security and Citizen Lab is a thing of terrifying beauty," writes Matthew.  "It describes a chain of no less than three previously unpublished software exploits, which together would have led to the complete compromise of the victim's phone" - and I'll add, had he simply clicked the link in that text message he received.



"But such compromises," Matthew writes, "don't come cheap.  It's easy to see this kind of attack costing a million dollars or more.  This is probably orders of magnitude more than it would cost to compromise the typical desktop user.  That's important. Not perfect, but important."



So, finally, "You're telling me I have to give up my desktop machine?  Not at all.  Or rather, while I'd love to tell you that, I understand this may not be realistic for most users.  All I am telling you to do is to be thoughtful.  If you're working on something sensitive, consider moving the majority of that work and communications to a secure device until you're ready to share it."



And I'll just mention, I've said on this podcast before, there are instances where I receive things on my desktop that I will not click.  I will not open them.  I truly wait, or go to an iPad, and I do them there.  Because I already have internalized and understand exactly what Matthew is saying here.  And he says:  "This may be a bit of a hassle, but it doesn't have to be your whole life.  And since most of us already carry some sort of phone or tablet in addition to our regular work computer, hopefully this won't require too much of a change in your life.  You can still use your normal computer just fine, as long as you're aware of the relative risks.  That's all I'm trying to accomplish with this post.



"In conclusion, I expect that many technical people will find this post objectionable" - well, I don't - "largely because they assume that, with their expertise and care" - see, okay, yeah - "they can make a desktop operating system work perfectly safely.  And maybe they can.  But that's not who this post is addressed to.  And of course this post still only scratches the surface of the problem.  There's still the problem of selecting the right applications for secure messaging - for example, Signal and WhatsApp - and finding a good secure application for note-taking and document collaboration and so on.  But hopefully," he concludes, "this post at least starts the discussion."



And I'll just add something that he didn't, but this is a topic we discuss all the time, is it is exactly those very limitations of our mobile OSes that we most chafe at.  I mean, iOS is more limiting than Android.  And so many people choose Android because it gives them more freedom, more flexibility.  It lets them have more power and do more what they want to.  iOS is too restrictive.  But iOS, as a result of being more restrictive, gives you more security.  It's the tradeoff.



And of course, as we know, our desktops are the least restrictive of all.  That's why I, even though I'm as expert as I could be on being careful, I don't know when I click a link what will happen.  And so if I don't have confidence in where that link came from or what it might do, I don't do it on my desktop.  Despite all of the protections that I have in place, I use something that I think is less exploitable.  And that's my iPad.  So I thought a really, really nice post, and I thank Matthew for taking the time to put that together.  I hope people who should see it will see it.  And I know that our listeners will appreciate it.  Basically, it backs up all of the policies and practice and experience that we've been talking about in this podcast for a long time.



LEO:  All right, Steve.  Are you caffeinated?



STEVE:  I'm recaffeinated, yes.  



LEO:  Yay.



STEVE:  As opposed to decaffeinated.  I don't want to be decaffeinated, but I want to be recaffeinated.  



LEO:  Recaffeinate.



STEVE:  So I got a kick out of this.  Matthew Clayton sent a tweet saying:  "Loved this detail from the New Yorker's piece on Russian cybercrime."  And he took a picture of a chunk of the newspaper, physical newspaper column.  And I liked, well, first of all - and he highlighted one chunk of it.  He said:  "Russian hackers accomplished a feat that Pentagon officials considered almost impossible:  breaching a classified network that wasn't even connected to the public Internet.  Apparently, Russian spies had supplied cheap thumb drives, stocked with viruses, to retail kiosks near NATO headquarters in Kabul, betting correctly that a U.S. serviceman or woman would buy one and insert it into a secure computer."



And of course some years ago we covered a similar sort of exploit where some security researchers just wanted to sort of see if people would plug unknown thumb drives, like found thumb drives, into their computer.



LEO:  Of course they will.  Of course they will.



STEVE:  Yes.  And so they, like, I don't know, got 10 of them and loaded them with some tracking malware, I mean, something benign because it was for research, but then scattered them in a parking lot.  Just sort of left them on the ground.  And sure enough, they got pinged by that device when some unwitting person who found it on the ground thought, ooh, I wonder what's on this?  Maybe there's something good here.  It's like, well, uh, no, not good.  But definitely something.  So, yikes.  So I thought that was very clever.  Basically, sell inexpensive thumb drives at retail, preloaded with malware, near to somewhere you hope to compromise.  And sure enough.



So many people asked, because I didn't say last week, when I was referring to monitoring my Internet connection and knowing that it was two out of three packets that were being dropped and so forth - and remember, you probably saw, Leo, because I emailed, I ended up emailing it to you and the gang, although I couldn't get it to you before the podcast because of the outage, a chart showing the packet loss history over time.



LEO:  Oh, yeah, wow.  It was, boy, whew.



STEVE:  Yeah, it was, like, not here.



LEO:  Not good, yeah.



STEVE:  Anyway, I have been using - so I just wanted to answer the question.  I've been using for years a piece of software called PingPlotter.  And there's PingPlotter Free, PingPlotter Standard, and PingPlotter Pro.  And unfortunately, they've been attempting to further leverage the success of their product.  It should not be as expensive as, I mean, for just a simple utility, it shouldn't cost what it does.  They try to sign you up on an annual contract.  But there is a PingPlotter Free.  It is very nice.  It's a utility that I use at PingPlotter.com.  And I recommend it, if you can find it for a price that makes sense to you.  I mean, the free version, certainly.



And what it does is it essentially, on a timed interval, it just, you know, it sends a ping to a destination.  And it also does a traceroute, so you're able to look at, by emitting packets with successively shorter time to lives, TTLs, the packets expire on their route, and that allows it to probe the path that you're going through and, like, find where is the problem where your traffic is dying.  So anyway, PingPlotter.com.  I like the utility a lot.  I wish you could get more for less money, but that's the way they do it.  And it's gotten more expensive over time.  I had Ping Plotter Pro that I bought a long time ago, full feature, and I purchased it for a reasonable price.  And now it's way more expensive.



Oh, and Richard Covington tweeted, he said, following up on our discussion, remember we were talking about covering the lens on your webcam versus the problem with doing it with a microphone.  And he had a good observation.  He writes:  "One can disable the  microphone on computers.  Just plug in the male" - Leo, your mic's open, by the way.



LEO:  Oh, sorry.



STEVE:  No problem.  "Just plug in the male mic plug with the wires tied together."  And that's like, oh, I hadn't thought of that.  If you have a computer that has a microphone jack, and you'll want to verify that this works, but that is an interesting idea is to use the hardware override of the built-in microphone to the external microphone to disconnect the hardware of the internal microphone.  You'd have to maybe get a little right-angle jack so it's not sticking out too far, or just the jack without any wires coming out of it.  And you might want them shorted.  He was suggesting shorting them so that you don't pick up any buzz or hum or anything because, if you short them, then it'll just like be grounding the microphone's input, and you'll get complete silence.  So good tip, Richard.  Thank you for that.



Also, Kevin Bryant actually cc'd me.  So he was tweeting out to the world, saying "GRC's ShieldsUP will only scan the first 1056 ports," as I've mentioned before, 1024 plus an extra chunk.  He says:  "Are there no one-click, scan-all-my-ports websites?"  And then he cc'd @SGgrc, which is how this came to my attention.  And I'll just say that this is something I considered a lot.  And I designed ShieldsUP back when it was behind my two T1s.  And scanning all the ports is a much bigger job.  In fact, it's a 64 times bigger job.  So I'm scanning 1024, plus a little bit.  But there are...



LEO:  You do the canonical ports.



STEVE:  Correct.  But he wants, Kevin is wanting to scan all 64K ports, all 65536.  And so we could definitely do it, but it's 65 times, or 64 times longer and more bandwidth and more work.  And I don't know what I'll do.  Maybe, eventually, I mean, once SQRL's finished, once SpinRite's behind me and brought back up to speed, it's probably time to look at ShieldsUP again.  Maybe switch to IPv6.  And then I don't know.  Maybe - I just don't know.  I mean, it feels like it's too much to do for free because it would be a substantial load for a free service.  So maybe...



LEO:  You could use Nmap and do it yourself.  I guess, no, it has to be outside your network, doesn't it, to be effective.



STEVE:  Right.



LEO:  But you could use Nmap from work to scan your home.  Or, no, I guess you couldn't because that's inbound.



STEVE:  Yeah.  And, I mean, there's a lot that I do that people take for granted, like I figure out closed, opened, and stealth.  There's a bunch of stuff I do about routers that have anti-DDoS technology in them.  And it just, you know, it's easy to use.  You just go to a website.  So maybe I would create a little subscription service, or a micropayment.  I just don't know.  But it is what it is for now.  You can ask it to scan any port you want.  That's all free.  Just not all of them at a single button.



And Simon Zerafa also sent me an interesting observation.  He retweeted somebody who found an "I am not a robot" CAPTCHA that was malicious because it was fake.  It was not from Google.  It was invoking a malicious script.  And we've talked often about how what you don't want to do is click on something malicious.  But what this person realized actually by encountering it, and Simon was nice enough to forward it, is that, if we get very comfortable with the "I'm not a robot" proof, like clicking on "I'm not a robot," nothing prevents that from being spoofed.  And where we're actually clicking on something that in this case activated a PowerShell payload.  And we were just talking earlier about how powerful PowerShell is.



So the problem is browsers are still not, you know, they're fighting this problem of trying to keep us aware of things that might hurt us without just popping up dialog boxes all over the place so that we just get trained to dismiss them without taking them seriously.  That's not a problem that seems to have a good solution at this point.



LEO:  I know it's not ShieldsUP, but I just ran Nmap on my own server, and it does at least go through and look at all the open and closed ports.  It doesn't show stealth.  Well, closed would be closed, but it doesn't show stealth ports, obviously.  But that's a good start, isn't it?



STEVE:  Yeah.



LEO:  That's my server at home.  Nmap.



STEVE:  And we talked about Nmap recently as a great local tool.



LEO:  Yeah.



STEVE:  Okay, Leo.  The coolest mind-blowing waste of time probably ever.  I'm not kidding.



LEO:  This must be good, wow.



STEVE:  I was a high-school freshman in October of 1970 when the October issue of Scientific American came out.  And in Martin Gardener's Mathematical Games column, he introduced the world to John Horton Conway's "Life," also known as "Conway's Game of Life."  What it was, was an incredibly simple cellular automata system.  And it wasn't the first, but it was arguably the best.  And so here's what that is.  You take a two-dimensional grid, like graph paper, which is obviously composed of cells.  And a cellular automata has rules that define the life and death, the binary state, the on-ness and off-ness, the living or not living state of individual cells on this grid.



And what was so perfect about what Conway did was he defined - and this was what is so fascinating about it.  Very few rules, just four rules, which were easy to know and memorize and implement, that made this thing, which was a machine, made it go.  So any live cell with fewer than two live neighbors, okay, so on a rectangular grid you're going to have a single cell will be surrounded by eight neighbors; right?  So there's eight cells, and those are called neighbors, neighboring cells.



Any live cell, any cell that's currently alive, with fewer than two live neighbors, dies because of loneliness or underpopulation.  Any live cell with two or three live neighbors lives on to the next generation.  So the next iteration.  So the way you think of it, you have the current state of the grid.  And then you go through and you, with that state, you determine what the next state will be by processing every cell to see whether it will be alive or dead in the next, based on these rules, in the next update.  Okay.  So any live cell with two or three neighbors lives on because it's got just enough to be - it's not going to die of loneliness or underpopulation.  It's just enough to stay happy.  But any live cell with more than three live neighbors dies from overpopulation.  And lastly, any dead cell with exactly three live neighbors becomes a live cell as if from reproduction.  So you create life if you have exactly three live neighbors.



So, okay.  So a couple simple cases.  Take a 2x2 - you have the whole grid clear - a 2x2 square of cells.  So two live cells next to each other, and then immediately below them two more live cells.  So each one of the live cells has its three neighbors.  So it gets to live.  Yet because in this configuration none of the dead cells around that block have exactly three neighbors, it's only when you have exactly three neighbors that you get to spontaneously come to life.  So that's called a "block" in the terminology, and it just sits there.  Nothing happens.  Generation after generation, there's no birth of cells around it, and they don't - and the ones that are alive, stay alive because they've got just the proper balance.



Now, take another simple example.  This is known as a "blinker."  It's three cells in a row.  So you've got a whole empty grid with three cells in a row that are on.  Okay, now, the cell in the middle of the three, it's got its two neighbors.  So it stays alive.  But the cells on the end, they've only got the one neighbor, that middle cell.  So that's not enough.  They die.  Looking at the cells surrounding this little set of three, only the two cells above the center and below the center have exactly three neighbors.  So that's the birth condition.



So what that does is that means in the next generation the cells on the end die, and the cells above and below the center come to life.  So in other words, this thing flicks back and forth.  It blinks between being a little horizontal line and a little vertical line.  Okay.  So there's the basics.  But when you start playing with this thing, oh, my goodness.  There are gliders which are able - they're a little five-cell configuration that move diagonally forever.  They just fly off the graph.  There are glider guns which are these amazing machines where these two things move back and forth and bounce into each other and, through some miracle, they emit a glider that then starts moving off the screen diagonally.



Oh, yeah, there's a glider gun.  And, oh, it's - there are space ships which move horizontally.  There are puffer-type breeders that move along, leaving glider guns in their wake.  And then the glider guns, of course, begin emitting gliders from them.  You have the notion of the speed of light because, of course, the speed of light is "C" in our universe, but it's an iteration.  So the speed of light is a grid step because, if you think about it, nothing in the Game of Life can move faster than one cell.  So then you rate the speed at which gliders glide and space - there's also space ships, I forgot to mention, space ships move in what fraction of "C" they're able to obtain.



Okay.  So with all of that background, some crazy person, I can't even imagine doing this, built a digital, a fully functional operating digital clock using this cellular automaton, so that the live cells generate the digits of the time of day.  And it ticks, and it has an a.m. and p.m. indicator.  And it's just like, I mean, when you understand how few and simple the rules are, and the idea that this is even possible, it is just - it's just mindboggling.  So again, I mean, it's got even colons.  And you can't have a colon.  A colon is an unstable body.  A colon will immediately, the entire center will go extinct.  I mean, I played with this enough as a kid that I can look at this, and I can tell you what will happen because this is the kind of upbringing I had.  But anyway...



LEO:  Just can't get mine to work.



STEVE:  And it's just an incredible piece of work.  Just incredible.  And I have to say, for anyone who's interested, you can find Life on every platform that has ever been created.  There was probably even a Nintendo GameBoy version.  Certainly you can get it on Android and on iOS and on Windows, on every platform.  And so if you're curious, just play with it, as Leo has been while I've been talking about it because you obviously found one on a web page; right?



LEO:  Yeah.  No, well, he actually links in his post on Stack Exchange for Code Golf, he links to a simulator that's a web-based simulator.  So you could paste his code into the JavaScript Conway Life Simulator.  Oh, I forgot to set the generation step to 512.  There we go.  I've clearly got some work to do.  Anyway, very cool.



STEVE:  Oh, and it's just a stunning piece of work.  Again, utterly useless, but fabulous.



LEO:  Yeah, I've talked to people who actually became researchers in AI because they started with Conway's Life.



STEVE:  Oh, Leo, it's just I mean, it's...



LEO:  So a lot of people did; right?



STEVE:  ...so elegant.  It's a simple set of rules.  And when you look at how rich the result is from those four simple rules...



LEO:  There we go, there we go.



STEVE:  ...that operate on a grid.  Look at that thing.



LEO:  This is insane.  I don't know what kind of amazing cognitive ability this guy has because...



STEVE:  I know.



LEO:  ...I don't know where you'd go to make this, or how you'd figure it out.  He doesn't publish the code, does he?  He just has the...



STEVE:  Oh, it's all - I think it's all there to be figured, yeah, to be reverse-engineered.



LEO:  Well, yeah.  There's a big binary blob, I mean, you know, of numbers.  Very cool. I mean, just amazing.  Just amazing.



STEVE:  Yeah.  And utterly useless.



LEO:  Yeah.  He probably spent years of his life.



STEVE:  But again, hats off.  Salute, you know?



LEO:  Very similar to writing, for instance, a general purpose Turing machine in Minecraft, you know, that kind of thing.  I've done that.



STEVE:  And Turing machines have been written in Game of Life, by the way.



LEO:  Oh, of course, of course, yeah.



STEVE:  And it is Turing complete.  You can solve any problem if you have enough little cells.  I mean, and back, you know, I was at the AI Lab at Stanford in '72 and '73, and everybody was implementing in PDP, what was it, PDP-10 code, like the fastest implementation.  I thought, some day I'll take an FPGA and do it in hardware.  It's just, oh, anyway.  It's just - it's a perfect piece.  And so for anyone who has never been exposed to it, now you have been.  I apologize for the loss of the next several weeks of your life.  If it hooks you, the hook could go deep because, boy, it's just an amazing recreation.  And in fact Conway had no computers at the time.  He did this on a Go board.



LEO:  Yes, exactly, yeah.  Totally cool.



STEVE:  So speaking of going, I got a tweet, I guess it was a DM because it looks long enough.  Jordan, @Rand0mByteZ, on March - and meanwhile your clock is ticking.



LEO:  Oh, yeah.



STEVE:  On March 1st sent me a note, he said:  "Hey, Steve," and he says in brackets, "[SpinRite Testimonial].  The CEO of my agency recently brought in her personal laptop, which held her recently created, but not filed, tax return."



LEO:  Uh-oh.



STEVE:  "The laptop was basically on life support."  And he says, "I don't even think Dr. House could have brought this thing back.  The OS loaded, but you couldn't do anything.  Right-clicking took over a minute to register.  The laptop's hard drive was just replaced not even a year prior." But on the other hand we know that laptop hard drives tend to get subject to physical abuse if, like, someone bumps them or drops them while the drive is spinning.  So that can be a problem.



He says:  "But I decided to break out SpinRite. I started a scan at Level 2.  The scan got to 0.14% complete and reported corrupt bits.  At 0.16% I realizing that this called for the big guns.  So I stopped the scan and turned it up to 11."  He's joking.  He says, parens, "(Started on Level 4)."  He says:  "I kicked off the Level 4 scan and let it run again.  It got to 0.19%, but it was running very slowly.  So I stopped the scan and thought to try it on Level 1.  Bingo.  Level 1 found and corrected the corrupt bits, and the rest of the drive was scanned, checked, and marked good.



"Rebooting the machine, the OS came up normal, and the laptop was running as if it were a new drive all over again.  The CEO got to file her taxes, which made me look like a superhero.  Upon payment for my services, there will be another yabba-dabba-doo coming your way.  Thanks again for the best HDD recovery tool out there.  I'm not sure why your competition even tried."  I don't really think we have any.  But anyway, Jordan, thank you for your report.



Okay.  Our two final pieces.  When I read this, I immediately had, like, one of those aha moments.  It's like, oh, my goodness.  I mean, it was so immediately clear to me that this was a problem I hadn't recognized before, and it probably will be for our listeners:  the threat presented by deliberately fooling AI.  AI, as we know, is all the rage now.  We have crazy computing power, insane amounts of storage, and massive connectivity.  What is guaranteed to happen, based on all of the history that we've seen of our adoption of technology, is that we will quickly grow to depend upon these technologies and to incorporate them into our lives, as we have with email, social networking, and most other technology that is useful and used on a daily basis.  And insufficient attention will be given to the question of how it might fail, or what we'll do if it does.



The problem is everything we've learned about security suggests that things generally work well only so long as no one is trying to deliberately subvert these technologies because our technologies are still not being designed, very much as Matthew said in his blog posting, still not being designed with an eye to malicious abuse.  That is, getting them working is hard enough.  And we generally ship it at that point.  Everyone's got a boss a couple levels up who says, "Why is this taking so long?  What's wrong with this?"



So what I ran across, thanks to one of our listeners, was a link to a blog posting at OpenAI.com titled "Adversarial Example Research."  They explain that "adversarial examples are inputs to machine learning models that an attacker has intentionally designed to cause the model to make a mistake."  And I love this analogy.  "They're like optical illusions for machines.  In this post," they continue, "we'll show how adversarial examples work across different mediums, and we'll discuss why securing systems against them can be difficult."



And that's all I'll share from this.  Anyone who's interested to dig in can go further.  But I just loved that analogy of optical illusions.  There's the famous one, maybe because it's so powerful, where you have two parallel lines, and then you present it with sort of a starburst of lines going in different directions.  And your eyes are incapable of not seeing the parallel lines as being bent, as being bowed.  No matter how much you tell yourself that they are still straight, they just don't look straight anymore.



And so the point is that there's an instance where - and of course optical illusions are many and famous because they trick our neural wiring into seeing something wrong.  And it must be the case that AI is massively foolable.  My intuition just says, oh, boy.  Let's see.  Where are we?  We're in Podcast 602 today.  So we're not quite two-thirds of the way finished with our three-digit podcast series.



LEO:  By the way, Steve says he's retiring at 999.



STEVE:  Probably, by the time we get to 999, we will have talked about real-world instances where AI was deliberately fooled.  Mark my words.  To me, this feels like a worrisome slam dunk.  Somebody will have created, I don't know what, something weird that someone puts on a street that causes self-driving cars to go insane because it interacts with their AI in a way that nobody thought of before; but somebody figured out that, if you have three X's in a row with a certain weird perspective somehow, the car goes off the cliff.  I mean, who knows?  But it feels to me like this is a very worrisome, very obvious problem that we're not going to be looking at until we see some exploitation of it, hopefully by researchers who are able to draw our attention to this before it hits the real world.



But everything we know tells us that the way this stuff is developed, the way we become dependent upon technology is kind of premature because we want it so badly; and that the technologies we end up designing are porous, that is, they're just not rigid enough.  They're not resilient in the way we need.  And to me, it feels like AI is really prone to having a bad reaction when something is presented to it that deliberately confuses it.  I just - it feels to me like that is just going to be way too easy to do.



And, finally, the dark side of automated domain validation certificate issuance, and thus the reason I titled this podcast "Let's Spoof."  I'm going to share, because it's got all the facts pulled together in a nice piece, a posting made yesterday by Vincent Lynch, who's with The SSL Store.  And while you may - and I was a little suspicious of his motives, this isn't about that.  I agree with him completely.  So his blog posting is  "A Call to Let's Encrypt:  Stop Issuing PayPal Certificates."  And I'm just going to cut to the chase so I don't lose anyone's attention here, to say that Let's Encrypt has issued - is everybody sitting down?  Leo, are you centered over your ball?



LEO:  I'm on my ball, baby.



STEVE:  988.



LEO:  Whoa.



STEVE:  PayPal certificates.



LEO:  Okay.  When there can be but one - the real one.



STEVE:  That might be a problem, yes, exactly.  There's only one valid one.  As he says:  "Data shows widespread use of certificates for phishing.  This is an open letter," he writes, "to Let's Encrypt regarding its issuance of certificates containing the word 'PayPal.'  Within the SSL/Certificate Authority industry, there is an ongoing debate about SSL certificates for malicious websites.  The big question is if CAs should be policing the content and nature of the sites they issue certificates to.  Should CAs be filtering out and rejecting certificate requests if they believe websites will use them for phishing or malware distribution?  Should CAs revoke certificates for websites that are reported and proven to be involved in these activities?"



In other words, and I'm paraphrasing here, I mean, I'm injecting my own editorial, whose problem is this?  This is a problem.  And in fact I should say phishing is the big worry that we've been addressing with SQRL for the last year.  And we did find a solution, but we can't implement it as easily as we want to today.  SQRL can protect, but we need a few more hooks that are sort of outside of SQRL's domain responsibility.  But the point is this is like the big problem.  And the question is, whose responsibility is it?



So continuing his post:  "Prior to Let's Encrypt, all major CAs" - the clock is ticking.



LEO:  I wanted to get the big turnover there.  Okay, go.



STEVE:  Ooh, yes, nice.  It still hasn't rebuilt itself, has it.  "All major CAs supported the view that certificates for malicious sites should be rejected or revoked, and Let's Encrypt has stirred the pot by taking such an aggressive stance on the subject."  Meaning not - somebody else's problem.  We're going to just issue them as long as you can prove you own the domain, a domain validation certificate, as we've discussed.  



"The Executive Director of Let's Encrypt has previously written about his view on a Certificate Authorities' responsibilities.  They think it's not a CA's job to determine if the site requesting a certificate is safe or legitimate" - like it's somebody else's problem - "and that even when one tries to, CAs aren't very effective at blocking the 'bad' sites.  As a result, Let's Encrypt forgoes the pre-issuance checks" - and we know they do because it's all automated - "that CAs have traditionally used to block 'high-risk' [he has in quotes] requests likely to be used for malicious reasons, such as phishing.  Instead, Let's Encrypt defers to services like Google's Safe Browsing and Microsoft's SmartScreen, which identify and block dangerous sites at a different layer."  Meaning the content layer after the connection has been established and given a green light, a big, happy, oh, look, you got a padlock, you're green, you're secure.  Thus the spoofing problem.



He says:  "Most of the commercial CAs disagree with Let's Encrypt's position, and this is a topic that is frequently debated.  For more background on this topic, I suggest reading a great post from Eric Lawrence, which inspired this post."  He says:  "I'm not asking Let's Encrypt to change its larger position.  I respect and understand its view and think it's a sensible position given its goals as a CA.  Given that the content filtering debate is such a heated topic, I would like to sidestep it all together and ask for something much simpler:  Stop issuing certificates containing 'PayPal.'"  That's it.



"Certificates containing the term 'PayPal' are being pervasively abused, and the continued issuance of these certificates poses a danger to the web by bestowing legitimacy to phishing sites. Let's Encrypt can address this without impacting its users or its mission.  That's it.  That's all we're asking.  Now you may be thinking, wait, isn't this content filtering?  As other CAs implement it, filtering involves complicated blacklists, submissions to multiple reputation and spam services, manual review processes, and a constant whack-a-mole game figuring out what misspelling or homonym the phishers are using this week.  There is no way for Let's Encrypt to implement similar measures without it compromising its mission or incurring large costs in developing, maintaining, and reviewing such measures."  In other words, that's what we pay other CAs to do.  And it's why only domain validation certificates are available through Let's Encrypt and not the higher validation certificates.



So he says:  "There is no way for Let's Encrypt to implement similar measures without it compromising its mission or incurring large costs in developing, maintaining, and reviewing such measures.  I think it is unfair to ask for that.  Instead, simply blocking 'PayPal'" - and he has it in quotes, 'PayPal,' that word - "and literally just 'PayPal,' no variations or misspellings, is an easy, feasible, and effective measure against the most dangerous and malicious use of Let's Encrypt certificates.  When Eric published his post, Let's Encrypt had issued 709 certificates containing 'PayPal.'  Now that number is 988."  And that's the end of his post that I quoted.



I jump over to just the beginning paragraph of Eric Lawrence's posting dated January 16, 2017, which he called "Certified Malice."  And so Eric writes:  "By December 8, 2016" - so early December three months ago - "Let's Encrypt had issued 409 certificates containing 'PayPal' in the hostname.  That number is up to 709 as of this morning."  So that was from December 8 to January 16.



"Other targets include Bank of America (14 certificates), Apple, Amazon, American Express, Chase Bank, Microsoft, Google, and many other major brands.  Let's Encrypt," he writes, "validates only that the, at one point in time, certificate applicant can publish on the target domain.  The CA also grudgingly checks with the Safe Browsing service to see if the target domain has already been blocked as malicious, although they 'disagree' that this should be their responsibility.  Let's Encrypt's short position paper is worth a read.  Many reasonable people agree with it."



And so I just thought this was interesting.  Everybody knows I've been promoting and a fan of and thinking that Let's Encrypt is a great idea for years.  We've watched it.  We've charted it.  I've explained its limitations.  I don't use it.  I use DigiCert.  I still, you know, I proudly use them.  I think for many places it makes more sense to have somebody creating a higher level of authentication.  Let's Encrypt, however, is the benefit of being free and automated.  So it gives you privacy.  It gives you security.  Unfortunately, we're in this problem where the browsers are celebrating the fact that you're over HTTPS, and we've trained users to believe that green is good in the URL, and so secure means something.  Unfortunately, it's so spoofable to say www.paypal.com-securityservices.com.  Well, the user doesn't understand that the www.paypal.com means nothing. It's the secondary securityservices.com that is actually the domain that the certificate is for.  So everybody is going to make this mistake.  The only thing I can think we need to do is, from the browser standpoint, start treating domain validation as the unwanted stepchild of TLS connections.  Somehow say, well, yes, it's good.  You're not in plaintext anymore.  But nobody has verified that you are actually at the domain you think you are.



I don't know how we solve the problem.  But, I mean, I love that we have a simple way for many sites where it didn't make sense for them to be over TLS, that now they can be.  It was completely foreseeable that this system that allowed automated domains to receive certificates would be abused.  I do think we have a problem that needs solving.



LEO:  Yeah.  I have kind of mixed feelings about it, only because...



STEVE:  Yeah.  It is a mixed-feeling problem.



LEO:  If you have a certificate from any domain, you can add PayPal to that as subdomain.  You know, wildcard domains make it possible.  And I understand why the cert people are saying, well, first off, we're offering a free service.  We can't afford to do certification validation above and beyond we'll check your site or send you an email.  And what they're saying is, well, technically, all the cert says is you have a secure link to a site with this domain name.  It's on the user and the browser and other people to figure out if that domain name is a malicious domain name or not.  Maybe it is now.  Maybe it won't be in the future.  Maybe it was, wasn't, but now is.  That's just the way of the web.



STEVE:  Yeah.



LEO:  I mean, I agree, they probably should have some obvious, I mean, PayPal's not going to ask Let's Encrypt for a cert, so they should probably have, you know, block Apple, PayPal, Google.  That would be an easy thing to do with probably no consequence.



STEVE:  Yeah.  Of course, then, where do you stop?



LEO:  Right.  That's the problem; right?  I mean, and I wonder if even other domain registrars do much beyond validating that, I mean, I know when I got a, what was it, I can't remember who it was, but all they did was send me an email.  It's not an extended cert.  They don't do any other validation.



STEVE:  Right, right.



LEO:  Well, Steve, we've done some good work here, I believe.



STEVE:  We have indeed.



LEO:  And now it's 10:45, so I think it's time to go.



STEVE:  And I'm sure we'll do some more good work next week, my friend.



LEO:  Every Tuesday, 1:30 Pacific, 4:30 Eastern 21:30 UTC.  That's when we convene the Security Now!, it's not a roundtable, it's really more like a kind of square table.  Our table, a two-top, for Steve and me to hash it all out.  Mostly Steve.  I'm just enjoying my tea.  You can come here and watch live, if you want, TWiT.tv/live, at that time.  You can also watch on YouTube, YouTube.com/twit.  We're live there.  Many, many, many TWiT apps, five of them on Apple TV alone, all of which stream live.  And of course you can get it on demand.  Now, Steve hosts an on-demand audio version of the show and a transcript.  That's the only place you can get that at GRC.com.



While you're there, pick up SpinRite, the world's best hard drive, recovery, and maintenance utility.  Play with all the other tools, do a little ShieldsUP, do a little Perfect Paper Passwords, all of that's there:  GRC.com.  We store all of the shows at TWiT.tv/sn, as well.  And of course subscribe because that's the best way to get it.  Then you don't miss an episode.  And this is one of the shows - most of the shows have the shelf life of a fish.  This show you want to keep listening to.  This is one of those where you'll want all the back episodes.  So subscribe.



STEVE:  Shelf life of a sequoia.



LEO:  A sequoia.  That way you'll never miss an episode.  Thanks, Steve.  We'll see you next time on Security Now!.



STEVE:  Thanks, Leo.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#603

DATE:		March 14, 2017

TITLE:		Vault 7

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-603.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week Steve and Leo discuss March's long-awaited Patch Tuesday, the release deployment of Google Invisible reCAPTCHA, getting more than you bargained for with a new Android smartphone, the new "Find my iPhone" phishing campaign, the failure of WiFi anti-tracking, a nasty and significant new hard-to-fix web server zero-day vulnerability, what if your ISP decides to unilaterally block a service you depend upon?, shining some much-needed light onto a poorly conceived end-to-end messaging application, two quick takes, a bit of errata and miscellany - and a look into what WikiLeaks revealed about the CIA's data collection capabilities and practices.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We've got a big, jam-packed show, all the security news.  Some wild stuff, too.  He's found another weird Game of Life creation, the Game of Life in the Game of Life.  You'll have to see it or hear it to believe it.  And then we're going to analyze that big dump of CIA hacking tools.  They called it "Vault 7."  Steve's take, coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 603, recorded Tuesday, Pi Day, March 14, 2017:  Vault 7.



It's time for Security Now!, the show where we talk about your safety, your security, your privacy, all about technology security with this guy right here.  He's looking at his hand because he's got to make sure he can do the live long...



STEVE GIBSON:  I've got to get the thumb out.



LEO:  ...and prosper.



STEVE:  Thumb out.



LEO:  Steve Gibson.  Steven "Nimoy" Gibson from the GRC, the Gibson Research Corporation.  He's one of the, like, greatest guys ever, and my close personal friend, but also a true security guru and a computer guru going back to the earliest days.



STEVE:  Love the technology, Leo, whether it's health or computers or bits or a little bit of science.  I just love technology.



LEO:  You have, I think, if I were to say what your signal ability is, you have a deep ability to understand highly technical topics and then distill it down in a way that people can readily understand it.  And I think that's the thing that people love this show most for.  Not that it's not a challenging show.  But, well, for instance, we've been waiting all week.  The Vault 7 release, purported to be CIA hacks from WikiLeaks, came out right when we began the show last week. 	



STEVE:  Yeah, it was Tuesday morning last week.



LEO:  And you said, as you often do, I want to have some time to digest it before I - unlike most of the rest of us, who will spout off just with the slightest notice, you like to actually think about these things.



STEVE:  Well, yeah.  So that's our main topic for the week is Vault 7.  We'll wrap up the podcast talking about that.  And again, as always, bring the Security Now!, I don't know, background and wisdom to a lot of what the press showed and how various companies have reacted and so forth.  But we've got, I mean, this is March 14, finally the long-awaited Patch Tuesday for March, for Windows.  We'll talk about that.  We have the release deployment of Google's Invisible CAPTCHA.  Getting more than you bargained for with a new Android smartphone - actually many, it's like 38 or 36, I think, is the count.  A new "Find my iPhone" phishing campaign I think I heard you talking about on one of the previous podcasts, might have been over the weekend.



An interesting bit of research in the failure of WiFi anti-tracking.  We talked about the anti-tracking of WiFi a couple years ago.  Turns out that it wasn't done right, unfortunately.  There's a nasty and significant new, very difficult to fix web server zero-day vulnerability which is affecting many major websites.  The question about what if your ISP decides to unilaterally block a service that you depend upon.  Shining some much needed light into a poorly conceived, end-to-end messaging application that I know I've heard you talk about recently, Leo.  A couple quick takes, a bit of errata, some miscellany, and then we're going to take a look at what we learned from Vault 7, and a different takeaway than I've seen anybody else talk about.  So another great podcast for our listeners.



LEO:  Good.  That'll be very interesting.  And of course, as soon as you said Patch Tuesday, I immediately opened my Windows laptop.  You know, it's the funniest thing.  Besides getting the regular Microsoft patches, Microsoft's now handling the Adobe Flash patches for Windows 10.  I guess they decided maybe let us do that, shall we?  Should we update Adobe Flash now for you?  So I'm going to get a bunch of updates, and I'm looking forward to hearing what have I gotten.



STEVE:  So our Picture of the Week is something that a number of our listeners sent me photos of, and then it got picked up by ExtremeTech, and I grabbed the shot off of their web page.  Their title is:  "Microsoft now puts ads in Windows 10 File Explorer, because of course."



LEO:  Oh, so frustrating.  So frustrating.



STEVE:  Because they can.



LEO:  Because they can.



STEVE:  And no one's going to tell them no because, after all, people have Windows 10.  So, yeah, this is just - I just look at my - I just sort of shake my head.  It's like, yeah, well, okay.  We are the customer, so...



LEO:  We talked about this with Paul Thurrott on Windows Weekly last Wednesday.  And there's a way to turn it off, but it's  completely obscure.  It doesn't say "turn off ads."  It says something like "turn off sync tool" or something.  It's ridiculous.  And I don't understand, well, I guess the operating system is free now for many people.  But it just...



STEVE:  Well, actually they're just paying for it in a different fashion.  And, you know.  So for those who aren't seeing the video, in your file browser this says, "Get the best deal on your cloud storage with OneDrive.  For $6.99 a month, an Office 365 subscription gets you 1TB" - and then it explains that's 1,000GB - "of OneDrive cloud storage as well as Word, Excel, and PowerPoint."  Then you have of course two buttons, Learn More or Not Now.  So, yeah, you know.



LEO:  They put ads in the menu.  Now they're putting ads in your File Explorer.  It's just ridiculous.



STEVE:  Yeah, well, that's what you get.  So you also get, today, on the second Tuesday of March, the long-awaited Patch Tuesday.  It was no bigger than they have been because, as we know, the technology is now different.  Basically it's all the things that are being changed are given to you at once, rather than granularly.  And as I said when we first talked about this upcoming change in Microsoft's patching strategy, as a developer I really empathize with how difficult it was for them to do what they had been doing.  The idea that - and I don't even know how you would go about this.  The idea that you could be individually offering fixes on a granular basis where a user could opt out of any of them, or even potentially remove them after the fact, yet the whole system would somehow continue operating even without that, I just don't even know how you did that.



So the fact that they're doing this this way makes a whole bunch of sense.  The important takeaway is we got in today's patch update everything we've been hoping for.  We got seven critical sets of patches.  There was an Adobe Flash Player update which was critical.  The Microsoft graphics component that we have been talking about for a couple months and waiting for, we got that.  That flaw in the SMB, the Server Message Block, so the Windows File and Printer Sharing problem.  That was where, if someone could induce your browser to reach out to a malicious SMB server, probably on the Internet, it's more likely to be there than on the Intranet, but basically any server anywhere could remotely execute code on your client.  So that's been fixed.  The problem with the Windows PDF library got fixed.  The problem with Microsoft's Hyper-V got fixed.  And then updates, critical updates - all these are critical - but then also to Microsoft Edge and IE.



So they fixed everything that we were hoping they were going to fix in February.  We got 'em fixed in March.  And then in addition there were 11 important update bundles, all documented in their security bulletin page.  So this would be a good thing to update, specifically because a lot of these have been, I mean, you know, some of these were zero-day, and that's why they were so critical is we knew, I mean, they were first discovered because they were being exploited in the wild, and there weren't good workarounds.  In fact, some independent security companies offered some patches that we talked about a couple weeks ago.  And I said, you know, I don't know.



LEO:  Yeah, that always makes me nervous.



STEVE:  I don't think I would go with that, yeah.  We did also talk a few weeks ago, we've been talking about Google's reCAPTCHA and talking about how, because it's a piece of JavaScript which runs in your client and is sourced from within the Google domain, it's able to acquire your browser's Google credentials for you when you visit a site that is hosting Google's reCAPTCHA CAPTCHA.  So what that means is that allows Google to deploy reputation scoring systems for you when you visit a third-party site that uses this reCAPTCHA.



Essentially, your browser, because the way cookies work, your browser makes a - it fetches the Google reCAPTCHA from the page you're visiting, which tells Google who you are.  Google is then able to assert to the page you're not a robot.  And that's what underlies just the click on the checkbox to say, yes, you're right, I'm not a robot.  Everybody agrees.  And then you're able to do whatever it is you want to do, post on a blog or log in or create an account somewhere where they're trying to prevent robots from doing that.



Well, we also talked about how in pre-release form for the last few months has been the so-called Invisible reCAPTCHA, where there's nothing on the page.  That is, it doesn't actually, I mean, if this has been reduced to checking a box, certainly a bot could check the box.  And people have been playing around with, like, well, do you have to kind of servo the cursor around?  Do you not always click in the same spot?  It turns out all of that was sort of on the surface.  Essentially, Google has enough reputation information normally to make this assertion on your behalf to the site you're visiting.  So the Invisible reCAPTCHA, it's in a web div, a division, at negative 10,000 pixels vertical.  So it's like, where is that?  That's, like, way up above where you can't see it.



LEO:  Oh, that's interesting.



STEVE:  Yeah.  So it doesn't even show.  But it still executes.  And if it decides it's not sure about you, then it will present itself on the page and maybe make you jump through some hoops - solve the puzzle, select the cat pictures, or whatever it chooses to have you do.  So in the normal case, you see nothing.  And in fact in the show notes, if anyone's interested, I've got two links for "invisible equals true" and "invisible equals false."  So you can demonstrate what it looks like.  It essentially looks like nothing.  It's just like as if the page is somehow able to determine that you're not a bot.  And of course now we know how they do that.  So it's sort a, in retrospect, an obvious thing.  Google deployed it incrementally over time, confirmed as they rolled it out very slowly that it was going to do the job.



So essentially we all tolerated this horrific CAPTCHA stuff.  I mean, there were some that I would just look at and go, okay, give me another one.  I have no idea what that is trying to be, you know, where they were just random "jibbles" of letters and numbers.  And it was like, okay, this is a problem.  That's all gone now.  So, yay.  The world wins.



Check Point, that's of course a very well-known security firm, discovered through their mobile threat prevention system, on two unnamed clients, a large telecommunications company and a multinational technology company who are clients of theirs, they discovered severe malware infections on 36 Android devices within those organizations.  So for it to be that widespread, it sounds like those organizations would have signed up for Check Point's Mobile Threat Prevention.  Whereupon Check Point, upon doing their job, would have said, uh, you've got some problems here.



So here's what's interesting, though, is that in - it's like, okay, Android malware is not that big a surprise.  But in backtracking what happened, they discovered that the devices arrived out of the box with this malware preinstalled.  One was the Loki trojan that we've talked about before, which first appeared about a year ago, around this time in 2016, in fact February of 2016.  It obtains root privileges and includes features such as grabbing the list of current applications, your browser history, your phone's contact list, call history, and location data.



They also found something called SLocker, which is mobile ransomware that uses AES encryption, as ransomware does, to encrypt all the files on the device and then demand payment in order to unlock it.  In their forensic analysis, however, they were able to verify that this was not installed by users doing something wrong after the fact.  The malicious apps were not part of the official ROM supplied by the vendor.  They were added somewhere along the supply chain.  Six of the malware instances that they found were added by a malicious actor to the device's ROM using system privileges, meaning that they could not be removed by the user, and the device had to be reflashed.  So I think that the takeaway here - oh, and I should mention also these were name brand devices.



LEO:  Most of them were really old.  And they don't say where they were purchased, but they say it's safe if you buy it from the company store.  My guess is these were purchased on eBay and places like eBay.  And I think if you're stupid enough to buy a phone from a third party on eBay, I would - this actually wouldn't even surprise me that this would be the case; right?



STEVE:  Correct.



LEO:  It's not - the manufacturer didn't do it.  It's whoever sold it.  It's a little self-serving because at the end Check Point says this wouldn't be a problem if you install check - I think that it probably is the case because they don't say that these were places, these were third-party resellers like eBay.  Right?  Who might [crosstalk].  And these were very old, for the most part, old phones, although I don't know what the Galaxy Note 8 is because that's not a product that's being sold today.  But it's the Note 2, the Note 4, the A5, the S4.  These are all fairly, you know, years-old devices.  Which sounds to me like it's eBay, probably.



STEVE:  So here's - I think our takeaway would be, I would say, if you were to purchase a phone, as you say, Leo, especially from some sketchy source?



LEO:  I'd reflash it.  I wouldn't even...



STEVE:  Well, yes, do that if you can.  But at a minimum, immediately, before you start using it, put on some, do some Android mobile antimalware scanning.  I did a little bit of browsing around, and it looks like BitDefender Mobile is currently near the top in the rankings.  They offer a free download and installation with 14 days to try the app before you need to pay for it.  And if you just, I mean, I think this is a nice tradeoff.  Download it.  Install it.  You can do a manual scan to make sure that there's nothing that it knows about.  They're claiming 100% hit rate on malware scanning.  Certainly it would know about year-old trojans or ransomware.  So just as a point of conduct, if you're going to get a new phone, why not download a free trial, run the scan, and then decide, maybe you want to keep it or remove it.  And at least you know that you've given your phone the opportunity to find out whether it's carrying more than you expect it is.



LEO:  By the way, it's the Galaxy Note 8.0, which is actually a three-year-old, a four-year-old tablet.



STEVE:  Ah, yes, right.  And I think I heard you talking about over the weekend a Find My iPhone phishing scam.



LEO:  Yeah.  I've been bit by it, yeah.



STEVE:  Our friend Brian Krebs - oh, really?



LEO:  Oh, years, well, not years ago.  But when Henry was in Barcelona last spring, so almost a year ago, he lost his iPhone.  And two or three days later I started getting texts purporting to be from Apple that said "click here."  And because he had just - now, whether that's a coincidence or somehow they knew he'd lost his iPhone, I don't know.  But when I looked more closely, it was not Apple.com, it was Apple.es.  Or, no, it was something like that.  Anyway, not Espana, it was Apple.estonia, I think.  Or maybe ee.  But nevertheless, I did click it first.  And it pulls up an Apple login, what looks like an Apple login page.



STEVE:  Yup.  Yup.  So what's happened is this has gone mainstream.  Brian Krebs has found, essentially, a rapid increase in this.  So this is the sort of - and the way he does, he's dug back in and has determined that it's organized crime, often located in Russia, that are attempting to phish owners' Apple login credentials in order to first unlock, then examine, and then ultimately wipe and resell these phones.  So someone loses their phone, maybe they're part of a family plan.  And so the remaining phone texts the other phone saying, hey, you know, if you've got my phone, I'd like to offer you a reward.  So then the person who has it is able to respond, pretending to be the, oh, look, you've lost your phone.  We're Apple.  Click this link, and you'll be able to determine where your phone is, lock it or whatever.  And of course the whole goal here is to phish the credentials from the phone's original rightful owner and then essentially acquire access to the phone that they wouldn't otherwise have.  So again, as we have often said, attacks don't ever get worse.  They only get better.



Speaking of which, a couple years ago, we talked about a problem with WiFi-based tracking of mobile devices.  The problem is that Ethernet, which is what WiFi runs on top of, was never designed with privacy in mind.  And we're talking 20 years ago Bob Metcalfe, originally at Xerox PARC and then at 3M, who was the inventor of Ethernet, back then it was a miracle if a network worked, rather than us putting all this extra requirement on top of it, like that it worked securely or it worked privately.  The idea that you could even tie a bunch of computers onto a common communications backbone, and they would be able at high speed to talk to each other, that was amazing.



So what you needed to have in order to do this was some kind of unique addressing.  And the solution was a 48-bit MAC address - M-A-C, all in uppercase, is the way it's normally seen, 48 bits.  And that is organized as 24 bits, which is to say three bytes, of registered manufacturer ID, and then another 24 bits of ID within that manufacturer.  The idea was that a company like 3M would have its own 24-bit designator, and it would generate NIC, Network Interface Controllers, each with their own guaranteed unique address, by incrementing the lower 24 bits.  So the 3M address, plus the serial number, the 24-bit serial number within 3M, would be concatenated into 48 bits.



And the idea was then that way, if you had different NIC adapters from different manufacturers, the upper 24 bits would be different because they're different manufacturers.  So there'd be no problem with colliding, with a whole 48 bits colliding.  Even if the lower 24 bits happened to have the same within the manufacturer serial number, the concatenation of 48 bits would be guaranteed to be globally unique.  So what that - back then, I mean, today it almost seems sort of like a quaint solution.  Well, isn't that cute.



What back then it guaranteed was you could just attach network interface cards, as many as you wanted to, from wherever.  And every single one on the planet, even though they weren't all connected together globally, but within your own little LAN, they would have different addresses, different MAC addresses.  And so that's the way packets on Ethernet find each other.  And of course we've talked in the past about how the address resolution protocol, ARP, that's the mapping between the IP address.  If you have Internet protocol running on top of Ethernet, it's the way for IP address to find the MAC address of the adapter with that IP and get the data where it's supposed to go.



Okay.  So then we go to WiFi.  Well, WiFi is an Ethernet protocol.  802.11 runs on top of Ethernet.  So what that says is that our mobile devices, and even PCs that we take out with us roaming, they have a MAC address.  And so some years ago, as privacy began to be a problem or a concern, people said, hey, you know, our smartphones that have physical global MAC addresses are a privacy problem because, essentially, even if you don't know the encryption of the IP content, even on an encrypted WiFi network, the underlying packet is where the MAC address is, and that's never encrypted.  So what that means is anyone passively just receiving packets out of the air, even if it's an encrypted access point, they see the physical MAC address of all the devices in that region.



So as we discussed a couple years ago, when this was first mitigated, devices began to back off of that and to randomize their MAC address.  They would still have a true global MAC.  But if they were just pinging access points in order to see who's in the neighborhood, like when you're not associated with an access point, and you say, okay, where can I connect, and you get that list of access points, well, what's happened is your device has sent out a broadcast saying, hi, I'm here.  Who can hear me?  And all the access points in the area respond with their SSID, which is what gets listed in that list.



The problem is, if that's your actual physical MAC address, you've just announced your identity, essentially, trackably, to all the access points that receive that, and anybody else listening.  So the change that was made was to - the realization occurred that we don't really have to use a fixed MAC address for this purpose.  We could randomize the MAC address when we're just out roaming, you know, driving down the street, because essentially WiFi is always out probing, looking for access points in order to list.  So there's a lot of this communications going on, all of which is, unless we mitigate it, is broadcasting a fixed globally unique ID as part of this request, this essentially sort of a WiFi Ethernet ping to say, hey, you know, who can hear me?



So a group of eight researchers at the U.S. Naval Academy decided to take a close look at this WiFi MAC address randomization, primarily on Android phones, although they also saw some problems on iOS.  And what they discovered, unfortunately, is that it was failing to provide the intended protections.  They have a long paper, I think it's maybe 17 dense pages of details.



But just from the abstract they said:  "Media Access Control" - which is what MAC address stands for - "randomization is a privacy technique whereby mobile devices rotate through random hardware addresses in order to prevent observers from singling out their traffic or physical location from other nearby devices.  Adoption of this technology, however, has been sporadic and varied across device manufacturers.  In this paper," they write, "we present the first wide-scale study of MAC address randomization in the wild, including a detailed breakdown of different randomization techniques by operating system, manufacturer, and model of device.



"We then identify multiple flaws in these implementations which can be exploited to defeat randomization as performed by existing devices.  First, we show that devices commonly make improper use of randomization by sending wireless frames with the true global address when they should be using a randomized address.  We move on to extend the passive identification techniques to effectively defeat randomization in around 96% of Android phones.  Finally, we show a method that can be used to track 100% of devices" - and that does include iOS devices - "using randomization, regardless of manufacturer, by exploiting a previously unknown flaw in the way existing wireless chipsets handle low-level control frames."



So this is not a huge concern.  The takeaway is that this is the kind of research that we need people to be doing because I read the whole paper, and there's lots of details that we don't need to get into, the point being that mostly from what looks like just laziness, this wasn't done right.  What they found was lots of simple mistakes in the logic of what was being done.  And in fact, many Android devices don't do any MAC address randomization at all.  It is absolutely present across iOS, but iOS is a monoculture; and so, yes, Apple did that in iOS, I think from back at 8, if I remember correctly.  It's been there ever since, and it's there universally.  However, not without some exploitability.  But that's an active attack rather than just passive listening.



They posited that maybe some chipsets didn't support it, but then they were able to verify that wasn't the case because they found that there were some devices with the same chipsets that weren't supporting it on Android that were also on Android, meaning that some manufacturers just don't care.  They're just not bothering to do this.  And who would know, if someone didn't look and didn't report on this?  So nice piece of research.  Nothing we as end users can really do except, if you were really concerned about this, then at least now you know that this is happening, that this global MAC which is in the hardware is passively leaking in a majority of cases and can be requested in an active attack across the board.



So what I expect will happen is that, to the degree that manufacturers care - for example, I would imagine Apple will care, and they will at some point fix this so that this newly revealed attack, which can be thwarted, but just isn't being right now, would be fixed.  And then other manufacturers, to whatever degree they care, can fix it if they choose.  But again, this is the kind of feedback loop that the industry needs, with researchers taking it upon themselves to check what's going on and then publish their findings.  So again, not a huge deal, but good to know.



There is a huge deal, however.  And this is a nightmare.  About 10 years ago there was an earlier version of something called Struts, which is a widely, now a widely used open source web application framework, based on Java, for Apache.  I think the first release was in '05.  And then a couple years later Struts 2 was released in 2007, so 10 years ago.



It turns out there's a vulnerability in the Jakarta file upload multipart parser, which is a standard part of the framework.  And Struts supports something called OGNL, which is Object-Graph Navigation Language, which is an expression language for getting and setting properties of Java objects.  So it's a sort of a scripting language for JVM.  However, Struts would mistakenly execute this OGNL language, even if it appeared in the Content-Type header, which is unconscionable because it then allows - it gives attackers control, which makes this very dangerous.



So here's the problem.  The awareness of this decade-old - wait a minute.  I'm sorry.  Struts 2 is a decade old.  This problem was introduced in 2012.  So, still, five years ago.  So for the past five years all of the Struts 2 server-side web application frameworks have had this problem.  The problem is that it's not a standalone dynamically linked or dynamically invoked library.  Rather, it is statically linked, meaning that it is compiled into these web servlets running on the server into Java executables.  That means that fixing this problem is not just a matter of running apt-get and fixing a module and then rebooting your system or installing an update.  The problem is this is compiled into often hundreds of little applets which are running applications on servers.  And every single one of them has to be recompiled from source, using the just-a-few-days-ago-made-available patched Struts 2 when this problem was fixed.



So an organization may have hundreds of these little Struts-using web apps, all with their own Struts JAR embedded in them.  And many of the apps may essentially be abandoned.  So they're still in use, and they're deployed; but they're not receiving ongoing maintenance because they're finished.  And it's been, what, five years.  The developers may have moved on, may be at other companies.  The point is it's way more difficult to fix this.



In Ars Technica coverage, Dan Goodin wrote that researchers at Cisco Systems said they were seeing a "high number of exploitation events" by hackers attempting to carry out a variety of malicious acts.  One series of commands that attackers are injecting into web pages stops the firewall protecting the server, then downloads and executes malware of the attacker's choice.  Payloads include "IRC bouncers" which allow the attackers to hide their real IP address during Internet chats, denial-of-service bots, and various other packages that conscript a server into a botnet.



And just to take this out of the realm of theory, I already had the show notes put together when I picked up on another piece of news about this.  Canada has just taken a major tax site offline due to these attacks.  Reuters, it was a story in Reuters that I saw saying that a newly discovered vulnerability in Apache Struts 2 software has forced the Canadian government to close down the Statistics Canada site used for filing federal taxes.  The site came under attack from hackers, but was immediately shut down before any damage could be done.  Well, of course, except that not having a site up is a problem.  So this is a big problem.



The security bug in Apache Struts 2 software is used mostly in websites of government, banks, and retailers.  It was reported last week, after Apache Software Foundation came out with an update to fix the vulnerability.  But again, the vulnerability was released.  Then it was reported two days later.  Except that this is not something you can patch and reboot your server.  You have to rebuild all of the applets that were statically bound to the previously, for the last five years, incorrectly functioning software.  So a number of security professionals, one was quoted, Chris Wysopal at Veracode said:  "This vulnerability is super easy to exploit.  You just point it to a web server and put in the command you want to run."  And it's easy to scan the Internet for servers using these vulnerable web applets, that is, delivering pages from them, and exploit them.  So this doesn't look good, and it's not clear that it's going to be fixed anytime soon. 



LEO:  Maybe we won't have to pay taxes this year.



STEVE:  Yeah.



LEO:  No.  Said no one ever.



STEVE:  We can wish.  So what would happen if your ISP decided to block a service that you depended upon?  We know that they block lots of things to protect us, so they believe.  For example, they for years have been blocking Windows Printer and File Sharing by blocking port...



LEO:  Thanks to you.



STEVE:  Yeah, ports 137 through 139 and 445.  And they'll block things like port 25 because they don't want people to run spamming servers within their networks.  So some things they do for themselves, some things they do for their customers.  Well, it turns out that the well-known ISP TalkTalk decided unilaterally, I mean, I'm not sure who they would ask to make it bilateral...



LEO:  Is it okay if we do this?



STEVE:  Yeah.  They just decided that TeamViewer was a problem because TeamViewer was being used maliciously by phishers and scammers, getting their clients, TalkTalk's customers, to download TeamViewer, pretending to be, for example, Microsoft Tech Support, and oh, your computer is infected.  Download this, and we'll take a look and fix it for you.



LEO:  Oh, yeah.  So maybe that's why, yeah, yeah.



STEVE:  Well, yeah, I mean, that's definitely why.  So it is a very popular, easy to use, remote desktop application.  The problem is it's also a good system.  I mean, it's frequently used by IT to support remote users.  So just out of the blue they added some port filtering to their entire network, specifically for the purpose of blocking TalkTalk.  Well, it's successful.  It worked.  So a whole bunch of users started complaining, initially, probably because they hadn't gotten of the attention of the right people.  I don't think TalkTalk deliberately was prevaricating.  But someone was initially saying no, no, no, it's not our problem, not our problem.



Finally they said, okay, yes.  In their posting, in their community blog, they said, or their page:  "Apologies for the confusion" - and this is a TalkTalk employee.  He's saying, "But I can confirm that we have implemented a number of  network changes that have blocked a number of applications including TeamViewer.  We constantly monitor for potentially malicious Internet traffic so that we can protect our customers from phishing and scamming activities.  As part of this work, we've recently blocked a number of sites and applications from our network, and we're working hard to minimize the impact on our customers.  We're working with TeamViewer and other third parties on implementing some additional security measures that would enhance the security to all customers," blah blah blah.



Anyway, the point is that this has been a major inconvenience to their customers.  And I read through some of this ongoing dialogue to see what was there, and I pulled a couple things.  One person said, quoting TalkTalk, "but we will continue to block any sites/applications reported by customers to reduce the opportunity for fraud to take place."  And this person responded, "Great.  What about the same consideration for customers who don't report any problems with sites and applications because they haven't had any?"  And this guy goes on to explain that he's been using TeamViewer via TalkTalk for many years without any problems.  Now all of a sudden, with no warning, we can't, and no one is saying how long, if at all, before we can use it again.  Corporations, businesses, and IT departments worldwide can use TeamViewer, but TalkTalk customers can't.  Extremely unsatisfactory customer service, and so on.



So to me, the takeaway here was, well, consumers don't have much control.  The only thing you could do, if VPNs are not blocked, would be to use a VPN in order to get your [audio dropout] outside of the control of an ISP in order, then, to get unfiltered access.  And I should mention that, when I had my two T1s, I was using Verio back in the day and had paid for expensive commercial bandwidth.  Now I'm using Cox, and I am behind Cox's filters.  And it was an inconvenience for me because there were things I was doing that, for my residential connection to GRC's remote servers, that Cox is blocking.  And I had to work around it.  I did because I have all the technology I need.  But most consumers don't.  And suddenly something that they were using and had been depending upon stops working.



LEO:  I don't know if this is the case with TalkTalk.  They're a British ISP.  But for Cox and Comcast, for instance, if you get their business class service, which is much more expensive, then they are much slower to block these kinds of things.  The other thing I'd note is that TalkTalk has had, of late, a big problem because Indian tech companies have somehow gotten the data from TalkTalk's customers, probably a breach; right?



STEVE:  It was a SQL Server mistake, yes.



LEO:  Yeah.  And so as a result a lot of their customers are targeted by these scammers, and they know stuff; right?  So that it's even more credible.  So I think TalkTalk's probably doing a rearguard action to protect themselves.



STEVE:  I think that's exactly right, yes.  I'm glad you reminded me of that because I'd forgotten that they had - they did have a breach.  And so with that data, that meant that TalkTalk's customers could be effectively targeted.



LEO:  Right.



STEVE:  And so now they're saying, oh, you know.  And now they're, like, reducing service in order to make up for the fact that their users are known.



LEO:  Well, we'll keep you from getting scanned.  Can you use - I wonder if you can use TeamViewer on a dedicated port?  Wouldn't that - well, depends what they're doing, I guess, if they're doing packet inspection or just blocking the port.



STEVE:  Yeah.  And I'm wondering, from some of the dialogue, I didn't look to see whether it was a point to point.  It might be running through the TeamViewer servers.  So they might be blocking access to the TeamViewer servers by IP range, for example, rather than by port number.  So that may be what's going on.



LEO:  Yeah, that's what's going on because they'll do NAT traversal, I'm sure.



STEVE:  Right, right.  And I'm sure I heard you talking over the weekend about Confide.  This got in the news.  And I think Matthew Green, our friend at Johns Hopkins, must have been having a bad day, because he had kind of some grumpier than usual tweets about this.  So I have to say, okay, so this is a venture-financed startup about four years ago.  2013 these guys appeared.  And this is one of those super slick, polished-looking sites.  If you go to GetConfide.com, G-E-T-C-O-N-F-I-D-E dotcom, I mean, that site is everything GRC is not.  It is, I mean, clearly it's got - stuff jiggles around and swoops in from the side as you scroll the page.  And, I mean, it just looks fabulous.  Super slick, polished, confidence-inspiring website.  And unfortunately what they offer is garbage.



LEO:  I'm sorry, I shouldn't laugh.



STEVE:  But, boy, is it pretty.  I mean...



LEO:  And also unfortunately it's being apparently used by a lot of whistleblowers in Washington, D.C.



STEVE:  Well, yes.  One of the many things that they offer is this quickly erasing messages, where I guess you draw your finger across the screen.  I mean, the fact that it's implemented in JavaScript - the whole thing is actually written in JavaScript.  It's like, okay.  That's not necessarily bad.  But it just sort of says, oh, this is more eyewash than anything else.  So in mid-February, middle of last month, Alan Woodward, a security researcher and professor at the University of Surrey, characterized Confide as, quote, "a triumph of marketing over substance."  And of course the home page, not only do they have military-grade encryption, but theirs is also, Leo, battle-tested.



LEO:  Ooh.



STEVE:  So the home page says:  "Your Confidential Messenger.  Communicate digitally with the same level of privacy and security as the spoken word."  Meaning that after it's been spoken, it disappears.  "With encrypted messages that self-destruct, Confide gives you the comfort of knowing that your private messages will now truly stay that way."  And I said in parents, "(or not)."



LEO:  This is the paragraph that would make anybody who listens to Security Now! laugh:  "All communication goes through transport layer security, preventing any possible man-in-the-middle attack."



STEVE:  Right.



LEO:  Well, of course.



STEVE:  Okay.  It actually says that.  I'm glad you read that right off the page because - so Matthew Green, apparently having a bad day, first he tweets:  "The encryption in Confide looks genuinely bad.  Don't use it, people.  What's the matter with you?"  And then he gives us a link to an Ars Technica article.  Then in his next tweet:  "Here's the technical blog post from Quarkslab."



LEO:  What's the matter with you?



STEVE:  What's the matter with you?  Then he says, "In short: no key fingerprinting, bad encryption mechanism, blegh."  And then he gives us the link to Quarkslab.  And finally, as if that wasn't enough, he says, "Oh, but Confide uses TLS pinning.  That's nice.  I'd ask" - this is Matthew in his tweet.  "I'd ask why people keep trying to reinvent their own end-to-end crypto, but I know the answer.  People are just the worst."  It's like, okay, Matthew.  Maybe you've had [crosstalk].



LEO:  Definitely a bad day.



STEVE:  It's just a bad day.



LEO:  What's wrong with you people?



STEVE:  So Quarkslab, their blog, I've got the link in the show notes if anyone really wants to go into it because these guys go into great detail.  They tear it apart with a step-by-step walkthrough, showing the way they reverse-engineered and examined the system.  In their summary, they say:  "TL;DR.  Confide server can read your messages by performing a man-in-the-middle attack."  In other words, this TLS that they boast about with its military-grade encryption...



LEO:  Battle-tested, don't forget.



STEVE:  Battle-tested, is to them, and they are the man in the middle.  And then in their own FAQ they ask themselves the question...



LEO:  But we would never. 



STEVE:  Oh.  Yeah.  How secure is this, and do messages really disappear?  And they say:  "We employ end-to-end encryption to ensure conversations remain confidential and are private to you. Even we at Confide cannot decrypt or see any messages.  Yes, after messages are read once, they disappear."  And that's like, yeah, okay.  Their big claim to fame is that they intercept the screen snapshot.  Of course you could take a picture of the screen with a different [crosstalk].



LEO:  Right.  Nothing can stop that, yeah.



STEVE:  Yeah, exactly.  That's why this whole, all of this nonsense about disappearing messages is like, as Matthew would say, "Blegh."  It's just...



LEO:  What are you people thinking?



STEVE:  Or actually as Matthew DID say.  Oh, boy.



LEO:  You've got to love Matthew Green.



STEVE:  Yeah.  As we know, producing a secure end-to-end encryption messaging system is truly difficult.  But that's a problem that's already been solved.  If you want maximum security, choose Signal or Threema, and take the time to verify your contact's key fingerprints.  And if you're using Signal or WhatsApp, turn on the not-on-by-default "notify if the fingerprint ever changes" feature.  And then pay attention to that if you're ever notified because there should be a reason for that to change.  And as we'll be discussing as the topic later in this podcast, end-to-end encryption is only secure if both ends are secure.



LEO:  Yeah, guess that's true, too.



STEVE:  Yeah.  The tunnel maybe absolutely impenetrable.  But if you can watch the traffic going in and out of the tunnel, sorry.



LEO:  The good reason for keys changing is if you change phones.  Right?  I'm going to install Signal on my new phone, and my key will change; right?



STEVE:  We talked about that.  I don't remember now...



LEO:  Maybe not.



STEVE:  ...if you can use the same key on the other phone.  I think...



LEO:  Maybe you can, yeah.



STEVE:  I think maybe you do, yeah.  But again, if the key changes, then you just need to reverify that it is still the person [crosstalk].



LEO:  New key.  Who dis?



STEVE:  Exactly.  So we are, as of last Saturday, March 11, that was the 28th anniversary of Tim Berners-Lee submitting his original proposal for the World Wide Web.  And I wasn't that impressed with what he just wrote.  It's a little political.



LEO:  Yeah, me neither.  Yeah, yeah.



STEVE:  Yeah.  It's like, okay.  So "I invented the web," he writes.  "Here are three things we need to change to save it."  And The Guardian picked up the story.  And the tag was, "It has taken all of us to build the web we have, and now it is up to all of us to build the web we want for everyone."



And so just the little, brief beginning of this, he says:  "Today marks 28 years since I submitted my original proposal for the worldwide web.  I imagined the web as an open platform that would allow everyone, everywhere to share information, access opportunities, and collaborate across geographic and cultural boundaries."  Okay, I don't think he did that 28 years ago.  It was network publishing 28 years ago.  It has certainly, you know, our definition of it has radically changed in that time.



Anyway, he continues:  "In many ways, the web has lived up to this vision [I would say it's far outstripped that vision] though," he writes, "it has been a recurring battle to keep it open.  But over the past 12 months," he writes, "I've become increasingly worried about three new trends, which I believe we must tackle in order for the web to fulfill its true potential as a tool that serves all of humanity."  And then he enumerates those three.  And I won't go into any detail, but those three are:  We've lost control of our personal data, it's too easy for misinformation to spread on the web, and political advertising online needs transparency and understanding.  Okay, well, to me, number three seems like a special case of, like, a bigger problem.



My own personal take, and I've sort of been referring to this lately, if I were to address what's wrong, I would say we need a usability fix.  We need a meta layer to wrap and hide what I would consider legacy protocol and domain name mess.  You know, the http, https.  And even hierarchical domains should all be hidden so that users interact with labels that can be mapped transparently to all of that other gobbledy-gook so users see Amazon and Google and Apple, and none of this other nonsense.  To me, that would be a nice change.  I don't disagree with the things that Tim said.  But it's like, okay.  I don't know that I think that's the biggest problem.  Leo?



LEO:  Also, how to solve.  I mean, I don't know how - does he propose a method?  I mean, maybe these are problems.  They aren't not problems.  But this is general problems of the 'Net, and I don't know what you do about that.



STEVE:  Right, right.  Well, and he talks about how there's been a fight to keep it open.  Well, we've lost control of our personal data.  The only way that gets fixed is legislation, heavy-handed, you know...



LEO:  Right.  And that's what worries me.  It sounds like government interference at this point.



STEVE:  Yes.



LEO:  Who's going to decide what political advertising is good and what's not?  That worries me more than anything; right?



STEVE:  Exactly.  And he says it's too easy for misinformation to spread?  Well, okay.  But then now you're talking about some sort of authoritarian control over, or maybe a reputation system, I mean, again...



LEO:  Yeah, I don't see any easy solution for what he's talking about.



STEVE:  No.  I mean, we're talking about, you know, it is very democratizing.  But these are the consequences of...  



LEO:  This is free speech.



STEVE:  Yes, exactly.



LEO:  Yeah.  We have more experience with free speech here in the U.S., I think, than they do in the U.K.  So maybe that's it.  



STEVE:  Let's hope we continue having that experience.



LEO:  Yes.



STEVE:  So a couple quick takes.  I got an interesting question that I wanted to address, just because it was perfect for our crypto technology.  Tom Elliott sent me a tweet saying:  "I have a dev telling me that storing SSN" - meaning a Social Security Number - "in SQL using an unsalted SHA-1 hash is secure.  Isn't this susceptible to time-memory tradeoff attack?"  And he says, parens, "(Rainbow tables and some GPUs)."



And so I responded - because he had DM'd me, so I could give him a longer response.  I said:  "Tom, that cannot be secure.  SSNs do not contain sufficient entropy to prevent having their hash brute-forced.  It doesn't matter whether it's SHA-1, SHA-256, salted, or unsalted.  The search space for a nine-digit" - which is what Social Security Numbers are - "all-numeric identifier is too small.  Also, storing the hash of an SSN, that is, putting the SSN behind a one-way function, suggests that the SSN is being used as an identity authenticating token.  Since it cannot be decrypted, it can only be verified in the future.  This is horrible design and policy since Social Security Numbers are inherently tied to people's identity.  This makes it much worse than 'your mother's maiden name' or 'your first dog's name.'"



Anyway, so I thought that was an interesting question.  I don't know what developer said, oh, yeah, you know, we're just going to hash the Social Security Number.  Well, okay.  If you're hashing it, that's because in the future you want to query for it again for someone to prove their identity.  But Social Security Numbers are leaked out on the Internet all the time.  So it's already tied to identity, which makes it a bad additional authentication token for identity.  Anyway, great question.  And even so, it just, you know, a nine-digit number is just not sufficiently entropic to be used because it would be trivial to brute-force that through any hash.  And even if it's salted, assuming that you've got the database with the hashed SSNs, you're going to have the salt there.  And again, it just doesn't have enough entropy to protect it.



Aaron Watt send me a tweet saying - actually to both of us, @SGgrc and @leolaporte:  "Panasonic hasn't patched their current Firefox OS for their Smart TVs in over a year.  Should I be worried about it?"  And I would argue, that's the wrong question because it's an IoT device.  We do know, after the Vault 7 attacks, that SMART TVs are not remotely vulnerable, as far as we know, but certainly can be exploited, as many IoT devices can be.



I would say, if you are worried, and if you do not need to have access to it in your main network, that is, if it's just an Internet-connected device, do what we've been talking about for all IoT devices, which is to put it on its own network segment.  Let it have Internet access, but don't let it see the rest of your network.  Isolate it.  And then, although there's still a problem from, for example, a privacy standpoint, if it could be used to spy on you, at least it can't be a beachhead from which attackers can then get access to the rest of your network. 



So network segmentation really does seem like an increasingly important thing.  And I imagine at this point a lot is required from people who want to do that.  I'll bet you that we see IoT-oriented routers in the future, where they're just offering...



LEO:  Oh, god, yeah.  Yeah.



STEVE:  ...a network segment, IoT network segment, and solve this problem for their users.



LEO:  The current crop of mesh routers in many cases could do that because they're already identifying devices and modifying paths and so forth, based on the device.  And so I don't think it'd be - it would be a fairly easy thing, I would imagine.  They wouldn't require hardware updates; right?  They could just - can't you do VLANs in software?  I wonder.



STEVE:  You can if the hardware permits it.  



LEO:  Right.



STEVE:  One of the things that I discovered was that many of the - if it's just a router connected to a switch inside the box, then you have a problem because a switch won't isolate.  But what we learned was that many of these firmwares are using a much more capable chip than one would think.  That is, they've configured the hardware to be a switch.  It is actually a router.  So, yes.  If they took the trouble to update their firmware, they could potentially offer actual port-level network isolation.



LEO:  If you can do a guest network on a router, doesn't that mean you have at least some kind of VLANing capability?  Isn't a guest network - depends, I guess, on how the guest network is handled.  But if it's isolated from the main network...



STEVE:  Yeah.  Normally guest networks will be a different subnet.  So you'd have, like, 192.168.0.something and then .1.something.  So it's a different subnet.  



LEO:  That's not isolating, though.



STEVE:  Right.  And normally it has, for example, in the WiFi case it has its own SSID and password.  But again, once you're on the guest network, then you could reach over to the other network.



LEO:  All right.



STEVE:  A bit of errata.  Two people tweeted, a Dan Hankins and TradMan.  Dan tweeted:  "@SGgrc Mic jack trick won't work on laptops, where jack function is software assignable.  There isn't and can't be a hardware cutout."  And TradMan said same, you know:  "@SGgrc Also the" - because he had sent me another tweet - "the mic plug hack isn't sufficient.  That just activates a software switch.  Even with mic plugged in, software can see the internal mic."



So I wanted to correct the record because I had shared a tip from another reader saying, hey, you know, plug something into that hole, and that'll disconnect the mic because it will do it, it'll connect - the hardware will route the mic out to the hole, the mic jack, which you then short in order to send nothing in.  But I'm sure these guys are right.  We talked in fact about how the RealTek chips, which are by far the most popular in the industry, all of those ports are software assignable as inputs and outputs.  Which is how, for example, even your speaker can be turned into a microphone.  The software just reassigns it as an input rather than an output.



So I'll bet these guys are right.  All of the recognition of something being plugged in is handled in software.  So while it might up the ante a bit, that is, it wouldn't just listen to the default microphone.  Malware might have to do a little bit more work.  It probably could.  So it's certainly not as good.  It's not the equivalent of putting an opaque piece of tape over your webcam that physically blocks the image.  We don't really have that power, from a software standpoint.



LEO:  Steve Gibson, Leo Laporte, and the CIA.



STEVE:  I was wrong about something else, Leo.



LEO:  Uh-oh.  What?  Never.



STEVE:  Last week I believed that I had found the coolest waste of time ever, by someone who built, remember, the working digital clock out of Conway's Game of Life.



LEO:  Yeah.



STEVE:  Okay, I was wrong.



LEO:  Something cooler?



STEVE:  Yes.



LEO:  By the way, I let that run overnight.  It was awesome.  It totally worked.



STEVE:  Thanks to some of our listeners who took and were interested and dug around, they found something even more incredible:  Conway's Game of Life implemented in itself.



LEO:  What?



STEVE:  Look at this YouTube link in this Miscellany here.



LEO:  Okay.



STEVE:  Our listeners won't be able to see what's going on, but our viewers - because it's just a fabulous, fabulous presentation.  I will tweet the link after the show.  And if anyone is interested in cellular automata, this is just - this is another, just an incredible piece of work.  Someone built Life in itself.



LEO:  I'm not sure I really understand what that means.  But I guess we'll watch, okay.



STEVE:  Watch.  Watch.



LEO:  This is Life, and it's expanding.  We're zooming out.



STEVE:  We're zooming out.



LEO:  Okay.  Is there audio?  I could turn on the audio.  Oh, yeah, perfect.



STEVE:  And those are little gliders that we're seeing moving together and then annihilating themselves.



LEO:  Okay. 



STEVE:  And we just keep moving out.  So that gives you a sense of scale. 



LEO:  We're pulling out farther and farther.  It's kind of cool.  Where the lines meet they explode, they disappear.



STEVE:  Yup.



LEO:  Yeah.  But then they continue to be created by the replicators at the end of the line.



STEVE:  On the edges, yup.



LEO:  Yeah, yeah.



STEVE:  To create a filled-in square.



LEO:  Yeah, yeah.  Okay.  Zooming out some more, we've got streets, it looks like.  Or I don't know, what could this be?



STEVE:  Watch.



LEO:  Wear headphones for the best experience.  It's a grid.  Some of the boxes are filled in; some are not.  Oh, is it going to spell Life?  No?



STEVE:  No.  That is...



LEO:  Oh, it just changed.  Oh, it's a Game of Life.



STEVE:  Yes.



LEO:  That's just crazy.



STEVE:  Is that unbelievable?



LEO:  Because, okay, so the squares that were filled in were pieces of life in the Game of Life, following the same rules.



STEVE:  Yes.  They were live cells.



LEO:  Oh, this is insane.  Oh, this is insane.



STEVE:  Oh.



LEO:  So it's Google or YouTube, Epic, Conway's Game of Life.  And you can see it for yourself.  Wow, that's wild. 



STEVE:  Yes.  It is just unbelievable.



LEO:  It is a Game of Life.  And then, as you zoom out, it is a macro Game of Life written in, I guess, I don't know even how to describe it.



STEVE:  Wow.



LEO:  Wow.



STEVE:  Okay.  And our final bit of miscellany.  It turns out that potatoes can grow on Mars. 



LEO:  If you read "The Martian," you'll know what this references.



STEVE:  In a nod to Mark Watney, the International Potato Center - who even knew there was such a thing?  There actually is.



LEO:  The International Potato Center?



STEVE:  Yeah, the acronym's a little different.  It's CIP, apparently the Center for, I don't know, International Potatoes?



LEO:  It's probably in French.  They always do everything backwards, you know.



STEVE:  This is from the Phys.org site, P-H-Y-S dot org.  And oh, my goodness, Leo, anyone who is a science geek if you don't - just put Phys.org into your browser and look at the home page of this site.  You will lose yourself in cool articles.  But anyway:  "The International Potato Center launched a series of experiments [I kid you not] to determine whether potatoes can grow under Mars atmospheric conditions, and thereby prove that they are also able to grow in extreme climates on Earth.  The Phase Two effort of CIP's proof-of-concept experiment to grow potatoes in simulated Martian conditions began on February 14 [last year] 2016, when," they write, "a tuber was planted in a specially constructed CubeSat contained environment built by engineers from the University of Engineering and Technology in Lima, based on designs and advice provided by the National Aeronautics and Space Administration," our own NASA, "at Ames Research Center in California.  Preliminary results are positive."



LEO:  There's a tater, right there.



STEVE:  Yes, you can grow potatoes on Mars.



LEO:  Tuber in a test tube.



STEVE:  Just in case you thought that might have been a little fictional.



LEO:  I mean, he did have to provide it with nutrients.



STEVE:  Yes.



LEO:  It was sterile soil, but...



STEVE:  Yes, there was a lot of recycling being done.



LEO:  Yes.



STEVE:  Yes.  He needed a bacteria.



LEO:  By the way, they have a Twitter account, CIPotato.



STEVE:  So Sean, oh, I didn't pronounce - I didn't practice spelling his name before.  Kloeckner, Sean Kloeckner.  I guess he's nearby, HB in the OC, he said.  He wrote:  "Took your advice on CRC errors."  This was interesting.  He said:  "Hi, Steve.  I've been a listener for probably a couple years now, and I appreciate all the advice you give on the show.  I sent in a previous note about Syncthing and what you think of it since I heard you complaining, in a good way, about BTSync previously."  Meaning that they refuse to document their protocol, so okay.



He says:  "Syncthing is totally open source.  I have to say it's been functionally everything BTSync is and works great as a Dropbox replacement."  And I'll just say, yes, I have heard good things about it.  I've looked at it.  But I've never had a chance yet to dive in deep.  He said:  "I no longer need to worry about my data in a company's hands, and it replicates to all my other PCs."  So that's a great tip.



He says:  "Anyway, I wanted to let you know I've been an avid listener and bought SpinRite recently.  I'm a Linux user and don't really need to buy software, but I know SpinRite would come in handy, and thankfully haven't needed it for anything other than testing and maintaining drives in my lab.



"I recently bought new PNY drives and used them for a ZFS root system using Proxmox, and noticed whenever I scanned my zpool there would be checksum errors that it would correct.  Few weeks went by without me touching them, and everything works generally okay.  But every time I would perform a scrub on my pool, it would still return errors."  Well, I'll explain what that means.  And that proves the point I will make.



He says:  "I learned in more detail that certain drives can return junk when under duress, but this is on a fresh install, every time with the same result.  Anyway, long story short, after seeing CRC errors in the syslog and then running SpinRite, it confirmed the CRC errors.  So I took your advice and replaced the cables.  I ran some further benchmark and dummy data tests on my pool and SpinRite.  And lo and behold, no more checksum errors.



"Love the show, wanted to pitch in my two cents for other listeners out there who may be in the same situation.  What you do is a public service."  Well, okay.  So my...



LEO:  So SpinRite works with ZFS, which is neat.



STEVE:  Well, okay.  So here's the problem.  A checksum error, when found, will force a repeat of the failed operation.  So as we mentioned before, the IDE or SATA and SAS cables, they add a checksum to the data transfer in order to detect a transmission error because, think about it, the computer is sending its data to the drive.  Well, we need the data to get there safely.  Then we need to trust the drive to store it.  But if the data, if there's an actual data in the transmission from the motherboard to the drive, the drive could be right.  In other words, it could correctly write what it received, but what it receives could be wrong. 



So consequently the transmission over the cables is CRC checked.  And SpinRite, as I had mentioned before, and this is what Sean was referring to, SpinRite will count and track those errors.  And you should never see any.  And some people see a lot of them.  Well, what that means is the cable has a problem.  So SpinRite showed that the cable was having a problem.  But the bigger point here is that you cannot depend upon a cyclic redundancy check, a CRC, to find them all because it's not big enough.  There's a statistical likelihood of it finding a problem.  When it detects a problem, it will force a reread or a rewrite.  That is, that transfer will fail because of its CRC failure.  And then it will be retransmitted, and it will probably work.



But the checksum, it is not big enough to guarantee catching everything.  It's not like an SHA-256, where the chances of a collision are diminishingly small.  So what he was doing was his system was finding checksum - he was finding transmission errors that were missed because his cables were so bad.  First of all, the badness of the cables were slowing down the system's operation because so many retransmissions would have been required.  But the fact that he was finding errors in his ZFS log meant that errors were being written.  So that proves that his cables were generating errors, and some were being missed.



So the takeaway is, if you run SpinRite, and it shows you you're getting CRC errors, the smallest effect that can have is an impact on performance because those errors are forcing retransmissions.  The bigger problem is, especially if you're not running a checksum ZFS file system, that is, if an error is missed, you will write the wrong data on your drive.  Reading is not such a big problem because when you read it again you'll get the right data.  You won't know you read it wrong because it'll be missed.  But, boy, writing it wrong, that then is it's wrong forever on the drive.



So cabling errors is what SpinRite calls them.  Because people kind of like, what's CRC?  Anyway, I call it a cabling error in the SpinRite UI.  And I do track them and check them and show them.  They need to be taken seriously because there's no guarantee they're all going to get caught.  So if you're seeing a lot of them, that increases the likelihood that some are going to get through.



Okay.  Vault 7.  So as we know, at early morning a week ago, seven days ago, WikiLeaks released 8,761 documents and files which allegedly and believably - even the CIA, they said, "We won't comment," but they did not deny.  This exposes and discloses the tactics and technologies the U.S. Central Intelligence Agency uses to hack into secure devices, systems, and communications.



This is the equivalent, roughly, of what we learned from Edward Snowden after he left the NSA, taking his large collection with him of these things that he felt he had an obligation to reveal to the world because he felt that this was wrong.  And so the devices covered by this are pretty much everything:  mobile, Android and iOS devices; routers; Windows, Mac, Linux PCs; many IoT devices from smart light bulbs all the way up through televisions.  And Kellyanne tells us also microwaves.  So pretty much the works.



I will note, as many of our listeners did, that "The Gibson" and "SQRL" appear as accident...



LEO:  What?  I didn't see that.



STEVE:  Yes, "The Gibson" is in there, as is "SQRL."



LEO:  What?



STEVE:  As accidental name collisions within the document dump.  So, no, they're entirely different things.



LEO:  [Humming "Twilight Zone" theme]



STEVE:  Okay.  So the big takeaway is, as you said at the top of the show, Leo, and I completely agree with you, there really was nothing hugely new here.  Imagine if someone were to listen to all previous 602 weeks of Security Now!, and after doing that, went out to find and collect everything that we discussed on the podcast.  That's what you'd end up with.  That's about what the CIA would have if they had done that.  That is, as I was looking through everything, there wasn't a single thing I saw that we haven't talked about.



LEO:  Really.  Interesting.  Wow.



STEVE:  Yes.  It's all - so essentially what this is, you know, we've talked about how - I love the word "porous."  I think that's exactly the right analogy because something which is porous, think of maybe like pumice or something, which is - it's not very porous, but it is a porous stone.  So if you just put some water on it, nothing's going to happen.  But if you pressurize water on it, you can force some molecules through.  That is to say, if you really want it bad enough, you can make it happen.



And, I mean, that's the perfect model for today's security, unfortunately.  Our security, and for all kinds of reasons, ends up being porous, even if it's social engineering.  Unfortunately, social engineering works.  You can trick someone into clicking a link.  So, and if you didn't try to trick them, then they wouldn't have clicked it.  But if you put pressure on them by designing something so that they will, then it can happen.



So that's where I think we are.  To me, this whole adventure with the CIA clearly demonstrates, as I just said, one of the fundamental distressing realities of today's computing and communication technologies, which this podcast often highlights.  When our defenses are inherently soft and porous, placing pressure upon them will cause them to leak.  Well-funded and highly motivated state actors such as the NSA and CIA can bring significant resources and thus a great deal of pressure to bear against the many technologies, none of which are particularly secure, or not absolutely secure, which we use in our daily lives.



And so, in looking at all the coverage of this and reading everything, I guess maybe perhaps the most controversial aspect of this is the notion that not a malicious hacker, but a taxpayer-funded organization would be discovering and concealing zero-day vulnerabilities in our systems, and keeping them to themselves for their own purposes and not disclosing them to their devices' manufacturers.



But other than that, I mean, everything we've talked about is stuff that they've got.  So essentially they're just collecting.  They're aggregating and deploying publicly disclosed, discussed in security forums, I mean, I'm sure they have people.  In the same way that Brian Krebs has infiltrated and penetrated the discussion groups where a lot of the organized crime groups are, you have to know that law enforcement has people who have an online persona, and they're collecting the same sort of intel and information for their own purposes.



Now, in response to this, there's been a response from the industry, which is what we would expect.  Apple has said that they've already patched "many," in quotes, WikiLeaks iOS exploits.  Their formal statement was:  "Apple is deeply committed to safeguarding our customers' privacy and security.  The technology built into today's iPhone represents the best data security available to consumers" - which we believe - "and we're constantly working to keep it that way.  Our products and software are designed to quickly get security updates into the hands of our customers, with nearly 80% of users running the latest version of our operating system.  While our initial analysis indicates that many of the issues leaked today" - meaning last week - "were already patched in the latest iOS."



And again, that's what we often see is that some of these things are older, yet we know that, due to patch delays, they can also still often be effective.  Apple finishes:  "We will continue working to rapidly address all identified vulnerabilities.  We always urge customers to download the latest iOS to make sure they have the most recent security updates."



And in an interesting little aside, just again, sort of a sense for how scattershot this also was, even the very popular Notepad++, which I use, fixes a CIA hacking issue.  They released v7.3.3 with the title "Fix CIA Hacking Notepad++ Issue."  And in their notes, this was a DLL hijack:  "The following DLL hijack works for both the portable and non-portable variants of Notepad++.  The issue of a hijacked DLL concerns" - and it was a DLL, S-C-I lexer dot dll [scilexer.dll], which is needed by Notepad++.  And they go on.  I won't go into the details.  But this was - it's not their fault.  Essentially, what was found among these documents, among many of these, was that the CIA had created a compromised version of scilexer.dll.  And if they could arrange to swap theirs for the real one, that gave them a beachhead.



And so what happened was I'm sure people going through this trove spotted Notepad++ referred to - and I've got the link on the WikiLeaks page for anyone who's interested - and notified the guys at Notepad++, saying hey, guys, do you know the CIA has, like, got a replacement for one of your DLLs?  And so what they did was they've cryptographically signed the real one now, and then they altered notepad++.exe to verify the signature.



Now, the problem with that is that that depends upon Notepad++ not being modified not to care about the signed signature of scilexer.dll.  And this is not a remote vulnerability anyway.  So the point is, if anybody is going to have access to our local machines or have some means for replacing files on the machine, then there's all kinds of mischief they can get up to.  But again, this is sort of like the CIA clearly, based on these documents, doing everything that they can, looking for any opportunity to get a wedge into our systems. 



LEO:  Was it your sense - you read through more of this than I did because there's 8,000 pages.  But was it your sense that almost everything here required physical access to the hardware?  I didn't see any remote exploits.



STEVE:  If there were zero-days, they may be able to get into things, yeah.



LEO:  [Crosstalk].  Yeah, yeah.



STEVE:  Yeah.  Certainly, for example, the much talked about TV spying on you.  The Smart TV hack, that apparently required malware on a USB to be stuck in, essentially modifying the firmware of the television, so that it pretended to be off when it really wasn't.



LEO:  A lot of this stuff is a stack.  You start with a compromising tool, and then you use that compromising tool to add something else, to add something else.



STEVE:  Right.



LEO:  So what seemed to be missing, certainly from the Samsung hack, but others, was just how would you get this on here?



STEVE:  Correct.  And one of the ways to differentiate this from the NSA was, you know, that was a massive vacuuming experiment or exercise, where they were just sucking, you know, they were tapping into main Internet backbones and sucking everything in.  This is much more targeted in nature.  And, I mean, it shouldn't surprise anyone that this is what the CIA is doing.  We want them to be doing it, not to us, but to other people.  And we presume that they are.



LEO:  NSA does signals intelligence generally.



STEVE:  Right.



LEO:  The CIA is a spy organization.  They're going after targets.



STEVE:  Right.  And not surprisingly, they're using technology which is vulnerable, which we discuss every week.



LEO:  I would expect them to.  I mean, all you have to do is watch a Jason Bourne movie, for crying out loud.



STEVE:  Yes, yes.  And so our bottom line, as I once indicated back in the time when we were talking about that Sony Pictures APT, the Advanced Persistence Threat breach, I would have a nervous breakdown if I was responsible for securing something as lumbering and massive as Sony Pictures.



LEO:  Yeah.  Or Yahoo, or...



STEVE:  It is, yes, it is simply impossible.  And so the NSA/Snowden and the CIA/WikiLeaks disclosures demonstrate conclusively, if nothing else, that despite all of their best efforts, those high-end government intelligence agencies are unable to secure their own working assets.



LEO:  Yeah.



STEVE:  They haven't.  So how could they possibly be trusted with any explicit golden key which would allow them to access our encrypted communications?  Past is prologue, and all of the evidence demonstrates that, even with the best of intentions -  and I don't suggest they don't have the best of intentions.  Well, I'll give them that.  But U.S. law enforcement cannot be trusted, should not, must not be trusted with any sort of carte blanche backdoor access to the Internet's encrypted communications.  Of course they want it.  It's fine for them to ask.  You can ask.  But we must not capitulate.



As our experience with OpenSSL vulnerabilities continues to demonstrate, any sort of cryptographic monoculture is inherently dangerous.  The system we have now, where a court order search warrant must be obtained and served, creates a heterogeneous system with distributed responsibility and built-in checks and balances.  Companies like Amazon push back, and Google, and Apple, and so forth.  Even requiring companies to be able to decrypt their customers' data, when they choose not to be able to, subjects customers to unnecessary risk.  And besides, if the CIA has all of this now-demonstrated technology, they clearly don't need to ask for permission.



LEO:  The other issue is, of course, not merely the golden key issue, but the Vulnerabilities Equity Processor, VEP, where the government had agreed a couple of years ago to, if they had an exploit, and it was leaking out, that they would immediately notify the companies.  And there was a process where they could appeal.  In fact, they were supposed to tell everybody every time, unless in the appeals process they could convince the VEP panel that they needed this flaw for espionage.



STEVE:  Ah.



LEO:  So that's all out the window, by the way.  I mean, I don't think, even in when it was created in 2014, it was much adhered to.  But this was the theory, is how do you handle this?  How do you...



STEVE:  I'm sorry.  I was going to say, and the FISA court never said no.



LEO:  Right.



STEVE:  You know, it became a [crosstalk].



LEO:  Yeah, I don't think it was FISA was involved in this.  There was a panel including intelligence agencies, executive branch people.  But I think that that's clearly not been, you know, the process hadn't been adhered to.  And so really the big question is people are discovering flaws all the time.  What is their moral obligation to reveal these?  And the moral obligation gets higher if they can't control them.  Right?  It's one thing if, well, we could find flaws that only we would have access to them.  It's another thing entirely if we can't keep a lid on them.  Then bad guys are going to get them.  And that actually hurts us, the citizens, because industrial espionage, hacking suddenly is empowered by tools created by our own government because they didn't reveal them to the companies responsible.



STEVE:  Well, and remember, too, in the case of zero-days, we only discover them when we discover them being used.  Which really leaves the question open, what is going on that we don't yet know about?



LEO:  Right.  Well, that was one of the things the NSA said in the Vulnerabilities Equity Process, that's hard to say, VEP, was, well, we have telemetry.  We'll know if these things have leaked out.  We'll know if these tools are being used because we're watching for the fingerprints of these tools.  Then, if we see that, oh, then we'll let you know, Apple.



STEVE:  In that case, I guess their alarms all went off last week.



LEO:  Yeah.



STEVE:  Agh.



LEO:  Well, to WikiLeaks' credit, they didn't reveal code; right?  I think there was one inadvertent release.  But all this code is being held onto.  It'll get - it'll come out.



STEVE:  It'll be interesting to see what they do.  WikiLeaks does have a tendency to a little bit overinflate what they're offering.  And then some of the things they promise never comes out.  I don't know what's going on behind that.



LEO:  Right.



STEVE:  But it'll be interesting to see, over time, if it comes out.  I'm glad, for example, that companies affected by this, like the Notepad++ guys, like Apple, are being proactive, going through this and making sure that anything that is now known publicly, they either have already dealt with or can deal with very quickly.  But again, it's like, yeah, I mean, we want our CIA to be good and to be protecting us from bad guys.  And if they hack somebody else's phone and are able to do that, well, that's their job.



LEO:  I mean, were people watching "The Bourne Identity" and thinking, well, none of that could ever happen?  Or were they thinking, yeah, this is probably the kind of capabilities the CIA has?  I mean, they...



STEVE:  I've got to say, I have to say, I know our listeners watching "Mr. Robot" are like, yes, that happens, that happens, that happens.



LEO:  Yeah, yeah.  Well, that's because "Mr. Robot" had a pretty good team of advisers, including our guest from last week, Marc Rogers, who was just really good on Triangulation.  Well, good.  You didn't say anything I didn't expect, but I'm glad to hear your take on this.  We've been waiting all week long to say, well, what, what does Steve say?  What does Steve say?



STEVE:  Well, and I really do, I hope the takeaway from this is let's leave things the way they are.  We do not need to make this easier.  Nobody needs a golden key.  They obviously already can get anywhere they need to.  It's just this demonstrates that not only is all the software they're attacking having problems, but so are they.  Their own software, I mean, their own networks and systems are having problems.  And so we all have to do the best job we can of keeping our security as high as possible.  Giving anybody some sort of unrestrictable access is just - it's a recipe for disaster.



LEO:  Which is why we've talked so much about not giving your phone and your computer to the Border Patrol when you cross into the country because - we talked on MacBreak Weekly about data exfiltration, them taking everything off your phone.  What we didn't mention is at that moment they also have the opportunity to maybe apply some of these handy-dandy hacks to your device.



STEVE:  Yup.



LEO:  And observe on you and, you know, watch you from then on.  Anyway, great subject.  Thank you, Steve.  We do Security Now! every Tuesday at 1:30 Pacific, 4:30 Eastern.  New time because we're in summertime now, here in the United States, so the UTC time for this, I know UTC doesn't change, but we do, is now 20:30, if you'd like to watch live.  We'd love it if you do, and join us in the chatroom at irc.twit.tv.  You can tweet at Steve, @SGgrc, if you've got questions.  With any luck, we might have a Q&A session next week.  You never know.



STEVE:  Let's hope it's a quiet week.



LEO:  You never know.  Yeah, be nice if it was.  You can also go to his website, GRC.com, and leave questions there at GRC.com/feedback.  While you're there, pick up SpinRite, the world's best hard drive maintenance and recovery utility.  And check out all the freebies Steve offers there, as well, including Perfect Paper Passwords and SQRL and the Healthful Sleep and all that stuff.  It's all at GRC.com.  He also has this podcast there, I should mention, 64Kb audio, and he is the unique purveyor of written transcripts of the show.  So if you like to read while you listen, or you want a searchable text file that will jump you to the part of the podcast you'd like to find, that's where you get that:  GRC.com.  We have audio and video at our site, TWiT.tv/sn.  You can watch us live.  There's apps on every platform, or at YouTube Live at YouTube.com/twit, or download the show after the fact.  We have on-demand also.  And everywhere you go to get podcasts you should be able to find Security Now!.  I think that does it.



STEVE:  My friend, until next week.



LEO:  We'll see you later.  Steve and I have a fight at the end of every show to see who gets the last word in.  See you then, Steve.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#604

DATE:		March 21, 2017

TITLE:		Taming Web Ads

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-604.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  This week Steve and Leo discuss developments in the New Windows on Old Hardware front, Cisco finds a surprise in the Vault 7 docs, Ubiquiti was caught with their PHPs down, Check Point discovered problems in WhatsApp and Telegram, some interesting details about the long-running Yahoo breaches, the death of the "eBay Football," the latest amazing IoT insanity, the incredible results of the CanSecWest Pwn2Own competition, a classic "you're doing it wrong" example, Tavis pokes LastPass again, some miscellany, and an interesting proposal about controlling web advertising abuse.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  The title of this show, "Taming Web Ads," is just a small portion, a small fraction of the total show.  We've got a lot of great stuff, including the results of Pwn2Own, some surprising hacks, and an IoT device that really just violates every rule.  We're going to talk about it next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 604, recorded Tuesday, March 21, 2017:  Taming Web Ads.



It's time for Security Now!, the show where we talk about you, your privacy, and security online.  This is the guy, man; this is the resource.  Everybody - college classes, IT professionals, CIOs, everybody wants to know what Steve Gibson thinks, from GRC.com.  Hi, Steve.



STEVE GIBSON:  Hey, Leo, great to be with you for Episode 604.



LEO:  604.



STEVE:  Yes.  And actually I learned that even security researchers are listening to the podcast.  I have some feedback from the guys we talked about last week at the U.S. Naval Academy that I'll share later in the episode.



LEO:  That always makes me nervous when the people we're talking about are listening; right?  Not you.  You have the confidence of your convictions.  You know. 



STEVE:  So, yeah, we have a lot of news this week, tons of stuff to talk about.  We've got some developments in what I call the New Windows on Old Hardware front.  Cisco found some surprises in the Vault 7 CIA doc dump.  Ubiquiti was caught with their PHPs down.



LEO:  Uh-oh.	



STEVE:  Check Point discovered problems in WhatsApp and Telegram.  We've got, finally, some really interesting details about the long-running Yahoo breaches as a consequence of the FBI's indictments in that case, where a lot of facts were revealed, that finally we have something we can talk about, which I'm really happy for.  And it's kind of creepy.  We've got the death of the eBay football that was one of the very first things we talked about years ago.



LEO:  Yeah, my first dongle, yeah.



STEVE:  Yeah.  They sent out email to all football users saying, eh, we're improving our security by eliminating the best option.  Okay.  Anyway, we also have, believe it or not, something so amazingly crazy in IoT insanity this week.  It's, wow.  Actually went to a class action settlement, it was so over the top.  We have some incredible results from the CanSecWest Pwn2Own 10th Anniversary competition, which pretty much demonstrates that, if you dangle enough money in front of security guys, they can pretty much poke a hole in anything that they aim at.  Because, as we know, we talked about this a few months ago that it was coming up, that this was the largest set of prizes ever, a million dollars' worth of prizes.



We've got another classic "you're doing it wrong" example.  Tavis has poked another hole in LastPass, which is already resolved, but worth talking about.  We've got some miscellany.  And I want to - and this isn't, like, the most important thing.  But it was an interesting, nicely written piece on Medium that I wanted to share with our listeners to wrap this up about taming web ads, from someone who is very sophisticated technically, has presented at O'Reilly's OSCON a few times, is involved in the open source community, and was able to quit his job because his page got enough success with advertising.  But it then became a problem.  So just a whole bunch of great stuff to talk about.  Oh, and a fabulous Picture of the Week.



LEO:  All right, Steve.  Let's get the security news.  Oh, you want to do the picture first?



STEVE:  Oh, Leo.  This is just - I love cleverness.  And this is just so clever.  This is one of those where you really don't even need to say anything.  The picture tells the whole story.  But I gave it the caption "Setting Traps for Autonomous Vehicles."  And of course everyone knows who understands the rules of the road that you're not allowed to cross a double line, but you can cross from the broken, sort of like the dotted line mated with a solid line, you're able to cross from the dotted line into the solid line side, but not the other way.



And so someone cleverly realized that, if you created a large circle with an outer dotted line and an inner solid line, then a car following those rules could enter into that region, but could never leave.  So anyway.



LEO:  I didn't even - I'm glad you explained it.  I get it.  So it's like a roach motel.  They can check in, but they can't check out.



STEVE:  Yes, exactly.  It's a trap.  The car would see the dotted line on its side and go, oh, and then roll across.  Now it's facing a solid line, and it can't ever leave.



LEO:  Aw, poor little car.



STEVE:  I love it.  Very, very clever.  So, okay.  In the news this week has been something that we initially touched on some time ago that was - at the time it was controversial.  Well, and actually it drove me to immediately purchase what would be my final Windows 7 laptop, and to build that monster machine that our listeners know that I built, what, maybe a year ago or so, using the last hardware that Microsoft promised to support all the way through the end of Windows 7 support, which is April of 2020.  And that was the Haswell chipset.  The succeeding chipset, which was Skylake, they said, well, we know that enterprises are slow to move.  So we'll support Skylake through 2017, but not through 2020.  But, so, okay, and so that at least gives companies some time to move themselves.



I, of course, am unwilling to ever leave Windows 7, which I think is the perfect operating system.  So I purchased some hardware that would be able to get its updates from Microsoft through the end.  So what started happening, and caught some people by surprise, was error messages that they began to get on later hardware.  And Microsoft has a support page where the title of the support page is, which is quoting their message:  "Your PC uses a processor that isn't supported on this version of Windows."  That's the error you receive when you scan or download Windows updates.



And so on this page where Microsoft is diagnosing what they call the "symptom," it says:  "When you try to scan or download updates through Windows Update, you receive the following error message:  Unsupported Hardware.  Your PC uses a processor that isn't supported on this version of Windows, and you won't receive updates."



And then they say:  "Additionally, you may see an error message on the Windows Update window" - which is unfortunately unclear - that says:  "Windows could not search for new updates.  An error occurred while checking for new updates for your computer."  And then it's got an obscure error code, 80240037, Windows Update encountered an unknown error.  Okay, well, it's not unknown, it's just unarticulated in this instance.



And so under the cause for this symptom their page says:  "This error occurs because new processor generations require the latest Windows version for support.  For example, Windows 10 is the only Windows version that is supported on the following processor generations."  And then they enumerate Intel 7th-generation processors, the AMD Bristol Ridge line, and Qualcomm's 8996 processors.



And they say:  "Because of how this support policy is implemented, Windows 8.1 and Windows 7 devices that have a seventh generation or a later generation processor may no longer be able to scan or download updates through Windows Update or Microsoft Update."  And then for their resolution of this problem:  "We recommend that you upgrade Windows 8.1-based and Window 7-based computers to Windows 10, if those computers have a processor that is from any of the following generations."  And then they continue.



Now, of course, this was picked up by the news, who characterized it as - Forbes said:  "Microsoft Admits Forcing More Users Onto Windows 10."  HotHardware had two stories:  "Microsoft Disables Windows Updates for Ryzen and Kaby Lake PCs running Windows 7 and 8.1," and also "Microsoft Apparently Ramping Up Heavy-Handed Tactics to Force Windows 10 Migrations."  Neowin said:  "Some new PCs running Windows 7 and 8.1 won't receive further updates."  And the Hacker News:  "Microsoft Started Blocking Windows 7 and 8.1 Updates for PCs Running New Processors."  So, you know.



And we talked about this.  This is expected, actually.  But certainly maybe to those who didn't know or people who are going to be disappointed that they were deliberately wanting to run an older Microsoft OS on newer hardware.  And it's difficult to regard this as something other than deliberate because Microsoft is continuing to support Windows 7 and 8.1 on the hardware of those OS generations.  Meaning Haswell, the hardware that I deliberately bought for this reason, will get support all the way out through 2020.  Now, and I'm not in a position to be able to definitively say how much effort this saves on Microsoft's part.  Maybe Paul and Mary Jo can shed some light on it.



LEO:  Yeah, well, we talked a lot about this.  I think, to me - and it was a little bit of a surprise when this whole thing came up about a year ago.



STEVE:  Right.



LEO:  It shows you - and this is what we concluded on Windows Weekly - how much back-patching Microsoft does, when a new processor comes out, for the new microcode in the processor, to make sure that Windows works.  You know, you would just assume, well, it's x86 architecture.  What would you have to do?  But it turns out there's a significant coding effort each time there's a new generation of Intel processors, of Haswell, of Kaby Lake and Skylake and all of that.  They have to add a considerable amount of back-patching to versions of Windows, to older versions of Windows, so that they'll work.



STEVE:  Right.



LEO:  You know, and the reason I kind of feel like it's not a commercial, you know, you could say, well, they're just trying to get you to buy the new version of Windows.  But the reason I think it's not that is I look at what happened with Skylake and the Surface Pro 4 and the Surface Book.  For eight months my Surface Book had all sorts of power nightmares because it didn't work well with Skylake.  The Intel Skylake processor was so dramatically different from the previous Haswell generation in terms of power management that this thing, it literally - Microsoft famously said it was a "tough computer science problem."  Remember that?  That was them getting the patches, the firmware fixes, so that you could - and now it does, by the way.  The Surface Book Pro works great.  Power management is great.  But it was, I think, really because of Intel's massive changes to power management in Skylake.  So I don't think it's unreasonable for a company to say, look, we would have to write a lot of code, we'd have to do a lot of work to get these older versions of Windows to work with new processors that didn't exist when we wrote them, so we're just going to cut them off.



STEVE:  And along with that you might argue that their metrics could show that it would largely be wasted effort.  That is, how many people are actually going to take advantage of Windows 7 or 8.1 on the newer hardware, especially when the new hardware probably comes bundled with Windows 10.



LEO:  Right, right.



STEVE:  So, yeah.  I can certainly see it their way.



LEO:  It's tempting to assume it's, you know, it's anti-competitive.  They want to make you upgrade, et cetera, et cetera.  But I think there might be a good technical reason for this.



STEVE:  Yeah.  And when we discussed this last year, I also agreed that, I mean, again, I can't imagine the nightmare of needing to maintain something as complex as Windows has become across a growing number of disparate hardware platforms.  So I would say I don't even know, I haven't looked to see whether Haswell chips are even still available.  But for our listeners, I mean, 7 is still the majority OS on the Internet.  I mean, there are more Windows 7 than Windows 8.1 and 10, despite Microsoft's efforts to move people away.



So people who - and I'm not even sure how you would get and register 7 now.  I can because I'm an MSDN subscriber, and I pay for the privilege for development purposes, to be able to make sure the software I write is backwards compatible to what is still the majority OS on the Internet, the majority Windows platform.  But if Haswell chips are available, if you don't want to be forced to be move to Windows 10, and if you want updates through the life of Windows 7 or 8.1 updates, then now would be the time to move to a Haswell chip, if you're on something older, and if it matters to you, because this is now more than just a theoretical problem.



LEO:  This is real, yeah.



STEVE:  This is real, yeah.  So Cisco was rummaging around in the CIA's leaked Vault 7 documents and discovered that the CIA knew something they didn't know about their own high-end IOS-based switches and routers.  Now, to be clear, this is not the low-end Cisco/Linksys consumer routers.  These are the big iron routers, like I had that were terminating my T1s.  I had a 3600 Series router running IOS.  That would have been vulnerable if I was using what Intel calls the Cluster Management Protocol, CMP.  They have something called Cluster Management Protocol which is used to link, as its name sounds, clusters of routers together.  But apparently, even without that explicitly running, because its transport is Telnet, having Telnet exposed to an untrusted machine or IP, that is, not having an access control list to restrict who has access to a router's Telnet protocol allows a remote code execution to be performed.



So this rocked the high-end router and switch community this week, or late last week.  Cisco produced an advisory immediately.  The only workarounds are turn off Telnet.  That's all you could do is shut down Telnet because, if it's on, and a bad guy can get access to it, then you're in trouble.  And if you cannot do that, then apply an access control list to restrict the IPs which can generate Telnet-based traffic.  Telnet is TCP-based, so an access control list is perfect protection, unlike the UDP, where you could spoof the source IP.  In order to bring up a Telnet connection, you first have to have the standard TCP three-way handshake, which inherently validates the source IP of the incoming TCP connection request.  So an access control list is all you need.



I have, for example, I use Telnet throughout my network.  There isn't a single instance of it where I haven't already, I mean, where I didn't already lock it down so that - because, I mean, Telnet, you just don't want to leave that open.  The problem is, you know, it's certainly been replaced with SSH, where you use a good certificate authentication everywhere.  But there are still older devices, and in this case many IOS devices, where they use Telnet for their admin access and control within an environment.  It's just - it's an old protocol.  The only protection that exists for it is username and password.  And you certainly don't want that open to attack because you cannot monitor people making a brute-force attack on that protocol.  So they are going to be producing a patch.



This affected as many as 300, or I saw more than 300 different Cisco switches and routers.  So anything with IOS or IOS XE is vulnerable.  So if any of our listeners know, if they're using or know somebody who is using pretty much any IOS-based Cisco device with Telnet management, where there's any chance that a bad guy can get access to a Telnet connection, you want to do some mitigation immediately.  Again, nothing for the consumer, at the low-end consumer end.  But I'm sure there'll be a fix for it.  It'll be an IOS update that will fix the problem that was found.



But it is spooky that this was something that our CIA apparently knew.  It was part of the Vault 7 document disclosure.  And much as last week we discussed how Apple was going through it, and the good news was many of these things had already been fixed in previous iOS updates in the case of Apple - a different iOS, of course, than Cisco's.  So Apple was continuing to go through all of this and make sure that there wasn't any other news.  And Cisco has.  And I imagine pretty much all of the major players want to make sure that there isn't something that is now becoming publicly known, I mean, and certainly even when it was privately known they would want to fix this, if they were able to.



Many of our listeners sent me news of this problem with Ubiquiti.  The first thing I need to say is that it was only one of the many OSes which Ubiquiti products are based on, and in this case the airOS.  So not the EdgeMAX OS, which our favorite little $49 five-interface router/switch is using.  But the airOS devices did have a problem.  And Ubiquiti, I would argue, did not respond the way they should have.  Toward the end of last November, so about four months ago, a security researcher, Thomas Weber, with SEC Consult in their Vienna office, discovered a command injection flaw in one of the CGI scripts that the airOS web server runs and has available.  That's the pingtest_action.cgi script.



And what was most disappointing, as someone who certainly likes at least the Ubiquiti non-airOS hardware, is that it was caused in part by the fact that those Ubiquiti products are today employing a 20-year-old - 20, I didn't even know that PHP was 20 years old - a 20-year-old version of PHP, v2.0.1, from 1997.  So, and it's in there.  And it turns out that the vulnerability can be exploited by luring an attacked user to click on a crafted link or just to surf to a malicious website because the entire attack can be performed via a single browser GET request because there's no, in that version of PHP, no cross-site request forgery (CSRF) protection.



This allows an attacker to open a port or to open a reverse shell to connect remotely into that device, which would then allow them to change the device's password, since the web service which executes that CGI script on behalf of whomever calls it runs with root privileges.  So even low-privileged read-only user accounts, which are easily created through the web interface, are able to perform the attack.  So Ubiquiti has four OSes.  They have the UniFi OS, the EdgeMAX OS, and the AmpliFi OSes, none of which were affected.  Only the airOS-based products are.  But there's like a long list of those.  So people who have followed our affection for the EdgeRouter X, we're all fine.  The EdgeOS or the EdgeMAX is not affected.  But airOS products are.



Now, there was initially a lot of back-and-forth with Ubiquiti.  Thomas was patient and kept reaching out.  They initially said they were already fixing it.  Then they said the proof of concepts that he had provided were not working.  Then they said, oh, yes, they are, and then it was being fixed.  And they said it would be out soon.  Then he asked them several more times for updates in February and never got any responses.  So finally he just - he went public with the news, but even then out of respect held back the proof of concepts.  That happened on the 16th, and that finally motivated them to release patches on the 18th.  So there's a series of patches available.  Anyone who is using Ubiquiti products with airOS should definitely update their devices to the latest immediately.



LEO:  This doesn't affect our EdgeRouter X, right, though?  This is not...



STEVE:  No, does not, does not.



LEO:  Yeah.



STEVE:  And so this is another example of, first of all, why we desperately need security researchers to be free to do this.  And perfect conduct, I would say, on Thomas's part.  The whole thing would have happened.  Who knows what was going on behind the scenes.  Maybe people changed positions, or somehow the communication faltered.  Maybe they got annoyed with his persistence.  I would argue that he's in a position of leverage.  And then he finally was forced to use the leverage, to let the world know that there was a problem.  Even so, he held back the details that would allow immediate exploitation until they got the patch done.



So anyone with airOS should update.  People with the wired products, as Leo, you just reaffirmed, are fine.  But still, again, we have to do everything we can to protect the system as it is now.  I'm so worried because you can imagine that there will be a lot of pressure on Congress to tweak the legislation, along the lines of the DMCA, to not explicitly exclude responsible research.  We have to have that maintained.  And if we don't, we're in deep trouble.



And here's another example:  Check Point, who as a consequence of their research discovered a vulnerability in the web browser interface for hundreds of millions of WhatsApp and Telegram accounts which allowed hackers, would allow hackers to take over those accounts.  And it wasn't clear to me whether this was found in the wild.  I mean, Check Point generally finds things happening and then figures out what was going on.  And I should have tracked this down more carefully.  But the headline that I wrote said:  "Check Point discloses vulnerability that allowed" - past tense - "hackers to take over hundreds of millions of WhatsApp and Telegram accounts."  But it wasn't clear to me whether they found this and responsibly reported it, or whether it was a zero-day.



On March 7th, a few weeks ago, Check Point discovered and reported to both companies a serious web-side attack which was made possible by the way they were both using the browser.  So of course they have device-side apps.  But as a convenience, they also allowed a browser-based solution.  In both cases of WhatsApp and Telegram, Check Point found a means for bypassing each system's file upload authentication, which would allow a malicious party to essentially send malicious content to the target victim which, if displayed in the browser, could leverage browser vulnerabilities to access the account's protected local store, and thus allow access to sensitive account information.



So what that would allow is that a malicious party could then send a file, for example, an image, which when viewed would immediately compromise the user's WhatsApp or Telegram accounts, giving a remote hacker full access to the user's account and all account data, including their profile, personal photos, chat history, contact lists, and so forth.  The attacker could then resend a malicious image to all of the compromised users' contacts and, in turn, compromise them.



So both companies quickly responded and repaired the vulnerabilities.  And thanks to the fact that this was a web-side attack, basically they were just able to update their web-side code and quickly lock this down.  Check Point's blog post for this, I put the link in the show notes.  If anyone's interested, it has a detailed walkthrough which, you know, they've done full disclosure now because the update was able to be pushed out immediately and fixed.



And for the longest time we've been talking about - for, like, years - the news of the Yahoo breaches, yet very little information was available.  Yahoo was, as we have discussed, very late in disclosing that they knew of any trouble.  They were dragging their heels.  Only after they essentially were found to have had major breaches would they then say, oh, okay, yeah, you're right.  Anyway, so what finally happened is that last week the FBI indicted four individuals, three Russians and one Canadian, with charges of hacking Yahoo, which their forensics demonstrate started back in 2014, so three years ago, which allowed them to obtain access to half a billion Yahoo accounts.



According to the FBI's forensic investigations and subsequent allegations, Yahoo was a persistent target of the attacks that these four individuals launched.  This began with spearphishing attacks, which induced targeted recipients to open malicious documents or click on links taking them to spoofed phishing websites, where, for example, in many cases Google Gmail credentials could be requested and harvested.



The attackers also targeted Yahoo's own employees using these same phishing attacks, social engineering attacks, essentially, eventually obtaining their account credentials; and then leveraged that access, for example, maybe their Yahoo email credentials were reused for Yahoo network access.  So they were able to basically spearphish Yahoo employees and, in instances where their email credentials were being reused - or maybe they spearphished them with like a fake Yahoo employee portal logon, you know.  Because again, as we have been saying, ultimately people, social engineering and people are going to end up being the weakest link, even if we make everything else work perfectly.



So in early 2014, they had achieved deeper access into Yahoo's corporate network.  And throughout 2014 they were doing this, and they got well in by September.  They installed their own software into Yahoo's network to cover their traces and scrub server logs of their activities.  A month later they had obtained information about Yahoo's own, what they call the AMT, the Account Management Tool, which Yahoo administrators use to manage and modify information about accounts, usernames, recovery email addresses, phone numbers, security questions and answers, and more.  So essentially a database of this stuff, the Yahoo user database, known as UDB, the User Database.



A month later, in November, they managed to acquire a backup copy of the entire Yahoo user database, in November of 2014.  That contained information for more than 500 million Yahoo accounts, which gave them access to the account of any user whose password had not been changed since that backup snapshot had been made.  Then, throughout the following two years, through 2015 and 2016, the attackers leveraged their access to this AMT, the Account Management Tool, and the information contained in that stolen UDB database snapshot, to target user accounts of interest.



And in instances where people, for example, government officials had Yahoo accounts and then also government accounts, the crossover was leveraged in order - again, using social information - in order to compromise those users.  Once they got into their Yahoo accounts, for example, if Yahoo was being used for account recovery on any other accounts, where it would be like, oh, I forgot my password, send it to my Yahoo email account, well, it would come into Yahoo email, and these guys who had access to those accounts would be able to get the account recovery information.



So then, as if that wasn't enough, I mean, this was a deep penetration.  They even figured out how to mint their own session cookies to bypass the need even for username and account credentials.  Remember that, as we know, that's sort of the ultimate win is a session hijack.  When a user's logging in, they authenticate their identity, and then the server provides them with a session cookie, which the browser continues to send back during the session, which is how you get that stateful login, how you stay logged in.



Well, if you figure out how to create your own session cookies, you don't need the user's password.  You simply give yourself a cookie to a logged-in session for that user.  They figured out how to do that.  So that then gave them complete, you know, the ability just to spontaneously be logged in to anyone's account who they wanted to.  And then they could, of course, read through that person's email history and obtain any secrets that they needed to.  So initially they were using Yahoo's own servers to mint the cookies sort of in vivo.  Then they did it in vitro.  They figured out how to take that code and run it on their own machines and just mint cookies with abandon.



So the indictment alleges that the attackers used these cookies to access the contents of more than 6,500 Yahoo user accounts, and then of course to later steal contact information from as many as 30 million Yahoo accounts as part of a spam marketing campaign.  The FBI's indictment also notes that the intrusion these guys got was powerful enough to allow one of the attackers, and actually it was this Canadian guy, to modify the results returned from Yahoo searches for a specific erectile dysfunction drug so as to refer to a specific online pharmacy for which the attacker would obtain a referral fee.



LEO:  That's just good business.



STEVE:  And throughout all of this, the FBI has found that Russian intelligence officials proactively assisted the attackers in hiding their Internet traffic and remaining undetected.  So this was all done with knowledge and sanction with the Russian government and its intelligence arm.  So now we know a little bit more about what went on behind the scenes.  And basically an early major web-based email company, as we know from coverage in the news, was deliberately shorting the security pleas of their own security people, downplaying the need for heightened security, up-playing the need for more features.  And this is the consequence of that.



LEO:  That goes on in every company; you know?  I mean, that's the problem; right?



STEVE:  It's a battle.  Yeah, the security people are running around, you know, screaming "wolf" with their hair on fire.



LEO:  And marketing goes, "You can't do that.  Nobody would use our product."



STEVE:  Exactly.  So I got a couple tweets from people regarding eBay "upgrading," in quotes, "a higher level of security" by retiring the football.  And in fact I got a screenshot of the email that was sent.  "We're making two-step verification more convenient," is what they said.  And this was someone, Barry.  It says:  "Hi, Barry.  Thanks for using two-step verification when you sign into eBay.  With your help, we've been able to give your account a higher level of security."  Uh-huh.



"We're going to make two-step verification more convenient by texting you a PIN instead of having you use your token.  All you need is a mobile device."  Of course, you didn't need that if you had the football, but now you do.  "If you use our app, you'll need the latest version of that, too.  Switching to the new two-step verification is as easy as grabbing your mobile device.  Why not do it now?"



LEO:  I'm willing to bet this is timed exactly to coincide with the battery life of those football dongles; right?  Right?  They don't want to replace the football dongle.  I got - that was the first football I got.  It was for PayPal, but it's eBay PayPal; right?



STEVE:  Yeah.  Well, now, what my guess is, is that they just decided - certainly they may have been dying.  They may not have wanted to replace them.  Remember, though, that they were also using a third-party service.  They were using that early VeriSign service.  



LEO:  Oh, yeah, yeah.



STEVE:  And it cost them every single time they did that.



LEO:  Oh.



STEVE:  Yes.



LEO:  That's dopey.



STEVE:  Yes.  And that's why the whole thing, the idea of sending this out to a third-party service, that was in the early days.



LEO:  They had to, probably; right?  I mean, or I guess, I don't know. 



STEVE:  Yeah.  Well, no.  Now, so that's the TOTP system, the time-based one-time password.  And of course our listeners sent this all to me because I had mentioned how I had set myself up with Hover to use their system.  And they gave me the option, send a PIN over text or use the time-based one-time password.  And of course we know that sending a PIN over text is not secure because cell traffic is not secure.  So it's not more secure; it's less secure.  The beauty of using a time-based one-time password is that, when you set it up, you're given a QR code.  You scan that into your authenticator app, and now it's able to autonomously generate a 30-second duration six-digit code which changes.



I did want to take this opportunity to note that because now I'm using it for LastPass, for Hover, and for Gmail, or just in general for Google, my Google identity.  Not one of those apps will export the token information after it's been imported, which is annoying if, like, you use multiple apps.  So what I have taken to do, and this is what I would recommend, is when you set this up with a company, or if this has been a problem for you, you want to reset it up - and I should say that these apps are unwilling to export it, which is probably a good thing because that would be a security vulnerability.  If they allowed you to export the secret key, which is what generates the non-predictable time-based six-digit token, then a bad guy could get those exports and then know your sequence from ever on.



So it's good that they're being secure.  The problem is you could argue there's a threshold where it becomes less secure, if it's so inconvenient for you because you don't have access to your authenticator app, that it causes more problem than it's worth.  So the solution is print those QR codes and keep them.  Which is what I've started to do.  So, for example, when each of those websites said, here's the QR code for your time-based one-time token, I printed that on hard copy and then said, okay, got it.  And now, if I'm setting up a new device, I choose whatever authenticator I like.  And I like Google's authenticator best, only because it gives, you know, lots of people make them now.  There's Auth.  There's LastPass has one.  Google has them.  I like Google's, only because it's more compact because, as I use this more and more, it's easier to see more of them on the screen.  It doesn't have to be huge.



So the idea is that I now keep a set of these printed QR codes.  Obviously I store them securely and safely because I don't want any bad guys to get them.  I also keep them in the same order because I want to show them to the authenticator app in the same order so that it's Hover, Google, LastPass on all of them.  That way that gets to be a habit, too, which is reinforced, in the same way that I always have the keys on my key ring in the same order.  I'm a little strange that way.



LEO:  I just do a screenshot of it, Steve, and put it in my LastPass, and then you have the QR code there, and you can just aim it at that.  But actually I stopped doing that because I now use Authy, which stores the secret in a database, an encrypted database.  Now, it's a little less secure because now there's a central encrypted database.  But I use a long secure password to protect it, and I feel fairly confident that it's secure.  And the nice thing about Authy is you log into your Authy account that uses telephone verification or SMS verification or email to log into it on any device.  It already knows all the secret numbers, so it populates it automatically.  And then before you use it, you encrypt it.  It also has a per-device encryption, if you want to use that.  So I feel like it's fairly - see, I don't know if you've looked at Authy.  But to me that's the easiest way to do it.



STEVE:  Right.  And that's certainly a good choice.



LEO:  It's a little less secure because there's a database online of your [crosstalk].



STEVE:  If you don't mind having a third party having all your tokens.



LEO:  Right, right.  But I just, you know, you don't need to print it out.  Just take a picture of it and put it in your LastPass.  Then you'd have it everywhere.



STEVE:  I'd just rather have it, again, you know, I have a car with no Internet connection, which I prefer.  And I just have hard copies.



LEO:  Yeah.  It's something I had, you know, we may be unusual.  But, I mean, I'm using a new phone every three or four weeks.  So I'm always setting up Authenticator; right?  So Authy for me is about the only way to do it.



STEVE:  Yeah, I agree.  In that case, that's more secure because otherwise you would be disinclined from using that level of authentication.  And I have to say, where we are at this point in the world, this use of a time-based token is the best we can do to augment a username and password.  As everybody knows, I'm working on SQRL, which may or may not gain traction.  We'll see.  And then of course there is the FIDO effort also.  But one way or another, until we get away from usernames and passwords, a time-based token is the solution that makes the most sense.



LEO:  Even NIST has deprecated SMS tokens.  Even the National Institute for Standards and Technology says don't use SMS. 



STEVE:  It's bad.



LEO:  I think it's more because of spoofing than anything else.  It's easy enough to spoof somebody's SIM card, or even just call the carrier and say, "I lost my phone.  Can you send me a new SIM card?"  "Well, who are you?"  "Well, I'm who I say I am."  Okay, it's on its way.  It's that simple.



STEVE:  So we're at about the halfway point.  Our next topic will be the most insane, you cannot make this up if you tried...



LEO:  Steve, you're such a tease.



STEVE:  ...IoT thing.  And believe me, I am not overstating it.



LEO:  All right.  Steve is caffeinated and ready for Part 2.



STEVE:  Okay.



LEO:  Uh-oh.



STEVE:  I am not making this up.



LEO:  He's serious.  He's tenting his hands, folks.  This is serious.



STEVE:  A company named Standard Innovations - and really there's nothing standard about this innovation.



LEO:  Terrible name.



STEVE:  [Standard Innovations], in Canada, manufactures and sells the We-Vibe.



LEO:  Oh, yeah.



STEVE:  A Bluetooth-enabled...



LEO:  I got one of these, by the way.



STEVE:  Okay.  A Bluetooth-enabled, remotely controlled vibrator.



LEO:  For review purposes only, I want to emphasize.



STEVE:  And it's smaller than I thought it would be.  So I guess size doesn't matter.  It itself has no user interface, though some might argue that the entire device is itself a user interface.  In order to be controlled, it's paired with its companion We-Connect app, which allows its user to vary rhythms, patterns, and settings.  Or, and get this, to give a partner in the same location or anywhere in the world control of the device.



LEO:  Again, for research purposes only.  Okay.



STEVE:  Yes.  There is a promotional YouTube video...



LEO:  No, no.



STEVE:  Which is NSFW.  You don't want to let the kids see the opening scene.



LEO:  Oh, how funny.



STEVE:  It's a line diagram.  After that it's okay.  But customers have alleged and complained that the manufacturer was - get this - secretly tracking their use.  And after a class action lawsuit was filed, the manufacturer recently reached a $3.75 million class action settlement with its users, following their allegations that the company was collecting data on when and how and with what settings the toy was used.



LEO:  Well, that's research.



STEVE:  Some would not consider this a toy.



LEO:  That's research.



STEVE:  And their slogan, of course, is "Play together, even when you're apart."



LEO:  Yeah.



STEVE:  Now, this is leveraging the Internet of Things in a new and creative way.  The lawsuit was filed in federal court in Illinois last September, alleging that without customers' knowledge, the app was designed to collect information about how often and with what settings the vibrator was used.  Lawyers for the anonymous plaintiff group contended that the app collected users' email addresses to allow the company to, quote, "link the usage information to specific customer accounts."  I mean, there are just so many things I could say, but I won't.  The suit alleges that customers' email addresses and usage data were transmitted to the company's Canadian servers.



When a We-Vibe was remotely linked to a partner, the connection was described as "secure."  But information was routed through We-Connect and collected by the Canadian servers.  The unhappy users allege in their lawsuit that they never agreed to the collection of this data.  Standard innovations maintains that users, quote, "consented to the conduct alleged."  And of course, you know, maybe in the fine print.  Who knows?  But this is not what people were expecting.



But instead of taking the case to court, the company agreed to settle.  According to court documents, an estimated 300,000 people purchased one of these Bluetooth-enabled We-Vibes.  And under the terms of the settlement, anyone who bought - and Leo, you could cash in here.  Leo, anyone who bought an app-enabled vibrator can receive up to $199.  However, anyone who actually connected it to the app...



LEO:  Which was, oddly, a small fraction of the total buyers.



STEVE:  I know, it was.  Of the 300,000 who purchased it, only 100,000 connected it to the app.



LEO:  Seems like that was the point of the whole thing.



STEVE:  Yeah.  I found that surprising, too.  The good news is those who did, who did connect it to the app, can collect up to $10,000.  So there's a cash proposition.  The actual amount paid out will depend on how many people file claims.  So I guess the settlement amount is fixed.



LEO:  The lawyers make the money, is who makes the money, yeah.



STEVE:  Yes, yes.



LEO:  I think this is a little BS-y.  Because, I mean, really, of course you had an account.  I don't know why you had an account, but you created an account so you could use the vibrator.  I guess so they could know which vibrator to control when you're across the country.  You'd have to identify it.  But what do they care how you - I understand why people are upset.  But wouldn't you think they were really collecting information?  What would they do with it?  I mean, who cares; right?  "Oh, my god, they're having sex."  They're going to sell ads based on that?  Maybe you're going to get those Yahoo emails.



STEVE:  Well, it does sort of bring new meaning to the term  "invasion of privacy."



LEO:  Well, I agree.  And I agree, they probably shouldn't have done that.  But I don't think there was any maliciousness intent.  But anyway.



STEVE:  Yeah.  They probably want to know how long the battery lasts.  So you're collecting a little bit of, you know...



LEO:  You know, lots of other products do that.  I think if it weren't a sex toy, it might have been, I mean, that makes it - now it's very loaded; right?



STEVE:  That's more salacious, yes.



LEO:  Yeah, yeah.



STEVE:  So I will say, though, on the other hand, the fact that it is, and people feel very private about their sex lives, this was, you know...



LEO:  Right.



STEVE:  Yeah.  I don't think...



LEO:  No, I understand.  And they knew, too.  That's why they paid up right away.



STEVE:  Yeah.  So I found this interesting.  Gizmodo...



LEO:  Does give a new meaning to "pen testing."



STEVE:  Oh, boy.



LEO:  I'm sorry I said that.



STEVE:  Okay.  Yeah.  Hopefully people don't know what pen testing...



LEO:  Steve is turning red.  I've never seen you turn red.  This is a first, ladies and gentlemen.



STEVE:  Hopefully people - I think that's just the color on the webcam.  Hopefully people don't know what pen testing is an abbreviation for.  So, wow, that's good.



LEO:  Moving along.  Yeah, good coffee, yeah.



STEVE:  Yeah.  So Gizmodo had the headline, "You Don't Really Need an Anti-Virus App Anymore."  And I thought this was significant.  I mean, this is what we've been talking about and saying now for some time.  Significant, though, that this concept is beginning to enter the mainstream, not just the security group and/or just this podcast, but it's getting some air.  Gizmodo just wrote, they said:  "Ten years ago, the first thing you needed to load on a brand new computer were antivirus and malware applications.  The Internet was a minefield of malicious content that could infect your entire home network with one errant click.  Yet things have changed dramatically.  Windows has much more robust security built in.  Browsers are smarter; and, hopefully, so are users.



"Instead of spending your money on a lot of security software that is often as invasive and irritating as the malware it protects against, think long and hard about going with a minimalist security setup and practicing safe browsing.  Below are the scant few software packages you really need, and where to find them."  And I won't go into detail.  But basically it captures what we have been saying now, here, for some time, that as we know, antivirus is now often increasing the attack surface by interposing less securely written and tested third-party, and I coined a term as I was writing the notes, "shimware."



And of course we know that browsers now, and OSes, are being constantly updated, routinely and proactively, in order to keep them current.  And that many OSes now provide their own built-in AV and firewall software and are incorporating their own proactive technologies like ASLR and DEP, Data Execution Prevention and Address Space Layout Randomization, to essentially harden the OS itself against these attacks.  So it's like many of these technologies have ended up percolating down into the underlying OS.



So again, it's nice to see that this is beginning to go mainstream, this idea that, eh, you know, it's no longer something that you need to worry about to that same degree.  And right on the heels of that, US-CERT has issued an advisory alert with the title "HTTPS Interception Weakens TLS Security."  This, of course, also following on what we have been discussing.  Their advisory says:  "Many organizations use HTTPS interception products for several purposes, including detecting malware that uses HTTPS connections to malicious servers.  The CERT Coordination Center explored the tradeoffs of using HTTPS interception in a blog post called 'The Risks of SSL Inspection.'"



And essentially, again, it restates some of what we've been talking about recently, that we've seen instances where, for example, in not properly validating certificates and in weakening the TLS connection by not fully supporting TLS to the same degree that the client on its end and the server on the other end do, this proxy in the middle ends up creating explicit security problems and breaking the guarantees that TLS was put there to provide.  So the point is that there is a significant degree of responsibility that comes with a device which is going to interpose itself for the purpose of inspecting the contents of encrypted communications.  And what we're seeing is these devices are not yet meeting that responsibility as well as they should.



I got a tweet from someone that reminded me about a question that often comes up relative to using SpinRite with USB devices.  I hope this is not his condition.  His Twitter name is TommyParalyzed.  So Tommy, I hope you're not.  Or maybe it's just with fear and not with a physical disability.  He said:  "Hey, Steve.  Have a USB 500GB HDD.  Do I need a USB-to-SATA converter to plug directly into the motherboard for SpinRite to recognize it?  Thanks."  And the answer is no.  But this is a constant source of confusion.



At some point, and I don't know where, it won't be in 6.1, SpinRite 6.1, the next forthcoming release, because I'm going to rush that out to immediately update it to much higher performance in order to allow it to deal with today's much larger drives in a reasonable speed.  I'm going to, essentially, because it's taken me longer than I expected because of the SQRL project interceding, I'm going to get it done more quickly.  I had always planned to explicitly support USB with a 6 point something after one.  So that will happen.



But today, as many people know, you do not need to wait.  However, you do have to hook the USB drive up when you're booting the system to run SpinRite.  The BIOS looks for connected USB drives when it boots, but it does not see them afterwards.  So the mistake that some people make is they boot their system up, get it running, then they connect the drive, and SpinRite is unable to see it.  In the future, it will be able to enumerate the USB bus, and it will be doing so constantly in order to find new attach announcements of drives.  It cannot do that today.  What it can do, though, is to rely on the BIOS to do that at boot time.  So connect your USB drive, have it up and running, boot the system.  Then SpinRite will see it and will run on it just fine.



Okay.  We talked a couple months ago about the forthcoming Pwn2Own competition and mentioned that it would be the 10th anniversary and would be offering higher, bigger prizes, more money than ever before.  And as they do every year, the competition order was decided by random drawing in the contest room on the first of the three days of competition.  This year's event featured 11 teams of contestants targeting products across four different categories, 30 different hacking attempts in total.  Each contestant was given three attempts within their allotted time to demonstrate their exploit.  And, boy, if anything ever shows us the true - is "porosity" a word?  Porousness?  Porosity.



LEO:  Porosity is right, yeah.



STEVE:  Porosity, the true porosity of security, it's the Pwn2Own competition.  And by upping the stakes, everything was just cut through like a hot knife through butter.  The 360 Security Team used a JPEG 2000, which of course is an image format, heap overflow in Adobe Reader.  Get this.  So a heap overflow in Adobe Reader, a Windows kernel info leak, a remote code execution through an uninitialized buffer in the Windows kernel.  That is all three in conjunction to take down Adobe Reader.  In the process they earned $50,000 and six points toward the Master of Pwn award for the entire three-day competition.



Two researchers targeted Apple Safari with an escalation of root on macOS.  In what was considered a partial success, Sam Gross and Niklas Baumstark earned some points for style by leaving a special message in the touch bar of the Mac.  They used a use after free, a so-called UAF in Safari, combined with three logic bugs and a null pointer dereference to exploit Safari and elevate to root privilege in macOS, earning themselves $28,000 and nine Master of Pwn points.



Tencent Security targeted Microsoft Edge, and they successfully exploited Microsoft Edge through an arbitrary write in ChakraCore.  They used a logic bug to escape the sandbox and earned $80,000 and 10 points toward the Master of Pwn.



Chaitin Security Research Lab attacked Ubuntu Linux, welcoming Ubuntu Linux to Pwn2Own with a Linux kernel heap out-of-bounds access, earning themselves $15,000 and three Master of Pwn points.



Then later in that first day - we're still on Day One - Tencent Security's Team Sniper used an info leak in Adobe Reader, followed by a use after free, to get code execution.  They then leveraged another use after free in the kernel to gain system-level privileges, winning $25,000 and six Master of Pwn points.



Chaitin Security Research Lab came back targeting Apple Safari again with an escalation to root, a different one, in macOS, successfully exploiting Apple Safari to gain root access on macOS by using a total of six bugs in succession in their exploit chain, including an info disclosure in Safari, four different type confusion bugs in the browser, and a use after free in the macOS WindowServer.  This earned them $35,000 and 11 points toward the Master of Pwn.



360 Security targeted Adobe Flash with a system-level escalation and a virtual machine escape.  They elevated to system privilege using four bugs.  They were unable to complete the VMware escape bonus portion, but what they demonstrated constituted a win and netted them $40,000 and 12 Master of Pwn points.



Tencent Security's Team Sniper came back again, successfully exploiting an Adobe Flash UAF, a use after free, escalated to system, and then a UAF in the Windows kernel for $40,000 and 12 points.  A different team at Tencent Security, the Lance Team, successful exploited Microsoft Edge with a use after free in the ChakraCore that elevated to system and used another use after free in the Windows kernel to earn themselves $55,000 and 13 Master of Pwn points.  And this goes on and on and on.



Tencent Security, 360, Tencent, 360, 360, Chaitin Security two more times.  Another one by Tencent, 360, I mean, so basically just a ton of collapses in Safari, in macOS.  In Firefox there was an integer overflow.  There was another problem in macOS with WindowServer, another exploit in Edge.  And, finally, the  Sniper Team at Tencent Security used a three-bug chain to win the Virtual Machines Escape, from guest-to-host category, with a VMware workstation exploit.  They used a Windows kernel use after free, a VMware information leak, and an uninitialized VMware buffer to go from guest to host and win themselves $100,000 and 13 points for the Master of Pwn.



So this just shows us, I mean, I didn't even count these:  one, two, three, four, five, six, seven, eight, nine, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21.  Twenty-one different exploits against browsers and OSes and VMs, all of these unknown, all brand new.  And they ended up taking home a whole lot of money as a consequence of their research.



LEO:  The VM escape seems the most worrying to me because so many people use virtual machines as a way of securely computing because they feel like they're isolated inside that machine.  The fact that you can escape, I mean, first of all, amazing bit of work.



STEVE:  Oh, my lord.



LEO:  Really amazing bit of work.



STEVE:  And exploit chains.  Not just one, but, like, a chain of six exploits in series, yes.



LEO:  It's incredible, yeah.  But that's dramatic.  I can't remember a previous VMware escape.  In fact, I think that that was always the holy grail of Pwn2Own was a virtual machine escape.  So that's a big deal.



STEVE:  Yeah.  The last one was that floppy disk driver bug, remember, that there was the floppy disk driver than nobody was even using anymore.



LEO:  Oh, I remember that, yes, yes.



STEVE:  But it was still there.  And there was, like, there was a state machine in the floppy driver that they were able to use in order to get out into the - back out into the kernel.



LEO:  Yeah.  You've got a better memory than I do.  I'd forgotten about that.



STEVE:  Well, I focus on this stuff, so.



LEO:  It's kind of your thing.



STEVE:  Yeah.  Yeah, so, wow.  So just like that, as a perfect, you know, where we are, this demonstrates that, much as we want this stuff to be secure, and it is to a degree, if somebody is sufficiently motivated, and we know who is, those who have a strong state sponsor behind them on any side of any ocean, who want to get into users' computers, these are very useful tools.  And we're in a cat-and-mouse battle to have these be as secure as they can be.  But the problem is they are just - they've become so complex that you cannot say that they're absolutely secure.



And now there are instances where there isn't even an effort at security.  Which brings us to Oil and Gas International.  And Leo, you should - it's OilandGasInternational.com is the site.  And click on the Login option there, and you'll be taken to a non-HTTPS login page.  This came to the news in the last week because the administrators behind Oil and Gas International...



LEO:  I think they've pulled the site down because nothing's coming up.



STEVE:  Yeah.



LEO:  I think they must have heard about this.



STEVE:  So the problem was that they complained, the Oil and Gas International people complained to Mozilla that Firefox was saying their page was not secure.  And it wasn't.  I mean, there wasn't even an effort at security.  Not only - and I tried to use their page with HTTPS, thinking, well, maybe they have HTTPS, but if you don't use it, then you don't go there.  No.  There's no HTTPS option.  They have no certificate.  Port 443 is not open to the world.  You cannot get an SSL/TLS connection, even if you wanted to.  So on this non-secured page, they offer you to log in.



Now, as we know, if you do, not only are your username and password going to them in the clear because there's no security, but in then being logged in, your browser is sending every subsequent query, along with a cookie to keep you logged in, that anyone looking at your unencrypted traffic can see.  So this is the Firesheep problem that we dispatched with years ago.  These guys are like, oh, you know.  Now, you could say, okay, you don't care about security.  You don't actually care about logging in.  Now, there's more here because, on that form, the login page, very conveniently it says - it asks the question, "Forgot your password?  Click here to receive it via email."



So, yes, it's also one of those sites which is not hashing your password.  Of course they're not because they don't care about security.  They're going to send you your password in email, if you click on the link, so that you're able to then log into this why-even-bother-having-a-login, crazily insecure site.



LEO:  It's not like they're selling anything.  It's just a news site for oil and gas.



STEVE:  Yes, exactly.  So the mistake they made is in complaining to Mozilla that Firefox was correctly saying you're asking people to log in on a site which offers no login security.  That's what Mozilla was saying.  And so their users who were using Firefox saw that it's now a black padlock with a red slash through it.  And then there's a black "I" to the left of that where you click on it, and it says, "This page is not secure."  And so people complained to this Oil and Gas International, who then in turn complained to Mozilla.  And Mozilla said, uh, sorry, that's a feature, not a bug.



LEO:  Somebody's passed along a Tumblr website called Plain Text Offenders which lists sites that will send you your password back in plaintext.  So this is apparently one of many, a great many, a very great many.  



STEVE:  And again, although no one is saying that they have to be secure.  It's like...



LEO:  Right [crosstalk].



STEVE:  Right.  And I would argue that this is exactly what Firefox and Chrome should be doing.



LEO:  Yes, yes.



STEVE:  I mean, this is it.  It's like, okay.



LEO:  Use at your own risk.



STEVE:  Yeah.  You can put your username and password in.  But, boy, this is a password you really don't want to use anywhere else.



LEO:  Yeah, right. 



STEVE:  Yikes.  So Tavis has been busy again.  He was looking through the LastPass web browser code and used something called "beautify" which apparently is the reverse of "minify."  It probably deminifies and then makes it more legible.  Maybe it should be called "legify."  Anyway, it's called "beautify."  And he noticed LastPass was using a particular domain name, a subdomain, or a machine in a subdomain, the numeral 1min, 1min-ui-prod.service.lastpass.com.



And the problem was the way that was being used gave it some privileges such that, with two lines of JavaScript, a user could user that domain to create a window which was privileged to issue remote procedure calls, that is, like subroutine calls, to the subroutines that existed, the hundreds of subroutines that existed within the LastPass browser extension.  Which among other things do things like filling in passwords and filling in forms and copying passwords to the clipboard and so forth.  So that was a bad problem.



He tweeted yesterday in the afternoon:  "Oops, new LastPass bug that affects 4.1.42 on Chrome and Firefox," which is the current newer version of the LastPass web client.  He says:  "RCE [remote code execution], if you use the binary component, otherwise can steal passwords.  Full report on the way."



He also a little bit later tweeted:  "I have a full exploit working without any prompts on Windows.  Could be made to work on other platforms.  Sent details to LastPass."  And then I have a link in the show notes to his posting in the Chromium bug Project Zero blog where he explains this, the "no popup" exploit.  I think it was like four lines of code.  LastPass immediately responded by killing off the DNS for that URL so that it would no longer function.  So they instantly neutered it globally and have a longer term fix on the way.  So they immediately fixed it.



And as you can see in the Chromium blog, in the Project Zero blog, his little blow-by-blow, that they fixed it, and then he went public with the news because immediately, since it was a real-time DNS request that would have to be looked up, killing off that domain name resolution would immediately close the hole.  So again, this is why this has to be kept legal, to allow security researchers to put eyes on, smart eyeballs on this kind of work, and then have the parties that are responsible respond as quickly as they can.  So bravo to everybody involved.



I did get a nice tweet from Lucas, who is at the U.S. Naval Academy.  Remember that they're the guys who did the research on the lack of sufficiently robust randomization, and in some cases completely bypassable randomization, in WiFi passwords, which our devices are primarily Android, but even iOS is a little bit better.  But it, too, can be exploited to create a trackable privacy breach.



So what I first saw was a tweet from Lucas to @SGgrc:  "Thanks for your coverage of our research about MAC address randomization.  You provided the best summary over other media outlets."  And so I wrote back, and I said:  "Thanks very much for your note.  I'm glad I got the details right.  Your work provided a perfect real-world platform for helping to explain the details of what's going on and why this stuff is so difficult to really get right.  And as I said on the podcast, more importantly, your work will doubtless help to drive the improvements our industry needs."



And then he responded:  "I want to add that I started to listen to your podcast about a year ago and dreamed of having something that made the show.  Furthermore, when we published..."



LEO:  Aw, that's neat.



STEVE:  ..."I said to one of my professors, 'How cool would it be if Steve Gibson picked this up on Security Now!?'  We were both ecstatic when we got caught up on episodes and heard the mention of our research.  Thanks again for your coverage."



LEO:  That is nice.



STEVE:  So, very cool.  Happy to be helping to spread the word of great research being done.  And it's what we need.



So I got a tweet from someone who calls himself RealBrainTraining.  He said:  "If an encrypted file is open when my computer enters sleep, the file is open when it wakes."  He says:  "Key in RAM is written to drive.  Security risk?"



Okay.  So, first of all, hopefully the hibernation file is encrypted.  We do know that the earlier versions, for example, of TrueCrypt were not doing that, but later they were.  And  it's definitely the case that you want the whole drive encryption to encrypt your hibernation or your contents of RAM.  Now, maybe if it's just an encrypted file and not the whole system, then it may well be, if you do not have hibernation encrypted, that the key is being written to the drive.



So I would say it's important to understand whether your particular system encrypts the hibernation file with the RAM contents because you're absolutely right that there is an active key in RAM being used to allow you to see the contents of that encryption.  It's also important to go a little bit further, to again recognize what I would call "security perimeters."  That is, if you have something decrypted on your system, while it is decrypted, there's a dynamic vulnerability there.



For example, I recognize that in the design of my SQRL client on Windows.  It uses many different triggers like initiating hibernation or the screen blanker going on or switching users or allowing the user to explicitly do it, an array of triggers, all which wipe, immediately wipe the contents of a secure region which is locked in memory, cannot be swapped out, and which I set up sort of like a sandbox.  I set it up on day one, and every single piece of security-critical information is kept in there so that it's not spread around through the code.  It's in one location, and that means I can trivially wipe it and know that anything that is sensitive is gone.



And so what you really want is you want products which were written from the beginning, especially when they're security related, with these kinds of problems in mind.  But also to recognize that dynamically, when something is decrypted, while it's available, there's a decryption key in RAM.  And if something is in your system, then it's inside your security perimeter and potentially has access to it.  And we've covered in the past how distressingly clever the bad guys have become about finding these keys in memory.  You might just think, oh, it's just some 128 bits, you know, 32 bytes sitting around in memory, out of gigabytes.  How could that possibly be found?  The problem is it's typically surrounded by metadata, and it can be recognized.  I mean, we see many instances of it actually - of 32 bytes out of billions being found with some reliability.



I had a couple questions, a couple people asking about Conway's Game of Life, asking me what version I play.  Several people said several versions are available in the App Store, and of course several are also available in the Google Play Store.  So I just wanted to mention that I haven't played with it at all.  I really got it out of my system back in the '70s when it was new.  I was working with mini computers.  I wrote several instances.  There was a lot of interesting work done on how to make the Game of Life go fast on machines of the time.  So those were sort of the challenges that we had back then.  And I just haven't bothered, I mean, I know it so well that I didn't bother to play with it recently.



But let me encourage anyone who has been intrigued by our mentions of it in the last couple weeks to download - the apps are free.  There are a bunch of them on all different platforms everywhere.  You can get them web-based, Windows and Mac native, and also mobile.  Just, you know, it really is an interesting intellectual endeavor to sort of play with it and see what they do.  The charm, I think, is that it is so simple.  So few rules guide the evolution of this little array, this little grid of cells, and yet it ends up producing such rich results.



And I did have someone ask about purchasing a PC from eBay.  He asks:  "Could the BIOS have malware after wiping the drive?"  And can he scan?  And that's really a good question.  We're in a place where I would argue, you know, we were talking about the idea of purchasing a smartphone from eBay, and that it could have preinstalled malware in the ROM that could bite you, I mean, could have ransomware that could come out and bite you sometime later, or give a bad guy a starting point to attack  you.  How would you know?  We know that there is BIOS-infecting malware.  Yet we're at this weird stage where there isn't yet any, that I'm aware of, useful BIOS scanning antimalware.  The only thing I could suggest you could do is, in the same way that you wipe a drive, would be to wipe the firmware.  That is, find the latest version of firmware and reflash the BIOS on a machine that you're getting from eBay.  You'd want the latest BIOS anyway.  And that would be the best you could do.



I don't know that - the problem is the UEFI BIOSes now, they're an operating system unto themselves, with their own file system and with modularity.  So you might want to do a little research to verify that reflashing the BIOS really does rewrite everything.  The problem is there might be some portion of it that you just can't get to.  I don't know whether that means that it could not be infected.  But I'd be concerned.  I mean, it's an interesting problem.  We know that firmware malware - firmware malware.  There's got to be a way to shorten that.



LEO:  I like firware.  Fralware. 



STEVE:  Yeah, fralware.



LEO:  But it can also live in video cards, which you can't easily rewrite; right?



STEVE:  Right, right.



LEO:  We've seen video card attacks.



STEVE:  Right.  I mean, we're in a sorry place at the moment.  And I don't think it's going to get any better.



And I finally got a tweet from someone in The Netherlands who says:  "In the Netherlands we seem to be getting more and more need" - that's what I wrote, I'm not sure what he meant - "not yet in politics so much, but amongst the people, for more and easier ways to electronically vote."  Oh, more and more need, I see, "not yet in politics so much, but amongst the people, for more and more easier ways to electronically vote reliably, securely, anonymously.  Couldn't something like SQRL be modified to that end?"



And one of the things that I've mentioned before, and this sort of put me in mind of that, is that even though SQRL is designed for two-party authentication, that is, no man in the middle, no requirement for a third party, it does support it, and it is secure.  So it could absolutely be used in a very robust way for a government to manage and control voting because you could go to a voting site, and the SQRL URL doesn't authenticate you to the site.  It authenticates you to a third party, which then authenticates you to the site.  So even though it can be anonymous, there are mechanisms available where you could have your SQRL identity, or a SQRL identity, bound to a nation, to a state that knows you that way, and then could allow you additional flexibility in a secure fashion.  So it remains to be seen how much of those capabilities actually occur.



Oh, and a piece of errata.  Many people noted last week when I was talking about, in fact, I was talking about Ethernet in the context of the U.S. Naval Academy WiFi MAC Randomization failures, about the invention of the Ethernet, and how, back then, how quaint it was that 48-bit MACs were like, oh, that's all we're going to ever need.  We'll give 24 to the manufacturer and 24 to the device of the manufacturer, concatenated to create a unique token.  And I referred to Bob Metcalfe as the inventor of the Ethernet, as he was when he was at Xerox PARC, and then when on to form, and I said 3M.  I meant 3COM.



LEO:  Of course you did, and I apologize for not stopping you.



STEVE:  Yup.  I just - my best friend works at 3M.  He's in the technical side, doing stuff with various of 3M's technical products.



LEO:  Adhesives.



STEVE:  And so I just said 3M.  I'm talking to him about 3M.  It's on my brain all the time.  So it just came out.  I didn't even hear myself say 3COM.



LEO:  3COM, of course.



STEVE:  So stick a CO between the 3 and the M, and you get that right.  Also, I did want to remind people about SquareIt.io.  That's the cute little puzzle toy that I mentioned some months ago.  Several people have tweeted, saying, "Darn it, Steve, you've just sucked up all my free time."  It's really, I just wanted to say, I have continued to enjoy it.  It's not difficult.  It's very clever.  And it's an endless game.  You never run out.  It was written by one of the people who did a different one that we talked about in the past.  I think it was the Infinite Loop guy.  And it's just - it is really nice.



So SquareIt.io.  And it's available both for iOS and Android.  And check it out, if you haven't.  And if you just sort of like having a doodle, it's just sort of a nice doodle, but very well done.  And I wanted to mention also that my favorite show on Syfy, that just stands out apart from everything else on Syfy, "The Expanse," has got its third season.  And, oh, boy.  I read all the books, and I'm up to date on where we are.  There's some amazing stuff about to happen, probably tomorrow evening, or maybe the week after.  So it's looking great.



And lastly, a very nice piece, thoughtful, I thought, published on Medium.com by a guy named Steve Meyers, who focuses on PHP and MySQL.  He gave a presentation on - he focuses on PHP and MySQL design and scalability.  He's with the Utah Open Source Foundation.  He spoke at O'Reilly's OSCON on the topic of database optimization for web designers.  His little short bio says:  "Steve Meyers has worked as a PHP and MySQL scalability expert for the past 15 years at such companies as Omniture, now part of Adobe; Spark Networks, owner of JDate; and CrimeReports.  He now runs some of the largest independent online communities of college sports fans.  When he's not too busy with all of that, Steve runs the Ski PHP Conference, assists with the OpenWest Conference, is a core team member of the Utah Open Source Foundation, and runs the Provo Linux User Group."  So he's active.  He knows what he's talking about.  He's in the middle of all this.



Very nice piece of writing he posted, titled:  "How to fix the problem with malicious and misbehaving ads?"  He wrote:  "In ad circles, I'm known as a 'publisher.'  In simple terms, that means that I serve ads on my site, and the ad networks pay me for serving their ads.  This is a wonderful thing.  In 2011 I was able to quit my day job in order to focus on my website.  I make enough money now that I don't need a 'real job' to pay the bills.  I love that advertisers made it possible for me to work for myself.



"I also hate advertisers.  Unlike many publisher sites, my users tend to stay for a while.  They just don't show up from a Google search and then go their merry way.  Many of them view 50-plus pages per day.  I've set up my AdSense settings so that I will inflict the least annoyance possible on my users.  This includes not allowing ads that autoplay video with audio, ads that expand past their allotted size, ads that pop up, ads that pop under, or anything else likely to annoy my users.  If the ads get bad enough, my users won't come back, and more likely they'll turn on an ad blocker.



"Advertisers and ad networks don't care about any of that.  They're just out to make sure their ads get the most visibility.  I use Google to serve most of my ads, and they make it difficult for me to block ads that are misbehaving.  They have tools that supposedly help with finding and blocking ads, but the tools are rarely useful.  For the last two months I've been trying to block a rash of ads that are driving my users away.  They autoplay video" - and Leo, I've heard you complain about that so much.



LEO:  Drives me nuts.  There's a great Chrome extension to prevent that, but still drives me nuts.



STEVE:  "They autoplay video with audio, which is bad enough.  But in addition to that they force the focus of the window to the ad."



LEO:  Yeah.



STEVE:  "If you try to scroll past the ad, they scroll back.  If you try to click on a form input, they refocus on the ad."



LEO:  Oh, that's terrible.



STEVE:  So you can't even enter content in a form.



LEO:  Oh, that's terrible.



STEVE:  "Since my site is a message board, that's kind of a problem, as it keeps my users from being able to post messages.  You may think that only shady brands would resort to using ads that are so blatantly misbehaving.  The ads I've been trying to block are from Ford, Arby's, and Whataburger.  These are national brands, not no-name companies that don't know any better.  And those are 'respectable' ads.  I haven't even discussed malvertising yet.  Some ads are trying to load viruses onto your computer.  Others will redirect you to the App Store, or another website.  With all this poorly behaved advertising being sent to our users, is it any wonder many of them resort to ad blockers?



"So, how do we fix this?  The answer is actually pretty simple:  browser-enforced content restrictions.  Imagine that, as a publisher, you could specify that the ads on your page would be unable to do certain things.  For example, you could specify that audio, pop-ups, expanders, and redirects are not allowed in a certain DIV tag.  The browser would ensure that any script loaded from that DIV tag would have those capabilities disabled.  Alternatively, the permissions could be denied on the SCRIPT tag.  That would have the same effect.  In order for this to work, it would need to specify what behaviors are not allowed, rather than what is allowed.  This would allow for additional behavior to be supported in the future.



"Do the browsers have any motivation to do this?  Google's main business is advertising, and they're the dominant browser on the market, so perhaps not.  However, I think it makes sense for them to support a standard like this because it's in their interest to keep people from using ad blockers.  Publishers need to be able to control their reputation.  If we want advertising revenue to continue to be a reasonable way for a website to make money, something needs to be done.  Publishers must be able to keep malicious and misbehaving ads off of their sites.  This solution puts the power in the hands of the publishers, where it belongs."  The question is, "Are the browsers willing to do it?"



And just so that people understand, the way the ecosystem works now is a publisher, like this guy, or anyone wanting ad-enhanced pages, creates a region on their page for an ad.  And they include a URL to the ad network, which is essentially blind.  It identifies them, essentially, it's a URL they received as part of their relationship with the ad network.  And so when a user visits their site, the browser gets the site's page content containing this region with a URL.  The browser then goes out and fetches it.  And the point is the page has no control over what happens in that region.  And script that runs in the retrieved text from the ad, that is, essentially it's like a mini page.  It's got script.  It can do anything it wants, that is, in terms of the script-enabled behavior of the code running in that content.



So what this author is proposing, and I think it's brilliant, is that it be possible to create disallowed behavior of scripts running within a region of a page, typically it's a DIV, a division, which the browser would provide as part of - and so the idea would be, the way this would evolve would be the HTML, the World Wide Web Working Group would devise an extension to the HTML5 standard to allow CSS or style to create restrictions on what scripts within a division can do.



LEO:  You just invented AMP, by the way.  You know about Google AMP; right?



STEVE:  No.



LEO:  Well, you should look into it because this is Google's response to this is a publishing platform - it's primarily intended for mobile - but a publishing platform that does exactly this.  It has very limited restrictions.  It's not even within a DIV.  It has very limited restrictions on what you can put in an ad.  It has kind of an approved thing.  And the idea is because mobile - ads become so untenable on mobile.



So if you're browsing around, and you find a news article on Flipboard, or Nuzzle is a good example.  And when you load it, you may have noticed this, you'll see a lightning bolt.  That means it's an AMP page.  And you'll notice it's very clean and loads incredible quickly.  Google caches all the content.  Google has a very limited set of JavaScript that's allowed.  It's a very stripped down thing.



But what I - and I complained about it at first.  But what I do like about it is it resolves in a noncompliant or in a setting where the browser can't understand what's going on, it resolves to legit HTML, even though it's using nonstandard tags for images and things like that.  So Facebook has its own way of doing this.  Google has a way that might be more palatable because it's open, and you can run your own server.  Anyway, I agree with what you're saying.  And Google does, too, apparently, because they offer this as an alternative to publishers.



STEVE:  So I guess we just need to push this out as a standard.



LEO:  It kind of is a standard.  You know, my problem was that they didn't go to W3C and say, let's get this as a standard.  But because it degrades gracefully - you could read more about it at AMPproject.org.



STEVE:  Good, I will, yeah.



LEO:  Yeah.  And I'd be curious next week maybe if you could let us know if this is kind of what you were thinking about.  The real problem I have with it, it's intended for mobile.  But right now more than 1.5 billion pages have been AMPed.  And if you have a WordPress blog, for instance, there's an AMP plugin.  So your stuff is automatically AMPed.  



STEVE:  Nice.



LEO:  And so that's kind of the way to do it.  It still has an ad, you know, people like this guy need ads.



STEVE:  Yes, yes, yes.  And he's not saying he doesn't want them.  He wants them.



LEO:  He wants responsible ads.



STEVE:  Yes, exactly.  He's wanting ads.  Well, for example, an ad that steals the focus so you can't enter contents in a form, well, you know...



LEO:  It's terrible.  It's terrible.  And AMP would not allow that.  I mean, AMP is much more restricted as to what you can do.  It's really - I think it's very interesting.  Clearly Google is just like this guy, saying, well, we don't want people to use ad blockers because our business is advertising.



STEVE:  Yup.  So it's a compromise.



LEO:  So how can we compromise, exactly.



STEVE:  Yup.  Very cool.



LEO:  It's an interesting conversation.  I completely agree.



STEVE:  I'll have a tune-up for next week.



LEO:  A tune-up?



STEVE:  A tune-up on AMP.  I'll know all about it.



LEO:  I'd love to know what you think.  And you can go back to our TWiG conversations.  We've even had Richard Gingras, who's head of Google News, on to debate it.  Because I was kind of against the idea of creating a non-standard extension to HTML. But I gave up when I understood that it would degrade to standard HTML.  So that's good.



STEVE:  Right.



LEO:  That's good.  All right.  Are we done?



STEVE:  We are.



LEO:  Can it be?



STEVE:  Two hours.  A great podcast.



LEO:  Holy cow.  How did that happen?  You'll find this show on the Interwebs.  Actually, why don't you go to GRC.com because not only will you find Security Now! there, audio and transcripts, you'll also find SpinRite, Steve's bread and butter and the world's finest hard drive maintenance and recovery utility, a must-have.  My slogan that I wrote:  If you have a hard drive, you need to have SpinRite.



STEVE:  Thank you.



LEO:  But you also will find so many other free things that he offers there.  It's just a great compendium of information and knowledge:  GRC.com.  We also have, of course, audio and video of the show on our website, TWiT.tv/sn.  You'll find it also everywhere you can find podcasts.  In our 12th year of spreading the security good word; so, you know, it's pretty much everywhere.



And you can watch live.  We do it every Tuesday, 1:30 Pacific, 4:30 Eastern, 20:30 UTC if you'd like to tune in and join us in the chatroom at irc.twit.tv.  We're on YouTube Live at YouTube.com/twit.  We're on Ustream at Ustream.tv/twit.  We're on Twitch, Twitch.tv/twit.  But the easiest would be just simply go to TWiT.tv./live, and then you can pick the stream you prefer.  There's even an audio stream, several audio streams, to make it easy on your bandwidth.  Steve, we'll just have to reconvene next week, I guess.



STEVE:  I think we should.  I remember 12 years ago when we had just started the podcast.  Every so often I would Google Security Now!.  It was like, oh, look, 12 hits.  And now it's just - it's ridiculous.



LEO:  Well over 100,000 listeners every week.  Puts it up there with a small cable channel.  And, by the way, it beats anything TechTV ever did.  Thanks, Steve, and we'll see you next week.



STEVE:  Thanks, my friend.  Talk to you then.  Bye.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#605

DATE:		March 28, 2017

TITLE:		Google vs. Symantec

HOSTS:	Steve Gibson & Jason Howell

SOURCE:	https://media.GRC.com/sn/SN-605.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  This week Steve and Jason discuss:  Google's Tavis Ormandy takes a shower; iOS gets a massive feature and security update; a new target for bot money harvesting appears; Microsoft suffers a rather significant user privacy fail; the U.K. increases its communications decryption rhetoric; a worrisome vote in the U.S. Senate; Nest fails to respond to a researcher's report; this week in IoT nonsense; a fun Quote of the Week; a bit of miscellany; some quickie questions from our listeners; and a close look at the developing drama surrounding Google's enforcement of the Certificate Authority Baseline rules with Symantec.



SHOW TEASE:  It's time for Security Now!.  We've got, of course, Steve Gibson.  I'm not Leo Laporte.  I'm Jason Howell, filling in for Leo while he's gone.  We're going to discuss a lot of really cool stuff today, some of it a little bit on the edge of the scary; but that's okay, that's what it's all about.  The lucrative LastPass shower may sound a little bit more risqu than it actually is.  Apple's mountain of security updates for iOS.  What exactly is the GiftGhostBot?  And Steve goes in-depth on how Google has thrown down the gauntlet with Symantec. Security Now! is next.



JASON HOWELL:  This is Security Now! with Steve Gibson, Episode 605 for Tuesday, March 28th, 2017:  Google vs. Symantec.



It's time for Security Now!.  This is the show where we talk about, well, what else, security, privacy.  I feel like right now this moment in time is ripe for a show like this, and that's obvious because Steve Gibson is here week after week, talking about security items from GRC.com.  Hey, Steve.  How you doing?



STEVE GIBSON:  And everybody realizes this is not Leo's voice that has introduced the podcast.  Jason...



JASON:  Either that or Leo got real good at imitating my voice.  No, I apologize.  Leo is actually out, away on vacation, enjoying, I don't know, I hope he's like on a beach somewhere.  At least that's where I'd be if I were him.



STEVE:  He called it a "mini vacation."  He sort of disappeared.  I only picked up on it when Tony shot me a note last week saying, "Hey, Jason's going to be your co-host next week."  I said, oh, okay.



JASON:  All right-y then.  Away to the beach he goes.  And in his place you get me.  Now, I'm of course on TWiT every day of the weekdays, anyways, doing Tech News Today with Megan Morrone.  We talk about a lot of security topics on that show.  Like I was telling you earlier, you take it to a whole 'nother level.  So again, Steve, I'm super humbled and honored to be on the show with you.  You do such a great job with Security Now!.  Thank you.



STEVE:  Glad to have you with us.  Today we're doing Episode 605.  And the nominal title, as has been the case for the last many months now, we've just been having news-packed podcasts.  And I like to wrap up the podcast with what is maybe arguably the big news of the week.  This one is titled "Google vs. Symantec" because, as I put it in the show notes, Google has dropped the other shoe, in this case on Symantec.  We discussed, I think it was late 2014 when Google discovered some misissued certificates for themselves and for Opera, which had been issued by Symantec or an affiliate or a partner or something.  And that raised some red flags.



Well, what has happened since is that it turns out the problem is way worse than was believed, to the tune of, I'm stepping on the lead here a little bit, but 30,000 misissued certificates.  So Google is doing what Google does.  And they're going to smack Symantec down pretty hard.  Anyway, we will end with that.  But we're going to talk about a very productive shower that Tavis Ormandy, our favorite security researcher from Google had, and the consequences of that, apparently in the late morning last weekend; and, unfortunately how he ended up discovering a huge, apparently very sophisticated, but also a significant problem in LastPass yet again.  LastPass, of course, is on it.



We're going to talk about how iOS yesterday got a massive feature and security update, well, a minimal feature, but massive security update.  There's a new target for bot money harvesting, which has been found going on in the wild, and how that affects us.  Microsoft suffered a rather significant user privacy fail.  The U.K. is increasing its communications encryption rhetoric worrisomely.  Then we had just late last week a vote in the U.S. Senate that has all the privacy people up in arms.  Nest has failed to respond to a security researcher's report.  We've got, as we have been now every week for a while, This Week in IoT Nonsense, another crazy device.  An interesting quote for the week.  A bit of miscellany.  We're going to do some quickie questions from our listeners to keep everybody engaged.



And then, as I said, we'll take a close look at this developing drama surrounding Google's enforcement of the certificate authority baseline requirements, which there's very little question Symantec has violated and is going to be paying the price for.  So I think a great podcast.



JASON:  You know, no big deal, just that few amount of security stories.  Not a big deal at all.  A question I have for you before we kind of move into all this stuff is, I mean, you've been doing Security Now! for quite a few years now.  How many years exactly?	



STEVE:  You know, shortly after the Internet began to really happen, what happened was GRC got its first static Internet connection.  It was an IDSL line, you know, with a block of IPs.  And something caused me to scan the neighborhood surrounding that IP address, our IP addresses.  And what I found was a whole bunch of people's C drives exposed.  That is, other people's Windows machines had file and printer sharing wide open.  I mean, and you could, with issuing a net command, you could map their drive to a drive letter on your own machine and do a directory.  I mean, this is the way it was.  We talk about it like being the Wild West today.  But, oh, my god, back then.



And so what happened was that, first of all, I protected ourselves from that, if we weren't already.  But that was what created ShieldsUP, was I thought, oh, my god, I have to, like, allow people to easily determine whether their C drives, their Windows drive is readable and even writeable by everyone on the Internet.  So I created ShieldsUP to create a simple test where anyone could just go to GRC's web page, and GRC's server would check their IP from outside and show them what was there.



Often people would, like, name their computer with their own name.  So I was able to get their administrative credentials and often greet them and say - it happened to Leo on the original Screen Savers.  Kate Botello found ShieldsUP.  And when you went there, it said, "Hi, Leo," you know, addressing him by name, and then showed you a tree of all of the devices on your machine that were accessible to the Internet.  And as it happened, back then at Tech TV they were behind a corporate firewall, so nothing was visible.  But many people at home, I mean, this even predated NAT routing.  So people just like directly connected their one PC to their ISP's Internet connection and were completely exposed.  So I don't know, what, 15 years ago, I guess?



JASON:  Somewhere around there.  That's fascinating, by the way.  I had not heard the back story, and I love that.  It's like an example of how much things may have changed in some regards, and how much things have actually stayed the same in that that regard.



STEVE:  We're still fighting, but, boy, it's a lot better than it was.



JASON:  Security Now! Episode 1, August 18, 2005.  So right around coming up on 12 years, then, which means you've been doing the show a long time.



STEVE:  Yeah, well, so that's the podcast.  But it was when I was in Toronto with Leo one Friday, I think it was - no, it wasn't a Friday because he had Fridays off.  But so it was one of the earlier days in the week.  We were in between shows, and he said, "So what would you think about doing a weekly podcast on security?"  And I said, "A what cast?"  I'd never heard the term.  He says, "You've never heard of a podcast?"  I said no.  Anyway, so back then I already sort of was involved in security, enough so that that's what Leo thought of me as.  And so that was probably, like, 13 years ago.  And so I'd already established myself.



So maybe it was more like nearly 20 years ago that I really started focusing on this.  I mean, hard drives and SpinRite was - in fact, that's what happened.  When Kate Botello found ShieldsUP, Leo's first reaction was, what, a security product?  I thought Gibson was a hard drive person.  And it was like, well, yeah, okay, you know, that's done.  And so now I'm going to go and do something else because hard drives, I've already figured how hard drives can be recovered.



JASON:  And it turns out there's a ton of overlap between the two anyway, so...



STEVE:  Well, it's technology.  I just love technology, as our listeners know.



JASON:  Yeah.  I guess what I was thinking is there's so - I feel like security news is so fast and heavy right now.  I'm just curious, like 12 years ago, I mean, in the span of 12 years, I have to imagine security, just as a focus for the show, you have so much more to work with now than you ever did.  It feels like a very intense time for security right now, I guess is what I'm saying.



STEVE:  Yeah.  I mean, like Facebook didn't exist.  And so, for example, [dropout].  And my family, they had no Internet presence.  I had one because I was a web guy with a server and a website.  But so think about what it means now that the world has their own content on the Internet, thanks to Facebook making it easy, basically, sort of to roll your own little personalized website.  So, I mean, there's so much more individual engagement today than there was decades ago when, I mean, yes, people had email.  Mom had, as she calls it, AWOL.  I said, no, there's no "W" in there, Mom, it's just AOL.



JASON:  Although kind of fitting in some ways.



STEVE:  Right.  So email was like everyone's first contact.  And they were consumers, but they weren't producers.  Now we have blogs.  We have Facebook.  So there's so much more engagement.  And of course we still haven't really figured out how to do security right, which is why we're in our 12th year and have no sign of running out of things to talk about.



JASON:  That's right.  We've got a lot of those things that Steve is talking about.  All right.  I guess we're getting right into the news.  I noticed at the top of it, this is totally applicable to me.  I am a very passionate and happy user of LastPass.  And I've been away, by the way, I've been away on a vacation for like the past week.  And because I was out of the country, I didn't have mobile Internet, so I was relatively offline, more or less, a few bits here and there, but not the way I am when I'm here in the states.  So I had really missed a lot of what had happened around LastPass.  I guess catch us up to date.  Where are we at?



STEVE:  Well, we will.  I did want to mention our Picture of the Week first.



JASON:  Oh, yes, absolutely.



STEVE:  We always have sort of a fun thing that typically our listeners find.  This one shows your classic boxy head, with antennas sticking out of his head, robot sitting behind a desk, reading a book whose title is "You CAN" - underlined and bold - "Pass the Turing Test!"  And of course now the Turing test is the famous sort of thought experiment that Alan Turing developed back in 1950.  The idea was - so, you know, of course he was famous for an early pioneer in computers.  And so the Turing test suggests that the challenge you give a computer is for interaction with it to be indistinguishable from that with an actual human.  That is, sort of to prove it's intelligent.



And so the idea would be it would be a blinded test where, for example, a person giving the test to an unknown entity wouldn't be able to see them because otherwise you could tell it's a boxy head robot.  But, for example, the only communication would be through a terminal, for example.  And so you ask questions, and then this entity which you can't see responds.  And so the question is, can you see any sign that this is a nonhuman, based only upon its responses?



So that was sort of the concept that Alan Turing first proposed was, if all we know is what this thing types, can we say is this a human or a robot?  And in essence, a CAPTCHA is sort of that.  It's I'm a robot; I'm not a robot.  The idea being the whole concept, the whole reason for a CAPTCHA was to allow a website to differentiate between a bot that might be visiting and, like, for example, trying to create a new account, or log in as a person or whatever, versus an actual human.  So CAPTCHAs are a practical form of Turing test.



So anyway, this picture ties into a fun Quip of the Week that a listener of ours, Chris Schrimsher, provided.  And he was quoting someone on Reddit who said, and I love this, he said:  "I'm not scared of a computer than can pass the Turing test.  I'm terrified of the one that intentionally fails it."  Which I thought - I got a kick out of that, the idea that the computer's like, okay, I'm going to pretend I'm not intelligent, so you underestimate me.  And we'll go from there.



JASON:  Yeah, that's kind of terrifying.  When I read that quote, I was like, oh, man, I hadn't thought of that.  Great, another thing to be worried about.  Excellent.  There's probably a really good reason that it might want to do that at some point.



STEVE:  Exactly.  So, yeah, exactly.  Skynet doesn't want to let you know that it has become intelligent until it's already secured all the assets that it needs, and then it will spring that on you all at once.  



JASON:  It's like a reverse-Turing test.



STEVE:  Exactly.



JASON:  Or quasi.



STEVE:  Being smart enough to deliberately fake you out and fail the test because it's not yet ready to reveal itself.  So, yeah, that'd be a little worrisome.



JASON:  There's no reason to expect that this won't happen at some point, and there will be a really good reason for it.  I've wondered if at some point we're going to get to the stage where there will be social networks specifically for robots, essentially, robots communicating with each other.  And I wonder if, at that point, they will be able to identify on a large scale whether they are in fact talking to other robots, or they are talking to humans.



STEVE:  Well, you know the smart...



JASON:  I don't know what the purpose would be for that, but there you go.



STEVE:  A commonly asked question of smart people like Bill Gates and - I can't think of any other smart - Alan Kay.



JASON:  Bill Gates is pretty smart.  He's good enough.



STEVE:  Dean Kamen and, I mean, like real thinking people, is are you worried about this, like, explosion in AI?  Like could Skynet happen?  And the more well-informed these people are, the more worried they are.  I mean, it's not like it's 50-50, or we're not sure, or no, probably not.  But, I mean, the guys that understand are, like, yes, we need to be paying attention.  Because famously we tend not to.  I mean, the whole history of security and the Internet is one of not paying sufficient attention.  The good news is the damage is well distributed.



For the longest time viruses were just annoyances.  They only existed for the sake of their own existence, rather than do anything.  Now, of course, we have things like cryptomalware that encrypts all your files and demands payment.  So, I mean, and this was a foreseeable development.  Well, let's hope that at some point in the future we're not looking back on how quaint it was when computers were word processors, and now they're our masters, because I'm one of the people who thinks, yeah, that's, you know, we're not that special that it's not impossible to, I mean, look at Watson winning, you know, chess is not anything that people can play anymore.  And Watson beat the best "Jeopardy" people.  I mean, I couldn't.  So it's like, okay, that's a little worrisome.



Anyway, Tavis Ormandy, our favorite security researcher and penetration tester at Google apparently does some of his best thinking in the shower.  On 12:20 in the afternoon last Saturday, March 25th, he tweeted, apparently shortly after toweling himself off, he said, "Aha.  I had an epiphany in the shower this morning and realized how to get code execution in LastPass 4.1.43.  Full report and exploit on the way."



So, okay.  So the version 4 - and, by the way, this is Chrome and Firefox.  It's the most recent version of the browser extension for LastPass.  The 3 version they are in the process of deprecating.  And I would imagine most LastPass users would have already moved to 4 and be current.  LastPass responded by 5:00 p.m. on Saturday, so they were paying attention:  "Our team is currently investigating a new report by Tavis Ormandy and will update our community when we have more details." And then Monday, yesterday, they put up a formal blog posting saying:  "Over the weekend, Google researcher Tavis Ormandy reported a new client-side vulnerability in the LastPass browser extension.  We are now actively addressing the vulnerability.  This attack is unique and highly sophisticated."



Oh, and I'll just interject here that Tavis has since said there's a fundamental architectural problem, that is, apparently this not a buffer overrun where you go, oops, we forgot to check for null or the length of a buffer.  This is something apparently very sophisticated, but also that's going to require some reengineering.  Tavis has given them 90 days and said don't rush this because, first of all, no one's providing any additional information.  Nobody has any idea what this is.  So this is being kept very close and quiet while LastPass addresses this.



And then they continue in their blog post:  "We don't want to disclose anything specific about the vulnerability or our fix that could reveal anything to less sophisticated" - than Tavis, but I added that - "but nefarious parties.  So you can expect a more detailed post mortem once this work is complete."  So anyway, so again, this is the kind of thing we want to have happening in our industry.  One of the common things, Jason, that we've developed in the podcast over the last couple years, really just from looking at the reality of what we keep finding, is that though we don't want it to be, security is porous.



In last week's podcast we ran through an enumeration of the recent Pwn2Own competition at the CanSecWest conference that was also last week, where basically every browser and OS and even the VMware virtual machine system was just cut through.  The ante has been upped a lot.  In some cases it required a chain of individual vulnerabilities in order to leverage that into a full exploit.  So it's not like one problem was found.  It took five or six, all used in sequence, in order to make this happen.  So it is difficult.  But the point was that when researchers were incented by sufficient prize money, in this case a million dollars aggregate for all of the breaches, and generally they were getting 25, 50, $100,000, just that incentivization was enough to cause them to look closely enough to find problems.  So of course all of those problems were exploited, but then fixed.



So from my standpoint, this is only good because we would rather have Tavis find it in the shower and have LastPass fix it on the QT, so that we get the update.  And then LastPass will tell us, and Tavis will tell us, and we'll figure out what it was that he came up with.  But nobody else.  And we would rather have that happen than for somebody else to figure this out independently and be silently and quietly exploiting this without us knowing.  So the modern model of security is we need researchers to be permitted to tear into these products as deeply as they need to, to responsibly disclose what they find, and then for the party responsible to respond immediately.



And what we have seen over and over again - and, I mean, so, yes, unfortunately it is over and over again with LastPass.  But they are always responding within hours.  And it's funny because the previous problem that Tavis found was resolved within hours of him informing them.  And he had started his 90-day counter, and it didn't even count down one day before this was fixed.  So that's what we want to see.  And we'll have a story that we'll be talking about a little bit later on in this podcast about Nest and how they have done just the opposite, how they ignored a report from a researcher, after confirming it, for four months, and just said, eh, you know, okay, we'll get around to it.  And then the researcher finally said, okay, sorry.  I've given you four months.  So I couldn't ask for anything better from LastPass.



And there was a comical tweet that followed this that someone sent to me, saying "We can all agree that Tavis is the problem with infosec.  And if he'd just stop finding bugs constantly, then cybers would be secure."  And of course he was joking because it is Tavis continually finding problems in all kinds of different systems which is increasing their security.  And somebody else sent me a tweet saying "LastPass:  Security done wrong."  And of course that's not what I think.  I've got LastPass browser extension loaded onto all of my systems.  It's what I use.  I would rather be using something which is, as a consequence of its popularity, has a history of being attacked and made more solid as a consequence, than some unknown, lesser product which no one has bothered to take a close look at because not enough people use it to incentivize anyone to look at it closely.



So all this is doing, I mean, these problems are the consequence of someone looking harder at the product they want to make more secure.  Clearly that's Tavis's goal.  That's why he's thinking about this in the shower is to make this the best product he can.  And he is.



Now, I should mention that this morning I had a dialogue with someone on Twitter, asking about KeePass, which is a non-browser extension, multiplatform, encrypted database used for keeping passwords.  And I've not talked about it because I've been so happy and still am with LastPass and the job that they've been doing and how responsive they are.  But the problem that LastPass has, and any browser-integrated password manager will have, is that the browser is like the hardest thing to secure.  In fact, we'll be talking about the - in fact, we'll be talking next about this iOS version 10.3 update that we got yesterday.  And I enumerate the security fixes in it.  And not surprisingly, WebKit, which is the Safari browser core, by far, well, the kernel was number two, but WebKit was number one in the most number of problems.



I mean, we're really asking browsers to do something technically they weren't designed to do.  They were designed - originally the concept was you'd have static content, and you'd use the browser to browse.  That's why it's called a "browser."  You use it to browse the web.  Well, now they've become application containers.  And they're highly complex application containers that are struggling to do that in a secure fashion.  So if you want integration of your password manager, I still think LastPass is, without question, the most secure solution.  If you want more security, the only way to get that, at the cost of convenience, is not to integrate in any way your passwords with the browser.



Now, unfortunately, a complex password is really something that you're going to have to stick on the clipboard in order to move it from your password manager database into the password field of the browser.  And so that represents a little bit of danger.  But if the password vault wipes the clipboard proactively, or if you make the point of overriding it, then you're okay.  So but if you decide that you don't need browser integration as much as you need more security than a browser can provide, then I think KeePass is a great solution.  The crypto is done right.  It's multiplatform.  You can run it on iOS and Android and Windows and Linux and Mac.  It's everywhere.  It's K-E-E-P-A-S-S.  Again, you have then the responsibility for synchronizing and managing and also copying and pasting the passwords which it has decrypted into your browser.  So you have to do more work, but then you do have the benefit of nothing being in the cloud and there not being this vulnerability.



I have no idea what Travis found.  No one does except he and the LastPass guys.  But whatever it was, it was probably exquisitely subtle.  And I think we'll have fun talking about it once we find out.  But so as a consequence I don't think LastPass is security done wrong, obviously.  I think they are doing it as well as they can, given the inherent tradeoff of wanting the convenience of a password manager integrated into the browser.  Which was always a challenging thing to do in a secure fashion.



So again, it's like, yes, Tavis is finding problems.  They're fixing them often.  I mean, it's getting their attention instantly.  In some cases an instant is all it takes to fix a URL, which was the problem last week, where they were able to fix that essentially on the fly, instantly solve that problem.  This one is going to take more time.  But again, they immediately turn themselves to the work of fixing it.  And I think that's all anyone could ask for.  It's all I'm asking for.



JASON:  Absolutely.



STEVE:  Until we completely get rid of passwords, which is what I'm working on.



JASON:  Well, thank you for that because we need it.  Yeah, I guess the big challenge, because I hear all the time, like I've recommended LastPass personally because I feel like the tradeoff of convenience and security is worth it for me, right, to have all of my passwords scrambled and long and everything, and let that deal with it.  Sure, there are potential issues; there are potential security exploits that might happen.



I think the big fear people have commonly is, well, it's one place that stores all of your passwords.  So if they get it wrong once, if they get it wrong once in a very critical way, there goes your trust in everything you have stored there.  The challenge is you never really, I mean, in the case of this, you never really know if the hacker also had a similar shower and came up with a similar thought in the shower, or not.



STEVE:  That's true.  Yup, that is, you know.  And in fact the problem we always face is you can't prove a negative.  So, for example, when the CIA has with Vault 7 all of these exploits they've been keeping to themselves, the problem is many of them are, like, zero-day attacks.  Well, we don't know that other people didn't independently figure those out and aren't using them, too.  So it's one thing for a bad guy to keep them to themselves.  A good guy like Tavis doesn't.  The moment he's dry, he goes and sends a note to LastPass, with whom he now has a dialogue open, and says, hey, I found something else you guys are going to have to take a look at, sends them a proof of concept after he verifies it, and then they're on the project.



The problem is here's the CIA is sort of in between that.  They're not wanting to give up their vulnerabilities that they find the way Tavis does.  They're not, you could argue, they're not a malicious attacker; yet they're wanting to be able to leverage these for their own intelligence-gathering purposes.  So they're really in a dicey sort of gray zone.



JASON:  For sure.  So you mentioned the iOS 10.3 update, which I heard of, with the new file system, is making things faster, some AirPod, Find My AirPods, CarPlay improvements.  Of course, you zeroed in on all the security improvements, of which there were a lot.  Tell us a little bit about that.



STEVE:  Oh, my lord.  Well, yeah.  So the first I knew about it was that Apple has had something called HFS, the Hierarchical File System, and HFS+, which is old.  And so one of the things that they did with 10.3 is they updated to a much more modern recent file system architecture.  Maybe users will see a little, like their storage requirements drop a little bit because it takes advantage of all the latest thinking in the way you arrange and store files.  But there were also, in 10.3, a ton of security fixes.



And in fact I enumerated them just sort of for giggles here in the show notes.  But, I mean, it's like, okay, how many pages of this?  Let's see, one, two, three, four, five, six, seven - seven pages with, like, many things per page.  And so I don't want to drag everyone through it.  But here again is another example of the reality of today's security.  There's no question Apple is as security focused as any company in the industry, I would say on a par with Google; and I give Google lots of props for the level of their security focus.  In the same way that Tavis is thinking about this 24/7, clearly, and Google's Project Zero, and they're finding problems all over the place.



Well, Apple's focus, of course, is their own stuff.  But they make a big point, I mean, security is a selling point, is a marketing feature for them, so they really care about it.  So they fixed a problem, or found and fixed a problem in the Audio system, in the Carbon subsystem, two problems in Core Graphics, three problems in the Core Text system.  And, for example, in Core Text, processing a maliciously crafted font file may lead to arbitrary code execution.  Okay, now, that's one of the things that we've been discussing also.  We've seen malicious images.  We've seen malicious documents.  Any time you are interpreting something, the interpreter has to also be perfect.



And so here font files are now, they're not just static data representations.  They are interpreted things.  And so you can make one that is deliberately malicious that will leverage a mistake or some bounds-checking lack in the thing that's doing the interpretation.  So in this case a maliciously crafted font file could cause a memory corruption issue.  And they fixed it by improving the input validation, that is, they found where somebody could do this, and so they added a check to make sure you could no longer do that.



And actually, so there were two instances of maliciously crafted font files and then a maliciously crafted text message, which leveraged some failure in their text message handling in the same fashion in this Core Text module.  There was a problem in the Data Access module.  There's a Font Parser that had three parsing problems.  HomeKit had a problem.  It said Home Control may unexpectedly appear on Control Center, and they said a state issue existed in the handling of Home Control.  This issue was addressed through improved validation.



HTTP Protocol had a problem.  The Image IO processing, in fact, here it is:  "Viewing a maliciously crafted JPEG file may lead to arbitrary code execution."  So we've already had some of those, and they're still having these problems.  They had four problems in Image IO.  The iTunes Store had a problem with an attacker in a privileged network position might be able to tamper with network traffic.  The kernel had one, two, three, four, five, six, seven, eight problems that were found involving code execution.  One, two, three, four, five, six, seven, eight - actually all of them were arbitrary code execution, which you don't want.  So they fixed those.



Keyboard subsystem had a problem.  The Libarchive, Libc++ application binding, the ABI had a problem.  Pasteboard had one.  Phone, Profiles, Quick Look.  Safari had four.  The Safari Reader had one.  Safari View Controller.  Under the Security module they have four.  Siri had a problem.  And then WebKit, as I said earlier, one, two, three, four, five, six, seven, eight, nine, 10, 11, 12, 13 problems.  Oh, no, 14, 15, 16 - it continues on the next page - 17 problems.  Because web stuff, I mean, a web browser is the massive nightmare interpreter of all time.  It's interpreting HTML.  It's interpreting CSS.  It's interpreting JavaScript.  I mean, it is a security nightmare.



And so here, mature as Apple's iOS is at version 10.3, or previously at 10.2 point whatever it was, even so, this far downstream there's still this much being fixed.  And so all of what we see in the actual way security works says that today the best we can do is to try not to introduce new bugs as the existing ones are found and eradicated.  And of course this is why I'm sitting in front of Windows XP SP3.  That's the system I'm still using.  I've got a system built for Windows 7, but I have a lot of life left in me.  There's no way I'm going to 10 because new code also has new problems.



And I'm much more comfortable using something that is time tested and well proven than anything brand new out of the box.  And XP was famously a security disaster for many years.  Steve Ballmer was prancing around on the stage of Microsoft saying Windows XP was the most secure operating system Microsoft will ever be introducing.  And of course that's not something you can ever say beforehand.  The only way you can make any assertions is looking back and seeing how it fared.  And it was a disaster in the beginning.  So all of those early Code Red and Nimda and MSBlast worms, all those worms were Windows XP worms that caused huge problems for the Internet.  It was like, whoops, well, you know.  So it took time to get it fixed.



So again, this shows, with 10.3, how many problems there still are.  I would argue iOS - and we were covering just recently that iOS, due to the tighter curation and the tighter environment that Apple has created, while it offers its users far less choice than the Android platform, because Apple has as much control over it as they do, it is demonstrably more secure probably than any other operating environment on the Internet today.  Even so, it's still having problems found and fixed.  All we can hope for is that the rate of creating new problems is substantially lower than the rate at which we're finding and fixing the old ones.



JASON:  As an Android user - I do a show about Android on this network.



STEVE:  Of course.



JASON:  It's called All About Android.  You should check it out.  I mean, that's one of the main things that I am very envious of on iOS.  And I think most Android users would probably agree, even though I'm using, you know, I've got the Google Pixel phone, so it's Google's phone, which means that they're going to do a great job of keeping this updated with their updates as they happen.  The majority of Android users don't enjoy that benefit.  And, man, if iOS, like, is this normal for Apple to have this security rich of an update?  I was looking through, I was like, man, does this happen like every time they update?  And I just had to go back and think about what we're not getting on Android from security updates.



STEVE:  This was a big one.  Yeah, this was a big one.  You know, we've seen a bunch of little double-point fixes, like 10.2 point something, and point something, and point something.  This feels like something they've been working on for a while.  And so they just said, okay, we're ready to go, and they dumped all these out.  And what we want is for them to be responsibly reported and then fixed quickly and then pushed out to us.  And so that's what Apple's been doing.  So yay for them.



JASON:  Okay.  Apparently a bunch of sites shouldn't look a GiftGhostBot in the mouth.  I prewrote that.  I hope you appreciate that.



STEVE:  Who was it who was asked, "Why do you rob banks?"  And the answer was, "Well, because that's where the money is?"



JASON:  I don't know, but they're brilliant.



STEVE:  Yeah, I always loved that line.  But so it shouldn't surprise anyone that, as we were mentioning before, cryptomalware has happened because, if you encrypt people's files, you can extort them for getting their files back.  So another place where there's money is in online gift cards.  So a company, Distil Networks, based in San Francisco, is in the position of monitoring the traffic of lots of websites.  And they observed a spike in traffic that immediately set off alarms and came to their attention.



What they found was a newly observed bot pounding on the websites of nearly a thousand companies offering gift cards which allow legitimate users to check their balances.  So someone gives you a gift card for Amazon, for example, or for Domino's Pizza, who knows.  And so often those sites have a page where you're able to turn the card over and enter in the gift card number and look up the balance that has been electronically transferred or assigned to that card.  And armed with that, because that's essentially the gift card number is the only authentication needed, because the idea is someone gives it to you and says here's, you know, 50 bucks on a gift card.  Spend it on pizza.  And so that's what you do.



Well, unfortunately, once again, we attack where the money is in cyberwarfare.  And so there is now something that has been called the GiftGhostBot, which is pounding on gift card supporting or issuing websites, brute-forcing millions of gift card account numbers per hour.  In one case, this Distil Network monitored more than 4,000 guesses in brute-forcing in the course of one hour, and so determining the card number, getting the balance available on the card.  And then, if found, the cards can be used and are used.  Essentially, the balances are drained one way or the other, used to purchase goods at the site; or, in aggregate, bunches of the card data is sold on the dark web to people who then resell them in order to drain the balances.



So for a cyber thief, the beauty of stealing money from gift cards is that they are inherently anonymous, and it's untraceable transactions once they've been stolen.  So the advice to people who may have received gift cards without standing balances is, if you can, not leave the money unused because the whole point of this GiftGhostBot is to pound on the website, crack the card number that is yours, find a balance, and then either use it immediately or send it off somewhere to be used by other crooks somewhere.



So the problem, of course, is that there had been little protection for this.  In response to this attack, companies are starting to take their easy lookup, you know, look up your balance pages offline in order to prevent bots from abusing them.  And so you may have to now make a phone call in order to check your balance.  At the same time, if that page is offline, then it's likely that the bot can't get it.  So what you may do, if you have any - if you know you have any gift cards you haven't used, is just go see whether you're able to check the balance online.  If you are, that's a problem.  You want to make sure you still have a balance and that the bot hasn't already drained it from you without you suspecting it.  And, if so, you probably want to do that yourself before the bot can.



Retailers will likely install CAPTCHAs.  This is someplace they should have put CAPTCHAs before, where, like, for example, all the other sites where you can create accounts and are doing something sensitive which had been previously subject to bot attacks have done so.  Now it's going to be the gift card sites that do that.



And I should mention also this is a pretty sophisticated bot campaign.  The bot itself masquerades its queries by rotating among more than 740 different user-agent simulations.  So it just doesn't look like the same thing making a web query every time.  It's captured at least 740 different profiles of ways browsers can query, and it chooses them and rotates among them at random so it doesn't look like a bot repeatedly doing the same thing.  It's widely and heavily distributed across various hosting providers and datacenters throughout the world.  So it's not just coming from one IP that would be easy to block.  And it's able to execute JavaScript, which it receives from the website, in its pseudobot client in order to fully appear like an actual web browser that a user is using.  So, I mean, so this is a sophisticated piece of work because apparently there are a lot of people with outstanding balances on gift cards, and this thing's just going to go out and find them all.



JASON:  Yeah, you get one of those gift cards, I mean, there's no urgency whatsoever. 



STEVE:  No.



JASON:  I discovered one not too long ago.  And I was like, wait a minute, I don't even remember getting this.  You run it through, it's got a balance, excellent.  So one question I have about this.  If it's already happened, and it's untraceable because gift cards are very anonymous, as you say, how does the merchant know that a customer that happened to is telling the truth when they say, hey, I have no balance?



STEVE:  You are exactly right.  And the problem is none of these cards have fraud protection, unless Visa and MasterCard and American Express and so forth.  The major credit cards will all hold you harmless in the event of fraud against your card.  But you're exactly right, there is no way for you to prove to the site that you didn't get the cash.  And the site will say, hey, sorry, we're out the money.  We gave it to somebody.  You're claiming it's not you.  First of all, you have no way of proving it's not you.  And so the loss is distributed among all the people who are holding gift cards with nonzero balances when they're zeroed.  Basically it's like a debit card where your money is stolen from your account, and it's gone, unlike a credit card where you are protected.



JASON:  Wow.



STEVE:  Yeah.



JASON:  Basically this was a little more complex than a war dialer.  Just a little bit.



STEVE:  Yeah.



JASON:  This is like a war dialer on steroids.



STEVE:  Yeah, war dialer was quaint.  What a quaint idea.



JASON:  Exactly.



STEVE:  So what's not quaint is what you're just about to take us into, Jason, is Microsoft's very disturbing default settings for documents uploaded to its Docs.com site, D-O-C-S dot com.  Over the weekend, users were very upset to discover that documents they had uploaded to Docs.com were publicly visible and searchable through the Docs.com search.  And to this day I don't understand what Microsoft was thinking.  If you use Docs.com, which you're able to to connect with Office 365, as your iCloud repository for Word and Excel and other similar documents, those are private by default.  You can choose to share them publicly, but you have to do that deliberately.  Inexplicably, if you upload documents to just store them there, too, they are public by default.



And in the show notes here I have a screenshot of the settings page for a user.  And in the third section under "Content I Like," there's one checkbox that says "Allow everyone to see documents and collections you like."  And it's enabled by default.  So this wasn't a mistake.  This was what Microsoft thought people wanted.  And the problem was, I mean, it's not even clear to me what Microsoft's actual policy is on this because they immediately reacted by removing the Search bar in order to presumably protect people from being able to use that in order to search.  The problem is, they were publicly posted.  So Google and other search engines all sucked them in and archived them.



I mean, so like these things are, you know, the famous expression is "Once you put it on the Internet, you can never get it back."  And so this happened by mistake.  In the various coverage of this over the weekend, some news outlets looked at - and also the original discoverers of this looked at what was there and found, for example, a list of maintenance logins and passwords for a number of devices, including metal detectors and other security devices; a list of names, addresses, Social Security numbers, bank account numbers, email addresses, and phone numbers, apparently passed to a debt collector on behalf of a number of payday loan and finance companies; medical data, including one physician's treatment logs and photos, as well as credentials for logging into medical records systems; a new employee enrollment document with instructions on how to connect to a corporate Intranet gateway for the first time, with default username and password information; employment acceptance letters; investment portfolios; divorce settlement agreements - I mean, you can't make this stuff up.  Credit card statements.  Files containing - multiple files containing login and password information, saved as Word documents.  So major privacy breach.



And in response to this, Microsoft says, officially:  "Docs.com lets customers showcase and share their documents with the world."  Uh-huh.  Yes, indeed.  They said:  "As part of our commitment to protect customers, we're taking steps to help those who may have inadvertently published documents with sensitive information."  Gee, I wonder how that happened?  "Customers can review and update their settings by logging into their account at www.docs.com."  Wow.



JASON:  I mean, I'm looking over the FAQ, and I'm looking at kind of the landing page for Docs.com.  The landing page says to upload your documents.  "Later, you can choose who may view your documents," which tells me that there will be some sort of element or step in there...



STEVE:  Proactive.  Proactive.



JASON:  ...before it kind of goes out to everyone.  But then when you actually dive into the FAQ, you know, it repeatedly says, like, this is about getting your work noticed for a broader audience, posting it publicly for everyone to see.  It seems to me like, I mean, maybe a service like Docs.com that's all about publishing documents for the public to see deserves to exist.  There just needs to be better education on Microsoft's part to say this is what it's for.  You might want to think twice about sharing certain types of documents because it's going out to everyone by default.



STEVE:  Yeah, it's called a blog.  No one thought they were blogging their employment contract.



JASON:  That's true, obviously.



STEVE:  When they uploaded it.



JASON:  Yeah.



STEVE:  Oh, goodness.  Just a bonehead move.  Oh, boy.  Just crazy.  Anyway, so the bad news is that stuff is out there.  The good news is hopefully, you know, Microsoft is being as proactive as possible.  But once the search engines index it, you can't ever get it back, essentially.



JASON:  Yup, yup.



STEVE:  Wow.



JASON:  Okay.  So I've been very curious to hear your thoughts on the next couple of stories.  We have, of course, last week's terror attack in London, which, horrible event, I think everyone would agree.  In kind of response, the U.K. government's stepping up its efforts to, well, I guess weaken or break or give us some sort of a backdoor into encrypted communications.  Tell us a little bit about this.



STEVE:  Yeah.  And of course just last week we had the four people killed in Westminster, which again reamplified this.  So the U.K. government is once again rhetorically upping the rhetoric, pushing for access to all encrypted communications and has singled out WhatsApp specifically.  Or as ZDNet phrased it in their coverage:  "The U.K. government is gathering itself for an assault on end-to-end encrypted messaging services, demanding that providers, including WhatsApp, offer intelligence agencies access to content following the London attack."



So last week, following the attack, the U.K. Home Secretary, Amber Rudd, said there must be - and we've heard this before:  "No place for terrorists to hide."  And it's important for spy agencies to have access to the encrypted messages sent by the terrorists; or, or failing that, a future way to do so.  Rudd said that providers of end-to-end encryption services such as Telegram, Signal, and WhatsApp provide a "secret place for terrorists to communicate with each other," and such services are "completely unacceptable.  We need to make," Amber said, "sure that organizations like WhatsApp, and there are plenty of others like that, don't provide a secret place for terrorists to communicate with each other."



Continuing, she said:  "It used to be that people would steam open envelopes or just listen in on phones" - yes, those days were quaint, too - "when they wanted to find out what people were doing, legally, through warrant.  But today," she says, "we need to make sure that our intelligence services have the ability to get into situations like encrypted WhatsApp."



And so as our listeners know, this is why I suspended work years ago on my own, I called it CryptoLink.  I still have the domain, CryptoLink.com, and I have the trademark.  But I stopped working on it because I was reading the handwriting on the wall, from the saber-rattling that we were seeing in the U.S., that our government and the crypto community were going to be coming to blows.  And I didn't want to be in a position of having invested hugely in a truly super-secure encrypted networking solution.  It was going to be my version of a VPN where it just worked, I mean, it just like did everything right, and it worked.  But the problem was I didn't want to be in a position where the government could say, hey, Gibson, you need to let us into this.  This is the position that all of these communications, these secure end-to-end encrypted communication companies are now facing.



So I don't know what's going to come of this.  We spoke last week about, no, a week before, I guess, yeah, about Vault 7.  And my takeaway from that was the fact that the NSA with the Snowden disclosures and then the CIA with the Vault 7 and WikiLeaks disclosures had demonstrated, if nothing else, they were unable to keep their own secrets.  So how can we possibly trust them to keep any sort of a master backdoor key secret?  They've demonstrated they can't.  I understand that they want more access to encrypted communications.  The fact is the Vault 7 documents also demonstrate they have ways to get that, even in the case of Signal and Telegram and WhatsApp.  And specifically, Telegram and WhatsApp were both singled out in the Vault 7 documents as they have a way to get that by either intercepting before it's encrypted, or intercepting after it's decrypted at either of the endpoints.



So again, I don't think this is going away.  The reason I wanted to just bring this up again is that, once again, another terror attack is used as an opportunity for the government to, in this case, the U.K. government to say the way things are today cannot stand.  And the technical community has to push back, as it and we have been, saying the way it is now is the way it has to be.



The only give that I think makes sense is for legislation that I expect to happen to force individual companies to provide some individual means on an individual basis, that is, not some master key, not some golden key that a third party can independently use in order to surreptitiously decrypt, but continue to force, at least as is the case in the U.S., to get a warrant from a judge proving reasonable cause, go to a company like Apple and say, "We have a warrant, and we also have some new legislation which says you must be able to provide us visibility into this particular person of interest's communications."  We know Apple can do that.



Bill Gates famously was asked by Charlie Rose a couple months ago, I happened to watch the interview, Charlie's very interested, he's an iOS user, said, "Can Apple see people's communications?"  And Gates said, "Yes, of course they can.  They control the iOS.  They can do anything they want.  End of story."  And so I think that - I think, much as I don't even like that, given the fact that law enforcement needs to see into communications, this is what I expect to have happen is that we will see some legislation.  And in fact this next story, speaking of legislation, leads into that perfectly.  And I made the little quip, this is not my own, I saw somebody say this on the 'Net:  "'ISP' may soon stand for Invading Subscriber Privacy."



Last Thursday the U.S. Senate voted to eliminate broadband - new, that is, rules that were imposed under the Obama administration in October of 2016.  The U.S. Senate voted to eliminate broadband privacy rules that required ISPs to obtain consumers' explicit consent before selling or sharing web browsing data and other private information with advertisers or other companies.  Those original rules, as I mentioned, were approved in October 2016 by the FCC's - our U.S. Federal Communications Commission - leadership, which was at the time in the hands of the Democratic political party here in the U.S.  But those rules are opposed by the FCC's new Republican majority and the Republicans in Congress.



So using its power under the Congressional Review Act to ensure that the FCC rulemaking "shall have no force or effect" and to prevent the FCC from issuing similar regulations in the future, the vote was 50-48 split straight down party lines.  But because the Republicans are currently the majority in the Senate, that's the way it went.  Of course, they also have a - we also have a Republican majority in the House.  And the way legislation works, both of those congressional bodies need to vote on legislation.  Then, if there's any difference between the legislation, that gets worked out.  And then, finally, the President signs it into law.  All of which is expected to happen.



So the only silver lining here is that all an ISP can see of encrypted communications - almost all of which is now the case thanks to Let's Encrypt, thanks to the NSA Snowden revelations a few years ago, which hugely increased the rate at which we moved into TLS connections and HTTPS - the only thing the ISP can see is our DNS queries.  If we're using their DNS servers, well, then they know exactly what domains we are looking up.



If we're using somebody else's DNS servers, but we're not encrypting the DNS using DNSCrypt, for example, then they're able to see the DNS traffic moving through them on its way to us.  So that's sort of metadata, inasmuch as they're not seeing what we are sending and receiving, but they do know to whom we are sending and receiving it.  And if we're not using a VPN, they're also seeing the IPs to which our traffic, the public IPs to which our traffic is going and coming from.  So if they look the IP up, they can see what website we're visiting.



So again, it's not clear what they want to do.  In the same way that I fear we're doing to see legislation in the future that could compel encrypting companies to provide, to be able to respond to warrants, I greatly fear the day when part of subscribing to an ISP, like Comcast or Cox or any of the others, will require us to accept their root certificate, which would then enable them to run a TLS decrypting middlebox, as we've called them, to decrypt our traffic on the fly, specifically for the purpose of then getting into it.  And then that requires us to trust them and all of their employees and the behavior over time of their organizations.  And that's horrifying because our ISP is a single point of failure for all of our traffic and all of their customers' traffic.



The advantage we have now is where we're talking to all these different websites, if a given website loses control of its certificate, then it's only that one site whose traffic is potentially exposed.  But if an ISP does this, it's a nightmare.  But again, this really does look like the way things are going, which is not good news.



JASON:  With a House vote right now, possibly.  But I know it's supposed to happen today, so we're going to hear about it tonight.



STEVE:  Yes, yes.  I wouldn't be surprised if it does - I expect it to pass.  I think that they - no, and of course it's in the guise of this "less government and less regulation."  So this is all part of that.  Trump famously said, you know, for every one new regulation, we're going to remove two.  And it's like, okay.  I don't know that you can just count them like that.  But that's what he's doing.  So the goal is less government involvement in regulation.  And so they're trying to deregulate this.  Unfortunately, some of these things are good because ISPs don't clearly have our privacy rights in mind.  They want to monetize what they see us doing.



JASON:  And, I mean, there are good, there are ISPs that are seen as being better in this regard versus those that are clearly not.  So there will, I mean, I guess there will always be a market for the ISPs that dedicate themselves to not following the herd in this regard; right?  And I guess you just have to hunt those out and go with them.



STEVE:  Yeah, and I wonder also if we're going to see ISPs becoming VPN hostile because the other thing you can do, of course, is run out through a VPN, and then they're unable to see anything that you're doing.  You're just this pipe of pseudorandom noise is all they see.  Now, of course the VPN has its own problems because the endpoint where you terminate is where all of the traffic for all of their customers comes out.  And that's a perfect place for the NSA to stick some of its taps, if it wants to see what's going on.  But then again, at least then, if your traffic is encrypted, you're getting it out of your ISP's control.  Anyway, yes, lots of interesting activity in the security world.



JASON:  That's the cheery news I was looking for.  Thanks, Steve.  No, I mean, this topic, man, this topic just keeps coming up and keeps getting crazier and crazier.



STEVE:  And that's the problem is I don't think it's going to go away.



JASON:  No, I don't think so either.  And it goes hand in hand with the encryption topic, as well, and the reduction in Net Neutrality laws which, I mean, the ISPs can understand when they're seeing potentially encrypted traffic.  Could they, at some point, make some sort of - draw some sort of line in the sand that says, if it's encrypted traffic, we throttle it to death?  Or something like that.  There's probably a lot of unintended consequences, if you go that route, just make life really difficult for those using encryption.



STEVE:  Yeah, in fact it's not practical.  You could throttle a VPN, but you could not throttle HTTPS because 100% is HTTPS now.  I mean, all of the sites, all of your use of Google.  GRC won't let you connect to it without HTTPS, nor will an increasing number of sites because, you know, we're all moving to HTTPS all the time.



JASON:  PCGuy8088 in chat says "Breaking:  House narrowly votes to repeat broadband privacy rules, 215 to 205."



STEVE:  Yup, right down the party line.



JASON:  Yeah, there you go.



STEVE:  Okay.  And so there's no doubt that our President will sign it, so what happened in October '16 will be reversed.



JASON:  And so then we go back to the way it was prior to the rules?  And if that's the case, was life bad on the Internet then?  You know what I mean?  Are we now at this point venturing into uncharted territory?  Or we've already been here before, we just understand more now why that's not good?



STEVE:  Okay.  So what the legislation that was put into law in October did was it was going to require ISPs to get individual customers, that is, subscribers' explicit permission.  And so now they won't have to.  They'll be able to silently look at our traffic.  And, of course, they know who we are.  That's the other problem.  It's not like a website where they're able to say, oh, you know, the cookies are anonymous, and we don't know who you are.  We're just aggregating this anonymously.  No, you have a fiduciary relationship with your ISP.  They have your billing information.  They know who you are.  And now this law allows them to take our behavior, connect it to us, and monetize that, sell it to advertisers, without our permission.



JASON:  Crazy stuff. 



STEVE:  Yeah, it's a little annoying.



JASON:  Yeah, just a little bit.  "Annoying," I don't know if that's a strong enough word, just to be honest.  I feel like "annoying" is kind of light.



STEVE:  So we talked about how LastPass has been so good about responding to problems found.  The flipside of this, unfortunately, of this typical good behavior was just shown to us by Nest.  Overall, Nest has been a disappointment.  And it appears to be a classic instance of beauty only being skin deep.  I immediately fell in love with that thermostat because it was gorgeous.  I think, you know, if they have nothing else, they have an amazing industrial designer because that Nest thermostat is just - it's a piece of art.  I mean, it looks like something that came from Apple, as opposed to some random startup.



But from all accounts, Nest's CEO is extremely difficult to work with.  And their corporate and product performance has disappointed many.  Last summer Ars Technica covered, I think it was in June of last summer, they had a headline reading:  "Nest's time at Alphabet:  A virtually unlimited budget with no results.  Nest quadrupled its employees, launched no new products, and caused constant bad PR."



So anyway, back in 2014 Nest purchased Dropcam, an acquisition which itself has not gone very well.  Dropcam's people were not happy.  A whole bunch of them quit.  But Nest has now a camera, thanks to purchasing Dropcam.  Well, back in, when was it, October of last year, security researcher Jason Doyle was poking around the Nest/Dropcam devices and found some troubling problems.  Nest sells these cameras as security cameras, the Dropcam, now Nest cameras.  Yet it's trivial to cause them to drop off the network, effectively blacking out the region the camera was designed to observe.



And it's worth noting, as we've said before on this podcast, that the very phrase "wireless security" has big problems and is a self-contradictory oxymoron.  My own home security system, as are all professional security systems, is low tech and hard wired.  Yes, it's more expensive to install.  And it's harder to install.  But by being wired, you're not relying on the ether to cover the security signaling over the air that all these little things that you just stick on your windows are doing.  And we've previously talked about the problems of jamming these systems and how possible it is to do that.



Well, Jason discovered three different denial of service, or maybe in this case it would be, instead of DoS, it would be a DoV, Denial of Video problems that he named Bluetooth-based buffer overflow via SSID parameter, Bluetooth-based buffer overflow via encrypted password parameter, and Bluetooth-based WiFi disassociation.



In the first case, it's possible to trigger a buffer overflow condition when setting the SSID parameter on the camera.  The attacker must be within Bluetooth range - but of course that's 40 feet, 10 meters typically, or 30 feet at least, and longer if you use a directed antenna.  So the attacker must be in Bluetooth range at any time during the camera's powered-on state.



Bluetooth - and here's one of the big problems - is never disabled, even after setup.  So this is one of those devices, an IoT device that has Bluetooth for setup and WiFi.  Yet, for reasons that it's difficult to understand, maybe they just didn't want to have an extra button on the back where you would press it in order to enter setup mode, and then the Bluetooth would disable itself until you pressed the button again.  Anyway, Bluetooth is always on.  Which means at any time, as long as a bad guy can get within Bluetooth range, they send a packet to the camera or cameras and knock it offline by triggering a buffer overrun in the handling of the SSID parameter.



In the second case, the same thing can be done with the encrypted password parameter.  Shoot that into the Nest cam, and it knocks it offline.  Now, in both of those cases it will reboot, but you have about a minute and a half before it comes back.  And of course you can do it again if you want.



In the third case, it's possible to proactively give the camera a new valid SSID, which it will then attempt to switch to.  If you have also provided it with a bogus access point, it'll associate with that one and just sit there happily forever after.  If you don't give it one, it will wait about a minute and a half and then finally decide, okay, that must have been spurious, and switch back.  So again, you've got about a 90-second blackout, or an infinite blackout, if you camp it on some other WiFi access point that, for example, you just brought along with you.



So, again, I never have problems with anyone making a mistake.  Now, I have a problem with policy versus a mistake.  And I would argue that the policy of leaving Bluetooth enabled for a WiFi device, that's something that is indefensible.  They should have simply had a little button on the back that temporarily enables Bluetooth until you're through the configuration and setup, and then it shuts down.  But they wanted to make it even easier to use.  Oh, just sort of magical, so you don't have to press any buttons.  Yes.  And unfortunately, neither do the attackers.



So there's a policy problem there.  But everybody knows mistakes happen.  But what matters is the way they address those mistakes.  So Jason - it was Jason; right?



JASON:  Yes.



STEVE:  I think that was his name.  Yes, Jason Doyle.



JASON:  Every time I read the name Jason...



STEVE:  No, wait a minute, that's also my co-host today.  So he informs Nest on October 26th, reporting the security bug through Google's vulnerability reward program guidelines.  On October 27th, Google's security team acknowledges the report was received and being investigated.  On November 1st - this is all moving along perfectly - Google's security team validated the reported vulnerabilities and filed the bug.  Two weeks later, November 15th, Google's VRP panel issued a $100 reward under nonintegrated acquisitions, so Jason gets a little token $100.  Then four months goes by with nothing happening.



Finally, out of frustration, on March 17th, Jason goes public with his disclosure, and Nest scrambles around saying, oh, oh, oh, oh, we'll get this fixed immediately.  That is not the way you want a company whose products' security you care about to act.  This is an example of the worst possible action.  The problem was reported responsibly.  The receipt of the report was verified.  The problem itself was verified.  And then four months goes by with no action at all until a public disclosure forces their hand.



Gizmodo, reporting on this, wrote:  "Now that the code for the exploit has been published, a motivated and knowledgeable burglar could theoretically use it on your home tonight.  If you own one of these cameras, the only real bulletproof solution to avoid the flaw is to disconnect them until Nest pushes a software fix.  Of course, disconnecting a camera doesn't exactly make you any safer.  Given that Nest has not updated the firmware in over a year, that's cause for concern.  Let's hope they hop to it with a fix."



JASON:  Yeah.  What this screamed to me is like we hear about IoT security over and over again.  I mean, this past year it's been a recurring issue.  And then I look at this, and I'm like, god, even IoT hardware owned by one of the largest companies in the world, Google, can't get it together to tackle something like this with urgency.  I mean, that's a great issue for all, but it's an even greater issue when a company like Google can't pull it off, or chooses not to.



STEVE:  Right.  Well, and in fact we have an acronym on the podcast - we have many because we like them.  IDIOT stands for I Don't IOT. Because it's just not very secure to do that these days.



JASON:  So you don't have any IoT devices in your home.



STEVE:  Do I?



JASON:  Any Hue light bulbs?



STEVE:  I don't think I do.  I don't think I...



JASON:  It's be really hard for me to get rid of my Hue light bulbs.



STEVE:  Those light bulbs, though, those are malware bait.  



JASON:  Oh, great.  Excellent.



STEVE:  Yeah.



JASON:  Ugh.  They've got to get their stuff together.



STEVE:  Anyway, so this week in I Don't IOT we have the report of a dishwasher/disinfector, I guess it's the Miele Professional PG 8528.  I saw a photo of it, so it's not something you're going to have in your kitchen, that's the good news, or at least not this version.  This is more of an industrial commercial size thing.  But it turns out that it is apparently based on some sort of an embedded Linux, and it has an embedded web server that identifies itself as the PST10 WebServer.



There's a classic web server flaw that actually IIS, Microsoft's web server early on had, which is called a "directory traversal" bug.  As anyone who's used the command prompt in any of our OSes know, dot refers to the current directory, and dot dot refers to the directory one level up or back the directory hierarchy.  So a common attack is to say GET /../../../.  And you keep doing that maybe, doesn't matter how many times, like 12 or 13.  And so what that does is, when a web server that hasn't been protected from this sees that, you don't know what directory the web server root is in.  That is, the web server's root, where you'd have like the default home page, that's not in the server's root.  That's in, like, maybe /www/myserver or something.



So that's in a subdirectory somewhere.  And so that's the root.  So if you tried to access a file on the root, it would be looking for that file in that subdirectory.  But for servers that haven't protected from this - and by the way, this has, like, got to be one of the oldest web server bugs there is, is you do this /../../...  What that does is that walks the GET request back up the file hierarchy to the actual server root, out of the web server root to the actual server OS root.  And then in this instance the guy who found this said /etc, you know, E-T-C, then /shadow, which allowed them to have the server retrieve for them what's known as the password shadow file, which then allows you to get access to the passwords in the OS.  Linuxes use this for additional security.



But it's just nuts that any server, any Linux web server that anyone could write in this day and age would be victim to this simple directory traversal attack, which has been well known, as I said, literally for decades, probably for 20 years.  And so in this proof of concept, this dishwasher has a web server open on port 80 of its IP.  And anyone who is able to arrange to get to port 80 is then able to issue a simple GET request, obtain the passwords for the server, and then of course presumably it's got Telnet listening.  And so then log into the server, get the admin credentials - because in this proof of concept we're seeing the root password entry from the file.



And then lord knows what it's going to do.  Maybe flood the kitchen by turning, I mean, you know, you have an operating system that has full control over your dishwasher.  It's probably, you know, this OS is probably running the various motors, the pumps, the valves.  And so you could get up to any kind of mischief like this.  And the way into your kitchen, by the way, Jason, would be through your light bulb.



JASON:  Oh, okay.



STEVE:  So that's the way...



JASON:  So what you're telling me is don't get this dishwasher.  That's like the overarching rule.  There's nothing more important than that one rule.  Oh, wait a minute, no.  Probably starts with a light bulb.  But I - they go all sorts of colors, Steve.  That's really difficult, for me to give it up.



STEVE:  I know, they're very seductive.



JASON:  And I can just push a button on my phone, and it turns colors.  Like that's so cool.  But I know.  I get it.



STEVE:  So a friend of the show, Simon Zerafa, sent me a fun tweet, forwarded from Jerry Gamblin.  He said:  "Sometimes hacking is just someone spending more time on something than anyone else might reasonably expect."  And of course, you know, that ought to be the byline of this podcast.  I mean, that's what we keep seeing over and over.  Tavis is thinking 24/7, even when he's showering, about how to get into LastPass, and coming up with new and clever ways to do that.  "Sometimes hacking is just someone spending more time on something than anyone else might reasonably expect."



And we've also talked about how the reason or the way the CIA was able to develop this stuff is on one hand, they were just scouring all of the public fora and conversations, looking for some way, you know, for all the different ways people were finding of getting in, and aggregating them.  And presumably they've got a big budget, and so they're paying people to be hackers, to spend their time looking at this, at the targets of opportunity, much more than anyone might reasonably expect.  And of course we saw the same thing with that crazy iOS security batch of fixes.  Somebody, a whole lot of different somebodies, took the time to find the problems.  



JASON:  That's the reality of technology as it stands right now.  Somebody needs to spend that much time poring through the minutiae.  And thankfully there are people that enjoy that.  I'm not sure [crosstalk] enjoy that minutiae.



STEVE:  Yes, and for the well-secured things, like today's browsers and OSes and VMs, it is much more difficult now.  Now you've got to string five or six different vulnerabilities together to create a serialized exploit in order to get something done.  So, I mean, you've really got to have your propeller well wound up on your beanie in order to be able to pull this kind of thing off. 



JASON:  It's going to be really, really long shower.



STEVE:  So, yes.  So a couple bits of miscellanea and some quick dialogue with our listeners.  We've been talking for the last couple weeks about Conway's Game of Life.  A number of people mentioned to me that Google was already ahead of the game.  We were talking about apps for that last week.  It turns out, you google "Conway's Game of Life," Google runs it on the page of answers that comes up, which I thought was just so fun.  You don't even need an application.  You just C-O-N-W-A-Y-'-S, "Conway's Game of Life."  And so for anyone who hasn't even bothered to grab an app, just google "Conway's Game of Life."



JASON:  Oh, yeah.



STEVE:  And actually I watched it for a while yesterday.  It's very cool.  You'll see some loafs and some blocks.  Some gliders get spontaneously created and go sort of shuttling themselves off the page, and blinkers, and stoplights, and all kinds of classic Game of Life structures get created.  So nice going, Google.



JASON:  Sorry, I'm lost.  I'm staring at that life flash before my eyes on a Google search page.  



STEVE:  So several people have asked me, because we've been talking about one-time tokens, and in fact we were just mentioning - I didn't have it on-hand when I mentioned it last week.  But we covered the news that eBay was officially discontinuing the use of this, what we always called the "football" because it's sort of football-ish shape.  I still have mine, and mine still works.  I push a button, and up comes a little number there.



And it's funny because I knew that we would have a problem if the battery died.  Apparently you cannot change the battery in this without losing the code which is only stored in RAM.  It's not stored in any kind of an EPROM.  So I wouldn't even let it, like, timeout itself.  It actually - I would look at the code, quickly memorize it, and then turn it off again in order to preserve the battery.  So consequently mine is still working after all these years.  But eBay was sending people news over the last couple weeks that they're increasing the security by discontinuing the use of the football.



Well, that was the time-based one-time password.  They're now switching to an SMS-based password.  And so several people have asked me, "Will the football still work?  Or will it stop working," he says in this one case, "because I don't want to change to the SMS version."  And the bad news is Google was paying VeriSign, which became Symantec.  I think, I don't know whether Symantec purchased the identity portion of VeriSign or only the domain registrar portion of VeriSign.  But eBay was paying per authentication, which I think is probably the reason that they've stopped supporting that.



They could certainly have switched to what everybody else is switching to, which is the software-based time-based token, the TOTP, Time-based One-Time Password, rather than going SMS.  I just think they're not that concerned about security.  I mean, so they're like basically backing off from the best security, which was to allow people to use a hardware token, to the least secure of the second-factor authenticators, which is SMS.



JASON:  Well, okay.  So speaking of this, there was Instagram news late last week that Instagram now supports two-factor authentication.  I went in to set it up today.  And sure enough, the only way that you can do it through the Instagram version is by SMS.  Why do some companies choose, because I would prefer to have it all inside an app and not have my SMS code being flown around everywhere, why do some companies choose not to do that?  Why do they stick to SMS, knowing what they know?



STEVE:  I think it's for the same reason that some companies say that your password can only be a certain length, or it's not going to be case-sensitive.  Case-sensitive passwords are much stronger, but they're more trouble-prone.  And so by saying, oh, your password's not case-sensitive, they think that'll make it easier for their users.  Unfortunately, it's at the cost of security.



So I think that SMS is easier to use than an app.  But the tradeoff is, and in fact this came up for me a few months ago, we were talking about this because I've switched away from Network Solutions.  I've switched to Hover as my domain registrar.  And when I was setting up, they gave me the option of SMS or a time-based one-time password.  Of course I went with time-based because it's fundamentally more secure.



The problem with SMS is that, on a per-login instance, you're using your insecure cell phone network to send you an SMS message.  And we know cell phone networks are not secure.  So it is way better than no second factor, but it is the least secure of the several second-factor alternatives, the most secure being that one time the website gives you a code, which then is used to key a software authenticating app, and you only need one of those apps because they're all able to store multiple keys.  And the beauty is then on a per-login basis, they're not sending you anything.  They're just asking you, based on the time of day, what is your app currently showing you in a 30-second period, enough to see it and type it in.



And in fact several people liked the tip that I gave last week.  We were talking about this, and I said, you know, the problem with these authenticating apps is none of them will export that key.  They are all write-only.  You cannot read the key back out.  So what I have taken to do is, because I have four different sites that I am now using the time-based token with, is when the QR code is displayed on the screen, I print that piece of paper.  I print the page.



So I now have four pages containing the four QR codes.  That way when I'm setting up a new device, and I'm just using Google Authenticator because I like how many it lets me put on one screen, you know, there are a bunch of them.  They're all compatible.  You choose whichever one you like.  Leo likes, I think it was Authy.  And that one does cloud synchronization, which again is a convenience tradeoff for security.  I'm not putting my master time-based tokens in the cloud.  The whole point of having them is not having them anywhere anyone can get them.



So I like Google's Authenticator.  And in fact, in a dialogue I was having with someone, he said that when his Android device, he has a Samsung, and he said he wipes and reloads his phone from scratch rather than updating it, which he says he feels keeps it operating at peak performance.  But that also means he's constantly having to reconfigure, re-set up his authenticator app.  And so he loved my tip and recommendation from last week of just printing them on paper.  Obviously, you need to store them securely.  But that should be something you're able to do because we're not worrying about a home invasion.  We're worried about somebody in Russia or China or somewhere else doing an online attack or a phishing attack or somehow spoofing us or intercepting our communications.



So the point is making it more convenient to use an authenticator I think ends up increasing your security.  And so I have these four pieces of paper for the four sites that I'm currently authenticating against.  And that number will grow over time as more people offer this feature because it's the best security available now, while we're still stuck using all of this ridiculous, how do we make our passwords more secure technology, until we're able to obsolete passwords completely.



JASON:  [Ahem]  SQRL.  [Ahem]



STEVE:  Exactly.  Which brings me to my next note here.  What's the status of SQRL?  Yesterday I just finished all of the server-side code.  It's completely wrapped up.  After the podcast today, I am going to work on the finishing details of the client, and then we will have something for people to play with probably pretty soon.  While that's happening, I'm going to go back and catch up all of the online documentation to make it current with what the spec which is currently implemented in the software does.  But that can overlap with people playing with it.  So I just - I have no idea when.  But it doesn't feel like it's far off from now.  And I can't wait for everyone to be able to play with it and see what they think.



JASON:  That's exciting.  Everybody's waiting for that, too.



STEVE:  Yay.  Me, too.  So I had someone ask:  "Apps like Google Authenticator support only the default HMAC SHA-1 version of the RFC," the time-based token RFC.  "I don't think collisions matter here.  Do you?"  So he's noting that SHA-1 is a concern, as we just saw, because the first collision in SHA-1 was mathematically found after a huge amount of processing was performed.



And the answer is no.  In an Authenticator app, we don't care about collisions.  We're just using the SHA-1, a keyed SHA-1, that's the HMAC SHA-1, as a key-based pseudorandom number generator.  So the secret key keys the hashed MAC, and then the time is put into it, and out comes an unpredictable number that changes every 30 seconds.  And so in this instance we're using the SHA-1 as part of a pseudorandom number generator where collisions do not matter.



Someone else asked, and we referred to it earlier in the show:  "Have you guys talked about DNSCrypt on Security Now!?  If so, any comments?"  And I've never done a deep dive into it.  It does use my favorite encryption, which is the Bernstein 25519 curve.  And DNSCrypt, D-N-S-C-R-Y-P-T dot org is the site where you can find it.  I would say, if you want to hide your use of DNS from your ISP, and also bring the security of your DNS up a bit, DNSCrypt is today the way to do it.  DNS is otherwise just UDP packets in the clear.  There is no protocol encryption for DNS the way there is for HTTPS, for example.  So everything you do with DNS is in the clear unless you wrap it in DNSCrypt, which is currently the only game in town, and I think a great solution.



Some guy asked:  "Hey, Steve.  While listening to Security Now!, you mentioned a small two-port router for isolating Internet of Things things.  What was its name and model again?"  Anyway, I have no problem remembering it because it's my initials.  It's the SG-1000.  And if you just put "SG-1000" into Google, that'll take you to the Netgate.com site, where this, I mean, it's just an adorable little thing, which is able to run the pfSense router firewall, and which you're able to use for really good network isolation.



And then, finally, someone said:  "FYI.  Don't know if you're watching.  'The Good Fight' continues to explore technology and the Internet like 'The Good Wife' did."  Back when "The Good Wife" was on CBS, we were talking often about they had some fun episodes where some guys at the NSA were listening in on the various conversations going on in the law firm.  I did watch the first episode of "The Good Fight," but it's CBS All Access.  And  I love to binge watch serialized shows when they're available, and there's nothing else that I need CBS All Access for.  So I'm just going to wait till the end of the first season, and then I'll sign up for the free week and watch "The Good Fight" all in that week and then unjoin because they let me do that.  But I did watch the preview, and I loved it.



JASON:  I haven't heard of it.



STEVE:  Huh?



JASON:  I haven't even heard of that show.  No.  And I never saw "The Good Wife."  I did hear a lot of good things about it, especially [crosstalk].



STEVE:  "The Good Wife" was a great show.  And this is basically, they called it "The Good Fight."  So it is a continuation of "The Good Wife."  But they decided, okay, we're going to make people pay for it.  So it's like, ah, okay.  Not me.  I'll wait till the end.



And finally, I love this note from a listener and SpinRite user because this is the textbook perfect way of using SpinRite.  Someone named Alfred wrote to me, and he said:  "Hi, Steve.  I've been a long-time listener and a long-time regular proactive SpinRite user.  I have never lost any data yet," and he says, "knock on wood.  But I have always changed drives whenever I see that SpinRite is finding an unusually high number of incidents, as it has a number of times."  Then he said:  "Additionally, your podcasts and research in security and health have made me a healthier safer life.  Thanks."



So I just wanted to mention that that's - I've said before, I understand that most people are going to purchase SpinRite when they are hit by a disaster, and SpinRite can hopefully bring them back.  And that normally makes believers out of them.  But Alfred is doing the best thing possible, which is periodically, like maybe every quarter, you know, quarterly, every three months, run SpinRite.  And on the SMART screen it shows you lots of detail about the rate at which things like error correction are occurring.  And so that is the most sensitive test ever created for metering the current health of your drive.  And if you just took a note of the numbers that are shown at the end of a SpinRite run, and then looked at them every three months over time, you would notice if there was a change.  And on a hard drive, change is not good.  There's no way that a drive is going to change for the better.  It's going to change for the worse.  I mean, if the numbers go down, that's sort of a miracle, but okay.  Probably they're going to go up.



And so what Alfred has done is when he's seen that happen, with hard drives being as inexpensive as they are nowadays, he just takes that as an early warning indication that things are not looking so good, and he moves his data to a new drive and continues.  As a consequence, he's using SpinRite for maintenance, proactively, preemptively, and never had any data loss.  So, yay.  Thanks for sharing that, Alfred.



JASON:  Fantastic.  Yeah, data loss is no fun, especially when you realize you could have done something, and it would have been a heck of a lot easier prior to.



STEVE:  Oh, it's like...



JASON:  It happens to you once, and then you change your habits.



STEVE:  Yes.  Who hasn't gone ooohhh.



JASON:  Dang it.



STEVE:  Gah.



JASON:  You can't go backwards.



STEVE:  If only.  If only.



JASON:  Yup, exactly.  All right, Steve.  Tell me why Google doesn't trust Symantec anymore.



STEVE:  So, okay.  Ryan Sleevi is another security researcher at Google.  Good guy.  I follow him on Twitter and watch his postings.  He, well, the title of his announcement was "Intent to Deprecate and Remove:  Trust in existing Symantec-issued Certificates."  Which is cataclysmic.  I mean, it's earthshaking for the industry, and arguably cataclysmic for Symantec.



As we've previously covered, back toward the end of 2015, in October, Google first discovered misissued certificates for itself and Opera.  Remember that Google knows, Google's Chrome browser knows the fingerprints of all of Google's certificates explicitly, so-called "certificate pinning."  And so the moment any Chrome user goes to any site that has a fraudulent Google certificate, Chrome phones home.  Chrome immediately tattles and says, hey, Google, guess what I just found in the wild.  So you can't get this over on Google.  And of course now Chrome is the majority browser on the Internet, is the number one web browser in the world.  And so Google has extreme vision into what's going on.



So three years ago, or two and a half years ago, they discover misissued Symantec certificates for itself and Opera.  But subsequent research has since revealed that the problem was much worse.  Google announced last Thursday that it will begin downgrading the level and length of trust Chrome, the major browser in the world, will place in certificates, that is, all certificates issued by Symantec.  Now, this is my opportunity to tell everybody how glad I am I left VeriSign.  Remember that Symantec purchased VeriSign back in 2015.  And VeriSign, as a consequence of having been around from the beginning, from like the dawn of the Internet, had a market share of around 30% of the web at that time.



I was with VeriSign because they were the granddaddy, in the same way that I was with Network Solutions because they were the granddaddy over on the domain registrar side.  And I have since left both.  Everybody knows who's been listening to this podcast that, whereas all of GRC's certificates were once VeriSign, I made the switch to DigiCert and have never been happier in my life and have never looked back.  And obviously I'm more happy today than I ever have been before.



I've met the DigiCert people.  I've asked them to help me with all kinds of bizarre things that I can't even imagine even asking VeriSign or Symantec to do, like dual-issuing a certificate, both in SHA-1 and SHA-256, and expiring the SHA-1 on midnight of New Year's Eve a couple years ago so that I could keep Chrome happy with not having an SHA cert which was valid in, what was it, 2016, yet still allow people using XP service packs earlier than 3 to access GRC, which was only available over HTTPS.  My point is, how could I ask, you know, VeriSign and Symantec couldn't care less.  DigiCert made this all possible.  So I am so happy that I made the move.  And as everyone knows I recommend them without hesitation.



So Google has determined that Symantec has not been taking its responsibilities as a certificate authority seriously, and has issued - are you sitting down, Jason? - 30,000 certificates, not a handful, 30,000 without properly verifying the websites that received them.  This is, of course, a serious allegation that undermines the trust users can place in the encrypted web.  And Google says it will begin the process of distrusting Symantec certificates in its Chrome browser.  Now, again, cataclysmic for Symantec.  They, of course, lashed out at Google's claims, calling them irresponsible and exaggerated and misleading.  To which I respond, yeah, uh-huh.  Who do we believe here?



So Ryan wrote in his Google posting:  "Since January 19, the Google Chrome team has been investigating a series of failures by Symantec Corporation to properly validate certificates.  Over the course of this investigation, the explanations provided by Symantec have revealed a continually increasing scope of misissuance with each set of questions from members of the Google Chrome team.  An initial set of reportedly 127 certificates has expanded to include at least 30,000 certificates, issued over a period spanning several years.  This is also coupled with a series of failures," Ryan writes, "following the previous set of misissued certificates from Symantec, causing us to no longer have confidence in the certificate issuance policies and practices of Symantec over the past several years."



Ryan wrote that Symantec's behavior failed to meet the baseline requirements for a certificate authority, creating what he called "significant risk for Google Chrome users."  Symantec allowed at least four parties access to their infrastructure in a way to cause certificate issuance; did not sufficiently oversee these capabilities as required and expected; and, when presented with evidence of these organizations' failures to abide to the appropriate standard of care, failed to disclose such information in a timely manner or to identify the significance of the issues reported to them.  These issues, and the corresponding failure of appropriate oversight, spanned a period of several years and were trivially identifiable from the information publicly available or that Symantec shared.



Ryan wrote in another post that Symantec partnered with other CAs  CrossCert, which is the Korea Electronic Certificate Authority; Certisign Certificadora Digital; Certsuperior S. de R.L. de C.V.; and Certisur S.A.  that did not follow proper verification procedures, which led to the misissuance of 30,000 certificates.  Ryan explained:  "Symantec has acknowledged they were actively aware of this for at least one party, failed to disclose this to root programs, did not sever the relationship with this party."  And he wrote:  "At least 30,000 certificates were issued by these parties, with no independent way to assess the compliance of these parties with the expected standards.  Further, these certificates cannot be technically identified or distinguished from certificates where Symantec performed the validation role."



He writes:  "To balance compatibility risks versus the security risks, we propose a gradual distrust of all existing Symantec-issued certificates, requiring that they be replaced over time with new, fully revalidated certificates, compliant with the current baseline requirements.  This will be accomplished by gradually decreasing the 'maximum age' of Symantec-issued certificates over a series of Chrome releases, distrusting certificates whose validity period - the difference of notBefore timestamp and the notAfter timestamp - exceeds the specified maximum.



"To restore confidence and security of our users, we propose the following steps:  First, a reduction in the accepted validity period of newly issued Symantec-issued certificates to nine months or less, in order to minimize any impact to Google Chrome users from any further misissuances that may arise."  So essentially over time, and I'll explain what the schedule is in a second, but over time they're going to start squeezing down from three years down to nine months, successively with future releases of Chrome.  That will allow websites who now have Symantec certificates to get updated ones.



However, that will, over time, they will have to be updated to shorter and shorter durations, down to nine months.  So essentially Symantec will no longer be allowed to issue a three-year cert or a two-year cert.  The only certs that Chrome will honor when it gets to the end of the successive set of changes is any certificates from Symantec with a life that has never been longer than nine months.



He said:  "We propose to require that all newly issued certificates must have validity periods of no greater than nine months, 279 days, in order to be trusted in Google Chrome."  And that becomes effective with Chrome 61.  I guess they're at 59. now.  "This ensures that the risk of any further misissuance is, at most, limited to nine months."



And, see, this is the problem.  Right now, unless they start mistrusting all Symantec certificates, since there's no way to determine where the certificate came from, they have to start throttling the duration of all of them in order to minimize the security risk.  Because they found 30,000 of them, or evidence that 30,000 were misissued.  But there's no way to determine on a specific certificate programmatically who issued it.  So they have to treat all of them from Symantec, that is, signed by Symantec's master certificate as increasingly untrustable.



So an incremental distrust is what they're going to be imposing, spanning a series of Chrome releases of all currently trusted Symantec-issued certificates, requiring that they be revalidated and replaced.  So with Chrome 59 (both the Dev, the Beta, and the Stable) that will only allow certificates to be 33 months old.  Chrome 60 (all three, Dev, Beta, and Stable) reduces that to 27 months.  Chrome 61, all three versions, to 21 months.  Chrome 62 to 15 months.  Oh, and Chrome 63 Stable to 15 months.  Chrome 63 Dev and Beta will be a little more aggressive at nine months' validity.  And then with Chrome 64, all three (the Dev, Beta, and Stable) will set the maximum, the acceptable certificate life from anything issued by Symantec to nine months.  And they will be in that purgatory until such time as they demonstrate that they are able to responsibly issue certificates.



And then, finally, quoting again from Ryan:  "Given the nature of these issues and the multiple failures of Symantec to ensure that the level of assurance provided by their certificates meets the requirements of the Baseline Requirements or Extended Validation Guidelines" - oh, I forgot this part, woohoo - "we no longer have the confidence necessary in order to grant Symantec-issued certificates the 'Extended Validation' status."  So they are no longer, even if certificates are EV, they are not going to show that in Chrome.



"As documented with both the current and past misissuance, Symantec failed to ensure that the organizational attributes, displayed within the address bar for EV certificates, meet the level of quality and validation required for such display. Therefore, we propose to remove such indicators, effective immediately, until Symantec is able to demonstrate the level of sustained compliance necessary to grant such trust, which will be a period no less than a year.  After such time has passed, we will consider requests from Symantec to reevaluate this position, in collaboration with the broader Chromium community."



Ryan finishes:  "This proposal allows for web developers to continue to use Symantec-issued certificates, but will see their validity period reduced.  This ensures that web developers are aware of the risk and potential for future distrust of Symantec-issued certificates, should additional misissuance events occur, while also allowing them the flexibility to continue using such certificates should it be necessary."



And so my take is Symantec got caught playing fast and loose, and rather clearly failed to appreciate that the privilege of essentially printing money by charging people for a pattern of bits comes with a significant and serious responsibility to assure the integrity of the identity assertions which are implicit for a certificate's holder.  They screwed up, and Google's going to hold them to account.  And I say bravo, Google.  I mean, it's not easy.  Google doesn't want to hurt Symantec, but Google's taking the trust implicit in certificates seriously.  And I think they should.  And I hope this serves as a lesson, not only to Symantec, I'm sure it will, but to the other CAs that are printing money and not earning the right to do so by being sufficiently responsible.



JASON:  Any reason for other browser makers to follow suit in this regard?



STEVE:  It would be nice if they did.  I'll be surprised if Mozilla doesn't.  I would be surprised if Microsoft did.  But again, being the premier browser, the majority browser, more than half of the Internet is using Chrome, that forces Symantec to change.  You know, I mean, if Chrome doesn't - okay.  And understand, this is driven at the web server end.  Anyone using Symantec certificates is going to be inconvenienced by the need to update their certificates, and they're not going to be able to get a three-year or a two-year cert.  They're only going to be able to get a nine-month cert.



So this clearly hurts Symantec significantly, which is why it's not something Google does lightly.  I mean, this is a huge blow, but it's one that Symantec deserves.  And what will happen is they're going to significantly lose market share.  I mean, they lost me for other reasons, just because of who they were a long time ago, and I switched to DigiCert, and I'm obviously glad I did so.  But this essentially forces other websites, either to, I mean, maybe they'll discount their certs in order to hold onto market share.  But it's going to be an inconvenience for all the websites which are using today Symantec certs, probably just due to inertia, from the early days when they were using VeriSign.



And of course that's why Symantec purchased VeriSign because they're a money printing press.  But they've demonstrated they don't have the discipline to have that privilege.  And so websites are going to be switching away from Symantec or having to continually get new certs, if for some reason they decide to change.  I think everybody should leave.  And you know where I think they should go.



JASON:  And I imagine it's a lot of work.  Like when Google spells out issuing new certificates, I mean, that's a huge pile of work for Symantec to do properly.



STEVE:  Yeah, yeah.  I mean, this is a big deal.  This is, you know, this is, yikes.  And you can't, I mean, EV.  I mean, all of my certs from DigiCert are EV because we're GRC, and I want to have that look.  I go through hoops with DigiCert to reverify that I am who I say I am, for the privilege of that extended validation that I want to have show on the browsers.  And nobody who has an EV certificate from Symantec will get that any longer because we can't trust that the person holding that certificate from Symantec is who they say they are.



JASON:  The prime reason you have a certificate in the first place.



STEVE:  Exactly.  The only reason.  That's what it is.  It's just an identity assertion.  It's an identity assertion.  



JASON:  Yeah.  Explicit reason of existence.  Thanks for explaining that.  Is there any final thoughts before we wrap it up?



STEVE:  We're done until next week.  And we'll have more next week for 606.



JASON:  Right on.  Steve, this is awesome.  It's such an honor to get to do the show with you.  And this is a lot of fun.  You know, Security Now! is filled densely with information, some of it slightly scary because security can be a scary thing when you're talking about security and privacy and identity and all that online.  But always fascinating deep dives, and I love it.  So continue the great work.  Really appreciate it.



STEVE:  Thanks.  And Jason, you were great.  It was great to have you.  And whenever Leo takes a vacation, glad to have you back.



JASON:  Just let me know.  Just let me know, and I would be glad to hop in.  Of course GRC.com, if you want to check in on all things related to Security Now! and everything that you're working on, Steve.  SpinRite, of course, must-have hard drive maintenance utility we talked about a little bit earlier.  Definitely go there if you haven't already, and a whole lot more:  GRC.com.  You can also find, I think, Security Now! audio, transcripts.  You post those on the site, as well; right?



STEVE:  Yup, yup.  And the show notes for this episode are already up and on the site for anybody who wants to grab them because the show notes have links and things for additional information.  



JASON:  Awesome.  And you can find the show also on our site at the show page for Security Now!, that's TWiT.tv/sn, as well as anywhere you're going to find awesome podcasts like Security Now!.  You're going to find them in the index.  Just look for Security Now!, and you'll find it there.  You can watch live, of course, every Tuesday, 1:30 Pacific, 4:30 Eastern, 20:30 UTC at TWiT.tv/live.  Just go to the page, select the service that you want to stream it from.  They're all listed there, makes it super easy to hop in and join in real time.  Thank you, Steve.  This has been a lot of fun.  Appreciate it.



STEVE:  It was a pleasure, Jason.  Talk to you next time.



JASON:  Will do.  Leo returns next week.  We'll see you next week on another episode of Security Now!.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#606

DATE:		April 4, 2017

TITLE:		Proactive Privacy

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-606.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week Steve and Leo discuss another iOS update update, more bad news and some good news on the IoT front, the readout on Tavis Ormandy's shower revelation, more worrisome anti-encryption saber-rattling from the EU, a look at a recent Edward Snowden tweet, Samsung's S8 mistake, a questionable approach to online privacy, celebrating the 40th anniversary of Alice and Bob, some quickie feedback loops from our listeners, an update on my projects, and a comprehensive examination of proactive steps users can take to enhance their online privacy.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  It's been a very, very big week.  And I'm going to warn you, we were going to talk about how to protect your privacy in this new era where Internet service providers can spy on you any way they want, but we ran out of time.  It's a two-hour show as it is.  So Steve's going to defer that conversation to next week.  But we do have a lot of security news, including Tavis Ormandy's shower thoughts and why it's a good thing he takes a lot of showers.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 606, recorded Tuesday, April 4th, 2017:  Proactive Privacy.



It's time for Security Now!, the show where we get together with Steve Gibson, the king of so many things, I can't even just pick one thing.  But I will say he is an autodidact and a polymath.  And this show is really a great example of how he can find a subject, dive deep into it, and explain what it means to you and me.  Hi, Steve.



STEVE GIBSON:  My friend, great to be with you again for 606, Episode 606.  Now, you know, 666, that'll be one to keep an eye on.



LEO:  Yeah, that'll be a fun episode.



STEVE:  We've got another year to go.  And fortunately we bypassed April Fool's.  Because that just...



LEO:  Oh, hate it, don't you?



STEVE:  Yes, I do.  It's just, you know.  And so you have to have your guard up, especially if you're in this industry, to like make sure - in fact, of course, theregister.co.uk never misses an opportunity.  And I didn't even remember.  Just, you know, I saw what they did.  It's like, okay, fine.  But I'm glad we did not have to make that decision.



LEO:  It's a minefield.  It's, you know, you just never know if what you're reading is true or not.  In fact, briefly I thought George Takei was running for Congress.  I briefly thought Google had bought Spotify.  And I briefly thought that they had released a product called Google Gnome.  But none of those are true.



STEVE:  Yeah, the only good news, I mean, because we have, like, the whole concept of fake news is big in the world at the moment.  And so at least April Fool's Day would concentrate it all into one event.



LEO:  That's true.  Get all the hoaxes together, yeah.



STEVE:  Exactly.  So what did happen that raised a lot of interest in our listenership in the past week has been, and actually I sort of predicted this last week when we got the news of the House of Representatives quickly passing, and without much apparent scrutiny, this rollback of some legislation from October of last year that the Obama administration had put in place, which has upset everybody - I mean, the EFF is all going crazy, and everyone's running around - the idea that ISPs will be able to collect people's browsing histories without their knowledge or permission and use that for revenue-generating marketing purposes.  And of course then the Senate, as we expected, said, oh, yeah, we think that's a good idea.



LEO:  Good idea.



STEVE:  Now, you know, this is, as we talked about last week, it's part of Trump's overall rollback of overregulation, which is one of the campaign themes.  And so no one has any doubt that...



LEO:  Oh, he signed it on Monday.	



STEVE:  Oh, okay, good.



LEO:  Signed it yesterday.  There was no doubt.  He signed it.  He didn't make a big deal about it.  He didn't show us the bill or anything.  But he signed it, yeah.



STEVE:  Right.  So I got all of this feedback from our listeners about VPNs.  And I've run across a good VPN review site that I want to take a closer look at before I talk about it. 



LEO:  Okay.



STEVE:  But I want to do a - but the problem is that just using a VPN doesn't actually solve any problem because, if you use a VPN, the instant your browser touches the public Internet, Google will lock onto you, know who you are, and simply say, oh, they're now on that IP.



LEO:  Right.  I've even said you're just kicking the privacy can down the road.



STEVE:  Exactly.



LEO:  Now it's not your ISP anymore, it's the VPN and anybody who sees you emerge.



STEVE:  Well, and I thought it would be interesting also to - because there's been a lot of misinformation, not surprisingly, in the popular press about whether, for example, Google and Facebook, how they operate as endpoints relative to an ISP.  And so the argument being that an ISP has this concentration opportunity because all of your traffic runs through them versus a Google or a Facebook, where you have to go there.  Well, that's not all accurate because largely HTTPS traffic is now going through the ISP, which they're blinded to; whereas Google and Facebook have an entirely different profile because, first of all, they're getting your decrypted endpoint traffic.  And their little beacons are scattered everywhere.



So anyway, so today's topic, I titled this "Proactive Privacy" because I want to start by sort of taking a step back and looking at the whole terrain of the technology that our privacy is being compromised by because I and our listeners have no control over what legislation is enacted, but we do have at this point lots of tools available to us.  So I just sort of wanted to kind of lay a foundation, looking at everything, looking at ISPs, the power of the ISP, the power of high spread endpoints, the power of a VPN, what does it provide you, what does incognito mode on a browser actually do, and, like, how all these little pieces fit together into a solution.



LEO:  Good.



STEVE:  Which is how we'll end this podcast.  In the meantime, we've got another iOS update update, more bad news, and some good news on the IoT front.  We have the complete readout on Tavis Ormandy's highly productive shower.



LEO:  Yeah.



STEVE:  Two Saturdays ago.  And I'm going to be on The New Screen Savers on next Saturday with you, Leo, to talk about that also to that audience.  We've got some more worrisome saber rattling coming from the EU, and it's really looking like we're going to get some anti-encryption legislation, probably first there.  And again, in this current U.S. administration, there seems to be little doubt that the same thing will happen here shortly afterward.  So that's certainly relevant to us globally.



I want to take a look at a recent Edward Snowden tweet where I think he got it completely wrong.  A mistake that Samsung made with the S8, which has gotten some attention.  A questionable approach to online privacy.  We're going to celebrate the 40th anniversary of Alice and Bob.  Some quickie feedback loops from our listeners.  An update on the status of my projects.  And then, as I started talking about already, a comprehensive examination of proactive steps that users can take to enhance their online privacy and how all of these little bits and pieces interact.  So I think a great podcast.



LEO:  Good, good.  I knew you would do this, yeah.  I was counting on it.  Of course, you know, we talked about NebuAd and Phorm when that was an issue.  We talked about Verizon's supercookies when that was an issue.  And I feel like this just gives these companies permission.  It's not, I mean, this is no different than it was prior to October of last year.  But now they've been told in effect by the government, nah, go right ahead.



STEVE:  Well, and you know, Leo, my greatest concern is that we're not far from them saying, from an ISP saying, well, if you're going to use our service, you're going to have to put our certificate in your computer.



LEO:  Which means they'd be able to see all that encrypted stuff, yeah.



STEVE:  Which creates them as a middlebox that then allows us to hide nothing.



LEO:  I can see that, at least I can see them attempting that, yeah, yeah.



STEVE:  Yeah.  And so the idea that they're being permitted to aggregate this information, it's just a tiny step from that to, oh, and if you're going to be using our service, we need to be able to protect you from the nasty bad Internet, so here's a certificate you're going to have to use.



LEO:  Right.  All right, Steve Gibson.



STEVE:  So our Picture of the Week is just - it doesn't really relate to security.  It's just I got a chuckle out of it.  It shows someone in a bomb disposal getup, you know, to make them bombproof, with the caption, "On my way to pick up the new Samsung Galaxy S8."  So burn...



LEO:  Literally.  Literally.



STEVE:  Ah, yes.  Like I love the Internet.



LEO:  Actually, you've got to figure the Samsung S8 will be the safest phone ever because the last thing Samsung can afford is any more problems with their batteries; right?



STEVE:  Yes, yes.  Boy, I tell you.  You know, I was flying a little bit, well, I flew up at the beginning of December to do the Christmas special podcast with you and to attend the Christmas party and so forth.  And the airports are all proactively warning you against the Galaxy Note 7.  It's like, okay, that's just really got to hurt.



LEO:  Bad for business, yeah.



STEVE:  So we're often seeing a pattern with iOS updates where, like, very shortly on the tail of a major update comes a double-point release.  And the same thing happened this week.  Just yesterday, well, last week was 10.3, and on last week's podcast we sort of just got a snapshot of the massive number of security fixes that were part of 10.3.  10.3.1 dropped yesterday with a little bit of mystery.  There's the statement that it is important because - but the only thing that Apple was saying was that an attacker within range may be able to execute arbitrary code on the WiFi chip.  There's a stack buffer overflow which this update addresses.



And I dug around everywhere I could look.  Nobody is saying anything more than that.  So we don't know whether this was freshly introduced by the 10.3 update, or this is something that was just found that didn't make it in time, which seems unlikely.  Or whether systems that did not yet update to 10.3 also have it.



Anyway, there just isn't anything more known.  But I tweeted to my followers yesterday that iOS users should update again.  It's another monster.  It's 580MB or something like that, and it takes a long time for the devices to churn through it, but seems to be worth doing because what this means, I mean, it's not like a super, super dangerous, they can get you from some foreign land.  You've got to be within WiFi range.  But it looks like it's an arbitrary code execution if you're within WiFi range of someone's phone, so something you want to take care of.



So it's mostly just sort of a mystery.  It's like, okay, well, it'd be nice to know more about this.  There is a CVE number associated with it.  And this did come from Google's Project Zero that no doubt notified Apple.  So maybe they only just found out about it, and they just immediately pushed out an update.  So, if so, that's good for them and good for us to get this fixed.



This one got a lot of press because - and I titled this the "We should have seen this one coming department" of Security Now!.  And, like, every news outlet covered this because it was just so juicy.  And the headlines were similar to "As many as 90% of Smart TVs are probably vulnerable to wireless hacking via rogue TV signals."  And this podcast and our listeners could almost anticipate this because we've been talking about how anything that interprets a complex signal is very difficult to secure.



Well, look what's happened to Smart TVs.  As we know, they are computers.  And they're smart.  And so a Swiss cybersecurity researcher, Rafael Scheel, who works for Oneconsult, showed a proof-of-concept attack using two different, fully patched, Samsung TVs.  And this is not to single Samsung out at all.  That's what he used.  And it is extremely likely that most, if not all, Smart TVs are vulnerable.  And the scary thing is that they can probably be taken over by broadcasting a malicious video image, that is, you know, in the same way that, for example, the multimedia interpreter in Android phones has had so much problems, because little edge cases were found in its operation.



Similarly, in this case, there is a web server that is always listening in the background.  And an over-the-air signal can exploit a vulnerability that exists in the web server in these Samsung Smart TVs to allow root privilege access and an attacker to then get the TV to essentially reach out onto the Internet in order to make the network that it's on vulnerable.  So this is not good.  So it's not just, for example, the Vault 7 weakness that we learned about, the so-called "Weeping Angel" attack.  And we talked about it a few weeks ago, and we said, yes, but that's a physical local attack exploit where you would need to stick a malicious USB dongle into the TV in order for it to take over.  Well, now we've got one where just receiving a signal over the air can do it.



Now, this particular exploit used the European digital broadcasting standard, which is DVB-T, which is the predominant digital over-the-air system in Europe, comparable to our ATSC, which is our HD over the air technology.  But it's the same.  So it's not that DVB-T is vulnerable where ATSC isn't.  It's just that this Swiss researcher used what he had handy, which was his own local digital over-the-air system.  There's no doubt that you can do the same thing over ATSC and, actually, probably any of the total of four different standards that exist globally.



So this, again, our takeaway is a TV like other IoT devices really needs to be segmented.  It's just, you know, where we are in the world today, these devices are not secure.  PCs - Macs, Windows, and even Linux machines - have, due to their history and just the nature of the way they've come together over a much greater period of time, have vastly more mature security and, similarly I would argue, vastly more sensitive data on them than, for example, your typical light bulb.  And you want to keep them that way.  I mean, IoT devices have vastly less mature security and vastly less sensitive data.



So we really have two very different classes of devices.  We have very secure systems with much more sensitive data in our PCs and much less secure systems with much less sensitive data and much less need for sensitive data in IoT devices.  These separate classes should really be separate security perimeters.  They should not be on the same network.  The default currently is for them to be on the same network.  It's predictable that in the future we will see routers that make strong network segmentation, truly secure segmentation, easy.



But for our audience, for those who are willing to do it, we already know we have the tools now to do that with things like the Ubiquiti EdgeRouter, which can create separate segmented networks where they can't see each other.  And the problem is that smartphones are kind of in between.  You often want to use, you want to control your IoT devices with your smartphone.  Yet it also does have arguably a lot of sensitive information on it.  I would suggest that somebody who was really security conscious should consider maybe a retired smartphone to use on the IoT side that you have scrubbed, and you've restarted it from scratch.  You've scrubbed it, wiped it completely, and then use that as your IoT interface device over on the IoT network.



LEO:  If you're security conscious, you shouldn't be using IoT.  Right?



STEVE:  True.  Well, exactly.  I mean, our favorite acronym is IDIOT, you know, I Don't IoT.



LEO:  Well, yeah.  I mean, if you're really that worried, you'd be crazy to use these devices.  They don't add that much convenience.  The problem is, as you point out, the TV, everybody's got one of those Smart TVs.  So that's not really an IoT device.  That's just an appliance.



STEVE:  Well, it is - well, it is.



LEO:  Well, no, it is an IOT device.  But it's an Internet-connected appliance.  And people don't get it for the IOT capability.  They get it as a TV, but it just happens to have Internet on it.



STEVE:  Right.  And DVRs are now on the Internet and doing things.  And we've seen instances where DVRs are, like, for example, DVRs were being attacked by the Mirai botnet.  So there's an example of - I guess my point is that these non-PC devices are less security mature.  People do want to have them in their home.  And if you are concerned about security, then taking the time to put them on their own network segment makes sense.



LEO:  Absolutely, yeah.



STEVE:  And think about where your valuables are.  Largely your valuables are in your PC.  And most PCs don't have a need to be messing with your light bulbs.  So they could be separated systems.  I just sort of think that's the right way to think about it.



LEO:  I'm not sure I'd agree with you that the issue is they're not mature.  They are certainly for, like, some things.  But those Samsung TVs are running Linux.  It's not that they're not mature operating systems.  It's that they're not paying any attention.



STEVE:  Right, right.



LEO:  Really.  I mean, Microsoft had to be forced to pay attention.  They didn't pay attention either, at first.



STEVE:  Yeah.  It's expensive to pay attention.



LEO:  Right.



STEVE:  Yeah.  The good news is Z-Wave, which is one of the major wireless, low-energy, low-power IoT systems, just on - it was weird.  It was on April 2nd, which is Sunday, is the date of their press release.  I don't know why they would have a press release dated April 2nd, but two days ago announced what they call "S2," which stands for Security 2.  And this is the kind of move which is beginning to move us away from the pure Wild West of IoT.  And the problem, of course, is there's already a huge install base of pre-Security 2 devices.



But this has all of the kinds of things we want to hear.  It's state-of-the-art security specification built into the latest SDK.  Devices to achieve the Z-Wave compliance seal have to be using this updated SDK.  It uses what's known as DTLS, which is something we've talked about in passing, but haven't talked about a lot.  That's essentially TLS over UDP, or so-called Datagram TLS, which lowers the power requirements by dispensing with all of the back-and-forth traffic that setting up a TCP connection entails, allowing secure encrypted UDP packets which are quicker and easier and consume less power.



This also requires devices to have public and private keys.  So for the first time they're getting asymmetric encryption using, happily, elliptic curve encryption, which also uses shorter keys and has much lower energy requirements because it's much easier to do the math for elliptic curve than the more expensive traditional RSA crypto.  And they're using elliptic curve Diffie-Hellman to do key agreement that prevents man-in-the-middle attacks as long as you're able to authenticate the endpoints.  And all of this is built into the lower level protocol supported by and supplied by the SDK.



So, again, all of this sounds like exactly what we need, where then third parties who want to simply have inexpensive IoT devices can get all of this just with a single click, essentially, adding their features on top of the SDK, and the underlying underpinnings are secure.  So bravo to the Wave Alliance, which is the large group of companies that are part of this Z-Wave base.  This is backward compatible, so of course that's a problem because they have to do it, they have to make it backward compatible to make it practical.  But it does mean that, at least moving forward, once this SDK is what Z-Wave IoT devices are built on, they get the advantage of finally the kind of, I mean, again, there may be mistakes that have been made.  But at least the policy, this represents the kind of security policy that we need in order to move forward.  So, yay.



Okay.  Tavis Ormandy's fruitful shower, which is really not a phrase I expected to be...



LEO:  On the show.  We'll be talking fruitful showers today.



STEVE:  We'll be talking about very fruitful showers.  So this was the weekend, Saturday, weekend before last.  Tavis, as we know, had an insightful shower.



LEO:  An epiphany, he said.



STEVE:  Yes, an epiphany.  So, and it was amazing.  So, okay.  Here's what happened.



LEO:  It was amazing.



STEVE:  And we only know about Tavis's amazing shower because the folks at LastPass, as they always have done, got on this immediately.  I mean, they were in a dialogue on Sunday.  So Tavis was impressed.  I'm sorry.  I wrote "Travis."  Tavis.



LEO:  Tavis, yeah.  Tavis is correct, yeah.



STEVE:  Yeah, Tavis, yeah.  So Tavis was impressed.  The 90-day clock barely had a chance to tick before this was already resolved.  And we now know all the details because all of us already have the update.



So here's what happened.  This involved something known as "content scripts."  So in a situation where you've got a web page, and then you have a browser extension, the browser extension is code that runs in the extension.  But there's still the need to inject some JavaScript into the page itself.  And that's called "content scripts" because it's JavaScript running in the page's content.  And that's done, for example, I mean, that's something you still have to do.



For example, there might be URLs in the page which are not clickable.  So the content script could run in the page, scan the page, look for URLs, and turn them into href links to make them clickable.  Or it could be responsible for setting the font size as a function of what device the page is being viewed on.  Or, for example, in the case of something like LastPass, it might add fields to forms, or remove them, or check them for security in order to warn the user that, oops, this is a submission that would not be secure.



So those, you know, so they're very useful.  But so this is scripting that the add-on injects into and embeds in the page to essentially make little modifications to the page's content to enhance what it does.



So Google's own documentation about content scripts states:  "Content scripts execute in a special environment called an 'isolated world.'  They have access to the DOM" - that's the Document Object Model which is, for example, the actual page's formal structure.  And by having formalized the structure of pages, it's then possible for scripting to understand, to interpret and understand the page and make safe modifications or parse its content and process it.  So that's the Document Object Model.



So Google writes:  "They have access to the DOM of the page they are injected into" - that is, these content scripts which execute in a special environment called an "isolated world" - "but not to any JavaScript variables" - this is Google saying this - "not to any JavaScript variables or functions created by the page.  It looks to each content script as if there is no other JavaScript executing on the page it is running on."  So complete isolation, thus called an "isolated world."



And Google says:  "The same is true in reverse:  JavaScript running on the page cannot call any functions or access any variables defined by content scripts."  Google says:  "Isolated worlds allow each content script to make changes to its JavaScript environment without worrying about conflicting with the page or with other content scripts.  For example, a content script could include JQuery v1, and the page could include JQuery v2, and they would not conflict with each other.



"Another important benefit," writes Google, "of isolated worlds is that they completely separate the JavaScript on the page from the JavaScript in extensions.  This allows us to offer extra functionality to content scripts that should not be accessible from web pages without worrying about web pages accessing it."  Okay?  Except that's not true.  That is, everything I just read, uh, not quite true.



LEO:  It's a vision they had for it.



STEVE:  Yes, it would be nice.  Okay.  So just to step back a bit, all of that, the whole need for that is a direct consequence of what a total cluster you-know-what JavaScript is.  I mean, it is a catastrophe.  It is an abomination.  But it's what we have.  As we know, it began a long time ago with Netscape that wanted to make pages more active.  And they also wanted nonprogrammers to be able to do this.  Just they didn't want to have to declare variables, not even to say, okay, is this an integer?  Is this variable going to contain an integer or going to contain a string?  They said, uh, let's just kind of have it figure it out.  And, oh, I mean, it's just - so sort of it's trying to be everything for everyone.  And as a consequence, it's just - it's an abomination.  But it's what we've got.



So as a consequence it's sort of - it's self-typing.  It's got something called "garbage collection," the so-called "automatic language," where if the interpreter figures out and it's its responsibility that you're no longer using a variable, then it garbage collects it because the variable becomes garbage, and so it frees the memory that it's associated with.  I mean, it's just it's an incredible nightmare.  But it's what we have.



So, for example, there's no notion of what's called "namespaces."  In a formal, well-designed language, you've got this concept of a namespace.  And having separate namespaces allows code to use a certain name, like a variable "n," and for different code existing in a different namespace to also freely use whatever variable names it wants, like "n," and not have those two different n's refer to the same thing.  But JavaScript doesn't have that.  There is no namespace concept in JavaScript.  So for a long time, and even today, code that's going to run in a page may very well stomp on the variables which the page uses, unless it's extremely careful not to.



So consequently there's this notion, there's all kinds of hoops have been created that code has to jump through.  One of the techniques is called "closures," where you're able to, like, enclose all of your JavaScript within a single variable, if you can believe it, and just worry about that one variable being unique, and then the object-oriented things that occur within this closure, I mean, it's just a catastrophe.  So along comes this, okay, we're going to create isolated worlds.



Okay.  Well, it turns out there is a place where worlds collide.  And this is what occurred to Tavis on a Saturday morning, March 25th, in the shower.  He was no doubt ruminating while he was lathering up about the code that he'd been reading.  You know, he'd been going through LastPass's code, looking at it, and going, "Okay, okay, hmm, hmm, hmm, hmm," you know, as Tavis will, looking for anything that seemed wrong.  Well, one of the problems in JavaScript is you do not need to define a variable.  You can, you know, and my code does.  But you don't have to.  So the idea is that the first time you use it, the JavaScript interpreter goes, oh, and kind of looks at what you're assigning to that variable and goes, oh, that looks like a number.  Okay, fine.  We'll be an integer today.  Or, oh, that looks like a string?  Okay, fine.  And it also freely mutates them back and forth as needed.



It turns out it's handy to be able to ask JavaScript if a variable has been defined, that is, are you currently aware of a variable by this name?  So you can use the "type of" function to ask for the type of a variable, like, are you an integer?  Are you a string?  And you can even say, are you undefined?  So one of the things you can do is ask whether a variable has been defined or not.



So it turns out that the LastPass authors used the undefined property as a means of turning on their own content, their injected content, in order to enable it or not.  That is, they were using this notion of these completely separate worlds, these isolated worlds.  And so throughout their content scripts, which they inject into the page, they use "type of" and then a variable, whether or not it's equal to "undefined."  So that's a way for them to determine whether some secure code should be allowed to run or not.  If the thing is not defined, then don't do this because they're using that as a switch.



What hit Tavis in the shower was that one of the ways, one of the things that the isolated worlds do have in common is the Document Object Model.  And although a page cannot directly define variables that would collide, the isolated worlds do share visibility into the same Document Object Model.  That is, there's only one page.  So even though the worlds may be isolated, the place where they have visibility, the place where they intersect, essentially, is in all being on the same page, that is, having the same Document Object Model.  And it turns out that the Document Object Model can assign properties to objects in the DOM, which has the side effect of defining the variable.  And so, believe it or not, somewhere during his rinse cycle, Tavis realized, holy crap, LastPass's approach is to use the undefinedness of their variables to turn code on or off.



LEO:  Oh, yick.



STEVE:  Yes.



LEO:  That's awful.



STEVE:  Yes.



LEO:  Do you think Joe Siegrist did this?  Or is this something that was done later?



STEVE:  I have no idea.



LEO:  That's sloppy.



STEVE:  Well, it's JavaScript.



LEO:  No, but there's no excuse.  I mean, as soon as you say "undefined," I mean, that...



STEVE:  And that's why I'm saying it is completely valid.



LEO:  It works.



STEVE:  Because this is how horrible JavaScript is.



LEO:  Well, it lets you do that.  But then it's incumbent on you...



STEVE:  No, it's formally...  



LEO:  It encourages you to do that, yeah.



STEVE:  Yes, exactly.  It's formally something that people do.  And I don't know, I'm sure, Leo, that you've used systems enough that you've run across apps where something will say "NAN," and it's like, what?  NAN?  What's NAN?  Well, that's Not A Number.  And that's literally the way JavaScript tells something that you're misusing.  And sometimes that pops out on the UI.  I mean, JavaScript is truly just a horrific abomination.  But it's what we have.



LEO:  Right.



STEVE:  So Tavis got a hold of the LastPass guys and said, "Guess what, guys, it's actually possible for the DOM, code in the DOM to assign a property to a DOM element which will cause a variable to no longer be undefined.  That turns code on in your code that you never intended to have turned on unless you were in control."  And they're like, "Holy crap."



LEO:  Yeah.  See, that's what's sloppy.  You know, you're taking advantage of a side effect.  I mean, that's sloppy, I think.



STEVE:  Well, I think it's brilliant on Tavis's part.



LEO:  Well, to figure it out, yeah.  No, but I'm saying it's sloppy for LastPass to have used that.



STEVE:  I disagree.



LEO:  Really?



STEVE:  I don't mean to be [crosstalk].



LEO:  It's colloquial in JavaScript.



STEVE:  Yes, exactly.  I mean, it's what everyone does.  And it just - but should they not have done it?  Well, yes.  Did they regret it?  Yes.



LEO:  Yes.



STEVE:  And how many instances do they have to fix?  More than 3,000.



LEO:  They did it a lot.  They did it a lot. 



STEVE:  Yes.



LEO:  This was a colloquialism they liked.



STEVE:  Oh, yes.  It wasn't one place.  It was, I mean, and again.  And so that demonstrates that this is sort of - this is what you do in JavaScript.  So, and everybody could look at that and see nothing wrong with it.  That's really my point.  And notice that it, I mean, on one hand, I guess I'm, you know, yes, they wish they hadn't done it.  Now they are saying they're setting all of those variables to false, and they're checking the Boolean-ness of them, you know, is it true or false.  Or they're setting it to negative one, or they're doing something so that there isn't this undefined-ness any longer.  But anyway, again, hats off to Tavis for, like, just ruminating on some code that he saw and going, oh, wait a minute, you know, that's not safe to do. 



LEO:  Yeah.



STEVE:  And I know that LastPass learned a lesson.  I hope all other JavaScript authors pay attention to this because, again, this is not something they technically shouldn't have done.  This is a consequence...



LEO:  Well...



STEVE:  It isn't.



LEO:  It's allowed, and it might be colloquial, but clearly it's a bad practice.  Right?



STEVE:  I don't think so.  I mean, again, that's why I would say every JavaScript author writing secure software needs to make sure they're not doing the same thing.



LEO:  This wasn't in "JavaScript:  The Good Parts," I can promise you.  Of course, it's a very thin book.



STEVE:  No.  Remember, yes, exactly, I was just going to say, remember it was one of our photos.  There was, like, "JavaScript:  The Bible" was two inches thick, and "JavaScript:  The Good Parts" was, like, you know...



LEO:  Yeah, tiny.



STEVE:  It was like the Appendix.



LEO:  The problem is programs want to be clever.  They want to save space.  They want to show off for whatever reason.  And so this is a clever trick, but it bit them.  I just don't think - don't be clever, be explicit.  I don't know.  I just - I feel like that's sloppy.  Now, it does raise some questions.  They fixed it on Friday; right?



STEVE:  If it weren't formally, see, it's in the formal language definition.



LEO:  But the side effect of testing a variable to turn on your code isn't.



STEVE:  True.



LEO:  So that's going a little - that's clever.  That's saying, oh, you know, I think you should be more explicit about that; right?



STEVE:  I don't disagree, yes.



LEO:  I mean, I understand that the side effect is in the spec.  But you shouldn't rely on it to turn on code.  Anyway, so but my question - here's the question.  A couple of questions come to mind.  Well, of course LastPass fixed it.  I immediately, though, and I will not put it back, took the Chromium extension out.  I am not going to use the autofill feature on LastPass because that's the JavaScript part.  I just stopped doing that because I don't trust them, if they did that.  That's one bug they've, you know, actually it's the third this month.  But that's one bug.  But who knows what other cleverness is in there?  Right?  Doesn't this tarnish LastPass?  I guess that's the question.



STEVE:  Only in the eyes of non-security-aware people.  And seriously, I would say that, first of all, what you did I completely understand.  But it shouldn't be LastPass, it should be any password browser extension.



LEO:  Yes, yes, yes.  And that's what I did, yeah.  I took them all off, yeah.



STEVE:  So the lesson I think we are learning is browsers are just not secure enough to be trusted to have all of our passwords.  That is, it is too difficult to secure them.  So, you know, I'm still using LastPass because I would rather use a piece of software that it takes serious Tavis showers to find problems in.  In other words, Tavis has found problems in every password manager he's looked at, and we've talked about other ones.  LastPass is just the biggest target, and it's got the largest market share.  So it's like, do you fire a person for making a mistake?  Or do you recognize that they've learned from their mistake, and they're a better employee for having done that?  I mean, and people come down on both sides of that question.



I guess what I focus on is policy.  And LastPass I think from day one has had the right policy.  They've jumped on this stuff immediately.  They fix problems within hours.  And so I consider it to be a well-vetted password manager.  Someone could certainly choose a less well-examined password manager.



LEO:  Well, that's the problem.  What do you use instead; right?



STEVE:  Precisely.  But we talked last week about KeePass as a...



LEO:  It's open source, yeah.  



STEVE:  Multiplatform, open source.  If you're no longer comfortable with integrating a password manager with your browser, then using an external repository that is encrypted and multiplatform, I completely understand that someone might choose to make that choice.



LEO:  Yeah.  And, you know, the other very important point, and the one I make, see, I'm asking these questions because I have to make a recommendation to neophytes on the radio show and stuff.  And it still is far better than what they're doing, which is reusing the same password all the time, or making up passwords that are combinations of birthdates and kids' initials.  So it's better to use a password vault no matter what.  These are obscure bugs, hard to find, hard to take advantage of.



I still want to make the assertion that it does show a sloppiness in programming that worries me.  I understand this is idiomatic JavaScript.  It's widely used.  But this is a security product.  And I worry that it shows a lack of attention to the detail in the JavaScript portions.  I'm not going to use the JavaScript stuff anymore.  I'm going to use their binary app.  Unfortunately, there isn't one for Linux, or even apparently Windows.  On the Mac it's easy.  They have a standalone binary app you can look up passwords in.  Then you have the risk of copying and pasting it.  But there's risks everywhere.



STEVE:  Yeah.



LEO:  I just worry, I think, I worry about their JavaScript practices.  Do you know what I'm saying?  I mean, it seems sloppy.



STEVE:  I guess my point is we wouldn't know unless Tavis had pointed this out.  And he pointed it out because he was scrutinizing it deeply. 



LEO:  I understand that.  But isn't it - maybe I'm wrong, and correct me.  It seems like bad programming practice to use the side effect of a test on whether a variable exists to turn on code.  That seems like just poor practice.  I understand it's idiomatic, JavaScript allows it, and many use it.  But it does seem like poor practice.  Maybe I'm wrong.



STEVE:  I think it's just as good, I mean, it's really no different than if you set the variables to false, that is, if you defined them all and set them to false and relied on that.  You can rely on...



LEO:  Oh, but that wouldn't have had the bug, would it?



STEVE:  Not this bug.  But it's the fact that you can define a variable through the object model creates...



LEO:  Yeah, well, that's another thing.  They should use strong typing.  You can use strong typing in JavaScript.  It doesn't force it, but you can use it.  Why wouldn't they use it?



STEVE:  Well, so I guess my point is that JavaScript formally allows you.



LEO:  I understand.  But you're writing a security product.  And it's well known in the general world - this is why I keep harping on, when kids say what language should I learn, I say learn a functional language.  Start with LISP or something like that because it won't allow side effects.  It punishes side effects.  The compiler - and it has strong typing.  You should have strong typing.  I don't have a problem with closures or garbage collecting.  



STEVE:  Hey, you're talking to an assembly language programmer.



LEO:  Well, I know you don't have garbage collecting.  You do it all by hand.



STEVE:  Talk about strong typing.



LEO:  Yeah, right.  Well, actually you have no typing.



STEVE:  Correct.



LEO:  You can stick anything in that register.  You just have to keep track of it.



STEVE:  It's entirely up to you, exactly.



LEO:  But in a way that actually is good because you know that.  You don't rely on some weak mechanism.  You know you can't just assume what's in that register.



STEVE:  Correct.  There are no assumptions.



LEO:  Yeah.



STEVE:  They always bite you.



LEO:  Yeah.



STEVE:  So I guess my point is, you know, I'm staying with LastPass; although, again, when I do the math, when I think about an external password vault, it can generate a random password.  I can copy and paste it in.  Now, the one thing it doesn't protect us from that an integrated password manager does is site spoofing, where the URL is a lookalike URL.  So if you go to a spoofed site that looks exactly like PayPal or eBay or something, or Amazon, you'd be more likely to cut, copy, and paste your credentials into that; whereas, if you have an integrated password manager, it's not going to fill the form in, and you're going to go, wait a minute.  Why doesn't it recognize this site?  And so it would tend to bring that to your attention.  And arguably that's still a major security vulnerability that no one has come up with a solution for.



LEO:  Let's say I'm listening to this, and I, you know, just on an excess of prudence, decide to move to KeePass or Pass or some other more secure but much less convenient solution.  But LastPass, how do I null my LastPass vault on their servers?  Should I just erase all my passwords and let it sync?  I guess I could do that.  I kind of wish there were a way that you - maybe there is a way that we can go to LastPass and say, "Can you please delete everything?  I don't trust you."



STEVE:  Okay, well, there's never been - okay.  All of this is endpoint.



LEO:  I know.  I know, I know.  I know.  There's never been a breach.  I know.



STEVE:  There's never been a breach.  But more importantly, and the reason I originally recommended them, was that the crypto technology gives them zero visibility into our stuff.



LEO:  It is Trust No One, right, on that password vault?



STEVE:  It absolutely is.



LEO:  Except that, if somebody did breach their vaults and downloaded it, and it is, you know, it's going to be a target because that's where a lot of people store their passwords, they would have, at their leisure, time to try to crack it.  But they're using PBKDF2, and they're using all sorts of salting, yeah.



STEVE:  Yeah.  And actually all of that is done on the endpoint side so that the only thing that's there is an absolutely high-entropy key.  So you actually cannot third-party crack...



LEO:  You couldn't brute-force it.



STEVE:  ...the LastPass cloud.  So stopping using LastPass and expunging it from your life, that's enough to prevent anyone from getting access to your stuff.  The only, as far as we know, the only way in is through one of the endpoints because the endpoints are what have to be able to access that vault.



LEO:  Right.  That's the vulnerability, of course.  And that's why I think it is prudent not to use the JavaScript plugin; right?



STEVE:  I have to say - but again, I continue to feel that, if you are going to, I would use LastPass.  



LEO:  Yeah.



STEVE:  Because it's been...



LEO:  Over any other JavaScript plugin, right, right, right.



STEVE:  Precisely.  Precisely.  Because it has been heavily vetted.  If Tavis turned his showerhead on anything else, they would melt like butter.  This thing has been, LastPass has been pounded on, again.



LEO:  And the good news is hackers don't take as many showers as Tavis Ormandy.  So we're safe in that regard.



STEVE:  And on that note I think we should take a break.



LEO:  Great discussion.  I really wanted to talk to you all about this because, you know, there are so many questions.  But I think I will, based on you and this discussion, I will continue to recommend LastPass to all our users.  And even, you know, because convenience is so important to normal users, even the browser plugin.  Because they're more likely to use it, generate good passwords and all of that.



STEVE:  Yeah.  And we have yet to actually have an exploit.  What we have is every website out there has lost the control of their passwords.  But we haven't actually had a single, that we know of, instance of exploitation.  So that says, exactly as you say, Leo, for the typical user, LastPass is what you want to use more than not.  For the super security-conscious person, I endorse what you're suggesting, which is go to a standalone vault solution and then port the data into the page on your own.



LEO:  I like - there's a solution, I wish it were more cross-platform, called Pass, open source solution, that uses PGP keys to encrypt the passwords.  So it's on your drive, encrypted with your PGP key.  It's a password vault, but it doesn't have any convenience at all.  You have to, you know.  And we know that clipboards are dangerous.  So you'll be tempted to use the clipboard, and that would be a bad thing.



STEVE:  Oh, and as a matter of fact, a friend of the podcast and someone who hangs out and contributes in GRC's newsgroups, Greg Bell, reminded me last week.  I was talking about the clipboard problem relative to KeePass, and he said that KeePass does a keystroke entry.



LEO:  Yeah, yeah.  Isn't that cool?  On Windows, yeah, yeah.



STEVE:  So that's much nicer, yes.



LEO:  That's really nice, yeah.



STEVE:  And you and I are going to talk about this again on The New Screen Savers on Saturday.



LEO:  I know.  At less length because it's only an hour show.  But we absolutely will.  Thank you, Steve, as always.  And I don't think I'm alone.  I think we all wait till Tuesday, and we go, okay.  Got to hear what Steve says on this one.  So thank you, I appreciate it.  We've got more, lots more to talk about.



STEVE:  So last week we talked about the U.K. Home Secretary, Amber Rudd, and her post-terrorist attack rhetoric about the need for law enforcement to have access into encrypted communications.



LEO:  [Growling]



STEVE:  I know.  And in fact she singled out WhatsApp, although also mentioned Telegram, Signal, and so forth.  Now, a couple days later, the EU Justice Commissioner, Vera Jourova, said actually a week ago, last Tuesday, that the entire European Commission is planning to propose new measures this June, so a couple months from now, to make it easier for police to access data on Internet messaging apps such as WhatsApp.  Now, of course, we know that the technology has been designed to thwart that.  So simply passing a law is just not going to make it immediately so.



Anyway, Jourova said she will announce three or four options, including binding legislation and voluntary agreements with companies to allow law enforcement authorities to demand information from Internet messaging apps "with a swift, reliable response."  And this announcement comes as interior ministers from other EU countries have amped up pressure on the Commission to introduce new rules to help police crack through secure encryption and demand private data for investigations.



And the position they're taking is interesting.  They're saying that non-legislative measures will be provisional "to have a quick solution" because they recognize that negotiations over EU laws can drag on for years before they're passed, unlike in the U.S. where we now - where laws that we want to pass apparently only take a few days to get signed into law, or to have laws overturned.



Anyway, finally she said that:  "At the moment, prosecutors, judges, also police and law enforcement authorities are dependent on whether or not providers will voluntarily provide the access and the evidence.  This is not the way we can facilitate and ensure the security of Europeans, being dependent on some voluntary action."  Jourova also said that the measures would make it easier for law enforcement authorities to request and access data from online services that are registered outside their jurisdictions.  And both the French and German interior ministers have followed suit and said, yes, they want the same thing.



So as we've been predicting for quite some time, it doesn't look to me like the current position that the developers of these applications have taken in the wake of the Snowden/NSA revelations are going to withstand the future legislation.  I think that what we're going to end up with is, my guess is, not any notion of a single golden key, something that law enforcement can have to unilaterally decrypt communications.  But it'll be the case that individual app providers will have to build some means to respond to court orders in order to give authorities access to specific communications, given a warrant.  I'll bet you that's what we're going to end up with here, at least in the U.S.  I don't know enough about the way the laws work in the U.K.



LEO:  The really good news in the long run is trust the math.  It's already out there.  Strong encryption exists.  It's available, not just from U.S. companies but companies all over the world.  And so people who want strong encryption and understand how to achieve it are never going to lose that.  But the bad news is people who use Apple Messages or Facebook or WhatsApp and assume that they're encrypted and protected will no longer be.



STEVE:  Well, I would say they will be - the reason I think this is what we're going to get is it'll be like the search warrants we have now, is we have this notion of illegal search and seizure.  You need, for example, a search warrant in order to break into someone's home and have legal access to the contents.  Similarly, I think we will see legislation where a warrant is provided to an Apple or to a Facebook compelling them to provide within some range the decrypted communications on a case-by-case basis.  So people feel in the U.S. safe against the police knocking their door down.  And I think people will feel safe, or should feel safe, that the default is that their communications is not in the clear, easily snoopable by people without legal warrant to have access to it.  So it's the way the U.S. and the Constitution we have has been slicing that.  And to me, I think that's what we're going to end up with.



And what I guess part of this will be, I guess the other question will be, and that's the point you raised, Leo, what about third-party solutions?  That is, will using undecipherable communications be outlawed?  That's the other shoe is that it's one thing to say Apple and Facebook and, like, mainstream providers have to provide, have to be able to respond to a search warrant.  But what about an individual who uses his own OpenVPN, for which there is no known backdoor, and it's open source, and we trust it as much as anything?  What about that?  That is, you know, a point-to-point private link that is not from a major provider.  As you say, trust the math.  We've discussed this before.  Will the use of non-decryptable communications be outlawed?



LEO:  Well, that's the endgame.  It's the only thing you can do.



STEVE:  Yeah, I know.



LEO:  You have to say it's illegal to use WhatsApp.  And then outlaw crypto, and only outlaws will have crypto.



STEVE:  Right.



LEO:  So, you know, but given the administration and their attitude towards this, it seems to me, as it does obviously to you, inevitable.



STEVE:  I think it's a fait accompli.  I think it's a matter of somebody writing up a bill, and the House and the Senate will pass it, and our President will sign it, and then there will have to be some technology redesign in order for the companies producing non-decryptable communications to be able to respond to court orders.



LEO:  It's inevitable that the White House and Congress will go for this.  But remember the intelligence agencies do not necessarily support this.  The NSA understands putting backdoors in crypto puts the nation at risk.  And they have said this many times.  So they will get - I think there'll be some pushback from our intelligence agencies, of all people.  The same people who are saying we need backdoors, when the FBI says, "We need backdoors," the NSA responds, "Shut up.  You don't want backdoors."  There's disagreement in the intelligence community.  So it's more - I'm not - it may not be a fait accompli.  But it's going to come up, for sure, for sure.



STEVE:  Well, Apple, to take a case in point, Apple could argue that this puts too much responsibility on them.  That is, they've designed a system where they cannot see into their customers' communications.  And they don't want to be able to see into it.



LEO:  Unfortunately, they can see iCloud, and they easily provide that information to law enforcement.  And that's kind of their tit for tat.  They're saying, well, we won't let you look at messages, but you can always look at the iCloud.  That's their way of kind of appeasing law enforcement.



STEVE:  Yeah, I mean, you know, law enforcement does not want, I mean, no government ultimately wants their citizenry to be able to have absolutely private communications.  They just - they don't.



LEO:  They can't spy on them.  They can't figure out they're terrorists.  They can't figure out what's going on.



STEVE:  Precisely.



LEO:  Now, I have to say this is not speculation.  We know what the impact of this is because we tried it once before.  The government outlawed strong encryption in browsers, remember, and enforced 40-bit encryption.  And to this day we have weak encryption in browsers.  We're fighting this battle 20 years later because of this bad policy in the '80s.



STEVE:  Yup.



LEO:  And we're still - a lot of the breaches, many of the breaches we talk about are because of this.



STEVE:  Yes.  Gee, 40 bits?  You think that's a secure key?  Uh, no.



LEO:  The government said, oh, no, no, you can't have strong encryption.  Well, and then look what happened.  So the problem is, yes, it helps us find bad guys.  But it also helps the bad guys find us.  The real problem is the phone.  Because while I can use - I know how to use encrypted messaging.  If my phone is not encrypted, all the stuff that's on my phone is available.



STEVE:  Right.



LEO:  And I don't - at this point nobody's making phones that have strong - I don't know where we'd get that.



STEVE:  I didn't put this in the show notes today, but did you see this news that apparently part of this administration's border policy...



LEO:  Yeah.  If you're not a U.S. citizen coming into this country, they can ask you everything, including how you like Trump, what your phone numbers are in your contact list.



STEVE:  And they're now saying your social media account passwords.



LEO:  All of your passwords to your accounts.  All of that.



STEVE:  What?



LEO:  Or you don't get into the country.



STEVE:  Ooph.



LEO:  We're not going to have a lot of tourism in the next few years, I don't think.



STEVE:  No, in fact it's already dropped off.  Well, speaking of dropping off, Edward Snowden tweeted something that I just thought, okay, Edward, that is so wrong.  He said:  "Huge," sounding like you know who.



LEO:  Huge.



STEVE:  "Huge:  USG confirms cyber offense funded at 9x the rate of cyber defense."  Then he says:  "Wonder why we can't stop foreign hacks?  This is why."  And I'm like, no, it's not.  It's the most ridiculous thing I've ever heard.  Okay.  So offense and defense sound like symmetric things; right?  You attack or you defend, like one for one.  But they're only reciprocal in literally a one-on-one situation.  When it's many to many, it changes it completely.  So my point is that it is entirely different to attack a single remote entity than it is to defend against all possible attackers attacking all possible targets that you're responsible for.



So, I mean, I would argue that offense is a lot more sexy, and so it gets dollars.  But the problem is there isn't a good - we don't have a solution for defense.  I mean, in the same way that I said years ago that I would have a nervous breakdown if someone said I had to be in charge of Sony's security.  It's like, no.  You can't secure that.  Well, so this notion that the reason we can't stop foreign hacks is we're not, as Edward suggests, we're not spending equally on defense as offense, that's just nonsense.  We can't stop foreign hacks because we can't stop any hacks.  I mean, because...



LEO:  Geez.  Oh, man.



STEVE:  ...our systems are porous.



LEO:  Couldn't you spend some money on, like, protecting the electrical grid or something?



STEVE:  Oh, I mean, there are definitely - yes, I completely agree.  There are things we are, for example, we're conspicuously not doing that we should, that those in the know are begging for money for, that they're probably not getting.  And so I certainly would agree with Edward that the idea that we're not spending as much as we should to defend specific aspects of our infrastructure, I would agree with completely.  But I just - the idea that he was putting offense and defense on the same footing in the Internet world just sort of seemed crazy because, again, it's easy to attack.  It's incredibly difficult to defend.



LEO:  Did you read, I don't want to distract you too much, but James Woolsey's opinion piece in The Hill about the real threat that North Korea would explode an air burst, use EMP to disable our grid.



STEVE:  Yes.



LEO:  Causing riots within just few days as grocery stores and everything else went down.  There was actually a response to it, a good response, I thought, from Popular Mechanics.  But Woolsey is head of a group that is exploring the risks of EMP, electromagnetic pulse.



STEVE:  Good.  I'm glad somebody is because that's, ooh, not good.



LEO:  He was a CIA director in the '90s.  And then the article is co-written by Dr. Peter Vincent Pry, who is chief of staff of the Congressional EMP Commission.  I didn't even know there was one.  But I'm glad.



STEVE:  They're actually - there's a non-nuclear EMP gun.  And if the Portable Dog Killer hadn't done the job, then, you know...



LEO:  But that's the funny thing about EMP.  It wouldn't harm the dogs.



STEVE:  No.  In fact, I've seen this EMP gun.  You can just cause a car to stall.  You just shoot this thing at it, and the car just, you know, it just stops.



LEO:  I think ultimately all of the things we talk about on this show are going to be small potatoes compared to the real-world risks we're facing.



STEVE:  Yeah.  So Samsung made a mistake with the S8 by allowing a person's face to unlock the phone.



LEO:  Ooh, yeah.



STEVE:  You know, it took, like, what, minutes...



LEO:  No, they did it at the demo.  At the event somebody did it.  They took a picture of themselves with their camera.



STEVE:  Seconds.



LEO:  And showed it to the - but in their defense, Samsung did say it's a less secure method than the fingerprint [crosstalk].



STEVE:  Well, and thus is the problem.  They have facial recognition, fingerprint, and iris security.  And so what they've done is they've mixed toy security with good security and, as a consequence, muddied the waters.  And I think that's a bad mistake.  It's like - so what's the point of having your face unlock the phone if your daughter can wave the phone in front of you while you're sleeping and unlock your phone?  And so it's either don't lock your phone, or have good security for unlocking your phone, rather than this basically toy security.



And so at this point - and so my point was that what they've done is they've introduced ridiculous security, which everyone is attacking, and said, oh, yes, well, that wasn't meant to be secure.  It was meant to just sort of be fun.  Well, okay.  But what you've now got in the first week after launch is a huge amount of bad press and people laughing at your brand new phone for having bad security.  So just it was a bad idea.



LEO:  Yeah.  But it's fun.



STEVE:  Yes.  Yes, it is.



LEO:  You know, there's a similar technology in Windows.  But it has two cameras.  It has a dimensional camera that can measure depth.  And then you can measure the depth of an eye socket, you know, things that a picture wouldn't fool.



STEVE:  Correct.  And what I was thinking was imagine having you, like, having to say something it shows you.  Then you get speech, and you get animation of the person's face, saying what it's asked you to say, which would be - it would raise the difficulty of spoofing much higher.  



LEO:  Yeah, yeah.



STEVE:  But they didn't do that.



LEO:  Actually, my banking app does that.  It says "Blink."



STEVE:  Nice.



LEO:  Yeah.



STEVE:  Nice.



LEO:  Isn't that clever?



STEVE:  Nice.  So, and here's another kind of, I don't know, questionable solution.  But I know it's going to appeal to some of our listeners.  So I wanted to share it.  It's called Noiszy.com, spelled strangely:  N-O-I-S-Z-Y dot com.  N-O-I-S-Z-Y dot com.  It is a Chrome extension which hides your actual Internet traffic among noise that it generates.  So the Chrome extension describes itself as, it says:  "They're listening.  Make some noise.  Whatever you do online, you leave digital tracks behind.  These digital footprints are used to market to you and influence your thinking and behavior.  Congress has voted to allow ISPs to collect and sell your online information without your consent.



"Erasing these footprints, or not leaving them in the first place, is becoming more difficult and less effective.  Hiding from data collection isn't working.  Instead, we can make our collected data less actionable by leaving misleading tracks, camouflaging our true behavior.  We can resist being manipulated," they write, "by making ourselves harder to analyze, both individually and collectively.  We can take back the power of our data.



"Noiszy," I don't know how you pronounce it.  "Noiszy [N-O-I-S-Z-Y] is a browser plugin that creates meaningless web data digital noise.  It visits and navigates around websites from within your browser, leaving misleading digital footprints around the Internet.  Noiszy only visits a list of sites you approve, and only works when you turn it on.  Run Noiszy in the background while you're working, or start Noiszy when you're not using your browser, and it sends meaningless data to these sites as long as you let it run.  This meaningless data dilutes the significance of your 'real' data by creating a campaign of misinformation.  You become more difficult for an algorithm to understand, market to, or manipulate.  You can outsmart the filter bubble."



To use Noiszy in Chrome, open a new tab.  Click the Noiszy icon.  Choose the sites you want Noiszy to randomly browse and click Start.  Noiszy randomly chooses from your list of sites.  Then it randomly chooses from and clicks on links within those pages, choosing only onsite links that don't open new windows.  There are delays between clicks of about a minute.  This makes web traffic appear to be more real and engaged.  After about two to five onsite clicks, Noiszy randomly chooses another site from the list and repeats the process.  This continues...



LEO:  It seems like such a bad idea.



STEVE:  ...even when your tab is in the background, until you close the tab or click Stop.  So there it is, folks.



LEO:  You're going to really annoy people.  I hope you don't have bandwidth caps, that's all I can say.



STEVE:  That's right.  Don't do this on your cell phone.  Don't leave it running by mistake.  Anyway, I got a bunch of people saying, hey, what do you think?  I don't know.  I think maybe we will wrap up this podcast by talking about the proactive things you can do to use the 'Net without being tracked, rather than try to throw a lot of garbage at the wall and hope that the stuff you care about isn't figured out.  Because, frankly, it probably will be. 



LEO:  Yeah.  We're in an interesting seesaw battle; aren't we?  On we go, Steve.



STEVE:  So Mikko Hypponen, who is F-Secure's chief security research officer...



LEO:  We had a great Triangulation with him.  He is really, really neat.  I love him.



STEVE:  Yes.  He tweeted:  "Happy 40th birthday to Alice and Bob!  They were introduced in an April 1977 paper by Rivest, Shamir and Adleman."  Those initials are R, S, and A.  So Alice and Bob, for those who haven't read a lot of crypto papers, they first appeared 40 years ago in this classic paper as the endpoints in a conversation, or the actors in sort of a thought experiment, as Alice and Bob is A and B, essentially.  So, okay.  So this is 40 years ago, 1977.  I was 22 and, you know, four years out of high school.  That's how long ago this was.  And I'll just read just the first three paragraphs of the abstract.  And what I'm struck by is first how modest-sounding this is, and how, like, how we take it for granted today.



So the paper was titled, and for anyone who's interested I have a link to the original 40-year-old paper in a PDF which is at the AI Lab at MIT's website.  It's titled "A Method for Obtaining Digital Signatures and Public-Key Cryptosystems," all which they had just invented.  The abstract reads:  "An encryption method is presented with the novel property that publicly revealing an encryption key does not thereby reveal the corresponding decryption key.  This has two important consequences:



"Number one, couriers or other secure means are not needed to transmit keys" - which had always been the case before - "since a message can be enciphered using an encryption key publicly revealed by the intended recipient.  Only he can decipher the message, since only he knows the corresponding decryption key."  And this was unheard of at the time.



"Number two, a message can be 'signed'" - they have in quotes because this was novel - "using a privately held decryption key."  So this is sort of the reverse process.  First you encrypt with a public key, which can only be decrypted with the private key.  Now you sign with your privately held decryption key.  "Anyone," they write, "can verify the signature using the corresponding publicly revealed encryption key.  Signatures cannot be forged, and a signer cannot later deny the validity of his signature.  This has obvious applications," they write, "in 'electronic mail'" - that was also new at the time...



LEO:  Brand new, yeah.  This new thing.



STEVE:  ... "and 'electronic funds transfer,'" as we used to call it, "systems."  So anyway, wow, four decades.  And now, I mean, you can't imagine life without this technology.  



LEO:  It's so brilliant, too.



STEVE:  Yes, and we've had so much fun talking about how you leverage the concept of asymmetric keys, where they are separate, and they're related, but you can't get one from the other.  And you can use them in all these amazing, clever ways.



So to close the loop with some of our listeners, the Fat Vegan Chef, and that's his Twitter handle, said, he said - he sent me a tweet.  "@SGgrc Can ISP install a certificate that will give them access to secure traffic unencrypted?  If/then browsing securely is" - he said "mute," but he meant "moot" - "and they can data mine all they want."  Okay, so, no.  If it were possible for an ISP or anyone on the Internet to unilaterally force a certificate on us, the jig would have been up a long time ago.  The game would have been over.



What will happen if this does happen is that we will be asked, we will be required by our ISP to download an app.  And running the app on our machine will install a certificate into our machine's root store.  That, for example, there are, in an enterprise environment, there are means by which the enterprise's Intranet's scripting can do that to your systems without your knowledge or permission.  That is, that's part of the privilege.  But a browser cannot.  It's got to be outside the browser, essentially at the OS level.  So it won't be something that can just be done.  It will require a proactive deliberate action from ISP subscribers.  And I dread the day that that becomes part of what we have to do.  I mean, that will - ooh, boy.  Let us just hope it doesn't happen.



Simon Zerafa, who frequently contributes to the show through pointing me to things, he quoted somebody that just made me shudder.  Frank Denis tweeted:  "Support for Random.org as a CSPRNG" - that's a Cryptographically Secure Pseudorandom Number Generator - "was added to libsodium."  And then has in parens "(will be the default soon).  Note that this requires libcurl built with OpenSSL."  And my response to Simon was, "Yuck.  What a kludge."  Though I suppose in an environment where there is absolutely no good local source of entropy, reaching out would be better than nothing.  But it's also easy to see how a spoofed cert and a DNS intercept could redirect to a fraudulent source of entropy.



Anyway, so I just - I haven't looked any closer at this but I need to because libsodium is, or was, a secure library.  But the idea of it using data from Random.org, which is a nice source of entropy, but it's over the Internet.  And so you can't trust it.  Now, I mean, maybe if it had certificate pinning so that it was tied to a verifiable cert, you know, that would help.  That would prevent a man-in-the-middle cert decryption.  But, boy.



Essentially what libsodium is doing is it must be assuming that it's on a platform where it just cannot generate any high-quality entropy itself.  We know that crypto needs high-quality entropy.  So maybe as a last resort it does this.  But, boy, I hope that anyone using it understands that's not just an immediate win.  It's got to be something that you do as a last resort.



Mark Gottselig asked:  "Is there an IoT security grade or review website you trust that we might be able to use to find secure IoT devices?"  Today I know of no such site, actually because I know of no secure IoT devices.  Something like what we were talking about earlier, like this Z-Wave Alliance initiative.  The problem is we're just too soon for that.  Right now we're still in the Wild West stage, and people are selling these things like crazy.



The good news is there's been a lot of attention focused on the lack of IoT security.  And those people who created the first wave are now saying, okay, we're going to have a problem selling this stuff unless we get rid of this reputation that IoT has as being so insecure.  That'll happen.  Then things will have to settle down.  And then maybe we can start taking a look at rating the security of the different technologies that we have.



Isaac says:  "If you're so worried about the government might do in the future to weaken crypto, then it's our duty to create it and resist."  And the problem is, as we talked about earlier, I completely agree until they make it unlawful.  I mean, I'm happy to create good crypto, and I'm happy to resist.  But I can't break the law.  And that's why I stopped working on CryptoLink years ago because I felt this coming, and I didn't want to invest years in creating a high-quality commercial product that might be outlawed by my own government.  So that's the problem.  And as you said earlier, Leo, if we outlaw crypto, then only outlaws will use it.



LEO:  Well, that's the thing.  Anybody who's motivated could figure out how to do it.



STEVE:  Oh, it's trivial to do.  We all know how to do strong encryption.



LEO:  You could put it on a T-shirt.



STEVE:  Yup.  Right.  Troy Carlson extended our "The 'S' in IoT stands for security," adding "and the 'P' stands for privacy."  Which I thought was good.



Dr. Suarez asks:  "Does the three-router solution protect privacy from ISPs if traffic is HTTPS and originates from the internal router?"  And, okay.  So ISPs, no ISPs can currently see into any HTTPS traffic.  So the three-router solution is sort of orthogonal to that.  It doesn't help or hinder.  What we want is our traffic to be over HTTPS.  In that situation, ISPs cannot see into our connections, but they can see to whom we are connecting.  And the only way to prevent that, as we'll talk about in a second, is using a VPN.



An interesting tip from a listener Jim Clark.  He said:  "My wife asked me to stop commercials from autoplaying in Firefox. We have Privacy Badger, uBlock Origin, and HTTPS Everywhere," he says, but that still didn't work.  He wrote:  "I started thinking of all the trouble about NoScript."  He says:  "I searched about:config and found media.autoplay.enabled was set to true.  I changed it to false.  She is happy now.  Love SpinRite."  So, and I tried that.  If you're a Firefox user, about:config, as we know, brings up a page with about a bazillion different little tweaks you can do.  And then so you use the search box, put in media.auto.  You only have to type a few characters, "au," and then up comes media.autoplay dot, and enabled will be set to true by default.  If you double-click it, it flips it to false.  And then nothing plays.



Now, the bad news is I was then unable to get a YouTube to play, even if I clicked on it.  Firefox said, uh, sorry.  So for me that wasn't practical.  Whatever I'm doing, I guess I'm just not running into things that are autoplaying.  Or maybe for me uBlock Origin is blocking them enough.  I'm not seeing that happen.  But for those who want to up the bar and are Firefox users, this really does shut down media autoplay.  Unfortunately, it seemed to me that it also shuts down media manual play, which may be too much for you.



LEO:  Isn't that weird.  I was just looking to see if you could do that in Chrome, and there is, if you do a chrome://flags, it's kind of similar to about:config in Firefox.



STEVE:  Right.



LEO:  These are experiments.  There's one called "Gesture requirement for media playback."  I don't know what the gesture is.  Then there's media playback and cross-origin iframes requires user gesture.  I'm just going to turn it on and see what happens.



STEVE:  Yeah, good.



LEO:  I don't know what the gesture is.



STEVE:  Let us know next week because I know that it's a constant problem for you.



LEO:  It is.  Well, I've actually been using an extension that disables it.  But like you, I find that it's very inconvenient because, when I want to play video, I have to turn the extension off and refresh.



STEVE:  Right.



LEO:  It's kind of a pain in the butt.



STEVE:  We talked last week about how there are a new class of bots which are pounding on gift card website portals and draining people's gift cards.  And we heard from a listener, @Liquidretro.  Jon said:  "I just checked some of my business's rebate gift cards.  One had been drained due to fraud.  Thanks for the heads-up."  So it's not just theoretical.  It's actually happening.



Oh, and I had an interesting counterpoint to the idea that ISPs might be forcing certificates on us.  A listener, Alistair Campbell, said:  "I'm skeptical that ISPs will be able to force install SSL certs, thanks to the proliferation of IoT devices with no interface for that."  And I thought, ah, that's a very good point.  We'll have to look at and see what IoT devices do.  Are they not using HTTPS?  Or do they have a minimal root store?  I mean, they probably don't have in their little tiny chips the 400-some-odd certs that we have.  So I wonder what their certificate chains look like, you know, how are they authenticating HTTPS connections, if they are.  That'll be interesting to find out.



But Alastair raises a great point.  If we have a large proliferation of non-PCs making HTTPS connections, then an ISP would not be able to force our light bulbs to accept their certificate.  So that creates, I mean, that might be a deal breaker, then, for that entire concept, which, you know, yay.  That would be great because the last thing I want is for my cable modem supplier to be intercepting my communications.  That would move me probably to a VPN, I think.  That just seems too creepy.



Oh, and one final thing.  And somehow this didn't occur to me when I was talking about the difference between SMS second-factor authentication and the time-based one-time passwords, the TOTPs.  I was grumbling that SMS is less secure, as indeed it is because every single time it's used you're relying on a cell phone text message to send you a six-digit thing, a one-time password rather than using an algorithm where you only need to exchange that sensitive data once over a certified TLS connection with a browser, which is probably a lot more secure, well, which we know is a lot more secure.  And then after that just time and crypto is able to autonomously generate the proper one-time password.  Anyway, this listener mentions the reason the companies want SMS security is so that they have your phone number.  And it's like, oh.



LEO:  Doh.  Of course.



STEVE:  Yes.  That hadn't, of course, that hadn't occurred to me.  And I made a note here, I've been meaning to mention this for a long time.  I can only reply to people through DMs, only because I just don't want to, you know, I've got 57,000 followers, and it just doesn't make sense for me to be tweeting replies to single people on my Twitter feed.  So I often will reply using a DM, only to be told that, oh, sorry, that person that you're replying to is not following you.



So I just, you know, I just did want to mention that, if you ask me questions or send me things, I often - I would love to respond, if I can.  But you have to follow me, not because I'm trying to make you follow me, but because I can't DM you unless you do.  And I do have my Twitter set up so everyone can DM me, whether I follow you or not, because I famously don't follow anybody.  And it's funny, too.  Because I was looking at Snowden's Twitter feed.  Someone sent me that tweet, and I wanted to verify that it actually was from him.  And I remembered then, not only does he have three million followers, but he follows only one account.



LEO:  What is that account?



STEVE:  The NSA.



LEO:  The NSA.



STEVE:  The NSA.  It's like, okay, Edward.  Point taken.  I wanted to give our listeners a quick update on SQRL.  As I had mentioned last week, the server side is all finished.  I'm now working on essentially bundling it up for installation, removal, and update.  I'm adding the install, remove, and update, not that it, well, it needs update.  But the install/remove is just because it's what people are accustomed to.  They're going to download SpinRite into their Downloads directory.  I mean, SpinRite.  Sorry.  SQRL.  And then they're going to double-click on it.  And so it needs to be able to "install itself," which it actually doesn't need to do because it's just an EXE.  There's no, you know, it's just one thing.  It's my kind of GRC code, 283K, and it's got all the multilingual stuff built in.  It's the full implementation of the SQRL protocol with all of the bells and whistles and features.  You just run it.



But most people are just going to click on the link and either download it or run it from GRC, and you could do that, too.  So it needs to be able to move itself into the normal place in the program files and to add an entry into the add/remove programs list and, you know, behave itself.  So I'm just going to - I'm adding that functionality, and also the ability to check for updates and update itself, either with permission or automatically and so forth.



Once that's done, then we're really close.  I will look through to verify all of the various functions and make sure that the user interface jargon is consistent.  And then I will, with great pleasure, be announcing it on this podcast for everyone to download and play with, in English only, at first.  At that point, while people are playing with it, I will synchronize the online documentation with what we finally ended up with, that is, bring the SQRL web pages current because they've been lagging behind.  I've just been working on the code.



And at that point I return to work on SpinRite v6.1 while we see whether SpinRite - I keep saying SpinRite - whether SQRL is able to gain traction.  And so I'm still - I'm not going to invest any time in the non-English version.  Certainly the website will all be in English only.  But as our listeners know, all of the hooks are in place to make it multilingual.  But I'm going to get right back to SpinRite, to work on SpinRite 6.1, because I don't want to invest in multilingual until we know that it's going to work, that it's going to take off, that it's going to succeed.



If it begins to gain traction, after 6.1 is out, then I will be able to relatively easily make it multilingual.  And we will use our listeners to help us translate the strings.  All of the strings that exist in the product are in a single separate file.  So I can simply publish that, and they're numbered.  And all of the references in the code are by number.  So if we simply translate the strings into their equivalent in other languages, it instantly turns this into a multilingual product.



And as for SpinRite, I'm going to come up with some way, and I haven't quite worked out the details, but I'm going to come up with some way to first make 6.1 available to everybody who has been supporting SpinRite and me through the years, before people can buy new copies of it.  So it's my way of saying thanks to everyone who has been continuing to purchase SpinRite and supporting this effort to get to 6.1.  I think everybody who has done that should get it first.  So I'm going to come up with a way to do that.  Okay.



LEO:  Nice.



STEVE:  Yeah.  That feels right to me, as a way of saying thanks for everybody's patience and support.



LEO:  That's really great.



STEVE:  So Proactive Privacy.  You know, Leo, we're out of time.



LEO:  Oh, come on, no, you can't do this to us.  Come on, we can be late. 



STEVE:  Really?  But we're at two hours of podcast, and we're at 4:00 p.m.



LEO:  Yeah, I know, but how long do you think this will take?



STEVE:  Well, I've got a lot I want to say.



LEO:  Twenty minutes?  People are going to be frustrated if you don't...



STEVE:  Yeah, I know.  But I just...



LEO:  All right.  It's up to you.  I mean, if you need a half an hour, that probably is too much.  And I don't want you to give it short shrift, obviously.



STEVE:  I really don't.



LEO:  We want to really do it.



STEVE:  I really don't want to.  So, shoot.



LEO:  All right.



STEVE:  Let's have less news next week because I really...



LEO:  Stop asking questions.  Stop hacking things.  No more news.



STEVE:  So I just - I have to say I'm sorry.  But we just spent two hours.  And I will make time.  Next week, no matter what happens, less superfluous stuff because I really - I care about this, and I know our listeners do.



LEO:  Yes, yes.



STEVE:  And, I mean, you know, the power of the major web players, the power of the endpoints, the power of incognito browsing modes, the need for a separate IP address is critical because otherwise you can get...



LEO:  There's a lot to talk about, yeah.



STEVE:  Yeah, there is.  There's too much.  And I just - I don't want to be in a hurry, and I don't want to be forced to...



LEO:  Good.  And I want to interact with you on this.



STEVE:  Good.



LEO:  So, yeah, you're right.  We shouldn't have to rush through this.  So we'll do it next week.



STEVE:  Okay.  I will make a lot of time for it next week.



LEO:  That's fair enough.



STEVE:  Okay, buddy, everybody.



LEO:  Although there's plenty of people wish this was a four-hour podcast.  I just want to say.



STEVE:  Just saying.



LEO:  It's always an option.



STEVE:  Just saying.



LEO:  We'll give you - I'll give you a whole day.



STEVE:  Hang an IV over me.



LEO:  I do have a quick plug because we did get nominated, I'm very pleased - you often get nominated for the podcast award.  The Webbys have added a podcast category, and we are nominated in this podcast category for the first time, which I'm very pleased about.  If you go to TWiT.to/webby2017, you'll see the People's Voice Awards.  And we're nominated in the Technology category for Triangulation.  But I consider it a nomination for the whole network.  So go to Podcast Digital Audio, look in Technology, and then you can vote for Triangulation.  Forget that other podcast from Vice Media or Marketplace.  Vote for me.  Vote for me.  We'd love, I would love to be able to go up onstage and accept a Webby Award during the first time they've given podcast awards.  So it's pretty exciting.  So just a little plug, TWiT.to/webby2017.



Steve Gibson, I'll give you a plug:  GRC.com.  That's the place to go to find Steve and SpinRite, the world's best hard drive maintenance and recovery utility.  If you've got hard drives, you really need SpinRite.  You can also find freebies there, lots of them.  Steve is very generous with his time and his research, and he puts it all on the website, GRC.com.  This show, too, audio plus transcripts, really nice transcripts at GRC.com.  You can ask questions there:  GRC.com/feedback.  You can tweet Steve, @SGgrc.



You can also get this podcast on our site, TWiT.tv/sn.  We do it live every Tuesday, 1:30 Pacific.  We started a little late today.  I apologize.  It's partly my fault.  That's 1:30 Pacific.  That'd be 4:30 Eastern time, 20:30 UTC if you want to stop by and say hi during the live taping.  But of course the main reason we make on-demand versions is so that you can listen at your convenience, wherever you are.  And that's at TWiT.tv/sn and wherever you subscribe to shows.  Great show, Steve.  Next week, privacy and how to protect it in the new age.



STEVE:  For sure.



LEO:  Yeah.  We'll see you next time on Security Now!.



STEVE:  Thanks, buddy.  Bye.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.










GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#607

DATE:		April 11, 2017

TITLE:		Proactive Privacy, Really!

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-607.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  This week Steve and Leo discuss Symantec finding 40 past attacks explained by the Vault 7 document leaks, an incremental improvement coming to CA certificate issuance, and  Microsoft's patching of a zero-day Office vulnerability that was being exploited in the wild.  They ask, "What's a Brickerbot?"  They cover why you need a secure DNS registrar, This Week in IoT Tantrums, a headshaker from our "You really can't make this stuff up" department, the present danger of fake VPN services, and an older edition of Windows reaching end of patch life.  They continue with some "closing the loop" feedback from their listeners and a bit of miscellany, then close with a comprehensive survey of privacy-encroaching technologies and what can be done to limit their grasp.



SHOW TEASE:  It's time for Security Now!.  My goodness, we've got a lot to talk about, including Steve's new 15-volume sci-fi opus; lots of security news, including an IoT company that's completely out of control.  And then at the end, I promise, and I know this because I'm speaking to you from the future, Steve will cover protecting your privacy as you surf online.  A really great how-to, coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 607, recorded Tuesday, April 11th, 2017:  Proactive Privacy, Really!



It's time for, you got it, Security - well, because you downloaded it - Security Now!, the show where we talk about security, privacy, protecting yourself online.  And we're going to do that this week, I promise you, with Security Now!'s host, Steve Gibson, the host with the most security.



STEVE GIBSON:  Well, we tried to get to our main topic last week, proactive privacy.  And as everyone knows, we spent two hours getting right up to the line.  But I didn't want to shortchange the topic because I know it will be of interest to our listeners.  In fact, there's already been some industry events that have followed on from what Congress has done.  So anyway, today's title is "Proactive Privacy, Really."



LEO:  Honest. 



STEVE:  We're actually going to get to it today.



LEO:  We promise.



STEVE:  And while I was putting the show together, it was funny, too, because people were still sending me topics and ideas and "Oh, Steve, did you see this?"  And they were saying, "Now, I know you're trying not to have too much to cover this week so you could get to the topic, but..."



LEO:  Oh, that's cute.  They know.



STEVE:  Yes.  But we do have a bunch of fun stuff to talk about.  Symantec has found 40 past attacks which are explained by the Vault 7 document leaks.  We're facing an incremental improvement in forthcoming CA, that is, Certificate Authority certificate issuance integrity, which is hopeful.  Today is Patch Tuesday of April, and Microsoft patched in today's patch a very worrisome zero-day vulnerability in Office that was being exploited in the wild.  We'll talk about that.  There's a new bot in town that has been named Brickerbot.  We're going to address the question of why you really need to secure your DNS registrar and how a Brazilian bank found out what happens if you don't.



We have This Week in IoT Tantrums, and a headshaker from our "You Really Can't Make This Stuff Up" department.  The present danger of fake VPN services.  An older edition of Windows today hit its end of patch life.  We've got some closing the loop feedback from our listeners; a little bit of miscellany; and then, as promised last week and delivered this week, a comprehensive survey of privacy-encroaching technologies and what we can do to limit their grasp.  So I think another great podcast.  



LEO:  And of course today is the day that Microsoft starts putting out the Creators Update for a lot of people.  So it's not only a Patch Tuesday, but some people will start getting the Creators Update, and over the next few days you'll be getting a Windows 10 Creators Update.  I know you won't be, but...



STEVE:  I don't even know what that is.



LEO:  Well, it's just, you know, I warn you because probably next week and the week after and the week after we might have some other things to talk about having to do with Windows 10.



STEVE:  Indeed.  And if there is, we certainly will.



LEO:  Yes.  All right, Steve.



STEVE:  So we have a really great Picture of the Week.  I just got the biggest kick out of this.  This shows two tweets.  Tavis Ormandy first tweets:  "I'm in Miami for @InfiltrateCon.  Let me know if you want to catch up."  And then the response from someone whose handle is "the grugq" says:  "Where specifically will you be alone and unguarded?  Asking for @LastPass."



LEO:  That's funny.  That's funny.  Although I doubt that that's actually how LastPass feels.  I'm sure they're grateful to Tavis.



STEVE:  Oh, of course not.  Oh, absolutely, absolutely.  



LEO:  That's funny.



STEVE:  So Reuters picked up the news that Symantec had said to them, essentially in a press release, although they weren't naming the CIA as a function of their corporate policy not to do so, they had, after Symantec looked over the Vault 7 document leaks, pieces clicked into place, and they were able to go back and look at data that they had captured of previous attacks where they hadn't known exactly what was going on.  And in 40 different instances, that is, cyberattacks against at least 40 organizations around the world, they found that the tools referenced in the leaked WikiLeaks documents that are ascribed, believed to be legitimately from a CIA document trove, matched perfectly the attacks that they had no attribution for until now.



So it shouldn't come as a surprise.  And these 40 corporations that were attacked by these were spread out around 16 countries.  So just sort of an interesting data point, that this is what you would expect.  And props to Symantec for being in the business, having their feelers out, collecting this sort of data, and then saying, you know, we ought to take a look at the stuff we've collected in the past and see if any of this now makes more sense.  And they found out, yes, apparently, indeed it does.



LEO:  Now, were they widespread like hack attacks?  Or were they targeted?



STEVE:  These were targeted against specific organizations.



LEO:  Okay, good, good.



STEVE:  So Symantec probably has contractual security relationships with companies all over the place?  And so they've got their monitors and probes in those companies' networks.



LEO:  Got it, got it.



STEVE:  And so they're collecting data and archiving it in order to understand what's going on.



LEO:  You have to wonder if they went to those companies and said, hey, by the way, you know that attack?  That was the CIA.



STEVE:  So we've talked endlessly about certificate authorities, about how we've got kind of this creaky system that's the best that we know how to put together at this point, given the technologies and tools that we have, that essentially allow two parties that have never met before, meaning a server and a client, to arrange at least a one-way trust relationship - that is, for the client to be able to know that it is actually connecting to the server it believes it is thanks to a third party, the third party being the certificate authority, where the server has proved its identity to the certificate authority and the certificate authority has given the server a certificate, essentially an identity assertion certificate, which it gives to the browser.  The browser then is able to verify the authenticity of the certificate by checking its signature, which can only be created, thanks to the magic of crypto, by that certificate authority, which thus proves through this chain that at one point the certificate authority was convinced that the server was who they said they were.



So that's the system.  Unfortunately, there are all kinds of ways this can break.  And we've talked about many of them.  Our old-time, long-time listeners will remember the podcast where I had a meltdown when I looked at the size of the certificate authority root store in Windows because I remember when it was 11 trusted CAs, and it was like 400.  It was like, what has happened?  Because the nature of the system I just described means that anyone can sign a certificate for any server.  And if you trust the signer, then you trust the server.  And so that does create a problem with abuse.



And, well, for example, we have seen situations where a fourth certificate authority - that is, not a first, second, or third party, but a fourth party, someone completely unrelated - issued a certificate for, for example, Google.com.  And we trust it because we trust all the certificates that party issues.  So the problem that any of the hundreds of certificate authorities we trust can sign a certificate for any domain, that's an aspect of frailty in our system.  Just last week an RFC that's been in the process for four years was formalized, and the CAB, the Certificate Authority Browser forum, or consortium, has formally required that, within six months, by September of 2017, all certificate authorities must honor a new record type for DNS.



We've talked about DNS.  Our listeners know how excited I am about the idea that someday DNSSEC will happen, sort of in the way that someday IPv6 will happen.  Eventually people will just give up or run out, and they'll have no choice.  And so someday we'll have DNSSEC, which provides end-to-end security for DNS.  We don't have that now.  So that represents a weak link in our existing system.  But the reason I'm so excited about DNS is that it is, once it's secured, it's this otherwise very well-designed hierarchical caching directory.  And you can put all kinds of stuff in it, not just IP addresses.



But, for example, we're already storing text records to use for helping to diminish spoofing, where for example the DNS says for an email server that valid email from, for example, GRC.com can only come from this domain or this IP.  And so somebody receiving email that wants to make sure it's not spoofed can make a DNS text record query for the SPF record and get what I am publishing as the only valid source for email from GRC.com.  So that's an example of how we've already extended DNS beyond IP addresses to other stuff.



Well, what is coming is known - and so the typical IP is an "A" record, an address record.  Or if it's IPv6, it's an "AAAA" record.  We also have NS records, name server records; and TXT, text records, and so forth.  Well, we're going to get a CAA record which stands for Certificate Authority Authorization.  And what this is, it's very much like the antispoofing for email, where the domain owner, like GRC.com, the domain owner is able to assert in the CAA record who is able to issue certificates for that domain.  So of course my CAA record will say Digicert.com.



And so what this does is it publishes the name of my authorized certificate authority, who is the signer for my certificates.  There's no enforcement here, but what this does is the CAB Forum is saying that within six months all certificate authorities must query a domain that they are being requested to issue a certificate for, for the CAA record.  And if that specifies a certificate authority other than them, they must decline the certificate.  So essentially it's a way of authorizing who you want to be able to generate certificates for your domain.



To the degree that that authorization is honored by other CAs, it will prevent a class of problems that we've had.  It's not strong protection, and no way is it cryptographically amazing.  It's like a hint or a clue.  It's, you know, this is who my CA is.  And you can do a comma-separated list.  You can also have a null list.  You can do double quote, semicolon, double quote [";"] which means nobody is authorized to issue certificates for the moment.  In which case, after September, or actually at any time that you're publishing a CAA record moving forward, if you yourself want to ask your CA to give you a new certificate, you'll have to put them in that record so that they can see they're authorized.  Once you've got your new certificate, you could change it back to a null list, which locks down any subsequent certificate issuance - but, again, for those authorities that follow it.



So this won't do anything to prevent deliberate malicious issuance of certificates.  But again, it's something that is easy to be retrofitted in, which we normally have a problem with.  It's very hard to - like, for example, DNSSEC.  It's having a hard time because we already have DNS.  IPv6 is having a hard time because we already have IPv4.  Here we can just easily layer this on.  Everybody's got six months.  Now, newer editions of BIND, the old-school DNS server, like from 9.8 on, I think, support it.  There's an RFC that explains it.  There's a bunch of existing services that either have it already supported, or it's coming online.



And it is possible, for example, if you have a retrograde version of BIND - I'm still running 9.2, I think, for my DNS - you can explicitly specify the record number.  If your DNS server doesn't know what a CAA record is, it's Type 257.  So there is a way to sort of override its lack of knowledge, sort of put in, like manually create a record entry, if your server doesn't support it.  But I will be updating myself quickly, or soon, because it's just - it's time to.  In fact, I think my other Unix server is running a newer, non-BIND DNS.  I don't remember now what it was that I chose.



But anyway, so just nice, backward-compatible, it's not going to end all the problems.  But for well-meaning certificate authorities that don't know they should not issue a certificate for a domain, starting in September of this year, about six months from now, they will be forced to verify that they're not excluded from issuing a certificate based on the policy being published by that domain.  So I think it's a nice step forward.  Just, you know, more good things.  And, boy, it took a long time to get it through the RFC process and through ratification.  But that has now finally happened.



FireEye first detected a new way of executing malicious code through a Microsoft Office OLE2Link object.  OLE2 is Object Linking and Embedding.  It's technology that has been around for about a decade, where Microsoft was sort of doing this whole document unification technology back in the earlier days of Windows.  They kept it quiet.  They notified Microsoft.  And Microsoft had plenty of time to fix it, which is the only reason we got a fix in today's Patch Tuesday for this.  However, prior to this morning, last week the news got out from other sources who detected this.  FireEye kept it quiet.  Microsoft certainly kept it quiet.



But the news got out.  It was a classic phishing email attack where a Word document would be sent to somebody containing an embedded OLE2Link object.  When the user opened the document, Office Word, still called winword.exe, would execute and issue an HTTP request to a remote server to retrieve a malicious .hta file.  HTA is a Microsoft abbreviation for HTML Application.  That appears as a fake RTF, Rich Text Format file.  And so by taking advantage of - once again, here's an interpreter which is being abused.  The RTF file format interpreter had a flaw that allowed a maliciously formatted RTF document to get itself execution.  So the ATA application loads and executes a malicious script.



There were several different documents that were observed.  It would terminate the Windows Word process, download one or more additional payloads, then download a decoy document for the user to see, while installing malware in the background.  And a fully patched right up to yesterday system would bypass all existing mitigations and manage to execute bad stuff on the user's machine.



Now, there is a registry tweak which can be employed to shut this down in the short term.  And we got news of that last week as soon as FireEye went public with this.  But now we have a patch, that's really what everybody should do at this point.  And of course it generated lots of coverage because it was an easy exploit.  Word is widely deployed, and there was no existing mitigations that were able to catch this.  So anyway, it's been mitigated now, and the vulnerability is gone from Windows.



And I think I heard you talking about this over the weekend, Leo, this Brickerbot?



LEO:  Yeah.  It's a new acronym, PDoS.  I like it.



STEVE:  Yes, the PDoS.  So Radware was the company that found this.  And I edited down some of their coverage.  They said:  "Imagine a fast-moving bot attack designed to render the victim's hardware nonfunctional."  Okay, so this isn't installing a bot in your camera or your DVR.  This is deliberately, permanently breaking it, or bricking it, thus the name Brickerbot.  This is a bot that bricks your exposed Internet of Things devices.  And PDoS, as you noted, Leo, is the new term for Permanent Denial of Service.  It's becoming increasingly popular in 2017 as more incidents involving this sort of hardware-damaging assault occur.  It's also known as "phlashing," P-H-L-A-S-H-I-N-G, phlashing.  Much like phishing, this is phlashing in some circles.



The PDoS is an attack that damages a system so badly that it requires replacement or, if possible, reinstallation of the hardware.  It exploits security flaws or misconfigurations, like we've been talking about, and can permanently destroy the firmware or basic functions of a system.



In Radware's case, they set up a honeypot which, over a four-day period, recorded 1,895 PDoS attack attempts performed from several locations around the world.  The sole purpose was to compromise the IoT device for the purpose of corrupting its storage.  They've seen two different ones.  There was one they called Brickerbot.1.  It first appeared, but was rather short-lived.  And then Brickerbot.2 appeared, which was clearly from the same authors, basically doing the same thing, but it was hiding its servers behind TOR.  So all of the IPs that Brickerbot.2 appeared to be coming from was quickly identified as Tor egress nodes, exit nodes.  So it was not possible to backtrack any further because Tor was doing what Tor does, which is hiding the identity of the person making the queries.



This is a brute-force telnet attack, which of course is the same exploit vector we've talked about that was used by Mirai recently to attack CCTVs and expose DVRs and things that had a public telnet port on the Internet.  It does not try to load a binary.  It does have a list of brute-force attacks it goes after.  The consistent thing it first tries is the admin username of "root" and the password of "vizxv,"  which is a well-known password for the Dahua brand DVR, DH-DVR3104H.  And if you google "vizxv," you'll find a bunch of references to it.



So it is a well-known telnet administrator username and password that gets you in, or at least it did.  I think, depending upon how prolific Brickerbot has been, those machines may no longer be responding to telnet or anything else.  Which of course brings them to the attention of their owners, who are scratching their heads, thinking, wow, I wonder why my...



LEO:  Send it back.



STEVE:  Yeah, my CCTV just doesn't work anymore.



LEO:  Send it back.  You know, this is a longstanding tradition of creating malware for good.  It's still illegal.



STEVE:  Yup.



LEO:  It's still malware.  But I can't - I don't completely blame the motivations.



STEVE:  No, you're right.  You could argue that, if they don't do this, then bad guys will...



LEO:  Mirai will come along; right.



STEVE:  Exactly.



LEO:  They're trying to get there before Mirai does.



STEVE:  Exactly.  Yeah.  So one disturbing thing is that, in that first attack where they were able to find the IP addresses, they were spread around the world.  The devices doing the attacking were exposing port 22, which is the SSH port.  And they found that they were running an older version of the Dropbear SSH server.  As a consequence of that, using Shodan, they were able to identify these as Ubiquiti network devices.  Among them were Access Points and Bridges with beam directivity.



And we talked about this particular subset of the Ubiquiti AirOS a couple weeks ago.  This does not affect our cute little Ubiquiti EdgeRouter X devices, or the EdgeRouter Lites.  But it does affect, apparently it's still some vulnerabilities in this AirOS that, you know, Ubiquiti said, well, we've got patches out for all this.  They did, remember, they dragged their heels.  But the problem is the fact that they're just making patches available is different from the Ubiquiti owners knowing the patches are available or maybe even caring.  Once all of these IoT things get deployed, they just pretty much get forgotten about until they stop working.  Or in this case they've been commandeered and turned into Brickerbot scanners which are looking then for other IoT devices that they can take over.



So anyway, yet another new - oh, I forgot to mention that the last thing that's done after these things crack in is, when they're all through figuring out where they are and running their commands, they proactively attempt to shut the device down.  They remove the default Internet gateway.  They then issue an "rm -rf."  "Rm" stands for remove in Unix parlance; "-r" means recursive.  And then a /*, which means everything you can from the root outward, basically wipe the file system.  It also then tweaks kernel settings to limit the maximum number of kernel threads to one in order to essentially clamp down on what the device can do.  It wipes the IP tables, firewall, and NAT rules, flushing them, and then adds a rule to drop all outgoing packets.



So, I mean, it really does everything it can to destroy the device and take it offline.  So maybe it'll come to the user's attention, and they'll fix it.  Who knows?  Maybe update their firmware or have somebody who knows something about Internet security to shut this thing down so it doesn't have, I mean, we're talking about an exposed telnet server.  I mean, it's hard to say the devices that do that don't deserve a little of their own medicine.  I mean, it's either, if they're not going to be fixed, then they're going to be commandeered into a botnet.  And that's not good for everybody else.



LEO:  Right.  Yup.  It's the Internet, self-healing.



STEVE:  That's right.  Never a pretty process, but someone's got to do it.



LEO:  Right, right.  Just don't get caught; okay? 



STEVE:  So on Saturday at 1:00 p.m. last October the 22nd, hackers changed the DNS registration of all 36 of a very large, well-known Brazilian bank's online properties, commandeering their desktop and mobile web domains, taking all visitors to the attackers' perfectly constructed fake spoofed site, where the bank's victim customers proceeded to dutifully hand over all of their account information, believing that they were at the bank.  This is the largest, longest DNS registration attack that we've seen.



Kaspersky was the group that found it and reported it.  They also believe that the hackers also were able to redirect all transactions at ATMs, that is, even automated connections, not user browser connections, at ATMs and point-of-sale systems to their own servers, thus collecting the credit card details of anyone who used their card for about five to six hours, which is how long it took for the bank to get control back of their DNS that afternoon.  A Kaspersky spokesman said:  "Absolutely all of the bank's online operations were under the attackers' control for five to six hours."  They watched malware infecting customers.  So it's not enough that they spoofed the banking site and got their, you know, "Hi, please log on," but they also gave them some malware for their trouble.



Kaspersky has not released the name of the bank that was targeted in the DNS redirection attack.  But the firm says it's a major Brazilian financial company with hundreds of branches, operations in the U.S. and the Cayman Islands, 5 million customers, and more than $27 billion in assets.  So not a small operation.  Although Kaspersky says it doesn't know the full extent of the damage caused by the takeover, it should serve as a warning to banks everywhere - well, and, frankly, for everyone everywhere who has an important website - to consider how the insecurity of their DNS might enable a nightmarish loss of control over their core digital assets.



Now, we know that the servers where the traffic was redirected to was Google's Cloud Platform.  Google was in no way complicit.  They were just offering Cloud web services like anyone does these days.  And so what happened was that somebody broke into, either hacked their password for their DNS registrar or broke into the registrar.  It's probably the former.  It's probably that they were subject to a brute-force attack, and the attackers were able to log into this Brazilian bank's registrar.  When you do that, part of the registration records for a domain, you know, the registrar is sort of the ultimate root of everything.  So the registry record says, what are the two name servers for this domain?  That is, essentially, for example, if it's a dot com, then this is the DNS servers that the dot com domain points to for the IP addresses for that domain.



So, for example, in my case, I'm using Hover now as my domain registrar.  And our listeners will remember that, when I switched - and this was the first instance where I talked about why it was so clear to me that using a time-based one-time password was better than a text record.  I immediately set up strong two-factor authentication so that I know what my key is; they know what my key is.  And the only way to log in, even if you did manage to brute-force my username and password, and I don't know what my password is because you know where it's stored, and you know how long it is, it's complete pseudorandom gibberish.  So good luck brute-forcing it.  And even if you did, then you'd still be asked what's your current six-digit, time-based, one-time password.



Clearly, this Brazilian bank didn't have that level of concern.  So somebody managed to brute-force their login, almost certainly.  And what's interesting is that they also had HTTPS certificates.  That is, people logged into the actual domain name and got the green "go" sign, got the padlock saying this is a secure connection because it was.  Because this wasn't a spoofed domain.  This was the real domain.  In fact, the customer shortcuts would have worked.  They didn't even have to type it in.  They just clicked the shortcut to their bank that they've been using for years, and it took them to their bank, except it wasn't.  And unfortunately our listeners can probably guess where the HTTPS, that is, the TLS certificates came from.  They were minted by Let's Encrypt six months before.



LEO:  Oh.



STEVE:  So what probably happened...



LEO:  But don't Let's Encrypt certificates expire in three months?



STEVE:  I don't know.



LEO:  I thought they were three-month certificates.



STEVE:  In this case they were issued six months earlier.  That much I do know.  It may be something that you have control over.  But in this case what happened was they probably briefly switched the domains they wanted HTTPS certificates for, immediately used Let's Encrypt to authorize the domain, and then switched it back.  So there was probably - so the point is that any registrar would have made the same mistake, except that Let's Encrypt being automated allowed them to set up a whole staging and get ready.  So they switched over, got the certificate, probably switched back so that nobody would realize what had happened, and then continued to get ready.



You know, the fact that they also did ATMs and point-of-sale devices, I mean, this was a big, well-orchestrated setup.  And of course they didn't know how long they were going to have.  As it is, they got five to six hours before the bank said, okay, we've got to get control of our DNS back.  So I don't blame Let's Encrypt except that, in this instance, the fact that it was automated would have allowed a brief change in DNS to slip through the automated check.  And then they could have put it back, and nobody would have been the wiser.  And notice that, in this instance, the CAA would not have prevented it.  They would have switched DNS to their DNS server, which could have authorized Let's Encrypt to issue the certificate.



So even if you had a CAA record, in this kind of attack, well, I mean, the problem is, if you, again, as Kaspersky says, and as we know, if you don't have your DNS pointing at you, you're really in a world of hurt.  So everyone who really cares about their DNS should think about how secure their logon to their registrar is.  It really, in many ways, it is the ultimately critical attack point for everything else downstream because all of your authentication just goes out the window if you don't have strong DNS.  Which, again, is why DANE, the DNS-based security, looks good.  But we still need DNSSEC in order to protect it.



LEO:  Steve Gibson, Leo Laporte.  We're talking security.  And as Paul Harvey would say, "Page Three."



STEVE:  So that break gave me an opportunity to dig into the question you raised, which is exactly right.  Because we talked about Let's Encrypt and the fact that the certificates would have a short duration.



LEO:  That's right.



STEVE:  But since they were automated...



LEO:  Big deal; right.



STEVE:  ...that didn't matter.



LEO:  Right.



STEVE:  Exactly.  And of course it caused a lot of controversy because people are used to multiyear certificates.  So back around the issuance date, I mean, as this was all beginning to come online, the executive director of ISRG that is the parent of Let's Encrypt said, "We're sometimes asked why we only offer certificates with 90-day lifetimes.  People who ask this are usually concerned that 90 days is too short and wish we would offer certificates lasting a year or more, like some other CAs do."



Now, this next paragraph I'm wondering about.  He says:  "Ninety days is nothing new on the web.  According to Firefox telemetry, 29% of TLS transactions use 90-day certificates."



LEO:  Huh.



STEVE:  What?  Okay.  And he says:  "That's more than any other lifetime."



LEO:  What?  Oh, you know why?  Steve, Google.



STEVE:  Ahh.



LEO:  Google does short-term certs; right?



STEVE:  Okay.  Yup.  Yup.  Because they're issuing their own, so, yes.  Exactly.  "From our perspective," he writes, "there are two primary advantages to such short certificate lifetimes.  They limit damage from key compromise and misissuance."



LEO:  Since we have no revocation possibilities; right?



STEVE:  Yes, exactly, exactly.  "Stolen keys and misissued certificates are valid for a shorter period of time.  And they encourage automation, which is absolutely essential for ease of use.  If we're going to move the entire web to HTTPS, we can't continue to expect system administrators to manually handle renewals.  Once issuance and renewal are automated, shorter lifetimes won't be any less convenient than longer ones.  For these reasons," he writes, "we do not offer certificates with lifetimes longer than 90 days.  We realize that our service is young and that automation is new to many subscribers.  So we chose a lifetime that allows plenty of time for manual renewal, if necessary.  We recommend renewal every 60 days.  Once automated renewal tools are widely deployed and working well, we may consider even shorter lifetimes."



So who knows.  Maybe there was a typo in Kaspersky's note, or maybe they saw a certificate that was older on a server in the same domain.



LEO:  More scary, maybe these guys, the bad guys figured out a way to kind of forge these certs and extend the date; right?



STEVE:  Yeah.  Don't know how you could do that.



LEO:  That's be hard to do, though, huh.



STEVE:  Yeah.



LEO:  That's built into the key, I presume.



STEVE:  Yeah.  And it wouldn't be their clock or time of day.



LEO:  Oh, okay.



STEVE:  It would be the time at the signer's end.  So I don't  know how you would do that.  And again, if you could, people would because it's like, hey, why not get a longer cert?  I'm 100% behind shorter duration with automated renewal.  I think that's just...



LEO:  We've given up on the notion that you'll ever have a revocation system that works, so that's the next best thing.



STEVE:  Yes, yes.  This week's IoT tantrum.  And I know you talked about this over the weekend.  A maker of smart garage door openers... 



LEO:  No, we didn't talk about this.  I love this story.



STEVE:  Oh, responded to a bad Amazon review by remotely disabling the customer's purchased and paid for and operating device.  The customer had left a comment, first on the support forum for this company - and it's Garadget, G-A-R-A-D-G-E-T, Garadget, so it's just kind of a cute play on "garage" and "gadget" - had left a comment on the support forum complaining about technical issues.  And in order to be safe for work, I changed the noun so that I could say it on the air:  "Wondering what kind of piece of" - and I changed it to crap - "I just purchased here."  So this customer was...



LEO:  That's it.  That was it.  That was the bad review.



STEVE:  Yes.



LEO:  Yeah.



STEVE:  No, no, no, no.  He says it on the forum.  Then they followed up with a negative Amazon review saying, quote, "Junk."  All caps, "DO NOT WASTE YOUR MONEY.  iPhone app is a piece of junk, crashes constantly, startup company that obviously has not performed proper quality assurance tests on their products."  And, frankly, that was then supported by the company's reaction, who responded online and posted their response, saying:  "Martin.  The abusive language here and in your negative Amazon review, submitted minutes after experiencing a technical difficulty, only demonstrates your poor impulse control."



LEO:  Oh, lord.



STEVE:  "I'm," writes this support person for Garadget, "I'm happy to provide the technical support to the customers on my Saturday night" - ever heard of online?  Come on - "but I'm not going to tolerate any tantrums.  At this time your only option is return Garadget to Amazon for refund.  Your unit ID, 2f0036, will be denied server connection."  So this company...



LEO:  Wow.



STEVE:  Yeah.



LEO:  It's like the Soup Nazi.



STEVE:  Exactly.



LEO:  No garage door for you.



STEVE:  No, you lift the garage yourself.  Oh, wow.



LEO:  No soup for you.  Oh, my.  And by the way, I think the support person was the founder of the company.  I think it's a one-man - it was a small company.



STEVE:  Yes, yes.  And, you know, so this guy, live and learn.  Garadget then defended itself in a subsequent post, saying it took action to "distance from the toxic individual."  Which also suggests maybe that not everybody should be in business for themselves.  And so this person then posts:  "Okay, calm down everybody."  Because I should mention there was this tremendous backlash throughout, both on their forum and on Amazon, with everybody taking umbrage at the fact that the company would respond to a negative review by blacklisting and denying service, denying the use of their product to this person.



So this guy says:  "Okay, calm down everybody.  Save your pitchforks and torches for your elected representatives.  This only lacks the death threats now."  He continues, "The firing of the customer," which is what he's calling it.  We fired our customer.  "The firing of the customer was never about the Amazon review."  Uh-huh.  "Just wanted to distance from the toxic individual ASAP.  Admittedly not a slickest PR move on my part.  Access restored, note taken."  So anyway, yeah.



LEO:  Wow.



STEVE:  Not the way you make friends and influence people in a positive fashion.  You know.  Anyway.  Okay.  Now, from our "You really can't make this stuff up" department, we have the newest entry in the ransomware division, called Rensenware.  It is a new twist on ransomware.  And I'm not kidding about this.  This is not an April Fool's joke.  This actually exists.  Instead of requiring an infected user who's just had all of their machine's data encrypted to pay a sum of money, typically in bitcoin, as we know, to regain access to all of their locked files, Rensenware actually requires them to reach a high score of 200 million points on the anime bullet hell shooter known as "TH12:  Undefined Fantastic Object."  But it must be played on the "lunatic" difficulty level.  In other words, either invite your grandson over, or beg Paul Thurrott to come unlock your machine.



Now, this was apparently a joke gone wrong.  The creator of Rensenware has apologized for the software.  He says:  "I made it joke, just laughing with people who like Touhou Project Series," says Tvple Eraser, who then released a tool to bypass the lock on the files for anyone who may have downloaded the original version by mistake.  He has also replaced the Rensenware version with a safer cut version that doesn't lock your files by forcibly encrypting them.  So it apparently it was a joke gone wrong.  He wasn't, you know, obviously he wasn't making any money on it.  He just thought, ha ha ha, this'll be funny.



The problem is apparently it's like virtually impossible.  I don't even know what an anime bullet hell shooter is, but it sounds pretty bad, especially when you have to play it on a "lunatic" difficulty level.  So the good news is, if you did happen to get caught by this - hopefully it didn't exist long enough for that to happen with much probability - there is an unlocker for it.



Nicholas Deleon is a writer for Motherboard.  And he wrote a really fun three-article series.  He didn't know it was going to be three articles when he started, when he wrote the first one.  But he knows what's going on in the industry.  He's active on motherboard.vice.com site.  So he was suspicious when, over the course of a couple days, he received different phishing emails from "MySafeVPN" with what would be convincing to anyone else technical details.  But Nicholas smelled a rat, and a story, and he got one.



The phishing emails referred, in different cases, in one case to Plex and another case to Boxee.  And he was familiar with both because he's deep in the industry.  And he remembered that both of their online forums had been hacked years before, and they had lost control of all their usernames and email addresses, which would make them perfect sources for phishing bait.  So somebody apparently obtained those lost databases and was scamming people who are concerned about what is now being called "America's War on Privacy" as a consequence of the decisions, the new legislation which the Congress generated and approved, and then Donald Trump signed into law, which prevented the legislation that would have gone into effect that required ISPs to proactively ask for permission to monitor their users, prevented that from happening, as we've been discussing.  And thus the incentive for the topic of this week's "Proactive Privacy, Really!" podcast.



Anyway, I'm not going to go into any further detail.  I did tweet the links for anybody who's interested, and they're in the show notes.  It's a very well-written, fun story.  He had email exchanges.  He actually spoke to these people on the phone and describes the conversation in detail.  He used Google's street view in order to find, apparently, a car rental, something called Fox Car Rentals somewhere, which is apparently where this place is located.  The upshot is three really fun stories and, of course, a takeaway for our listeners because we'll be talking about VPN systems and services here toward the end of this podcast.



And it's not surprising that fake VPNs or VPNs are going to be surfacing in order to target consumers' concerns because the most obvious thing one would do, if your connectivity provider - if you're worried about them snooping on you, is to wrap your traffic in a VPN so that it's encrypted as it passes through and out of your local ISP's control.  So again, we'll be talking a lot about VPN services.  But I did want to just note, because it won't be talking about fraudulent or spoofed VPN services, but that's a thing.  And also you certainly want one with as much integrity as you can find.  And I will mention later - I know, Leo, you're a fan of Sonic, and in doing a little bit of digging into this that Sonic offers a VPN, a free VPN service to all of their subscribers as just part of the service. 



LEO:  I didn't know that.  I'll have to add that to the ad.  You know, though, I mean, okay.  So if you're trying to avoid snooping by your ISP, using your ISP's VPN might not be a good solution.  On the other hand, Sonic is one of the few ISPs that has pledged not to snoop on you, not to intercept traffic.  They fight federal law enforcement orders.  I mean, you know, they're [crosstalk].  They're one of the good ones in the EFF.  I'm only sad that everybody can't get it.  I mean, it's kind of geographically limited.



STEVE:  I can't.



LEO:  I know.  You have to be in Northern California, pretty much.  They were at one time thinking about expanding beyond this area.  And maybe they are now, too.



STEVE:  So I was about to say that there's been some news that maybe Google Fiber will be coming to Southern California.  But then, you know, Google.  It's like, okay.



LEO:  All the fibers [crosstalk].



STEVE:  You know, if ever there was a connectivity provider that is all about knowing who you are and leveraging that, when they bought, well, I mean, they're all about advertising.



And in the "just have to say it because why not," this Patch Tuesday, today, marks the first month that Windows Vista will not and will no longer receive any further updates.  2017, April 2017, is end of support life for Vista.  And I put in my notes, "Does anyone care?"



LEO:  Oh, lots of people care.  You still use XP, Steve.  Right?



STEVE:  Yes, but, I mean, is anyone still using Vista?  Vista was...



LEO:  Well, okay.  Yeah, that's - yeah.



STEVE:  You know, as we remember, it was troubled from the start.  Remember WinFS?  It was supposed to have this super fancy amazing file system.  Microsoft was going to change everything.  And it's like, uh, okay.  And Vista was really late.  I mean, I guess this was in early Ballmer days because of course Steve Ballmer famously launched XP and then turned his sites on, you know, let's do the next thing.  And that was going to be Windows Vista.  So the Win file system got aborted.  There were all sorts of fabulous features that we were promised that were all rolled back.  Remember that it also had a far, far too aggressive and intrusive UAC.  It was like that thing was popping up constantly and driving people crazy.



And so one of the nice things that they fixed in 7 was they toned the UAC down, still giving you basically a compromise so that it wasn't in your face nearly as much as it was in Vista.  Windows 7 did end up inheriting many of Vista's innovations.  But basically it was, okay, let's get away from this thing as quickly as we can.  So anyway, for what it's worth, 2017 was that.  Now, we will be discussing the same date in five years.  2020 is end of life - wait, no.  Yes.  No, three years.  2020, April 2020 is the similar date for Windows 7.



LEO:  Yeah.



STEVE:  So three years from now they'll be no longer issuing monthly patches for Win7.



Okay.  To close the loop with some of our listeners, just some fun things here.  I did get a tweet from someone whose handle is ReliefTwitcher.  He said:  "@SGgrc Have had media.autoplay off in Firefox about a year."  Remember we talked about that recently, that you could flip that off to kill autoplay.  But then I had a problem that, in at least one case, I didn't mess with it too much, that, for example, I couldn't get a YouTube video to play, even when I clicked on it and pounded my fist on the browser and said, come on, play.  It just wouldn't do it.  



So he said:  "Found the same as you with YouTube and other video sites.  Easy solution:  Click a tiny bit into the progress bar, then click play."  And he says:  "'autoplay off' is great for sites where you don't want to block all ads totally."



LEO:  Oh, clever.



STEVE:  So nice workaround, @ReliefTwitcher.  Thank you for that.  And then a number of our listeners got a big kick out of last week's rant, mine, about JavaScript.  Rasmus Beck said:  "Watching Security Now! Episode 606.  Getting the feeling @SGgrc doesn't quite like JavaScript."  And Kyle said:  "@SGgrc But tell us how you really feel about JavaScript."  You know, when I was describing it as a heinous abomination and so forth.



LEO:  Oh, that, yeah, yeah.



STEVE:  So I did want to just say to our listeners, don't get me wrong.  I think that JavaScript is an abomination, while at the same time understanding why it is so, and managing to write my own beautiful code in JavaScript.  Password Haystacks functions entirely thanks to JavaScript being able to perform, keystroke by keystroke on the fly, brute-force search depth analysis.  And by the way, nearly 4,000 uses of that page a day, 3,844 times that page is brought up every day.  And remember the Off The Grid Latin Square Solver.



LEO:  Right.



STEVE:  That I wrote in JavaScript.  And because there were so many more Latin Squares possible than just 2^256, any normal pseudorandom number generator doesn't have enough entropy.  So I created the Ultra High Entropy, that is, the UHEPRNG.  And there's a page for that, Ultra High Entropy Pseudo-Random Number Generator, specifically to create enough possible Latin Squares that I felt like, okay, it's worthy of the Latin Square safety.  Also remember I did that animation, GRC.com/animation.htm is that really cool demonstration of how data is stored on a magnetic platter and reread and reconstructed.  And then nobody knows about it, but there's even a breathing pacer.  GRC.com/breathe.htm runs a nice, simple little JavaScript app that just helps you breathe more slowly and deeply.



LEO:  This is something I never knew about.



STEVE:  Because that triggers a strong parasympathetic reaction and relaxes you.  I mean, that's the whole meditation and yoga and all that.  It turns out there's something known as stretch receptors.  And so breathing deeply and with your lower lungs, so called "belly breathing," pushing your lung out rather than your chest out, is very good for that.  So anyway...



LEO:  Someone needs to make a Steve Gibson Swiss Army Knife iOS app that is just your website, all the stuff in there.



STEVE:  Well, and in fact I did the breathing pacer like this in JavaScript because now it's multiplatform.  And I specifically did it so that it would run on an iPad or on an iPhone.  So my point is, yes, I have all those feelings about JavaScript.  And as a language, I understand why it is the way it is.  But it is an abomination.  Yet it's also what we have, if we want to do client-side browser scripting, run code in the browser.  You have to type the URL in, Leo.  There's not even a...



LEO:  There's no menu for your breather, breather?



STEVE:  No.  GRC.com/breathe.htm.  And that'll take you to it.  And it'll just...



LEO:  Steve's the last guy in the world, by the way, still using .htm.  It's okay, Steve.  You're an old-school guy.  That goes back to the days when Windows only could have a three-letter document extension.



STEVE:  Yeah.



LEO:  So what do I do?  I should just follow the breathing here?



STEVE:  Yeah, if you can.  You hold your breathe at that point, and then you do a very slow, 10-second exhale.  So this gives you three breaths per minute, or a 20-second breathing cycle.  And the idea is you want a faster inhale and then a hold and then a slower exhale.  And over time it triggers a very strong parasympathetic reaction, if you don't pass out.



LEO:  You must have amazing lungs.  I cannot come anywhere close to this.  You can change the parameters, though.



STEVE:  Yes, you're able to tweak them to suit your purpose. 



LEO:  That's really cool.  I didn't know you did this.  Wow.



STEVE:  Yeah.  So anyway, so I know JavaScript.  I've used it for many interesting projects.  It has its place.  But, yes, it's just - they're in the process of trying to fix it because remember that this came from Netscape back in the day because they just said, let's do some simple scripting tool for nonprogrammers.  Okay?  Scripting tool for nonprogrammers.  You know you're destined for trouble.



LEO:  That's a bad start.  Yeah, that's a bad start.



STEVE:  If that's your definition, if that's your goal...



LEO:  Oh, and then let's build it into every browser.  Why not? 



STEVE:  Oh, yeah.



LEO:  I should point out that Steve so eschewed JavaScript for so long that he did his whole menuing system in CSS at GRC.com.



STEVE:  Yes.  JavaScript-free.



LEO:  Yeah.  But you finally, at some point, said, well, let me do some JavaScript using the best parts, the good parts.



STEVE:  Yes.  The things I do in JavaScript are those you cannot do on the server side.



LEO:  Right.



STEVE:  So it makes sense for that.



LEO:  So if you're really interested, kids, might be good to just check, you know, look at his source.



STEVE:  Yeah, just look.



LEO:  Learn a thing or two, kids.



STEVE:  I didn't obfuscate any of that, so you certainly can look at it.



LEO:  Learn how Steve does this, yeah, yeah.  Nice.  I like it.



STEVE:  So Manuel Cheta said:  "SGgrc Will these tools actually help against ransomware?"  And then he cites The Hacker News saying:  "No More Ransom - 15 New Ransomware Decryption Tools Available for Free."  And the bad news is, well, only maybe.



LEO:  You know, I saw that headline, I thought, well, that's dopey.



STEVE:  Yes.  There were some well publicized mistakes.  



LEO:  Poorly written ransomware.



STEVE:  Exactly.  In a few instances where the encryption was done wrong, the key was left behind, or the key was static, and once it was figured out it applied to all of them, you know.  The point was ransomware not done right, which is what we could all wish for more of; but unfortunately, in general, no.  CryptoLocker hits you, it's game over.  It's get out your bitcoin wallet or go find some bitcoins and so forth.



LEO:  The unfortunate thing about articles like this, I got a call on the radio show from a guy saying, oh, I don't have to worry about ransomware.  No, you really do.  This doesn't work for most ransomware.



STEVE:  Yes, yes.  This is a little bit like, okay, I'll buy SpinRite once my drive crashes.  Well, okay.  But you could also prevent it from crashing.



LEO:  Did I tell you, I was having dinner with a guy who is on the board of a - I don't want to say the name, but it's a federal banking thing.



STEVE:  Okay.



LEO:  And so he's at the board meeting, and they tell him, yeah, we're stockpiling bitcoins in case we get bit by ransomware.



STEVE:  [Choking]



LEO:  I know; right?  I know.  Well, I'm glad they're doing that.  That's good.  They're really prepared.



STEVE:  Yeah.



LEO:  Geez.



STEVE:  Well, although I have to say, it also, I mean, although that's fatalistic, and it's not proactively secure...



LEO:  Well, I hope they're doing other things, too; right. 



STEVE:  Yes, yes.  But as we've said, if your organization is not completely in your control, if you're Sony Pictures, you know, and there's a zero-day flaw, and any one of your employees in the organization clicking on the link in email is all it takes, well, yeah, having some bitcoins handy may not be a bad idea.



LEO:  As Plan Z. 



STEVE:  Yeah.



LEO:  And, by the way, don't count on it working.



STEVE:  No, exactly.  Although - yes.  You can't count on it.  And so...



LEO:  There are some honorable ransomware creators out there.



STEVE:  So my argument about hard drive failure is, well, better to prevent your drive from dying.  And in the ransomware case, better to have a current backup so that you're able to fall back to a snapshot that is useful for you.



And finally, Gengar said:  "@SGgrc If I fail school because of this game, can I come work for you?"



LEO:  Uh-oh.



STEVE:  Then he sent me a screenshot.  He's on Level 4000.  That's the next page of the show notes, showing Level 4000.  And I wanted to take the opportunity to remind our listeners:  Squareit.io.  It is really charming.  It is getting half a million downloads in one month.  As you can see from Level 4000, it doesn't end after Level 17 like some of the annoying things we've, I mean, fun, but unfortunately not deep enough puzzles we've talked about before.  Both iTunes and Android.  And it's simple, but it's just right.



Now, I mean, I see a lot of these things.  Our listeners know I love puzzles.  So I'm actively vetting, like, more of stuff than you can believe.  And few things make it through.  You know,  Blockwick was a win.  We've talked about various ones.  This one is charming.  It's free.  Squareit.io.  And so I'll just recommend it again.  Once you get the hang of it, this guy probably - he's obviously become an expert.  But it's not super difficult, but it's, again, there's an art to making these things that aren't just sort of brute-force puzzles.  They're elegant.  And Squareit.io is really elegant.  And in fact I think that's also the website; right?  Yeah, Squareit.io is the website.  And you can find links there to the app on the various mobile platforms.



Michael Surette asked:  "What's up with ShieldsUP?  My open ports 22 and" - wait, 22?  Okay, well, SSH, I hope you're secure.  And 25, he's got an email server for some reason.  "My open ports 22 and 25 show as stealth unless I do a comma-separated custom probe."  So he's wondering why, when he does like an all ports probe, they're coming up stealth, but he knows they're open.  And if he only asks ShieldsUP! for those ports, it shows them correctly.  This happens so often, I wanted to explain.  It's his router.  Increasingly, routers have sort of lame, but it's like, eh, okay, scanning protection.  So if the router sees a bunch of incoming TCP SYN packets scanning for ports from a fixed IP, from a given IP, it will put up a time-based port block on that IP.  And since that's not GRC, that's shieldsup.grc.com, it doesn't affect your connectivity to GRC.  I scan from a dedicated IP that identifies itself as what it is in its reverse DNS.



But the point is, if you believe you have open ports, and you do the full port scan, as soon as the scan gets going, the router says whoa, and just puts up a complete shutdown on the ShieldsUP! IP so that all of your ports appear stealth, although it's actually your scanner working on your behalf.  That's always a feature you can turn off.  And so if you turn that off in your router's configuration, then do a full port probe, you'll see them.  Or you can selectively probe a few, which won't trigger the router's protection.  And that's why the full port scan shows stealth, whereas a selected port shows the truth because a few ports isn't enough to upset the router and cause it to preemptively block anything else incoming.



And then our last bit of feedback from our listeners.  Tom Corwine brought up a great point in two tweets.  He said:  "Re all this talk about ISPs forcing us to install their own CA cert.  How would our IoT devices use secure connections?"  And then his second tweet was a follow-up saying, "Come to think of it, those IoT devices don't even need to use a public CA.  Their manufacturers can mint their own certs."



And I replied, I said:  "Tom, that's exactly correct.  We need a public CA system only so that clients and servers don't have to have any foreknowledge of each other.  They both rely upon a mutually known third party.  But the IoT model is different.  The devices can embed a cert for only the manufacturer's servers."  And I'll note that, if they were to do that, they would probably then also not be connecting over port 443.  They'd probably just choose like 4343 or 4433 or whatever they want to.  The point being that that would eschew the ISP's filter that is only going to be filtering 80 and 443, and allow them to connect and encrypt their connection using a certificate that they have from their own server.  And in that case, it can be an extremely long-lived certificate.



It's only the CA guidelines that restrict to two or three years for certs, as we were talking about before.  I, for example, I have a certificate, www.steve, that I use here just so that I'm able to do my own testing of security stuff.  And I have my own DNS that points that to the www.steve machine.  That's obviously completely invalid.  There is no Steve TLD.  But I made a cert that was good, I think it was for 50 years.  So I just don't have to ever bother with it again.  It's completely useless for any other purpose, and I have been using it for years.



So the point is you can certainly make a certificate with a super long lifetime.  And were I in the IoT business, I think that, I mean, I don't even see a downside to it.  I would embed that certificate and then protect it because you don't have the advantage of expiration.  On the other hand, an IoT device isn't going to be receiving updates on a dynamic basis, so it wants to have something very long-lived.



So, Tom, thanks for giving me an opportunity to note that.  I'll bet that's - it'll be interesting to see if IoT devices are using security to the degree they are, and whether they are connecting to remote servers over 443, standard HTTPS, or are they making just some other non-HTTPS port?  If that's the case, then we're back to ISPs being able to tell us we need to put their certificate in our devices if we're going to use their service because IoT won't cause that to break down.



And we have some errata.  Fun errata.  Our listeners will remember that my head exploded last week.



LEO:  It's come together nicely, though, afterwards, so that's good.



STEVE:  Yes, thank you.  I read that news that the Random.org site was being incorporated into libsodium, the crypto library I love that SQRL uses, that more and more people are using because everything except this was being done right.  And the news was that they were going - it would be the default soon for their cryptographically secure pseudorandom number generator.  And I just put my head in my hands; and I said, oh, my lord.  And our listeners will remember that I was, like, trying to think, okay, what possible, possible utility could this have because the only way I could possibly see it of use is if you were on some sort of platform where there was just absolutely no available source of local entropy that some sort of isolated stranded device could come up with, then, okay, kinda maybe.  I mean, just you could use packet-timing.  Use anything.  But don't go to Random.org, which is an otherwise nice site, but don't build it into your library.



Um, I didn't look at the date of that news.  And what was 10 days ago?



LEO:  Yeah?  April Fool's Day, maybe?



STEVE:  Uh-huh.



LEO:  Whoops.  Goddamn April Fool's Day.  Goddamn it to hell.



STEVE:  So it got me.  Yup.



LEO:  Son of a bitch.



STEVE:  Thank god.  I mean, good.  I have no problem being wrong on this one.  Please, thank god.



LEO:  Oh, I should have caught that.  I apologize.  I should have caught that for you.



STEVE:  I mean, you know, I mean, I was, like, stretching.  It's like, okay, really?



LEO:  It makes no sense, it makes no sense.



STEVE:  Well, how could you - how, what, how, what?



LEO:  It's a pretty subtle April Fool's joke, to be honest.  It's a very subtle April Fool's joke.  Maybe a little too subtle.



STEVE:  Yeah.  So I was awake till 2:25 a.m. yesterday morning. 



LEO:  Uh-oh.  Why?



STEVE:  Finishing the 15th book in the series.



LEO:  Honor Harrington?



STEVE:  No.  That was another one, one with a marathon.



LEO:  Yeah.



STEVE:  This I can recommend without reservation.



LEO:  But you didn't read all 15 in the last week, did you?



STEVE:  No, no.  I found out about it from my sister's husband, my brother-in-law, at Christmas.  I was telling him about the Tanis Richards series by M.D. Cooper.  I said it's really fun military sci-fi, four books.  And he said, okay.  He said, "All I read is military sci-fi."  He said, "You've got to read the Frontier Saga."  And so I came home, I finished whatever I was reading at the time, and I thought, okay.  Leo and listeners, oh, my goodness.  First of all, yes, I read 15 books in the old-school style of visually scanning printed words on a page.



LEO:  Good lord.



STEVE:  I know.



LEO:  Are you insane, man?



STEVE:  I write in assembly language, and I read the way my grandfather did.



LEO:  Read dead trees.



STEVE:  It is a fantastic series.  I would say I like it better than any of the other two, Honor Harrington or Tanis.  They're both - the Tanis Richards M.D. Cooper author, the Intrepid Saga.



LEO:  But wait, Steve.  You've only read Part 1.



STEVE:  I know.  It is 15 books in a series, with a planned five-series set.  So 75 books in total.



LEO:  Who is this Ryk Brown?  And what drugs is he on?



STEVE:  It's R-Y-K Brown.  If you are a Kindle Unlimited subscriber, as I am, they're all free.  That is to say, they're all in that program.



LEO:  So you didn't do dead trees.  You did eInk.



STEVE:  Oh, oh, yeah, yeah, yeah.  I don't do dead trees anymore.  I do eInk.  And I love my Kindle.  Anyway, I just - everything about this, I don't want to go on for too long, but everything about this is good.  And this is not a spoiler because everyone knows I don't do spoilers.  You learn instantly, probably from the back cover of the book, that this is in the future, and that something known as the Bio-Digital Plague has ravaged the Earth and our colonies and knocked us back to the Stone Age.  We're in the process of recovering, having lost everything, and we discover a data ark hidden in the Swiss Alps, which is the sum of all human knowledge prior to this plague, which helps to bootstrap us back.  Unfortunately, there's a spinoff of humanity, our bad guys, who have decided to survive by taking what they need from others.



So at the beginning of the book, a bad accident puts a very green crew in charge of one of Earth's very few FTL-capable ships.  And this first 15 books is just - the Honor Harrington stuff, there are fabulous good parts, but like five in the 20 books of the Honor Harrington series.  And Honor Harrington is being made into a series of movies.  I so wish that the Frontier Saga was doing it in its place.  This is so much better.



Anyway, for people who like military sci-fi, the people are not all super-enhanced.  It's different than some of the other sci-fi where you've got implants and telepathy and all kinds of other crazy stuff.  These are just regular old people in the future.  But again, the way the plots are woven together, there's nothing stretched-out feeling.  Lots of action.  Great characterization.  You care about the people.  Anyway, enough said.  TheFrontierSaga.com.  The author is Ryk, R-Y-K, Brown.  And if you are a Kindle Unlimited subscriber, you can read them all as part of your subscription.



And, finally, this week's Pithy Slogan:  "When in doubt, encrypt.  When not in doubt, be in doubt."  So I thought that was good.  When in doubt, encrypt.  When not in doubt, you should be in doubt.



And I got a classic nice note from a customer, Brett Parks, whose subject was "SPINRITE [in all caps] saves the day yet again."  He said:  "I've been using SpinRite for, what, 20 years now or more.  When all else fails, it will =at least=" - he brackets it in equal signs, sort of chevrons - "it will at least bring a hard disk back to where it will at least be bootable and readable.  And it just did that again with the 1TB hard drive on my primary machine, which went belly up and took my accounting, taxes, email, client lists, yada yada yada, along with it.  It took a couple of runs, but SpinRite got it back to where I could," he says, "I could get EVERYTHING [all caps] off it. Not bad for old school, eh?  Thanks yet again.  Brett Parks, Lexington, South Carolina."  And Brett, thanks for sharing your experience.



Oh, and I did want to note, remember last week I said that I was going to come up with some way to provide some sort of benefit to the listeners of this podcast who have been supporting me by purchasing SpinRite, and in many cases anxiously waiting for 6.1.  And I said I'm going to figure out some way to thank people for that.  And I know what I'll do.  I am, as a consequence of having spent so much time with SQRL - and by the way, I'm just having a ball working on SQRL.  I had to tear myself away from it yesterday in order to pull this podcast together, but making lots of very fast progress.  I'm working now on the self-uninstaller.  I don't want to have a separate uninstaller because, well, I maybe don't need one.  But the trick is how to get an executable file which is running to delete itself.  It turns out that's not easy.  I've got it working so that you would just be able to say, "Go away," and it will do that for you.



But anyway, what I'm going to do is, to compensate for how long it's taking me to get going, I'm going to get 6.1 out sooner.  But I know that I will be at a point that I will consider it safe, but at prerelease.  And at that point it will be available to our listeners exclusively.  So I'll work out the details when we get to it.  But I just know that, I mean, essentially, the work I was doing on 6.1 before, I had a whole group of people who were testing it as we were going along.  That's how, for example, that I know that it is able to run at half a terabyte per hour.  It's already done that.



So I'm going to - my priority is going to immediately get a 6.1 out the door, even before it's over on the Mac platform.  Just get it running on the existing PC platform because I can do that sooner.  But then I will shortly do a 6.2 which adds native Mac support.  And with .3 and .4 and so forth, I'll add native USB and so forth rolling forward.  But that's what I'm going to do.  I will - it's always the case that something is workable and usable, even if it doesn't have all of the finish and polish on it.  I mean, for example, we've been using SQRL for a year, but not in a way that I felt it was ready to get the full seal of approval.



So SpinRite 6.1 won't take nearly that long.  I mean, as I said, when I suspended it to go to SQRL, it was coming along.  I was much more ambitious about what I was going to do.  I'm going to dial that back now in order to get something out fast so that people who have 6 will be able to get this tremendous benefit of its compatibility, freedom from the BIOS, able to operate at literally the absolute maximum speed the drive is able to transfer at.  That is, that's where I am because this thing allocates a 32MB buffer, whereas drives are typically limited to a 64K buffer.  So this thing is a vastly larger buffer that allows streaming transfers that can also be overlapped.  So anyway, no stone unturned in the performance of this thing.



And Leo, we're going to discuss Proactive Privacy and do the Proactive Privacy Roundup.



LEO:  We made it.



STEVE:  Yes.  We have time.



LEO:  We have time.  We have a whole half hour.  I hope that's enough.



STEVE:  Yup. 



LEO:  All right, Steve.  We've been waiting two weeks for this.



STEVE:  Okay.  So our ideal, the ideal situation would be for a user to have an entirely separate one-to-one relationship with every entity that they transact with on the Internet, so that there is no cross-entity leakage, so that where they go only knows about them.  And there's no concept of tracking.  The word was never minted because it was never possible.  We know that's not the world we live in.  So what are, first of all, what are the tracking hooks?  What are the things, the technologies that start us being able to be tracked?



Well, the first one is the great-granddaddy of them all, which is our IP address.  As we know, in order for Internet traffic to get back to us, when it's outbound it must carry essentially a return address so that the server that we're connecting to or the service that we're reaching or email, you know, whatever it is needs to have the IP, the public IP address for us.



So the first and most obvious thing is our IP address.  Now, it's true that those are not typically static over the long term.  Back in the old days, where you were dialing in with a modem into a modem pool for CompuServe or AOL or EarthLink or whatever, you got an IP address literally assigned for that connection.  It was only valid for that connection.  When you disconnect and reconnect later in the day to check your email, you're going to have a different IP address.



Now, IP addresses technically, as we know, are "leased," is the term, through DHCP.  And there's, in part of the DHCP protocol, the lessee says, "I need to renew my lease.  Here's the IP I currently have."  And so there's actually in the protocol is an attempt to remain static, that is, to keep the same IP.  So, for example, in my case, where I've got a cable modem, and I have a router which is independent, and it's never powered off unless something sort of catastrophic happens, I mean, typically almost never.  The last time it was, was when we had that weird connectivity problem, and I was trying everything to see, to verify it wasn't my problem, it was something upstream at the cable provider.  Well, I did, I power cycled it and so forth.  But even there, when I was completely off the 'Net for half an hour and got back on, I was able to recapture my same IP.  DSL connections, which are up statically, will tend to have a static IP.  So in general, your IP won't change for a connectivity session.  It's not guaranteed not so, but it generally doesn't. 



Now, it is the case that, where a household is behind a single NAT router, anyone who's interested in who you are, if they only have your IP, then that reduces them to household-level granularity.  That is, the IP address wouldn't say whom within the house, if that's all they had.  But that's certainly better than nothing.  And as we'll see by the end of this, all of the evidence that we know of demonstrates that there is a huge industry that surrounds tracking.  There is a lot of money involved.  There's huge incentives.  And while there are smart people who are working on blocking tracking, the people who are working who are being the developers, the programmers, the coders, whose paycheck is dependent upon them figuring out clever new ways to track us, that is, not to get shaken, they're doing everything that they can, and they're just as good.



So the point is the only rational position to take is, if you care about this whole issue of tracking, you must assume that everything that we can think of, we who want to resist being tracked, everything we can think of, the other people, the trackers have thought of, too.  And so, for example, I'll make the case that IP address, low granularity as it is, transient as it is, it is still a powerful signal.  And, for example, if you suddenly became unrecognizable, that is, you changed OSes, you changed browsers, you used a freshly wiped and scrubbed system, you were in incognito mode, but you didn't change your IP, you're not fooling anyone because even though, you know, somebody looking would see, would know the IP you had five minutes ago and would lock onto you with at least knowing you are the same IP, even though you are otherwise completely unknown.  And all of your attempt not to be connected would be lost because you hadn't also changed, arranged somehow to change your IP at the same time you had done everything else.



Okay.  So that's IP.  Cookies, of course, is the second oldest and most mixed-blessing tracking system.  Cookies, as we know, were originally conceived by Netscape as a means for maintaining our session state.  And they're still used today for that.  Session state acknowledges the fact that our browsers do not have, typically do not have a persistent connection, that is, it's not like an old-school Lear Siegler or Hazeltine terminal, wired like with a modem to a remote mainframe, like back in the CompuServe days, where you're typing essentially on a console of the mainframe which has been remoted to you.  Instead, a browser makes a query, it receives a response, and it disconnects.  And then the user looks at some things, clicks a link, that makes another query.  And it receives a response, and it disconnects.



So the question is how are those two separate events, those two queries, associated with being the same person?  Of course first-party cookies is the way we do that.  When the server responded to the first query, it gave the browser a cookie tagged with its domain and various other requirements.  For example, if this is a session cookie, we hope that it has a secure tag on it so that, if ever an HTTP query was made to that domain, that cookie would not be transmitted.  Instead, it would only be sent back by the browser with subsequent queries over a secure connection.  And there's expirations and persistent versus session and other sort of variations.  But that's the concept.



I would argue that a lack of foresight, probably - well, maybe not, maybe just the wrong decision, or just the way things evolved, but what we ended up with was no explicit consideration for sourcing content from third-party sites.  That is, a website originally hosted all of its own content because there just wasn't anybody else you could go to to say, here, have some images for me.  And they go, what?  Yeah, we don't do that.  So everybody was hosting all their own stuff in the beginning.



Then, especially with the advent of advertising, where a site could generate revenue by hosting somebody else's commercial content on their own page, suddenly now the web browser is going to a third party.  Well, third parties, unless explicitly prevented, because your web browser is asking for content from that site, that site, that is, an advertising domain, a web beacon domain, a Facebook Like button, a Google Analytics bit of script, whatever that content is, it's making a query to that other domain because it's not the primary, the first-party domain.  That's what makes it the third party.  But within the constraint of that domain, cookies are allowed unless disabled.



Now, what's interesting is, I mean, and I've been aware of this concern about third-party cookies forever, GRC has another non-advertised, not linked to GRC's main menu, deep bunch of technology known as Cookie Forensics.  At GRC.com/cookies is a set of pages which have been in place for, I don't know, like a decade or so.  So, for example, as a consequence of that, I know, because I looked this morning, that of GRC's 43,576 unique visitors last week, and this is updated at midnight on Sunday night every week, a snapshot is made and updated from the stats that were accumulated.  So 43,576 unique visitors last week.  79.21% of GRC's visitors, so just shy of 80%, had third-party cookies enabled.



But among those visitors who were using Safari, only 18.83% of Safari visitors had third-party cookies enabled.  Why?  The tyranny of the default.  Apple's Safari browsers are alone in the industry in disabling third-party cookies by default.  And in fact five years ago Google agreed to pay a $2.5 million civil penalty to the FTC after they were caught deliberately using some JavaScript trickery to leverage a small flaw in Safari's iFrame handling at the time in order specifically to place tracking cookies into Safari browsers that were configured not to allow it.



And even today there is discussion among developers who are being thwarted by Safari.  These are the people I talked about who are on the payroll of advertisers and tracking companies, annoyed that Safari is causing them so much trouble.  I say bravo to Apple.  And I wish, I mean, and we've talked about first- and third-party cookies a lot on the podcast because it's always been the primary tracking technology built into browsers.



Anyway, GRC.com/cookies for anyone who is interested.  And I even have a cool page that shows a graph of the percentage of which types of cookies are enabled and disabled by browser vendor.  And back at the time, many browsers had cookie-handling flaws which that page was demonstrating, that is, the Cookie Forensics.  You're able to run, I should mention also, there is a forensics page.  You can perform a test yourself on your own browser.



And, for example, I had third-party cookies enabled in Firefox.  I'd forgotten about that.  Because remember when they removed it from the UI, where it used to just be a checkbox you could turn off, and they buried it under, like, almost hard to find.  You have to go to Tools, Options, then Privacy.  And then you look at that page, and it's not apparent.  And then under the History dropdown you have to change it to "Use custom settings for history."  Only then does it reveal a bunch of settings, and among them are "accept cookies from sites."  And then, under that, "accept third-party cookies," you can set it to "never."



So before I did that, and I used GRC's cookie forensics, it showed me that in the second half of that page was all red showing that, like yours shows, Leo, that you are accepting third-party cookies on your browser currently.  If you turn that off and rerun the test, it'll show you that they are being blocked, and you don't have a problem.  And again, cookies are not the only way people track us, but it is certainly a way for them to grab hold and lock onto us.



So even today, problems remain.  I found an interesting link I won't go into in too much detail because it's JavaScript trickery.  But the NCC group that we've talked about in the past, a great security company, they have a post about setting cookies for third-party websites in different browsers.  And their TL;DR reads:  "This post discusses the results from our research into the ability of third-party websites to set cookies for third-party websites across different web browsers.  The ability to set cookies in this manner not only facilitates tracking, but also opens up other opportunities and avenues of attack."



And the short version is they have a really nice chart where not one single browser was immune to all variations of the attack - not Chrome, not Safari, not IE11, not Firefox.  I didn't see Edge.  I don't think I remember seeing Edge there.  But the point is that even today there are still problems with the way cookies are handled because, like so many things, it wasn't designed to be bulletproof from the start.



And here's the problem.  After all, I mean, first of all, turning off third-party cookies is simple.  I would argue that since Apple and Safari do it, and a huge chunk of the web is not broken for them, and the tracking people are pulling their hair out with frustration, the first thing anyone should do, it's trivial, turn off third-party cookies.  Thank you very much.  And then verify that they're off.  One of the problems we used to see is that - and the forensics page, the Cookie Forensics page at GRC, you're also able to just, by the way, to say Google, GRC Cookie Forensics, and it'll take you to that page because Google has found it long ago.  You can, if you turn off cookies, some of the browsers would not renew their cookies, but they would still issue stale cookies.  And the forensics page shows you the age...



LEO:  Eww.



STEVE:  ...of the cookies.



LEO:  Who wants stale cookies?



STEVE:  Nobody wants to have a stale cookie.  So, yes, you could get stale cookies.  Now, with all that said, we still have a problem, Houston, because HTTP redirection chains, which are becoming much more prevalent, completely thwart third-party blocking by allowing first-party queries.  You may have noticed, for example, that the links on Google no longer take you, as they once did, actually to the thing Google has indexed.  Those are all Google links.  And so you're clicking on a Google link whose tail contains the URL of the site you're going to go to, which is Google's way of acquiring information about you because, I mean, they already got information about you when you brought the Google page up.  But now they know what you clicked on.



And you may have noticed that sometimes you click on a link, and your URL flutters a few times.  It goes [vocalizing], and then you kind of get somewhere, it's like, what the hell just happened?  Well, what just happened was you went through a chain of maybe five or six other sites that have chained themselves together only for the purpose of tracking you.  What happened was you went to a site not where you thought you were going, a third-party site.  But because your browser went there, it's briefly first-party.  It got a cookie from you.  And then it forwarded you to the next site in the chain, which got its cookie from you, or gave you its cookie, and then sent you to the next site in the chain, and so forth.



So this is the argument that some people have made about why bother blocking third-party cookies because HTTP redirection and JavaScript tricks still allow sites you are not directly visiting to get first-party access to you.  And this is one of the ways that happens.  I counter by saying, if third-party cookies are still today making tracking developers pull their hair out, then that's every bit of reason to disable them because obviously everything still works.  And so just don't make it easier for the bad guys.



And finally, plugins have historically been another source of tracking problem.  Plugins, I mean, we've talked about flash cookies for a long time.  There were instances where you could wipe out all of your browser cookies, but if you still had Flash, the site could use a Flash cookie in order to create that persistent link to you in order to repopulate your first-party cookies.  And there were even services that actually sold that facility.  They said, "We have technology that prevents us from losing track of people.  Even if they delete all of their browser cookies, we're still locked onto them."  So browser plugins was one other way that there was a problem.



Okay.  Next in line is things your browser sends in its metadata.  That is, other stuff than just the query or the  cookie.  And I had to look at the date on this because 16 years ago, on March 17, 2001, the people in one of GRC's newsgroups stumbled on the fact that the EarthLink custom browser was apparently embedding a unique token into every query.  I created a page about it.  It generated a huge amount of brouhaha on the Internet.  And EarthLink quickly produced a statement and demonstrated that what they were doing was generating - they had created this weird-looking serial number thing which was static for individual users.  And when we compared notes, those who had an EarthLink browser, everybody's seemed to be different.  So it looked exactly like something meant to fingerprint us.



And it turned out that we got a breakdown of how it was composed, and it was a whole bunch of characteristics about the machine it was running on, things like screen width and screen height and so forth.  Still, it's the kind of identification information that people didn't like having their browser transmit.  And EarthLink removed it shortly thereafter because I think it was the kind of thing that was there, and they weren't even using it.  And after everybody screamed about it, they said, okay, fine, bad idea.  The point is, none of this is new.  Sixteen years ago GRC had a page, GRC.com/su - that's for ShieldsUP!.  So GRC.com/su/earthlink.htm.  And that thing is 16 years ago, where we were looking at the privacy consequences of our browsers embedding stuff that we're not happy with.



Of course, we've talked about something similar to that with Panopticlick.  Panopticlick is the EFF's project.  And I ran it just before, actually while putting the show notes together on an otherwise rather locked-down Firefox browser, mine.  And so the test results came back, is your browser blocking tracking ads?  Yes.  Is your browser blocking invisible trackers?  Yes.  Does your browser unblock third-parties that promise to honor Do Not Track?  No.  Does your browser - which is, you know, good.  Does your browser protect from fingerprinting?  No.



And, boy, I then click on "show full results for fingerprinting."  This thing has matured more.  My browser, unfortunately, has a unique fingerprint.  And if you look at the details of what the fingerprinting returns, now it's gone from a few things to an encyclopedia that apparently is unique about my machine.  So that's a thing, too.  That is, metadata that our browser can - I'm sure they're running scripting.  So metadata that the browser or JavaScript is able to use in order, again, to come up with something unique about us.



And lastly, DNS queries.  Remember, as we've mentioned, that even an ISP that cannot see the content of your HTTPS connections, they do know the IPs to which your traffic is bound.  And unless you do something to hide your DNS from them, they also know every domain you and your browser look up.  And believe me, as we know, that's no longer just where you go, it's everything that your browser sucks in from the web.  The browser needs the IP of all of those domains.  And so those are outbound DNS queries.  And even if you don't use the ISP's server, that is, if you're using OpenDNS, for example, doesn't matter.  DNS is not by default encrypted.  Even if your other traffic is, DNS is UDP with no encryption.



So one thing to consider is using DNSCrypt, D-N-S-C-R-Y-P-T dot org.  It's becoming increasingly mature.  It's open source, both clients and servers.  There are clients for Windows, macOS, Linux, Android, iOS, and even routers.  So you could put DNSCrypt in a router which is able to host it.  And your systems all just use your router's DNS.  It encrypts your queries out to a public DNSCrypt resolver.  And there are many now public DNSCrypt resolvers around the world.  And that prevents anybody other than the DNS server from knowing who you're asking the IP addresses for.



So those are the various technologies, both intended and unintended, and metadata for compromising us using the technology.  Of course the other point, and this is a point, Leo, you have correctly made frequently, is what about the major web players?  We've talked about Google and Google Analytics.  Google knows who we are, if we have an account at Gmail or Groups or an account with any Google property.  Our system contains their cookie, and we want the convenience.  I don't even think you could use Google without at least having a first-party session cookie.  And if you only did that, you'd have to log in again every time you reopened your browser.  So it makes more sense to use persistent cookies to cut down on that.  At least Google has gone HTTPS.



But Facebook also knows who you are, if you are a Facebook user.  And everywhere over the 'Net they plant their little Like buttons.  Those Like buttons are browser queries back to Facebook that are cookie-enabled.  And unless you have blocked them, they're a beacon telling Facebook not only who you are, but where you are right now and what you're doing.  And the fact is any major player on the web, a Google, a Facebook, a Microsoft, a whomever, Bing and so forth.  They are giving your browser cookies.  And anytime your browser ever fetches anything from them, no matter where you go, they know who you are, and they know where you are.



And so this is one of the reasons to help people who are concerned about their ISPs, like what this legislation means.  It's why it's like, okay, well, calm down because arguably, as we're moving more towards HTTPS and TLS connections, in the same way that we're going dark to the NSA, who can no longer see our traffic, we're also going dark to our own ISP, who cannot see our traffic.  They can see our DNS.  So a simple thing to do would be to disable third-party cookies because we know the tracker people are still being driven crazy by that, and configure yourself to use DNSCrypt.  And then most of the majority of your activity and the DNS you look up is being blinded.  But remember, the ISP still sees the IPs you go to.  So for that, we need to talk about a VPN.



It's worth also mentioning first the option of using incognito browsing modes.  That's not a complete solution, but it's part of an us going dark toolkit, that is, the idea of wiping our history.  We don't want to have visited links show up because we know there have been hacks that show they're able to probe the visited link cache in our browser by noting what color they are because browsers display visited links in different colors, and that's something that somebody can remotely obtain.  And we also want to not transmit any cookies from our non-incognito mode.  So it's sort of a way of just looking like somebody unrecognizable.  But if you've got the same browser plugins, the browser plugin could be deanonymizing you.  And if you're using the same IP, we know that that can transiently allow someone to make the bridge from incognito to non-incognito and back and forth.



Okay.  So a VPN.  The good news is VPN crypto is rock-solid and bulletproof these days.  I would argue that OpenVPN is the leading contender as a protocol.  It is both an implementation, but it is also a protocol.  There is a very nice VPN known as SoftEther.org which offers a bunch of clients and servers, and it offers OpenVPN as a protocol.  The advantage of OpenVPN is it's rock solid.  Many VPN providers, like proXPN, for example, offer VPN as a solution, that is, as their primary protocol.  So the advantage of that is there are OpenVPN clients everywhere.  It is rock solid.  There are point-and-click installers that are easy to use.  What it does is, as we know, it encapsulates all of your traffic out to the VPN server.



Now, it's worth making sure, though, that your instance of a VPN is encapsulating DNS because there are some that don't.  There are some that are protocol specific; and, for example, they'll only encapsulate your HTTP and HTTPS traffic, maybe your email.  That is, they're selective, rather than encapsulating everything.  So if you thought you had full encapsulation, but your DNS was not being encapsulated, you'd want to know that.  You can use DNS Spoofability Test to find out.  If you run the DNS Spoofability Test at GRC, among many other things, it shows you which DNS servers are resolving your current DNS queries.  When you bring up a VPN, you hope that changes.  If the same DNS servers are resolving your DNS queries with your VPN up, and you use the DNS Spoofability Test, then that means your VPN is not a full-isolation VPN.  And then the question is what other than DNS, if it's not encrypting and concealing your DNS queries, what else is it not encrypting and concealing?



Now, the bad news about VPNs is they don't proactively do anything more than that.  They are super useful for protecting your local traffic from local snooping.  Your ISP, the hotel you're in, the caf you're in, over open WiFi, the airport you're in, et cetera.  So that there are certainly use cases for it.  And it also changes your IP, which is crucial if you want to immediately go dark, along with doing a lot of other things.  But VPNs don't otherwise proactively do more than that.



And the other problem is that there is the concern of the emerging traffic on the Internet being at well-known VPN server, so-called, you know, you could call them exit nodes.  Not quite the same as Tor because, of course, Tor is largely used by people who are trying to evade, I mean, like proactively putting up with much slower Internet use in order to be anonymous, to have their IP be anonymous.  So Tor exit nodes tend to attract much more negative or law enforcement-style intelligence attention than just random VPN servers on the 'Net.  But there is still some concern about the inherent traffic concentration that VPN exit nodes create.



And I have in the show notes a bunch of notes about choosing a VPN provider.  There are a number of VPN ranking sites.  I don't have any preferred one.  Of course, you know, proXPN has been a long-time sponsor of the TWiT Network, and we know that they don't log.  But there are many sites that rank and rate and have, like, large spreadsheets.  I would imagine there's probably even a Wikipedia page that has all kinds of characteristics of VPNs.  So many are good.  You do want to be careful that you've got one with a reputation.  I forgot that one of the things that Nicholas did at Motherboard when he was looking at MySafeVPN was he did a WHOIS query, and he noted that their registration for MySafeVPN was dated March 30th.  So they have not been around a long time.



LEO:  My brand new SafeVPN.



STEVE:  Exactly.  So checking the reputation of any of these places is something that you would want to do.  And of course seeing how long the WHOIS record has been there is one way to do it.  I remember noting that mine was only six months younger than Microsoft.com.  So, you know, we both got into this game in the early days.  So as for Tor, eh, I was hoping that the Tor browser would make a more proactive effort at protecting privacy.  It has a few mild tweaks to it.  It has NoScript installed.  It has HTTPS installed.  And some default settings have changed.  But it's not like they built in a really good query filter that absolutely strips the browser headers to the bare minimum.



That's what I would do.  I would have, you know, a bogus User-Agent and an Accepts header for what content you can accept with no extraneous information in it.  I mean, I would really strip it to the bone.  They don't.  So I don't think Tor buys you that much, if what you're really concerned about is just, like, depending upon how much time you intend to be in private mode.  And it's not easy to stay there because you're going to have problems with services being as sticky as you want them to be.



One last thing I'll note is that in the last couple weeks I saw an Indiegogo project called Filter, which was yet another bogus filter router project where they were claiming to do lots of inspection of your traffic in order to protect you from malware and other problems.  Well, we know that just as the ISP cannot see into encrypted traffic, neither can your router.  So unfortunately they are dramatically overselling what they're able to do.



So with all of that foundation, the last page of the notes here is strategies and tactics for maximizing your online privacy.  As I said at the top, due to the tremendous pressure to track and profile, and the massive amounts of money behind that, we must assume that everything that can be done to track and profile is being done.  And I will remind us of this week's slogan:  When not in doubt, be in doubt.



LEO:  I love that slogan.



STEVE:  Isn't that great?  Yes.  You must change your public IP because, that is, so think about, like, you're about to embark on something where you need to go dark.  You need to emerge on the Internet anonymously.  So you must change your public IP because, again, if it can be done, we must assume it is being done.  And if any of these people on the Internet have been seeing a lot of traffic from this IP, and suddenly a browser query comes out with no headers and no cookies and looks completely anonymous, but from the same IP that something was just coming from two minutes ago, okay, suddenly that new traffic that just blew its anonymity has been associated with the immediate previous traffic.



So changing your IP has to be part of it.  Go to another location.  Shut down your cable modem or DSL long enough to obtain a new IP.  Maybe if your router has a release-and-renew DHCP, you might be able to force a change of IP.  And of course now you can just put "my IP" into Google, and they'll tell you what your public IP is.  Or use a VPN.  So go to a different location or pretty much use a VPN.  That will immediately change your public presence to the IP of the VPN server or wherever you've gone.  You must use incognito mode, that is, switch to that mode where there is no browsing history to be brought forward.



And one thing to consider, if this was something you wanted to do from time to time, would be to wipe and freshly set up an Ubuntu Linux system on a retired laptop, just for browsing via a VPN.  That is, make it your black ops laptop.  Start it fresh.  Set the desktop to bright red to remind you, never log in with any of your accounts.  You use it for anonymously poking around or doing whatever you want to do.  Or maybe use a VMware VM where you're able to use its snapshot feature so you're always able to wipe any changes and roll back to a pre-surf point.  But when you switch into anonymity, you've got to change your IP.



I would also argue, why not change your browser vendor?  Use a different browser when you're in that mode.  And by all means you want to be encrypting your DNS with DNSCrypt.  I think if you want to switch into that mode, change your IP, change your browser, use incognito mode, don't log in with any normal presence, and use DNSCrypt, you're good to go.  You're about as proactive as you can be.  Oh, and by all means turn off third-party cookie tracking because even to this day it's making the developers pull their hair out.



LEO:  And we're done?



STEVE:  We are.



LEO:  That's everything I need to know?  The complete set?



STEVE:  That's the whole tune-up.



LEO:  The whole kit and caboodle?  You know, we've got to put this section out as a special for everybody who wants to go through your step-by-steps and do that.  That's great.



STEVE:  I think we end up with a nice set of guidelines.



LEO:  Yeah, very good.  Wow, thank you, Steve.  I really appreciate that.  And as some people, whoever's watching the video stream might have seen, I'm slowly adopting your tips as you go.



STEVE:  Cool. 



LEO:  Yeah.  We do Security Now! every Tuesday, about 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  If you want to tune in live, you can.  I'm watching the chatroom.  Steve is not.  He's busy.  He's doing a show.  But I am.  We also invite you to download it and watch it after the fact.  Steve's got copies on his website, GRC.com.  He also has something that's unique to his website, which is human-created transcriptions.  Elaine Farris does such a good job of getting all the geekery in, properly spelled and all that.



STEVE:  She really does.



LEO:  Yeah, GRC.com.



STEVE:  I get into a SQRL mode, and the rest of the world just -  I worked 14 hours day before yesterday on SQRL.



LEO:  Oh, Steve, wow.  You love what you're doing, though; right?



STEVE:  I had a ball.  I love what I'm doing.  



LEO:  Are you doing - your side of SQRL is all assembly?  Or are you doing some JavaScript and stuff, too?



STEVE:  All assembly.



LEO:  All assembly.



STEVE:  Actually, there's a little JavaScript on the demo page because it pings the server to see whether a SQRL, like a mobile SQRL has authenticated, in which case the page magically updates, and you're logged in.



LEO:  Ah, neat.



STEVE:  So, yeah.  Cool technology.  I will be doing a - I'm making great progress.  So we'll be having a full SQRL demo and coming out party here before long.



LEO:  We'll name that show "SQRL Mode."



STEVE:  Squirrely mode.  It's finally here.



LEO:  Squirrely mode.  GRC.com has lots of other stuff that Steve has worked on over the years.  You heard us talk about it, including, of course, SpinRite, the world's best hard drive and recovery and maintenance utility.  Got to have that if you've got a hard drive.  We also host Security Now! audio and video on our website, TWiT.tv/sn for Security Now!.  And you can subscribe everywhere.  You know, we've been around long enough that it's in every podcatcher and so forth.  So just subscribe, and that way you'll get it every week.



And I think there are a lot of people who not only listen every week, but save every episode because it's like having the six-foot shelf on how computers work and security and all that.  So I encourage you to do that:  GRC.com, TWiT.tv/sn, or your favorite podcatcher.  I guess that's it, Steve.  I hate to say it, but I'll see you next week.



STEVE:  Will do, my friend.  Till next week.  Bye.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.


GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#608

DATE:		April 18, 2017

TITLE:		News & Feedback Potpourri

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-608.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  This week Steve and Leo discuss another new side-channel attack on smartphone PIN entry (and much more).  Smartphone fingerprint readers turn out to be far more spoofable that we had hoped.  All Linux kernels prior to v4.5 are vulnerable to a serious remote network attack over UDP.  There's a way to prevent Google from tracking the search links we click (and to allow us to copy the links from the search results).



They cover the latest NSA Vault 7 data dump nightmare, the problem with punycode domains, four years after the public UPnP router exposure, the mixed blessing of hiding WiFi access point SSID broadcasts, some miscellany, and then a collection of quick "closing the loop" follow-ups from last week's "Proactive Privacy" podcast.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson's here.  We've got a jam-packed show, lots of news.  We'll talk about, oh, all that, what is that, a puny URL attack?  We'll talk about routers.  This is a great router exploit explanation from a couple of years back that is such detail, I just really enjoyed it.  And of course Steve will even answer some questions.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 608, recorded Tuesday, April 18th, 2017:  News & Feedback Potpourri.



It's time for Security Now!, the show where we cover your privacy, your safety, and your peacefulness online with the man in charge, Steve Gibson at GRC.com.



STEVE GIBSON:  I look a little blurry.  Am I blurry?



LEO:  Not to me.



STEVE:  Okay, good.



LEO:  Did you put your contacts in this morning?



STEVE:  I just did.  I was glad you were running a little bit late because of the Facebook thing.  It's like, oh, good, I have till 2:00.



LEO:  No.	



STEVE:  But you ended up picking up some time.



LEO:  I made up time.



STEVE:  It all worked.



LEO:  We had the jet stream at our tail this hour.



STEVE:  This is just sort of a general News & Feedback Potpourri episode, which is the title, because a lot of things happened this week that were interesting.  And last week's coverage of the proactive privacy, which was extremely popular with our listeners, generated a huge amount of feedback.  And so we're going to talk - we'll deal with some of that feedback at the end of the podcast, but handle all of the news and some fun stuff and miscellany.  And I think we've got another great couple hours.



LEO:  Sounds fantastic, Steve.  Are you pausing for me to do an ad so soon?  I will.



STEVE:  Oh, well, you're right.  I forgot to say, this week on Security Now!...



LEO:  This week, what's coming up.



STEVE:  ...we've got another new side channel attack on smartphone PIN entry, and actually much more than that.  It turns out also in a related research, but different, smartphone fingerprint readers turn out to be far more spoofable than we had hoped.  And I've got a takeaway mission for our listeners.  All Linux kernels prior to 4.5, which is very recent, are vulnerable to what is being characterized as a serious remote network attack over the UDP protocol.  Nah, but we'll talk about that.



It turns out there's a way to prevent Google from tracking the search links we click on, which we discussed last week, how all of those Google search results are redirects, so Google can see what you choose.  And that also allows us to copy the links from the search results, which is one thing that's been annoying me because we lost that ability at some point in the past.  We've got the latest NSA Vault 7 data dump nightmare.  The problem with punycode domains.



LEO:  Oh, good.  I'm glad you're talking about that one.



STEVE:  Yeah, yeah, yeah.  Really, really interesting hack.



LEO:  Yeah, yeah.



STEVE:  And which has been fixed, by the way, in all but Firefox now.  Chrome's already updated.  Edge, IE, and Safari were not vulnerable.  But there is a switch that Firefox users can flip in order to protect themselves.  Four years ago we discussed the public exposure of the Universal Plug and Play protocol.  Now, a little more - it was like January, I think it was, of 2013.  We now have the first full public disclosure from the guys who found it about exactly what that was and what the consequences are.  We're going to take a close look, thanks to a question from a listener, about the mixed blessing of hiding WiFi access point SSID broadcasts.  And then, as I said, some miscellany, and then a collection of "closing the loop" follow-ups from last week's Proactive Privacy podcast.  So yabba dabba.



LEO:  Sounds good.  Yabba dabba do.



STEVE:  So our Picture of the Week.



LEO:  Oh, I love it.  Oh, my, my.



STEVE:  Well, for those who are listening and don't have the benefit of seeing it, this shows a keyless entry keypad of transparent, probably illuminated in the evening with a green LED - I'm sure I've seen these before.



LEO:  Oh, yeah.



STEVE:  It looks like it's a garage door opener keyless entry keypad.  And in this case it's two columns of five buttons with an ENTER.  And the ink, the legend of the first four is completely rubbed off.  I mean, you can barely see a 2.  But the 1 and the 3 and the 4 are completely missing; whereas the 5, 6, 7, 8, 9, and 0 don't look as if they've ever been pressed.



LEO:  Nobody, yeah, unh-unh, never, unh-unh.



STEVE:  And so, let's see, what could this person's PIN possibly be?



LEO:  Now, there are a few permutations.  I mean, that's probably 1234.  But it would be 1342.  How many would that - 16?



STEVE:  There are 24.



LEO:  Twenty-four.



STEVE:  So assuming that it's a four-digit PIN, which it sure looks like, the first could be any one of four.



LEO:  And it's ain't 1111 or 2222.



STEVE:  Right.



LEO:  It's got to have all four.



STEVE:  Because they're all rubbed off, exactly.  So the first one could be any one of four.  Then the second one could be any one of the remaining three, so that's 12.  Then the third one could be either of the others, which would bring us to 24.  And the fourth one is the remaining digit.  So if it's one of each digit, and this looks like it is because all four of them are rubbed off...



LEO:  Oh, yeah, they're equally rubbed.



STEVE:  ...it's one of 24.  So you could - and a garage door entry probably doesn't have a lockout because then you couldn't get into your garage, if you fumbled.  So my guess is it would take you maybe 30 seconds, maybe a minute to get in.



LEO:  Unless, and somebody in the chatroom said, they sell them this way.



STEVE:  That would be brilliant.



LEO:  Wouldn't that be brilliant?



STEVE:  Yes.



LEO:  I would buy it that way.  Wouldn't that be funny?



STEVE:  That would be brilliant.  What you want is you want to buy a used keyless entry pad like this, and then reprogram it with digits not rubbed off.



LEO:  Right, right.



STEVE:  And nobody will ever - they'll spend all their time thinking, why didn't that work?  I've tried all 24 combinations.



LEO:  What did I miss?



STEVE:  Wonderful, wonderful.  So whoever it was who sent me that, thank you very much.



LEO:  Neat.



STEVE:  Okay.  So we're constantly talking about ways that - essentially new, innovative side channel attacks.  We've talked about how PCs, you know, lights can flicker, hard drives can make noise, speakers can make noise, of course WiFi signals or RF emanations and so forth.  We talked a while ago about a remote camera looking at somebody entering their PIN, but unable to see the face of the phone, just looking at the relative motions of their hand, and how that was enough to dramatically reduce the search space required to determine what the PIN was.  Well, some researchers at the School of Computing Science at the  Newcastle University in the U.K. did some research and developed a JavaScript embed, essentially, a JavaScript page that was able to ascertain where the user was typing on their screen in order to determine what PIN they had entered.



So the abstract reads, and then we'll discuss it, it says:  "In the first part of this paper we propose PINlogger.js, which is a JavaScript-based side channel attack revealing user PINs on an Android mobile phone.  In this attack, once the user visits a website controlled by an attacker" - and it's important to understand for everything that follows, that's all you have to do.  No privilege, no popup, nothing special.  You just go to a web page, you know, not an app, just a web page, where somebody loads this JavaScript on your browser.



"The JavaScript code embedded in the page starts listening to the motion and orientation sensor streams without needing any permission from the user.  By analyzing these streams, it infers the user's PIN using an artificial neural network."  Well, yeah.  When I read that the first time, I thought, okay, not obviously a biological neural network.  Yes, it's artificial.  "Based on a  set of 50 four-digit PINs, PINlogger is able to correctly identify PINs in the first attempt with a success" - so they've narrowed the search space a bit to make it a little bit easier for them - "with a success rate of 82.96%, which increases to 96.23% and [then finally] 99.48%" - so almost 100% - "in the second and third attempts, respectively.  The high success rates of stealing user PINs on mobile devices via JavaScript indicate a serious threat to user security," they write.



"In the second part of the paper, we study users' perception of the risks associated with mobile phone sensors."  They write:  "We design user studies to measure the general familiarity with different sensors and their functionality, and to investigate how concerned users are about their PIN being stolen by an app that has access to each sensor.  Our results show that there is significant disparity between the actual and [the user] perceived levels of threat with regard to the compromise of the user's PIN.  We discuss how this observation, along with other factors, renders many academic and industry solutions ineffective in preventing such side channel attacks."



Okay.  So here's the technical background.  All of the most popular browsers - Chrome, Firefox, Safari, Opera, and Dolphin on the Android platform - support these functions and enable this PIN harvesting by default.  So any web page we visit can perpetrate this attack.  Geolocation data has previously been identified as a clear privacy concern, so modern web browsers ask their users' permission before returning this information to a web server.  And in fact I frequently now see, and I imagine our listeners have, our browser producing a popup when we visit a website, saying this website would like to know your location.  Which really annoys me.  I don't think I've ever said yes; you know?  And so typically you can say okay to allow, or cancel to decline.



But at this point in time, JavaScript code running in a web page does not require any similar permission to access a huge range of other sensor data, such as in this case device motion and orientation.  There is no notification while JavaScript is reading the sensor data stream, which allows browser-based attacks to be carried out covertly.



And our physics-aware users could imagine that, if you have essentially linear and rotational acceleration and position information, which is what our phones provide to, in a streaming fashion, an inquisitive web server, then as you tap the screen, the coordinates of your tap will produce a physical shift in the phone.  If you tap left of the center line, you'll tend to produce a twist.  And I don't know if that would be yaw or pitch or what in aeronautical terms.  But you'll rotate the phone around its long access.  If you tap high, you'll rotate the phone along its width access, you know, the horizontal access, and so forth.



And so you can imagine that, if you have a simple 10-digit pad or 12-position pad, that each one of those taps will produce a distinctive transient in the linear and rotational acceleration axes of the phone.  And that's picked up by the sensors and dutifully streamed back to a website that is monitoring your phone's instantaneous orientation.



So this is all based on, as many of these problems that we're seeing arise, a W3C specification that was, you know, I'm sure it was generated in the best of interests, except one has to wonder, this is very much like we were talking a few months back about the battery, the state of the battery charge and how, if that was a high-resolution feedback, as it initially was, and then when it was recognized to be a privacy threat, they chopped some least significant bits off of it so that it would be less of a fingerprint.  But the instantaneous charge on your battery was being reported and could be used to disambiguate you as you go from one website to the other.  And of course last week was all about the challenge or the drive to disambiguate users and our privacy interest in remaining anonymous and ambiguous as we move around the web.



So the W3C spec for motion and orientation sensor data provides the device orientation, the physical orientation of the device expressed as three rotation angles in the device's local coordinate frame, meaning relative to the device itself; its linear acceleration, in X, Y, and Z Cartesian coordinates, relative to its own physical orientation, that is, to its own hardware; the device's acceleration including gravity, so that's going to give you how it's being held, if it's being picked up, if it's lying horizontal, if it's on its edge and so forth; and then, finally, the rotation rate around all three axes, the instantaneous rotation rate.  So that's a wealth of information.  And these guys streamed that in from contemporary smartphones.  And with some limitation of the total search space, we're able to obtain nearly 100% guessing recognition just based on the information obtained from the user tapping on their screen.



So the question is, how available is this data?  Turns out it's way more available than we would like.  I mean, you'd probably want only the foreground tab that you are looking, I mean, if you wanted it at all, you'd want only the foreground tab that you were actively looking at on the foreground process, that is, your browser, while it has system focus, to be able to stream this data.



Unfortunately, in the case of Chrome and Dolphin on iOS, an inactive tab that includes the sensor listeners has access to the sensor measurements.  And in Safari it's worse.  Safari allows inactive tabs to access the sensor data even when Safari is minimized, and if the screen is locked.  So if you've visited a site - and, I mean, this gives me chills because pretty much I have the entire 'Net open in tabs under Firefox, but of course not on my phone.  But that means that a tab you haven't visited for weeks, but you didn't explicitly close, could be sitting there continuously monitoring the physical movement of your phone. 



And so of course we can extend this, too.  These guys managed to figure out where on the screen you were tapping.  But it could be useful for someone to know is the phone moving, to infer if you are in motion, if you are at rest.  If the phone rings, does it suddenly move as it gets picked up?  I mean, so you can imagine, again, we know that there are entities in the world that are actively interested in doing everything they can to penetrate privacy barriers.  So this makes that very easy.



In the second part of their study, they asked people, they surveyed people, like what was their awareness of the sensors in their phone?  They found 100% recognition of the camera and the touchscreen, which is not surprising because those are things that people most often explicitly and deliberately interact with; 97% recognition of microphone, Bluetooth, and GPS; 93% recognition of WiFi; 83% recognition of fingerprint.  And then in decreasing order of awareness was rotation, orientation, ambient light, motion, touch ID, device temperature, NFC, barometric pressure, ambient temperature, proximity, gravity, accelerometer, gyroscope, magnetic field, ambient humidity, ambient pressure, and then, lastly, the hall effect sensor, which is a magnetic field awareness.  So I guess the point is...



LEO:  How many people were aware of that?  I mean, I didn't even know that existed.



STEVE:  It was down in the way...



LEO:  .0001.



STEVE:  Yeah, it was.  But so here we have this dilemma.  The W3C is, with HTML5, is standardizing the access.  And all of the browser vendors want all the bullet points filled out on the comparison chart.  Like, oh, yeah, we have that; we have that; we have that.  So unfortunately, only something like our physical location, presumably GPS, although Firefox on my desktop is asking me where I am, so it doesn't have any GPS on this desktop machine.  So I'm not sure what that would be sending back, if I let it.  But I just always say no, you don't need to know where I am, some random site.  It's like, it's just being nosey.  It has nothing to do with location.



So I think it's worth noting that this stuff is being done.  And I'm so thankful that we've got researchers like this who are bringing this to light because, in the same way that recognizing that GPS was being fed back and then the browsers proactively said, okay, that's going a little too far, we're going to get affirmative permission from our users before we allow that to get sent back to a website requesting it.  It's not clear to me, I mean, I guess you could envision a scenario where a website might get some benefit from knowing the instantaneous positioning of the app.



I mean, well, for example, how about a marble race, you know, a web-based marble maze game, where you're tilting the phone, and the marble is rolling around.  Well, you could do that all in JavaScript, and it would need access to the phone's orientation sensors on the fly.  So that was probably - you could imagine that as being the justification for that.  But in that case, it wouldn't be cumbersome to say, you know, to just click a button and say, yes, I give this page my permission to know how I'm holding my phone.  Otherwise, it's very clear that this could be used.



The clear takeaway is, on Android devices that have this, and I would argue that iOS really needs to do this also, if they're streaming this as they apparently are, you really want a keypad that scrambles the digits.  It's not nearly as convenient because you can't memorize it with two spare neurons and never have to think about it every time you unlock your phone.  Of course, everybody's now using their fingerprint reader, which we're about to talk about.  But you definitely want a scrambled keypad option.  You know, turn that on if your phone offers it.  And if you don't have a fingerprint reader, or maybe even if you do, after you hear this next story about fingerprints, but you definitely want to make yourself look for the digits which are going to be randomly arranged on the pad surface every time so that nobody can use any of these techniques in order to figure out what you're doing.  Yeah, the hackers are clever.



Okay.  So it turns out some other research conducted by New York University and Michigan State University found to their chagrin that the fingerprint readers that we currently have in our smartphones are not nearly as robust as we want them to be and assumed they were.  I'll share just the first paragraph of their abstract, and then we'll discuss it.



They wrote:  "This paper investigates the security of partial fingerprint-based authentication systems, especially when multiple fingerprints of a user are enrolled.  A number of consumer electronic devices, such as smartphones, are beginning to incorporate fingerprint sensors for user authentication.  The sensors embedded in these devices are generally small, and the resulting images are therefore limited in size.



"To compensate for the limited size, these devices often acquire multiple partial impressions of a single fingerprint during enrollment to ensure that at least one of them will successfully match with the image later obtained from the user during authentication."  And of course all of us who have trained our fingerprint readers know that's the case.  We're told to, like, move our thumb around and position it differently and so forth, you know, and get the edges during successive reads.



"Further," they write, "in some cases, the user is allowed to enroll multiple fingers, and the impressions pertaining to multiple partial fingers are associated with the same identity (i.e., one user).  A user is said to be successfully authenticated if the partial fingerprint obtained during authentication matches any one of the stored templates."  And of course what we recognize is that would tend to reduce integrity of the search because we've broadened the search space.



"This paper investigates the possibility of generating a 'MasterPrint,' a synthetic or real partial fingerprint that serendipitously matches one or more of the stored templates for a significant number of users.  Our preliminary results on an optical fingerprint dataset and a capacitive fingerprint dataset, which is what we have on our phones, indicate that it is indeed possible to locate or generate partial fingerprints that can be used to impersonate a large number of users.  In this regard, we expose a potential vulnerability of partial fingerprint-based authentication systems" - which is what we're all using - "especially when multiple impressions are enrolled per finger.  In their simulations, the researchers were able to develop a set of artificial 'MasterPrints' that could match real prints similar to those used by phones as much as 65% of the time."



Okay.  So we all know, and Sherlock Holmes famously demonstrated, that a person's entire fingerprint is globally unique.  But think about this.  How unique is a much smaller portion of an entire fingerprint?  How unique can a smaller piece be?  And when I was trying to come up with a good analogy, I thought, okay, think of this as like having an ultra-secure 20-digit PIN, but only needing to provide any three successive digits from within that PIN.  When you think about it, I mean, it's obvious to us that that's not going to be secure.  But the 20-digit PIN looks, oh, wow, you know, no one's ever going to get that.



But, I mean, the analogy is that neither are our current technology capacitive fingerprint readers on our smart phones.  They're not looking at our whole fingerprint.  They're only getting a window of it.  And in fact that's why we're told to do multiple impressions and move our finger around in order to create a larger virtual image of the fingerprint.  And then, when we give it a partial print, it looks for it.



And so what's happened is we're seeing another instance where manufacturers' desire to minimize their users' inconvenience has prevailed.  And so what they chose was - the decision was we absolutely want to minimize false negatives.  We don't want to frustrate users by having them keep trying to unlock their device by putting their finger down and saying, no, we don't think that's you.  No, we'd better look again.  Oh, try it again.  Instead, it pretty much works the first time, every time.  But if we challenge how that's the case, it turns out that what we actually have is a tendency, through minimizing false negatives, to create false positives.



And after reading this study, it occurs to me that I've never attacked my own phone's fingerprint reader by asking the people I'm meeting to try unlocking my phone with their fingerprint.  And I'm going to start doing that.  Every time it occurs to me, I'm going to say, hey, I want to see if your fingerprint will unlock my phone.



LEO:  Oh.  Interesting test.



STEVE:  Yes.  And I want to extend this to our listeners, and I will look for any feedback because, I mean, it's not something we typically do.  I have no problem if somebody, if my best buddy or a server at a restaurant is successfully able to unlock my phone.  I would like to know that.  I'm not going to leave my phone with them again.



LEO:  Maybe not the server at the restaurant.  With friends.



STEVE:  Anyway, I'm really interested.  I'm going to obtain as large a population study as I can and get some actual feedback on how resistant my own iPhone 6s is to being unlocked by somebody else because this suggests that we may be getting a false sense of security from our fingerprint readers.  Yes, they always conveniently unlock for us.  But to what degree have we sacrificed their discrimination in order for it not to be frustrating?



And Leo, one thing I've noticed, because for a while I'm sometimes having my meal out on a cold patio, temperature seems to be the largest impediment to my iPad, actually, in this case, unlocking.  When my hands are cold, it doesn't like me nearly as much as when they're warm.  So I never really worked to figure out why that was the case, but I thought that was interesting.  That's the one thing I've noticed that seems to fool it.



LEO:  Hmm.



STEVE:  Okay.  So we've got a potential problem in all Linux kernels prior to the very recent v4.5.  And, boy, if you look at the - there's a link, I think it's the Mitre link, the second link in the show notes, Leo.  I think about halfway down they list the vulnerable kernels.  And it's, like, all of them.



LEO:  Oh, boy.



STEVE:  I mean, it's just like it goes - it scrolls and scrolls and scrolls and scrolls.  Okay.  So what was found - and this has been known for a while.  It's been kept quiet until it could be deployed.  If it's not - okay, so maybe it's the NIST link, the N-I-S-T link.



LEO:  Yeah.  I'll go back, yeah.



STEVE:  One of them scrolls, and it's just like, whoa.  Okay.  So what was found was a subtle problem that has long been present, like always been present in the Linux kernel.  It's in the UDP, the kernel's networking stack, the udp.c file.  And this is the worst of the worst, at least potentially.  It is a network remote code execution over UDP.



Now, what mitigates this maybe is that it is in an option of the way UDP datagrams are retrieved by userland code.  So Linux utilities running in user space need to be accepting UDP packets.  The normal way you do that is you just say, give me the next UDP packet.  You're able to either check to see if one's available, or you're able to wait for one.  And either way you then say, okay, thank you very much.  You provide a buffer, and the kernel copies the newly arrived packet into your buffer, and now you have it.



Okay, well, there is an option called MSG_PEEK, M-S-G underscore P-E-E-K, the MSG_PEEK flag.  You're able to set the MSG_PEEK flag which says, here's my buffer.  I want to peek at the most recent received, like next to be delivered UDP packet, but don't take it, meaning leave it available.  And so there are various reasons you might want to do that.  But they're uncommon.  That is, the point is that normally what you want to do is just give me the next packet.  Now give me the next one.  Now give me the next one.  You don't often want to say "I want to peek at it, but leave it there."



So at the same time, a string scan of the Linux source tree reveals hundreds of instances where the MSG_PEEK flag is included in a UDP API call.  So somebody's doing them, for some reason, a lot.  I saw pages of them in the glibc library, and I didn't go any further.  So it is considered a critical vulnerability network attack, easy to do, meaning low complexity.  No privileges required.  No user interaction required.  So potentially very worrisome.  Google has it fixed in their April 17th Android security bulletin.  It affects their Pixel, the Pixel XL, and the Pixel C; the Nexus Player,  the Nexus 5X, 6, 6P and 9; the Android One and Android.  And of course, as we know, it also affects all other previous Android non-Google phones that may never get patched.



What we don't yet know, and there's nothing that I could find, I spent some time digging around, I couldn't find any clear definitive example of an exploit.  There was some mention in one form of wget using it.  But wget is normally used for fetching files over TLS or just HTTP or TCP connection, not so much over UDP.  There is DTLS, which is the TLS over UDP.  As we know, TLS normally runs over TCP, but there is a way to run TLS over UDP called DTLS.  And that's present in OpenSSL.  But again, you need to have a userland app doing this, and then an attacker able to deliberately malform a packet which a vulnerable version of Linux, which they all are, could then use.



So it is a critical remote code execution.  Everybody's running around.  And of course here's the problem is that there are, as we know, many Linux-based devices which who knows when they're going to get fixed.  We've got smartphones, non-Google Android devices, routers, televisions, DVRs, higher end IoT devices of all kinds.  They've all got versions of Linux where this is vulnerable.  So while there isn't any, like, we don't know of a zero-day event involving this.  We have a patch for the kernel.  But there is zero doubt that would-be attackers are right now carefully examining every instance of Linux's user-space code where the MSG_PEEK option is used.



And again, a simple string search through the source tree finds hundreds of them.  And bad guys are looking for a way, some way to leverage this.  So hopefully routers don't have UDP exposed.  Of course, if they've got Universal Plug and Play exposed, we'll be talking that in a minute or two, then that is a UDP-hosted protocol, so that would be a problem.  So my sense is we're probably going to be talking about this in the future and in a worrisome fashion.  I mean, maybe it's the case that this option is in fact obscure enough; that the collision incidence, the likelihood of it actually being exploitable is rare.



But there's no, I mean, this is just - this is nirvana for attackers because, unfortunately, our ecosystem today has got a ton of Linux-embedded OS, all of which are presently vulnerable, and few of which are being dynamically updated.  And if there's an instance where the MSG_PEEK was used in a common fashion, and I haven't seen any indication of that yet, but that would allow potentially - in the worst case it would crash or cause a kernel fault or a reboot.  I mean, and as we know, that's the way you begin.  And then afterwards you figure out how to execute code that you have also supplied in that same UDP packet.



So clearly the takeaway is, well, first of all, somebody would need to be able to send your Linux machine a UDP packet.  If they are able to do that already, you've got problems because you shouldn't have UDP, I just don't see a good reason for having UDP exposed.  Now, maybe, if you had like an OpenVPN server, and you do want UDP protocol to be used as the carrier for your VPN tunnel because that's far more effective than using TCP.  But the question would be does OpenVPN ever do a MSG_PEEK on the UDP traffic it's receiving?  Those are the kinds of things we'll be finding out in the future.



So I would say the typical user, behind a router that itself doesn't have any UDP ports open, is probably okay because it does require a bad guy to get a UDP packet onto your kernel network stack and for it to be serviced by an application which is using MSG_PEEK in this way, and for the packet to be an exploit.  Again, it's such a juicy opportunity.  I'm afraid we haven't heard the last of this one.



So John Gruber is a well-known blogger.  He has the Daring Fireball blog.  And one of our listeners shot a note to me following from some of my comments about the Google search.  And John's posting, he said about DirectLinks:  "Safari Extension That Circumvents Google and Facebook Link Redirects."  And he says:  "Great little Safari extension from [looks like] Canisbos:  The extension circumvents certain techniques used by Google and Facebook to track link clicks."  He writes:  "When you click a link in Google search results, Google" - and this is what I did not know - "Google uses JavaScript to replace the actual link with an indirect one, which they use for click tracking.  Google then redirects the browser to the actual destination after logging the click.  DirectLinks" - which is the name of the Safari extension - "disables the JavaScript that replaces real links with indirect ones, so that when you click a search result link, Safari goes straight to the destination.



"If you've ever tried dragging and dropping a URL from Google search results and getting a Google redirection URL instead of the actual URL you wanted," and he says, parens, "(and Google's JavaScript will show the actual URL in the status field if you hover over the link so it's impossible to tell that's what's going to happen), this extension," he writes, "is for you.  There are obvious privacy benefits, as well."



Okay.  So the idea, I just assumed that Google was delivering a page to me that had their redirect links in the body of the page, rather than JavaScript doing it on the fly.  So I thought, oh.



LEO:  Well, think of it as like a URL shortener; right?  It's very similar to that.  That way, I mean, why would they modify the resulting - you don't want to modify, like if go to TWiT.tv, you don't want to modify my TWiT.tv page.  You just have a redirect.  They've always done that.  And, by the way, that's how they monetize.  And if you like Firefox, that's what pays for Firefox because Google then can give Firefox money.  So while you can do this, gosh, it seems - all right.



STEVE:  Okay.  So I'm not sure we're...



LEO:  If you're worried about your privacy, I understand that.  Well, no, if you look, if you click a link on a Google search page, it's not a direct link to that page.  It's a link to some JavaScript, some tracking JavaScript; right?  That's what they're talking about.



STEVE:  No.



LEO:  Oh, I misunderstood.



STEVE:  JavaScript on the page rewrites that link on the fly.  So, and I had talked about this a couple weeks ago, where for example, like when I'm assembling notes or wanting to send someone a link, I'll right-click on the link, and even though it shows me the search result, I end up with this ridiculously long gibberish.



LEO:  Right.



STEVE:  That comes from Google.  Well, it turns out...



LEO:  And that's why, that's how they track it.



STEVE:  Correct.  So they're tracking what links I'm clicking on in the search results they provide.



LEO:  Right.



STEVE:  And in the process, they're obscuring the link.  That is, I'm unable to right-click and save the link.  Anyway, so the point is that, for our listeners, if not for you, Leo, it is possible to trivially disable this for all browsers because uBlock Origin is, as we remember, is a very facile web filtering system.  So I have uBlock Origin running, and it's sort of a "set it and forget it" tool.  But so when I realized from John's column that this was JavaScript, I thought, oh, that's interesting.  It would be much more convenient for me to have real links in Google search results, rather than redirect links.



So, and I have here a picture of the screen in the show notes, where because the UI, I had completely forgotten how to use this thing.  I mean, we talked about Gorhill, who's the author of uBlock Origin.  In fact, if anyone is interested, it is Security Now! Episode 523, which we recorded on the first of September, 2015, titled "uBlock Origin."  And so in the advanced view, where it gives you a lot of granularity, there are these two, believe it or not, unlabeled columns.  And again, this is not friendly.  But it turns out that, if you click on the right-hand column, which is the per-site rules, versus the left-hand column, which is the global rules, you click in the right column for first-party and third-party scripts and just turn them red and then refresh the page.  Everything works, except you have real links rather than links where Google tracks what links you follow from their search results.



So for anyone who's interested, Safari's got the extension that John Gruber mentioned.  But anyone using uBlock Origin on Firefox or Chrome can easily just say I want to turn off scripts in Google.  Now, this is per site.  So, for example, that's www.google.com.  With that in place, I was then using drive.google.com, and any other property that doesn't collide with www.google.com will be okay.  This may be too broad because this is going to kill those scripts on anything that matches www.google.com.  Whereas presumably the Safari extension is more specific.



But I just thought it was interesting.  I didn't realize that JavaScript was embellishing these links, and that just by saying no to JavaScript on that page, I mean, you know, we used to run with NoScript on all the time.  And so in fact maybe that's why I was never seeing this before is that I never had script running on Google, and I was getting physical links rather than redirect links.  And I just didn't appreciate the difference until I switched over.  And actually I gave up on running NoScript because too many things need it these days.



So we have another dump from the seemingly bottomless pit of the NSA WikiLeaks Vault 7.  And this one generated another round of breathless press coverage last Friday on April 14th, mainly focusing on something called EternalBlue.  An example of the press coverage was one site that said:  "The Shadow Brokers, an entity previously confirmed to have leaked authentic malware used by the NSA to attack computers around the world, today released another cache of what appears to be extremely potent, and previously unknown, software capable of breaking into systems running Windows.  The software could give," they write, "nearly anyone with sufficient technical knowledge the ability to wreak havoc on millions of Microsoft users."



And they continue:  "The leak includes a litany of typically codenamed software implants with names like OddJob, ZippyBeer, and EsteemAudit, capable of breaking into and in some cases seizing control of computers running versions of the Windows operating system earlier than the most recent Windows 10."  And then they conclude:  "The vulnerable Windows versions ran more than 65% of desktop computers surfing the web last month" - and I'll note, uh-huh, but not this month - "according to estimates from the tracking firm Net Market Share."



So the good news is, as is so often the case, I mean, what we've always seen, both in the original Snowden leaks of the CIA documents and now here, many of these things are very old.  And so many of them, because they're just sort of a repository trove, which could be useful if they're encountering somebody, if law enforcement or intelligence is encountering someone with an old, long not updated Windows machine that hasn't been fixed, then some of these things could still apply.  But Microsoft apparently had access to this before its disclosure on Friday.



And so the March update - which remember that February got canceled.  First it was postponed, then it was canceled.  So that big March update, which is MS17-010, that fixed these things last month.  So anybody who has been keeping their machines updated and is using a Windows which is still receiving updates, already has these things fixed.  So there was one called EternalBlue which was fixed last month, as was EternalRomance and EternalSynergy.  And I sorted these in order of when they were fixed.



There was one called EskimoRoll which got fixed in 2014.  EmeraldThread got fixed in 2010, running backwards.  EducatedScholar got fixed in 2009, and Eclipsed Wing in 2008.  So Microsoft's on top of this.  They did release a TechNet blog posting where they explained, of what was released, when these were fixed.  And so even the worst were all fixed last month, and many fixed years ago.



So this is sort of typical of what we see being disclosed.  And while, yes, if this had been released in January, and Microsoft hadn't released a fix, then this would have been a problem.  But one way or another, Microsoft did fix these things, even the most recent of them, in the March patch cycle.  So as long as your machines are caught up to date, or are up to date, you're probably not in bad shape.



Okay.  So we're all familiar with ASCII, the American Standard Code for Information Interchange, which is an eight-bit code of which really seven tend to be used, which expresses upper and lowercase, the Latin character set, the digits and the special characters.  Basically, what we type on our - what an English speaker types on a standard keyboard is the ASCII character set.  But as we know, there are way more characters than 127, which is what you're able to get with seven bits of binary, excluding zero.  So Unicode was created, which is a multi-byte encoding which, for example, well, which is a huge encoding space.  Now we've got 16 bits, which is, as we know, 64K possible characters.  And so it's got everything in there.  It's got all the different languages, and now it's got emojis and all kinds of other stuff.  



Well, the question that browser vendors faced was, okay, the transport, the traditional transport for our communications is a byte.  It's not a word.  That is, it's eight bits of which we really only use seven.  It's not 16.  So how do we - and this, of course, is also the case for the domain name system.  The whole DNS system is oriented toward non-Unicode characters.  So how can we have a domain name which has got multilingual characters, accented characters, the things that non-English speakers have every right and reason to want.  They want the accent aigu to show on their name if their name has one, or if their domain does.



So this weird encoding was created, and the name is a play on Unicode.  So we change that to "unycode," and then we change it to "punycode," P-U-N-Y-C-O-D-E.  And it is a way of encoding in ASCII, that is, in seven-bit printable standard ASCII, to encode Unicode characters.  And there are a number of examples.  Now, I couldn't remember - so I've been aware of this problem for a while.  And I don't know, it apparently only came to light and got a lot of attention in January.  But it feels to me like I've been aware of this as a problem for a lot longer than that.  So it may well have been that it just received attention.



So, but here's the point.  It is possible to leverage punycode to create domains which, when displayed by our browsers that understand punycode, look like different domains.  And in fact on Firefox today, no other browser is currently still vulnerable.  Chrome was; but Google, because they have the ability to immediately fix things, mine this morning was already fixed.  Chrome 59 fixes this.  In the show notes I have links that you can play with to some of the coverage.  There's a great link with full coverage of the problem.



The only people today who really need to worry are Firefox users because Mozilla apparently, for reasons I don't understand, is still on the fence about what they want to do with this.  But the good news is there's a switch which, in that about:config monster list, that anyone can set to true.  And I would argue everyone should.  Although now Chrome is fixed.  IE, Edge, and Safari have not been vulnerable.  That means that it's only Firefox users that would be fooled.



So here's the danger.  In this demo - I have a link in my show notes.  It's HTTPS, and that's a key, https://www.  And then this punycode is all lowercase, xn--, and then 80ak6aa92e.com.  If you put that into Firefox - again, https://www.xn--80ak6aa92e.com, where you get taken is a site that is not Apple.com, but your URL says https://, it's got the green padlock, everybody's happy, and the domain that's displayed is www.apple.com.  Because that punycode is Unicode encoding for the same characters, Apple.com.  And so somebody registered the punycode domain, that is, the guy that really brought this to everyone's attention registered the punycode domain equivalent in Unicode for Apple.com and put up a site there.  And when you go to the crazy punycode link, it shows you Apple.com.



So clearly, had this person had malicious intent, he could have sent email where the non-visible URL was this punycode, you know, looking like it was email from Apple telling you to do something.  You would click on the link.  You would go to what absolutely looked like Apple.com with a valid TLS certificate showing the happy green padlock, everything looking perfect.  You'd inspect the URL:  www.apple.com.  Absolutely, you know you're at the right place.  Except you wouldn't be.



Now, if you - let's see.  I didn't think to look at the certificate.  I should have looked at...



LEO:  It reflects the "xn" site, not Apple.com. 



STEVE:  Okay, good.  Right.  It would have to, yes, good.  So, okay.  So nobody needs to worry about this.  So this was a very potent phishing attack because any inspection, any careful inspection would show that you're exactly where you think you should be, yet you would not be.  And short of looking at the certificate, there would be no way to know.



Firefox is vulnerable to this now.  So this works now under Firefox.  If you go to about:config, which brings up a bazillion options, and put into the search bar, which is the only way to find anything among those options, just enter "puny."  There's only one item that matches that string.  And that's network.idn_show_punycode.  And you want to change that to true to show the punycode, rather than have Firefox conveniently show you the Unicode.  You don't want the Unicode.  You want the actual ASCII punycode.



And so once you do that - and in the show notes I've got two links you can use to verify, to test and verify that your Firefox is no longer going to be confused.  One of them is the Apple.com, and another one is Epic.com, which is another site that was set up in order to demonstrate how spoofable this feature of browsers of turning punycode into Unicode could be, and how it could be leveraged.  So this is, again, just like, whoops, a nice feature, but clearly subject to abuse.  And luckily we got this fixed, hopefully before anybody got caught out.



This is real quick.  There was an interesting article by Joseph Cox, writing for Motherboard under the title:  "Your Government's Hacking Tools Are Not Safe."  And this follows from what we've been talking about.  He wrote:  "From Cellebrite to Shadow Brokers to the CIA dump, so many recent data breaches have shown there is a real risk of exposure of government hacking tools.  The hackers will get hacked."



He writes:  "Recent data breaches have made it startlingly clear hacking tools used by governments really are at risk of being exposed.  The actual value of the information included in each of these dumps varies, and some may not be all that helpful in and of themselves, but they still highlight a key point:  Hackers or other third parties can obtain powerful tools of cyberespionage that are supposedly secure.  And in most cases, the government does not appear to clean up the fallout, leaving the exploits open to be reused by scammers, criminals, and anyone else, for any purpose."



So as we know, this was exactly my own takeaway a month or two ago when we first covered these Vault 7 leaks because we've seen a clear pattern that our intelligence and law enforcement agencies are, despite their best efforts, unable to keep their own secrets.  And I do not blame them, per se, because no large human-based organization can perfectly keep secrets.  We just don't have the technology for doing that today.  All of the evidence continues to demonstrate that this is true.  And the CIA, the NSA, and FBI are not exceptions.



And this, of course, as I have said, in turn argues against any form of a mono-key technology that would be legislated for access to encrypted communications.  I expect that companies will likely be required to modify their encryption technologies on a company-by-company basis, as they comply with legally issued court orders in order not to be held in contempt of court and probably heavily fined.  But I think it's crucial that we maintain a heterogeneous encryption environment and not force all companies to provide the equivalent of a universal generic encryption backdoor, which would allow law enforcement to unilaterally obtain access to virtually, well, all otherwise lawfully encrypted data.



So that's the takeaway that I hope our listeners and legislators will get.  I mean, they have strong, you know, those who would argue against letting law enforcement have what they want here have, I think, a strong weapon in their arsenal now because we keep seeing instances where our own intelligence services are unable to keep their secrets safe.  And again, I don't blame them.  They're large.  They're bureaucratic.  They've got a ton of people.  To do the job, lots of people have to have access to these tools.  That's going to happen.  But that also says we need to distribute the danger and not concentrate it into any kind of a single golden key, as James Comey famously said he wanted the FBI to be given.



A little more than four years ago, back in January of 2013, researchers from DefenseCode responsibly revealed the existence of a remote root access vulnerability in the default installation of some, at the time, Cisco Linksys routers.  Our podcast listeners who have been with us for a little more than four years will recall that I immediately responded, because this is bad, I immediately responded by adding a test to ShieldsUP!, which is there to this day, been there for four years.  And today that test for publicly exposed Universal Plug and Play service has found 51,854 positive exposed results.



And the ShieldsUP! system has always maintained a 1024-deep, a 1024 IP-deep most recently used list, an MRU list, specifically to avoid redundantly multi-counting repeated tests.  So if somebody uses ShieldsUP! multiple times, or if somebody uses the Universal Plug and Play, as long as their public IP is still within the most recent 1024 IPs, which probably keeps it there for several days, I'm only counting them once.  So that 51,854 is a pretty good number.  It doesn't mean that the same person coming back month after month wouldn't get multiple counted.  But on the other hand, that's probably what we want.  We want to see that there's still that device which is exposed.



So I have a link in the show notes here to the whitepaper that DefenseCode just published.  They waited four years.  And anyone would have to agree that that's plenty of time.  What we now have is full disclosure.  And Leo, this may have been what - I'm sure I remember seeing all those Linux versions.  But go halfway down, and you'll run across pretty much the who's who of routers.  I don't know if there's any router not on that list.  It's just amazingly comprehensive.  And I think at the time we discovered, if I'm recalling correctly, that it was some demo code, some demonstration code in Intel's UPnP source which was never meant to be deployed; but it was just like, okay, here's a sample of how you would do Universal Plug and Play.  And they just - the router vendors put it into their firmware and used it.  And it was like, okay, wait.  We never meant for that to be used in production.



Okay.  So just to back up a little bit, remember that Universal Plug and Play is the protocol which Microsoft and Intel together created to allow sort of zero-configuration hole-punching through NAT routers to allow devices in the network to create a public presence, even though they're behind a NAT router.  Because they wanted it to be literally plug-and-play, meaning it just works, there's no password, there's no authentication, there's nothing you have to do.



So this is part of the problem with this protocol.  I mean, it achieves what it wants, but it's dangerous.  At the same time, it was never meant to be bound to the public WAN interface.  It was only meant to be a LAN-facing protocol that would allow clients in the LAN to perform some autonomous configuration of the router.  For example, the Xbox is a big user of this.  Suddenly you're just on an Xbox network, and you didn't have to do anything because the Xbox used the UPnP on your router in order to map a bunch of ports from the public Internet onto itself. 



Okay.  But that's the service facing the LAN that does that.  It should never be exposed.  So what we have today is the complete document on the vulnerability.  And Leo, for the last...



LEO:  I've been scrolling.  I'm only through the N's.



STEVE:  Exactly.



LEO:  By the way, I didn't know there was a - oh, it's not Nerf, it's Neuf.  I thought there was a Nerf router.  Yeah, it's amazing.



STEVE:  Again, there's, like, nothing that didn't make the list.  So the takeaway for our listeners is absolutely positively make sure - and I guess this would apply to people who didn't hear the podcast on January of 2013.  So, and I don't have the podcast number here.  But if you, for example, if you Google "GRC UPnP," "Security Now! UPnP," you'll find it [SN-387].  So anyone since then may not have heard that you absolutely need to turn off Universal Plug and Play on the WAN interface.  If it's on, you probably have the ability to turn it off.  If not, then you want to make sure you're running the latest firmware for your router, which is always a good idea, and it will probably have it shut down.  Cisco dragged their heels, back when it was announced.  First they said it had already been patched, but then it turns out it hadn't been.  And then they ran around and created a series of Linksys firmware updates.  And we covered all this back at the time it was happening.



But now the world knows exactly what the remote root access vulnerability is.  And any devices which are still to this day vulnerable - and one thing I don't have is a graph over time.  I could create one if it was important because I have a log, an anonymized log, of every event where a positive result was found.  So, for example, I could create a graph showing, if there was, a decrease in the amount of vulnerabilities found for tests taken.



Anyway, the point is GRC.com, ShieldsUP!, right on that front page, after you click through the entry page, is a big orange-looking icon thing, which we put up immediately four years ago.  Click on that.  We'll tell you whether your router has Universal Plug and Play exposed.  And nearly 52,000 people have had this thing say, yes, you're exposed.  Hopefully they then figured out how to turn that off on the WAN side, and then they tested again, and it said, nope, you're no longer exposed.  The bad guys now know, they now have every detail about this as a consequence of DefenseCode's full disclosure.  They waited four years.  Nobody, I think, could say that was too soon.  But we do need to make sure, those who are security conscious, that you've got Universal Plug and Play turned off, especially now.



Some guys at Princeton, three guys at Princeton and one person at Stanford, actually Jonathan Mayer, who we haven't heard much from recently, published a paper titled "The Future of Ad Blocking:  An Analytical Framework and New Techniques."  It's long and verbose, 17 pages of academic review of the legal and technical history and foundation of adblocking, the fact that a sponsor tag is supposed to be placed on ads, they say, making it clear whether inserts on pages are ads or not.



And they developed a theoretical add-on which defeats the adblocker blockers.  And it's sort of obvious, but they took the time to do it.  This doesn't solve some of the problems that an adblocker does in that their solution doesn't trigger adblocking awareness on the site you visit because it still fetches the content.  Yet when it sees that it is an ad, and they have some heuristics that they use in order to make that determination, they alter the document object model to put a translucent overlay over the ad and label it as an ad.



So you're able - so your browser still fetches the ads.  But they're sort of toned down.  Their contrast is reduced by being placed underneath a translucent white cover so that you can sort of see through it if you want to.  And in their page they demonstrate some examples of this.  You can see through it if you want to, but it's not nearly as much in your face as would otherwise be.  And the sites are still generating the revenue from the ads.  The users are being less assaulted by them.  There is of course the problem that malvertising still could be present, and the advertising tracking that some people object to would still occur.  But it visually tones them down.



So it's not clear to me what's going to go, you know, what's going to happen.  They did develop a - I don't remember if it was Chrome or Safari.  I think it might have been a Chrome plugin.  I think it was a Chrome extension which highlights in a red box, it doesn't do this translucent fading, it just draws a big red frame around everything that they think is sponsored content, to demonstrate the technology without actually being as active as they would propose their system could be, if that's what you wanted.  My sense is, you know, every so often as I'm surfing around, I've got uBlock Origin on, as I mentioned before.  And the pages I visit are - they seem to have ads on them, but they're not going crazy.  So maybe it's using that ad network, the EasyList, where there's been some compromise made about how assaultive the ads are going to be visually.



Every so often I'll run across a page that brings up a notice and says, oh, you're using an adblocker.  If you'd like to use our content, please turn it off.  And I go, okay.  I turn it off, and then I use the page.  I think that's, at this point, a reasonable compromise.  And one can imagine that a hybrid of this visual blocking technology could be adopted where you normally run with an adblocker on.  But if a site objects to that, then you visually block the ads, but still pull the data and have your browser sort of semi-display the ads, but not in a way that is too obtrusive.  So an interesting compromise.



John Schneider sent me a tweet asking a question that comes up a lot, so I wanted to handle that before we get into the rest, the second part of this, which is the feedback from last week's Proactive Privacy podcast.  He said:  "Steve, would you please address the pros and cons of hiding SSIDs on home routers in a future SN podcast?"  He says:  "iOS 10 warns this is a bad thing."



So again, I get this, people are asking the question all the time.  And Leo, I'll bet on the weekend on The Tech Guy show you're probably having security-conscious listeners asking, too.  We've talked about this in the past and how it is mostly cosmetic, and it does not provide much security.  Turning off the SSID broadcast from your router prevents its automatic discovery and listing in anybody's list of available access points who is within range of your router.  So all your neighbors, you know, you and all of your devices won't see it, and none of your neighbors will see it.  That is, in that list, in the presented list of, you know, these are access points you could use.



But with that comes an inconvenience and actually some - not much actual gain in security, and some behavior you may not be aware of.  First of all, the inconvenience is that you must then manually tell your computer, here is the SSID that I want, and the password.  So you have to manually give it that information because you can't select it from that list because it's not in the list.  Also, many systems will then not remember that.  That is, you know, if you have the "automatically connect to routers I have previously connected to" option, which is very convenient if you routinely move between various locations, that feature disappears, and you're forced to manually reassociate, essentially, with the access point every time.  So all OSes - okay.  So there's that.



But then all OSes that connect automatically broadcast the SSIDs, whether hidden or not, of all the networks they have connected to in the past.  So while the non-broadcast access point's SSID is not being broadcast to anyone new, it will then be broadcast by all devices that once connected to it.  So they're blabbing this hidden SSID, even if the access point isn't.  And all over-the-air traffic contains that non-broadcast SSID.  So anyone actively sniffing the radio traffic will still see the access point's SSID if any device is currently associated with that access point.



So typically you've got all kinds of IoT things in your home that are associated.  Well, that means that anyone actually looking at, you know, sniffing Channel 11, which is probably what your system is using, on your devices, but they can of course also see whatever channel you're looking at.  If they're promiscuously sniffing your WiFi traffic, they can look at the packets and discover the SSID that everybody's using.



So I guess, given somebody who was fully up to speed about what's actually going on, you can make a decision of whether it makes sense for whatever use case you have of hiding the SSID, that that's a good thing to do or not.  But for what it's worth, it's really - it certainly isn't actually keeping anybody from discovering, if they want to, that your access point is there and what its SSID is.  And as your devices roam, they are broadcasting that hidden SSID all the time because they once connected to it, and they're still looking to see if it might be around for them to reconnect to.



And a couple little bits of miscellany.  I just wanted to note, I got a kick out of this, Leo, and I don't know if you had covered it over the weekend, about the Burger King Whopper commercial.



LEO:  Yeah.  Isn't that funny?  Yeah.



STEVE:  Yeah.  I loved it.  I thought it was very clever.  And I don't think it was malicious.  For our listeners who don't know, at the end of a Burger King commercial they deliberately said what I cannot say on the air, which is the trigger phrase for the Google device, followed by "What is the Whopper burger?"  Which they deliberately put that at the end of the commercial, which then triggered all of the Google devices that were listening, that were within earshot of the commercial, to elaborate in a detailed description of the composition of Burger King's Whopper.  Which I thought, again, was just very clever.  But Google didn't think it was so funny, and they immediately blocked it.



LEO:  Also, in the interim, there was a little problem with people changing the Wikipedia entry that came up and saying that the Whopper was made of toenail clippings and - so it turned out maybe not to be the best idea ever.



STEVE:  Oh, good.  I'm glad you were able to provide that additional information.



LEO:  Yeah, yeah, the perfect idea.



STEVE:  Whoopsie.  Whoopsie.  So we have, from our continuing series on epically cool ways to waste time, we have, believe it or not, generating sequences of primes in Conway's Game of Life.



LEO:  Oh, boy.  Oh, boy.



STEVE:  Yes.  Nathaniel Johnston, an assistant professor at Mount Allison University in Sackville, New Brunswick, Canada, posted - and I have a link in the show notes for anyone who's curious - just a beautiful piece of work.  He wrote:  "One of the most interesting patterns that has ever been constructed in Conway's Game of Life is primer, a gun that fires lightweight spaceships that represent exactly the prime numbers."



LEO:  What?



STEVE:  I know.  Just amazing.  "It was constructed by Dean Hickerson way back in 1991, yet arguably no pattern since then has been constructed that's as interesting.  It seems somewhat counterintuitive at first that the prime numbers, which seem somehow random or unpredictable, could be generated by this relatively simple pattern in the completely deterministic Game of Life."



So here's how it works.  "The gun works by firing lightweight spaceships" - now, our listeners will remember that a glider is the smallest object.  It's five live cells, which sort of flips back and forth and runs diagonally - I don't remember.  I think it's at one - is it a quarter or a half lightspeed?  Okay.  So that's a glider.  There's a lightweight spaceship which moves more slowly, but is able to move horizontally.  So this gun works "by firing lightweight spaceships westward and destroying them via glider guns that emulate the Sieve of Eratosthenes."



And of course remember that the sieve is one where you take all numbers, and then, if you imagine them in a grid, you drop out the even ones, then you drop out the third ones, then you drop out every, well, every fourth you've got because you dropped out the evens.  Then every fifth one.  And so you essentially drop out those that are multiples of primes, and you end up with just prime numbers left behind.  So that's the way the sieve works.  This Dean Hickerson figured out how to implement that in Life, Conway's Life, by essentially launching all possible lightweight spaceships and then arranging to kill them with intersecting gliders which emulate the sieve.



And he says:  "A lightweight spaceship makes it past the left edge of the gun at generation 120N, if and only if N is a prime number."  Anyway, and in the picture you can see a stream of them.  Essentially they're spaced out so they represent a stream of primes where they are like three and five and seven and eleven and so forth, where those represent - and the sieve has knocked out the ones which are not prime in between.  Just incredibly cool.



Oh, and then someone tweeted:  "Steve, I've been watching your show for years.  I know you love Kindle.  Which one do you own?  How do you prefer to read your eBooks?"  And I was reaching down.  I had mentioned before how I am really loving most reading.  And this is the way I have my screen set up, Leo.  It's an iPad mini.  And I have it - so I use the Kindle software on the iPad Mini, set to inverse so it's white text on a black background.  And then I turn on the night shift and crank it to its maximum orangeness, which creates what I find to be just a really pleasant, sort of a neon orange glow, very easy on the eyes and not glare-y at all.  So I use the iPad Mini when I'm indoors, and then a traditional eInk Kindle when I'm outside.



LEO:  Which eInk Kindle do you use, though?  I think that's what he's asking.



STEVE:  I'm using the latest one.  I bit the bullet, even though it's ungodly expensive, and I already have other Kindles.  But it's - it don't know if you've seen that thing.



LEO:  Yeah, like 200-plus bucks, yeah.  It's crazy.



STEVE:  Oh, it's way - and of course I didn't want ads in it, so I had to pay more for that.  But it's almost a little square form factor.  The cover has, like, three quarters of the battery, and it's removable.  And when you remove the cover, it's incredibly thin, except sort of where you hold it, where you have a couple buttons.  And the feature that annoys me is that I don't know why they don't do this, I saw so many people complaining that it's a touchscreen and has physical buttons, but you cannot turn off the screen change, the page change by touch.  And I'm often, my hand just sort of wanders, and it touches the screen, and it turns when I don't want it to, even though I've got physical buttons.  I don't understand how Amazon can not let us have the option of turning off the touch page turn.



LEO:  Oh, you got the Oasis.  You got the $300 one.  



STEVE:  That's the one, yes.



LEO:  [Vocalizing]



STEVE:  It's crazy expensive.



LEO:  Yeah.



STEVE:  But look how skinny it is.  Oh, Leo, it is really nice.  But it will hurt you to buy it.  And of course it is...



LEO:  I have a Paperwhite.  I'm very happy with the Paperwhite.  I have a Voyager, as well, or a Voyage, I guess it's called.



STEVE:  Yeah, I've got all those, too.



LEO:  Yeah.  I use the Voyage, which is actually not cheap.  It's still 200 bucks.



STEVE:  Yeah, I know.  They are pricey. 



LEO:  When they got to 300 pixels per inch, that's when I said, all right, I have to buy that.



STEVE:  Oh, it is, it is a beautiful screen.



LEO:  That's a nice display, yeah.



STEVE:  It really is.  And lastly, Chris Hall tweeted on Saturday the 15th.  He said:  "Ryk Brown's Frontiers Saga is amazing."  He says:  "On third book since Tuesday."  So he learned of it from the podcast last week.  He said:  "Highly recommend this to any Star Trek fan hungry for more."  And I'm not going to drag our listeners through this again.  I just want to say I'm now - I had thought I was going to wait for the second series to be written before I started, but I couldn't.  I have finished the second book of the second series.  So technically that's Book 17.  I'm into 18.



Leo, it may be the best sci-fi series I have ever read.  And I say that with full recognition of Peter Hamilton's work, and all the other authors we've talked about in the past.  I love their stuff.  Many of theirs I would argue is almost more kind of classic sci-fi because it's imaginative.  I mean, Peter Hamilton with his wormholes that you run trains through, and the Moties on Pandora's Star.  Just, I mean, there's, like, really new stuff.  But Chris's comment about this being like Star Trek.  Remember how I said that my company once as a birthday present gave me the writer's kit for Star Trek episodes, and how I learned from reading that that the goal was not to do gee whiz technology.  It was to tell human drama set in that future context.  And so, yes, warp drive and phasers factored in.  But the stories weren't about warp drive and phasers.



This Frontiers Saga is like that.  The thing that makes it so compelling is the characterization is so good.  The writing is topnotch.  You end up really caring about these people.  And when you start into the second set, it develops this Star Wars-like mythology stuff, kind of with an Obi-Wan and a Luke and kind of other whispers of that.  It's just - I'm just having the most fun ever.  So I just did want to say I think - someone sent me a tweet saying that the first book, which technically it's big, it's the first three of his what he calls "episodes," where it's 15 episodes per series.  He said it was free on Amazon.  When I looked, it wasn't.  But I think for the Kindle it's like three bucks.  It's three something, 3.15 or 3.53 or something.  It's less than $4.  And, boy, if you're curious, by the end of the third episode, by the end of your $3 and whatever cents, you'll know if you want more.  I just - it's just fabulous.



LEO:  And all 15 volumes are the same people and all that?



STEVE:  Yes, yes.  It is one coherent storyline.  And I'm realizing now he must have mapped this whole thing out.  And, oh, I mean, I just - I can't spoil it for anybody.  But, I mean, it's just - it's really compelling.  When I was thinking about like the - what was that last Hamilton, the road wandering off somewhere?  I can't remember even what the title was [The Great North Road].  You know, I dragged myself through.  And again...



LEO:  It was okay, yeah.



STEVE:  New things, interesting ideas.



LEO:  It's one of his mysteries, so it's a [crosstalk] mystery.



STEVE:  Yeah, yeah.  This is, again, there is technology.  There's very clever use of weapons.  There's no, like, magic power where people get to do anything they want, so they have to live within a constrained universe, which of course is what I think makes the kind of puzzles I like the best is where it's like what you do with these constraints.  It's just there's cleverness, but it's about people.  It's about this group of people and really compelling bad guys, too.  So you want to - I'm just like [vocalizing], I hope justice gets served.



LEO:  If you want to put it on the Voyage, six bucks for the first three, that's a pretty good deal; right?



STEVE:  Oh, Leo.  And is Audible available?



LEO:  Yes, yes.



STEVE:  I saw some reference to it.  Okay.



LEO:  Yes.  It's also available on Audible.  Not nearly as good a deal.  They don't have the first three for one price.  So you're buying them all, one by one.



STEVE:  Yeah, so again...



LEO:  I might read it first and see.



STEVE:  I would say - I bought them all for my nephew, even though it's his father who told me about it at Christmas, my brother-in-law, just because I needed him - he likes sci-fi.  He travels a lot.  So he has an opportunity to read.  And it's just so good.  So, okay.  I'm going to try not to talk about it anymore.



LEO:  Let me - I'm just curious.  I want to play just what it sounds like, and you can see if...



STEVE:  Great.



LEO:  You don't listen to audiobooks.



STEVE:  I don't.



LEO:  So it's not even really an issue for you.  But I'm just curious.



STEVE:  No, but I know that we have some listeners who do.



[Audiobook clip]



LEO:  A little nasal for me.  I'm not sure - I'll read the book first and see.  And if you have Kindle Unlimited, the whole series is free.



STEVE:  And I do, and it is.  I mean, I feel guilty.



LEO:  That's a lot of words.



STEVE:  I mean, I actually purchased them for my nephew because I wanted him to own them.



LEO:  Well, he's got my $6 now.



STEVE:  And the author deserves his money.  And I'm wondering if he - he calls these "episodes."  And he's got his plan is a five-series of 15 episodes per series.  I wonder if he's, like, set it up to be made into a TV series.



LEO:  Oh, maybe.



STEVE:  Because as I said last week, I would much rather see this than Honor Harrington.  I loved Honor Harrington.  It had some fabulous parts.  But they were few and far between.  This thing just - it never lets you go.  Anyway, it's just I'm - okay.  So, sorry to take so much time, but wow.



A quick note about SpinRite from a Patrick McFarland.  I got a kick out of this.  Actually I have this and then a picture, but they're not related.  He said:  "@SGgrc SpinRite saved the day again!  My son's Xbox 360 hard drive was failing.  Plugged it into a PC and ran SpinRite on Level 4.  Good as new."  So there's one example of, yes, we know it fixes DVRs.  It fixes Xboxes.



And then the Photo of the Week is a kick because we also see on the next page of the show notes that it fixes the NASCAR Global  VR arcade-scale game.



LEO:  That's so funny.  That's an arcade game running the SpinRite interface, yeah.



STEVE:  Screen saver, yup, yup.  So this is this multi-thousand-dollar, full-on NASCAR race game, where you sit in the seat, and you've got the rubber wheel and gas pedals and accelerators and shifts and everything.  And apparently it broke.  So somebody who listens to the podcast or is a SpinRite owner, and he's probably a podcast listener because he tweeted this to me, apparently is responsible for these.  So he opened it up and booted SpinRite on this NASCAR game.  And we can see a PC keyboard off to the left, so he started it going and then wandered off so SpinRite could fix the NASCAR game.



LEO:  So funny.



STEVE:  SpinRite recovering a dead PC-based high-end arcade game.



LEO:  I want to go to this arcade.  It's got some good stuff in there.  It kind of looks like - that's probably the back of the shop there.  All right.  We have still some time, and I think you still have much to talk about.  So let's go.



STEVE:  Yes, we do have both.  So, and I think it'll work out just perfectly.  One thing, first of all, I got so much positive feedback last week from our Proactive Privacy roundup.  That is, sort of all the different ways that the technologies we use could be leveraged, and arguably are being leveraged, in order to track us and deanonymize us and so forth.  One thing I completely forgot because it's like hiding in plain sight, and that's our email address.  How many different sites do we identify ourselves using our email address?  Typically, that's your so-called username.  That's your currently unique token.  And then you use a password.



Well, so by using a common email address across a large number of sites, and notice the email address is not being encrypted, or if it is, it is decryptable because they need - the server end, unlike a password, where all you need to do is verify it, the email address needs to be decryptable or just left in the plain, just left in the clear, so that they can email you for various management tasks, password recovery and so forth.



So there is, you know, we're identifying ourselves deliberately by using a non-unique, or even if we go to some trouble to add like a dot something after the name and before the "@" sign, which we know sort of creates a subcategory and can be treated as different addresses, even so it's still obvious to anyone looking at it what's going on.  So anyway, I just, again, I just completely missed that when we were talking about it last week.  So I just did want to mention that, yeah, there's that hanging out in the breeze, too.



Many people were interested in VPN services.  And I ran across another site that I would tend to trust, only because of the nature of their motivation.  And that's the TorrentFreak.com site.  They have, for a number of years, been going out of their way to send questionnaires out to a huge number of VPN providers, asking them a series of questions.  Number one on the list of 12 questions is "Do you keep ANY [in all caps] logs which would allow you to match an IP address with a timestamp to a user or users of your service?  If so, what information do you hold, and for how long?"  And it goes on, 11 other questions.  I won't drag our listeners through it.



There's a link in the show notes to that page at TorrentFreak, although I imagine if you just google "TorrentFreak VPN Services Anonymous Review," that's their title.  And this one is this year.  It is a recently conducted review.  I'm sure you can find the page.  And, I mean, it is - I just - it scrolls forever, going through every single VPN they queried and showing all of the results from each of them to those 12 questions.  I was hoping they would have a table to summarize it, but I didn't see one, because that would make it much easier just to see this thing in a grid display of red and green summary responses.  But for those who are interested in finding themselves a VPN, you might want to check out TorrentFreak.com.  And it's their VPN Services Anonymous Review, which is very recent and looked very comprehensive.



Bobbob1016 asked, he said:  "SN-607, privacy section doesn't mention not using Google.  You say Google gets first party on redirect, but what if you're not using it?"  And it's like, well, okay.  Certainly Google knows who I am because I have a presence in the Google domain.  And Google knows who I am across all their properties.  So I suppose if you have never - if you don't have Gmail, if you've never used Drive, I mean, I'm not sure how you would avoid Google, I guess is what I'm saying.  You'd have to really not use it and never do anything that would deanonymize you to Google.



And even so, even if you were unknown, then the presence of Google Analytics, which is so pervasive over the 'Net, would mean that Google would be tagging and following you, not knowing who you are, but knowing everything you did and everywhere you, an anonymous entity, went.  So certainly Google is connecting, in my case, connecting my real-world identity to everywhere I go and what I do.  It's like, okay.  I understand that and accept it.  Just it's very hard to prevent that from happening.  I'm not even sure how you could unless you blocked all scripting, and we know that's no longer practical.  But even, you know, I'm not even sure that blocked JavaScript would prevent a passive HTTP query back to Google.  It might not.  So I don't know.  That would be a tough battle.



Nick Bedford said:  "I just encountered an issue with blocking third-party cookies.  I couldn't log into Jetstar Airlines account until I unblocked them."  And that's something we didn't talk about last week, but it certainly is possible that there are sites that use third parties, not only for like affiliate collaboration, but as part of their operational system.  So, for example, Jetstar Airlines might have some other service, for example, that handles reservations or login or something, not advertising, but functionally related to the core service such that, if you blocked the access of that third party to their own cookies when you're at Jetstar, they're no longer able to interact as they expect to.  So there can be some downside.  It's rare.



And what you could do, if you, for example, were a uBlock Origin person, would be to use uBlock Origin to globally block all third-party cookies and then selectively, by site, whitelist given sites.  So you could allow Jetstar Airlines queries to enable third-party cookies only there, but otherwise not.  So there are ways around, given the tools that we have, to get the best of all worlds.



David Benedict said, or asked, he says:  "Listening to SN-607.  So would having your ISP NAT connections be a bonus now to your privacy?"  Meaning, and we've talked about this before, where in the maybe foreseeable future, if we don't start getting direct IPv6 connections, ISPs that are running short of IPv4, of their own IPv4 allocation, that is, they have more subscribers than they have IPv4 space, could themselves use a NAT router in order to expand their subscribers.  Not everybody at the same time, I mean, it would start getting a little tricky if they had high use.  But in the same way that we use a NAT router in our homes so that many devices are able to use a single IP, an ISP could do it at that level.



So, yes.  That would weaken one useful, but not very strong aspect of tracking that we talked about last week, which is your own public IP on the Internet.  That would be obscured.  But it's hard to provide an exact quantity.  But that's a tiny piece of the puzzle relative to all of the ways that our browsers' queries, with cookies and browser headers in the query, and the existence of other extensions can be used against us.  So, yeah.  Some addition, but probably not worth writing home about.



John wrote:  "OAuth identity providers also use redirection and get to track your movements, at least where OAuth is used."  And that's something also that I have often talked about in the past, I forgot to mention last week in our roundup.  So thank you, John, for reminding me.  And that is the fact that, when you, quote, "Login with Google," "Login with Twitter," "Login with Facebook," handy as that is, that's a query going to that third party to whom you authenticate.  And so that third party knows where you're logging in as part of the bargain of using OAuth.  So it's convenient, but it does mean that a third party is seeing everywhere you use them to log into other sites.  Which of course is one of the things I like about SQRL is that it's a strict two-party system.  So it's TNO, and nobody knows where you go and what you do.  As far as the SQRL protocol itself works, you still have all the other privacy protection problems that we talked about last week.



Ryan asked, or said:  "Important caveat to DNSCrypt.  Destination site names leak if that site is using SNI to do HTTPS.  Many sites use SNI these days."  And that is a great point that I forgot to mention.  We've talked about it in the past.  That's a Server Name Identification, SNI.  That's the extension to TLS which allows one IP on the server to host multiple TLS certificate, HTTPS-enabled sites.  The idea is normally there is a binding between the IP address and a certificate.  For example, GRC uses that, such that the GRC IP implies the certificate for the site you're visiting.  But in hosting providers, for example, you want multiple sites at a single IP, just to save on IPs.



So the web browser that understands this SNI extension will add to that client hello packet, the first packet that goes out that is not encrypted.  It establishes the TLS handshake, establishes the encryption channel.  But that first packet needs to say "This is the host that I'm looking to connect."  So an ISP that was nosey could be capturing the client hello packets and see who you are connecting to, not only at the IP, but use the client hello to disambiguate among the hosts at that IP.



So, Ryan, great point.  Thank you for bringing it up.  And there's no way to protect that.  That's in the TLS spec that's in the first packet that goes from the client to the server, which is the way the server knows which certificate to use in responding to the client hello with its server hello and goes from there.  So that is not masked by the TLS connection.  It would be by the VPN carrying that because the VPN tunnel would be wrapping all of your traffic, including the TLS setup packets.



And, finally, Malcolm Hannan-Smith said, in a pair of tweets, of SN-607, last week's Proactive Privacy podcast:  "It is very important to live test an active VPN with a site like Whoer.net."  And Leo, you're going to want to go there:  W-H-O, as in who, E-R.  W-H-O-E-R dot net.  And then in his second tweet:  "VPN testing."  He says:  "I used proXPN VPN last year.  It said the PC was safe, but testing showed my real ISP IP address was visible."



This is a cool site.  I wasn't aware of it.  I checked it out in detail last night.  And Leo, there's a button at the bottom for extended information, which you can press.  Whoer.net.  Go there.  You will be surprised.  And in fact I discovered that the WebRTC feature in my browser, my Firefox browser, was leaking my internal LAN IP and a few other bits of information which WebRTC is known to leak.  There is a sub-article there.



I have a link in the show notes to "How to Disable WebRTC in Various Browsers."  I'm not using WebRTC for anything.  So under Firefox, again, about:config.  If you put in "media.peerconnection.enabled," and probably you can just put in "media.peer," that would be enough to match, you want to turn that to false, then retest again.  And sure enough, WebRTC was no longer providing some information I didn't want it to be.



But this Whoer.net is wonderful.  And it's an outfit who is offering their, obviously, their own VPN service for five bucks a month.  And I'm sure they pass their own test with flying colors.  But they're also allowing you to check your standard connection and your VPN connection.  And I would by all means recommend it.  It's free, and you're going to learn something.  So Malcolm, thank you very much for sharing that with us.  And to our listeners, thank you for sharing another podcast with us.



LEO:  Yeah, that's a great site.  I'll play with that at home.



STEVE:  Yes, yes, yes.



LEO:  Thank you, Steven.  News and feedback.  That was a potpourri.  And now you smell minty fresh.



STEVE:  Oh.



LEO:  Yes.  We do the show every Tuesday, 1:30 Pacific, 4:30 Eastern.  That's 20:30 UTC, if you want to tune in and watch live.  I know a lot of people doing their taxes today in the U.S. are probably listening while they bang their head against their screen.



STEVE:  Thanks to Sue, I already have my California rebate check.



LEO:  Nice, very nice.



STEVE:  My refund, yes.  I would be in prison if it weren't for Sue because I would have really intended to do that, but I just would have, well, not gotten around to it.



LEO:  We do also on-demand versions.  So does Steve.  Let me point you to Steve first, GRC.com.  That's his website.  It's where you'll find SpinRite, his bread and butter, the world's best hard drive recovery and maintenance utility.  You also find all the freebies he gives away, and this show.  In fact, not only the audio of the show, but a great transcript, so you can read along.  It's written by a human, so it's not gibberish.  It's actually technically sound.  And that makes it also very useful for searching.



STEVE:  You know that Elaine told me last week that one of the reasons her proofreading is such a substantial portion is she goes through and verifies that every single double quote is balanced.  It's like, yeah.



LEO:  Wow.  That's amazing.  Well, it's a work of art.  You should check it out.  You can also get video, as well as audio, from our site.  We don't have the transcripts.  We have video instead.  That's TWiT.tv/sn, all of our shows available on demand at our site, but also wherever you get your favorite podcasts.  Do subscribe.  That way you'll get every single episode of Security Now!.  We'll be back here next Tuesday.  I hope you will, too.  Thanks for joining us.  See you, Steve.



STEVE:  Thanks, Leo. 



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#609

DATE:		April 25, 2017

TITLE:		The Double Pulsar

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-609.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week Steve and Leo discuss how one of the NSA's Vault 7 vulnerabilities has gotten loose.  A clever hacker removes Microsoft's deliberate - and apparently unnecessary - block on Win7/8.1 updates for newer processors.  Microsoft refactors multifactor authentication.  Google is about to add native ad-blocking to Chrome - and what exactly are abusive ads?  MasterCard's building a questionable fingerprint sensor into their cards.  Are Bose headphones spying on their listeners?  Ten worrisome security holes are discovered in Linksys routers.  MIT cashes out half of its IPv4 space.  We've got the return of two meaner Brickerbots, some errata, a bit of miscellany and, time permitting, some "closing the loop" feedback from our podcast's terrific listeners.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Oh, boy, do we have a lot to talk about.  Steve will answer some of your questions.  We will talk about the latest from the CIA code dump, Vault 7.  It's actually spreading now to actual Windows PCs.  In fact, surprisingly, to a lot of Windows PCs.  We'll talk about what to do to mitigate for that.  And Steve has some explanations of the fingerprint saga that we started last week.  It's all coming up next.  Why don't you watch?  Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 609, recorded Tuesday, April 25th, 2017:  The Double Pulsar.



It's time for Security Now!, the show where we cover you, your security and privacy online with this cat right here, Steve Gibson of GRC.com.  He is our mentor, our leader in more cases than not.  The question is, what would Steve do?  Hello, Steve Gibson.



STEVE GIBSON:  Hey, Leo.  Great to be with you again for Episode 609.



LEO:  Wow.



STEVE:  Following on the third dump of what is presumed to be the NSA documents in the so-called Vault 7 dump from these Shadow Broker guys, one of them has gone wild...



LEO:  Uh-oh.



STEVE:  ...on the Internet.  So the title for today's episode is "The Double Pulsar," which is believed to be an NSA-designed backdoor which is being dropped in by one of the other vulnerabilities, which Microsoft patched in March.  Yet this is a great lesson for us about to what degree does it even help that Microsoft has patched these things.  So we're going to talk about that; how a clever hacker removed Microsoft's deliberate and apparently unnecessary block on Win7 and 8.1 receiving updates for newer processors; how Microsoft has refactored multifactor authentication; how Google is apparently, the Wall Street Journal reports, planning to add native adblocking to Chrome.



LEO:  I know.  Hell froze over, yeah.



STEVE:  Counterintuitive.  Which, well, but, boy, is it good news.  And then we're going to look at exactly what abusive ads are because that's now been formally defined.  MasterCard has announced they're going to be building what I consider a questionable, for reasons we'll explain, fingerprint sensor right into their next-generation cards.  The question about Bose headphones spying on their listeners.  Ten worrisome security holes were discovered in state-of-the-art recent Linksys routers, 25 different models.  MIT cashing out half of its IPv4 space.  The return of two even meaner Brickerbots.  A little bit of errata, some miscellany, and time permitting - and today since we're getting a late start we may squeeze on the backend some "closing the loop" feedback from our listeners, which we may get to next week.  So we'll just play it by ear.



LEO:  Good.  Good.  We'll get it all in.



STEVE:  So our Picture of the Week reminded me how much I love curves.  And I noticed - in the context of a graph, of course, orthogonal axes.  And it's just at some point in my past I realized that that's the way I think.  I think in terms of a relationship between two variables where a curve describes them.  And I've mentioned this a long time ago, but one of my first jobs out of college was I was the third person in a small company, Minicomputer Technology, that made, not surprisingly, designed and manufactured hard disk controllers for the large, large 5MB, and in some cases you could get 10, or if you were really pushing it with something the size of a dishwasher, 20MB hard drive.  And I was doing some of the sales and marketing in addition to the engineering.



And what I realized was, and this was really a consequence of one of the brilliant founders of the company, that the relationship between cost and complexity was not a straight line.  That is, in terms of the cost of the controller.  As the disk controller increased in complexity and functionality, its cost went up.  And if it was a straight line, it didn't really matter where you were on the line because you would always get the same amount of return for your investment.



But it turned out that the way we designed ours, the curve had a real knee in it.  And what I realized is, if you operated at that knee, at that inflexion point in the curve, you could - there was a place where the design could give you a lot of function and very low cost.  All of our competitors took the wrong approach.  They went high function, high cost.  And so we were just selling these things like crazy.



Anyway, the point is that maybe it was then that I realized that's just the way I see things.  Anyway, today's picture of the week is just a wonderful cartoon from cartoonist Zach Weinersmith which shows the relationship between, on the horizontal axis, our knowledge of physics, and on the vertical axis, how much the universe make sense.  And it's wonderful because, if you imagine physics knowledge moving with time, moving forward with time, how the universe makes sense is going up and up and up and up and up and up and up.  And so it's making more sense to us as we're better understanding physics.  And then, as this curve shows, at some point, probably when we hit quantum physics...



LEO:  Yeah, or string theory.



STEVE:  ...this thing, yeah, just crashes down to zero.  And it's like, now, suddenly it doesn't make any sense whatsoever anymore.  So anyway, I just love how much meaning you can put into a curve.



LEO:  Good point.



STEVE:  And of course, yeah, they're really valuable that way.  So great Picture of the Week.  Thank you, whoever it was who shot me that note.



DoublePulsar.  So this is from our "this didn't take long" department.  Less than two weeks after the third dump of the Shadow Brokers documents - which we believe, and all evidence indicates, originated with the NSA - the numbers vary by researcher, but tens of thousands of Windows machines are today infected with the DoublePulsar backdoor that was disclosed in this third document dump.  So 14 days.  And it's a major outbreak.  In fact, it's being considered the worst outbreak since Conficker, which we covered on Podcast No. 193, back on April 23rd of 2009.  And the little tag on the podcast says "Steve analyzes Conficker, the sophisticated worm that has spread to more than 10 million PCs worldwide."  Now, this has not spread to that number.  On the other hand, that was, what, eight years ago?  And here we are again, still with this kind of, with sort of this nature of problem present, which is surprising.



Okay.  So DoublePulsar is a RAM-resident implant, that being the term that we first saw being used in the Snowden release of the CIA documents, an implant being something that is implanted into a system for some purpose.  And it gets into these machines through the EternalBlue exploit, which we discussed last week because Microsoft patched it in March, in that delayed, well, in that big update which covered all of these various Eternal* exploits, one of which was EternalBlue.  But across the industry malware researchers are comparing this, as I said, to Conficker because it is really serious.  Conficker leveraged the Windows RPC, the Remote Procedure Call.



And, surprisingly, as we discussed last week, this EternalBlue exploit was an SMB, the Server Message Blocks exploit, that is, the port 445.  And as we said, anybody behind a router is safe; although Woody, writing in his Woody for Windows column in InfoWorld, reminded us that, even if your machine doesn't have an exposed port 445, most Windows machines are gluing themselves together on Intranets through 445.  I mean, it is the intermachine communications port that Microsoft has settled on.  So if any other machine in your Intranet could get infected, then it could spread within to essentially all the machines in an Intranet.  So, I mean, this thing is worrisome.



So there's no real consensus about how widespread this is.  But more than five million Windows machines - so listen to this.  Five million Windows machines currently have port 445 publicly exposed.  Which is astonishing to me.  And I would ask our listeners to send me a reason, if they know.



LEO:  Well, especially since - the chatroom is saying that typically ISPs will block that port; right?



STEVE:  Correct, correct.  ISPs are blocking it.  And if you're behind a NAT router it's blocked unless you...



LEO:  You open it for some reason.



STEVE:  Exactly.  And so I can't, I mean, I can't imagine anyone crazy enough to deliberately make that port open.  And any newer machine has a firewall that's running by default since Service Pack 2 of XP.  It's going to be blocked even if you put the machine right on the network.  So, I mean, and any of the Windows servers, they'll deliberately open the ports they need for, like, 80 and 443 for HTTP and HTTPS, and maybe FTP and so forth.  But they're not going to open 445 unless you really want them to do that.  So I'm stunned at the news that a mass scan of the Internet in the last two weeks has shown more than five million - actually it's 5,561,708 machines, IPs, answering TCP connections on port 445.



LEO:  That is kind of amazing.  Geez.



STEVE:  It's just shocking.  Now...



LEO:  There's got to be some explanation.  I mean, there's something going on.  I would think.



STEVE:  They've got to be old.  They have to be someone who just stuck them on an IP without any concern.  Or maybe just like, oh, look, our connectivity is hampered.  Let's turn off the firewall.



LEO:  Right.



STEVE:  It's like...



LEO:  I wonder if there's some commercial service or gaming machine or, you know...



STEVE:  No, 445...



LEO:  ...the thing is there are so many Windows machines that five million, you know, that's just kind of - it's crumbs; right?  It's just crumbs.



STEVE:  Right, right.  So there may be people who don't have a NAT router, who have their machine on.  Maybe their machine's infected with something else that turned off their firewall.



LEO:  Hmm, yeah.



STEVE:  You know, it's like, wow.  And of course Universal Plug and Play is on by default in all routers.  So if anything got into your machine and said, oh, open up that port, I mean, you could have a poorly designed light bulb which said, yeah, we'd like to have 445 exposed.  And so now you're open.



Anyway, so that's Windows machines with 445 exposed.  Of those, there is a publicly posted, widely used script on GitHub, and I've got the link in the show notes, of essentially a script that pings those machines for the presence of the DoublePulsar implant.  So of those five million machines, there are reports of as many as 50,000 today vulnerable and infected.  One malware hunter who goes by the Twitter handle @Below0Day, zero as in numeral "0," who I just got a tweet from David Redekop, who said that the account's just been shut down by Twitter.  So I guess his postings pushed them over the line.  He did a 24-hour Internet scan.  And I have a screenshot of what came up on the terminal.  It took a little over a day, about 25 hours.  And he found - he's the person who found 5,561,708 machines.  Of those, 30,626 instances of DoublePulsar implant were detected.



LEO:  So a small fraction of the total machines with 445 open were infected.



STEVE:  Right, right,



LEO:  That's interesting, too.



STEVE:  But, well, but like 10%.  So it's like, well, not quite 10.



LEO:  And presumably it will spread.



STEVE:  Correct.



LEO:  To all the rest at some point.



STEVE:  Correct.  So Binary Edge, that's a Swiss-based security firm, reported finding more than 107,000 infected machines in their recent multiday scan.  Errata Security's Robert Graham, who as we know came to early fame - he was the original author of the BlackICE personal firewall back in the early personal firewall days.  He scanned and found 41,000 infected machines.  Dan Tentler, who's the founder and CEO of the Phobos Group, did their own Internet-wide scan.  They found somewhere between 62,000 and 65,000 and said that about 3.1% of vulnerable machines were already infected.



So the numbers are varying, but there's no question, I mean, certainly everybody who looks finds tens of thousands, many tens of thousands of existing infected machines.  Now, it's not assumed that these were existing implants.  They are very likely recent infections as a consequence of these documents going public because this is very well engineered.  It's a drop-in, script kiddie-compatible exploit that you just - it's trivial to use.



The good news is, if there is any here, is that it's a RAM-resident implant.  It doesn't write anything to the file system.  Of course we know that that's one of the ways that these things hide because typical malware scans for known viruses in files, and we're not seeing much RAM scanning at this point.  And if we are, it's typically not in the kernel.  This thing uses the - since the TCP/IP stack that does all this Internet traffic lives in the kernel, the exploit is in the kernel.  And DoublePulsar installs itself in RAM, hooks into the kernel, and rides along on top of port 445.  It doesn't open up its own port.  Instead it just monitors the port 445 traffic, which is how these various security scanners are looking for it is they know how to ask, to generate a query on port 445 that the DoublePulsar will grab and respond to.



So Matthew Hickey, who's a founder of the U.K. consultancy Hacker House, added that:  "The fact that people are using these attack tools in the wild is unsurprising," he said.  "It shows you these tools are very well developed, very weaponized, and don't require a lot of technical sophistication.  So attackers are quick to adopt them into their repositories and toolkits, and they're using them as-is."



Then Kaspersky added a little bit of technical detail, saying that:  "DoublePulsar works on older Windows Server versions with older versions of PatchGuard kernel protection.  Modern versions of Windows such as those derived from Windows 10 have better kernel checks that could help block or prevent these hooks deep into the OS.  Once DoublePulsar is on a compromised host, an attacker can drop additional malware or executables onto a machine, meaning that this bug will quickly move from the exclusive realm" - and I would argue it already has - "from the exclusive realm of nation-state hackers to cybercriminals, and it may be a matter of time before ransomware and other commodity malware and botnets take advantage of these exploits to spread.  One drawback for the attacker is that, since the attack lives in memory, once a machine is rebooted, it's gone."



On the other hand, as we know, it comes back up, and it gets reinfected because nothing will have changed.  "DoublePulsar also comes with a kill or burn command that won't remove the infection, but does prevent others from making use of the backdoor."



So anyway, I have a link to Woody's column in InfoWorld.  He went further and has a really nice breakdown for anyone who's concerned about which versions of 7, 8.1, and 10, which build versions and knowledge base patch levels you need to have.  And of course the short version is, just be current.  Make sure that whatever you're using - 7, 8.1, or 10 - that you updated with the March patches because he makes the point that, even if you're not publicly exposed, you could still be attacked from other machines on your network if anything, either this or something else, got into them, since it wouldn't have to be a 445 port exploit that compromised a machine.



But, for example, if something - take Sony, for example.  We don't even know today, we never had any details about how this exploit went so wide.  But if something got into Sony, and this, for example, was known before it was patched, which was only in March, then that would allow an Intranet, essentially a massive Intranet exploit within an organization that could then allow you to go from a receptionist's computer, for example, and then leverage that into getting onto a server, and then you install this thing.  And so it's a way of going from machine to machine, breaking through the security that would otherwise exist.



So, wow.  A bad problem.  Microsoft is questioning these numbers, I think because they don't like them.  But it's a little difficult to question eight different completely separate security organizations that have all run their own scans and all, while they're disagreeing about the exact quantity, they're all upwards of tens of thousands of machines that have this thing in them today.  So we'll see how this goes in the future.



LEO:  But was the attack published by WikiLeaks?  How are people getting the code?



STEVE:  Yeah, it was.  It was in this...



LEO:  They published - it was irresponsible.



STEVE:  Yes.  It was in that third dump by the Shadow Brokers.



LEO:  Because at first they - oh, it was Shadow Brokers, not the WikiLeaks.  It was the Shadow Brokers.



STEVE:  Yeah, the Vault 7 disclosure.



LEO:  No, that is WikiLeaks.



STEVE:  Mm-hmm.



LEO:  So I thought they weren't going to publish code.



STEVE:  Ah.  They published enough.



LEO:  Okay.



STEVE:  Yeah.



LEO:  Okay.



STEVE:  So this is a little controversial.  And I just thought it was interesting.  A GitHub user who goes by the handle "Zeffy" created a patch that removes a limitation that Microsoft deliberately imposed on users of seventh-generation Intel processors which prevents those users from receiving, for example, last month's or this month's Windows Updates, if they still use Windows 7 or 8.1 with Kaby Lake or Ryzen PCs.  So it was controversial, of course, even though, as we know, Microsoft told everybody well in advance that this is what was going to happen.  Still, people who were choosing to use Windows 7 or 8.1 on the latest hardware discovered that they could no longer receive updates.



What was interesting was that this Zeffy guy on GitHub, he was just sort of curious exactly what was done.  So he took a look at the updates that were in Knowledge Base 4012218, which was the March 2017 Patch Tuesday, and discovered two new functions which Microsoft added:  IsCPUSupported and IsDeviceServiceable.  Those two functions return a Boolean result, true or false, yes or no.  Making a one-byte change to "IsCPUSupported" so that it returns a "1" rather than a "0," and everything works.  Meaning that it's not that the updates aren't compatible in some fashion with these later version processors.  But Microsoft simply wanted to enforce their policy that they would not allow newer processors to operate on older versions of Windows and continue to receive updates.       



LEO:  So that solves the question because we thought maybe it was a technical issue.



STEVE:  Correct.



LEO:  It's not.



STEVE:  Exactly.  And so that's what's annoying is it's not that they had to do any, like, the engineering of the updates is not compatible, which always did, I mean, I get it that Microsoft doesn't want to have to not support older architectures, but why not wait until they actually don't support older architectures, rather than enforcing the policy because it's a policy?  When in fact doing so is denying users of Windows 7 and 8.1, which are being kept updated.  For example, I'm getting them on my Windows 7 because I bought Skylake on purpose so that this wouldn't happen to me.



So people who bought newer machines, choosing to stay with older versions of Windows, aren't getting the updates that people on older hardware are through 2020.  So three more years of updates, just because.  So that's annoying.  If anyone is interested - and I'm not suggesting this is a good thing to do because this requires patching and essentially hacking a couple files.  All the information is on GitHub.  I have all the links in the show notes.  If somebody is in this position, it's well vetted.  It works.  But it does mean that every time Microsoft, like every month Microsoft will probably refresh this, and I wouldn't be surprised if it doesn't work in a month or two.  Microsoft will decide, okay, we're just not going to make it as easy to do.



But the cat's out of the bag.  We now know that they just added a test - it tests the CPU ID.  Is this CPU seventh-generation or not, or later?  And if it is, it says "CPU not supported."  Not for any good reason except because they said that's what they were going to do.  And what's annoying is in the process they're denying people updates for security, which they're saying are important, which people could otherwise have.



A lot of our listeners were wondering about this Microsoft Authenticator change which was announced last week.  And I called it "Microsoft refactoring multifactor."  As a result of this announcement, a Microsoft Windows account may now be registered with the Microsoft Authenticator app which is available for iOS and Android - and, interestingly, not for Windows Phone - after which the app will receive a Windows logon confirmation prompt.  So you unlock your mobile device, acknowledge the request, and you're logged in.



So the question has been, is this multifactor?  And Microsoft says yes because they think that phrase is the holy grail, like being multifactor is automatically more secure.  I would say no, it's not multifactor, since "multifactor" means multiple secret factors.  And since your username is not a secret, you have been previously relying on your password as a single-factor secret.  So when you add, for example, your username - and remember, your username is often your email address, which we know is not secret.  So when you add, for example, a time-based six-digit one-time token, that's another secret that's making it multifactor.  What Microsoft has done is saying you don't need your password.  If you register your Windows account, you log in with your name just to say this is who I am, and then your phone will ping and go, you know, are you logging in?  And so you say yes, and then you're good to go.



LEO:  But it is, well, but it is something you know and something you have.



STEVE:  No, it's one factor.  And so here's...



LEO:  Well, something you know is your username, admittedly not very secret.



STEVE:  That's not a secret.  So that's not a factor.



LEO:  You had to use, by the way, for that to work on your phone you had to use your password to activate it on your phone.



STEVE:  Or fingerprint, which your spouse might be able to unlock.



LEO:  No, not just your fingerprint.  It won't do it the first time unless you use your password.  When you install the app, you have to log in fully to your Microsoft account.



STEVE:  Okay.



LEO:  So you log into your Microsoft account on your phone; right?  And then you can lock that with a fingerprint from now on.  So you log on on the computer.  I mean, you did it all on the phone originally.



STEVE:  So here's my point.  We only need to resort to the added encumbrance of multiple factors.  I'm not saying this is bad, Leo.  Don't...



LEO:  But if you don't have my phone, it's not going to work.



STEVE:  Correct.  So it's one...



LEO:  You have to have the phone.  It's one factor.



STEVE:  ...strong factor.



LEO:  Steal my phone; right.



STEVE:  That's my point.  We have only needed to resort to the added encumbrance of multiple factors because the factors themselves have been individually weak.



LEO:  Right, right.



STEVE:  So having more individually weak single factors, where they must all be correct in aggregate, provides us with stronger final security.  And so what this is...



LEO:  So Google and Duo and others do this.  But you log on on your computer with your name and password, and then it fires up the acceptance on your phone.  So you would call that true two-factor.



STEVE:  Correct.



LEO:  Okay.



STEVE:  Because you the attacker would have to provide multiple things.



LEO:  Right, secrets, yeah.



STEVE:  And of course I'm all for the idea of a single strong factor because that's the entire basis of SQRL.  SQRL is a single factor, but extremely secure solution.  So again, I'm not saying that this is like a bad thing for Microsoft to do.  I think they should just say, instead of saying, "Oh, this is multifactor," they should say, no, but it's one - and I understand they can't explain this to everybody.



LEO:  Right.



STEVE:  But we can to our audience.  If you have one really strong factor, that's good enough.



LEO:  They could very easily just make you enter your password on your computer, like Google does.



STEVE:  Yes.



LEO:  And that would make it true two-factor.



STEVE:  Yes.  And then you would be multifactor.



LEO:  And it's better, that's better than a text message, or arguably even an authenticator.



STEVE:  Oh, I agree.  And I think having it tied to something like that, where your mobile device has authentication, I think that's, I mean, that's good, useful security because it means that, if someone grabbed your computer, they could not log on.  And in fact your phone would get pinged when they tried.  You'd go, oh, look, someone's trying to log on, and it's not me.  So I think it's good.  What Microsoft is, you know, and all the press coverage they're getting is specifically because they do not ask you for a password because everybody hates passwords.  And so they're saying, yeah, we've eliminated the password.  It's like, yeah, okay, fine.  And as long as they don't have any other bad compromises in their system, I think one strong factor is arguably all you need.



So I know you picked up on this news because you've mentioned it before, Leo.  Google, the Wall Street Journal reports, is planning to add native adblocking to Chrome.  And I think this is fabulous because they're going to do, very likely, if they pursue this - Google has not commented on the Wall Street Journal's reporting, but the Wall Street Journal's probably, I mean, this makes sense if nothing else.  But we don't have confirmation from Google.  But I expect them to be able to do for advertising very much what they did for security.  And it's been a mixed blessing, as we know.



For example, it was Google's leveraging the power of their Chrome platform that forced changes in the TLS and certificate infrastructure on the Internet because, if Chrome wasn't going to support some features, everybody had to run around and scramble in order to accommodate them.  I mean, I went to great lengths to keep GRC able to be viewed right up until New Year's Eve of 2015 in order to make Chrome happy, yet still allow GRC visitors who could only use SHA-1 signed certs to get to GRC.  Thus the power of what Google decides to do.



So the Wall Street Journal said:  " Alphabet Inc.'s Google is planning to introduce an adblocking feature in the mobile and desktop versions of its popular Chrome web browser, reported by people familiar with the company's plans.  The adblocking feature, which could be switched on by default within Chrome, would filter out certain online ad types deemed to provide bad experiences for users as they move around the web.  Google could announce the feature within weeks, but it's still ironing out specific details and still could decide not to move ahead with the plan, the people said."



LEO:  They were going to, remember, put encryption in Gmail, too, and didn't do that.



STEVE:  Yeah, yeah.  Although this to me seems real clean.  Encrypted Gmail, okay.  "Unacceptable ad types would be those recently defined by the Coalition for Better Ads" - and I've got a link here in the show notes below, Leo, that breaks out what those are - "an industry group that released a list of ad standards in March.  According to those standards, ad formats such as pop-ups, autoplaying video ads with sound, 'prestitial' ads" - which is a term I hadn't encountered before.  Instead of "interstitial," these are "prestitial" - "with countdown timers are deemed to be 'beneath a threshold of consumer acceptability.'  In one possible application Google is considering, it may choose" - and get this - "choose to block all advertising that appears on sites hosting offending ads, instead of the individual offending ads themselves.  In other words, site owners may be required to ensure all of their ads meet the standards, or could see all advertising across their sites blocked in Chrome."



LEO:  Woohoo.  Of course no Google ads violate these standards.



STEVE:  Correct.  And that's why I think this is a brilliant move because...



LEO:  You know what, it was forced because the choice was let everybody use adblockers, and then you're really dead meat, or  do something meeting them halfway.  I think they had to do this.  This is an example of us winning, in effect.



STEVE:  Yes, exactly.  The Wall Street Journal in their reporting said that the "Uptake of online adblocking tools has grown rapidly in recent years, with 26% of U.S. users now employing the software on their desktop devices, according to some estimates."  So again, Google, as we know, in 2016 they made $60 billion in revenue from online advertising.  They're seeing that threatened because users are responding to obnoxious, I mean, I've listened to you so many times annoyed by self-starting videos playing, like when you're trying to do a podcast and something's there making noise.



LEO:  All the time, yeah.  It's really annoying.



STEVE:  Yeah.  So I just say bravo to Google for this.  So on the desktop they're saying that pop-up ads, autoplaying video ads with sound, prestitial ads with a countdown, and large sticky ads would be banned.  All of those also on mobile.  Plus mobile ads, if a site has an advertising density higher than 30%, if the animated ads are flashing to grab your attention, if they are positional ads with a countdown, or full-screen rollover ads, those additional four categories would be banned on mobile.



And I hope this happens because what this would do, I mean, this is, again, in the same way that Google leveraged their clout in order to force security to be improved, they're helping us.  I mean, I only have adblocking on because, as I've often commented, I look at someone's machine that doesn't have it, and I'm thinking, how can you even see through the ads in order to get to the content?  So if this gets fixed - and the point is it'll have to get fixed because, if Chrome won't display it, that's half of the market.  Half of the install base now of browsers are Chrome.  And so if Chrome won't do it, the ads will have to back down.  So, yay.



This is a bad idea.



LEO:  Uh-oh.



STEVE:  Yes.



LEO:  We haven't done a "they're doing it wrong" in a while, you know.



STEVE:  Well, this is they're doing it dumb, at least.  The headline on the MasterCard press release reads "Thumbs Up: MasterCard Unveils Next-Generation Biometric Card."  Now, it's clever, I'll give them that.  Anybody who's received a credit card in the U.S. at least - and of course this is the EMV standard, standing for Europay, MasterCard, and Visa - you'll have that little contact area a little above the center line on the left-hand side of the card, above the account number and name.



LEO:  Let's take a look at Lee M. Cardholder's card. 



STEVE:  Yes, exactly.  And he's got an expiration date that I don't think is possible.



LEO:  12/23.



STEVE:  Yeah, that's way out there.  And so the point is, as we know, you stick your card into the terminal, and it only goes about, maybe, what, a little over a third of the way?



LEO:  Yeah, yeah.



STEVE:  So the right-hand side of it is sticking out.  Well, they very cleverly put a biometric thumb reader, thumbprint reader in the card.  So the card itself, and it's probably capacitive as opposed to optical, so it's probably a capacitive reader.  And you can do this because the card is powered by the contact strip.  So it doesn't have...



LEO:  Ahhhh.



STEVE:  Yes, that's why this is a...



LEO:  It's clever.



STEVE:  It's a clever idea.  You now have a card receiving power from the terminal, so it doesn't have all of the problems of a battery and thickness and all that stuff.  It does have some problems, though.  The problem is, as we know, fingerprints are not exact.  Which means the card has to know how to decrypt itself.  That is, it has to contain the information in it to authorize the transfer.  If, for example, it were a PIN pad, where you had to enter a lengthy PIN for security, then the PIN could be hashed, which would be - the exact PIN could have an exact hash that would generate an exact key, which could be used to decrypt the information about your identity and then authorize the payment.



But a thumbprint is not exact.  And this is why this whole thing fails.  I mean, it's better than nothing, but it's a gimmick.  From a cryptographic standpoint, it means that a fuzzy match must be allowed.  That means a fuzzy match doesn't produce an exact result.  That means that a decision is being made somewhere in there, is this the thumb that I was trained on or not?  And in the same way that the hacker changed one byte in Microsoft's March update in order to reenable updates that Microsoft has banned, somewhere there's a single jump command.  There's a single decision being made, is this a matching fingerprint or not?  And the point is you're not using information that the card doesn't have.  You're just saying, yeah, that looks like a thumb I recognize.  Well, that means a hacker can hack that in order to unlock the card.



So it's, yes, it's better than nothing.  But if we look at the technology that had to be employed, it doesn't mean that this is cryptographically secure.  And of course you have to wonder then also how you can hand this to a restaurant server and have him or her run your charge.  Because unless you're going to follow them into the back...



LEO:  But by itself that makes it more secure; right?  He can't do anything without you.



STEVE:  Exactly.



LEO:  You know, in Europe what happens is they don't bring it in the back.  They bring a little reader to your table because you have to do the chip, and you insert it and then enter a PIN because they do chip-and-PIN, which we don't do.



STEVE:  Right.



LEO:  So you're saying a PIN would be better than this fingerprint.



STEVE:  Well, we know that PINs have been bypassed through a different technology because you enter the PIN into the terminal, not into the card.  So what I'm saying is, if you entered - and unfortunately the PIN is just compared with the PIN that's in the card, instead of the PIN being used to decrypt information in the card.



LEO:  So a PIN is just as bad.



STEVE:  Yes.  The PIN is just as bad.  And this is no better, unfortunately.



LEO:  All right.  Look, it seems better.



STEVE:  Oh, I know.  That's the point is security through obscurity.  But it doesn't, you know, because in fact the fingerprint doesn't give you a precise, like, password equivalent.  It's just a gimmick.



LEO:  And this is why Apple Pay and Android Pay are still the best way.



STEVE:  Right.



LEO:  Most secure way.  You're not giving any information to the merchant.  You're just giving them a token.  You have to use the fingerprint reader on your device to verify that it's you, and those are much better.  It's been solved, frankly.



STEVE:  Yup, yup.  So the headlines all over the place again...



LEO:  This is why - you've got to explain this because I didn't even read the article thinking, well, I don't understand how that would work.



STEVE:  Yeah.  And it doesn't, Leo.



LEO:  Oh, good, thank god.



STEVE:  Yeah.  Even Consumer Reports, that is otherwise a trustworthy organization, I think, in general, but maybe the security and technology's a little tricky.  Or again, we know that oftentimes people who write the articles don't put the headlines on them.



LEO:  That's right.  That's right.



STEVE:  I had the problem for the eight years I was writing the Tech Talk column.  Sometimes I would just cringe when I looked at the headline.  It's like, oh, no, that's not what I said.  But they do it because they want to get readers. 



LEO:  Right.



STEVE:  So the headline is "Some Bose Wireless Headphones Track and Share What You Listen to, Lawsuit Says."  So, no.  It turns out that there is an optional Bose Connect app which users of headphones which support it, and there's, I don't know, I was going to put them in the show notes, but I thought, okay, I'm not going to read all of those for everybody.  It's just, if you have them, you know it.  They're smart Bose headphones that are connected, no doubt with Bluetooth, to a Bose Connect app which gives you additional features which you're able to use.  For example, you can change the amount of noise cancellation that the 'phones offer.  So there's a little bit of a hook to it.



Well, apparently someone discovered, probably by looking at the traffic that this app was generating, that it was harvesting.  In fact, I was thinking of this story when at the end of MacBreak Weekly you guys were talking about the Unroll Me or Unroll It or whatever it was.



LEO:  Yeah, Unroll.me, yeah.



STEVE:  Yes.  A serious privacy breach.  These guys have been caught, Bose has been caught apparently doing the same thing.  Without any explicit user permission, they are going way beyond - because it's not a media player app.  It's just there to interface with your headphones.  But they are sending back everything you listen to, all of the media that you play with these headphones, and essentially everything you do with them, continuously recording the contents of the electronic communications that users send to their Bose wireless products from their smartphones, including the names of the music and audio tracks they select to play, along with the corresponding artist and album information, together with the Bose wireless product's serial number.



So anyway, this has resulted in a class action suit about collecting all customer data without permission, which the plaintiffs allege is in violation of the Federal Wiretap Act to do this.  The complaint says:  "No party to the electronic communications alleged herein consented to Bose's collection, interception, use, or disclosure of the contents of the electronic communications."  And the attorney representing the plaintiffs said:  "This case shows the new world we are all living in.  Consumers went to buy headphones and were transformed into profit centers for data miners."



I have, if anyone's interested, a link to the Bose complaint PDF.  And there is a company called - I have it here in the notes somewhere.  I'm looking for the domain name.  It was - I'm not seeing it.  Oh, Segment.io is the company whose - and they're one of the recipients of this - a company whose home page says "Collect all of your customer data and send it anywhere."



LEO:  Yeah, baby.  They're actually a sponsor of our network, so...



STEVE:  Ah, well.



LEO:  But they're not about - they don't do the customer collection stuff.  They just take - they integrate with whatever it is you're using for the data collection.  So they're not actually, I mean, I don't know.  I don't know.  That's an interesting conundrum there.



STEVE:  That's a question.



LEO:  They do plumbing.  So you put in your app whatever...



STEVE:  The hooks.



LEO:  Put in the hooks.  So it's up to companies not to do stupid stuff.  And then they plumb it over to whatever databases you want to keep track of.  They don't in fact send it to marketers or anything.



STEVE:  So last Thursday Tao Sauvage, who's a security researcher with IOActive, published the results of his reverse engineering of one of the most recent models of Linksys routers.  And of course we've been talking about them, unfortunately, a lot recently.  This is completely separate from those previous discussions, again.  So this adds to that.  In his case, he purchased a recent EA3500 Series router which is part of their  Smart Wi-Fi router series.  And this made me shudder.  Smart Wi-Fi is the latest family of Linksys routers, which includes 25 different models that use the latest 802.11N and 802.11AC standards.  Okay.  So that's the good news.



The bad news is that they can be remotely managed from the Internet using the Linksys Smart Wi-Fi free service.  So he didn't even look at that.  I mean, again, remotely managing your router from the Internet?  What could possible go wrong?  And by the way, there are four WRT models among those 25.  So there are 21 EA models and four WRT.  I've got the list in the Linksys link to their own disclosure.



So this guy and a friend extracted and forensically examined the router's firmware, identifying simply by inspection and then verifying by sending some packets at them, 10 different security vulnerabilities ranging in risk from low to high.  Six of those 10 are remotely exploitable by unauthenticated attackers.  Two of the security issues they identified allow unauthenticated attackers, meaning anybody on the public Internet, to create a denial of service condition on the router.  So you can crash it.



By sending a few requests or abusing a specific API which will respond without authentication, the router becomes unresponsive and reboots.  The admin is unable to access the web admin interface, and users are unable to connect until the attacker stops the DDoS.  So this you could imagine would be fun for kiddies to blast people who they want to keep off the Internet, if you have a Linksys router that has got this exposure.  And it appears that this is exposed by default.



Attackers can also bypass the authentication protecting the CGI scripts to collect technical and sensitive information about the router.  So there are CGI scripts whose authentication can by bypassed, which allows them to obtain the firmware version and Linux kernel version, the list of running processes, the list of connected USB devices, and the WPS PIN for the WiFi connection, of course which then allows you to get onto the router if you're within range.  Unauthenticated remote attackers can harvest sensitive information using available APIs to list all connected devices and their respective operating systems, access the firewall configuration, create FTP configuration settings, or extract server message block, that is, SMB server settings.  Furthermore, an authenticated hacker, meaning someone who can log in remotely, which raises the bar, but unfortunately only eliminating 88%, still allowing 11% of the vulnerable devices, of which there are about 7,000 at the moment.



So an authenticated attacker on 11% of the currently exposed 7,000 routers can inject and execute commands on the operating system of the router with root privilege.  So one possible action for such an attacker would be to create backdoor accounts, gain persistent access to the router.  Backdoor accounts would not be shown on the web admin interface and cannot be removed using the web admin account.



It should be noted that they did not find a way to bypass the authentication protecting that vulnerable API.  And that authentication is different from the authentication protecting the CGI scripts which can be bypassed.  And, however, this is where they discovered that 11% of the approximately 7,000 currently publicly exposed Linksys routers were using default credentials.  That is, admin and password or admin and admin, whatever the default login is that's out there flapping in the breeze, 11% of the 7,000 routers.  Which then allows somebody to log in and obtain root and put in persistent accounts.



So this is a nightmare.  They responsibly disclosed the vulnerabilities back in January and have been sharing the technical details with Linksys.  Since then they've been in constant communication with Linksys to validate the issues, evaluate the impact, and synchronize their respective disclosures, which were both made last Thursday.  And these guys, the IOActive guys, noted in their report that Linksys has been exemplary in handling the disclosure.



So I think Linksys has a better owner now in Belkin, who purchased Linksys from Cisco some time ago, a better owner in Belkin than they did in Cisco because Belkin has apparently jumped right on this.  They're being very proactive.  They have published security advisories offering temporary solutions to prevent hackers from exploiting these vulnerabilities while they work on getting new firmware available.  And as we know, it's often the case that you need to go get the firmware for your router.  Shodan can be used to search for and has been used to search for these vulnerable devices.  That's what turned up 70,000 of them.  69% are in the U.S.  The remainder are spread around, with 10% in Canada, 1.8% in Hong Kong, 1.5% in Chile, the Netherlands has 1.4%, and then on to smaller percentages.  So nearly 70% of them are in the U.S.



I've got links to the Linksys note with a list of all the vulnerable versions.  If you happen to have an EA Series or a late-model WRT, it's not the older, very popular WRT54 or whatever they were.  They're all more recent routers that have the Smart Wi-Fi stuff.  And so Linksys says enable automatic updates, disable the guest WiFi network if you're not actively using it or when it's not in use, and by all means change the default admin password.  And I would say, my god, turn off WAN side admin if you don't really, really need it.  Or minimize the attack surface by only enabling it if you're, like, for whatever purpose you have for needing it, if you're going to be away.  It's just always a bad idea to have that enabled.



MIT, I love this little piece of news, is selling off half - get this - of their 16 million IPv4 addresses.  Back in the 1970s MIT's senior research scientist and a researcher with the MIT Computer Science and Artificial Intelligence Lab, which is CSAIL, saw the importance of IPv4 addresses and requested an early allocation of them, both to support research and to eventually support all of computing at MIT.  They were given the entire 18-dot Class A IPv4 network, so all IPv4 addresses beginning with 18, 18 dot anything dot anything dot anything.  And as we know, that's 24 bits.  So that's 16 million IPs.  14 million of those 16 million were never used.  And they recently concluded that at least 8 million, or half of their original allocation, are excess and could be sold without impacting their current or future needs.  The funds raised from the sale will support MIT's migration to IPv6.  And Amazon was the winning bidder, purchasing that IPv4 space from MIT.



LEO:  Interesting.



STEVE:  Yes.  And we've talked about the IPv4 space depletion in the past and how IPv4 is a commodity which is pulling some serious money.  And Leo, the link here in the show notes to IPv4auctions.com is really interesting.  It'll give you and our listeners an update on what's going on.  They are subject to discount in quantity, but they are currently selling for around 11 to $12 per IP.



LEO:  Wow.



STEVE:  Yeah.  And so what MIT said was that they're going to take this cash windfall from Amazon and use it to build out their IPv6 infrastructure.  And they already have a bazillion IP - well, everybody can have a bazillion IPv6 IPs because there are - I think they had a nonillion number, that is, MIT's chunk.  But, so, yeah, you're scrolling now through the recent auctions for various size networks of IPv4 space.



LEO:  Why is there variation in price?  Are there some numbers better than other numbers?



STEVE:  Well, it's the size of the network.  Normally you get a quantity discount.  So the larger the network, the larger the unbroken block, the lower the price per IP.



LEO:  Yeah, but I see a - oh, I guess the /24s are all about 3,500.  All right.  Yeah, you're right.  There's a consistency.  There's some variation, but /22s are 12 grand.  What is MIT selling off?  Is that a /4?  A /8?



STEVE:  Let's see.  That would be a /9.  And you never see those.



LEO:  You don't see any of those in here.



STEVE:  No.  So that's a huge - that's eight million.  Now, we don't know what price Amazon paid.  But eight million times $10, that's $80 million, which Amazon said, yeah, we'll buy it.  



LEO:  It's worth it to them.



STEVE:  Yeah.



LEO:  Of course.



STEVE:  I mean, for all that stuff they're doing, absolutely.



LEO:  Yeah, yeah.



STEVE:  So very, very cool.



LEO:  Some people aren't rooting for IPv6.



STEVE:  Well, IPv4 is here.  And so if you're Amazon, and you can drop $80 million in order to get eight million more IPv4 addresses, I could see where it makes sense.  If you've got a serious, serious cloud need for them, for, like, the hosting that they're doing.



So also last Thursday - Thursday was a busy day last week - Brickerbot 3 and 4 both surfaced.  Now, remember that Brickerbot was so named because it is a bot that bricks your devices.  It goes beyond just inhabiting them.  It uses a series of commands to try to erase your file system from your IoT device.  It uses the same entry point, the Mirai exploit, which is essentially any busybox-based Linux device that has the Telnet port publicly exposed with the factory default credentials would be a potential victim.  So these are security cameras, some DVRs, as we know, basically the things that got pwned by the Mirai botnet previously.  Brickerbot is going after them and, when it can, just killing them.  Just wiping them out.



Brickerbot 3 and 4 are clearly from the original author.  The attacks are matured.  They've eliminated some things that weren't effective.  They've added at least four more different ways of bricking devices.  And they're also attacking more ferociously and from geographically distributed IPs and different ones than before.  And the industry has heard from the author.  The author goes by the handle Janit0r, with a numeric "0," J-A-N-I-T-0-R.  And he reached out to a Victor Gevers, following up from a comment that Victor had made in one of the first articles about Brickerbot.1 and .2, as opposed to .3 and .4, who confirmed that he's the author and had two things that he was quoted saying.



First he said:  "Like so many others, I was dismayed by the indiscriminate DDoS attacks by IoT botnets in 2016.  I thought for sure that the large attacks would force the industry to finally get its act together."  Okay, well, we're talking light bulbs, people.  "But after a few months of record-breaking attacks, it became obvious that, in spite of all the sincere efforts, the problem could not be solved quickly enough by conventional means."



Second quote:  "I consider my project a form of Internet chemotherapy."  Actually, maybe that should have been the title of this podcast.



LEO:  Yeah.



STEVE:  Anyway, he says:  "I sometimes jokingly think of myself as 'the doctor.'  Chemotherapy," he writes, "is a harsh treatment that nobody in their right mind would administer to a healthy patient; but the Internet has become seriously ill in Q3 and Q4 of 2016, and the moderate remedies are ineffective."  So this vigilante is killing off devices that he is able to access and that have writeable file systems.  Okay.



A couple bits of errata.  Vasile noted from Episode 608, he said:  "Just to be meticulous," and he said, "I know you treasure 100% accuracy, Unicode has space for up to" - and then he did this in hex format - "0x110000 code points, more than could fit into 16 bits.  They can be encoded in multiple ways, ranging from variable-length UTF-8 to fixed-size UTF-32."  And he's completely correct.  Remember last week I talked about how ASCII uses 128 because essentially it only uses the lower seven bits.  Extended ASCII is twice that because it uses all eight bits.  So it's got all of the seven with the high bit off, and then additionally all of another seven with the high bit on.  So that gives us 256 code points.



Unicode is divided into planes, with 16 bits per plane.  And last week I was only referring to what's known as the basic multilingual plane, which is 16 bits and, as I said, 64K code points.  But, and Vasile is right, there are also up to 16 additional supplemental planes, each having an additional 64K code points, for a grand maximum total of 17 64K planes, totaling 1,114,112 code points.  Which is arguably why you could accommodate pretty much every emoji that you ever needed to without worrying, I mean, and even with different skin colors, which we're now seeing.



So thank you, I'm glad to have the correction and to note that I was just talking about the basic multilingual plane, which is 16 bits.  But as he notes, yes, and there are 16 more of those.  So we're not going to run out of space in Unicode.  And in fact I'm well versed in this because GRC's SQRL client is, as everyone will remember, explicitly multilingual.  And I'm using UTF-8 encoding in order to be able to handle any character set that should come along.



Also Rick, who tweeted as @rpodric, said:  "@SGgrc Just a note regarding the apparent fix in Chrome 59 for punycode" that we discussed last week.  He said:  "59 is the dev version.  57 is current stable, with 59 due by June 6."  Now, this is a puzzle to me because everyone was saying that Chrome was broken.  And my Chrome is fixed, and I'm back on 49.  So I don't know if it got fixed earlier, if maybe Google pushed out a fix just for this.  But I did put links for everyone to be able to verify specifically that the tweak, if they're using Firefox, where you turn off the punycode recognition, and it will show you the raw true domain name in the URL.  I put that there so people could verify that that was working for them.  You might want to check Chrome.  I just assumed it was fixed for everyone.  So I don't understand why everyone, like that morning of last Tuesday, apparently it was being fixed.  So maybe Google just pushed out a fix across the board in order to fix that.  I didn't track down the details to determine that.



LEO:  There were patches pushed out.



STEVE:  Yes, good.  Two bits of miscellany.  @elheffe said:  "Not sure if I should thank you or be mad.  Frontier Saga is sucking my productivity away."  And so I replied to him, I said:  "Yeah, tell me about.  I've finished all 19 books in print.  And if anything, the second series starts off even better than the first.  Book 3 of the second series," I wrote, "Is unbelievably good.  Worth reading everything up to there just for the setup."  I mean, I'd have to say Book 18, oh, my lord.



LEO:  How many thousands of pages is this?  I mean...



STEVE:  It would be thousands because they're all, like, 350 - 250 to 350 pages.  They vary a little bit.  And then he replied to my response, saying:  "I'm halfway through Book 6.  Had to tear myself away to work on cleaning the garage.  You weren't kidding about it being nonstop action.  Thanks for everything you do.  Keep the recommendations coming."  And I just - I wanted to cite this one.  I've had a lot of people come back and say, okay, I'm not getting anything done any longer.



Okay.  And finally, just a bit of fun.  This is a less than two-minute-long YouTube video.  I was reminded of it by someone who tweeted this, saying, "This explains so much."  And it comes off really well in audio.  So Leo, if you could share this YouTube video, the link is in the show notes for anyone who wants to share it around.  But it's just too fun.



[YouTube:  Turbo Encabulator]



BUD HAGGERT:  For a number of years now, work has been proceeding in order to bring perfection to the crudely conceived idea of a transmission that would not only supply inverse reactive current for use in unilateral phase detractors, but would also be capable of automatically synchronizing cardinal grammeters.  Such an instrument is the Turboencabulator.



LEO:  We should point out that this is a scientist - and we know that because he's wearing a lab coat and a pocket protector - standing in front of a blackboard with a sign on it that says "catalytic converter."  And to his right is some sort of, it looks like, frankly, it looks like the space shuttle, some sort of spacecraft.



STEVE:  Sort of a schematic of a transmission.



LEO:  Schematic, yes.  We'll continue.



[YouTube:  Turbo Encabulator]



BUD HAGGERT:  Now basically the only new principle involved is that instead of power being generated by the relative motion of conductors and fluxes, it is produced by the modial interaction of magneto-reluctance and capacitive diractance.



The original machine had a base plate of pre-famulated amulite, surmounted by a malleable logarithmic casing in such a way that the two spurving bearings were in a direct line with the panametric fan.  The latter consisted simply of six hydrocoptic marzlevanes, so fitted to the ambifacient lunar waneshaft that side fumbling was effectively prevented.



LEO:  He's really serious about this.



STEVE:  It's so good.



[YouTube:  Turbo Encabulator]



BUD HAGGERT:  The main winding was of the normal lotus-o-delta type placed in panendermic semi-boloid slots of the stator, every seventh conductor being connected by a non-reversible tremie pipe to the differential girdle spring on the "up" end of the grammeters.



The turbo encabulator has now reached a high level of development, and it's being successfully used in the operation of novertrunnions.  Moreover, whenever a forescent skor motion is required, it may also be employed in conjunction with a drawn reciprocation dingle arm, to reduce sinusoidal repleneration. It's not cheap, but I'm sure the government will buy it.



LEO:  $750 million.  Oh, that is hysterical.  That is great.  What is this?  What?  What's the story?



STEVE:  Had you not encountered that before?



LEO:  Never seen that before.  That's classic.



STEVE:  Oh, it's very rare that I'm able to show you something that you haven't seen before.



LEO:  Classic doubletalk; you know?



STEVE:  Oh, god, it's just - and in his white lab coat, and he's deadly serious.  Oh, anyway, it's just [crosstalk].



LEO:  And you know anybody in a lab coat is probably pretty sophisticated.



STEVE:  Oh, yeah.  Don't try that at home.  Anyway, I just wanted to share that with our listeners.  Anyone who hasn't encountered it, it's just wonderful.  And I don't know how you could find it on YouTube.  Again, I have the link in the show notes.  Maybe look up "encabulator."  I think that's the - I bet if you google "encabulator" you could probably...



LEO:  Probably go right to it, yeah.



STEVE:  You can probably go right to it.



LEO:  Or "waneshaft" and "girdle spring."  You know...



STEVE:  That would do it, yeah.  And the double flamulated dipple guard.



LEO:  I want to memorize that.



STEVE:  It's wonderful.



LEO:  Just come in so handy.  Oh, my god.



STEVE:  So I do have an apology from someone who didn't give me his real name.  He goes by "KeenDreams."  And he said:  "A slightly different SpinRite story with an apology."  This was dated last Wednesday, the 19th.  He said:  "Dear Steve:  First off, thanks for the informative podcast.  I've been listening since I started grad school five years ago and have learned quite a bit thanks to you.



"I was recently feeling nostalgic and decided to buy an old Win95 laptop off eBay to play some of the DOS games from my youth.  It was great fun at first, but my excursions into the world of Commander Keen" - and by the way, that's this guy's handle, KeenDreams - "excursions into the world of Commander Keen were interrupted a week later when the laptop stopped booting.  The first thing I thought was my copy of SpinRite, which saved my butt back in undergrad once or twice.  When I booted it up, however, I was surprised to see a name I didn't recognize at all in the license field.  Confusion came over me as I starred at the screen.  Then it dawned on me.  I must've pirated it.



"I felt so bad that I couldn't start the scan until I sent a 'yabba dabba doo' your way.  But needless to say, the old machine was back up and running after my now-legitimate copy of SpinRite worked its magic.  My sincerest apologies, Steve."  Hey, the guy has nothing to apologize for.  "I would use the excuse of being a poor undergrad who desperately needed his research papers back, but that doesn't change a SpinWrong into a SpinRite."



LEO:  Oh, I like "SpinWrong."  I like that.



STEVE:  Well, he bought a copy.  And as our listeners know, I understand reality, and I appreciate the note sharing his success.  And I replied.  I said to him, "Look, thank you.  I appreciate your support and sharing this."  And I also told him, if he's got Windows 95 or, for example, 98, you have got to try ChromaZone.  95 and 98 and machines back then were able to use eight-bit color mode, which is the 256-color mode.  ChromaZone was a product that I wrote...



LEO:  Using the Turbo Encanabulator, I might add.



STEVE:  Yeah, exactly.  It was how I taught myself Windows.  As we know, I always say, if you want to learn a language, find a problem to solve in that language.  And so I wanted to learn how to program Windows.  So, I mean, ChromaZone is in many ways my masterpiece of Windows programming.  All kinds of custom controls, doors that slide open, slide switches that have multiple positions, just a 3D sphere that you rotate with the mouse cursor.  The problem is it's all 16-bit assembly language.  And it is a palette-editing tool.  That is, on machines that could barely run DOS, this thing animated the entire screen.



And what was unique about it is it is a screen saver construction set.  More than 500 - back then we had a BBS.  And so we were publishing the screen saver creations of ChromaZone customers at the time.  If you google - I think it's GRC.com/chroma.htm.  It's not linked to the website.  It's not in the menu.  But you can see some sample pictures of what ChromaZone does.



Anyway, the point is I sent him all the ChromaZone files in my reply email so that he could bring it up and play with it.  I think I provided 400 screen savers, and our customers provided an additional 500.  So you were able to create, you were able to design, it was like a screen saver construction set.  And back on machines, as I said, that just couldn't do anything, this thing animated the entire screen.  So it was very fun.



LEO:  That's hysterical.  It's an orphan page now.  Aw.  Aw.  Oh, and I have just been handed, I didn't realize, but I've just been handed the GE Manual for the Turboencabulator because this is HBK-8359, in case anybody wants to get it, from December 31, 1962.  Function, operation, technical features, ratings, the whole thing is here.  I guess this was owned by Roger L. Pommerenke.  And you could see, in fact, all of the - to measure inverse reactive current in unilateral phase detractors with display of percent realization is the purpose, of course.



STEVE:  And why does that sound familiar?



LEO:  You can get some accessories:  8 ounces 5% tetraethyliodohexamine with 0.01 halogen tracer solution, or the interelectrode diffusion integrator.  All of these can be added to your device, if you haven't run out of money after the $750 million.  And even reference texts, too.  You know, "Zeitschrift fr Physik der Zerfall von Dunge" and other esteemed journals.  So this - I'll pass this along to you because...



STEVE:  Very nice.



LEO:  This is an historic document for the Turboencanabulator.  And, man, I just think this is - it's great that we have this.  I didn't realize.  Burke brought it.  Or, no, Alex Gumpel brought it over because, in case, we have the manual.  In case we need to use...



STEVE:  Well, if your Turboencabulator ever goes wonky on you...



LEO:  The worst.  The worst.  And it's vital to the operation, of course.



STEVE:  Absolutely.



LEO:  I did buy the Kindle version of - because it was 5.68 - of that Frontier series.  It was cheap.  It was like five bucks.



STEVE:  That's for the first three books, I think.



LEO:  Yeah.  I know.  I didn't get all 18.  



STEVE:  One through three, yeah.



LEO:  I'm going to give him three volumes, see [crosstalk].



STEVE:  Well, see what you think.



LEO:  I've read only a few pages because I had other stuff to read for shows and stuff.  But I really liked - I was kind of pleasantly surprised.  I really enjoyed it.  So I think I will like it, yeah.



STEVE:  Good.



LEO:  Can't wait.



STEVE:  And for what it's worth, the author clearly has this planned out.  I read his whole back story because I'm just curious where he came from.  And he, like, wrote something a while ago.  He used to run a PC repair shop.  So I was wondering, I wonder if he knows about SpinRite?



LEO:  Oh, of course he does.



STEVE:  Yeah.



LEO:  You should write to him.  He's in - where is he?  He's in the Bay Area, I think, for some reason.



STEVE:  Yeah, Ryk Brown.  I think you're right, I think he's in the area somewhere.  So closing the loop with our listeners, a couple fun, or a little collection of fun things.  Andy Patuszak said:  "I really like your idea of capturing and keeping two-factor authentication setup QR codes.  But the problem is, I have had two-factor authentication in a few places for years.  Is there any way to get those QR codes again?"  And the answer is I've not found any.  No one will normally give them to you.  Most authentication apps don't export them, which I think is good.  You want them not to volunteer them because that subjects them to being captured.



But so I responded to him, and I'll share my response.  I said:  "Andy, I had the same problem, since I had already established several accounts with Google Auth, which won't export," and I said, "for which I'm glad for security's sake.  Fortunately, every service I have encountered so far will allow you to change your TOTP, your time-based one-time password secret.  So I just logged in one last time with the original code, asked the site for a new one, and then printed that new one out on paper for all future needs on all devices."



So I had a lot of feedback from people who liked that idea, who are beginning to build up their collection of printed-out two-factor QR codes.  And so if anyone else has had this problem, you're normally able just to tell a site where you already have an account established, I want to change mine.  And like, okay, here's a new one.  And so that's the one you then capture.



LEO:  It makes the old one no good, so obviously...



STEVE:  Correct.



LEO:  Yeah.  So you have to change - see, the problem is, if you've done it already in a few things, and then you want a new one for a new phone, you've got to go back and change them all.  Your authenticator's broken everywhere else because you need - but that's okay.  What I do is I get a screen cap and put them, as I mentioned before, put them in LastPass.  So they're secure, and they're there; right?  And then I just take a picture of it into the phone.



STEVE:  Right.  Okay.  So...



LEO:  LastPass, by the way, will give you your QR code again.



STEVE:  Oh, will it?



LEO:  Oddly enough, yes.



STEVE:  Oh, interesting.



LEO:  I'd show you mine, but...



STEVE:  No, no, no, no, no.  We have some early results from our listeners' fingerprint tests.  Paul Dawson said:  "Hi, Steve.  After listening to SN-608" - that's last week - "and your article about smartphone fingerprint sensors, I decided to put mine to the test.  I have an iPhone 6 with iOS 10.3.1," the latest.  "I've taught my phone two fingerprints - well, both thumbs, actually.  I have asked" - get this - "84 people to try and unlock my iPhone with their fingerprint.  I am glad to report that not one of them managed to achieve this."



LEO:  Well, that's good.



STEVE:  Yeah.  "Since my iPhone locks out the fingerprint detector after a few failed attempts, I even used my passcode to allow people several additional attempts at gaining access.  From my findings, I think I am happy that the fingerprint system is secure enough for me.  Best regards, Paul Dawson, Lincolnshire, U.K.  P.S.:  Love the show."



Andy Norman said:  "Surely our own prints from other fingers are likely a closer match than a random finger.  Have not managed to unlock iPhone with my other fingers."  And I don't have an opinion one way or another about whether one's other fingers tend to track the other ones.  I don't know anything about that.



Terry E. Snyder, Jr. said:  "I have an iPad Air 2 and secure it with my thumbprint.  I discovered to my chagrin that my three-year-old son is able to unlock my iPad with his thumb.  The first time I saw it happen, I was using the feature that locks an app to be the only app he is allowed to use.  Next thing I knew he was out of the app.  I just thought the app crashed.  The next thing I knew he was able to unlock my iPad without my fingerprint.  And I watched him never try to type in my passkey.  After hearing the latest Security Now! episode, it finally all makes sense.  Thanks for a great show, and long-time SpinRite user.  Looking forward to its next release and SQRL."



And finally Phil said:  "Wow.  After listening to @SGgrc, I let my wife try to unlock my phone with her fingerprint.  Works about one in 10 tries.  Scary."  And then he said in a separate tweet:  "After removing a bunch of saved fingers, it stopped working.  Wonder what I got in there?"  So of course we know the more fingers you put in, the softer the matching will be because it's going to be not looking for one match, it's going to be looking for any of those.  So now you're taking an inherently fuzzy match and making it way more fuzzy so that it's way more tolerant because it wants to match any of the things it's got registered.  So if somebody was concerned, at the cost of the inconvenience of registering fewer fingers, that's clearly going to increase the security of your system.



But then NeoRenfield tweeted something that I thought was interesting.  He sent me a screenshot from his Android phone where Google says that the Pixel fingerprint reader "may be less secure than a strong PIN, pattern, or password."  So they acknowledge right upfront that there's fuzzy matching going on.



So the screenshot says, under the topic of About Fingerprint Security:  "We strongly recommend locking your screen to help protect your device.  Your device's fingerprint sensor gives you a convenient unlocking option.  But there are a few things to keep in mind:  One, a fingerprint may be less secure than a strong PIN, pattern, or password.  Two, a copy of your fingerprint could be used to unlock your phone.  You leave fingerprints on many things you touch, including your phone.  And, three, you'll be asked to add a backup PIN, pattern, or password.  Remember your backup because you'll need to use it sometimes, like after restarting your device, or if your fingerprint isn't recognized."



So props to Google for right upfront saying, yeah, it's convenient, but it's a fuzzy match.  So it's not like a long, strong password.  On the other hand, you always have it with you, and you don't have to memorize your fingerprint because it is you.



BlueLED sent me a link to Gorhill's documentation on uBlock, which I was unable to find.  And as we already know, Gorhill is a cantankerous developer.  He does things his way, for his own reasons.  And when I was putting the show together last week, talking about uBlock Origin, and reminding myself how his undocumented UI functions, I couldn't find the documentation.  So anyway, this BlueLED person sent it me.  Thank you.  I have the link in the show notes, which is an explanation of the various columns in that expanded UI and what they all do.



Steven Doyle asked:  "If ISPs were to start requiring certificate installation, would your HTTPS fingerprinting still indicate a man in the middle?"  And the answer is yes.  As our listeners know, I created that HTTPS Fingerprinting page specifically because the only danger or concern at the time was corporate middleboxes, as we're calling them now, which intercept HTTPS TLS connections and break open the encryption for the purpose of checking them for malware and content.  I never foresaw until recently the concern that maybe ISPs were going to end up becoming essentially like the corporate middleboxes for ISP end users, much as corporations are looking at all the traffic of their customers, or their employees, rather.



So this certificate inspection may end up being far more useful and have widespread purpose than I recognized at the time.  And so, yes, it will - the idea is that GRC has no overlord.  I'm getting an unfiltered connection direct from Level 3, a Tier 1 provider on the Internet backbone.  So I'm seeing the certificate from the website.  And so the question is, is the certificate that an ISP's customer's receiving, does it have the same fingerprint, the same hash as the one that GRC sees?  There's a potential for false, what would that be, false negative, that is, for GRC getting a different certificate and the user getting a different certificate from the user in some instances where that's the design of a large, huge website, like maybe an Amazon or a Google, where they're minting their own certificates from their own intermediate CA, which allows them the ability to do that.  In that case, there could be a difference.  But your typical website has one certificate that everybody receives.  And in that case there should be a match.



But in any event, if you get a match, you know it's definitely - that your connection from your ISP is definitely not being filtered.  And so, yeah, because of the way it is, it's detecting any interception of your connection, whether your employer, or perhaps in the future your ISP.



Chris Sullivan asks, he was experimenting with the "puny" Apple site that we discussed in SN-608.  And he says he found that LastPass did not get tricked and would not give his creds to a phony site.  And that is, of course, that's one of the nice things about these integrated password managers is they match on the actual ASCII, that is, the seven-bit domain name which is in the DNS in order to decide if you're at a site that they know about.



For what it's worth, SQRL gives you the same kind of protection.  It also would not be fooled.  What's being fooled is the user visually when the punycode is converted into Unicode and displayed as it's meant to be by browsers that do that.  And I think we're seeing the rapid end of any browser doing that.  Firefox, you know, I imagine they will get around to flipping that switch by default soon because this is just such a problem otherwise.



And Thomas Smailus asks:  "How was punycode ever anything but a bad idea if the DNS system doesn't also support it cleanly?"  And I agree.  This is, you know, here we have a problem of the original DNS, which has not changed, I mean, even from day one.  We're extending the records that DNS can serve.  We're trying to secure it with DNSSEC, but that migration is coming along slowly, as does all core fundamental changes to the Internet.  And the problem is it just - it was designed in the U.S. by people on Unix systems on minicomputers, PDP-11s with seven-bit code.  So the original RFCs are seven-bit ASCII.  So the only way to extend it was by creating some ad hoc, after-the-fact hack which would allow an expression of higher code point alphabets.  But it was going to be incompatible unless we did something like this.



So this was the best we could do, trying to layer something on top of a system that was never designed to have it.  And I don't disagree that it was ever not a bad idea.  It's unfortunate that this can be abused and that hackers are going to take advantage of it.  I think that just means that browsers are going to lose the ability to show people large character set domain names.  Maybe they'll show them both, you know, side by side or something.  I don't know.  Or maybe if the visual display doesn't match the domain, do it in a different color or something.  Who knows?  We'll see how this evolves.



Two last ones:  Martin Badke says of the keypad, which was our Picture of the Week last week, where the 1, 2, 3, and 4, the print, the ink was completely rubbed off.  He said:  "The garage entry keypad COULD have repeated digits.  Number of codes possible then would be 4^4.  Still sad."  He said:  "I've seen similar for cash safes."  And I would counter that, if the keypad had one digit repeated, which would be 4^4, then only one button would have the ink rubbed off of it.  So the picture showed four buttons rubbed off, and we know from those typical keypads they tend to be a PIN of three or four digits.  So it does look to me more like it was 24 combinations, rather than 4^4.  And what would that - that would be 256, it was 4^4.  Nah, I think it's probably 24.



And then, finally, P. Hoffman said:  "Possible mitigation for ISP snooping:  OpenVPN server in Amazon's EC2."  That's their Elastic Cloud, or Elastic Computing Cloud service.  He says:  "Less than a dollar a day.  Easy to change the IP address at will.  Your thoughts?  Thanks."  And that's not the first time I've heard that suggested.  And I think that makes a lot of sense.  In fact, I imagine that's really worth exploring.  You had the advantage then of not being tied to a VPN provider whose business is VPN, where you inherently have a high-density concentration of interesting, potentially interesting to authorities, traffic emerging from a single location, the VPN endpoint.  Instead, it's just Amazon.  And all kinds of traffic is coming in and out of Amazon.  And now they've got eight million fresh new IPv4 IPs to assign to their customers.



So the idea of running an OpenVPN server, spinning one up when you need one, I think makes a lot of sense.  You have to take a look at the economics in terms of cost and traffic and how the pricing works.  But I think it makes a ton of sense.  So, yes, if it can be done economically, I love the whole profile of that.  And that's our podcast.



LEO:  Perfect timing.  You're an amazing fellow.  You planted an arrow in your Encontabulator, figured out exactly - Turboencontabulator, figured out exactly where to fire it.



STEVE:  Yes.  The Entabulator by itself was the first version, and they decided they needed to beef it up a little bit.



LEO:  Turboentabulator.  You bet, yeah.



STEVE:  So it is the Turboentabulator, yeah.



LEO:  Got to watch that video again.



STEVE:  It's wonderful.



LEO:  You'll find Steve at GRC.com.  That's his home.  He has lots of great stuff there, including SpinRite, his bread and butter.  Got to have bread and butter if you've got a home.  All     you've got to do is go to GRC.com and go to SpinRite and buy it.  And that way, if you've got a hard drive, you can maintain it.  You can recover it, if you need to.  Awesome product.  You'll also find the podcast there, audio and transcripts.  And lots of other wonderful freebies.  You can ask him questions at GRC.com/feedback, for the show.  You can also tweet him.  @SGgrc is his Twitter.  He even takes direct messages, if you've got a tip.  He likes tips.



You can go to our website, TWiT.tv/sn, for video as well as audio versions of the show.  And of course everywhere, you know, you get your podcasts you could subscribe.  And we would love it if you would subscribe because that kind of evens out the download numbers for us, makes it easy for us to keep track of who's listening, how many, and all of that.



We do the show, if you'd like to watch live, pretty much always, well, it's always on Wednesday, pretty much always at 1:30, although that may vary.  Sometimes we're held up by previous shows.  So that's a rough estimate.  It's not a train station.  1:30 Pacific, 4:30 Eastern, 20:30 UTC.  Stop by on Tuesdays, not Wednesdays - see, I said it wrong already - Tuesdays, and join us for the conversation.  The chatroom is irc.twit.tv, and you're always welcome in there, too.



Meanwhile, we're going to let Steve go and get ready for Tech News Today.  Thank you, Steve.



STEVE:  Thank you, my friend.  See you next week.  



LEO:  Bye-bye.



STEVE:  Bye.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#610

DATE:		May 2, 2017

TITLE:		Intel's Mismanagement Engine

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-610.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  This week Steve and Leo discuss the long-expected remote vulnerability in Intel's super-secret motherboard Management Engine technology, exploitable open ports in Android apps, another IoT blows a suspect's timeline, newly discovered problems in the Ghostscript interpreter, yet another way for ISPs and others to see where we go, a new bad problem in the Edge browser, Chrome changes its certificate policy, an interesting new "vigilante botnet" is growing fast, a proposed solution to smartphone-distracted driving, ransomware as a service, Net Neutrality heads back to the chopping block (again), an intriguing new service from Cloudflare, and the ongoing Symantec certificate issuance controversy.  Then some fun errata, miscellany, and some "closing the loop" feedback from our terrific listeners.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Lots to talk about, including a bad bug in Microsoft's Internet browser, a zero-day that's been exposed already, a mess-up in Intel's chip that's been a problem for almost a decade and is a big security flaw, and a lot more stuff like that.  You know, it's a nightmare.  But we'll talk about all of it and how to protect yourself, next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 610, recorded May 2nd, 2017:  Intel's Mismanagement Engine.



It's time for Security Now!, the show where we cover your privacy and security online with this guy right here, Steve Gibson, the man in charge, GRC.com, and our security guru since 2005.  Hi, Steve.



STEVE GIBSON:  Hey, Leo.  Great to be with you again for Episode 610.



LEO:  Wow.



STEVE:  At the beginning of May.  And in fact it was interesting that our main topic occurred on May 1st because this is truly Mayday for Intel.



LEO:  Oh, boy.



STEVE:  The title of today's podcast is Intel's Mismanagement Engine.  So we're going to discuss the long-expected remote vulnerability in Intel's super-secret motherboard management engine technology.  Also there was a paper given at a European security conference, an IEEE conference, where five researchers from, I think it was the U. of Michigan, I've got it in my notes, took a look at something that no one had looked at closely before, and what they found was worrisome.  And that's exploitable open ports in Android apps.



LEO:  Oh.



STEVE:  We have another instance of an IoT device blowing a suspect's BS timeline.  And you can just imagine, when you read through the details of this, they're just so screwy.  I'm sure when the police came out to investigate they were, like, looking at each other, going, oh, come on, we're supposed to believe this?  There are some newly discovered problems in the widespread Ghostscript interpreter.  Yet another way for ISPs to see where we go online, another one that sort of escaped my overall summary, so I wanted to mention that.  There's a new bad problem in the Edge browser which is very worrisome and was disclosed irresponsibly by an Argentinian researcher, for which there's no fix, and there are proof-of-concept exploits, that allows - well, we'll get to that.  Bad.



Chrome is changing their certificate policy, which is interesting.  There's a very large and suspiciously well-designed new botnet growing in size that's being called a "vigilante botnet."  An interesting proposed solution to smartphone-distracted driving.  The emergence of ransomware as a service.  And of course we have to talk briefly at least about the concerning reversal by this new administration on the previous administration's position on Net Neutrality.  And this is worth rallying all of our listeners and everyone within reach as we approach the time for public opinion later this month, after May 18th, because it's back on the chopping block again.



There's an intriguing new service from Cloudflare for protecting IoT devices that we need to talk about and of course remind our listeners that they are a sponsor of the TWiT network.  There's of course then the ongoing controversy over the Symantec certificate misissuance and what the browsers should do to deal with it.  And as if that wasn't enough, we have some fun errata, some miscellany, and then a little bit of "closing the loop" feedback from our terrific listeners.  So, yes, 610 and going strong.



So I forgot to mention that we do have about a two-minute audio clip to play.



LEO:  Uh-oh.  Okay.



STEVE:  It's wonderful, following on from last week's very popular Turbo Encabulator.  However, whereas that one was a bunch of mumbo jumbo gobbledygook, this is actually completely mathematically correct, but wonderfully obtuse.  Anyway, it's missile guidance explained.  And it's just audio, so there's no visual, so it works in the podcast.



There's another one that I just tweeted out that's more like the Turbo Encabulator that involves the Muppets' Cookie Monster, which is - unfortunately it's completely visual, but it is just hysterically funny.  I had no idea the Cookie Monster could be so expressive.  Anyway, but that's not what we're going to show.  I put it in the show notes, and I tweeted it because - and you can also just google "Muppets Analytical Computer," and you'll find it on YouTube.



But the third link from the bottom of the second page, from the second-to-the-last page, the missile guidance explained, is just two minutes of audio that we will play when we get to it.  But I wanted to give you a heads-up.



So our Picture of the Week has been in my queue.  I have a backlog of Pictures of the Week, and nothing jumped out, so I pulled this one from the Security Now! backlog which I thought was interesting.  And it's apropos a story that we'll be getting to about the open ports which it's noting that that - this shows a graph of four different OSes:  Android, Windows, iOS, and OS X from March of 2012 through March of 2017, so essentially a month ago.  So five years during which time the percentage of Windows OS drops from - it looks a little higher than 80, down to 37.91%.



But during the same time, Android moves up from looks like around 4% up to 37.93 - in other words, higher than Windows.  More Android than Windows.  And during this same time iOS sort of putters along, slowly growing from it looks like a little more than Android, but now way behind.  It drifts.  It maybe goes up from 5% up to around maybe 12 or 13.  And OS X pretty much floats around below 10 and dropping down just a little bit.  But anyway, just sort of interesting that, as this graph demonstrates, Android is now "the most popular," at least in terms of instances in the world.  I'm not sure I would call that popularity, but in terms of count, since so many devices are Android-based.  But that's significant.  And of course it means that the security of Android, as we know, is important.



Okay, so this week's top story, I titled it "A True Mayday for Intel."  About a year ago - I looked back through the transcripts backlog, and I couldn't find the specific podcast where we discussed this in depth because we've discussed it many times.  We've touched on it many times.  And that's the so-called Intel Management Engine, which exists in a number of different forms.  There's something called IAM, which is Intel Active Management; SBT, which is Small Business Technology; and ISM, which is Intel Standard Manageability.  It's been around since, like, 2008 with the - and I had it written down.  I don't see it.  Is it the Kalem?  Or I can't remember which processor.



LEO:  Nehalem?



STEVE:  Yes, that's the one.  



LEO:  Nehalem, yeah.



STEVE:  From them through Kaby Lake so, like, up through just now.  And the ME has had different versions, this Intel Management Engine.  But versions from V6 through 11.6, which is current, are the area of concern.  The problem, as we discussed it before, is that this cannot be turned off.  There's no way to disable it.  You can't turn it off in the BIOS.  You can enable additional features; but the baseline set of features, there's just no getting around it.  It's a separate ARC processor that actually runs in one of the Intel chipset components that surrounds the main Intel processor that does all the memory management and I/O glue and slot management and so forth, providing USB and PCI functions and, you know, BIOS and all of that.  It's always running.



Intel has gone to tremendous lengths to keep it a secret.  So it's not open.  It's never been subject to scrutiny.  People have been for years worried about it and chipping away at it.  But as we discussed previously at length, Intel did it, like used every trick in the book to hide what this is.  And that alone is a concern, the idea that there is something in all motherboards for the last nine years, from 2008 on, which is outside of all non-Intel scrutiny.  The protocol is not documented.  It's available under NDA and has been licensed to, like, three companies.  Unfortunately, one of them is Symantec.  Let's hope they can do a better job with that than they have with certificate issuance.



So there are a few companies that provide enterprise access functionality that allow enterprises to manage their deployed machines throughout the enterprise.  So not just servers, but laptops, desktops, tablets, anything essentially with an Intel chipset for nearly the past decade.  And in fact our listeners will remember that I was pulling my hair out for a couple weeks.  I was bringing up a new Intel-based 2U server about a year ago, and it was causing - over at the Level 3 datacenter.  And I was getting these IP address conflicts.  There was an ARP storm, and interfaces were fighting each other.  And just nothing I could do.  I couldn't turn it off.  I couldn't figure out what was going on.



Finally, because these machines have multiple LAN interfaces, when I moved it from LAN 1 to LAN 2, that is, away from the primary NIC, all of that went away.  And I remember, when we were discussing all this at the time, verifying that only the primary NIC on multi-NIC motherboards has this IME interface, and that what anyone could do would be to switch to a secondary or tertiary, anything but the primary NIC, and you would be okay.  Because there's otherwise no way to turn this off.



Well, the other shoe has dropped, and we now have a confirmed exploit.  I mean, this is what everybody was worried about, and this is as bad as it gets because unlike current versions of OSes themselves, and to an even greater degree browsers, and to some degree even apps, the motherboard BIOS, while there may be patches available, there isn't an auto-patching mechanism.  And even though only enterprises really need or use this, this is the conundrum, is it's on and cannot be disabled through any means.



And while I was putting all this together, I dug into the background, thinking, you know, is there a scanner?  Is there some way that we could check?  The problem is, this is the problem with anything that is secret and proprietary and rigorously undocumented is nobody knows anything about this, except we now have a confirmed exploit.  I found one site where some guy for years has been pounding on Intel, telling them this is a problem, and they've apparently just been ignoring him.  



LEO:  That's what's really frustrating is that this has been known.



STEVE:  Yes.



LEO:  For years.



STEVE:  Yes.



LEO:  That's really frustrating.



STEVE:  And Intel's just, oh, that's not a problem, that's not a problem.  So suddenly it's Mayday.  They have issued just yesterday, on May 1, 2017, patches for all of their firmware for all of these motherboards.  And while that's the good news, the problem is they're all, I mean, like I've got, I just found, I just checked my Lenovo X1 Carbon.  It's got it, and it's in there, and it's running.  And I don't want it.  But so it'll be one - as soon as I'm through with the podcast I will see about whether - and I'll talk about it next week, what I find.  And I'll check GRC's Security Now! newsgroup because I'm sure that the people there will be interested in finding out whether there are firmware updates for their systems.



LEO:  How could you patch this?  Because isn't this in hardware?  I mean...



STEVE:  No, it is, I mean, it's "deep firmware" is probably the way to describe it.  So there are versions, and they do have patches.  But there is no - and an enterprise could deploy these patches.  But just so people understand, this is a rootkit.  I mean, that's the best way to describe it.  For my notes I wrote:  "Recent Intel x86 processors implement a secret, powerful control mechanism that runs on a separate chip that no one is allowed to audit or examine.  When these are eventually compromised, they'll expose all affected systems to nearly unkillable, undetectable rootkit attacks."  And the guy I'm quoting said:  "I've made it my mission to open up this system and make free, open replacements before it's too late."  And that's what we - we talked about this last year.  There is a project to replace this with a publicly available, open source solution.



This guy goes on:  "The Intel Management Engine is a subsystem composed of a special 32-bit ARC microprocessor that's physically located inside the chipset.  It's an extra general purpose computer running a firmware blob that is sold as a management system for big enterprise deployments.  When you purchase your system with a mainboard and Intel x86 CPU" - that is to say with an Intel chipset - "you are also buying this hardware add-on:  an extra computer that controls the main CPU.  This extra computer runs completely out of band with the main x86 CPU, meaning that it can function totally independently, even when your main CPU is in a low power state like S3, suspend.



"On some chipsets, the firmware running on the ME implements a system called Intel's Active Management Technology.  This is entirely transparent to the operating system, which means this extra computer can do its job regardless of which operating system is installed."  So it doesn't mean Windows.  It could be Linux.  It could be, you know, or like no OS, if it's just sitting there waiting to be deployed.  But it gets worse.



"The purpose of AMT is to provide a way to manage computers remotely.  This is similar to an older system called Intelligent Platform Management Interface (IPMI), but this is more powerful than that.  To achieve this task, the ME is capable of accessing any memory region without the main x86 CPU knowing about the existence of these accesses."  I mean, it is a classic hardware backdoor.  "It also runs a TCP/IP server on your network interface, and packets entering and leaving your machine on certain ports bypass any firewall running on your system."  This cannot be blocked by anything that you do running on top of it.



"While AMT can be great value-add, it has several troubling disadvantages.  ME is classified by security researchers" - and this is what we talked about at the time - "as Ring -3."  You know, normal apps run at +3.  Ring 0 is the OS.  Well, this is way beneath the OS.



LEO:  I didn't even know there was a ring that's a negative ring.



STEVE:  Ring -3.  "Rings of security," he writes, "can be defined as layers of security that affect particular parts of a system, with a smaller ring number corresponding to an area closer to the hardware.  For example, Ring 3 threats are defined as security threats that manifest in user-space mode.  Ring 0 threats occur in kernel level.  Ring -1 threats occur in a hypervisor level, one level lower than the kernel.  Ring -2 threats occur in a special CPU mode called SMM" - that's System Management Mode - "a special mode that Intel CPUs can be put into that runs a separately defined chunk of code.  And if attackers can modify the SMM code and trigger the mode, they can get arbitrary execution of code on the CPU."  But that's the main CPU.  And this is -3, even below that.



Okay.  So Intel rates this, their own problem, as "critical remotely exploitable."  I would love to know what external vulnerability this represents, but this is part of the problem.  The information is so blacked out that I could find nothing about how to scan for it, how to detect it, I mean, like anything.  But here's the concern is that systems will not update themselves.  Unlike browsers and OSes and many apps, the BIOS doesn't.  You normally need to go get it.



Now, people like Lenovo, who have tried to integrate system management, controversial as it is, I would imagine that, if you are using the Lenovo "keep your system up to date," I know that it does BIOS updates because I've been a longtime ThinkPad and then a Lenovo user.  I imagine that they could use that mechanism to push that out.  But if you've just got, in the last nearly a decade, any Intel system that has this IME, the Intel Management Engine technology in it, then we don't really have a way of gauging, you know, I don't want to run around, hair on fire, screaming that the sky is falling.



LEO:  Well, I mean, there's mitigation.  If you don't use the built-in network, but use your own network card, you're safe; right?



STEVE:  Correct, correct.  Yes.  



LEO:  You know, it has to be these managed computers.  It's in every Intel chip, but I don't - it's not - it's my sense it wasn't, the management engine wasn't enabled unless you have a managed system.



STEVE:  No, that's not correct.



LEO:  That's not correct, okay.



STEVE:  It's absolutely, it is absolutely enabled.  Now, one thing you can do on Windows machines, if you browse through your list of services, and I did this on my Win7 X1, and there it was, Intel Management Engine, a service running.  And that's the other problem is that this is also vulnerable to local exploit, not just remote exploit.



LEO:  So Intel says you have to have vPro technology for this, which is not all Intel chips.



STEVE:  Correct.  Good.  I'm glad you mentioned that.  Yes, that is right.  Wikipedia's page has already been updated.  It was updated immediately to be current about this.  They write:  "Currently, AMT is available in desktops, servers, Ultrabooks, tablets, and laptops with Intel Core vPro processor family, including Intel Core i3, i5, i7," and the Xeons.



LEO:  But that vPro was sold as an enterprise system.  So, I mean, I'm sure it's on your ThinkPad X1 Carbon because that's an enterprise computer.  But I bet you a lot of, I mean, I wouldn't assume - for instance, I have a Mac with a Nehalem processor.  I would assume - or actually with a Xeon, as well.  I would assume it's not enabled there because they're not vPro systems.



STEVE:  Well, okay.  So we probably need to not use the word "enable."  It's present.



LEO:  Available, right.



STEVE:  Right, right, right.  If it's present, it's on because we can't turn it off, unfortunately.  But right.  So maybe it's just not there.  I've got a bunch of links at the bottom of this about...



LEO:  What a mess.



STEVE:  There is something, you can google "Intel management engine verification utility."  I found that.



LEO:  Ah, there you go.



STEVE:  It's dated 2010, and Intel's page says it supports XP through Win7.  I'm sure it runs on Win10 because Win10 will run things that Win7 does.  So google "Intel management engine verification utility."  It's a small little - it's about half a meg, 500 and some K, a zip file containing six files.  You can run that.  And so that will check your machine locally to see if you have the Intel Management Engine present.  And it confirmed that my X1 Carbon did.  How to Geek has an article, "How to Remotely Control Your PC Even When It Crashes," where they go...



LEO:  But that's the point of the management engine.



STEVE:  Correct.



LEO:  It's an enterprise feature designed for the IT department to manage your system remotely.



STEVE:  Right.



LEO:  So it's not nefarious that it exists.  It's just...



STEVE:  Oh, no, no.  And I never meant to explain it.  The problem is Intel was absolutely secretive.



LEO:  Right.



STEVE:  And it was always a concern.



LEO:  Always, especially in the open source community.  They hated that this thing existed.



STEVE:  Right.



LEO:  It's just that you don't, you know, you buy an open source system, you build open source, and you still have some proprietary blob that you can't examine.



STEVE:  Exactly.



LEO:  And now all of their fears are proven true.



STEVE:  Correct.



LEO:  Terrible.



STEVE:  And then the last link is, it's an Intel community site titled "How to Find Intel vPro Technology-Based PCs."  And unfortunately it's not a simple way.  But that link, also in the show notes, it takes you, I think there are like four different ways you can check your system to see whether it has vPro technology and thus has this ME component.  And bottom line is it would be good to look for any firmware updates.  It's not something we all do all the time.  I mean, and even the firmware documentation generally says, unless you are actually having a problem that you know this firmware will fix, better just to leave well enough alone.



In this case, if you've got the problem - again, because it's so underdocumented, I can't gauge how exploitable this is.  I don't know, for example, what traffic this is sniffing on the primary NIC on a motherboard to know whether, like, how a remote exploit would be affected.  Because, for example, if it were some obscure port, and it had to come in on an obscure port, or if it was an obscure protocol, I mean, it could be anything that works in an Intranet.  I don't even know if it's for sure that it's routable.  That's the problem is it's just - it's a big black box.  But now we know Intel is calling it a critical, remotely exploitable vulnerability.



LEO:  Are there exploits that we know of?



STEVE:  No.  As far as we know, it's been done.  Now, I did run across anecdotal supposition.  But again, that's just all it is.  We call that now "fake news," I guess, in this era.  But there are people who are claiming that this has been exploited, but without evidence.  So I ignore that because Intel really did try to tighten this down.  I believe you have to have a certificate that the Intel Management Engine recognizes.  So traffic needs to be signed.  It is encrypted and secured with TLS.  So, I mean, it looks like the bar is likely very high.



On the other hand, Intel has called this critically remotely exploitable.  So hopefully, I mean, it's good news that they're finally responding.  I wish I could gauge its true exploitability for our listeners, but there's just no information.  So I will certainly be looking for more.  This just happened yesterday.  So again, as you said, Leo, even though a number of researchers have been saying to Intel, you know, tell us about this, and several people have been saying - in fact, at the very end of this report I have a site, SemiAccurate.com, which is not the most encouraging...



LEO:  Only half accurate, yeah.



STEVE:  Not the most encouraging domain name.  And the page is remote-security-exploit-2008-intel-platforms is in the URL at SemiAccurate.com.  And this guy just rakes them over the coals, saying that he's been pounding on them forever to, like, fix this, telling them, and they've just been ignoring him.



So anyway, I have some new 1U Intel servers that I will be deploying.  The one that I've already got in place has got its own external physical hardware firewall because it's where I will be bringing up the public GRC forums to support SQRL.  They're actually - they've been online for almost a year now, but I haven't taken them public yet.  And so it's already protected.



But, I mean, it is an Intel motherboard, and it's a state-of-the-art chipset, so I'm sure it's got this.  And I will be updating its BIOS before it sees the light of day.  And it's just - it's just frustrating for a researcher to be kept in the dark by something that looks like it's really important because how do you mitigate this if you know nothing about it?  But all we can do is respond by updating our BIOSes.  And, wow, hopefully - I don't know.  This needs to be a lesson somewhere about the danger of absolutely black, black boxes.



LEO:  Well, it's an opportunity at this point for AMD, and I hope AMD takes advantage of this.  There's been some encouragement for AMD to make sure that their new Ryzen processors are coreboot, libreboot compatible.  They'll have - they don't have this management engine, don't have any unknown blobs.  And I think a lot of people, certainly in the free software space, would be jumping onboard.  Could be very good for AMD.  I would - that's disappointing.  Disappointing, yeah.



STEVE:  Yeah.  So, okay.  Speaking of disappointing, five researchers at the University of Michigan have published their research, which went public last week during the IEEE European Symposium on Security and Privacy.  The research paper, and I've got a link in the show notes to the - yeah, it's like a 17-pager.  It goes on and on - but titled "Open Doors for Bob and Mallory."



I should mention we talked about Alice and Bob as being the standard characters that are used to, sort of in schematic form, to talk about two parties communicating securely.  Well, Mallory, by convention, is the man in the middle, thus man and Mallory, so "M" for Mallory.  And so this is "Open Doors for Bob and Mallory:  Open Port Usage in Android Apps and Security Implications."  And I'll just share their abstract where they pull all of this 17 pages down into the kernel, but summarizes beautifully what they found.



They write:  "Open ports are typically used by server software to serve remote clients" - of course we know that - "and the usage historically leads to remote exploitation due to insufficient protection."  You know, can anyone say "Mirai botnet"?  Of course that's open ports, or Windows printer-and-filesharing.



"Smartphone operating systems inherit the open port support, but since they are significantly different from traditional server machines in performance and availability guarantees, little is known about how smartphone applications use open ports and what the security implications are.  In this paper, we perform the first systematic study of open port usage on mobile platforms and their security implications.  To achieve this goal, we design and implement OPAnalyzer, a static analysis tool which can effectively identify and characterize vulnerable open port usage in Android applications.



"Using OPAnalyzer, we perform extensive usage and vulnerability analysis on a dataset with over 100,000 Android applications.  OPAnalyzer successfully classifies 99% of the mobile usage of open ports into five distinct families, and from the output we're able to identify several mobile-specific usage scenarios such as data sharing in physical proximity.  In our subsequent vulnerability analysis, we find that nearly half of the usage is unprotected and can be directly exploited remotely.  From the identified vulnerable usage, we discover 410 vulnerable applications with 956 potential exploits in total."  So just shy of a thousand exploits.



"We manually confirmed the vulnerabilities for 57 applications, including popular ones with between 10 and 50 million downloads on the official market, and also an app that is preinstalled on some device models.  These vulnerabilities can be exploited to cause highly severe damage such as remotely stealing contacts, photos, and even security credentials, and also performing sensitive actions such as malware installation and malicious code execution.  We have reported these vulnerabilities and already got acknowledged by the application developers for some of them.  We also propose countermeasures and improved practices for each usage scenario.



"To get an initial estimate on the impact of these vulnerabilities in the wild, we performed a port scan in our campus network and immediately found a number of mobile devices in two minutes which were potentially using these vulnerable apps.  We've reported these vulnerabilities to the relevant parties through the vulnerability tracking systems" - and they've now got registered CVE and CERT registrations.  "Some of them have been acknowledged.  We encourage readers to view several short attack video demos."  And there's a site that I have the links for here in the show notes.



So finally, under their "Threat Model," there are three ways these can be attacked.  They said:  "The threat to an app with open ports comes from the attackers with the ability to reach these ports.  In the design of popular smartphone operating systems such as Android, ports are reachable from both the same device, for example, another app or a script on the web page, and another host in the same network with the victim device.  Thus, compared to the majority of previously reported smartphone app vulnerabilities that only consider the threat from on-device malware, open port apps additionally face threats from network attackers," in other words, the local network attacks and web attackers, meaning malicious scripts, which is much more diverse and also of wider range.



"More specifically, in this paper we consider the following three adversaries:  Malware on the same device, a local network attacker, and malicious scripts on the web."  And I'll just note that they write:  "When a victim user visits an attacker-controlled website using their mobile device, malicious scripts running in the handset's browser" - or delivered through an ad - "can exploit the vulnerable open ports on the device by sending network requests, which doesn't require permission.  For each of these three threat models, we have prepared short attack video demos on our website to help readers more concretely understand their practical exploitation."



And so if we harken back to Firesheep, remember that the scenario there was in an unencrypted WiFi environment such as, for example, the often-used Starbucks example.  What Firesheep allowed was it was doing promiscuous sniffing of all the network traffic and parsing the nonencrypted, that is, the HTTP transactions.  Any query that, for example, a browser that was at the time, back then, logged into Facebook using HTTP, in order to maintain the persistent login, the session cookie was sent with every browser query back to Facebook.  So a passive sniffer of network traffic could grab that cookie and log in, essentially clone the logged-in session by itself sending that cookie, and they would be logged in as someone.



So that's what Firesheep did.  It was called Firesheep because it was a plugin that ran on Firefox that just - it was freaky.  If you ran Firesheep at Starbucks, down the left-hand column would come up the identities of people surrounding you in the coffee shop, and you could click on one of them and be logged in as them.  So those days changed because pretty much now all of those major services are HTTPS exclusively, and so all of those, all of that traffic is encrypted.



What this means - so relative to that, now we have a situation where Android apps running on Android devices are in many cases opening up listening ports for whatever reason.  And they're often open and left open and are vulnerable.  So a port scan within a nonencrypted WiFi environment like Starbucks will find those open ports and can often identify the apps and then exploit them.  In other words, we're always talking about how our contemporary desktop OSes now always have a firewall between the OS and the external Internet.  And so we have that line of defense.



Then we also have a NAT router that we're behind.  So that allows machines in the private network to communicate with each other.  But both the local firewall and the NAT router protect us from the public Internet.  The concern here is that it turns out, and what these researchers found, is a substantial number of Android apps are opening ports which are then vulnerable to local scan.  And in order to provide their functionality, they are not firewalled.  Now, you'll still be protected within your local network by the NAT router that bridges you to the public network, but this is still a large attack surface.



So anyway, I'm glad these guys did the research and have brought it to everyone's attention.  I once - I'm sure I talked to you, Leo, about how annoyed I was that iOS didn't make it easy for me to move things back and forth between my Windows-based desktop and an iOS device.  I found an app which is exactly like this, which brings up a web server or an FTP server - actually in this case I use it as an FTP server.  And I've got it now on several of my iOS devices.



I, of course, because I am very security conscious, I always make sure to turn it off when I'm not using it so that it closes that port.  But it's very handy.  I fire it up.  Like if I want to just grab a big photo, for example, without having to email it to myself because I don't have iMessage under Windows so I can't message it to myself.  So I'll turn this little server on, put the photo in it, and then I have a shortcut that allows my browser to immediately bring up the FTP server that the iPad is now hosting, and I can just click on it and suck the file right over.



And so this is one of those apps.  There is an Android app that is a WiFi filesharing utility.  And the problem is that port is open and makes this app vulnerable.  And as we know, on the first level is access to the server.  But then what we find is that the servers are not themselves secure.  So even if the server was password protected and trying to only offer limited things, unless, I mean, I would argue, unless it's been pounded on, it's almost sure to have some mistake made that could convert this into either denial of service, crashing the app, maybe the phone, depending upon where the server is running, how deep in the kernel the service is, or maybe give someone access to even more data.  And these researchers apparently were able to do that.



So I guess the takeaway for our listeners is, if you're aware that you are running any device like this, that is, any app in an Android-based device, be sure to turn it off.  Disable it when you're not actively using it, very much the same way we've always been saying turn off Bluetooth unless you need it on.  It's been a constant annoyance that every time Apple updates iOS they turn it back on again.  And so it's like turning it off if you just don't need it saves power and also reduces your attack surface.



And for people who are a little more technically savvy and curious, there are, and we've talked about them before, local port scanners, which - so you could, if you looked at - you could turn your Android device on, determine what its local LAN IP is.  That would be a 192.168 dot something dot something, typically.  And then, from a different machine, you could run a local port scan, scan all 64K, you know, 65535 ports of that IP from your machine and see if you find anything open.  The scan shouldn't take long, but it would give you an idea of what ports that Android device has open.



And then what's important to remember is that those ports are open wherever you go.  So as you roam around, and the device connects to various networks, all of those ports are exposed.  So doing a local port scan of an Android smartphone probably, this research would suggest, is very worthwhile.  If nothing else, if it shows nothing open, then you have the peace of mind of knowing that.  But if it surprises you, you'll want to find out what apps are opening what and for why because, if you can reach them from a machine through WiFi, anybody else can, anytime you're connected to a nonencrypted network.  And probably even a network you have to log into, although it wouldn't be as easy as doing a passive scan.  You have to work, bad guys have to work a little harder.  But this looks like potentially big nugget.



LEO:  All right, Steverino.  I have your audio lined up for whenever you want that, by the way.



STEVE:  Okay.  We'll get to it in a few minutes.



LEO:  Yes, yes.



STEVE:  Okay.  So we've had some fun talking several times about how IoT devices are in some cases, I guess "ratting out" would be the expression, their owners or their users.  We had that case where there was some strange death in a hot tub, and the owner of the house claimed to have no knowledge of what was going on, yet his IoT-enabled water meter showed some huge amount of water consumed between 2:00 and 3:00 a.m., presumably to wash the blood away.  And so as a consequence of that - and of course we've also talked in terms of law enforcement wanting to get subpoenas for anything that the Amazon Echo device may have overheard and so forth.  Well, we have another one.  



LEO:  This one's wild.  This one is wild.



STEVE:  It is so bizarre.  And this is why I was saying at the top of the show that law enforcement officials, presumably the police who showed up to respond to a 911 call, had to have just been eyeing each other, like how dumb does this guy think we are?



So as I understand it - and I'm not going to go through the whole story.  I've got the link here.  Sophos covered this in their Naked Security blog.  And the punchline is that a man's wife was murdered, shot by his .357 Magnum, which he had purchased some time before.  Their marriage had apparently been under stress for some time.  They had two sons.  They weren't getting along.  Apparently he was taking money from her accounts.  He had a girlfriend on the side, and he said to her that he would be leaving his wife.  When the police arrived, responding to the 911 call which he placed, one arm and one leg were zip-tied to the chair, or a chair, and his other arm, as I pictured it, was somehow zip-tied like up to his neck.  And it's like, okay, is it not obvious that he did this to himself?  But I guess, you know...



LEO:  I can't move.  I can't move.  Well, except for this arm.  And this leg.



STEVE:  And he tells a story about a large, 6'2 hooded camo-wearing intruder who was the perp behind all of this skullduggery.  Anyway, the point of all this is that - so he outlines all of this.  And then the police tap into social media and look for any other evidence.  And it turns out that his wife was wearing her Fitbit because she was planning on going out, I think it was yoga or exercise of some sort that morning.  And so it, of course, recorded, as the Fitbit does, all of her movements.  And the timestamped record that they were able to recover contradicted the husband's version of events by more than an hour.  Like there was a complete disparity in the timeline.



So anyway, yes, I guess people attempting to perpetrate crimes are going to have to be very careful about what IoT devices, whether they're baby monitors or Fitbits or anything which is monitoring the environment.  Not as easy as it once way to get away with stuff.



LEO:  No, and there's cameras everywhere; and, golly, it's just a lot harder than it used to be.



STEVE:  So one of our constant themes on the show is the problems with interpreters, how surprisingly, but almost understandably, difficult it is for interpreters to be secure.  The media interpreters, image interpreters, all of these fancy formats we have are interpreted, meaning that even the old TIFF, the tagged image file format, well, it's composed of modules with tags which label what the module contains.  And so an interpreter displays a TIFF image, or a PNG, or a GIF, or a JPEG, or an MPEG, or an MP3.  The MP3 is a compressed - it's compressed by having a representation of the audio, which is then an MP3 player reads that representation and reconstructs an audio approximation of the original sound.



So a classic 28-year-old interpreter, I mean, not quite as old as SpinRite, but it's been around 28 years, since 1988, is Ghostscript.  And it turns out that there are some serious problems in Ghostscript.  The security advisory that I could find - this has just happened, so it hasn't percolated out through all of the various places where Ghostscript is in use.  But, I mean, it is the go-to standard Postscript and PDF interpreter, which reads those high-level page descriptions and converts them into a raster image for display or printing.  And so the specific vulnerabilities that I saw affected essentially all recent versions of Ubuntu from 12.04 LTS all the way up through 17.04.  So if you're an Ubuntu user, the danger is, as we've often talked about with Adobe PDFs - there's another classic.  Adobe Reader, how many years of material has this podcast had thanks to Adobe's PDF problems?



Well, turns out Ghostscript may be entering the same sort of zone of having lots of problems.  A researcher, Kamil Frankowicz, took a close look at the latest release and discovered multiple significant vulnerabilities:  Ghostscript improperly handles parameters to the rsdparams and eqproc commands, which allow an attacker, exactly as was the case with Adobe's PDF Reader, to deliberately craft malicious documents that could disable OS protections and thereby allow and enable execution of arbitrary code; or, if they're not quite slick enough, cause a denial of service, that is, cause the application to crash.



He found use-after-free vulnerabilities in the color management module of Ghostscript, which could also at least cause a denial-of-service application crash.  He found a divide-by-zero error in the scanned conversion code in Ghostscript, which an attacker could again leverage, and multiple null pointer dereferencing errors which, again, could be leveraged for attack.



So this is the advantage of open source.  As we know, open source doesn't automatically magically make something more secure.  But it at least enables someone to examine the code and find problems.  This, of course, is the problem with IME, Intel's Management Engine, is they've chosen to keep it closed and protected to an insane level so that you can't even reverse engineer it.  It's all encrypted, and it decrypts on the fly into RAM in order to run in this hidden 32-bit ARC processor.  So there's, like, here's two different examples.



So neither open nor closed specifically says whether or not it's secure.  But with something like Ghostscript, which is open source, if somebody takes the time to look at it, to carefully read the code - and the people who wrote it can't do it.  It's just it's a fundamental law of the universe.  You just cannot see errors in your own code.  You have to have somebody else look at the code, wanting to find problems, and they just reveal themselves to somebody who's sufficiently skilled in finding these kinds of security vulnerabilities, you know, a Tavis Ormandy sort of guy.



So anyway, I would say Ubuntu users be on the lookout for updates to Ghostscript, and anyone else, probably any variant of Linux.  Debian had a problem with a licensing change because Ghostscript has been taken under the wing of someone who has moved the Ghostscript code from pure GPL to a variant of that, that I know that Debian had a problem with.  But I would imagine it's still prevalent.  And it's not clear whether these were newly introduced, or whether they've been longstanding.  But, I mean, it is the case that the oldest Ubuntu, 12.02 LTS, was subject to this.  So I would just say to our listeners, if you know that you've got Ghostscript around, if you're a Linux user and you're able to display PDFs, certainly Ghostscript, which is the display interpreter, unless you're running a version of Adobe's PDF for Linux, it's worth checking out.



Following up on our podcast from a few weeks ago on all the various ways that information leaked, we talked about what a VPN could do, what cookies were doing, first-party versus third-party and so forth.  Somebody a few weeks ago - and I put this in my notes, and unfortunately I can't give credit to him because it got away from me.  But thank you for reminding me that Server Name Indication (SNI), which is an extension that was added to SSL and TLS way back in 2003, is yet another way that where we are going on the Internet can escape.  We've talked about SNI in the context, not of a privacy concern, but as a feature enhancement in the past.



The way servers traditionally operated, they wanted to bring up HTTPS connections - or generically, more broadly, SSL or now TLS connections - is at the server side you would bind the server's security certificate to an IP so that any connection on that IP would be secured under the certificate bound to that interface, to that IP.  So essentially the IP represented that domain that the certificate covered.  The problem, of course, arose where you wanted multiple hosting.  You wanted multiple domains, all served from a single IP.



So in order to accommodate that, an extension needed to be added to the client hello packet, that is, the first packet going to the server after the TCP, the underlying TCP connection is brought up.  Then the client, like typically the user's web browser, it sends the client hello packet, which among other things lists all of the SSL or TLS protocols that it supports.  And remember, so that allows the server to look at the ones it knows and hopefully choose the best one from among those that they have in common.  And so then the server hello goes back to the client, saying this is the one I've chosen.  And then the client hello also has a nonce, a big random number, and the server also chooses one.  And so they exchange their nonces, and that allows them then to negotiate a key and so forth.



Well, part of the, in the updated spec, part of the addition in the client hello, the first packet the client sends, is the SNI, the Server Name Indication, which now all browsers, even the popular WGET, WGE Tool, command line file retriever tool, it knows about it.  Everything knows about it for years now.  It's been in all browsers for up to 11 years.  That packet has to be in plaintext.  It is not encrypted.  It can't be encrypted because it's before the encrypted tunnel is brought up.  In fact, that packet, and by reading the Server Name Indication out of the client hello, a multiple domain, a multiply hosted server can then choose which certificate to respond with based on the domain name that is in that packet.  So unfortunately it's unencrypted, can't be encrypted.  It's in plaintext.  And anybody sniffing traffic, even when everything else is encrypted, an ISP - we did talk about how an ISP could even, if you were encrypted, could see what your IP was.



Well, it turns out that, yes, not only that, but in the client hello packets which are labeled brightly so that servers are able to understand them, an ISP could capture those and look at the SNI, the Server Name Indication, in every outgoing client hello and see which domain you're going to.  So they didn't even have to do reverse DNS, and they don't have a problem with multiply hosted sites, not knowing which site you're going to at an IP, because the client hello tells them.



So I just wanted to add that, too, and thanks to our listener for reminding me that Server Name Indication is yet another way that our privacy leaks, despite everything we want to do.  And there's no solution for that.  All you could do would be to VPN yourself past your ISP's view, and then let your traffic out onto the Internet in some public location where you're not worried about it being captured and looked at.



Okay.  Now, this is the week of bad problems for some reason.  Mayday.  Yeah, this is really bad.



LEO:  It's so bad he can't even say it.



STEVE:  Microsoft Edge browser has a vulnerability, believe it or not, that is not patched, that was not disclosed responsibly, that allows arbitrary sites that you visit to steal your cookies and passwords for other sites.



LEO:  Well, that's kind of a flaw.



STEVE:  I know.  That's why I'm breathless.  A serious same-origin-policy bypass.  We've talked many times about how crucial it is that browsers honor the same-origin policy.  That's the idea that code running, like that came from a certain domain, cannot just arbitrarily go look at some other domain.  It's able to go make other requests of its own domain, that is, of its own origin, the same origin, but not others.  So the sandboxing, within-origin sandboxing, is crucial.



So this security researcher, he, like, focuses on browser security.  And in fact his domain is called BrokenBrowser.com.  He's found in the past some more than 500 vulnerabilities in browsers and gleefully reports them.  His name is Manuel Caballero, based in Buenos Aires.  And apparently he has no interest in responsible disclosure.  Doesn't seem to be any - no concern given to it.  He has posted proof-of-concept exploits.  He's got videos demonstrating it.



This affects Microsoft's premier Edge browser, for which there is no current patch.  And again, I'm sort of speechless.  This vulnerability can be exploited to allow an attacker to obtain a user's password and cookie files for their other online accounts.  It leverages some mistakes Edge's developers made in the handling of so-called "domainless pages" such as about:blank.  About:blank is a domainless page.  And it turns out that there have been problems in the past that Edge has fixed, and this guy found another one, a way around the previous fixes.  So it feels like there's a fundamental architectural mistake that was made in the Edge's design which they have now been patching because this just shouldn't be a problem.  And he keeps finding other ways around it.



So versions of proof-of-concept demos are hosted online at his site at BrokenBrowser.com.  And since you may not - and, I mean, they actually work.  So if you go there with a Microsoft Edge browser, you can have your other sites, your Facebook and Google and Amazon and so forth, password and session cookies shown to you.  But since you probably don't want to do that, he has also posted video demos which are available.  And all of this is now in the public domain.  He notes that the vulnerability can be customized to dump the passwords or cookies of any other online service, including Facebook, Amazon, and others.  The flaw affects only Edge because universal cross-site scripting and same-origin bypasses, as he writes, tend to be specific to individual browsers.  Well, thank goodness for that.  But this is still bad.



Because, as we know, modern ads deliver JavaScript code to browsers, attackers can leverage malvertising campaigns to automate the delivery of this exploit to thousands of victims or more.  Manuel explained that attackers are able to use malvertising to push their malicious code into cheap banners shown on popular sites.  If an attacker is hosted inside a Yahoo banner, and the user is logged into their Twitter account, that user can be owned with no interactions at all.  And this is true, and he demonstrates it.



So this is bad.  This is like the flipside of responsible disclosure.  It's unfortunate that this is now out in public, and apparently there's no fix.  The only thing I could suggest people do, I mean, if this is a concern, because there doesn't seem to be a workaround, I mean, look for workarounds.  I will certainly mention it next week.



LEO:  Or don't use Edge, would be the workaround.



STEVE:  I would say yes, not use Edge.  I would switch to Chrome and stay away from Edge until this gets fixed.



LEO:  Although, since that's the primary rendering engine on Windows 10, I wonder...



STEVE:  I know.



LEO:  ...how often it gets used without your intention.  Probably in email; right?



STEVE:  Yes, yes, good point.  Very good point.  So where are we?  This is Tuesday the 2nd.  So our May Patch...



LEO:  [Crosstalk].



STEVE:  Yes, our May Patch Tuesday will be next Tuesday.  It would be wonderful if Microsoft had time to fix this by then.  This has to have lit a fire under them because this is really bad.  And it sounds like it should be an easy fix.  Whatever it is this guy found, I would imagine they can patch their way around it and get it fixed.  And let's hope they do that.



So Chrome has made a change to the way they handle certificates that I thought was interesting.  They are deprecating in Chrome 58 their use of the subject name field, which sort of - or it's called the subject common name, which is the way the certificate identifies itself.  The problem is that the way the common name was originally defined was never very rigorous.  For example, I think I've got www.grc.com as my main certificate's common name.  But I noticed that, ever since I started using DigiCert, they also placed it, that is, the www.grc.com, in the SAN field, the subject alternative name.  And traditionally the subject alternative name, as its description sounds, are alternative names.



And this is, for example, how you can get one certificate from a certificate authority which can be used on, like, three different domains, where for example I might have GRC.com, www.grc.com, and media.grc.com.  You would have the common name would be one of them, and then the alternative names would be the other two.  But I noted that DigiCert, again, they're on top of their game, they were always putting all three in the subject alternative name.



Well, it turns out that the industry has been souring over time over the use of, like the actual value in the common name as standing for something.  So for a while Chrome's behavior was to prefer the subject alternative name field; but, if it was missing, then to fall back to using the common name, assuming that the common name would be the domain name that was bound in the certificate.  Although, again, it was never rigorously defined.  The format wasn't defined.  It's considered an untyped field, which makes everybody nervous.  Whereas the subject alternative name, being more recent, was rigorously defined from the start.



Anyway, so the change is significant.  That is, with 58 the fallback path is removed.  And from now on, so this represents really a change for the industry, the common name will still be in certificates.  But browsers moving forward, and Firefox is on the same trajectory, will no longer be using the common name for any use other than display purposes in the certificate.  The browser will no longer rely on that.  So the reason this could affect individuals is that a popular thing to do is to generate self-signed certificates.  And many of the original self-signing tools do not support subject alternate name fields.  They only self-sign.  And the domain that you're signing is in the common name.  Well, users of those certificates will suddenly discover that Chrome no longer honors those self-signed certificates.



The good news is there are newer tools for generating fully compliant certificates, but that may mean that people who have long-expiration self-signed certificates are going to need to create updated certificates, which Chrome from 58 on, and soon Firefox, will continue to honor.  So just sort of an interesting, again, something that Google, I think, is doing the right thing in doing in continuing to move forward and clean up a little bit of what's been done before.



We have a mystery botnet that's got a lot of people wondering what's going on.  It's a new IoT botnet that's being called the "vigilante botnet" which has been growing rapidly.  It was first spotted in October of last year, of 2016.  It's also known as Hajime, H-A-J-I-M-E.



LEO:  Hajime [crosstalk] botnet.



STEVE:  Hajime botnet.  What's puzzling people is that it is extremely well designed and sophisticated, with a robustness and feature set that surpasses its overtly malicious rivals like the Mirai botnet, for example.  It is expending a huge amount of effort to infect other IoT devices.  But unlike Mirai, once Hajime affects an IoT device, it closes the backdoors behind itself, securing those devices it has infected against further attacks.  It blocks access to ports 23, 7547, 5555, and 5358, which are known to be the most widely used vectors for infecting IoT devices.  And thus, at least temporarily, it sanitizes that device in its wake.



But rather than using the more common fixed command-and-control server architectures that we have been talking about, Hajime employs a decentralized peer-to-peer network to issue updates to infected devices, making it far more difficult for the botnet to be taken down - I would argue impossible - by anyone.  And what's also strange is that, when infected devices are also equipped with display terminals, every 10 minutes or so it displays a signed message describing its creators as, quote, "Just a whitehat securing some systems." 



LEO:  Uh, yeah.



STEVE:  And then it says, "Important messages will be signed like this.  Hajime Author.  Contact closed.  Stay sharp."



LEO:  Uh-huh.  Uh-huh.



STEVE:  It's like, okay.  So unlike Mirai and other IoT botnets, Hajime lacks DDoS capability.



LEO:  I like that you've adopted my pronunciation, Hey Jimmy.  Hey Jimmy.



STEVE:  Hey, Jimmy.  



LEO:  Hey, come on over, Jimmy.



STEVE:  ...lacks DDoS capabilities and other hacking skills or capabilities, except for the propagation code that lets one infected IoT device search for other vulnerable devices and then infect them.  Kaspersky's security researchers noted that, quote:  "The most intriguing thing about Hajime is its purpose.  While the botnet" - it never gets old.  "While the botnet is getting bigger and bigger" - now, we're talking 300,000 infected devices at this point, by the way.



LEO:  Wow.  Wow.  That system has a big network.



STEVE:  A third of a million, "partly due to new exploitation modules [so it's evolving also] its purpose remains unknown."  Kaspersky says:  "We haven't seen it being used in any type of attack or malicious activity.  Its real purpose remains unknown."



And Radware's write-up provided some additional interesting technical details.  They said:  "The distributed bot network used for command and control and updating is overlaid as a traceless torrent on top of the well-known public BitTorrent peer-to-peer network, using dynamic info hashes that change on a daily basis.  All communications through BitTorrent are signed and encrypted using RC4 with public and private keys.



"The current extension module provides scan and loader services to discover and infect new victims.  The efficient SYN scanner implementation scans for open ports on TCP port 23 and TCP 5358.  Upon discovering open Telnet ports, the extension module tries to exploit the victim using brute-force shell login, much the same way Mirai did.  For this purpose, Hajime uses a list consisting of the 61 factory default passwords from Mirai and adds two new entries, 'root/5up' [root is the username, 5up is the password] and 'Admin/5up' [admin is the username, 5up is the password] which are factory defaults for Atheros wireless routers and access points.  In addition, Hajime is capable of exploiting ARRIS modems using the password-of-the-day 'backdoor' with the default seed as outlined here.



"Hajime does not rashly follow a fixed sequence of credentials.  From Radware's honeypot logs they were able to conclude that the credentials used during an exploit change depending on the login banner of the victim."  I mean, this thing is top-drawer.  "In doing so, Hajime increases its chances of successfully exploiting the device within a limited set of attempts to avoid the system account being locked out, or its IP being blacklisted for a set amount of time.  Radware also suggested that the flexible and extensible nature of the Hajime botnet would allow it to be used for malicious purposes, including conducting real-time mass surveillance from Internet-connected webcams.  However, since Hajime has no persistence mechanism, as soon as the infected device is rebooted, it goes back to its previously unsecured state, with default passwords and the Telnet port open to the world."



Now, there's no evidence of this, but I wouldn't be at all surprised if this is, I mean, I would be happy, in a way, if this was the NSA.  That is, here we have a situation where Mirai brought down DynDNS last year because so many of these IoT devices were infected that a hugely powerful, what was it, it was 600GB or something, a ridiculous amount of traffic was able to be generated.  So now along comes a botnet which you cannot take down, that uses high-end, torrent-encrypted, BitTorrent system intercommunication with rotating password-of-the-day seed-based passwords that protects the devices it infects from subsequent infection, stays in RAM, doesn't hurt them, doesn't destroy them, but takes them essentially out of service and is, due to its architecture, incredibly hard to kill.



This feels like a well - I mean, and at the same time is a massive surveillance network, should the owner of this network choose to exploit these IoT devices.  We've never seen any evidence of this.  And reverse engineering of the implant demonstrates all it does is rapidly find other vulnerable devices, presumably those that have been recently rebooted, and reinfects them with it before anybody else can find them, in order to essentially cleanse this otherwise very worrisome IoT install base of this latent problem.  This, to me, this feels like a state actor who's solving this IoT problem for us.



LEO:  So does it give you any clues that Hajime is Japanese.  It's the first name of a boxing manga series, comic book series.  Hajime no Ippo.



STEVE:  Well, and I don't know who named it.



LEO:  Ah.  Maybe they saw that text string in there or something like that.



STEVE:  Could be.  I did not find any reference to where the name came from.



LEO:  Anybody in the chatroom speak Japanese?  Does Hajime mean anything in Japanese?  Hmm.



STEVE:  Oh, I do know what it means because it looked it up this morning when I got the pronunciation.  But now I don't remember.



LEO:  I like Hey Jimmy.  Hey Jimmy.  Ha, hey Jimmy, how are you?  Good to see you.



STEVE:  Yeah.  Ugh.  So we're all familiar...



LEO:  It means "beginning" in Japanese.



STEVE:  Ah, interesting.



LEO:  It's a Japanese martial arts term.  That's scary.



STEVE:  Yeah.  It is, a little.  So we're all familiar with the concept of a Breathalyzer, where if law enforcement pulls you over when you're driving, and believes that maybe you're intoxicated, traditionally you could blow into this device, and it would register your current blood alcohol level as indicated by the alcohol content in your expiration.  Well, NPR reports that legislation has passed in New York which may pave the way for a "textalyzer."



They write:  "If you're one of the many who text, read email, or view Facebook on your phone while driving, be warned:  Police in your community may soon have a tool for catching you red-handed. The new 'textalyzer' technology is modeled after the Breathalyzer" - except it's not - "and would determine if you had been using your phone illegally on the road.  Lawmakers in New York and a handful of other cities and states are considering allowing police to use the device to crack into phones because, they say, too many people get away with texting and driving and causing crashes."



I'm going to skip a bunch of this reporting, which is about a personal story of somebody who was involved in a distracted driver crossing the center line and causing an accident that resulted in a death and how difficult it was to generate probable cause to get a subpoena to pull the records.  But skipping down, it says:  "Even though New York and most other states ban texting and other kinds of cell phone use while driving, [this individual] Lieberman says those lawsuits are difficult to enforce.  The takeaway is our current law is a joke.  Lieberman, along with the advocacy group he cofounded, has been working with a company" - and we know them, Cellebrite, which are of course the people who...



LEO:  Oh.



STEVE:  Uh-huh.



LEO:  They make the cell phone suckers.



STEVE:  Exactly.  "Cellebrite has been developing the textalyzer.  It would be able to determine whether a driver illegally was using a phone in the moments before a crash.  Cellebrite engineer Lee" - and here's a name, Papathanasiou, P-A-P-A-T-H-A-N-A-S-I-O-U.



LEO:  Yeah, that's Greek, Papathanasiou.



STEVE:  Papathanasiou.



LEO:  Opa!



STEVE:  Papathanasiou.



LEO:  Papathanasiou.



STEVE:  "...demonstrated the device for lawmakers" - who could finally pronounce his name - "and reporters at the New York State Capital in Albany earlier last week.  He says a police officer just goes to the driver and attaches a cord to connect the device to the phone.  The driver doesn't even have to let go of the device.  Papathanasiou said:  'They simply tap one button.  It will process for about 90 seconds or so, then display what the last activities were - again, that could be a text message and so on [but also web activity and touchscreen use] - 'with a timestamp.'  The device would display a summary of what apps on the phone were open and in use, he says, as well as screen taps and swipes.  'For example, if it was a WhatsApp message or a call, it will indicate what the source was, the timestamp, and then what the direction of the communication was, so if it was an outgoing call versus an incoming call.'



"Papathanasiou says the technology still is not yet fully developed, but would be tailored to what's legal in each jurisdiction that approves its use.  And he insists that the textalyzer would only capture taps and swipes to determine if a driver was using the phone, that it would not download content, and that it would be able to tell if the driver was using a phone legally, hands-free.  In New York, the bill authorizing police to use the textalyzer has passed out of one committee and is pending in the next.  Lawmakers are interested in the device in New Jersey and Tennessee, and in Chicago as well as other cities, as they consider ways to get drivers to focus on the road instead of their phones."



So I'm a little suspicious of whether they can pull this off because they've demonstrated success in getting into phones, but at least at the moment that's not something that the phones support, although this is one of those things where it might not be surprising to see that kind of technology officially enabled in phones for exactly this kind of purpose.  I don't know.  But we do know that this distracted driving is a real issue.  I look at cars weaving on the road, and we've talked here many times about how lights will turn green, and no one's car moves.



LEO:  Right.



STEVE:  They're all taking a timeout, and they go, oh, oh, oh, and then off they go.  Yeah.



LEO:  Wow.



STEVE:  We know we have SaaS, S-A-A-S, software as a service.  Now there is RaaS, R-A-A-S, ransomware as a service.  Yes, for just $175, "a new ransomware service on offer from a Russian-speaking user is reputed to be a boon to less technically capable cybercriminals.  Going by the name Karmen, K-A-R-M-E-N, anyone can deploy this easy-to-use drop-in ransomware kit, without any need to understand how it works.  The security firm Recorded Future posted last week that a Russian-speaking user called DevBitox [D-E-V-B-I-T-O-X] has been advertising the ransomware in underground forums."



Karmen, the name of this ransomware, "is part of a worrisome new trend known as Ransomware as a Service.  It allows less technically skilled amateur hackers with little technical knowhow to inexpensively purchase access, in return for which they receive a complete suite of web-based tools to develop their own ransomware attacks.  In Karmen's case it offers an easy-to-use dashboard interface.  Buyers can modify the ransomware, view what machines they've infected, and see how much they've earned."  Yeah, I guess it was inevitable.  But ransomware is going to be with us for the foreseeable future.



And finally, the FCC has announced its plan to reverse Title II's enforced Net Neutrality.  The Verge reported, and this has got heavy coverage:  "The Federal Communications Commission is cracking open the Net Neutrality debate yet again with a proposal to undo the 2015 rules that implemented Net Neutrality with Title II classification.  FCC chairman Ajit Pai called the rules" - that is, the rules enacted in 2015, so we have a new FCC chairman in the Trump administration - "'heavy-handed' and said their implementation was 'all about politics.'"  I guess that may be true.  "He argued that they hurt investment and said that small Internet providers don't have 'the means or the margins' to withstand the regulatory onslaught."  Okay.



"Ajit Pai said last Wednesday:  'Earlier today I shared with my fellow commissioners a proposal to reverse the [what he called the] mistake of Title II [classification] and return to the light touch framework that served us so well [as he put it] during the Clinton administration, the Bush administration, and first six years of the Obama administration.'"



The Verge writes:  "His proposal will do three things:  First,  it'll reclassify Internet providers as Title I information services," which I happen to think is a mistake.  "Second, it'll prevent the FCC from adapting any Net Neutrality rules to practices that Internet providers haven't thought up yet.  And, third, it'll open questions about what to do with several key Net Neutrality rules, like no blocking or throttling of apps and websites, that were implemented and put in place in 2015.



"Pai said the full text of his Net Neutrality proposal would be published" - and presumably it was last Thursday afternoon because this statement came out last Wednesday.  "It'll be voted on by the FCC at a meeting on May 18th."  So a little more than two weeks from today.  "From there, months of debate will follow as the item is opened up for public comment."  And I think all of us listening need to take advantage of this and make sure that our representatives in Washington know what we feel about this.  "The commission will then revise its rules based on the feedback it receives" - let's hope that's true - "before taking a final vote to enact them."



And then The Verge continues:  "Strong Net Neutrality rules were passed in 2015 and have been in place for about two years.  Those rules reclassified Internet providers as 'common carriers' under Title II of the Telecommunications Act, which subject them to tough, utility-style regulations."  Amen.  "The FCC has previously mandated under Title II that Internet providers follow a few key rules:  no blocking of sites and apps, no throttling the speed of sites and apps, and no paid fast lanes.  The rules applied to both wired and wireless Internet providers and also gave the commission oversight of 'interconnect' agreements between Internet providers and big content companies like Netflix.  Internet providers have, of course, been unhappy about this, as they'd rather not have the FCC looking over their shoulder and limiting what they're able to do with their network.  They sued to overturn the rules, but so far the rules have held up in court.  But that may not last."



So once again, we have all of us little folks who are trying to keep the Internet open, yet we have major, huge, well-financed, deep-pocket ISPs that say they need the flexibility to do with their traffic what they want.  They're wanting to be considered information providers rather than common carriers.  And to my thinking, maybe a solution would be for them to bifurcate.  If they want to do some things, then split themselves into half so that there's a common carrier portion that carries the traffic, and then an adjunct that can offer content.  But mixing these things together is just a recipe for trouble.  And I just hope there is, yet again, another very loud outcry for keeping things as they are and keeping our ISPs regulated under Title II.



LEO:  Yeah, we fixed this last time.  Tom Wheeler wasn't going to do this, the chairman of the FCC at the time.  And he opened it for comment, and literally more than a million comments.



STEVE:  Yup.



LEO:  And it convinced him of the merit of choosing Title II.  That and the President's urging that he do so.  I don't - I think we're in a different climate, so we'll see what happens.  But we can make those millions of comments again.  I think we need to.



STEVE:  Yeah, it's absolutely worth doing, to re-voice our position.



LEO:  Yup.



STEVE:  Cloudflare, who as our listeners know has become a sponsor of the TWiT Network netcasts, has launched a new service.  And I don't completely understand it, but I'm sure we will with time.  It's called Orbit.  You can find out - there's a short write-up at www.cloudflare.com/orbit, O-R-B-I-T.  And it is a new service to protect IoT devices.  In their write-up they said:  "Technology is changing, shifting towards a world where low-cost connected chips power products used by billions of people around the world.  Everything from jet turbines and oil rigs to cars, cameras, and clothing are coming online.  And while these tiny chips unlock incredible potential, they are a liability if not secure."  To which I add, "Amen."



"When PC vulnerabilities are discovered," they write, "software vendors issue a patch, which end-users are required to download and install.  These patches keep PC software up-to-date and secure.  IoT devices also require patches, but the PC security model cannot scale to 22 billion devices.  IoT manufacturers often haven't built over-the-air update mechanisms and are terrified that updates will brick a user's device.  In the meantime, consumers never think about having to upgrade their Internet-connected 'toaster,'" Cloudflare has in quotes.



"Cloudflare Orbit solves this problem at the network level by creating a secure and authenticated connection between an IoT device and its origin server.  Orbit takes the Internet out of IoT.  Behind Orbit, devices are" - and then they have "I*oT."



"Orbit allows device manufacturers to instantly deploy 'virtual patches' and block vulnerabilities across all devices on the network simultaneously."  In other words, Cloudflare is imposing - and this is me speaking - Cloudflare is imposing itself as a proxy server network in between all of these IoT devices and the public Internet, or the server that is intended to serve the IoT devices that exists on the public Internet.



And then going back to what they say:  "This keeps malicious requests from reaching devices, buys time for IoT manufacturers to carefully QA their updates, and keeps devices from leaking data or launching DDoS attacks."  And I should mention also that that also allows them - that allows Cloudflare to put up firewall rules instantly to close ports or filter traffic, which are discovered to be vulnerable for IoT devices.  And it uses mutual authentication with client-side TLS certificates.  We of course are always talking about server authentication, where the server contains a signed certificate that asserts its identity.  They're suggesting the use of client-side certificates.



Now, this of course is questionable because you want to keep your certificates private.  And it's not clear how an IoT device can inherently keep its certificate private.  For example, GRC and any public server goes to great lengths to keep its private key, which is in its certificate, private.  So I don't know how you enforce that.  It's an increase in security.



A spokesman for Cloudflare said:  "Orbit sits one layer before the device and provides a shield of security, so even if the device is running past its operating system's expiration date, Cloudflare protects it from exploits.  And while devices may be seldom patched, the Cloudflare security team is shipping code every day, adding new firewall rules to Cloudflare's edge.  Orbit" - and this is something I didn't know, but it's already in place.  "Orbit has been built in collaboration with a number of IoT vendors and already protects over 120 million IoT devices.  It allows IoT companies to write logic on Cloudflare's edge and create firewall rules that are immediately updated to the Cloudflare Orbit layer for all devices, without having to write and ship a patch."  And I noted that - is it Eeros?  Eero, E-E-R-O?



LEO:  Eero.  Eero.



STEVE:  Those devices are being protected by Cloudflare Orbit.



LEO:  Oh, nice.  Another sponsor of ours.



STEVE:  Yeah.



LEO:  All the sponsors are working together.  I like it.



STEVE:  Yeah.  So this is not something that can be added afterwards, that is, this needs to be - an IoT vendor would need to decide they want to take advantage of this service.  And so they would work with Cloudflare to - so essentially Cloudflare sort of becomes a CDN for IoT firmware and also an Internet proxy for the IoT device to hide behind so that it creates - essentially, Cloudflare is giving IoT vendors their scalability in order to provide useful services and features to Internet-connected devices.  Which I think sounds like a really cool idea.  So it's not something that an end-user needs to worry about.  But it's something that IoT device manufacturers - it's a service they could use with Cloudflare to enhance the security of their IoT devices.



LEO:  Fantastic.



STEVE:  So cheesy things aren't going to bother.  But devices that care about security and are wanting to use this as a value-added benefit could avail themselves of this.



LEO:  Nice.



STEVE:  So bravo to Cloudflare and those device vendors who choose to use it.



And I just did want to mention that Mozilla and Chrome are continuing their back-and-forth with Symantec.  I read through a mind-numbing dialogue of we said this, and they said this, and then they proposed this, and we read that, and then we proposed that.  Symantec is, of course, pushing back as fiercely as possible and wanting to do as little as possible.  And this of course is, as we've discussed in previous podcasts in response to rather gross violations of the responsibility of a CA, which Chrome and Mozilla have both decided they're going to take action, that they're not going to let this stand.  Things like removing EA certificate issuance completely, enforcing short certificates moving forward until Symantec proves and takes clear measures to demonstrate that they will be responsible moving forward and so forth.



So browsers, the browser vendors, are attempting to both appear and be understanding and reasonable, while also feeling that their true responsibility lies with their users, who are inherently trusting their browsers to keep them safe.  So, I mean, this is one of those situations where there's going to be pain.  And now there is an ongoing struggle to decide where the line gets drawn.  So I just did want to mention this is ongoing.  I'm kind of keeping an eye on it, but we don't have any conclusions yet.



Some bits of errata:  Irwin Wessels shot me a tweet saying "There are not individual emojis for each skin tone variety.  They're modifiers/ligatures."  And so that prompted me - so thank you, first of all, Irwin.  That prompted me to dig in a little bit.  And believe it or not, we now have emoji racial diversity.  It is in the unicode spec under "diversity," if you can believe that.  Of course we had the original sort of Smurf yellow emojis.  Those were the ones I always used.  And I misstated last week that now we had, like, an explosion of emojis.  But apparently the unicode space was big enough to accommodate them.  And we also talked last week about, yes, there are, what, 17 million, more than 17 million - wait, no, it was 17 planes of 65K.  I don't remember now how many million.  But there was, like, plenty of unicode space.



Well, it turns out that what was actually done was that a skin tone modifier was added to the unicode spec quite some time ago, in Unicode v8, back in mid-2015.  And so we still have the original Smurf yellow, but then we have five human skin tones ranging from white through dark, kind of light white to dark brown.  And that's an escape character, essentially, which can be appended to any of the existing Smurf yellow emojis to turn them into one of five skin tones.  So thank you for the correction, and I find that kind of interesting.



Oh, also I misspoke when I referred to the EternalBlue and Eternal, you know, the various Eternal* things, and DoublePulsar, as being Vault 7, thus CIA leaks.  I meant to say, and they are from, the NSA's Equation Group, which was released by Shadow Brokers.  And I think you actually corrected me, Leo.  But somebody else noted that, as well.  Actually, several listeners, so thank you for that.



And then this is not quite errata, but this is under the category of "There's got to be a simpler way to say that."  Joel Dittmer quoted me regarding punycode.  He said:  "Classic @SGgrc."  Apparently I said:  "I don't disagree that this was never not a bad idea."  What?  I don't disagree...



LEO:  Double negative.



STEVE:  ...that this was never not a bad idea.  Is that a double or a triple?  Anyway...



LEO:  Never not a bad idea is a good idea.



STEVE:  I don't disagree that this is never not a bad idea.



LEO:  It was always a good idea is never not a bad idea.



STEVE:  So, so there.  I'm not sure what that means.  Anyway, under Miscellany, I did want to point our listeners to BadSSL.com, a cool and quick little website that checks your browser client for its feature set, shows whether it supports, continues to support old things that it should not, whether it supports new good things.  You need to - it's not quite obvious when you bring up the page.  You need to click on something in order to make it run the tests.  Then a bunch of things spin around, and then it shows you what your browser is doing.  So BadSSL.com.  And at the bottom of that page they remind us about that our friend Ivan Ristic, also over at SSLLabs.com has a very nice client-side SSL test with a number of different features, as well.



I talked about the idea of bringing up a VPN on Amazon's EC2.  And I just saw, as we were starting the podcast, that there's a formal project for that.  But there is that, but there's also you're able to set up a proxy server so that you can run all of your stuff through an Amazon EC2 VPC instance.  So I've got a - I just wanted to make a note of that.  And the link for the steps to do that, just 10 steps you run through, is on GitHub.



Several people said, after my mentioning ChromaZone, the very first thing I wrote for Windows, the tool that I used to teach me how to program Windows, Jame, I guess it's Jame Bong, said:  "I desperately need to play around with ChromaZone.  How do I get it?"  Many other people said the same thing.  I'll publish it all publicly.  I'll put that page back online and put links to the code so people who can set a machine to 256 color mode can play with it.  I'd be happy to have people do so.  I'm really proud of it.  It's sort of my masterpiece of Windows programming.  And now, Leo...



LEO:  I'm ready.



STEVE:  Two minutes of technically completely accurate - this is not gibberish as the Turbo Encabulator was.  And I know our listeners will love this because it is technically accurate description of missile guidance.  But it is wonderful.  So everybody listen up.



[Audio Clip]



MALE VOICE:  The missile knows where it is at all times.  It knows this because it knows where it isn't.  By subtracting where it is from where it isn't, or where it isn't from where it is, whichever is greater, it obtains a difference, or deviation.  The guidance subsystem uses deviations to generate corrective commands to drive the missile from a position where it is to a position where it isn't; and arriving at a position where it wasn't, it now is.  Consequently, the position where it is is now the position that it wasn't, and it follows that the position that it was is now the position that it isn't.



In the event that the position that it is in is not the position that it wasn't, the system has acquired a variation, the variation being the difference between where the missile is and where it wasn't.  If variation is considered to be a significant factor, it, too, may be corrected by the GEA.  However, the missile must also know where it was.



The missile guidance computer scenario works as follows:   Because a variation has modified some of the information the missile has obtained, it is not sure just where it is.  However, it is sure where it isn't, within reason, and it knows where it was.  It now subtracts where it should be from where it wasn't, or vice versa.  And by differentiating this from the algebraic sum of where it shouldn't be and where it was, it is able to obtain the deviation and its variation, which is called "error."



LEO:  It's just algebra.  Very straightforward.



STEVE:  Yes.  So anyway, I loved how pedantic that is.  And it's technically correct, so...



LEO:  It's accurate.  It's just 1 minus x2, yeah.



STEVE:  In the show notes, and I tweeted this, is something that does not work over the podcast, over an audio podcast.  But believe me, it is so good.  Four minutes and 10 seconds, the Cookie Monster, the Muppets' Cookie Monster consuming a machine as it describes itself.



LEO:  Somebody said this is actually Ed Sullivan.  It's that old.



STEVE:  Oh.  And I had no idea that the Cookie Monster could be so descriptive.



LEO:  Yeah, yeah.  Yeah, this is "The Ed Sullivan Show," I gather.



[Video clip]



LEO:  Yeah, you really should watch the video.  Jim Henson.  That's pre- "Muppets Show," I think.  He's eating it.  



[Video clip]



LEO:  He's eating it.  All right.  You get the idea.  I encourage our viewers to [crosstalk].



STEVE:  It's so good.  It's just fabulous.



LEO:  Yeah.



STEVE:  And it's got a punchline.  It keeps going, and it gets better.  So really it's worth it.  I tweeted it.  And again, google "Muppets analytical computer."  If you google "Muppets analytical computer," you'll find it.



LEO:  Wow.



STEVE:  Another listener tweeted:  "Got the first Frontier Saga book.  Let me tell you, wow, amazing, and I haven't been able to put it down."  And that echoes the sentiments I'm getting from many of our listeners, so I'm glad to have another recommendation that people are enjoying.  And I heard you refer to it also over the weekend.



LEO:  Yeah, John's been reading it.  I bought the cheap three volumes on Kindle, although I notice it is available on Audible, so if I like it I might go to the audio version.



STEVE:  I think you played it, and it was a little nasally for you.



LEO:  Yeah, yeah.  



STEVE:  You didn't like the...



LEO:  That's right, yeah.



STEVE:  You didn't like the guy who was reading it.



LEO:  I'm enjoying reading it.  Occasionally I need to read letters on paper, or something like paper.



STEVE:  Yeah.



LEO:  Yeah.



STEVE:  Brent Longborough is a long-term Twitter finder and sender.  I recognize his name.  Anyway, he said:  "Hi, Steve.  After years of shameless freeloading off a friend's SpinRite, today I purchased my own copy.  Thanks and apologies."  Brent, no apology needed.  You're legit, and I'm glad to have you.  And you'll be able to play with 6.1 during its early pre-release stages, as will everyone who has 6 at the time.  And, gee, I don't know how to pronounce this, Engrpiman said:  "@SGgrc SpinRite saved my database server.  RAID 1 disk failed.  Used SR to bring drive back to life.  Made backup, got new drives, restored from backup."



LEO:  Wow.  That's like a whole novel in 140 characters.  A beginning, a middle, and an end.  Conflict, resolution, the whole thing.



STEVE:  Got the whole thing.



LEO:  The whole thing.  I'm thinking Engineer Pi Man.



STEVE:  Oh, nice.



LEO:  But I don't know.  It's like reading license plates; right?



STEVE:  Yeah, it is, yeah.  



LEO:  E-N-G-R-P-I-M-A-N.



STEVE:  And then four little bits of feedback from our listeners closing the loop.  John, @Mr._John_Morris, said:  "Listening to SN-607 and thought I'd share the reality of Chrome cookie settings, not respecting settings."  And then he sent me a photo where actually it was two side-by-side photos where he showed he had disabled cookies in Chrome, or third-party cookies, or first, you know, some cookies, and then went over and looked at them and found them.



And I was familiar with this.  I wrote back to him, and I wanted to share with our listeners, some browsers will continue to store, but not to send cookies.  Some will even continue to receive an update, but not store or not - I'm sorry, but not send cookies.  This is exactly why I created that Cookie Forensics page because, if you look there, you'll notice it shows like how stale the cookies are.  We were talking about this a couple weeks ago when we were talking about stale cookies.  Because back then when I created it, sometimes you could turn off cookies, and some browsers would send the cookies they had, but wouldn't update them newly.  So then they were stale.  Some browsers continued to update them, but didn't send them.



So the fact that the browser still has the cookie and didn't delete it doesn't mean that it's still sending it, but it may just have it.  And then, if you were then to reenable cookies, it would immediately start sending the cookies that it had.  The point is this is very complex, potentially very complex and unintuitive behavior.  But the Cookie Forensics page, because it actually runs through multiple cycles of exchanging cookies back and forth and analyzes the whole set of transactions, it's able to weed everything out and then summarize everything for exactly the way the browser is working.  So you can't - it's not something you can statically inspect.  You need to see how it actually acts in vitro, or vivo, wherever that would be.



Richard Hardy tweeted:  "Watching the Security Now! DoublePulsar episode, I had a thought about the port 445 being open.  What about PCs in a DMZ?"  And so that's a very - so the point was I was wondering, how could there be so many systems with 445 open?  And he notes, well, if you set up a DMZ on your router, by definition unsolicited incoming traffic is not dropped, it's sent to that IP.  And it's true.  If you then had a machine that either didn't have its own local firewall running, and probably 445 would have to be a Windows machine or a Linux machine that had Samba running on it, if it didn't have a firewall or did have 445 open, then, yes, it would be vulnerable to the SMB.



The good news is all Windows systems since Windows XP SP2 have had a software firewall in and enabled by default.  So even that would protect you unless something on the machine had poked a hole through it on purpose.  So again, the DMZ is worth noting because that does certainly create a vulnerability to the machine that is the recipient of unsolicited traffic, very much like having it on the public Internet.  But at least Windows machines where the 445 port problem is the most acute probably are protected themselves.



Joan tweeted:  "There are ads related to my location popping up on my Facebook feed.  How do I stop this if setting up a VPN isn't enough?"  And Joan, I guess I would refer you back to our podcast a couple weeks ago about the real privacy protection, the second one, where we actually finally made time to get to that, because this is the problem is that even if you VPN, then where you go still knows who you are because that's conveyed through your cookies.  So what you would need to do would be to turn on incognito browsing and VPN so that your IP would change, and your browser would then respect your privacy in whatever your browser's version of incognito mode is.  Then you should not be known.



So those two things, turning on incognito mode and use a VPN.  Suddenly your IP will change and your browser should then, I mean, maybe fingerprinting would still be a problem with the browser.  I'm not sure, browser by browser, how good a job they do.  But they'll at least block cookies so that you are no longer logged in, and the places you go don't obviously know who you still are, even though you're coming from a different location.  Which is the only thing a VPN really does for you is just shift your location to where the VPN's public presence is.



LEO:  You can also, in many cases, a browser has a setting for location sharing.  You should do that and everything you just mentioned.



STEVE:  Correct.  



LEO:  Because if the browser's giving up your information, it kind of doesn't matter what you do. 



STEVE:  Right.  That's a very good point.  I've got mine set up, I don't know if it's the default, but mine sometimes [crosstalk].



LEO:  It usually asks; right?  Yeah.



STEVE:  Yes, and they prompt, you know, do you want this site to know where you are?



LEO:  You can in the settings then go look and see who's got permission and who doesn't.  And you can change the settings, say never do it, yeah.



STEVE:  Nice.  And lastly, finally, Nate G. says, actually in a pair of tweets:  "The biggest roadblock with getting friends and family onboard with a password vault solution has been the master password.  Any suggestions?"  And he continued:  "My wife, for example, sees the benefit of them, but has had problems remembering a high-entropy password in the past.  Unwilling to try again now."  And so, you know, empathizing with him, I wrote back and I said:  "Nate.  Best advice would be to use maybe, for example, five memorable real words with one deliberate misspelling.  Not the best solution possible, but not a bad compromise."



LEO:  I'll tell you what I do.



STEVE:  Okay.



LEO:  That I find very easy, and I tell people to do this.  If you have a poem memorized or a song lyric memorized, let's say "The Jabberwocky."  "Twas brillig, and the slithy toves did gyre and gimble in the wabe."  You don't want to use the words, but you could use the first initials of each word.



STEVE:  Right.



LEO:  Perhaps adding punctuation and uppercase and lowercase letters, depending on some arbitrary rule that you conceive of.



STEVE:  Or maybe even a comma where there would be a pause.



LEO:  That's what I would recommend.  I don't want to say what I do, but that's what I would recommend.  And then, to make it long and strong I then add numbers, either say a child - some number that you remember.  You know, the one I always tell people is the childhood phone number because that's something that probably is not on record anywhere, but it's something you also memorized as a child; right?  That was like the most important - or your childhood zip code or address.  That gives you an additional padding.  And the two combined I think would be fairly strong.



STEVE:  I think that's good.  And I'll bet that, after a while, you begin to memorize it.  I've got some gibberish that I've been using for, you know, in safe places for quite a while.  And I just type it without even thinking now.



LEO:  It was the best of times, it was the worst of times.  You know, people memorize stuff.  And there's usually a phrase or two.  And what's the length?  I would say 15 to 20; right?  Just keep going until you have 20 words and then add a phone number.



STEVE:  I would say shorter than the Canadian national anthem.



LEO:  You heard me trying to remember that.  Well, but there's a good example.  If you remember the national anthem, maybe not yours, some other country's national anthem, that'd be a perfect example because in your head you could actually sing it and type your password.  Not the words, just the initials.  I think the words might be a little bit brute-forceable.  I don't know.  Do you think bad guys have initial brute-forcers?



STEVE:  I think that in general the password vaults are, as we know, are generally safer against brute force.  



LEO:  Yeah.



STEVE:  For example, we know that LastPass does a strong, what is it, 500 or more now...



LEO:  A thousand, I think.



STEVE:  ...iterations of PBKDF.



LEO:  PBK - yeah.



STEVE:  And so it makes it difficult to brute-force it.



LEO:  To brute-force it even if it were a bad password; right?



STEVE:  So it's certainly, yes, so it's certainly better to use a bad password with a password vault than, like, I mean, this makes me wonder how good the passwords Nate's wife is using in general are.



LEO:  Yeah.



STEVE:  She's probably using, like, not, I mean, the vault allows you to just reduce it to a single one gnarly password.  But frankly, the attack profile is such that it doesn't have to be, you know...



LEO:  That gnarly, yeah.



STEVE:  That gnarly, yes.



LEO:  Okay.  That's good to know, yeah.  And then turn on two-factor, and then it's really gnarly; right?  Because...



STEVE:  Yup.



LEO:  Yeah.



STEVE:  Yup, that would be perfect.



LEO:  LastPass supports that, so that's - I always do that, too.  Steve, we're done.



STEVE:  Oh, my goodness.



LEO:  That was a quinti venti latte day if I ever...



STEVE:  Big bad news week.  Wow.



LEO:  Lots of it.  But we'll do it again.  How about this?  Do you think there'll be enough to do a show next week?



STEVE:  Oh, I'm afraid there will be.



LEO:  Patch Tuesday?



STEVE:  If nothing, following up on some of these nightmares.



LEO:  We do the show every Tuesday, 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  Stop by.  Say hi.  Watch live.  It's everywhere.  On our website, TWiT.tv/live.  YouTube has it, YouTube.com/twit.  Ustream, Twitch, we all have TWiT channels.  And then join the chatroom, too, irc.twit.tv because that's like the cool kids in the back.  And there's a lot of, you know, you don't watch the chatroom.  I know you've been in the chatroom many times, but you don't watch it during the show.  But there's always lots of stuff, you know, comments.  It's great.  Research.



If you can't watch live, on-demand audio and video is always available at our site.  And Steve's got audio and, uniquely, he's got human-transcribed transcriptions of the show.  Elaine Farris does a great job.  So you can go there and read along with Steve.  And you know, while you're there, pick up a copy of SpinRite.  It wouldn't hurt you to give Steve a little money, support him:  GRC.com.  He's got a lot of free stuff there, too.  And then you wouldn't feel guilty using that and listening to the show; right?  And we'll be back next week.



STEVE:  And you'll be able to get the pre-release of 6.1 as it's coming along, before everybody else.



LEO:  And is it coming along?



STEVE:  As soon as SQRL's behind me.  And we're making great progress on SQRL.



LEO:  Good, good.  He's going to get that SQRL back.  If you're just tuning in, don't worry, just hang out with us a while, it'll all make sense.



STEVE:  SQRL.  SQRL.



LEO:  SQRL.  SQRL.  Thanks, Steve.  We'll see you next time on Security Now!.



STEVE:  Thanks, Leo.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#611

DATE:		May 9, 2017

TITLE:		Go FCC Yourself

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-611.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  This week Steve and Leo discuss much more about the Intel AMT nightmare, Tavis and Natalie discover a serious problem in Microsoft's built-in malware scanning technology, Patch Tuesday, Google's Android patches, SMS two-factor authentication breached, Google goes phishing, the emergence of ultrasonic device tracking, lots of additional privacy news, some errata and miscellany, actions U.S. citizens can take to express their dismay over recent Net Neutrality legislation, and some quick closing-the-loop feedback from our terrific listeners. 



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  There is so much to talk about.  A little more detail on how, kind of surprisingly, how bad that Intel AMT bug is and how poorly - in fact, this could be the show of just bad programming all around, including a missile that's got twice the hardware memory because memory leaks, why fix them when you can just double the memory?  Yes, I'm not kidding, it's Security Now! coming up next.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 611, recorded Tuesday, May 9th, 2017:  Go FCC Yourself.



It's time for Security Now!, the show where we cover your security and privacy online.  This is our guru, our sensei, our doctor of security, our Leonard Nimoy of security, Steve Gibson of GRC, Gibson Research Corporation, and a man who's been around this block a few times.  Hi, Steve.



STEVE GIBSON:  Hey, Leo, great to be with you again.  So on Sunday John Oliver on his "Last Week Tonight" show gave us a shortcut to an otherwise very difficult-to-find page on the FCC.gov website.  The domain that they chose is characteristic and typical of John Oliver and that.  It's also the title of this week's podcast:  Go FCC Yourself.



LEO:  But it's a double meaning because you can actually go yourself to this FCC page now.  This is a URL shortener, or a redirect.



STEVE:  Correct, correct.  Yeah, exactly.  They established a redirect to help people find it.  Shortly after the show aired, the site went down because...



LEO:  Everybody went there.  As it always does when John Oliver points to your site, yeah.



STEVE:  Exactly.  So we'll talk, we'll rap this up this week by talking about that.  And also the EFF has a much more polite domain called DearFCC.org.  So I would recommend that our listeners who care about and are concerned about the potential loss of Net Neutrality pending the comments that we discussed last week, might want to go to both GoFCCYourself.com and DearFCC.org and give them some short comments.  The FCC site, the GoFCCYourself.com page, looks like - I don't know if the site is slow because it's so busy.  There isn't a way to get a total number of comments.  You can see over on the left there, Leo, that the 17 dash whatever that is, 108, I think, that will take you to the history of past comments.



LEO:  Is it open for comments now?



STEVE:  Yes, yes.



LEO:  Oh, my.



STEVE:  And if you go to the prior page, you type that +Express link, and that will take you to a short form that you fill out to demonstrate that you're a U.S. citizen and that you're going to be affected by the legislation and so forth, and then submit it.  But unfortunately, you can't get a total.  We can't see how many there are.  I sorted by submission date backwards and saw that the comments began appearing on April 27th.  And if you look at the most recent ones, they're all today.  So they've been - comments have been submitted, but there isn't a way to know how many hundreds of thousands of them there are.  But I think the message is getting through.



LEO:  I'm filing right now.  I have some comments.  I got some comments for you, FCC.



STEVE:  So we naturally have a lot to talk about.  We're going to talk about the updates on the Intel AMT nightmare.  Tavis was joined by a co-Project Zero worker, Natalie, and revealed a horrific problem that he found - finally he's turned his sights away from LastPass, thank goodness, for a while, aimed them at Microsoft, and found an incredibly bad flaw which is patched today, in today's Patch Tuesday for May, making this a very important thing to do because Tavis, as soon as the patch became available, presented full disclosure.  And we'll talk about what that is.  And this is a classic problem of security software making your system less secure by creating a new attack surface.  And what's fun, I love this, you can't even download the proof of concept because the Windows scanner crashes when it sees the proof of concept come in because of course it's looking at everything that comes in, thus the problem.



Anyway, we got Google's May Android patches.  A bad problem with second-factor SMS authentication being breached.  Of course the big Google phishing problem that they had last week.  The emergence of ultrasonic device tracking that we talked about sometime ago, but kind of like, eh, really?  And it ends up, yeah, really.  Lots of additional privacy news.  Some errata, miscellany.  And then we'll come back to and more formally talk about this Net Neutrality feedback option, and then some quick closing-the-loop feedback from our terrific listeners.  So, yes, another jam-packed podcast. 



LEO:  Yay.  I'm so excited.  And you've interrupted me, but I'm almost done with a comment to the FCC.  And I will complete that because I think it's very important that everybody stand up.  And I have actually some standing here because, if it weren't for a free and open Internet, TWiT, my business, my job would not exist.  We've been doing this for 12 years.  And because we have equal access to our audience, to you, via the Internet, we can compete with big companies like Netflix.  But it requires that Comcast give us equal access, that we be at a parity with big companies.  And we don't have the vast resources to pay for that access.  We rely on a free and open Internet.  So I think I have something to say about this; you know?



STEVE:  I think it's really critical that the providers of our connectivity, the so-called "last mile," the ISPs, be regulated as a common carrier, that is, a neutral carrier of content, independent of what that content is, where it originates from and so forth.  That's just - that's the way it has to be.



LEO:  Yeah.  Clearly.  And somebody said, well, we don't want government regulation of the Internet.  It's not government regulation of the Internet.  It's government regulation of companies, calling on the service providers to preserve the Internet.



STEVE:  Exactly. 



LEO:  And those companies have proven themselves not to have the same interests we do.



STEVE:  Yeah, I mean, you and I are both entrepreneurs.  We're both small company owners.  We love and appreciate the concept of capitalism.  But what all of the evidence demonstrates is that unrestrained capitalism is not in the benefit of the individuals that would like to avail themselves of it.



LEO:  Not when you get to monopoly size, and that's what we have.



STEVE:  Yes.  As soon as companies get big, they start leveraging their position in order to reduce choice and to eliminate competition.  They're acting in their own best interest, but that's not the same as the public interest.  And so I think you could argue, I think our government does have a responsibility and a role in doing things that keep companies' actions in the interest of the public to the degree necessary.  So, yeah.  I'm glad, I've heard you make the distinction between regulating the Internet versus regulating the providers of our connectivity.  Those are different things.



LEO:  Yeah, they're the gatekeepers.  And if they're not kept scrupulously fair, their gatekeeping can create an imbalance and mean that innovators like you and me don't have equal access to our customers, that only the big companies will.  And that's not what we want.



STEVE:  Right.  Speaking of which...



LEO:  Yes.



STEVE:  Our Picture of the Week...



LEO:  Oh, let me get it here.



STEVE:  ...is a year-to-date graph of Internet-wide port 16992 scanning activity.



LEO:  Ooh.  It's gone up a lot.



STEVE:  Yeah.



LEO:  What's going on?  What is that?



STEVE:  Well, that's the AMT port, the Intel management engine port.



LEO:  Oh, wow.  Oh, wow. 



STEVE:  And what this shows is that there was an initial - first of all, it had been all quiet, just kind of little bumps, probably just random scans of things, all the way through the year, until looks like about maybe March 10th, when there was a weird spike in hits per day.  This was at the SANS Institute honeypot, where they maintain a sort of an Internet-wide scanner.  Then it kind of quieted down but was, you know, more active.  And then later in March, and pretty much through all of March, scanning was dramatically higher.



And what's interesting is that that's true despite the fact that there was no public disclosure of the IME AMT problem that was the title of last week's podcast, until May Day, until last Monday.  So something was going on.  Somebody knew that something was happening, that there was a potential exploit.  And in fact 16992 is the non-encrypted port which, if AMT was "provisioned" is the technical term that you keep seeing, but enabled on a system that was exposed to the public Internet, this would allow connection to that port.



And what we learned at the end of last week, between these two podcasts, between last Tuesday's podcast and today, is that the problem was unbelievably bad, such that - and I heard you talking about it over the weekend also, Leo - such that Intel describing it as an "escalation of privilege" is almost laughable.



Anyway, we will talk about that because the technical detail of the bug in the firmware was amazing, and it's just great material for the podcast.  But for what it's worth, in March - and it looks like it sort of - I guess this graph cuts off, so we don't know about April.  We don't know what April looked like.  But at least all of the year was quiet until a little blip early in March, and then a lot of scanning through March for this port that would have allowed Word exposed on a system where the enterprise administrators had enabled this management technology, would allow those machines to be taken over trivially.  And we now have all the details that we'll be talking about in a moment.  And in fact right now because that's the first topic.



So what we learned was that some guys at - and I had their name written down, I don't see here.  Embedded, was it?  I'm not seeing their name.  They're a Berkeley company.  I was trying to go off of my notes.  I'll come to it.  So as I said, late last week, actually it was Thursday or Friday, depending upon where your international dateline is, we found out that, through some independent discovery, what was going on.  And also apparently some disclosure when the news got loose.



The folks at Tenable wrote on their site:  "On May 1, 2017, Intel disclosed the AMT vulnerability, but details of that vulnerability were not made public."  Thus of course on last week's podcast all we were able to say was, yes, we were worried about the idea of this low-level firmware-based separate CPU that was underdocumented/undocumented, that Intel was trying to keep anyone from knowing too much about where information is available only under license, and a few companies are manufacturing management tools.  Intel does have some that you're able to download to talk to this.



Anyway, Tenable says their researchers were able to overcome the challenge of nondisclosure and, they say, make Tenable the first to deliver Intel AMT vulnerability detection capabilities to their customers shortly after Intel's announcement.  So to give us a little bit of background into what's necessary, I took a snippet from their coverage.  They said:  "The first thing our research team tried was to set up a known vulnerable target.  After some searching, we found," they wrote, "a Dell computer that had Intel AMT support, but there was a problem.  It was not configured/provisioned for what we needed.



"The Intel Management Engine Interface driver was installed, but the Local Management Service (LMS) was not.  Intel's AMT documentation says the AMT configuration tool acuwizard.exe requires LMS to be running.  So we searched and found a software package for installing LMS on Dell's website.  After LMS was installed, we were able to configure and provision AMT on the computer, giving us access to AMT via the web interface."



So first of all, one of the very confusing things about all of this is that there's still some lack of clarity on local access versus network access.  And this is crucial because you could have a false sense of security if you verified that, for example, this LMS service was not present.  Well, LMS service that these guys are talking about is the way within Windows to talk to the underlying technology.  But its lack of presence doesn't mean that the underlying technology is not there.  There are aspects which must be enabled.  There are other aspects that do not need to be enabled and cannot be disabled.



And, I mean, and this of course is what we were talking about when we discussed this originally some time ago when we described it as a backdoor onto hardware that cannot be disabled.  There's some subtlety to that, but it is true.  That is, all systems can be pinged using a different port that we'll get to in a second.  But so as I understand it, the LMS driver needs to be put onto a system in order for the hardware to be enabled, or some aspects of this to be enabled for non-local network access.  Thus that would create the vulnerability.  And that's why I explained earlier that this port scan would have been for a system, probably on an enterprise network, where AMT was being used to manage it, so it had been enabled for network access rather than only for local access, which would otherwise be the default.



So this is better than the concern I expressed last Tuesday that any system that had this was vulnerable.  Any system that had it could be found with an AMT ping, but the ports that allow this access are probably disabled for network access by default, but not local access.  And then there's some other confusion about whether netstat, the netstat command can be used to reveal these ports because that would be local, and maybe network, depending up on what netstat returns.



So again, there's still some confusion.  But it is not the case that every motherboard that has IME and the Intel Management Engine and AMT is remotely attackable out of the box, that it does need to be enabled.  When it is enabled - and this is where everyone was running around with their hair on fire at the end of last week.  When it's enabled it's rather horrifically broken.



So there are a bunch of ports.  There's 16992, which is described as Intel's AMT HTTP.  The AMT service actually runs a web server.  And so, for example, you can use a browser or an Intel client app to connect over HTTP to your own system's port 16992.  Or if it's enabled for network, for remote network access, anybody can connect to this web server at port 16992.  The next port, 16993, is the equivalent for HTTPS.  So it's very  much like the traditional public web ports are port 80 for HTTP and 443 for HTTPS.  Intel has 16992 and 16993, respectively, for HTTP and HTTPS.



Then there's two additional ports:  994, which is called the AMT redirection for TCP; and 995, which is the redirection for TLS.  So similarly non-encrypted and encrypted for system redirection, which is storage redirection, KVM, in order to do remote management of the system.  Then there are two additional ports.  And I couldn't tell, I think these are either ICMP or UDP.  I don't believe that they're TCP.  And those are lower numbered ports 623 and 664, which cannot be disabled.



Oh, I forgot to talk about when they're enabled and disabled.  16992, the first one, the AMT HTTP, Intel's documentation says this port is always open locally, but it may not be opened to the network.  So you can always get to that from your own local machine, but externally you may not be able to get to it.  And apparently 993 tracks that as available when TLS is enabled, and that can be done additionally and separately.



Port 623, Intel's documentation says, is always enabled.  And 664 is also always enabled.  And then the last port is 5900, which is familiar to people who have used VNC.  That's the remote control program, and their system supports VNC for using KVM connections to the system.  So there's a bunch of ports that, in the worst case, machines can be pinged.  And I don't know what the constraints are for remote network.  But the pinging needs to be done from - I forgot to mention, both the source and the destination ports must be the same, 623 and 664.  That's important because those are below 1024.  And all ports below 1024, by convention, can only be accessed by system-privileged applications; that is, an application running in user space cannot bind itself to a port below 1024.  And that's deliberately done.



So Intel essentially added a little bit of additional protection.  You need to have a process running with system privileges in order to emit traffic from that system's own port 623 and send them to the same port on a remote system.  But that does allow you to ping the presence of one of these Intel-based systems on the network.  And that can't be turned down.  So at least it does allow a way of probing for those.



So the guys at SSH.com wrote:  "This is like giving everyone with Intranet access" - so local Intranet access - "root privileges on every server whose AMT port they can communicate with," they wrote, "including janitors who can plug into the internal network.  This also means root access to every virtual machine, container, and database running on those servers."  And they wrote:  "People with internal firewalls and dedicated management networks are in a better position," meaning that a dedicated management network would not be part of the globally accessible Internet.



LEO:  And if you're not using the Ethernet port that's controlled by AMT, like if you're using WiFi to get on the 'Net, you're fine, too; right?



STEVE:  Correct.



LEO:  It's not like sitting there waiting to scoop up all the traffic.  It can only see what comes in on that NIC.



STEVE:  Correct.  And in fact, when I was having that IP address conflict because there was ARP going on for it to obtain an IP for itself, nothing I could do was able to disable that on my server.  Finally, when I moved the connection to the secondary NIC, the problem just went away.  And only on the primary NIC.



LEO:  And they say it's vPro-enabled Intel chips only.  That's not all.



STEVE:  Correct.  Although I did see some statements here in the intervening week saying that there were some non-vPro that also had this.



LEO:  It's my understanding that AMT is in all the chips.  It's just not enabled in the non-vPro chips.



STEVE:  Ah, okay.



LEO:  And perhaps enabling, you know, that's a soft term.  Who knows, you know.  If the hardware is there, the code's there, maybe they can get it to run, even if it's not a vPro chip.



STEVE:  And this also raises the specter of something getting into your system and enabling it behind your back.



LEO:  Right.



STEVE:  And thus basically opening a backdoor.  And then Universal Plug and Play comes in and maps the ports through, so you're no longer hiding behind your NAT router.  You're able to map an incoming port through to essentially a backdoor that was opened by someone that briefly got into your system.  So it's sort of spooky to have this.  On the other hand, you could argue that, if malware is in your computer, and it can use Universal Plug and Play, it could just map a backdoor to itself.  So it's not clear that that's a lot better.  Although turning it on would probably leave it on, so it would be probably subject to living through a reboot, where malware might not be or might be found.  Nothing is going to - because this exists outside of the OS, no malware scanner would find this, either, because it isn't malware, it's just something that's turned on.



So what happened - oh, it's Embedi, the guys in Berkeley, California are Embedi, E-M-B-E-D-I.  And they specialize in embedded system security.  They reverse engineered and examined Intel's AMT code and posted their results.  And I don't think this was responsibly disclosed because, as I understand it, we don't yet have patches.  Patches are supposed to be happening this week.  But to characterize what they found, and I'm going to explain the technical details next, but Ars Technica headlined:  "The hijacking flaw that lurked in Intel chips is worse than anyone thought."



They wrote:  "A remote hijacking flaw that lurked in Intel chips for seven years was more severe than many people imagined because it allowed hackers to remotely gain administrative control over huge fleets of computers without entering a password."  And this is what I heard you refer to over the weekend, Leo, that you didn't need to use a password.



LEO:  Anything worked.



STEVE:  This according, well, not...



LEO:  How could you write software like that?



STEVE:  Okay, well, here it is.



LEO:  Okay.



STEVE:  "While studying the Intel AMT Implementation and Reference Guide, the researchers learned that various AMT features are available through the AMT web panel, which is supported by [as I mentioned] the integrated web server, which listens to ports 16992 and 16993.  To protect the AMT from unauthorized access, the web server provides several methods of authentication and authorization of a remote user.  Intel AMT supports both Digest and Kerberos" - I always pronounce this wrong - Kerberos authentication.  I just don't say it...



LEO:  I'm not stepping in here.



STEVE:  "...Kerberos authentication, although the admin account always uses Digest authentication."  And in fact the Embedi guys wrote:  "An admin account which is present by default and always uses Digest authentication seemed like an interesting thing to dig deeper into."



Now, we've never talked about Digest authentication because it's not something that end-users typically encounter.  But it's a longstanding RFC-based means for a client to prove that it shares a secret with a remote server.  And it's an HTTP protocol-based challenge-and-response authentication mechanism.  So the way it works is this.  The client first issues a query to the server, just a standard HTTP, like I want index.html, whatever.  I want a page.  Well, because that first query won't have the additional authentication stuff, the query is rejected by the server with an HTTP 401 Unauthorized response.



Now, for example, we're all familiar with the famous 404 Not Found.  When you go to typically an old-school web server and ask it for a page that the server doesn't have, it returns a 404, the Page Not Found.  So a similar response is the 401, which says to the client that made the request, sorry, you need authorization if you're going to access that resource on the server.  And that 401 Unauthorized response, in its headers it includes a realm header and a nonce header.  You know, a "nonce" is a one-time gibberish-looking thing that will never be repeated, which prevents replay attacks.  So the server says, here's some gibberish.  You need to prove that you know the secret.  So take this gibberish and do something with it.



So what happens is, if the client believes it knows the secret, it has a username and password that it has been previously assigned.  Those are essentially its compound secret.  It takes the username, concatenates the realm that was returned by the server, and so that concatenates the password and takes that string and does an MD5 hash.  MD5 is an old-school 128-bit hash that's been around forever.  So basically it takes a username, the realm, and the password, concatenates them, takes the hash.  Then it takes the access method, which is typically like a GET, but it could be a HEAD or a POST, depending upon what it wants to do, and concatenates the URL that it's asking for, and it does another MD5 of that.  And then, finally, it takes the first hash, concatenates the nonce that was returned by the server, and then concatenates the second hash, and hashes that to create a final MD5 128-bit hash.



So as you can see, that basically is a hash that uses secrets that the client has, information that the server provided, some information that will never be repeated, the nonce.  It does three different MD5 hashes on all that stuff to create a 128-bit result which, when converted to hex, since hex is four bits per character, that would be 32 hex digits.  So the idea is that the client, just to sort of restate this, makes an unauthenticated query which induces the server to respond, sorry, 401 Unauthenticated.  But here's some stuff which you can use to authenticate yourself.  So the client, taking its secrets and the stuff the server provided in the 401 Unauthorized reply, does all this hashing like crazy, and then responds this time with additional headers in its query, providing the final hash of all of its results, saying, okay, fine.  Now give me what I want.



What the server of course does is it also knows - it knows what stuff it's sent to the client.  It knows the username and password that the client has.  But notice they didn't exchange them, so that's the shared secret.  At no point do the username and password actually go over the wire.  But they both know what they should be.  So the server computes using the same algorithm, the same hash that the client should have computed if the client knew the secret, the username and password.  And then the server, of course, compares the two hashes, the computed hash that it generated and the authenticating hash that it just received from the client that is requesting the resource.



So the logic of the string comparison, believe it or not, is unbearably broken in Intel's code, to the point where it really makes you question, like, anything Intel would ever do.  So first of all, the most obvious thing for any code to do that wants to verify an MD5 hash - notice that, as we know, hashes are fixed length; right?  MD5, we're not using it any longer for, like, big things because it's 128 bits, just not enough.  The hash doesn't have enough entropy in it to be strong enough.  So SHA-1, of course, is 160 bits.  That's better.  But SHA-256 that we're now using, like its name says, is 256 bits, lots of bits, twice as many bits as an MD5 hash.  But the RFC is old, and it's based on MD5, and that's what we're using.  And in this case we're using it three times, and it's probably good enough for this.



But the point is hashes are always fixed length.  By definition, that's what they do.  So if you know that this Digest-based HTTP authentication is MD5, then anything the client provides has got to be 32 hex characters.  So the first thing the code should do would be to check the length of the hash the client provided.  Is it 32 characters?  If not, there's a clear protocol error, and the authentication should fail.  But Intel's code doesn't do that.



Failing that, properly written code that wishes to compare two strings, because these are two hex strings of 32 characters each, but if you're just going to compare two strings for equality, you should compare the lengths of the two strings to verify that their lengths are identical because, if they differ in length, then they can't be the same.  But Intel's code doesn't do that, either.



So if you're not going to compare, if you're not going to verify the known length of the string, and you're not going to at least compare, verify that the lengths of the two strings are the same, okay, then you at least want to compare the contents of the strings through the correct and expected length of 32 hex characters.  But Intel's code doesn't do that, either.  Believe it or not, what...



LEO:  What does it do?



STEVE:  What the researchers discovered was that Intel's AMT code was using the length of the client's provided string for the comparison.  They didn't use the server's calculated string length.  They used the client's provide string length.  And if the client provided a string length of zero, no comparison was ever performed.



LEO:  Wow.



STEVE:  And the strings were considered to be equal.  I mean, you can't make this up.  This is unbelievable.  So, I mean...



LEO:  You've got to wonder what engineer thought that was a good idea.



STEVE:  That's my point is this is unbelievably worrisome.



LEO:  Why do you even type that?  I mean, it's really puzzling.  That's very puzzling.



STEVE:  Right.  No, I mean, it is.  So in the code there's a string compare where it's pointer to one string, pointer to the second string, and the length, the number of bytes to scan from those two pointers to verify their equality.  You would think, first of all, the length to scan should have been coded at 32.  But, okay, now, that could be troublesome because the client could have provided less than 32.



LEO:  Right, right.



STEVE:  So you wouldn't want to scan past the end of the client's buffer.  That's a classic buffer overrun.  You wouldn't want to scan to the length of the string because it might not be null terminated.  It might be much larger.  So there you would overflow the stack allocation if you provided a much larger authentication.  But this guy - but we always know we're expecting a 32-character string.  It's a hash that's been converted from binary to hex, from 128 bits to 32 characters.  It has to be 32 characters.  So, but no.  They didn't check for size, then hard code 32 in for how far to scan, failing after a size check.  Instead they said, well, we'll scan as much as the user provided, even if it's zero.



LEO:  Even if it's none.



STEVE:  Even if it's none.  Oh, my lord.  So you simply provide a response to the authentication challenge with a null hash answer, and it says, okay, welcome.  You're authenticated as an admin with root privilege.



LEO:  Perfect match.  Wow.



STEVE:  Yeah.  Who wouldn't want that on their motherboard?



LEO:  Well, that's the problem.  I mean, it should be even a higher standard for stuff that's going to be on a chip.  I mean...



STEVE:  Lord, yes. 



LEO:  It's not in nonvolatile - how is it stored on here?  Is it hard?



STEVE:  It's all in firmware.  



LEO:  It's firmware.



STEVE:  And so Intel is massively rushing out patches.



LEO:  So, okay, good.  So the firmware can be rewritten and replaced with something else.



STEVE:  Yes, after eight years.



LEO:  Yeah.  Well, you have to find it first.  But you'd think they'd have had a code audit.  I mean, that's the kind of thing code audits look for.



STEVE:  And this is the problem.  Here again, I mean, we know that just being open source doesn't by itself automatically mean that the software is secure.  But it at least provides an opportunity for an audit to be performed.  And, for example, that's how some problems in TrueCrypt were found was when we, I mean, the good news is nothing horrible was found.  But some things that could have been cleaned up and made better were then made better.  So, but in something like this, where it's like, no, no, no, we're not letting you look, you just have to trust us, it's like, okay.



LEO:  All right.



STEVE:  A researcher by the name of Natalie Silvanovich was the first to discover this.  From Tavis's tweet, it looked like she brought this to his attention, probably brought him in in order to do some of the really deep stuff that we know Tavis does, whether he's showering or not.  Tavis posted to the Chromium Project Zero blog, where he posts his things:  "MsMpEng" - which is Microsoft's Malware Protection Engine - "Remotely Exploitable Type Confusion in Windows 8, 8.1, 10, Windows Server, SCEP, Microsoft Security Essentials, and more."  Meaning, okay, this is the core - and that doesn't mean that Windows 7 is not vulnerable because that's Microsoft Security Essentials is there.  This is the common code that Microsoft uses throughout their products for malware scanning.



And I'll just share what Tavis wrote because it's succinct and perfect.  He wrote:  "MsMpEng is the malware protection service that is enabled by default on Windows 8, 8.1, 10, Windows Server 2012, and so on.  Additionally, Microsoft Security Essentials, System Center Endpoint Protection" - that was the SCEP that I talked about before - "and various other Microsoft security products share the same core engine.  MsMpEng runs as NT AUTHORITY\SYSTEM without sandboxing" - means it's the kernel, it's the root - "and is remotely accessible without authentication via various Windows services, including Exchange, IIS, and so on."



Now, wait a minute.  "Is remotely accessible."  What he means by that is, because it is listening to everything that's happening in the system to protect it, that it is an exploit surface.  It is exactly like we've been talking about other third-party antivirus products reducing the security of the system by introducing their own flaws.  Because they're trying to get - they are essentially a man in the middle.  If they're not perfect, then they create new problems that weren't there before.  And that is exactly what's happened with this core component that is ubiquitously present in all Microsoft platforms.



He writes:  "On workstations, attackers can access mpengine by sending emails to users.  Reading the email or opening attachments is not necessary."  Again, because when your email client stores the email after receiving it on your system, it gets scanned by the mpengine.  Visiting links in a web browser, instant messaging and so on, all represent vulnerabilities.  "This level of accessibility is possible because MsMpEng uses a file system minifilter" - which is a Windows API term - "to intercept and inspect all file system activity.  So writing controlled contents to anywhere on disk" - in browser caches, temporary Internet files, downloads, even unconfirmed downloads, attachments, et cetera - "is enough to access functionality in mpengine.  MIME types and file extensions are irrelevant to this vulnerability, as MsMpEng uses its own content identification system.  Vulnerabilities in MsMpEng are among the most severe possible in Windows due to the privilege, accessibility, and ubiquity of the service.



"The core component of MsMpEng responsible for scanning and analysis is called mpengine."  He writes:  "Mpengine is a vast and complex attack surface, comprising hundreds of handlers for dozens of esoteric archive formats, executable packers and cryptors, full system emulators and interpreters for various architectures and languages and so on."  I mean, for anyone who's been following this podcast and understands how much vulnerability this represents, it's just mindboggling.



He says:  "All of this code is accessible to remote attack.  NScript is the component of mpengine that evaluates any filesystem or network activity that looks like JavaScript.  To be clear, this is an unsandboxed and highly privileged JavaScript interpreter that is used to evaluate untrusted code, by default on all modern Windows systems."  He writes: "This is as surprising as it sounds.  We have written a tool to access NScript via a command shell for testing, allowing us to explore and evaluate it."  And I'll skip all of that detailed stuff.  All of the details are there in the posting that I do have a link to in the show notes, for anyone who's interested.



But it finishes, saying:  "Before executing JavaScript, mpengine uses a number of heuristics to decide if evaluation is necessary.  One such heuristic estimates file entropy before deciding whether to evaluate any JavaScript.  But we found that appending some complex comments is enough to trigger this.  The attached proof of concept demonstrates this.  But please be aware that downloading it will immediately crash MsMpEng in its default configuration and possibly destabilize your system, of course, because you can't download it because MsMpEng will evaluate it and in doing so trip the fault that Microsoft has patched in today's Patch Tuesday update.  Extra care," he says, "should be taken sharing this report with other Windows users via Exchange or web services based on IIS and so on.  As mpengine will unpack arbitrarily deeply nested archives and supports many obscure and esoteric archive formats such as Amiga ZOO..."



LEO:  Gotta have that.



STEVE:  Yeah, "...and MagicISO UIF format."  He says:  "There is no practical way to identify an exploit at the network level, and administrators should patch as soon as practically possible."  He says:  "We have verified that, on Windows 10, adding a blanket exception for C:\ is enough to prevent automatic scanning of file system activity."  And he says:  "You can still initiate manual scans, but it seems prudent to do so only on trusted files."  On the other hand, adding a blanket exception to C:\ basically disables the whole system.



Now, Microsoft has an advisory, 4022344.  What I did this morning was I looked at the version of the Microsoft Malware Protection Engine before installing today's patches.  And, for example, on my Windows 7 machines I open Microsoft Security Essentials and so bring up the dialogue.  In the upper right is a little Help with a triangular dropdown menu.  Drop the menu down, choose About.  That pops up a dialogue, and you'll see a number of version things there.  Before I installed the updates, it said engine version was 1.1.13701.0.  It was vulnerable.  I installed the Patch Tuesday updates, looked again, and it was now 1.1.13704.0.  So you want 704.



And in Microsoft's advisory they say:  "For affected software" - which is to say everything - "verify that the Microsoft Malware Protection Engine version is 1.1.13704.0 or later."  So this is bad.  I mean, this is, like, unbelievably bad because essentially it means that anything that allows, like any means by which a Windows system will write something that it receives to disk, even if it's in a cache, if it's email, if it's, I mean, like anything that comes in, essentially, has an opportunity to trigger this.  It's now known.  There's no doubt that the bad guys are going to be attacking it.



And for everyone listening to this podcast who is in the process right now of installing their Patch Tuesday patches, you're going to be okay.  But we know that there's a long tail on patch installs and updates.  And this is bad.  As far as we know, it has not yet been weaponized.  But that seems like just a matter of time.



So, I mean, I wish there was a - I don't know why it was that Tavis, I mean, Tavis does disclose Project Zero things as soon as they are fixed and the patches are made available.  It would be nice if there had been a few months of time for them to actually get installed because this seems really bad, given that it can be leveraged.  I mean, in the worst case it's a denial of service, meaning that it will crash the recipient's computer.  But as we know, that's where these things begin, and they quickly get turned into remote code execution exploits.  It's hard to imagine this won't happen because any system that has this, I mean, even XP had an old version of Microsoft Security Essentials.  I mean, this is widespread.



So I don't know when this got introduced.  So it may not be that really old ones had this.  I didn't see if the earlier versions were not affected.  But this is, you know, always in Windows, always on, always protecting people, but only if it does a perfect job.  And this looks like there's a good chance that it might not.  So Natalie, props; and Tavis, thank you for the compete teardown of this.  Yikes.



And speaking of Patch Tuesday, I'll just say that across all of the platforms, more than 20 vulnerabilities were fixed in every platform.  They varied a little bit, the exact number, from platform to platform.  But in every case four were rated as critical, and this was one of those.  So definitely this is one you don't want to delay on fixing.  I mean, really, immediately you want to get yourself updated.



Meanwhile, Google on the 5th released their May updates, which they documented on the 1st.  They fixed 17 critical vulnerabilities, six of which affected the Android media server components - we've talked about Stagefright in the past, and it's been a real source of problems for Google and Android - which can be used to execute malicious code remotely.  They also fixed four critical vulnerabilities related to the Qualcomm components in Android's handsets, including Google's Nexus 6P, the Pixel XL, and the Nexus 9 devices.



And Google wrote:  "The most severe of these issues is a critical vulnerability that could enable remote code execution on an affected device through multiple methods such as email, web browsing, and MMS when processing media files."  So I know that non-Google Android devices are far less expensive than Google's.  But we're learning that these are computers, and they need to be kept updated.  And so were I to be using an Android device, I would just get it from Google because... 



LEO:  Yeah.  And Google's are the best, frankly.  Not necessarily more expensive than, say, a Samsung Galaxy 8.  But I was just looking at my Samsung Galaxy S8.  It only has the April security patch.  My Google Nexus, I'm sorry, Pixel got the May update, like May 5th, immediately.



STEVE:  Right.



LEO:  And that's what you want.



STEVE:  Right.  And thank you.  I forgot to mention that Google said security patch levels of May 5, 2017 or later address all these issues.



LEO:  Yeah.



STEVE:  So just check to make sure that you're up to date.



LEO:  The Sami is vulnerable.  The Pixel XL is not.



STEVE:  Yeah.



LEO:  That's too bad.



STEVE:  So not surprising to our listeners, there has been a big breach of texted second-factor authentication.  And everyone will remember when I was setting up my Hover account that, when I was offered a choice of time-based one-time passwords, TOTP, versus texting, I didn't hesitate.  I knew that time-based was more secure.  Because the last thing I want is every time I need to authenticate, to have that be an opportunity for some attacker to obtain that code.  So it's much more secure to get it once over a secure channel when you're setting up your account and then use a TOTP authenticating app to dynamically produce the magic six digits.



So the problem, as we talked about back some time ago, is with SS7, the so-called Signaling System 7, which is the very old technology still used today to interconnect more than 800 telecommunications companies.  But as we've discussed, the system is so old that it lacks any kind of endpoint authentication.  It is an entirely non-authenticated system.  But despite that fact, we have taken, we the world, to sending secondary identity authentication factors through the unauthenticated mobile telecommunications system.



LEO:  Sigh.  Yeah.  Sigh.



STEVE:  What could possibly go wrong?  So finally, and foreseeably, in January attackers exploited these well-known SS7 weaknesses to bypass the second factor in authentication that banks were using to prevent unauthenticated or unauthorized withdrawals from online accounts, after first using traditional banking trojan implants to perform the first stage of account compromise, which allowed them to learn account balances and get account information.  They then selectively compromised the SS7 system to redirect the text verification messages banks used to verify funds withdrawals.  So instead of those verification messages being delivered to the phones of the designated accountholders, the text messages were delivered to numbers controlled by attackers, who then used both the information gained from the banking trojan and their interception of the "are you sure you want to transfer all the money out of your account" confirmation code to do that, transferring them to their own accounts and making off with it.



LEO:  Wow.



STEVE:  And these attacks were confirmed by the affected banks.  Not good.



LEO:  Unh-unh.



STEVE:  Now, I noted that - I just recently cleared the cookies from a browser because I was doing some experimenting, and saw that I was no longer authenticated through browser interaction on Twitter.  And of course I had set Twitter up with a time-based token.  But I remember at the time that it didn't - that I was, like, having to turn on both SMS and time-based.  And I found to my annoyance yesterday that I reauthenticated with my time-based token, but I did also receive an unwanted SMS.  So I need to see whether I can turn that off now and keep time-based, but disable SMS.  Or maybe I'll just give it a bogus phone number, although I don't want it going anywhere else.  I just don't want them to send it.



LEO:  Send it to 555-1212.  That's directory assistance.



STEVE:  Ah.



LEO:  Yeah.



STEVE:  Cool.



LEO:  I doubt very much that anybody's going to hack that.  It's not a cell phone, for one thing.  What if you sent it to a landline?  Maybe they wouldn't even accept it because you couldn't verify it.  That's the problem.  You can't send it to a  nonexistent number.  You have to verify it.



STEVE:  Ah, right, because they're going to send you a verification text.



LEO:  Yeah.  Google does, and a number of companies do offer - so they'll say, I could send you a text or even call you with the verification.



STEVE:  Right.



LEO:  So you can use a landline.  And the landline would not be, well, certainly not to SS7.  It might be something else that it could be vulnerable to.



STEVE:  Yeah.  I just don't like the idea of a per-authentication...



LEO:  That's why we have authenticators.



STEVE:  Yes, exactly.



LEO:  Yeah.



STEVE:  So we need to talk about last week's massive phishing attack on Google...



LEO:  Oh, good, I was hoping you would.



STEVE:  Google users, yeah.



LEO:  Google Docs, yeah.



STEVE:  Yeah, Google Docs.  One of our listeners sent a note saying:  "Exploit is spreading quickly.  I've received it twice today."



LEO:  Oh, yeah.  



STEVE:  And Google was very comprehensive in their response.  On Wednesday they wrote, or they said:  "On Wednesday, May 3, we identified, investigated, and resolved an email phishing campaign that affected some accounts in your domain."  So this was sent to people who Google verified after the fact had been affected.  This issue was addressed within approximately one hour from when Google became aware of it.  Although we could argue, and many have, that Google was informed in 2011 by some researchers who knew of this problem.  And then apparently the code, a proof of concept, was posted on GitHub in February.  And it's believed that in fact the GitHub-based proof of concept may have been used by the attackers to help them create this attack.  So one has to question whether Google couldn't have fixed this beforehand.



Anyway, they said:  "To assist you in understanding what happened and better educating your users on email security, we are sharing details on how the campaign worked and how we addressed it.  We're also providing a CSV file identifying the users on your domain who were affected."  So they said:  "The affected users received an email that appeared to be from a contact" - like one of their contacts - "offering to share a Google Doc.  Clicking the link in the attacker's email directed the user to the attacker's application, which falsely claimed to be Google Docs, and asked for access to the user's account.  If the user authorized the application, it accessed the user's contacts for the purpose of sending the same message to those contacts."



So it was a classic mail worm, essentially.  "This access only retrieved contacts and sent the message onward.  Customer data such as the contents of emails and documents were not exposed.  Upon detecting this issue, we immediately," writes Google, "responded with a combination of automatic and manual actions including removing the fake pages and applications and pushing updates through Safe Browsing, Gmail, and other anti-abuse systems."



Then they wrote:  "We have taken the following steps to protect your users:  Disabled the offending Google accounts that generated the phishing link; revoked any access that the affected users authorized to the attacker; disabled malicious projects and apps that sought access.  In addition, Google is taking multiple actions to combat this type of attack in the future, such as updating our policies and enforcement on OAuth applications, updating our email filters to help prevent campaigns like this one, and augmenting the monitoring of suspiciously behaving third-party apps that request consent from our users.



"Finally, as a general precautionary measure, you may choose to take the following actions regularly for your users:  Review and verify current OAuth API access by third-parties, run OAuth Token audit log reports to catch future inadvertent scope grants, and set up automated email alerts in the admin console using the Custom Alerts feature or script it with the Reports API.  We thank you for your continued business and support.  If you have any questions, please let us know by contacting Google Support and referencing the issue number" - I hope that's not a count - "37950384.  Sincerely, the G Suite Team."



So this is a problem that we have.  And our listeners will remember that I have recently kind of gotten down on OAuth.  OAuth I described, and I still do, as a really bad web kluge.  I mean, it is.  It's full of problems.  It's had security flaws.  But the underlying problem is that it is so prone to phishing.  You go to a site that says "Log on with Google."  "Log on with Facebook."  And once upon a time this was a rarity.  Now it's becoming increasingly common and is really subject to abuse.



In fact, a couple weeks ago someone in the SQRL newsgroup commented that some of SQRL's mitigations against phishing could be ignored, arguably, because users are being trained as a consequence of this "Log on with Something Else" to assume that what's happening with SQRL is correct.  And so this is something that had changed, this prevalence of OAuth, in the years since I proposed SQRL and came up with this mitigation against the spoofing problem that had always been present.  And because this has always been a worry for me, our listeners who've been following along may remember that summer before last I completely halted work on the project in order to focus on the phishing problem and ended up coming up with a solution, an absolutely bulletproof solution for same-device login, that is, where you've got SQRL running on your Windows machine, for example, or a Linux or a Mac machine using Wine.  And you just click on the SQRL QR code to log in.



The idea is that by allowing the browser and the SQRL client to communicate, there is absolutely no way for a network-based attacker to spoof because the remote server that you're logging into returns the authentication secret to the SQRL client, which then uses its local communication with the browser to give it to the browser, which the browser then uses to log you on, thus cutting any attacker out.  It makes it absolutely spoof-proof.



But around that time Windows 10 was in the process of getting ready to happen.  And there was the threat that Microsoft in Windows 10 was going to cut off access for their Edge browser to the so-called "localhost" IP, which has always been present:  127.0.0.1.  Localhost is used for all kinds of things.  But Microsoft decided, eh, you know, we just think it would be better if we didn't allow Edge browser to access localhost.  So they announced they were going to shut that down.



Well, that's the way the browser could communicate with a SQRL client without needing an extension in the browser to do so.  It made it just, like, automatic and transparent, and it would work perfectly.  But we couldn't have SQRL depend upon a feature that Microsoft was about to kill in Windows 10.  So I left the - we call it CPS, Client-Provided Session, meaning that the SQRL client provides the authorized session to the browser and solves this problem, I mean, better than anything else ever done, completely solves it.  So I left that in, thinking that, okay, well, at least browser extensions for SQRL when they're created could take advantage of it to also solve the problem.



Two weeks ago I said, wait a minute, we need to revisit this.  What finally ended up happening?  Well, it turns out Microsoft was unable to follow through with their threat.  There was such a backlash from the user and developer, especially the developer community, that they were not going to be able to use localhost, I mean, it's like all these Windows machines have IIS server built into them.  You can create websites and check your JavaScript.  And it's so useful to be able to access servers running in your own machine with your own browsers.  Everyone who's a developer does that.  And Microsoft was saying no.



Well, they were unable to follow through.  So they changed the default shortly before release, enabling it by default.  If you go in Edge, if you go about:config, I think it's that, or maybe it's about:settings, it'll bring up a page.  And the second option there is to turn off access to localhost.  But it is on by default.  So by default everyone will be able to do this, and it's coming back into SQRL by default.  So we will have, as I wanted a year and a half ago, bulletproof spoof protection in SQRL.  And things like man-in-the-middle attacks and website spoofing and so forth cannot affect SQRL.



So I hadn't talked about SQRL recently.  I was in the process of working on the installer and remover code as I get this thing ready to be finished.  But this brought me to a stop about a week ago when someone said, you know, users are not going to know that, if they're logging into Joe's Blog, and this thing says Amazon, that they're not, like, using their Amazon identity to log into Joe because that's what users are accustomed to now.  And he was right in saying that.  So we've got full spoofing protection back for same-device login.  And I'm really glad for it.



LEO:  Steve Gibson, I can tell he's perusing the newsfeeds, see what's happening out there.



STEVE:  Yeah, so anyway, yeah, as you said, James Comey, director of the FBI, just got fired.  So he of course was in the process of investigating the Trump campaign ties to Russia.



LEO:  I'm sure it had nothing to do with that.



STEVE:  Nothing to do with that.  Yes, Trump apparently said, "While I greatly appreciate you informing me on three separate occasions that I am not under investigation, I nevertheless concur with the judgment of the Department of Justice" - of course that's Jeff Sessions - "that you are not able to effectively lead the Bureau," Mr. Trump said in a letter dated Tuesday to Mr. Comey.  "It is essential that we find new leadership for the FBI that restores public trust and confidence in its vital law enforcement mission," Mr. Trump wrote.  Well.



LEO:  Wow.



STEVE:  Yeah.  I did want to mention, just for people who were interested in the reach of Google, somebody tweeted me, and I'm glad for it.  I forgot about this.  I think we talked about it once:  myactivity.google.com?  Which is a somewhat sobering, but I would argue open-kimono look at what Google has.



LEO:  Yeah.  I mean, they tell you what they know.  



STEVE:  Yeah, they do.



LEO:  As opposed to other people who may not.



STEVE:  Yeah.  So myactivity.google.com.  And, I mean, again,  you're looking, you're scrolling through all of your past searches.  And it's like, oh, uh, yeah, you really are watching.  So, yeah.  Just, again, it's nice to see.  I think you're able to delete things from there also, if I recall.



LEO:  Yeah.  To their credit, I mean, they collect everything.  But they at least tell you what they collect, and they in most cases give you the opportunity to turn it off.



STEVE:  Yeah.  So a paper which was presented last week at the second annual IEEE European Symposium on Security and Privacy, which occurred in France, in Paris, was titled "The emergence of ultrasonic cross-device tracking."  Oh, I'm sorry, no.  That was my title.  "Privacy Threats Through Ultrasonic Side Channels on Mobile Devices."  And I've been meaning to mention to you, Leo, we know that this actually can be done because that's how that cool little EKG monitor that you like operates.



LEO:  Ah.



STEVE:  That little black strip with the two silver pads?  The reason it says it needs access to your microphone is it uses frequency-modulated ultrasonics in order to instantaneously send the voltage that it's picking up between your hand into your phone, which then converts it into an EKG waveform.  So anyway, what's a little worrisome, we talked about SilverPush before.  That's this idea of applications surreptitiously listening for high-frequency, essentially dog whistles that are being used to convey information.



Four German security researchers analyzed a large repository of Android apps.  In the abstract to their paper, they wrote:  "Device tracking is a serious threat to the privacy of users, as it enables spying on their habits and activities.  A recent practice embeds ultrasonic beacons in audio and tracks them using the microphone of mobile devices.  This side channel allows an adversary to identify a user's current location, spy on their TV viewing habits, or link together their different mobile devices.  In this paper we explore the capabilities and current prevalence and technical limitations of this new tracking technique based on three commercial tracking solutions."  So these are being commercially offered as SDKs to people for this purpose. 



"To this end," they write, "we develop detection approaches for ultrasonic beacons and Android applications capable of processing these.  Our findings confirm our privacy concerns:  We spot" - get a load of this - "ultrasonic beacons in various web media content and detect signals in four of 35 stores in two European cities that are used for location tracking.  While we do not find ultrasonic beacons in TV streams from seven countries, we spotted 234 Android applications that are constantly listening for ultrasonic beacons in the background without their users' knowledge."



And it turns out these are not all obscure applications.  McDonald's and Krispy Kreme are among the applications that are using this technology.  So with the headline:  "Hundreds of privacy-invading apps are using ultrasonic sounds to track you," ZDNet summarized it, saying:  "A new privacy-busting technique that tracks consumers through the use of ultrasonic tones may have once sounded like the stuff of science fiction novels, but today it's reality.  These near-silent tones cannot be picked up by the human ear, but there are apps in your phone that are always listening for them."  I would say there may be apps in your phone.



"This technology is called ultrasonic cross-device tracking, and it works by emitting high-frequency tones in advertisements and billboards, web pages, and across brick-and-mortar retail outlets or sports stadiums.  Apps with access to your phone's microphone can pick up these tones and build up a profile about what you've seen, where, and in some cases even the websites you've visited.  The technology is still in its infancy, but it's growing in popularity."



And then in quoting this research, they said:  "In the past year, researchers found 234 Android apps that include the ability to listen for ultrasonic tones without the user's knowledge.  And these are not all obscure apps since numbered among them are McDonalds and Krispy Kreme.  None of the 234 Android applications disclose their tracking capabilities in their privacy policies."



Anyway:  "A Google representative said that the privacy policies enforced on all apps available in the Play market require developers to 'comprehensively disclose how an app collects, uses, and shares user data, including the types of parties with whom it's shared.' The representative" - that is, Google's representative - "didn't respond," Ars writes, "to a follow-up question asking why none of the five apps cited in the research findings disclosed the SilverPush functions.  At the time this post went live," writes Ars, "all five apps remained available in the Google Play Store."



So I don't know what this means.  It would be nice, for example, if it were possible to audit all of the installed apps for their access to the microphone so you could just, after the fact, look to see what apps had acquired permission, if you had given it to them at install time.  The other thing that's annoying is typically apps, or they may not, allow you to selectively disable the features that they're asking for.  I don't know how granular apps are under Android.  But it would be nice if something...



LEO:  It's pretty granular.  But I don't - yeah, that's interesting.  I'll have to look and see.  I mean, you can.  But that may break the app.  I mean...



STEVE:  Well, but apps that don't have a reason to listen are, I mean, you could argue, you say, no, I don't want you using my microphone.



LEO:  Yeah.  Yeah, I'll look.



STEVE:  So we've been covering this story, and there was a little bit of news, so I just thought I would keep us current.  And that is that in that bizarre case of the homicide in the hot tub that we've referred to a number of times, where the user was very IoT - the user.  The homeowner who had the hot tub was very IoT enabled, and the use of a huge amount of water in the middle of the night was regarded as suspicious as relayed by his water meter that was IoT enabled.  We will recall that Amazon was refusing under First Amendment rights, and I guess not Fifth Amendment, but saying basically "freedom of speech," saying they're not going to disclose what the Echo may or may not have overheard.  So they've reversed themselves.



LEO:  That got dropped, though; right?



STEVE:  Yes.



LEO:  Yes, yeah, they said they would, but then the thing got dropped; didn't it?



STEVE:  Well, no.  As I understand it, Amazon has recently agreed to release any data obtained by the - any data after the Arkansas defendant agreed to allow the authorities to ask Amazon to do so.  And he said, "I have nothing to hide.  You're welcome to do that."



LEO:  Okay.



STEVE:  So they've been saying no.  Apparently the guy said, yes, fine.  Nothing happened.  And so they said, okay, fine, we'll release what we have.  Which is very likely little.



Also, very quickly, a Miami judge has now ruled that compelling password production is not a Fifth Amendment issue.  That is, it is not compelling the production of your password.  Giving it over to authorities does not violate your protection against self-incrimination.  So we're seeing lots of judgments coming down both ways on this.  I've got much more than I'll bother going through here.  There was a nice coverage of it in Techdirt, who concluded, saying:  "This decision will be appealed.  But the decision cited by this judge appears to indicate that this will only delay the inevitable."



And, as I have said here, "Sooner or later, this issue will have to be addressed by the Supreme Court," but this writer in Techdirt says, "but I wouldn't hold my breath waiting for it to happen.  The Supreme Court frequently takes a pass on timely issues, leaving circuit appeals courts to do most of the heavy lifting.  There have not been sufficient Fifth Amendment cases of this type in federal appeals courts to press the issue.  So far, the only thing that's been made clear in multiple cases is fingerprints are worse than passwords when it comes to locking law enforcement out of phone contents."



And then some news from the U.K.  Newsweek reported, and also BBC picked up on it also:  "According to leaked documents, the U.K. government plans to ask for powers allowing intelligence agencies to spy on people in real-time by introducing encryption 'back doors' to communications firms.  Privacy advocate organization Open Rights Group obtained a leaked copy of the government's draft Technical Capability Notices - called TCNs - regulation, which it has published in full on its website.



"The document forms part of a so-called 'targeted consultation' into the Investigatory Powers Act, which was brought into law last year, meaning it has not been publicized to the tech industry or the public.  Under the proposals, all communications companies - Internet providers, messaging apps, and phone networks - would be forced to provide police with real-time access to a person's web browsing with one day's notice."



Jim Killock, who is the executive director of Open Rights Group, said in an emailed statement to Newsweek for their coverage:  "These powers could be directed at companies like WhatsApp to limit their encryption.  But if the powers are exercised, this will be done in secret."  Killock previously described the Investigatory Powers Act, referred to widely as the "Snooper's Charter," as the "most extreme surveillance law ever passed in a democracy."  Writing an opinion piece for Newsweek last year, Killock said:  "The Investigatory Powers" - is there a better way to say that, "investigatory"?  I guess that's the way you say it.



LEO:  Investigatory, yeah.



STEVE:  Investigatory.  I don't know why I have a hard time.



LEO:  Say IPA.



STEVE:  Investigatory.  Yes, "The IPA mostly permits and codifies all the illegal practices revealed through whistleblowing and court action."



Meanwhile, here in the U.S., our ex-FBI director James Comey said that his organization is unable to access half of mobile devices, actually 46%, and supports new legislation.  During Senate testimony - apparently his last - last Wednesday, Senator Chuck Grassley asked whether the FBI director still believed that it was not necessary to push for a law to solve the so-called "going dark" problem.  Comey replied:  "It may require a legislative solution at some point," adding "I could imagine a world that ends up with legislation saying, if you are going to make devices in the United States, you figure out how to comply with court orders."  And he said also:  "The shadow created by the problem called 'going dark' continues to fall across more and more of our work," and blamed the "ubiquitous default disk encryption on devices."  



Unfortunately, given the people that Trump is appointing, I would not be surprised to see the next FBI director taking a stronger position.



LEO:  Well, let's be fair.  Our Democratic Senator from California, Dianne Feinstein, is also in favor of this.



STEVE:  Yes.  Very, very good point.  Yes, very good point.



LEO:  It's a kind of - one of those bipartisan, apparently, issues.



STEVE:  Yup, I'm glad you said that. 



LEO:  Everybody can agree that privacy is dead.



STEVE:  So this next piece, I'm not going to go over it in detail.  His posting, I have a link to this guy's posting in Google Docs for anyone who is interested.  But this just is - this makes me sad.  The coverage appeared in Ars, and Ars said that U.S. border agents threatened to, quote, "be dicks and take my phone if I didn't unlock it."



So this starts:  "Albany, California.  As he sat in a darkened corner of a neighborhood bar, Aaron Gach, G-A-C-H, an artist and lecturer at a local art college" - and, by the way, U.S. citizen - "told Ars about what happened to him in a February 2017" - so a few months ago - "episode at San Francisco International Airport, where he finally agreed to unlock his iPhone and have it be searched by border agents" - without cause - "rather than risk being detained and delayed further.  A U.S. citizen, reentering the country at SFO International, is harassed without cause and detained for interrogation until he finally relents and unlocks his phone so that the customs investigators can rifle through it."



So I've skipped a lot of the back story.  It's all online.  And again, links in the notes.  But this dialogue I wanted to quote.  So they ask:  "Can we check your phone to verify the info you provided?"  And this is after a long back-and-forth interrogation of, like, where do you live, where were you visiting, what countries did you go to, are you sure, did you go to any others, are you sure, blah blah blah.



So he says:  "This is where I began asking lots of questions.  I also asked to see the written policies authorizing their actions, and we waited while they went to get a 'tear sheet.'  Following are some of my questions and their answers."  So they ask, I'm sorry, he says:  "Is there a problem with my travel arrangements?"  Their answer:  "I'm sorry, but I cannot provide any details."  "Is there a concern about the arts venue?"  Answer from the border official:  "I can't really say at the moment."



He asks:  "What is it you want to check on my phone?  Is it something in particular that I can just show you?"  Answer:  "We're looking for information pertinent to our investigation."  He asks:  "Do I have a choice in the matter?  What are my rights in this situation?  As a U.S. citizen, don't I have equal protections under the Constitution, regardless of whether or not I am in an airport or outside of one?"  Their answer:  "I understand your concerns, and I'm hoping we can get you on your way as soon as possible.  Of course you have a choice, but we can also be dicks and just take your phone as part of our investigation if we see fit."



LEO:  What?



STEVE:  "Your phone and its contents are part of your personal effects which are subject to examination when crossing any border into the U.S."  He says:  "That doesn't sound like much of a choice.  What happens if I choose not to unlock my phone?"  They respond:  "We can detain your phone and any personal effects needed to assist in our investigation."  And he asks:  "For how long?"  They say:  "Not long. Just until we're done with it, and then we would ship it back to you."



LEO:  Oh, like you're going to want it after that.



STEVE:  Exactly.  And he says:  "Well, how long would that be?  Days, weeks, or indeterminate?"  Answer:  "Indeterminate."



LEO:  Wow.



STEVE:  He says:  "But I could leave?"  And then they say:  "As soon as we're done."  And he says:  "Am I also being detained, then?"  "No, we're not detaining you, just your personal effects."  He says:  "So I can leave, then?"  And they say:  "As soon as we're done here.  Hopefully we can get you on your way shortly."  And he says:  "Can I be present when you search my phone?"  "No, I'm afraid not.  But for this investigation I can tell you that we are only conducting a manual search, and not a digital extraction.  However, I can tell you from others that have refused to unlock their phones that I can't make the same claim, and your information may be copied for later review."



Then they ask:  "Is there any reason in particular why you don't want to turn over your phone?"  And this guy says:  "I believe strongly in the Constitution and in my right to privacy.  I have nothing to hide, but the only way I know if I actually have any rights is if I try to exercise them.  But it sounds like I don't actually have those protections in this situation."  And it goes on.  And as I said, it makes me sad.



LEO:  Well, it's terrifying.  American citizen, coming home. 



STEVE:  Yes, yes, yes.



LEO:  Ars says, well, the guy's an activist.  He's involved in  Greenpeace and Copwatch, but those aren't - he's never been arrested, has no arrest record.  Basically they don't like him.  And that's the problem is there's a lot of power given these guys.  And it terrifies me.  I'm traveling out of the country a couple of times this year.  We actually decided not to go to Cuba because of this.  I just don't want the hassle of coming back from Cuba.



STEVE:  Yeah, also in his story he had a laptop in his luggage.  And although it was protected, there was information that it was not backed up.  So he had information that he wasn't willing to lose if he continued this.



LEO:  They keep it, yeah.



STEVE:  They started going through his luggage.  And in order to head them off at the pass before escalating this, he finally capitulated.



LEO:  The guy's not a terrorist.  All right?



STEVE:  Right.



LEO:  He's an American citizen.



STEVE:  Yes.



LEO:  I don't understand this.  We have a lot of visitors, international visitors.  And fairly frequently they say, yeah, I'm carrying a burner phone, or I was very careful to wipe - I talked to somebody yesterday.  "I was very careful to wipe my phone before I got off the plane" because of stuff we've talked about.  But we should point out most people, I mean, it's a very small percentage of people that this happens to.  And many of these people say nothing happened.  But at the same time, if it does, you don't want it to happen to you.  And there doesn't seem to be much of a rhyme or reason as to whom it's happening to.



STEVE:  Well, it's creepy.  I'm a U.S. citizen.  I've done nothing wrong.  I've traveled abroad.  I'm coming home.



LEO:  I mean, they can search your luggage.  They can search your luggage; right?  This is - they think this is like searching your luggage.  I would submit that it's a little more than searching your luggage, and there's a huge security risk if they take the phone.



STEVE:  Yes.



LEO:  I mean, basically, if I give them my - I'm not going to bring an expensive phone out of the country.



STEVE:  No.  I think that's exactly right.  I think you either back up your phone and wipe it, and you travel with it like that, so they say, "Can we unlock your phone," you go, oh, yeah, here, sure.  I don't even have it protected.  Here, just turn it on.  It's unlocked for you.  And then when you get home you wipe it again, and then you restore from backup.  You just...



LEO:  I guess the real question is how does this protect us?  Because if I'm a bad guy, the last thing I'm going to do is have the secret plans to my secret on my phone.



STEVE:  Right.



LEO:  I'm not going to do that.  So I'm not sure how this protects us.  And it definitely sends a chilling effect to people who want to come to the United States, and to people like me who want to travel.



STEVE:  And it's sad for citizens.  It's like, wait a minute.  I mean, yeah.  Wow.  So photoshopping is a thing.



LEO:  It is a thing.  Yes, it is.



STEVE:  As we know, image editing, yes, image editing is so prevalent and has been available for so long that we have a new term, "photoshopping."  Oh, that picture was "photoshopped."  Meaning it wasn't actually, you know, that image that you're seeing never actually existed in the physical world.  It was digitally created.



Well, it turns out that as a consequence of evolving technology and sort of speech recognition turned upside down, we are now seeing the emergence of voice-mimicking tools.  Voice cloning technology has been in the works for several years; and, before long, just as with photoshopping an image, which has become a thing, so too will the ability to impersonate anyone saying anything.  And lord help us when that happens.  And when you think about it, traditionally, how many spy novels and movies and TV plots, I mean, like, wearing a wire, the reason you wear a wire is to capture somebody saying something that's compromising.  So you pat down, are you wearing a wire, blah blah blah.



So traditionally playing back a compromising recording of someone saying something has been regarded as strong evidence of wrongdoing and created a strong belief that the subject in question had actually uttered those phrases because you could hear them doing that.  But that assurance will soon disappear.  Apparently there is already some software to which you can feed some chosen words, very much like in that "Mission Impossible," one of the recent ones, where somebody wanted to capture someone's voice so they restrained him and said, "Read this card," and it was a nonsense sentence.  But the point was that the sentence contained expressions of all the various phonemes that were required in order to then synthesize that person's voice saying whatever they wanted to.



So anyway, I just wanted to put it on our radar.  I expect before long we will start seeing our late-night comics no longer having to chop up things that people said in order to create out-of-context funny things, but actually just be able to synthesize someone's voice saying whatever they want.  And so we lose, just as we can no longer believe an image, believe what our eyes tell us in an image, we won't be able to believe what our ears hear because it could just be somebody that was digitally synthesized.



I did want to give a caution to anybody who may have recently downloaded or updated their version of HandBrake for the Mac.  HandBrake is a very popular DVD-ripping and media transcoding app, popular because it's just a GUI, and it's robust and very simple.  I used it years ago when I wanted to copy DVDs that I owned for my own purposes, of course, fair use.  And it's great.  But over the weekend one of the official download servers, download.handbrake.fr, was compromised, and v1.0.7 was made  malicious.  So it's the actual HandBrake from the actual server is actually bad.  So if you don't have 1.0.7, you're okay.



If you do have it, you could take the fingerprint of the file, the SHA-1 checksum.  And in the show notes I show the checksum and some simple instructions for making the checksum.  For example, if you open Terminal in a Mac and just type "shasum," space, and then drag the HandBrake.dmg file, drop it in the Terminal, that enters the path to the file as where the cursor is in the Terminal, and then hit Enter, and it'll show you the hash.  And it should start out 0935a43.  If it does, that's malicious.  So I should have said it should not start out 0935a43 because, if it does, it's malicious.  So anyway, if you didn't download HandBrake recently, don't worry about it.  You've got 107 and did download it recently, you want to make sure you didn't get a bad one.



Two bits of errata.  Many people pointed out our funny video from last week, that that was not the Cookie Monster.  Apparently it was - I guess it was an early version of the Cookie Monster, or maybe the Cookie Monster was derived on "Sesame Street" from this monster on "The Muppet Show."  But this was a Muppet monster.  Cookie Monster was blue, so a different color, and did not have teeth the way the monster we showed last week did.  Anyway, a number of people saying...



LEO:  Sorry, Steve, but I know it's not the Cookie Monster, man.



STEVE:  Well, and also, people also took exception to my referring to emojis as "yellow Smurfs."  They said, "Steve..."



LEO:  Smurfs are not - they said this in the chatroom, and I chose to ignore it.



STEVE:  Yes.  Smurfs are not yellow.  Smurfs are blue.



LEO:  Oh.



STEVE:  And several people suggested that maybe Simpsons are yellow, so they were Simpsons.



LEO:  Smurfs are not yellow, Steve.



STEVE:  What I meant to say, of course, was that they were Smurf-like, but yellow in color.  But I didn't make that clear.  So...



LEO:  It doesn't matter.  Doesn't matter.



STEVE:  Yeah, I know.  This is fun.  Turns out all memory leaks are not necessarily important, Leo.



LEO:  What?



STEVE:  Now, we know memory leaks are caused by the allocation of memory, which is not then freed after it's no longer required.  So, for example, a program might explicitly allocate some memory for some purpose, like a buffer, to receive something.  And if it forgot to release it back to the operating system, then when another buffer came in it might allocate another buffer to receive a buffer, and not release that.  Over time, that leaks memory.  All of those successive buffers are asked for from the operating system, and eventually the OS will run out.  It'll say, I don't have any more, you've got it all, because it wasn't being released after use.



Then there's also automatic allocation, where a fancy language, for example, does dynamic memory allocation.  For example, JavaScript has that, where you just use things, and it's up to the language to figure out, oh, look, he's not using that anymore, and it frees it.  So you hope that the dynamic memory allocation, or the automatic allocation, is working right.



So I got a kick out of this.  Posted to, back in the old days, the old Usenet on the Internet, the NNTP newsgroups - which of course GRC is still using - Usenet, the comp.lang.ada forum, so that was the computing language, Ada was a language forum, on March 31st - oh, and I forgot the year, I think it was 1995 - by Kent Mitchell, who was consulting to Rational Software Corp., that at the time was a big deal, in reference to previous postings by Mitre Corp. and IBM's Watson Research Center.  So this is in the early days, when gurus hung out on Usenet, and it wasn't just a big massive porn database, which it later devolved into and then became unusable, which is why GRC runs a private newsgroup server, where we have wonderful groups of great people.



The subject was, does memory leak?  And so Kent posts:  "I was once working with a customer who was producing onboard software for a missile.  In my analysis of their code, I pointed out that they had a number of problems with storage leaks.  Imagine my surprise when the customer's chief software engineer said, 'Of course it leaks.'  He went on to point out that they had calculated the amount of memory the application would leak in the total possible flight time for the missile, and then doubled that number."







LEO:  Oh, what?



STEVE:  For safety.  This is why North Korea...



LEO:  Couldn't you just get rid of the leak?



STEVE:  This is why North Korea is having a problem.  They didn't double.



LEO:  Yeah, double the memory.



STEVE:  "They added this much additional memory to the hardware to 'support' the memory leaks.  Since the missile will explode when it hits its target or at the end of its flight, the ultimate in garbage collection is performed..."



LEO:  That's true.



STEVE:  "...without programmer intervention."  And all of the leaked memory - we could say that the buffers had been destroyed in more ways than one.  



LEO:  Wow.



STEVE:  Isn't that fun?  Yes. 



LEO:  Yeah.



STEVE:  Deliberate memory leak in a missile because it really doesn't matter.



LEO:  Can you - wow.  



STEVE:  As long as the leak will outlive - as long as the memory will outlive the missile's life in flight, who cares?



LEO:  I can't think of any reason why you would allow, I mean,  allow a memory leak.  Wouldn't you just fix it?  Wouldn't that be...



STEVE:  Yeah, that really does sound screwy.



LEO:  Is there any compelling reason why a memory leak should exist?



STEVE:  You might say that it would take up more code.  On the other hand, they've allocated a lot more memory to deal with the leak.  Maybe the code was in ROM, and they leaked.



LEO:  Oh, could be.  They couldn't fix it.



STEVE:  And so they could have had limited ROM, and so they said, well, rather than make the ROM bigger, we don't care if the RAM leaks.  So who knows?



LEO:  Interesting.



STEVE:  I got a couple tweets relative to SpinRite that I appreciated.  First was from someone who was confessant - it was one of those confessional weeks - named Steven.  His Twitter handle is @Dillinger65.  And he said:  "Hi, Steve.  I needed to pirate a copy of SpinRite, fixed income and all.  SpinRite saved my old laptop's hard drive.  One day I'll pay for a copy."  Like, okay, well, thanks for confessing.  I'm glad - and I said back, I said, "Okay, you know, glad it worked for you.  Most people pay for their copy."



LEO:  It's like Augustine.  St. Augustine very famously said - what did he say?  He said:  "Make me good, lord, but just not right now."



STEVE:  That's right.



LEO:  Not quite yet.



STEVE:  And I would appreciate payment.  I mean, it is what allows me to do everything else I do.  So when that stops happening, I'll stop being able to do all this.



Javier Figueiredo, he sent an interesting picture.  He said:  "SpinRiting at Level 2, a two-year-old Kingston SSD.  Does this image look like a healthy disk?"  Oh, no.  So there are several things.  First of all, so this is an SSD that again demonstrates SpinRite's value, even on solid-state media.  The image is showing a bunch of red.  And you don't want to see red there.  This is on "ecc corrected."



And so what's showing is that this bar, the way SpinRite's SMART Monitor works is it always starts the bars off where they are, wherever they are, at the beginning of the scan because any reduction in the bar during the scan represents a drop in the drive's self-reported health as a consequence of SpinRite doing nothing more than asking for the drive's data back, just like computers always do.  And so we see 1,137,031 uses of error correction.  And SpinRite shows the minimum, the maximum, and the average error rate per megabyte of data recovered.



But most significantly is the drive itself screaming for help, saying, uh, ow, you know, you're asking me to read, and I'm having to work too hard to do so.  So Javier, this is definitely SpinRite showing you, back up absolutely right now.  I mean, maybe relegate this to somewhere, a noncritical application.  But this is bad.  And so I responded to his tweet saying, "No, those red squares show that the drive's own self-reported health is dropping under stress.  That should never happen."



And lastly, Keith Pottratz sent me a tweet saying:  "Ran SpinRite on two computers that were not running slow.  SpinRite revealed that both drives were failing.  Saved the day."  So once again, a little preventive maintenance can go a long way.



Finally, getting to the topic, the nominal topic of today's podcast, as I said at the top of the show, two URLs:  GoFCCYourself.com will redirect you to - oh, and by the way, that's HTTP.  John Oliver and company at his "Last Week Tonight With John Oliver" did not do an HTTPS secure - they didn't get a certificate and so forth.  So just put into your browser GoFCCYourself.com.  That will redirect you to the FCC.gov secure site to a page which is difficult to find.  So thus the reason they created this shortcut that redirects you to the page.  If you click on the 17-108 there, over toward the left, you can see previous comments.  If instead you click on the +Express, that's the express submission, where you can submit your own comment and express your feelings about how you feel about the pending potential loss of Net Neutrality.



The EFF, of course, stepped up with their own site.  They have a page at DearFCC.org, and it's very nice.  They wrote:  "Dear FCC.  The FCC has asked for public comment on new rules about Net Neutrality.  Use this form to submit a comment to the FCC.  Learn more about the FCC rulemaking process.  A nice" - and it is - "fill-in-the-blanks form letter that the EFF's new site will email to the FCC for us."  So I would recommend to our listeners...



LEO:  I've already done it.



STEVE:  Do both, yes.



LEO:  Yes, I've already done that.



STEVE:  A listener, Sable, he said:  "I didn't catch which iOS FTP server app is your preferred one.  Good enough to recommend?"  And I would say yes, with the caveat that you not leave it serving all the time.  As I mentioned when I talked about it, I turn it on, I pull a file from an iOS device, and then I turn it off because that's not something you want to leave on.  It's just called FileApp, F-I-L-E-A-P-P.  I have a link to it in the show notes.  And for what it's worth, I do like it, and it's much more than just that.



They describe themselves as FileApp being "a file and documents manager for iPhone, iPad and iPod Touch."  FileApp reads many files types such as PDF, Office documents, and plays multimedia contents; will let you store files and folders on your iOS device just like Windows Explorer or the Finder on the Mac.  Does USB file transfer to Mac and PC using DiskAid or iTunes filesharing.  Offers robust wireless file transfer which is username and password protected via WiFi and has both an HTTP and an FTP server.  Stores files sent from any third-party app, mail, Safari, and so forth.  Allows to "open in" any compatible app, pages, numbers, iBooks, and so forth.  And lots of other features.  So FileApp is the name of that little goodie.  Also does file encryption using iOS data protection.



Adam Rixey asked:  "Does the Internet SNI" - remember that's the Server Name Identification that shows which domain name we're going to - "make DNSCrypt fairly useless?"  And I would say no because DNSCrypt is about much more.  It's not just about privacy, it's about man-in-the-middle attacks and spoofing.  Since DNS is not encrypted, unless you manually create an encrypted tunnel, which DNSCrypt does, anybody could intercept the response UDP packet and change the IP to theirs, and your server would dutifully go there and believe that's where it was, appropriately.  So the SNI is a privacy leak.  DNSCrypt is not only privacy, but also the integrity of all of your use of DNS between you and the DNS server at the other end that supports it.



And, finally, John Sey says:  "What do you use for Windows 7 backup and recovery?"  He says:  "I'm sure I heard on Security Now!, but cannot find the reference."  And I wanted to mention it to you also, Leo, because over the weekend you mentioned the previous one that I used to like.



LEO:  Because I can never remember the one you like.



STEVE:  I know.



LEO:  This is a generic name.  It's Disk Imager.



STEVE:  Image for Windows.



LEO:  Image for Windows.  All right.



STEVE:  Yeah, so I used to like Drive...



LEO:  I mentioned Drive Snapshot, yeah.



STEVE:  Yes, I used to like Drive Snapshot.  But Image for Windows is HTFS-aware, or HFPS?  I forgot what...



LEO:  NTFS?



STEVE:  NT, NT, I started off with the wrong letter, yes, NTFS.  Which Drive Snapshot wasn't.  So Drive Snapshot, for example, especially nowadays where we've got - it only knew about FAT32.  So you would have to break up an image into multiple, multi-gig files.  Whereas Image for Windows is fully aware.  It's bootable.  They've got a bare metal version so you're able to, like, image from the hardware.  Supports Linux and, like, all kinds of features.  And it's like $38 for all flavors of all OSes.  So Image for Windows, made by Terabyte.



LEO:  Got to remember that.  Image for Windows.  Image for Windows; okay.



STEVE:  Those are my guys, Image for Windows.



LEO:  Are we done?  You just stopped talking.



STEVE:  It's 4:00 p.m., and a two-hour and 20-minute podcast.



LEO:  There is no pressure to make it shorter.  Just a couple of things.  I did check, and the Apps Manager in Android, this is Samsung's version of it, but the Apps Manager in Android does let you do granular app permissions.  So if you have an app that has access, let's say, to the microphone, you can go right to permissions, and they've got switches, and you can turn it off.



STEVE:  Nice.



LEO:  And it presumably won't break the app unless for some reason it decides, you know, the developer said, well, without listening to what you're doing, we don't want to give you a free app.



STEVE:  And I know our listeners.  Turn it off until it complains and then decide if you want it back on.



LEO:  Yeah, yeah, exactly.  So they do, you know, it asks for permissions upfront.  But it's nice to have that granularity.  And that's in the settings for Android under Apps.  You can look at each app individually and control that.  So that's a good solution.  I also - there was something else I wanted to correct, but I can't remember what it was.  But that's okay because we'll get emails and have to fix it next week.  Wasn't you, it was me.  I don't remember what it was.



We do this show every Tuesday, 1:30 Pacific, 4:30 Eastern.  Sorry about that laptop lid in your face there.  I'm updating Windows.



STEVE:  No, I'll just rest my chin on it.



LEO:  I can open it right there for you.  We do this, I'm sorry, 1:30 p.m. Pacific, 4:30 Eastern time, that's 20:30 UTC.  You can watch live, and usually the streams are good.  We had trouble with the YouTube stream today, but that's all right.  We're restarting, as soon as the show is over, everything.  And so you could find that at TWiT.tv/live or at YouTube.com/twit or Ustream.tv/twit or Twitch.tv/twit.  We love it if you watch live.  And if you do, you might as well be in the chatroom at irc.twit.tv.  It's very helpful and fun and a nice place to be.  But of course, if you can't be here, you can get on-demand versions of the show.  Steve has audio plus transcripts.  And somebody in the chatroom said, "Now, will Elaine understand that when we're talking about a NIC, that we're talking about an N-I-C, not an N-I-C-K?"



STEVE:  Oh, you bet she will.



LEO:  Of course she will.



STEVE:  You bet she will.



LEO:  That's why having a pro makes a big difference.  So if you like to read along with the show, GRC.com.  And while you're there would be a good time to erase your debt to society and buy a copy of SpinRite.  That's there, as well as many free things, including SQRL.



STEVE:  Improve your karmic balance.



LEO:  That's it.  Forget the debt to society.  Just what are you going to say when you meet Kerberos at the gates of Hades?  "I stole SpinRite?"  Oh, I don't think so.  So that's where you do it, GRC.com.  You can join us at our website, TWiT.tv.  We've got audio and video of the show.  But the best thing to do is subscribe because this is one of those shows you just want the complete set.  Get all 611 episodes and enjoy them all.  And the best way to do that is to subscribe.  Find your favorite app and subscribe to the show.  There's plenty of TWiT apps and podcast apps and Stitcher and Slacker and Pocket Casts and Overcast and all that.



Steve, as always, a pleasure.  Thank you for all you do for all of us.  And I will see you next week.



STEVE:  Okay, my friend.  See you then.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#612

DATE:		May 16, 2017

TITLE:		Makes You WannaCry

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-612.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  This week Steve and Leo discuss an update on the FCC's Net Neutrality comments, the discovery of an active keystroke logger on dozens of HP computer models, the continuing loss of web browser platform heterogeneity, the OSTIF's just-completed OpenVPN security and practices audit, more on the dangers of using smartphones as authentication tokens, some extremely welcome news on the Android security front, long-awaited updated password recommendations from NIST, some follow-up errata, a bit of tech humor and miscellany, closing the loop with some listener feedback, and then a look at last week's global explosion of the WannaCry worm.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here, and of course the topic of the day, the WannaCry ransomware exploit.  How did it work?  Why did it happen?  Who is at fault?  Who not to blame?  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 612, recorded May 16th, 2017:  Makes You WannaCry.



It's time for Security Now!, the show where we cover your security and privacy and safety online.  And never in our nation's great history has there been a greater need for a man like Steve Gibson, a man who can save us.  Steve, the host of the show, welcome.  It's good to see you.



STEVE GIBSON:  Well, all of our listeners knew what today's topic would be.



LEO:  Oh, yeah.



STEVE:  So I had some fun with the title.  So this week's Security Now! #612, titled "Makes You WannaCry."



LEO:  I felt so guilty because I got sick, as you know, and I missed doing the radio show.  And I think of all the shows to miss, this was the worst show I could have missed all year long.  On the other hand, the good news is there'll be more of the same soon.



STEVE:  There will.  And you can sort of - more is known about it now.



LEO:  Well, yeah.	



STEVE:  So you can do a really comprehensive wrap-up on this coming weekend.



LEO:  Well, I'm going to listen carefully to what you have to say.



STEVE:  And the other blessing is that, because there's a lot of interesting things to talk about, about WannaCry, is that it so dominated the rest of, I mean, like the entire week, that we won't have the normal 16 different topics that we also need to talk about. So we have a reasonable amount of week's news.  We're going to talk about...



LEO:  The hacking community was just all sitting back in awe.



STEVE:  Just, like, stunned.



LEO:  Watching, you know.



STEVE:  They were all running around focused on a single thing, rather than up to their regular mischief.  We have an update, sort of a follow-up on the FCC's Net Neutrality commenting system.  The discovery of an active keystroke logger present on dozens of different HP computer models.



LEO:  That just shocked me.



STEVE:  Yeah.



LEO:  I'm really curious about that.



STEVE:  It's just sloppiness.  I'm sure it wasn't malicious, but that doesn't mean it isn't a problem.  We've got sort of an interesting continuing movement that I called "the loss of web browser platform heterogeneity" that I want to talk about.  Our gang over at the OSTIF, who's been funding the review and auditing of open source software, who previously did VeraCrypt, just finished their audit, or funding and working with the audit of OpenVPN and found one bad problem and some other things to get cleaned up.  So we'll talk about that.



More on the dangers of using smartphones as authentication tokens, which has been a recurring theme for us recently.  Some very welcome news, finally, on the Android security front, coming with Android O later this summer.  We'll talk about how there's going to be a deep rearchitecting that actually is underway and already present in the developer preview that should significantly improve Android's ability to stay current with patches, even for non-Pixel and Google and Nexus devices.



Oh, and some long-awaited and much overdue updated password recommendations from the NIST.  And the number one on the list is just a kick because this is the advice we've been giving for so long, that ran contrary to the formal advice.  Well, they finally figured that out, too.  We've got a little bit of follow-up errata, a bit of tech humor, some miscellany, some closing the loop with our listeners, and then we're going to get into everything that is known about WannaCry, from how it happened, what happened, what it is, some technical details, a little bit of Microsoft CYA from Brad Smith, the president and chief legal guy there.  And that'll bring us current.  So I think another great podcast.



LEO:  Yeah, I'd love to talk to you about Brad Smith's piece because I do agree with him about some things.



STEVE:  I do, too.



LEO:  I'm not sure Microsoft is free from blame here.



STEVE:  Yes.  He disavowed any responsibility.



LEO:  Yeah, that I don't...



STEVE:  It's like, eh, well.



LEO:  But at the same time there's no such thing as perfect software.  And everybody...



STEVE:  And what does it mean, then, that they updated XP?  I mean...



LEO:  Isn't that interesting.



STEVE:  Yeah, exactly.



LEO:  Yeah.  I mean, they said never again, but there's so many XP machines out there.  I think that was them being a good corporate citizen.  But again, it's your take that matters because this is Security Now! with Steve Gibson, not some guy named Leo.  All right.  Back we go to Steverino.



STEVE:  I have to put my spinner down so I can focus.



LEO:  Oh, are you a fidgeter?  Now, you sent me, and I don't have it because my 14 year old stole it, that is the best spinner I've ever seen.  It's called the iSpin.



STEVE:  The iSpin.  Amazon does have them.



LEO:  It's made entirely of brass.  It's machined.  It does have bearings, I know, because I took it apart.



STEVE:  Yeah, you're able to unscrew the hub and sort of see them.  And he actually recommends you do that because you don't want to ever put any lubrication in it because the viscosity would slow it down.  But you want to just blow it out if it collects any dust over time.  



LEO:  Anyway, you sent me one, and I am grateful.  And one of the things that's cool, and I think I've demonstrated this already, you can remove an arm, and then it's unweighted.  And it's a great way to learn physics.  I told Michael, I said, "Try this and then tell me, come back, you can use it, but you have to tell me why that works."



STEVE:  Yup.  And I was playing with it again last night.  I have a desk lamp which is LED and dimmable.  If you dim an LED lamp, normally what they're doing is they're reducing the duty cycle of the on time.  So it creates a shorter burst, much more like a strobe.  And spinning it under a dimmed LED light is really fun because it does like the stop-motion thing.  And as it's slowing down, the interaction of the spin versus the flicker creates phased beating effects.  So, yeah.  Anyway, as I said to you before the show, I have an idea that I'll steal some time on the weekend and experiment because I think I have a fun upgrade for this.



LEO:  These things have just spread like topsy through the schools, to the point where schools are actually sending people home.



STEVE:  Actually, they've banned them, yes.



LEO:  Yeah, yeah.



STEVE:  So our Picture of the Week I got a kick out, and it's apropos of today's topic.  It's the classic Venn diagram that we all learned and is a very useful means of showing the amount of - sort of the nature of sets.  It's used in set theory for, like, you'd have two different groups that have some things in common.  Well, in this Venn diagram we've got two, so-called the "disjoint set," or the "null set," where there are two separate regions with no overlap, the one on the left labeled - oh, I'm sorry, and the title is "The Flaw in the WannaCry Extortion Scheme."



And the flaw is that there is no overlap between these two sets, the first one being people who use Windows XP, and people tech-savvy enough to figure out how to pay ransom with a bitcoin.  And of course you're going to have people infected who can't pay if they want to because that's like, what?  What's a bitcoin?  And then you've got all the bitcoin people who are like, I would never be using XP anymore.  You crazy?  And it is an interesting aspect of this last week in WannaCry that, as extensive and as huge a cyber event as it has been, the last number I saw was like $53,000 in equivalent bitcoin payments.



LEO:  Yeah.  Brian Krebs says that - he wrote a book called "Spam Nation" where he talks about this, that that's not unusual.  That these guys don't really care.  If they make 50 grand they're very happy, some guy in Moldova who is unemployed.  And they don't really care that it's the consequence to billions of dollars' worth of consequences because they got 50 grand.



STEVE:  Well, and - right.



LEO:  Although the latest news makes me think there's more to it.  But go ahead.



STEVE:  Well, the backup for that idea also is that previous malware didn't make anybody any money.



LEO:  Right.



STEVE:  Very much like the Gmail phishing scheme that we talked about a couple weeks ago, it didn't do anything bad to you.  It was just sort of a proof of concept.  It's like, oh, I'm just going to propagate because I can.  So this would have happened, even if it didn't have the ransomware backend, just because you could.  Like all of, like, MSBlast and Code Red and Nimda and all of these previous Internet worms.  So the fact that it made money was just sort of a bonus.  But the fact that it encrypted people's systems, I mean, like all the important files on their system and brought huge chunks of IT infrastructure to its knees made it much more devastating than if it had just been a worm propagating for the worm's sake.  Anyway, we'll get to that in a second.



LEO:  I guess the point is that, depending on who created it, the devastation may indeed have been the goal.



STEVE:  Oh, interesting.



LEO:  Maybe you haven't heard the latest.



STEVE:  I haven't heard the latest.



LEO:  I'll fill you in when we get to it.



STEVE:  So definitely come back when we get there.  The FCC is acting a little oddly.  They're reporting spamming of the FCC Net Neutrality commenting site, and they claimed DDoS attacks.  And the moment I heard that, I remembered that my own experience - and I don't think I did it that night, but it was like sometime later the following day.  The site is just horrifically designed.  I don't know what the database backend is.  But as I mentioned on the show last week when I talked about the GoFCCYourself.com, which was the referral domain that John Oliver gave us on HBO's "Last Week Tonight with John Oliver" Sunday before last, was that you couldn't get a total of the number of the comments.  It was just very clunky in its design.  So my suspicion was that it was just lots of people trying to submit comments, and it wasn't a classic DDoS attack.



LEO:  And there was spamming happening from the anti-Net Neutrality side.



STEVE:  Yes, yes.



LEO:  They were all duplicate comments.



STEVE:  Correct.  So it wasn't even very well done spamming.  And I did note also that their site doesn't have any bot protection on it.  So it was like, okay, again, another strike against it.  Who's going to do a good form submission that doesn't prevent it from being spammed?



LEO:  It's a government website.  What do you expect?



STEVE:  Well, yes, the government, exactly.



LEO:  But, by the way, the same thing happened last time.  The difference was that the Chairman of the FCC didn't declare that they've been DDoSed and close the whole thing down.



STEVE:  Right.  Yeah.  So a guy named John Bambenek, a threat intelligence manager at Fidelis Cybersecurity, was quoted saying:  "There don't appear to be any indications of a DDoS attack in the sensors we use to monitor for such things.  It appears the issue with the FCC is less of a DDoS attack as traditionally defined, and more of an issue of crowdsourcing comments generated by John Oliver and reddit," who also picked up on it.  And of course we did on this podcast on the following Tuesday, which was last week.



Also Jake Williams, CEO of cybersecurity firm Rendition InfoSec, said the FCC "offered no support" to prove a DDoS had occurred, adding:  "There was no observed dark web chatter about such a DDoS before or after the event, and no botnets that we're monitoring," he said, "received any commands ordering a DDoS on the FCC's site."  And also, when the press asked for additional background from the FCC on the so-called DDoS attacks, no response was forthcoming.  So I think they were unfortunately just probably embarrassed by the amount of both legitimate and illegitimate traffic that came to there in order to lodge their protest.



This was much tweeted from our listeners, the news that the HP Conexant audio driver was found to be logging keystrokes.  As Ars put it:  "HP is selling more than two dozen models of laptops and tablets that covertly monitor every keystroke a user makes.  The devices then store the key presses in an unencrypted file on the hard drive."  So in looking at this, several of the characteristics of this argue that it was not deliberate, that it's just highly irresponsible keystroke logging with permanent stroke-by-stroke persistent storage that has been in place for apparently maybe as much as two years, or more than two years.



The driver, which is digitally signed by Conexant, who is a legitimate audio chip maker that HP uses in many of their machines, was digitally signed on the day before Christmas, December 24th of 2015, so more than two years ago.  The security firm that first spotted this, modzero, disclosed this information, noting that v1.0.0.31 was then later extended with even more problematic functions.  The most recent version is 1.0.0.46, which implements the logging of all keystrokes, they wrote, into the publicly for any user readable file.  It's at C:\Users\Public\MicTray.log, M-I-C-T-R-A-Y dot log.  So any users of HP machines ought to, if you're curious, go look at C:\Users\Public\MicTray.log to see if your passwords are all present there. 



Now, here's why, you know, several things suggest that this was not deliberate or malicious.  For example, the file is not appended to after each login.  It is overwritten.  So it feels like some forensics and developer code that was there for development was left in.  And these guys wrote, so like trying to make it seems as bad as possible because they're the security firm who found it, so they want to have found something big, they wrote:  "If you regularly make incremental backups of your hard drive, whether to the cloud or to an external hard drive, a  history of all keystrokes for the last few years could probably be found in your backups."



Okay, well, yes, because that's how you would extend the badness from the fact that this is overwriting that file with each login, rather than deliberately continuing to persist them over time.  They wrote that the keylogger is included a device driver developed by Conexant, a manufacturer of audio chips that are included in the vulnerable HP devices.  They said:  "This type of debugging turns the audio driver effectively into keylogging spyware.  On the basis of meta-information of the files, this keylogger," they wrote, "has already existed on HP computers since at least Christmas 2015," as I had said.



So what can we do?  There is a program, this MicTray, either MicTray, M-I-C-T-R-A-Y, 32 or 64 dot exe, which exists in the Windows System32 directory.  So they suggest deleting or renaming the executable so that no keystrokes are recorded anymore.  You will lose some functionality.  The reason an audio driver has any business with the keyboard, I should explain, is that it listens to keystrokes globally within the system in order to implement hotkeys.  So this Conexant driver supports various hotkeys for, like, muting the audio or turning it up and turning it down.  And so this was probably put in there for that reason, for diagnostics and debugging, but then just left on by mistake.



Now, this is one of those things which probably in itself wasn't malicious.  It isn't sending them anywhere.  It isn't phoning home.  It isn't doing any of the things it could be doing.  But it's also become much more worrisome, now that it is widely known and has been publicly disclosed, such that, if anything nasty gets into an HP machine, it would almost be malpractice on the malware's part not to go check to see if it can suck up all the keystrokes which have been logged since you logged in.  So it's worth taking some action.  I imagine that HP will quickly get an update from Conexant and push that out, make updates available so it's worth, you know, depending upon your profile, who you are.



What you probably could do would be to replace the log file with an empty text file, rename it, and then set it to be read-only.  That's a simple prophylactic measure.  If you create a file that it wants to create and make it empty and then set it to read-only, most software is not smart enough to look for that, change it to writeable, and then delete it, and then start writing to it.  So my guess - and you could easily just tell, you know, you can see whether the scheme, the simple strategy works.  If so, that'll keep it from logging anything because it just won't be able to until you get yourself an update that no longer tries to do that at all.



But this is an interesting opportunity, and this ties into something we'll be talking about in a few minutes, and that is to note that the Windows platform has never been secure.  I mean, it predates a worry that we have today about security, or a consciousness, an awareness of security.  For example, the fact that debuggers have the ability to attach themselves to any other process is both convenient for developers and horrific for security.  When we talk about DLL injection, that's something that you can do.  You can just inject a DLL into some other program's operating process space and cause it to run.  So now you're running your code or that DLL code in the context of another process, which the process probably doesn't want to have happen.  But Windows is like, oh, well, you know, think of the flexibility.  It's like, uh-huh.



And, you know, this goes back to my original Windows metafile observation, where I didn't think it was malicious that Microsoft allowed a Windows metafile to contain native code, which a metafile tag allowed you to jump to.  It made total sense to me that back then some developer thought, well, if we need some function that we didn't build into the metafile interpreter, let's just let it run natively.  And everyone thought I was crazy for thinking that.  It's like, no, you just don't understand the world in which all this happened.



So similarly, Windows apps can freely establish what are known as "global hooks" for many different things.  You can hook the mouse.  You can hook the keyboard.  You can hook screen events. And this allows the kind of flexibility that we're used to having in Windows, where you can have macro programs that are able to record keystrokes and inject the keystrokes into applications for macro playback.  Well, isn't that handy.  Well, yes.  And you can also get up to all kinds of mischief doing the same thing.  And various of the Windows assistive features depend upon those sorts of things.  And of course we know that programs can capture the screen, which you may not want them to do, if it happens to have sensitive content on it at the moment.



So Windows traditionally has been very developer-friendly and a power user's environment.  And of course this is part of what Microsoft plans to be changing with Windows S, which implements among other things a highly restrictive application sandboxing so that all of these sorts of things which have historically been possible on the Windows platform, those will be lost under Windows S.  And so there will be a definite tradeoff between the power user functionality and flexibility we're used to and Windows S, which will be implementing a next-generation API with applications that you can only run if you get them from the Windows Store, and everything that that brings with it.  So it's kind of like the closed environments that we see, for example, with Apple and the iOS environment where, yes, you do get better security, in trade for and losing a lot of the freedom and flexibility that can be convenient and handy.



And this brings me into the next topic, which I called "We're Losing Browsing Heterogeneity."  BleepingComputer had a nice post.  I spent some time there because they also had a lot of coverage about all of the WannaCry stuff.  But I liked their coverage of this.



They wrote:  "A one-liner in the Windows Store policy is the reason why we'll never have the original Chrome, Firefox, Opera, or other browsers available through the official Windows Store.  Included in the Security section of the Windows Store policy, this line is specifically addressed at browsers and reads the following," they wrote, quote from Microsoft:  "Apps that browse the web must use the appropriate HTML and JavaScript engines provided by the Windows platform."  And that's not just any Windows platform, that's the new universal Windows platform API applications.



So BleepingComputer goes on to say:  "This means that every browser currently listed on the Windows Store is nothing more than an offshoot of Microsoft's EdgeHTML, the HTML and JavaScript engines found in Edge."  And then BleepingComputer notes:  "Apple and Google have similar policies."  So this is not just something Microsoft is doing, and I don't mean to paint it as that.  That's why this is bigger than that.  What we're seeing is this loss of browser choice in platforms moving forward.



BleepingComputer says:  "The policy is similar to what Apple has done with iOS, the company forcing apps to use its web rendering engine to process web content.  Google took a harder stance with ChromeOS and forbade other browsers altogether.  In both cases the companies cited security concerns as their engineers have worked to secure their operating system around those web rendering engines.  In the case of the Windows Store," they write, "this revelation came to light after the launch of Windows 10 S, a tethered version," as they described it, "of Windows 10 that will only allow the installation of Windows Store apps.



"As users kept asking when will Google and Firefox port their browsers to the Windows Store so that they could use them in Windows 10 S, the question was finally answered last week when a developer tried to convert his Chromium-based browser to an .appx version compatible with Windows Store distribution.  The Windows Store crew specifically told the developer that they cannot approve his browser because of the aforementioned policy that mandates that all apps that access the Internet use the approved HTML and JS rendering engines," JavaScript rendering engines.  "The developer shared his experience and official communications with a ZDNet journalist."



So, "Microsoft's policy effectively bans standalone third-party browsers.  While some were hoping to see Chrome or Firefox available on the Windows Store as UWP" - that's the Universal Windows Platform, the next-generation API that replaces the earlier Win32 and .NET platforms - "this may never be possible, as this would mean that Google, Firefox, and other vendors would need to rewrite their browsers from scratch to use Microsoft's EdgeHTML."



And they said:  "This will never happen unless Windows 10 S becomes a huge success, and browser vendors see a benefit to port their browsers.  In this case we won't see UWP versions of the original Chrome and Firefox engines, but only so-called bastard browsers," as BleepingComputer describes them, "like we have on iOS.  For example, you can't call Firefox for iOS a true Firefox browser.  It's just an older version of the WebKit engine with a Firefox UI lookalike on top, and lacking many of Firefox's original features.



"The conclusion is that Microsoft has effectively banned any self-standing third-party browser from the Windows Store.  Additionally, Windows 10 S users better get used to using Edge or any of the other" - and here again they say - "bastard browsers that use Edge's repackaged core."  And I'm a fan of Edge.  As we've talked about on this podcast, it is a beautifully reimplemented from scratch, you know, they scrapped IE, and they started over, and they did a whole lot of things very right.  I mean, it's a beautiful piece of technology.  So I don't mean to disparage it in any way.  It just means that, if you're going to go Windows 10 S, Edge will be your browser.  Which is not a bad thing, but it also means your browser won't be, can't be, actual Firefox or Chrome.  That will no longer be an option.  And of course we also know that you no longer have a choice in search providers.  If you go 10 S, Bing is your search engine, and that cannot be changed.



And finally the article ends, saying:  "The original Chrome and Firefox browsers" - what we have today - "built around their native engines will remain accessible to Windows 10 users via standalone installers only."  But again, 10 S will refuse them.  So I just - I thought this was interesting because browsers are now the way we connect to the Internet.  We've noticed that they are also the primary point of attack.  They're the way bad things get into our systems and into networks, much more so today than it used to be when it was something quaint like email macros or Flash exploits.  Now it's the browser.



And so it makes sense from Microsoft's standpoint, if they're determined to make Windows secure, then the freedom and the flexibility that we have that Windows has historically enjoyed, that will be lost.  Windows 10 S will be a choice.  And I guess I did see somewhere - I'm sure you're more up to speed on this than I am, Leo, because I have not looked at it.  As I understand it, 10 S users have an opportunity to move to Windows 10 Pro, like for the rest of the year or something.



LEO:  If you buy Microsoft hardware, yes.  So if you buy the new Microsoft laptop, which comes with 10 S, and this is the only Microsoft device that does, you get a free upgrade through the rest of the year.  And that's the only way they can get people to buy it because nobody knows if 10 S will do.



STEVE:  Right, right.



LEO:  It might be another [crosstalk].



STEVE:  It might be another Windows Phone.



LEO:  Yeah.  Well, exactly.



STEVE:  So our friends at the OSTIF have...



LEO:  Oh, them.  Who are they?



STEVE:  Working with Quarkslab.  They're the guys that have been financing a number of these open source audits, to everyone's benefit.



LEO:  Good.



STEVE:  OSTIF.org.  And they just finished, actually I know of this because they sent me a tweet.  They sent a tweet saying:  "Steve, you did excellent coverage of our VeraCrypt audit on Security Now!.  We just published our OpenVPN results."  And the results were very worthwhile.  The audit was worthwhile, and it did drive a version change and update and fix from OpenVPN.  I just checked the log on a FreeBSD machine, and it doesn't yet know about 2.4.0, which is now the latest and what anyone using OpenVPN should go to.  It's not a massive problem.  I mean, there was one vulnerability rated critical/high.  So it's worth doing.  But your OS will probably need to wait for a build for it of OpenVPN, and 2.4 is what you want.  I had, I think, one of my machines is 2.3.13 or something.  So they audited...



LEO:  2.4.1, is that...



STEVE:  2.4.anything would be good.



LEO:  Oh, yeah, good.  Thank you, Arch.



STEVE:  Yes.  So they audited OpenVPN and the NDIS6 TAP driver, the Windows GUI and the Linux versions.  Oh, wait, wait, I'm sorry, I was wrong.  They audited 2.4.0.  And what was updated was 2.4.2, so you do need a dot increase to .2.



LEO:  Checking my updates right now.



STEVE:  So the auditors wrote:  "The public disclosure of these vulnerabilities coincides with the release of OpenVPN 2.4.2, which fixes all of the high priority concerns.  OpenVPN," they wrote, "is much safer with these audits, and the fixes applied to OpenVPN mean that the world is safer when using this software."  They wrote, "We have verified that the OpenVPN software" - and this is important - "is generally well-written, with strong adherence to security practices."  And you want that because even an auditor can miss something.  But it's worth taking some comfort in just the general conduct of the people who wrote this over time that they kept their eye on the ball, they didn't get sloppy, and that there was a lot of attention and focus put on or kept on the security aspect.



I do have a link here, you can just go to OSTIF.org to find it, but I've also got a link in the show notes to the whole report that goes into greater detail.  But it was one critical/high vulnerability fixed, one medium vulnerability, and then five low or informational vulnerabilities and concerns.  So again, as we've said, it's great to have the source open, but it only is useful if somebody who didn't write it, reads it.  And these guys did.  So again, thanks to the OSTIF for pulling together the funds to finance people who are security aware looking at their source code.



There's a bitcoin exchange, they believe that they are among the biggest, if not the, named Kraken, who describe themselves as a San Francisco-based world's largest bitcoin exchange in euro volume and liquidity.  They write of themselves:  "Kraken's clients also trade U.S. dollars, Canadian dollars, British pounds, Japanese yen, and other digital currencies on a platform consistently rated the best and most secure bitcoin exchange by independent news media."



They said:  "Founded six years ago in 2011, Kraken was the first bitcoin exchange to have its market data displayed on the Bloomberg Terminal, the first to pass a cryptographically verifiable proof-of-reserves audit, a partner in the first cryptocurrency bank, and one of the first exchanges to offer leveraged bitcoin margin trading."  Oh, wonderful.  "Kraken is trusted by hundreds of thousands of traders, institutions, and authorities across the world," they write, "from Tokyo's court-appointed trustee to Germany's BaFin-regulated Fidor Bank."  And, finally:  "Kraken is backed by investors including Hummingbird Ventures" - oh, those are real guys - "Blockchain Capital, and Barry Silbert's Digital Currency Group, among others."



I say all that to preface their interesting comments about using and rethinking the security of phones, smartphones specifically, as authentication tokens.  They wrote:  "Heed this or perish."  They say:  "Let's begin with the assumption" - which is a little specious, but still it's what they want us to assume - "that within 24 hours your usual mobile phone number will be hijacked by social engineers."  As I said, okay, well, maybe for some people.  "They will use your number to gain access to every" - but certainly this is possible, if it happened.  "They will use your number to gain access to every account you own that utilizes phone-based authentication and account recovery, such as your email.  They will then use that access and information to compromise more accounts, and harass, steal, blackmail, and extort you and your associates.



"In the past month," they write, "there have been at least 10 cases of people publicly involved in the cryptocurrency scene being victimized by mobile phone hijacking."  Okay, so targeted attacks.  "The consequences have been expensive, embarrassing, enduring, and in one case life-threatening.  If you are in any way publicly involved in cryptocurrency, consider yourself an active target.  You need to immediately audit the security of your accounts, especially email, social media, social networking, and mobile phone.  Somehow," they write, "the masses have been led to believe" - yes, well, and we know how - "that phone numbers are inextricably bound to identities and therefore make good authentication tools."  Of course we on the podcast, as we've been covering recently, know otherwise.



They say:  "There's a reason that Kraken has never supported SMS-based authentication.  The painful reality is that your telco operates at the security level" - and I got a kick out of this - "of a third-rate coat check clerk.  Here's an example..."



LEO:  Where can you find a coat check clerk these days?



STEVE:  Yeah.  "Here's an example interaction.  Hacker:  Can I have my jacket?  Telco:  Sure, can I have your ticket?  Hacker:  I lost it.  Telco:  Well, do you remember the number?  Hacker:  No, but it's that one right over there."



LEO:  Oh, boy.



STEVE:  "Telco:  Okay, cool.  Here you go.  Please rate us 10 out of 10 on the survey."  Uh-huh.  And I won't go on.  The article goes on, for anyone who's interested.  But I appreciated that.  I thought, first of all, it was interesting that the known weakness in smartphone authentication via SMS, as we've discussed, is now being used increasingly in targeted attacks.  For people who are not prone to being targeted, you shouldn't run around with your tinfoil hat on, and the sky is not falling.



But given a choice, you absolutely want time-based authentication, not an SMS message per instance.  And if you can disable SMS in favor of anything else for account recovery, that would be good because remember that typically SMS is hopefully only an additional factor.  Somebody first has to have your username and your password, and then also another second factor beyond knowing your password.  The problem is it's often used for account recovery.  I forgot my password.  Oh, well, we'll send you a blurb to your phone number in order to recover it. Well, that's the huge Achilles heel in where we are today is account recovery.



And it's funny, I mean, in anticipation of this, SQRL already incorporates a facility for preventing that.  Once a user becomes familiar with SQRL and realizes and understands how it works and gets it, they're able to set a sticky switch in SQRL that asks the sites they visit to please disable, preemptively disable any non-SQRL account recovery because, again, if you leave that there, then that's still a glaring hole in your authentication.  So this essentially is sort of a beacon that your use of SQRL broadcasts, as you use it, which you would only turn on once you understand how it works and you're comfortable with it, so it's not on by default.  But once you want to, as you visit sites again with SQRL, it makes a sticky setting at the site saying "Decline anything but my subsequent use of SQRL."  So again, lots of nice little tidbits that have always been there.



And the good news for Android, speaking of happy tidbits, this is from the Android Developers Blog.  They wrote:  "On the Android team" - and I've paraphrased this down for the podcast, but I'll say it in their voice:  "We view each dessert release as an opportunity to make Android better for our users and our ecosystem partners.  One thing we've consistently heard from our device-maker partners" - and, yes, from this podcast - "is that updating existing devices to a new version of Android is incredibly time consuming and costly."  As we know, like leaving IPv4 is time consuming and costly, so you don't do it unless you have to.  Leaving SHA-1, same thing.  I'd rather not.



So they write:  "With Android O, we've been working very closely with device makers and silicon manufacturers to take steps toward solving this problem, and we're excited to give you a sneak peek at Project Treble..."



LEO:  Oooh.



STEVE:  "...the biggest change to the low-level" - thank you, Leo - "a low-level system architecture..."



LEO:  I did it wrong.  Ooooooh.



STEVE:  The crowd goes wild.



LEO:  Ooooooh.



STEVE:  Oooh.  The biggest change to the low-level system architecture of Android ever.



LEO:  I'm getting ready for tomorrow's keynote at the Google I/O conference.



STEVE:  Good.  Oh, and you're going to be there, aren't you.



LEO:  Yeah.  And this is one of the announcements from it, I'm sure.



STEVE:  The traditional cumbersome and time-consuming update and patch flow has been this:  First, the Android team publishes the open-source code for the latest release to the world.  The silicon manufacturers, the companies that make the chips that power Android devices, must then modify that new release for their specific hardware.  Next, the silicon manufacturers pass the modified new release to device makers, the companies that design and manufacture the Android devices.  The device makers then modify that new release again as needed to customize it for their devices.  The device makers work with the carriers to test and certify the new release.  And, finally, the device makers and/or the carriers make the new release available to users.  So is it any surprise that we're still using IPv4 and that most Android devices are way behind?  That process is so clunky that it just doesn't happen.  



"Project Treble," they write, "re-architects Android to make it faster, easier, and less costly for manufacturers to update devices to the new version of Android.  Android has been a fabulous success" - patting themselves on the back, I mean, and it's true in the marketplace - "because it presented and rigorously enforced, through its compatibility test suite known as CTS, its application-layer API."



And, now, again, we understand because we've discussed it often in terms of the architecture of modern computer design.  The reason that Windows was able to succeed as it did is that the Windows API allowed applications to be written to the API.  And as long as future and succeeding versions of Windows maintained that API layer consistency, they could change the plumbing underneath, and apps were compatible.  That's how Windows got its backwards compatibility.  So what Android did with this compatibility test suite was to implement, literally it's a million different tests that validate and verify the consistency of that crucial interaction between applications and the Android OS, and everything that is not the application.



"Project Treble," they write, "will be doing for the Android OS framework" - that is, the under-plumbing, ooh, I like that term - "what CTS did for apps.  The core concept is to separate the vendor implementation  the device-specific, lower-level software written in large part by the silicon manufacturers  from the Android OS framework."  In other words, they're going to create another API layer, another interface layer way down in the OS, which will be similarly rigorously enforced.



They write:  "This is achieved by introducing a new vendor interface between the Android OS framework and the vendor implementation layer.  The new vendor interface will be validated by a Vendor Test Suite (VTS), analogous to the CTS" - which was the Compatibility Test Suite at the application layer - "to ensure forward compatibility of the vendor implementation."  Or the other way to look at it is backward compatibility.  As Android is updated and patched, those will no longer need any vendor involvement.



They write:  "Once a stable vendor interface has been defined to provide access to the hardware-specific parts of Android, device makers can deliver new Android releases and updates to consumers only by updating the Android OS framework, without any additional work required from the silicon manufacturers."  So the silicon manufacturers, they don't want any of this.  They just want to pump out silicon.  So they've been forced to become software developers to support their silicon.  And unfortunately, they're always the starting point for the percolation of updates.  It's got to go to them first.  And then it moves back up the system, finally, maybe, eventually getting to the user.



So they say:  "The underlying silicon manufacturers will not need to be constantly bothered and bombarded with updates which require their constant attention and involvement.  This will all appear in Android O, and the new Project Treble architecture is already up and running on the Developer Preview of O for the Pixel phones."  And O is slated for formal launch later this summer.  And we'll find out more about it on tomorrow's Google presentation.



LEO:  Yeah, I think so.



STEVE:  Neat.  Oh, I bet, for sure.  That's big news.  That's - I'm delighted.  I mean, we talk about the troubles that Android has, and we've been saying that at this point really the only recourse a really truly security-conscious person has is to stick with one of the very major players that are demonstrating that they are right on, really up to speed on keeping the phones current.  This, though it's late, I think it's exactly what Google had to do in order to fix this problem with Android.



LEO:  This is kind of like a Ring 0 kind of a thing.



STEVE:  Bravo, yes, yes.



LEO:  So you have to be highly privileged.



STEVE:  Yeah.  Well, I would say, what it is, it adds another layer.  At this point Android only had two layers.  It had the application layer and everything else.  So there was one border definition which was the application API.  What Google has done is they've added another layer.  And Microsoft did this.  That's the HAL, the Hardware Abstraction Layer in NT.  It never really got used.  Well, it did for a while because there was that funky MIPS support, remember, that NT used to - there was a MIPS support, and I want to say ARM, but maybe that was pre-ARM.



LEO:  Oh, no, there was ARM, I think, yeah.



STEVE:  Yeah.  There were a few other hardware architectures.  So Microsoft's original NT, that whole redesign, the HAL, the Hardware Abstraction Layer, was meant to allow the NT OS to run on very different hardware.  Again, what does abstraction mean?  It means you create a virtual hardware definition.  The software talks to that, and the hardware beneath it emulates that hardware definition.  So that's what Android O does.  It creates another sort of a slice, a cut point through the OS where everything hardware is below, and the hardware only has to meet the interface spec at that layer.  And then Android OS only needs to talk to the interface spec.  So nice piece of work.



And there is always a performance hit.  Whenever you abstract and create a definition, what that actually is is an interpretation.  It's an interpreter.  And we know that those are never as fast as native.  So it may well be that 10 years ago - Android launched in '07.  So 10 years ago they couldn't afford the hit.  They needed every bit of performance that they could get.  And so just they couldn't do another interpretation layer down in the OS.  They had to go full speed.  Now chips have become so much more powerful, with ridiculous numbers of cores in a smartphone, that they say, yeah, now we can afford an interface layer for the hardware, and we're not going to feel it because the hardware has gotten fast enough to mask any interpretation overhead that might be introduced.  So bravo.



LEO:  Hey, hey.  That's good.



STEVE:  And speaking of bravo, check this out.  The NIST, what does that stand for, National Institute...



LEO:  National Institute for Standards and Technology.



STEVE:  Yes, thank you, has updated their formal recommendations, removing, among other things, periodic password change requirements.  Gone.



LEO:  Yay, yay, yay.



STEVE:  I know.  How many times have we said, "That makes no sense.  Do not make people change their passwords."  And so many corporate listeners of ours say, yeah, my company makes me change my password every month.  And then we talk about all the workarounds, like sometimes you can't change to the one you had before, so then...



LEO:  Oh, those are really crazy.



STEVE:  People have, like, probed the system and realized it has a six-deep stack, and so they change their password to six nonsense things and then back to the original one to force the memory off the back of the stack in order to not have to change their password.  I mean, and anyway, so NIST said this guideline was suggested because passwords should be changed when a user wants to change them, or if there is an indication of breach, not just because some amount of time has passed.  And a guy named Mike Wilson, who's a founder of PasswordPing, said:  "There have been multiple studies that have shown requiring frequent password changes to actually be counterproductive to good password security."  Yeah, surprise.



So anyway, the good news is remember the old slogan back in the early, well, back in the mainframe days of IBM?  "Nobody ever got fired for choosing IBM."  Even though they, like, had lost their edge, they were no longer like the best, other non-IBM solutions were arguably less expensive, higher performance, more reliable, better service contracts, I mean, they were, like, better.  But it was like, ooh, crap, you know, what if I'm wrong?  Well, nobody ever got fired for choosing IBM.  Well, similarly, the NIST guidelines that were saying, yes, you should change your passwords periodically, well, IT departments probably had no choice but to follow the guidelines because then they couldn't get fired.  It was like, nobody ever got fired for not doing...



LEO:  Well, it's one less reason to fire you, anyway.



STEVE:  Yeah, exactly.  So everybody listening, if your company is saying you must change your passwords, in the show notes I have the link to the entire document.  In fact, that document is incredible.  Actually, it was just published today, by the way, Tuesday, May 16.  It just came out.  And I don't know, I mean, I've got a tiny little scroll thumb here.  I don't know how many  equivalent pages it is.  It goes on and on and on and on.  Anyway, it'll be the topic of a future podcast because there's a ton of stuff about authentication in there that I think will make some interesting material for our listeners.



LEO:  Good, good, good.



STEVE:  They also say drop the algorithmic complexity song and dance.  "No more arbitrary password complexity requirements needing mixtures of uppercase letters, symbols and numbers," NIST wrote.  "If a user wants a password that is just emojis, they should be" - god, I wonder how this happened at NIST.  They must have just had a...



LEO:  They read some papers, I guess.



STEVE:  Yeah.



LEO:  Somebody got in there.



STEVE:  Wow.  Just emojis, "they should be allowed."  They said, "It's important to note the storage requirements."  And of course, as we know, salting, hashing, and MACing such that, if a password file is obtained by an adversary, an offline attack is very difficult to compromise.



LEO:  We also know that it's better to have variety in your password.



STEVE:  Well, it's better to have...



LEO:  But you don't want to tell the hacker what the limitations are.  Right?



STEVE:  Correct.  Yeah.  What you always want is entropy.  Entropy is the key.  So however you obtain entropy, that's what you want.  And so, for example, Mike here, again from PasswordPing, said:  "Like frequent password changes, it's been shown repeatedly that these types of restrictions," that is, on password complexity, "often result in worse passwords."



LEO:  Yeah.  That's the other reason, yeah.



STEVE:  So let people do what they want, but enforce.  Enforce.



LEO:  Right.



STEVE:  And again, I have sort of a nice compromise in SQRL.  I built an on-the-fly complexity evaluator, but it doesn't work by enforcing any specific guidelines.  Users can do anything they want.  It's just that the better their passwords are, the shorter it can be and rank a higher level of complexity.



LEO:  Clever.  [Crosstalk].



STEVE:  So that's built in there, too.  I figured while I was at it.  Oh, and then, finally...



LEO:  Now NIST-compliant.



STEVE:  Oh, goody.



LEO:  SQRL, now NIST-compliant.



STEVE:  And you are never required to change your password.  But finally they said:  "Require screening of new passwords against lists of commonly used or compromised passwords."



LEO:  No more monkeys.



STEVE:  It's right here in the show notes.  It says "No more monkeys."



LEO:  I wasn't reading that.  That came out of my head, straight out of my head.



STEVE:  And mine because of course we've all had a lot of fun with that.  "NIST notes that dictionary words, usernames, repetitive or sequential patterns all should be rejected.  One of the best ways to ratchet up the strength of users' passwords is to screen them against lists of dictionary passwords or known compromised passwords."  And of course "monkeys" used to be number six.  It's fallen down to 123456, unfortunately.  Anyway, we will come back to these guidelines in the future because, oh, this document is full of goodies.



Justin Garrison shot me a note saying:  "Quick correction on last week's Security Now! 611 with respect to AMT over WiFi.  AMT can be set up with Intel WiFi chips, but it does require additional settings to be enabled."  Paraphrasing, if you have a laptop that supports AMT, where AMT has been provisioned, if AMT has had its optional wireless support also turned on, and if you're running Windows, then connecting your laptop to a public wireless network means that AMT is accessible to anyone else on that network, that is, your AMT in your machine, the null authentication bypass vulnerability, is present.  So if that machine has not received a firmware update, anyone will be able to access the AMT system within that machine using the null authentication bypass.  If you're a corporate IT department, and if you have AMT enabled over WiFi, turn it off, now.  And I've got a link for additional information in our show notes.  And Leo, we have reached our lighter side moment. 



LEO:  Ah.



STEVE:  A two-minute-and-40-second, brilliantly conceived and assembled, spoof on the Amazon Echo which appeared on last Saturday night's SNL, "Saturday Night Live."  If you google "Saturday Night Live," or I guess you can just google "Amazon Echo Silver"...



LEO:  That'll do it.



STEVE:  ...our listeners can find it.  But I wanted to play the audio and show it to our podcast video listeners, just because it's a bit of really brilliant fun.



LEO:  And what's really nice about this one is it works really well in audio, as well as video.



STEVE:  Yeah.



LEO:  And I have but one word to say, which is "Alessandra."  Listen.



[BEGIN VIDEO CLIP]



ANNOUNCER:  The new Amazon [indiscernible] has everyone asking Alexa for help.



ELDERLY PERSON:  Alyssa, what time is it?  What the hell is wrong with this blasted thing?  Amanda.



ANNOUNCER:  But the latest technology isn't always easy to use for people of a certain age.



ELDERLY PERSON:  Kids done bought me a busted machine again.  Odessa.



ANNOUNCER:  That's why Amazon partnered with AARP to present the new Amazon Echo Silver, the only smart-speaker device designed specifically to be used by the greatest generation.  It's super loud and responds to any name even remotely close to [indiscernible] so they can find out the weather.



ELDERLY PERSON:  Allegra, what is the weather outside?



ECHO SILVER:  It is 74 degrees and sunny.



ELDERLY PERSON:  Huh?



ECHO SILVER:  It is 74 degrees and sunny.



ELDERLY PERSON:  Where?



ECHO SILVER:  Outside.



ELDERLY PERSON:  What about it?



ECHO SILVER:  The temperature outside is 74 degrees and sunny.



ELDERLY PERSON:  I don't know about that.



ANNOUNCER:  The latest in sports.



ELDERLY PERSON:  Clarissa, how many did Old Satchel strike out last night?



ECHO SILVER:  Satchel Paige died in 1982.



ELDERLY PERSON:  How many he hit?



ECHO SILVER:  Satchel Paige is dead.



ELDERLY PERSON:  He what, now?



ECHO SILVER:  Died.



ELDERLY PERSON:  Who did?



ECHO SILVER:  Satchel Paige.



ELDERLY PERSON:  Huh.  I don't know about that.



MALE VOICE:  Even local news and pop culture.



ELDERLY PERSON:  Anita, what them boys up to across the street?



ECHO SILVER:  They are just playing.



ELDERLY PERSON:  They what now?



ECHO SILVER:  They are just playing.



ELDERLY PERSON:  You say they're just playing now?



ECHO SILVER:  Yes, they are just playing.



ELDERLY PERSON:  I don't know about that.



ANNOUNCER:  Pair it with smart devices like your thermostat.



ELDERLY PERSON:  Oh, Sandra, turn the heat up.



ECHO SILVER:  The room is already 100 degrees.



ELDERLY PERSON:  Are you trying to kill me, Alizay?



ANNOUNCER:  The new Amazon Echo Silver plays all the music they loved when they were young.



ELDERLY PERSON:  Angela, play black jazz.



ECHO SILVER:  Playing, uh, jazz.



ANNOUNCER:  It also has a quick scan feature to help them find things.



ELDERLY PERSON:  Amelia, where did I put the phone?



ECHO SILVER:  The phone is in your right hand.



ANNOUNCER:  And it has an "uh-huh" feature for long, rambling stories.



ELDERLY PERSON:  So then I gave him $5.  And he said I only gave him $1.



ECHO SILVER:  Uh-huh.



ELDERLY PERSON:  I said, "I know I gave you a five."



ECHO SILVER:  Uh-huh.



ELDERLY PERSON:  Because I only had a five and a one on me.



ECHO SILVER:  Uh-huh.



ELDERLY PERSON:  And here's the $1 right here.



ECHO SILVER:  Uh-huh.



ELDERLY PERSON:  Well, I mean, you tell me who's crazy.



ANNOUNCER:  Amazon Echo Silver.  Get yours today.  I said, GET YOURS TODAY.  To order Amazon Echo Silver send a check or money order to Amazon.com right now.



[END VIDEO CLIP]



LEO:  So funny.  I've played it now several times, and it doesn't get any less funny.  It's still amazing.



STEVE:  It's just, well, yeah.  The acting is great, the various performances, and just well conceived.



LEO:  Yeah, yeah, yeah.  Very nice.



STEVE:  A couple weeks ago, Leo, a caller to your weekend show was asking you about adding smartphone connectivity to an older auto.



LEO:  Right.



STEVE:  And it may not surprise you to know that I have a solution.  It turns out...



LEO:  You have an older auto, as well.



STEVE:  I do.  And I now have a hands-free speakerphone on my phone, and a USB that can play a thumb drive directly in the audio system without the need to use the cigarette lighter FM transmitter kludge.  It's a little more expensive, but this is an amazing company.  It's called Grom Audio, G-R-O-M A-U-D-I-O, GromAudio.com.  They have a range of different retrofits for a huge range of older autos.  So I just wanted to share that both with you for the future and also with our listeners, who may be wanting to look at increasing an older auto's connectivity without exposing it to more dangers from the Internet and so forth.



LEO:  How does it connect to the automotives?



STEVE:  All of the in-dash entertainment systems have a big monster plug on the back, like for supporting CD changers and other add-ons.



LEO:  Right, right.  You have to get under the dash to do it.



STEVE:  Yes.  So installing it does take some time.  A buddy of mine has, I think it's an '05 Infiniti.  And he was complaining that when he rents cars for work they have all these new features, but his 12-year old Infiniti doesn't.  Anyway, Mark took his entire center console apart one weekend after buying one of these, installed it, and now he's got the tunes playing from his - I think he has attached - he used the older iPod connector because that's the iPod that he had.  And so he's got his iPod playing in his 2005 Infiniti.



LEO:  Very nice.  



STEVE:  So anyway, GromAudio.com, for anybody who's interested.  Jonathan Bennett sent me a note saying:  "SpinRite owner and long-time listener here.  My son just introduced me to a puzzle game I thought you might like for iOS and Android, Squaredance."  I saw this as I was catching up in my Twitter feed to prepare the show, so I've not looked at it.  But I've never seen, well, not recently seen anything rated so highly.  It's got almost 100% five stars.  Apparently, it's great.  So I wanted to thank Jonathan.  I can't vouch for it yet, but I will give our listeners an update next week.



And then he finished, saying:  "I have found the Frontier Saga audiobooks available through my library.  Thankfully, each book is short so I can fit in Security Now! between books, and I don't have to fall too far behind.  You were right," he wrote, "the action has been nonstop so far.  I'm not quite done with the third book, but wow.  Thank you for all you and Leo do."  So Jonathan, thanks for the pointer to Squaredance on iOS and Android.  Looks great.  Looks apparently really cool.  So say the reviews that I read.



And, finally, Garrett Bane, who is a longtime listener, wrote:  "I used my personal copy of SpinRite at Level 2" - as he should.  He says:  "...on my work laptop with an SSD" - thus Level 2 because you only want to do a read scan - "to confirm that the drive was, in fact, dying.  The early warning allowed me to pull all my backups together and prevent any loss of data.  Thanks to listening to SN since day one from my iPod, I have a better understanding than most of the Level 1 techs here.  So glad for how far pod," he says, "netcasts have come, and I appreciate all the work you and Leo put into the show.  I'm looking forward to 6.1, 6.2 and 7" - he's talking about future SpinRite releases - "when we will see performance improvements and will be able to run it on a Mac.  Please feel free to share my story/testimonial on Security Now! and use my name.  Thank you.  Garrett, Jackson, Wisconsin."  And Garrett, thanks for sharing it, as well, and helping me remind our listeners about SpinRite.



LEO:  All right.  I think it's time to talk about WannaCrypt.  Almost.



STEVE:  We've got a little closing-the-loop feedback to do.



LEO:  Oh, good, okay.



STEVE:  Someone whose Twitter handle is @vega_ska said - he asked an interesting question, something that I don't think we've ever talked about, regarding load balancing.  He said:  "Google's DNS resolves to 12 IPs, but Google.com resolves one IP.  How does this work?"



This is an interesting bit of incredible forethought by the original designers of DNS.  The idea is that any domain can have, that is, any DNS server can present more than one IP for a given domain name.  So Microsoft.com, I think, is the same way.  Google.com, if you actually use a tool like Nslookup or Dig, you'll see that you get back a block of IPs.  Every DNS server which answers a DNS query that has a block of IPs, whether it's the authoritative original server or a caching server somewhere out on the Internet, that previously asked and the answer has not yet expired, rotates the list of IPs every time it is asked for the IP list for a given domain.  So there's automatic built-in, round-robin load balancing built into DNS queries.



The beauty of that result is that typical users or consumers of the IP list from a DNS query start with the first one.  And they go with it if they get a connection there.  But if no machine is there, then they go to the second one and the third one and so forth.  So just by simply allowing a list of IPs, and by formally setting the practice of rotating that list circularly, you get the best of all worlds.  You get that the first one in the list is always rotating among all the people who ask, so thus the balancing of the load among all the IPs.  And you get the fault tolerance of, well, if the first one in the list doesn't answer, for whatever reason, you go to the next one.  So, neat question, and I just thought it was a - it's something we've never talked about before.  Believe it or not, there's still a few things like that.



Someone who is a frequent Twitter participant, Glasair Pilot, asked:  "Why would engineers spend the money for Android mics that have a frequency response into the ultrasonic?"  And that's something we hadn't touched on when we were talking about the prevalence of ultrasonic monitoring and spying and tracking.  And the answer is that, in this context, ultrasonic doesn't mean like medical ultrasound in the super high kilohertz or even low megahertz range.  It only means non-audible, which is to say anything above 20 Hz.  And the fact is many very small microphones have a natural frequency response well above 20 kHz, maybe 25, maybe even 26, 27, 29.  So it's not that they're more expensive or that there was any intention behind the fact that the microphones that are used in smartphones just happen to extend beyond our hearing range, it's just that mechanically they do.  And so people realized, oh, look.  Well, like the EKG machine that you really like, Leo, the cute little portable EKG.



LEO:  Heart attack, yeah.



STEVE:  Yeah, that uses a frequency-modulated ultrasound.  But again, in this context, I mean, "ultrasound" does have a formal definition.  Well, technically the word means "supersonic," above sonic.  And maybe that would be a better term, instead of saying "ultrasonic," to say "supersonic."  Meaning just above our hearing range.



And then Chris Ebert, sort of on the same lines, asked, he says:  "It seems it should be possible for mobile OSes to filter ultrasonic frequencies from the microphone before providing it in the API."  And I agree.  I think it's just something that they don't do.  Maybe they will.  It's kind of handy.  And the bad news is, if they did, then the cute little frequency-modulated supersonic EKG machine would stop to work, as would - maybe there are legitimate purposes for it.  So, but maybe the user should have the choice of turning on a supersonic filter on their microphone for that purpose.  Or maybe it'll just be added in the future.



Oh, and Michael Cunningham, another frequent Twitterer, brought a Ubiquiti router firmware update to my attention.  We've been previously talking about the non-EdgeRouter updates that Ubiquiti released for all of their airOS machines that had some concerns.  This one does apply to our cute little EdgeRouter X boxes.  Oh, and by the way, Elaine got one last week.  So Elaine is following on - not only does she look up all the words and make sure everything is spelled correct, but she's following along with some of the security advice and now has herself a little Ubiquiti EdgeRouter X.  And she wrote me, said, "It is adorable, just like you said."



Anyway, they fixed that UDP problem we talked about a few weeks back, which is the vulnerability of the - remember the MSG_PEEK flag where software might look at the UDP packet's contents without removing it.  And the code for checking the checksum, the "unsafe second checksum" was what the problem was called, that was where the problem was.  So on April 28 Ubiquiti released v1.9.11 for the wired Ubiquiti EdgeRouter family.  There's the X, the non-X, and several others.  So if you are an owner of those, the ones we've been talking about and really like, you might want to check with Ubiquiti for a firmware update.  You want v1.9.11 or maybe later.  They don't do them very often, so I imagine that's the one that's there now.



Andrew Douglas asked:  "So how do TOTP apps work?"  He says, "I use them, but have no idea how those six magic numbers work.  Any chance you could dig in a little on Security Now!?"  And I'll just say quickly that it's very cool.  Basically it is a cryptographically driven time sequence.  And the security requirement is that the sequence be unpredictable, and that at any given instant there be no way for a bad guy to predict what the six digits will be based on any knowledge of the previous six digits.  So we accomplished that by using what's known as a keyed HMAC.  Well, that's sort of redundant.  A keyed MAC, a Message Authentication Code, which is an HMAC.



An HMAC, a hashed MAC, is a well-known security primitive which uses a hash several times in order to mix a secret key into the hashing process.  So we feed the time of day into this keyed hash, where the key is a secret, and out comes, depending upon the size of the hash, a binary blob from which six digits are extracted.  And that's your token.  And every 30 seconds the time is updated, and then that is rehashed with the same secret in order to get a different six digits.  So it's as simple as that.



And, finally, we actually heard from the Department of Defense, who weighed in on the exploding missile logic.  We'd talked about how it really wasn't necessarily to do garbage collection and worry about memory leaks if you were writing the flight control software for a missile because, as long as the memory would not become exhausted within the expected maximum flight time of the missile, the unreleased memory leak was not going to be a long-term problem.  This person wrote:  "I work with the DoD.  Missile RAM issue:  It's likely far, far cheaper to double the hardware" - meaning the RAM - "rather than pay a contractor to fix the memory leak.  (Contract ending, other important bugs to fix, et cetera).  After all, what's the hardware cost per missile?  Likely very small.  How many missiles?  Likely not that many."  So we've heard from our listeners about various topics from last week.



WannaCry, or Makes You WannaCry.  So what do you get when you combine an extremely old and powerful, always on and present, networked, remote code exploit requiring no user interaction, with moneymaking file encrypting malware, the inherent vulnerability of social engineering phishing attacks, and then toss in a capable backdoor while you're at it?  Well, it makes you WannaCry.  This global, potent, and capable cryptomalware payload, which leverages the NSA's leaked, ubiquitously present, and well-weaponized EternalBlue Windows SMB vulnerability for propagation, is what we got.



Now, there was a lot of disagreement about the name for this thing.  The reason is it never formally declared its own name.  I went with WannaCrypt, although I added a second "N" because it seems a little more grammatically correct.  It's been called WCry, WanaCryptOr, WannaCry, Wana Decryptor, et cetera.  For me, what was definitive was the source code refers to itself in several places as WanaCry.  And, more importantly, the encrypted file header, that is, the new header added to every file that it encrypts, the first eight characters are W-A-N-A-C-R-Y-!.  So WannaCry.  I think that is probably the correct name for this.  And people were a little confused because Wana DeCryptor is the name that's also present.  But that's, of course, the decrypting side, the part where you're no longer crying, except maybe over the bitcoins that you just had to spend, but you're in the process, hopefully, of getting your files back.



Although I watched whoever it was, Junior Cyber Somebody, come out before yesterday's press briefing with Sean Spicer.  And what he said seemed to have about as much mis-fact as it did correct fact.  So I don't know what to believe from what he said.  He said that there had been no evidence of anyone decrypting their files after receiving payment.  He's the only place I ever heard that said.  So I would always look for multiple sources of confirmation.  I assume that people who pay get their files back.  But he threw his statement, which was the official U.S. position statement, threw that into question for me.  So I don't know one way or the other.



We do know that today there have been at least five identified variants.  And I've seen infection estimates fall between 200,000 and 300,000.  Yesterday the same person said 300,000.  And there is the site which was created by the attacker who created the inadvertent kill switch we'll talk about in a second.  And I glanced at it this morning, and it was now above 300,000.  I don't remember how far above, but it was north of 300,000.  So assuming that he's doing IP single counting and not multiply counting IPs, he had set up a web server at this bizarre domain, and the code in the WannaCry infection checks that site for a connection.  If it obtains a connection, just anybody answering TCP, it then no longer proceeds.  So that does allow him, not only to neuter the infection, but simultaneously track a count of how many infections are present.



Earlier I read that it was only looking - it was performing a DNS lookup, and that he simply looked up - that the malware performed a DNS query.  If it got a resolution to DNS, it didn't infect.  However, I subsequently saw the source code, and the source code actually performs a connection with an integral lookup as part of it.  So you do have to have something there answering a TCP connection.  Not even a web server, just a hello, yes, here's a TCP handshake, and then this thing shuts down.



So this is a worm by the classic definition of a worm, meaning something that is self-propagating with no user input, which itself, from a single instance, has the potential of taking over a large number of other systems, meaning that the infection itself is also a scanner which successfully scans for other machines to infect.  So it meets the definition.  When it installs itself, it looks at the local block of IPs where it is and immediately scans the /24 subnet, that is, the 256 IPs in its own network neighborhood, for anything else, that is, any other machines to infect.



So it is a virulent Intranet infector.  And in fact it is believed that's how things like the U.K.'s NHS network, and even FedEx here in the U.S., and also the Telefonica telephone ISP in Spain got themselves massively infected was that somehow a single point of entry was obtained, and then these organizations had huge Windows infrastructures with, it should be noted, un-updated Windows machines, that is, not since March, or maybe un-updatable Windows machines that we'll talk about in a second.



So there's still some question among the security community whether there was also a phishing campaign to get this into organizations.  Spain's Telefonica believed that there was a PDF file containing a .hta, an HTML application exploit, that someone, you know, a phishing email, clicked a link.  And in this case, thanks to EternalBlue, it didn't just compromise that person's machine, but it got in, and then it went crazy within the Intranet.  So this follows what we talked about when we were talking about EternalBlue originally was, because it's a Windows SMB exploit, I rhetorically asked, how can this be a danger on the Internet?  Well, it turns out there are tens of thousands, as we later learned, tens of thousands of apparent SMB 445 ports exposed publicly.  And all it takes is one.



It takes one machine somewhere in an organization that has that exposed, one way or the other, for the other WannaCry worms to find it.  Because not only, when WannaCry sets up, does it start looking on its own Intranet, it also launches 128 public Internet scanning worms to scan all of the public Internet.  It uses the high-quality cryptographic entropy source built into Windows, if available.  Otherwise it falls back to a pseudorandom number generator that's still just picking IPs, 32 bits, at random, and tries to make a port 445 connection.  So that's what happened was that, once this thing started, it found all the other publicly available SMB ports on the Internet.  And with each one that it found, it infected it, and that one became a scanner in pure Code Red, Nimda, MSBlast style, the kinds of attacks on a global scale we haven't been talking about, the world hasn't been subject to, for years.



So why was the U.K.'s NHS hit so hard?  By some estimates, approximately 90 percent of the NHS in U.K. has remained on Windows XP and did pay Microsoft hugely, I saw a number, 5.5 million pounds quoted, I think it was pounds and not euros, to extend support for an additional year past XP's patch death.  And I think I remember us reporting that at the time, that they decided to just buy another year of support because they wanted it.  But...



LEO:  Well, they're regretting that now.



STEVE:  That was in 2014.



LEO:  Because Microsoft didn't offer a patch even if they bought support; did they?  



STEVE:  Correct.  That only gave them one year of extended support after the end of the extended support.  So that brought them up short.



LEO:  You can't - I have to say, though, and I've talked to our IT guy Russell about this.  He's got a few hospitals.  They're really, every time you do an upgrade, they have to recertify the stuff.  And it's not - there are quite a few businesses for whom moving to the newest versions is expensive and not a great idea.



STEVE:  Which is the perfect segue to the next topic.  I have it here in bold:  Is blaming the victim here justified? 



LEO:  Right, right.  It's tempting.



STEVE:  I said it's easy to do, but I think it's more subtle than that in the real world.  In today's environment, everyone should accept all available updates.  That's clear.  But notice the way I deliberately phrased that:  All available updates should be accepted.  What about when updates are no longer being made available?  What about when XP, embedded in a decade-old MRI scanner, which is working fine otherwise, or a passenger ticketing kiosk, or a banking ATM machine?  In many cases their manufacturer would love to profitably sell an updated system.



But what if the one you have is already working just fine?  Or if there isn't budget available for wholesale replacing 90% of a massive IT infrastructure?  Or what if the company who created the devices 10 years ago is long since gone?  And as you say, Leo, the need for compliance to have certification of any changes.  They're in the real world.  There are real reasons for a sane IT policy of, okay, we're going to attempt to retain security oversight and control to the best of our ability.  And in this case that failed.



LEO:  It's why hospitals get bit disproportionately by these attacks.



STEVE:  Yes.  I mentioned this thing having a weird kill switch.  A security researcher, looking through the code, found this bizarre domain.



LEO:  No, you're not going to read it.



STEVE:  No, I'm not.  I'll just give people a sense for it.



LEO:  I can zoom in on it.



STEVE:  Iuqerfsodp9ifjapo and so forth.  That's about a third of it.



LEO:  Looks like a Gaelic town is what it looks like.



STEVE:  That's just not, you know, clearly a big long piece of gibberish dotcom.  The code uses an older Windows TCP connection function to attempt to make a TCP connection.  If it succeeds, for whatever reason, it shuts down.  So what this means is, if there is a server there, do not infect.  So it was clearly a kill switch.  Or maybe he put it in, and he was going to write some other code, but then forgot to?  And so the default of making a connection just said, oh, instead of doing what I was going to do, do nothing.  Who knows what's behind it.



But what was discovered was the hacker registered the domain, set up a server to monitor the infection, and discovered that not only did it monitor it, it shut it down, so it stopped infecting.  The problem is that it was, well, first of all, all the variants that have subsequently appeared don't have that.  And there's even a variant where that domain and that feature has been hex-edited out of the binary.



LEO:  Okay.  Now, I have a theory about all this.



STEVE:  Okay.



LEO:  And do you want to hear - now, it starts with this morning's report from NPR that North Korea might be involved in this.  And the reason some security researchers think so is there's a lot of commonality in the codebase with the Lazarus attackers, the Lazarus Group who did, of course, Sony.  Some of the code is very similar.  So, now, of course code can be copied.  But when I first saw this, and I was wondering what you thought, the fact that it was translated into, like, 15 different languages...



STEVE:  It was very language-complete, yes.



LEO:  And it was also very helpful.  It had links to what bitcoin is and where to buy bitcoin.  The fact that they raised so little money, but put so much effort into it, and then this kill switch, kind of made me think, governmental operation?  And so the theory - and of course it's hard to confirm because you can't.  But the theory is perhaps this was not an attempt to raise money, but to cause global disruption or some other things.  By the way, it didn't hit the U.S. hardly at all.  It mostly hit Russia and Asia before it got shut down.



STEVE:  I would argue, though, that is a consequence of the install base. 



LEO:  Yes, possibly, yes.



STEVE:  You can run pirated Windows XP very easily.



LEO:  And something like this doesn't get geographically contained very easily.  I understand that, as well.  



STEVE:  Right, right.



LEO:  There's no conclusive evidence at all.  But it isn't inconsistent with a hacker that might have motives other than mere financial gain.



STEVE:  To that I just shrug.



LEO:  What are you going to say; right?



STEVE:  Yeah.  I just don't have an opinion on that.  But, yeah, it's certainly possible.  I did find a couple interesting little bit of technical details.  My favorite one was to look a little more closely at what the actual EternalBlue exploit was.  And it turns out - get a load of this, because we've talked about these sorts of problems.  There is, in this SMB code that's been around forever in Windows, is a very subtle buffer overflow in a memory move operation.  The size of the move is calculated.  But the calculation has a type error in the math.  A dword, that is, a 32-bit double word, is subtracted into a word.  So again, a very subtle error.  It's not clear to me whether someone would have looked at the source code and spotted that.



The fact that it's from the FuzzBunch makes me think that it was some sophisticated fuzzing, meaning that a whole bunch of crap was being thrown at the ports of a Windows machine, logging what was being thrown, in case the junk that was being throw at it caused it to crash.  Because, if so, then they would go in and forensically determine when exactly the crash occurred, what had been thrown in it that caused it, and then reverse engineer the exploit from there.  Which is the way a lot of these things have been found.  So that's my guess.  But I thought, oh, yep, here once again a dword was subtracted, and the result was a word.  So the point being that the subtraction tried to be stored into a smaller container, a 16-bit value, from a 32-bit subtraction, which then wouldn't fit.  And you could get up to all kinds of mischief that way.



And, finally, Brad Smith, Microsoft's President and Chief Legal Officer, weighs in.  He has sort of a rambling post about social responsibility and so forth, in which he takes no responsibility for this.  The title of his posting was "The need for urgent collective action to keep people safe online:  Lessons from last week's cyberattack."



And I snipped all of the preamble and conclude with him saying:  "Finally, this attack provides yet another example of why the stockpiling of vulnerabilities by governments is such a problem.  This is an emerging pattern in 2017," he writes.  "We've seen vulnerabilities stored by the CIA show up on WikiLeaks, and now this vulnerability stolen from the NSA" - notice he didn't say, I mean, there's no punches pulled here, it's like no "maybe" or "believed to be" or anything.



He just says:  "Now this vulnerability stolen from the NSA has affected customers around the world.  Repeatedly, exploits in the hands of governments have leaked into the public domain and caused widespread damage.  An equivalent scenario with conventional weapons would be the U.S. military having some of its Tomahawk missiles stolen."  Okay.  "And this most recent attack represents a completely unintended, but disconcerting link between the two most serious forms of cybersecurity threats in the world today:  nation-state action and organized criminal action."



Finally:  "The governments," he writes, "of the world should treat this attack as a wake-up call.  They need to take a different approach and adhere in cyberspace to the same rules applied to weapons in the physical world."  The problem with that is attribution, of course.  He says:  "We need governments to consider the damage to civilians that comes from hoarding these vulnerabilities and the use of these exploits.  This is one reason we called in February for a new 'Digital Geneva Convention' to govern these issues, including a new requirement for governments to report vulnerabilities to vendors" - yes, Microsoft would like that - "rather than stockpile, sell, or exploit them."  Or lose them.  "And it's why we've pledged our support for defending every customer everywhere in the face of cyberattacks, regardless of their nationality."  And regardless of the OS's age.  "This weekend, whether it's in London, New York, Moscow, Delhi, Sao Paulo, or Beijing, we're putting this principle into action and working with customers around the world."



LEO:  Well, he's right, of course.  It would be great if they didn't do it.



STEVE:  Yes.



LEO:  But they never won't do it because - and if governments don't, well, then, bad guys will. 



STEVE:  Yup.



LEO:  So that's foolish optimism.



STEVE:  It is a conundrum.  I mean, I get the NSA's plight.  They're wanting to use things they find for their own purposes.  And if they tell Microsoft, Microsoft will fix them.  And, unfortunately, the worse they are, worse in this context, the more valuable they are because the more powerful they are, and the more they can leverage them as they need.  So, boy.  As you said, Leo, it is the world we live in today.



LEO:  I think they want a variety as opposed to power because they're, in theory, not doing mass collection.



STEVE:  Right.



LEO:  They're targeting individuals.  So a variety of exploits that work in a variety of situations would be more useful.  But that's why they stockpile them.



STEVE:  Well, and remember, EternalBlue was only one thing.



LEO:  We're going to see so much more of this.



STEVE:  Yes.  The thing that made this worm a worm, that gave it its teeth, was the EternalBlue component.  We've had cryptomalware for a long time, and it's a problem, and it's annoying.  But it's not this.  It was the...



LEO:  Well, did it make this that much worse?



STEVE:  Yes, yes.



LEO:  Okay.



STEVE:  That was the worm.  It was an SMB worm, and that's what brought down NHS in the U.K. is that it was the wormness of this, and the fact that it encrypted all the system's files.  So this is to your point, Leo.  For the NSA to have EternalBlue doesn't mean the NSA has a worm.  What they had was a powerful SMB exploit that could be used as a worm, as it was, but they could also use it for surgical intrusion, which is probably what they were using it for. 



LEO:  Much more likely, yeah.



STEVE:  Yes.  And just one final note.  Simon Zerafa, also a frequent contributor through Twitter, said:  "No WannaCry Ransomware on Windows XP.  So why did MS release the XP patches?"  And this follows from somebody, a security researcher who noted that there were aspects of the code that they believed wouldn't function in XP.  Well, clearly the NHS got obliterated as a consequence of XP.



LEO:  Apparently it works okay.  It worked well enough.



STEVE:  But I would also argue that, with this SMB exploit having been developed into a worm that was this bad, Microsoft was forced to move.  And this of course is very reminiscent to the MSBlast worm because Microsoft was adamant about ignoring my years of begging them and warning them about raw sockets in XP until the MSBlast worm used raw sockets in Windows to blast the crap out of Microsoft and scared them and forced them to remove raw sockets from XP, or to dramatically neuter them to the point where it could no longer be used for this purpose.  So Microsoft doesn't listen to anybody unless they absolutely finally have no deniability and no other choice.  And so, yes, they fixed this in XP also.  Yay.  I have an update.



LEO:  I mean, I think they're doing now - one of the things you're critical of them with the browser thing is they're trying to create with 10 S a more secure operating system.



STEVE:  I'm not critical of them.



LEO:  Towards this goal.  But, well, you see the downside is...



STEVE:  They're creating a choice.



LEO:  Yeah, and they'll be kind of a homogenous system as a result.



STEVE:  Oh, I don't want to use Bing.  I just...



LEO:  Yeah, I know.



STEVE:  I have an anti-Bing bias.



LEO:  Yeah.  It's an interesting puzzle because, you know, I understand why Microsoft says, well, we can't turn off SMB1 on all our systems ever made because people rely on it.  And that would hurt those hospitals just as much as this did.



STEVE:  Yup.



LEO:  That's a very difficult thing.  The only thing we know for sure is it's only going to get worse, a lot worse, in my opinion.



STEVE:  Well, and rather than turning it off, they might as well patch it.  I mean, you know, now it's fixed.



LEO:  Yeah.  But they didn't know the flaw was there until the Shadow Brokers dumped the exploit from the Equation Group.



STEVE:  And then they immediately responded.  I mean, and we think that made...



LEO:  Yes.  They got it patched two months ago.



STEVE:  Yes.



LEO:  Just not everybody applied it.  I don't know.  And then another thing they're doing, they're requiring people with Windows 10 to apply patches.  You can only defer for so long.



STEVE:  And they didn't back patch, either.  And now they have.  But notice that they didn't, even though it was really, really, really bad, they didn't do it until they had to.



LEO:  You'd better believe they're going through those other Shadow Broker exploits, and they'll be back patching all of them.  What did they do yesterday?  There was a bunch of stuff.



STEVE:  Yeah.



LEO:  Steve Gibson.  He's the guy.  Thank goodness we have him.  He's the man in charge at GRC.com, Gibson Research Corporation.  That's where SpinRite lives, the world's best hard drive maintenance and recovery utility.  You can get your copy there.  You can also get your copy of this show there, and not only 64Kb audio, but nice transcripts that you can follow along as you listen, or search and find the part that you're most interested in.  A lot of people find it easier to understand Steve if they're reading at the same time and their brain gets it in two different ways.  I would read, listen, and watch.  Do it all.



We do like you to check out the video.  That's available at TWiT.tv/sn, as is the audio.  You can also subscribe in your favorite podcast utility.  Actually, that's the best way, and then you won't miss an episode.  And I think this is like the 10-foot shelf of technology.  You want to get every episode in your collection so you can always go back and learn because a lot of this stuff is still the same.  There's a lot of great stuff in these 600-some episodes.  Steve, we'll be back here next Tuesday, 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  What do you think?



STEVE:  I'm ready for it, my friend.



LEO:  I'll see you then on Security Now!.



STEVE:  Thanks, Leo.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#613

DATE:		May 23, 2017

TITLE:		WannaCry Aftermath

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-613.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  This week we examine a bunch of WannaCry follow-ups, including some new background, reports of abilities to decrypt drives, attacks on the kill switch, and more.  We also look at what the large Stack Overflow site had to do to do HTTPS, the WiFi security of various properties owned by the U.S. President, more worrisome news coming from the U.K.'s Theresa May, the still sorry state of certificate revocation, are SSDs also subject to Rowhammer-like attacks, some miscellany, and closing the loop with our listeners.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We have a, I would say, a potpourri show, everything from a silly riddle to the original diagram of Ethernet from 1973.  And a surprisingly young picture of Steve Gibson from 1984.  That and a lot more, including an analysis of the WannaCry aftermath, all coming up next on Security Now!.



LEO LAPORTE:  It's time for Security Now! with Steve Gibson, Episode 613, recorded Tuesday, May 23rd, 2017:  WannaCry Aftermath.



It's time for Security Now!, the show where we cover your security and privacy online, rapidly becoming the most popular show on our network.  And I have to credit this guy right here, our security guru, Steve Gibson of the GRC Corporation.  We were at - hi, Steve.



STEVE GIBSON:  Hey, Leo, great to be with you.



LEO:  We were at the Maker Faire.



STEVE:  Oh, didn't that look like fun?



LEO:  It was fun.



STEVE:  I was just drooling, looking at all of the stuff there.



LEO:  Well, a lot of people came by to say hi.  And to a person they said, "You want to know what my favorite one is?"  And I said, "Yeah, I guess."  Security Now!, they all love Security Now!.  Maybe it's because it's the geekiest show we do.  Makers like Security Now!, I guess.  Anyway, take it for what it's worth, a data point.



STEVE:  Well, thank you.  I appreciate the feedback.



LEO:  Well, I love the show, that's for sure.



STEVE:  That's great.



LEO:  And today we're going to WannaCry some more.



STEVE:  Oh, well.  Naturally there's been lots of, you know, with anything as big as WannaCry, it doesn't just end like last Tuesday when we did the show and sort of pronounced it done.  There's more.  So as I was pulling everything together, I realized about half of what we had to talk about was still WannaCry.  So the first half of the podcast we'll discuss the aftermath and various things.  Like there have been claims that there's a way to decrypt it.  There's questions about does it actually infect XP.  And even some interesting reporting, I think it was from the Washington Post, we'll get there in a second, about the NSA's position on this, which - and I've got some interesting feelings about that, as well.  So we'll examine a bunch of WannaCry follow-ups, including some new background, reports of the ability to decrypt drives, attacks on the kill switch and more.



We'll also look at what the large Stack Overflow site had to do in order to implement HTTPS.  It's a beautiful sort of case in point of a big, sprawling, complex site and how it turns out not to be as simple as just using Let's Encrypt.  Then there's an interesting question about the WiFi security of various properties owned by the current U.S. President.  And you can imagine it's not good.  In fact, it's very worrisome.



And speaking of worrisome, there's news coming from the U.K.'s Theresa May about the continuing plans - the headlines are odd.  They talk about the U.K. wanting to create a new Internet.  And it's like, what?  And then - which is like a strange headline.  But it turns out what they want to do is to basically really put the screws to the Internet that we have.  And so what the people doing the headlining were trying to imply was it's so onerous that it's a different Internet.  It wouldn't be like the one we have now.



LEO:  Oh, I see.  



STEVE:  Yeah.  I also want to talk about the still sorry state of certificate revocation, which of course we dipped into deeply a few years ago when I realized what Chrome was not doing, and I created the revoked.grc.com domain specifically to demonstrate and allow users to test the revocation awareness of their browsers.  The revoked.grc.com domain is a deliberately revoked certificate.  And the question is, can your browser go there or not?  And Chrome just happily does, even today.  But there have been additional movements that I want to sort of catch everybody up on.  Then the question of whether SSDs are also subject to Rowhammer-like attacks.  And some research suggests that there are two different types of attacks that can be employed against SSDs to damage them deliberately.



And then of course we've got some miscellany and some closing the loop with our listeners.  So I think another jam-packed, information-laden podcast.



LEO:  Of course.  I expect nothing less for you, my friend.  I've got your Picture of the Week ready to go, Steve.



STEVE:  Ah.  So, yes.  Yesterday Bob Metcalfe, the inventor of Ethernet, tweeted that it was the 44th anniversary of his invention of Ethernet.  And so our Picture of the Week, he also included in his tweet, was a hand-drawn, because this was 1973, this was the year I graduated from high school, he was inventoring...



LEO:  He was inventoring.



STEVE:  Inventoring, yes.  He was at the Xerox PARC, the Palo Alto Research Center, PARC.  And so this was his hand-drawn note.  And I was going to say that back in '73 we didn't have, like, the kind of computers...



LEO:  You didn't have Vizio or Trello or anything.  You've got to...



STEVE:  Exactly.



LEO:  This is a napkin, my friend.



STEVE:  Exactly.  And so he had a typewritten description of it, but he needed a graphic to go with it.  And I just love this picture because in the upper left he shows like a bunch of Altos, which was the GUI workstation which Steve Jobs famously saw and said, okay, that's the way you do these things.  And so he has sort of this kind of stick figure-looking network with Altos hung onto it in different places.  There's a Dynabook.  There's a Nova, which was a popular minicomputer of the day.  There's a PDP-11.  And then he has it labeled with an arrow pointing to it, a "cable-tree ether," meaning a cable-tree conductive medium.  And then in the second diagram to the lower right he shows, again, a cable Ether, and then what he calls a "telephone coax booster" with a pair of copper lines.  And he points to it, saying that that's a "telephone ether."  And then he's got a wacky little thing that this telephone ether connects to that says "telephone radio booster" and a little, looks like a cactus, but it's meant to be a radio antenna sticking up.  And that's his radio ether.



So, I mean, he foresaw WiFi back in 1973.  I mean, this is exactly what we have today.  And I will come back to this in our Miscellany because I want to talk about the conceptual leap that he made, that he had to make, and why it was so different than the solutions of the time, and how very much like the original concept of the Internet, which we've discussed before, how counterintuitive it was that you could autonomously route packets with no guarantee for their delivery.  That is, the system itself couldn't guarantee delivery, but it just made a best effort, and how that was enough to revolutionize the world's communications.  And so this is sort of a - this is a small, down at the connectivity part, but it's a similar innovation.  So anyway, just a neat photo this week.  I'm so glad that Bob gave me the opportunity to talk about it.  And he and I have met on a number of occasions.  He's a great guy and a real - an engineer's engineer.



So the first thing I wanted to note was, and it was the Washington Post had some interesting coverage and some background about the NSA's reaction to and feelings about the loss of their control of this very potent hacking tool, which was turned into, as we know, this WannaCry worm.  The Washington Post writes:  "When the National Security Agency began using a new hacking tool called EternalBlue, those entrusted with deploying it marveled at both its uncommon power and the widespread havoc it could wreak if it ever got loose.  Some officials even discussed whether the flaw was so dangerous they should reveal it to Microsoft, the company whose software the government was exploiting, according to former NSA employees who spoke on the condition of anonymity given the sensitivity of the issue.



"But for more than five years, the NSA kept using it  through a time period that has seen several serious security breaches  and now the officials' worst fears have been realized.  The malicious code at the heart of the WannaCry virus" - which we actually know is a worm - "that hit computer systems globally late last week was apparently stolen from the NSA, repackaged by cybercriminals, and unleashed on the world for a cyberattack that now ranks as among the most disruptive in history.



"The failure to keep EternalBlue out of the hands of criminals and other adversaries casts the NSA's decisions in a harsh new light, prompting critics to question anew whether the agency can be trusted to develop and protect such potent hacking tools.  Current and former officials defended the agency's handling of EternalBlue, saying that the NSA must use such volatile tools to fulfill its mission of gathering foreign intelligence.  In the case of EternalBlue, the intelligence haul was 'unreal,' said one former employee.  'It was like fishing with dynamite,' said a second.  The NSA did not respond officially to several requests for comment for this article."



And then the story goes on to reiterate a bunch of background that we've already discussed.  And as I said last week, I understand that this is an extremely tough call.  My feeling is it's easy for those on the sidelines to jump up and down and say that the NSA should not secretly develop and then keep secret such powerful vulnerabilities.  But I think it's very important to note that what is unfortunately missing from all of the reporting of this story are any details as a function, I mean, as a consequence of the nature of the fact that they have to be kept secret, of what the use of this longstanding SMB Windows vulnerability may have been to U.S. national security during the time that it was both available for the NSA's use and secret.  We don't know how useful it was.  We quoted, or the Washington Post quoted that comment saying that "the intelligence haul was unreal."  But we have no details.  We don't know what valuable intelligence it may have uniquely allowed to be gathered.  And of course, had that been patched, that asset, that powerful intelligence-gathering asset would have been killed.



So horrific as its escape doubtless was, maybe if we knew how our intelligence services had been able to use its unique powers during the time of its availability, we might feel differently. But we're unlikely to know one way or the other.  My only point here is to observe that we do not have the benefit of the full story.  We don't know that it might change our judgment if we did know.  So I thought - I appreciated the Washington Post reporting on that, and a little hint that it really was such a powerful tool.



And on the heels of this comes the proposed PATCH Act, a new bill designed to prevent occurrences like WannaCrypt.  And I'll explain what it is and give some background and why I think it's a total crock.  Okay.  So SecurityWeek reported on this, and it was picked up from additional coverage, but basically a repetition of what is known, which is there is a bill now.  So SecurityWeek writes:  "Following the worldwide WannaCrypt ransomware attack that leveraged the EternalBlue exploit developed by and stolen from the NSA, Microsoft's chief legal officer" - as we discussed last week and quoted Bill saying - "called for governments to stop stockpiling zero-day exploits.  His arguments are morally appealing, but politically difficult.



"Now, however," writes SecurityWeek, "he has partial support from a bipartisan group of lawmakers."  And there's a bunch of them:  Senators Brian Schatz, who's a Democrat from Hawaii; Ron Johnson, a Republican in Wisconsin; Cory Gardner, Republican from Colorado; and U.S. Representatives - those were senators - U.S. Representatives Ted Lieu, Democrat in California; Blake Farenthold, a Republican in Texas.  Schatz announced yesterday that they had introduced the - and this is where PATCH is an acronym, actually - Protecting our Ability To Counter Hacking Act...



LEO:  Oh, please.



STEVE:  I know.



LEO:  They must spend more time on that than the actual bill.  I swear to god.



STEVE:  I know.  Well, and just wait because, I mean, it's so useless - of 2017.  "Its purpose is to establish a Vulnerability Equities Review Board..."



LEO:  That already exists.  Obama set that up.  Didn't work.



STEVE:  Exactly, "...with permanent members including the Secretary of Homeland Security, the Director of the FBI" - whoever he is - "the Director of National Intelligence, the Director of the CIA, the Director of the NSA, and the Secretary of Commerce, or in each case a designee thereof.  Its effect will be to seek a compromise between the moral requirement for the government to disclose vulnerabilities," and then in parens SecurityWeek writes "(Microsoft's Digital Geneva Convention), and the government's political expediency in stockpiling vulnerabilities for national security and deterrence purposes.



"In a statement issued yesterday, Schatz wrote, 'Striking the balance between U.S. national security and general cybersecurity is crucial, but it's not easy.  This bill strikes that balance.  Codifying a framework for the relevant agencies to review and disclose vulnerabilities will improve cybersecurity and transparency to the benefit of the public, while also ensuring that the federal government has the tools it needs to protect national security.'"  Okay, except that, if what they're suggesting is that EternalBlue should have been made public five years ago, obviously there would be huge NSA pushback against that.



So anyway, continuing:  "The bill does not go so far as to mandate the disclosure of all government zero-day exploits to relevant vendors for patching, but instead requires the Vulnerability Equities Review Board to develop a consistent and transparent process for decision-making.  It will create new oversight mechanisms to improve transparency and accountability, while enhancing public trust in the process.  It further requires that 'The head of each federal agency shall, upon obtaining information about a vulnerability that is not publicly known, subject such information to the process established.'  In this way the Vulnerability Equities Review Board" - it's hard to even say that.



LEO:  That exists, by the way.  So this must be some sort of update to this.



STEVE:  Right, "...has oversight of all zero-day vulnerabilities held by the government agencies.  It also maintains the controls relating to whether, when, how, to whom, and to what degree information about a vulnerability that is not publicly known should be shared or released by the federal government to a non-federal entity, that is, whether the public interest requires the vendor be able to patch the vulnerability."



LEO:  It's a balancing act, obviously, because if you're going to have spy agencies, they're going to collect these.



STEVE:  Well, and my take is this is total nonsense, a bunch of politicians who want to appear to be responding by creating more government bureaucracy - which is exactly, by the way, what our present administration was elected to reduce.



LEO:  Right.



STEVE:  This will simply add regulation without effect.  The CIA and NSA will loudly and probably honestly assert their need for these for national security.  They will downplay the downside and explain how we're already losing the cyberwar, and how forcing voluntary disarmament would be unilaterally laying down our arms and capitulating in the cyberwar.  They'll argue that foreign governments who lack the PATCH Act's attempted oversight controls will still be free to discover and use the very same software flaws against us; and that, as we have seen, even when patches were already made available, machines were still victimized.  That is, you know, Microsoft patched this in March, and this happened, what, two weeks ago.



And they'll also note correctly that Microsoft only back-patched XP and Vista because of and as a result of the proven severity of the problem.  Which would argue that, if they had informed Microsoft, oh, by the way, here's an SMB flaw, Microsoft certainly would have fixed the OSes that are widely in use and recent, but would never have bothered to fix XP and Vista.  They did that only because it was such a problem.  So that argues that even informing Microsoft of bad problems doesn't guarantee that all of the machines online will get fixed.  So I would argue that this is just bureaucracy with no effect.



LEO:  I think there has to be some discussion.  I mean, unless you want to abolish antiterrorist efforts, I don't think it's unreasonable for the government, for the NSA and the CIA, to stockpile exploits; right?  I mean, we're not saying that.  And then it's also reasonable, and I think the NSA knows this full well, it was the Director of National Intelligence who set up the original vulnerability equities process in 2008.  They understand the risk also of holding on to these.  And so I think it's not inappropriate for some group of people who represent both the public interest and the interests of the intelligence community to somehow hash this out.  I mean, otherwise you just keep everything.



STEVE:  Well, if it comes down to money, and as we know, so much often does, then if nothing else maybe this - okay.  And if putting more money into protecting the secrets would have prevented that leak, then maybe that's the solution, that is, the fact that the NSA lost something that generated headlines, that has been such a huge problem.  If they needed more money in order to - a budget in order to further and better secure their secrets, that's what they should ask for and get.



LEO:  Yeah, that's a good solution, yeah.



STEVE:  Right.



LEO:  But, I mean, it's just as hard to make perfect software as it is to make a perfect spy.  And I think this is real world.  There's no obvious right answer.  But I think it's appropriate to try to find an answer.  I'm not sure how this differs from the existing vulnerability equities process.  But this is a...



STEVE:  Well, and a bunch of guys sitting around saying, oh, we found a really juicy one, we need to keep it.  How is anyone going to say, no, you have to tell?



LEO:  There's some evidence that Heartbleed, the NSA knew about Heartbleed for several years before it was discovered.  It never leaked out.  It was discovered.  Do you want, I mean, these are tools the NSA uses, not for - one of the things is this is generally, as far as I know, not used, ever used for mass surveillance.  It's used for targeted surveillance. 



STEVE:  Because they don't want it to get out.



LEO:  Right.



STEVE:  Yeah.  So they're going to find someone somewhere and use it in order to perform a network penetration.  And, yeah, I just - I think it's going the way it should.  But I guess my point is the idea of this being a proper subject of a committee is ridiculous.  To me, this doesn't make sense for a bunch of bureaucrat heads of departments to sit around and say, gee, I mean, the worse it is, the more powerful it is, and so the more useful it is.  The mechanics just don't make sense.  So, I mean, certainly if the NSA were to find something that they thought they should disclose, they would.  They would say, hey, Brad - I meant to say Brad.  I said Bill earlier.  Brad Smith, here's a goodie for you.  You probably just need to patch this soon.  And they would.  But that's - it seems to me it's the discoverer needs to react responsibly, and no committee should reasonably expect to be able to impose their own judgment on...



LEO:  Well, I'm not sure that's true because there are, I mean, this is the existing VEP.  And they have to answer questions like how much is the vulnerable system used in the core Internet infrastructure, in other infrastructure systems in the U.S. economy?  Does the vulnerability, if left unpatched, impose significant risk?  How much harm could an adversary or criminal group do with knowledge?  These are appropriate questions.  And I think who else is going to answer these questions?  You've got to sit down with people who both want to protect, you know, understand the issues and discuss it.  How badly do we need the intelligence?  Are there other ways we could get it?  These are all questions that were supposedly going to be asked in the original process.



STEVE:  Those questions are great and completely useless.



LEO:  You're assuming bad will on the part of the spies, who are going to say, no, no, you can't have anything.  But I think...



STEVE:  No, I'm not.  I'm assuming self-interest.  I'm assuming that they're representing, I mean, they understand how difficult it is to find a problem like this and how valuable it is to their needs.  And I just don't think them explaining it to a panel of other agency heads is going to have any useful result.  They're going to say, "Yeah, this is really bad, which makes it really important, really valuable."  So it's like, I don't know, just the concept of discussing it just to me seems...



LEO:  Well, there you're giving up because they're just going to keep it.  If there's no process at all, then there's no challenge to them just keeping it.  Then you're just giving up and saying, well, they're going to have it.  Nothing you can do about it.



STEVE:  Yeah.  And, see, I think the challenge is fake.  I think it's a fake challenge.  I think it's a made-up bureaucratic circle jerk.



LEO:  Just to show that they care, even if they don't.



STEVE:  Yeah.  Yeah.  So there's a great cartoon on the next page of the show notes, Leo, that is the lead-in to this WannaCry kill switch.  We discussed that it existed, and it didn't make sense to us.  We now believe that we understand what it was for.  And it was of dubious value.  It turns out that when malware is being forensically reverse-engineered, it's put in a forensics sandbox.  And malware often makes DNS queries out to its command-and-control servers.  And so this forensic sandbox responds to the DNS queries with its own local server IP in order for the malware to attempt to connect to a command-and-control server.



So a way for malware to detect whether it is sandboxed and being forensically examined is to just ask for a gibberish DNS domain that doesn't exist.  And if it cannot get a connection to a server at that DNS domain, it'll just assume it's not in a sandbox and do its dirty work.  But if it does get a connection to a domain it knows it just made up, it was all gibberish characters - remember I insisted on reading most of that domain name last week - then it goes, oh, no, I must be in a sandbox, and so it alters its behavior.



LEO:  Smarter way to do this would not be to have a hardcoded domain but just a random string each time; right?



STEVE:  Exactly.  Exactly.  There's no way...



LEO:  Which makes me wonder if this is really the rationale behind it.  But maybe they really wanted a kill switch, like a real...



STEVE:  Yeah.  This cartoon is wonderful because it shows some bad guys in masks with a skull-and-crossbones behind them, saying, "Look, I included a few lines to check whether we've been sandboxed, so white hats can't detect us."  And so the other guy says, "I'll just ping a nonexistent domain."  And then, "If the nonexistent domain responds, it means we're in a sandbox, so we won't encrypt any files.  That way no one will notice us."  And then the first guy says, "Ha ha, well done.  We're going to get so many bitcoins."  Then, "a few hours later," our now famous - unfortunately famous, he's not happy, by the way, how famous he is.  We'll get to that in a second.  He says:  "Huh, what's this inactive domain hardcoded in their code?  I should reserve it.  You never know."  And then in the final frame it says "Sh*t" when these guys realize, ooh, we've been foiled.  And of course...



LEO:  Curses.  Foiled again.



STEVE:  This thing completely neutered as a consequence of registering - you're right, Leo.  If they'd just used a random number, just used the little CryptoAPI that they were already using to give them a chunk of entropy, convert that to seven-bit ASCII, stick a dotcom on the end of it and send it out, then yes, then something - then this very simple trigger wouldn't have happened.  But that's the best theory for what they were trying to do.



LEO:  Unless they really wanted a kill switch.  There might have been other reasons they wanted a kill switch.



STEVE:  Yeah, yeah.



LEO:  Who knows?



STEVE:  So, meanwhile, what that creates is a server answering TCP connections at a known domain, and thus a known IP.  So what's the next logical thing to happen?  Hackers now trying to reignite WannaCry with nonstop botnet attacks.  In other words, the kill switch server is now being DDoSed continually by various Mirai and Mirai-derivative botnets in order to force it offline so that, when the WannaCry worm reaches out to see if it should encrypt or not, if it is unable to get a connection because the server, the kill switch server is being DDoSed, it'll go, oh, no one there, so off we go to encrypt the files on the drive. 



There are a few problems with this.  First of all, only newly infected or rebooted systems propagate.  So a new infection or a rebooted system, when the worm comes back alive from a reboot, they spend only the first 24 hours scanning for other vulnerable machines.  Then they stop.  So the point is it's not as if putting the kill switch server under a persistent DDoS would immediately bring the WannaCry, the existing infection base of WannaCry back to life.  Instead it would only - they'd have to keep it offline, and then only new infections or rebooted systems would then reach out to see whether the kill switch server is available and attack.  So it's like, yeah, okay, fine.



And, by the way, the company is working, the company behind the guy who found this, an L.A.-based security firm, is now gone to an unnamed DDoS protection service.  And I didn't bother to check to see.  It's easy, I mean, it's unnamed, but it's easy to figure out who it is because their servers are going to answer the IP for that domain.  And so they're doing everything they can to help keep the TCP server at that domain responsive in order to keep the original variant of WannaCry from propagating.  As we know, immediately that code with hex edited out and another version launched.  And there have been, I think now the last count I heard was five variants of this.  So, and then also I didn't put it in the notes, but there's even another much different variant which uses six different exploits from the NSA, rather than just two.  Rather than just EternalBlue and DoublePulsar, this uses a whole bunch of them.  So the feeding frenzy has begun.



And so the question is, who is MalwareTech?  Who is this person?  Unfortunately, he's in the U.K., and the U.K. has rather notorious reporting.  He apparently tried in vain to remain cyber and not identified in the physical world.  Nowhere on either his Twitter page nor in his blog are there any names, details, headshots, anything that would connect him to the real world.  Which should make it clear that he wished to remain anonymous.  But he was dogged by British tabloid reporters who dug deep into his entire online past and finally outed or "doxed" him as being a 22-year-old British security researcher named Marcus Hutchins.



On the other hand, thanks to being known, the ethical hacker group, HackerOne, has awarded him $10,000 for his efforts, and Marcus stated that he intends to split that award between charity and educational resources for students who cannot afford them.  In the show notes here I have a snapshot of several of his posts.  He tweets as @MalwareTechBlog.  And so he tweeted:  "I knew five minutes of fame would be horrible.  But honestly, I misjudged just how horrible.  British tabloids are super invasive."  He tweeted:  "Journalist doxed a friend, then rang them, offering money for my girlfriend's name and phone number.  One turned up at another friend's house."  He tweeted:  "Tabloids here don't care about the story, they care about every detail of the person behind it and will go to extreme lengths to find out."  And then, finally, he said:  "One of the largest U.K. newspapers published a picture of my house, full address, and directions to get there.  Now I have to move."  So that's the world we live in today, unfortunately.



Okay.  On the question of decrypting WannaCry, there's been a lot of confusion and misinformation, a real sort of a development frenzy to see whether there was a way to decrypt.  And now there's some sanity that has come to this, and things have settled down, and we sort of know what's going on.  There's a function in the Windows cryptographic API called CryptReleaseContext.  So the idea is that, when you're going to start doing a bunch of cryptographic work, you acquire a cryptographic context from Windows.  And so Windows gives you a handle, and then you apply different actions, different methods, different functions against that handle.



So the handle is an abstraction of a cryptographic context, the idea being that Windows is supposed to handle all of the messy business for you.  So you say things like "Create asymmetric key," and it just does.  And then you say, "Use that key to do this or do that and so forth."  And so Windows provides this interface and deals with it all.  Well, when you're done, you want to explicitly destroy any sensitive information that may have transiently existed.  So you release the context.  You say to Windows, here's that handle, that context handle you gave me.  I'm releasing it back to the system.  You hope that Windows proactively wipes and zeroes all of the memory that was involved in any sensitive operations.



It turns out, believe it or not, that - I'm not sure about Windows 8.  But at least we know Windows 10 does, but Windows 7 does not.  So XP, Vista, and 7, the API up through 7 at least does not proactively wipe the memory.  As a consequence, sensitive memory contents containing both the symmetric key and even the two primes which were originally obtained in order to compose the private key, the RSA private key, have been found after the fact in memory from XP through Windows 7.  So if an XP, Vista, or Win7 machine is encrypted with WannaCry, and if it's not powered down, it's very likely that the key can be found.



And there is now - it looks like the best software, I've got links to the various GitHub projects, there's one called WanaKiwi, W-A-N-A-K-I-W-I.  That looks like it's the best and easiest to use.  You can simply run it, wanakiwi.exe.  You just run it.  It will find the process, scan the process memory, find the private key, find the primes, reconstruct the RSA key from them, and then use that in order to return all of your files, to decrypt all of your files.  Obviously it's with lots of limitations.  You can't have used the machine a lot, or that increases the probability of something overwriting that memory, which was released back to the operating system.  It should have wiped it.  Windows 10 does.



But apparently, we don't know about 8.1, but Windows 7 is known not to.  And in their testing of it, under XP, every attempt to decrypt when the machine had not been powered down - and I'm not sure about rebooting, whether that would wipe the memory.  It doesn't necessarily wipe the memory when you reboot.  But that probably makes it much more difficult to find.  Within these limitations, it looks like it's possible.  But that's as a consequence of these developers using the CryptoAPI.



LEO:  So funny, it's another Windows bug that's being used to mitigate.



STEVE:  Yup.



LEO:  Father Robert's going to do it on Know How.  He's already infected a machine.  We have a little mini network.  Last week he WannaCried it.  This week he's going to WannaCry it and see if the disinfection works.



STEVE:  Cool.  Nice.  And, for example, this is one of the reasons I'm not using any unknown functional CryptoAPI in the SQRL code.



LEO:  NACL, baby.



STEVE:  It's all my own, yeah.



LEO:  You're not even using NACL?



STEVE:  I used Bernstein's crypto stuff.  But I maintain my own security region, which is swap locked, so it can't be swapped out.  And my code is constantly zeroing that region.  The moment it's through with anything sensitive, it proactively wipes it.  And I've got various monitor daemons that are always checking to make sure that nothing happened that allowed anything to stay there.  So I've been, I mean, that's the way you write secure code is you, from the first moment, you are scrupulous about the way you manage the secrets in your code.



LEO:  Not Microsoft, apparently.



STEVE:  No.  And our friend Simon Zerafa tweeted:  "98% of WannaCry victims running Windows 7.  Can we finally nail the lid shut on the XP myth?"  And in the show notes here is a picture that Kaspersky created and that BleepingComputer quoted, showing the various infection levels.  What we know is that XP can be infected, but the worm doesn't propagate.  The worm's SMB attack fails on XP, but it does succeed on Windows 7.  So the reason so many Windows 7 machines are infected is actually a mistake in the packet composition for the exploit that didn't infect XP.  So, yes, we have closure on that, too. 



LEO:  All right, Steve.  We continue on.



STEVE:  So how many infections?  The latest count from the WannaCry sinkhole, the kill switch server, is 416,989 individual IPs.  Now, apparently that's not including the 604,102 unique IPs from manual visits to the domain, so non-worm visits, just people curious.



LEO:  I've done that.  I typed it in.



STEVE:  I'll just go check that crazy...



LEO:  See what's going on there.



STEVE:  Yeah.  So just shy of 417,000 infected machines.  Wow, that's a biggie. 



LEO:  Yeah.



STEVE:  The Wall Street Journal, of course, has their content behind a pay wall.  And there was a blurb in Apple News that I saw yesterday, but I didn't read it there at the moment.  And now I can't get to it, and I'm unwilling to give - I just don't use The Wall Street Journal enough for it to make sense for me to pay for what's there.  But I just thought - the title of the article was "All IT Jobs are Cybersecurity Jobs Now."  And the subhead was "The rise of cyberthreats means that the people once assigned to setting up computers and email servers now treat security as top priority."  And I just, even though that's all of the story that I have, that's really as much as we need for this podcast.



LEO:  Oh, neat, yeah.



STEVE:  Yeah.  And I just thought that was an interesting sign of the times, and of course I think really good news, that rather than there being, like, some guy whose job is security - we've talked about this, that it's not something that one person can be responsible for in a huge organization.  All of IT, everybody has to have at least that in the back of their mind, at least an awareness of the security consequences and implications of everything they do.  And, sure, you could still have a czar who keeps reminding them and refreshing them.  Someone sent me a note over the course of the last week that their company actually sets up fake external pseudo entities that send mail into their company, trying to trap their own employees into clicking on a link.  And then, if it works, they get a visit from someone in the corporation saying, you know, yesterday you got this email, and you clicked on it, and we need to have a talk because you can bring down the entire organization.



LEO:  Yeah, yeah, isn't that great.



STEVE:  So, yay.  Okay.  So Stack Overflow.  As a developer, when I'm in a hurry, and I just want a quick answer to something, I google a phrase, and more often than not Stack Overflow is the first five responses.  And the content is not super high quality.  There's a lot of people giving their opinions.  But it's a starting point.  And very often there will be a link to something that will be definitive.  And it's like, oh, great.  So it's, for me, I mean, I'm intimately familiar with Stack Overflow.  They have a huge, you know, Stack Exchange is the family.  And it's a huge and sprawling website.



So yesterday, Monday, yesterday, as I wrote here in the show notes, the well-known, very popular, and sprawling Stack Exchange family of developer-oriented websites completed their conversion to HTTPS.  Nick Craver, who's a software developer and systems admin for Stack Exchange, who spearheaded this work, yesterday detailed the journey in a very lengthy blog post.  I have the link here in the show notes.  And I read through a lot of it.  Nick is sick and tired of everyone saying "Just use Let's Encrypt," as if that would simply solve every problem.  So his detailed posting explains and will be of interest to anyone who wants to gain a better understanding of why "Just use Let's Encrypt" is not the answer for every case.



To get some sense for the nature of the challenge, Nick had a bulleted summary, noting that, "For example, we have hundreds of domains, many sites and other services, many second-level domains - stackoverflow.com, stackexchange.com, askubuntu.com, and so on.  Many fourth-level domains, for example, meta.gaming.stackexchange.com."  He says:  "We allow user submitted and embedded content, images, and YouTube videos and posts.  We serve from a single datacenter.  We have ads and ad networks.  We use websockets, north of 500,000 active back to our server at any given time.  We get DDoSed, so we have to have proxies.  We have many sites and apps communicating via HTTP APIs."  And he says in parens, "(creating proxy issues).  And we're obsessed with performance."  And he says in parens:  "(Maybe a little too much.)"



So anyway, to summarize, he says:  "The most common question we get, why not use Let's Encrypt?"  And he says:  "Answer:  Because they don't work for us."  That is, Let's Encrypt won't work for us.  "Let's Encrypt," he says, "is doing a great thing.  I hope they keep at it.  If you're on a single domain or only a few domains, they're a pretty good option for a wide variety of scenarios.  We are simply not in that position.  Stack Exchange," he writes, "has hundreds of domains.  Let's Encrypt doesn't offer wildcards.  These two things are at odds with each other.  We'd have to get a certificate or two every time we deployed a new Q&A site, or any other service.



"That greatly complicates deployment and either, A, drops non-SNI clients" - you remember that's Server Name Identification that allows multiple domains to reside at a single IP.  And he notes that, even today, around 2% of their traffic are non-SNI-aware clients.  So they're one in 50.  "Or, B, requires far more IP space than we have."  Meaning that they would need, in order to accommodate that, they'd need to bind individual certificates to IPs, and they don't have that many IPs.



And he says, "Another reason we want to control the certificate is we need to install the exact same certificates on both our local load balancers [which terminate the - and I'm adding this - which terminate the TLS connections and then forward the traffic to the servers behind them] and our CDN proxy provider.  Unless we can do that, we can't fail over, that is, away from a proxy, cleanly in all cases.  Anyone that has the certificate pinned via HPKP" - which is HTTP Public Key Pinning - "would then fail validation.  We're evaluating whether we'll deploy HPKP, but we've prepped as if we will later."



And then he finally says:  "I've gotten a lot of raised eyebrows at our main certificate having all our primary domains plus wildcards.  Here's what that looks like."  And I have a picture of it in the show notes, and I had to smile when I saw who Stack Exchange chose to build for them an amazing certificate.  And our listeners can probably guess:  It's DigiCert.  And they produced an amazing certificate because it has a high-assurance extended validation.  Oh, I'm sorry, no.  The EV is an EV root, but it is also an SHA-2 high-assurance - I'm trying to decide if the Stack Exchange certificate is EV.  Can you go to www.stackexchange.com, Leo?



LEO:  Sure.



STEVE:  And see if you get an EV flag?  Because I'm assuming, since it's signed by - they have a high-assurance intermediate and the EV root.



LEO:  Yes, it's green.  That means it's [crosstalk].



STEVE:  And so you can't normally get that.  I mean, EV certs, I mean, they had to go to some serious hoop-jumping in order to create a certificate like this.  You can see...



LEO:  Yeah, can't do wildcard, there's all sorts of, I mean, this is not - Let's Encrypt never promised to be the ultimate solution that would put everybody else out of business.



STEVE:  Correct.  And then in Nick's Q&A he asks himself the question:  "Where do you get certificates?"  And then he answered:  "We use DigiCert.  They've been awesome."



LEO:  Yeah, so do we.  That's what TWiT.tv is.



STEVE:  And everyone knows I do, too, yes.  So they got DigiCert to make them a very special extended validation certificate with a massive SAN, which we were also talking about recently because remember that there's going to be a deprecation of the use of the certificate's name in favor of only domains explicitly listed in the Subject Alternative Name, the SAN record, which in this case enumerates all of the various domains and wildcards for subdomains underneath those domains.  And of course issuing that would have taken a great deal of careful work on DigiCert's part because they needed to firmly, and with EV veracity, verify and prove ownership of every root in that second-level domain set.



So bravo to Stack Exchange for making the effort, a significant effort to go to HTTPS, and DigiCert for providing them with the ability to do that, essentially.  And, yes, as you say, Leo, and as he explained, Let's Encrypt is not the simple answer except for simple certificates.  So it achieves what they were trying to achieve, which is lowering the bar and making it difficult for someone to say, oh, we are not HTTPS because we can't - doesn't make sense for us to afford a certificate.  Well, now simple DV, simple server certs can be free.



Okay.  So ProPublica and Gizmodo co-published this report on hacking Mar-a-Lago.  I've got the links in the show notes, but I'll just share the top of it, which is kind of fun and summarizes it.  They wrote:  "Two weeks ago, on a sparkling spring morning, we went trawling" - in more ways than one - "along Florida's coastal waterway.  But not for fish.  We parked a 17-foot motor boat in a lagoon about 800 feet from the back lawn of The Mar-a-Lago Club in Palm Beach and pointed a two-foot wireless antenna that resembled a potato gun toward the club.  Within a minute, we spotted three weakly encrypted WiFi networks."  And I assume that means...



LEO:  WEP; right?



STEVE:  ...WEP encryption.  "We could have hacked them in less than five minutes, but we refrained."  Of course they're publishing this publicly, so you bet they are saying they refrained.  "A few days later, we drove through the grounds of the Trump National Golf Club in Bedminster, New Jersey with the same antenna and aimed it at the clubhouse.  We identified two open WiFi networks that anyone could join without a password.  We resisted the temptation.



"We have also visited two of President Donald Trump's other family-run retreats, the Trump International Hotel in Washington, D.C., and a golf club in Sterling, Virginia.  Our inspections found weak and open WiFi networks, wireless printers without passwords" - we know how juicy those are - "servers with outdated and vulnerable software, and unencrypted login pages to back-end databases containing sensitive information.



"The risks," they write, "posed by the lax security, experts say, go well beyond simple digital snooping.  Sophisticated hackers could take advantage of vulnerabilities in the WiFi networks to take over devices like computers or smartphones and use them to record conversations involving anyone on the premises.  'These networks all have to be crawling with foreign intruders, not just ProPublica,' said Dave Aitel, chief executive officer of Immunity, Inc., a digital security company, when we told him what we found."  And of course that's the first thing I thought and our listeners would think is, yes, ProPublica is not going to do anything that would create a felony cyber intrusion offense for them.



LEO:  I'm surprised the Secret Service didn't come on over and say, "Hey, what you doing with that potato gun?"  I mean, seriously.



STEVE:  Yes, I know.  I know.  I was watching something, I don't remember now who it was.  It was some show where somebody resides next to Mar-a-Lago and, with a reporter and a camera crew, walked out onto the beach and walked over to Mar-a-Lago and walked right up onto the grounds.



LEO:  I'm sure the President wasn't there at the time, though; right?



STEVE:  Yeah.



LEO:  Don't you think there's a little better security when he's there?  Not that I...



STEVE:  We would like to hope that.  The problem is we already know that printers are easily taken over and used as a beachhead.  And apparently there are open printers with no passwords.  It's like, oh, lord.  So, yeah.



"Security lapses," they write, "are not uncommon in the hospitality industry which, like most industries and government agencies, is under increasing attack from hackers.  But they are more worrisome in places where the President of the United States, heads of state, and public officials regularly visit.  U.S. leaders," they write, "can ill afford such vulnerabilities.   As both the U.S. and French presidential campaigns showed, hackers increasingly exploit weaknesses in Internet security systems in an effort to influence elections and policies.



"Since the election, Trump has hosted Chinese President Xi Jinping, Japanese Prime Minister Shinzo Abe and British politician Nigel Farage at his properties.  The cybersecurity issues we discovered could have allowed those diplomatic discussions and other sensitive conversations at the properties to be monitored by hackers."  And of course the last thing we want in this case is for anyone to be blackmailed as a consequence of information that would be disclosed that way.  Yikes.



And speaking of yikes, Theresa May is said to be creating a new Internet, to be controlled and regulated by the government.  What we understand now, though, reading into this a bit, is that she is proposing to simply hyper-regulate what Internet connectivity they have control over.  This was reported by the Independent.co.uk, saying that:  "Theresa May is planning to introduce huge regulations on the way the Internet works, allowing the government to decide what is said online."  And Leo, prepare yourself because I know how you're going to feel about this.



"The plans will allow Britain to become 'the global leader in the regulation of the use of personal data and the Internet,' the manifesto claims.  The manifesto suggests the government might stop search engines like Google from directing people to pornographic websites.  'We will put a responsibility on industry not to direct users - even unintentionally - to hate speech, pornography, or other sources of harm,' the Conservatives write.



"Particular focus has been drawn to the end of the manifesto, which makes clear that the Tories want to introduce huge changes to the way the Internet works.  'Some people say that it is not for government to regulate when it comes to technology and the Internet,' it states.  'We disagree.  In harnessing the digital revolution, we must take steps to protect the vulnerable and give people confidence to use the Internet without fear of abuse, criminality, or exposure to horrific content,' the manifesto claims in a section called 'the safest place to be online.'"



They write:  "'Our starting point is that online rules should reflect those that govern our lives offline,' explaining the justification for a new level of regulation.  'It should be as unacceptable to bully online as it is in the playground, as difficult to groom a young child on the Internet as it is in a community, as hard for children to access violent and degrading pornography online as it is in the street, and as difficult to commit a crime digitally as it is physically.'"  Yeah, good luck with all of that.  That's me saying that.



"The manifesto also proposes that Internet companies will have to pay a levy like the one currently paid by gambling firms.  Just like with gambling, that money will be used to pay for advertising schemes to tell people about the dangers of the Internet, in particular being used to 'support awareness and preventative activity to counter Internet harms,' according to the manifesto.  The Conservatives will also seek to regulate the kind of news" - wow, fasten your seatbelt, Jeff Jarvis - "that is posted online and how companies are paid for it.  If elected, Theresa May will 'take steps to protect the reliability and objectivity of information that is essential to our democracy' and crack down on Facebook and Google to assure that news companies get enough advertising money."



LEO:  Ha.



STEVE:  What?



LEO:  What?



STEVE:  "If Internet companies refuse to comply with the rulings - a suggestion that some have already made about the powers in the Investigatory Powers Act - then there will be a strict and strong set of ways to punish them."



LEO:  Jeez.



STEVE:  "'We will introduce a sanctions regime [ugh] to ensure compliance, giving regulators the ability to fine or prosecute those companies that fail'" - I know - "'that fail in their legal duties, and to order the removal of content where it clearly breaches U.K. law,' the manifesto states.  In laying out its plan for increased regulation, the Tories anticipate and reject potential criticism that such rules could put people at risk.  'While we cannot create this framework alone, it is for government, not private companies, to protect the security of people and ensure the fairness of the rules by which people and businesses abide,' the document reads.  'Nor do we agree that the risks of such an approach outweigh the potential benefits.'"



LEO:  Wow.



STEVE:  I know.  It's breathtaking.  I mean, it is goodbye the Internet that we have known, and another example of personal responsibility being apparently just something that we shouldn't be responsible for.



LEO:  Right.



STEVE:  Yeah, wow.



LEO:  What you gonna do?



STEVE:  So an update on the state of certificate revocation.  To sort of remind everyone where we've come from, the problem is that certificates are issued for a fixed time period.  They have an expiration date typically no longer than three years, in some cases two, in some cases one, and in some cases even shorter, in the case, for example, of Let's Encrypt, where you have an automated issuance system, so it makes sense to shorten the expiration time because you've reduced the burden to near nothing to reissue.  And that's one way of mitigating the problem with revocation.



The revocation problem is that, if a certificate is newly issued for, say, for three years, and then something happens that it gets out of control, the certificate's private key, which is essentially the certificate, signed by an authority, escapes the control of its owner.  Then that would allow someone to maliciously use that in concert with a domain intercept or a traffic rerouting to take somebody to a site that is able to legitimately appear as the domain that that certificate covers.  Or, horrifyingly, in the case of Stack Exchange, all the domains, the hundreds of domains that certificate covers.



So it was always recognized that there needed to be a means of managing revocation, that is, some way, once a certificate - because, like, the certificate is a static, self-contained assertion of its own validity.  The statement it's making is that anyone who presents the certificate is the valid owner of the domain that is bound, the domain name bound in that certificate.  And it has an expiration date.  It has a not valid before/not valid after pair of date and time stamps.  So it can exist independently and is assumed to be valid until the expiration date, unless something intercedes, thus revocation.  It can be revoked by the CA, the Certificate Authority, but how is the knowledge of that revocation conveyed to somebody who wants to trust the assertion that the certificate makes?



Originally, we had what's known as a CRL, a Certificate Revocation List.  Every certificate authority would publish a list and would maintain the list periodically.  The problem is, as the Internet exploded, and as more sites wanted HTTPS and initially SSL and then TLS secure connections, we had an explosion of certificates.  With that came an explosion in the size of the revocation lists.  And the model of revocation lists was always kind of broken because it would mean that a user's browser that connected to a domain would get a certificate from the domain's server, would then look in the certificate for the URL of the signing authority's CRL server, and would ask that certificate revocation list server for the list of all revoked certificates.  And this list could be huge.



Technically, once the certificate has naturally expired after its three-year life, plus some slop for clock error, then that certificate could be removed from the list because the certificate has expired itself by virtue of its own - the time and date stamp that it carries.  But still, that means that all certificates that have not yet expired, plus some fudge factor, would have to be on the list.  And the model's broken because the browser just wants to check one domain.  Instead, the model is ask a CA for every revoked certificate that isn't yet expired in a single list.  And then the browser goes and searches it to see if the one it cares about is in the list.  So that was all bad.



Chrome came along from Google, and of course they basically did nothing.  Google went their own way and created what I actually revealed to be a wholly nonfunctional solution known as CRLSets.  Which, when I looked at them, I realized, well, this can't work because it's basically a small list of highly publicized, high-profile revoked certificates.  There were, like, seven of them when I first looked, plus a bunch of EV certificates that had been revoked.  But back then most certificates weren't, and even today most certificates are not EV.  Most of them are just DV, domain validation, not extended validation.



And so I created a revoked.grc.com cert, and it generated a lot of news.  So Google added it.  It was like Certificate No. 7 in their revoked list.  And so I thought, okay, fine.  And I created another one.  And they haven't bothered to revoke that one because then I would just create a third.  The point is Chrome doesn't do this, either.  Completely broken.  On the other hand, if you go to revoked.grc.com in Firefox, you get an error page that says "Security error:  Revoked certificate."  So, gee, Firefox is doing it.  Chrome isn't. 



Okay.  So what was created then was an online facility.  Instead of getting the whole list, the idea was, in what's called the OCSP model, the Online Certificate Status Protocol, the browser would fetch a certificate from a site it wants to have a secure connection to.  Receives the cert.  In the cert is an OCSP URL which gives it the domain of the server that serves the OCSP protocol.  So now the browser simply asks for a specific domain.  Is this domain valid?  And the OCSP server says yes or no, depending.  So that's, like, way better, but it's got some problems.  If the OCSP server doesn't respond, then does it fail not trusting or fail trusting?  Because if it fails not trusting, an outage of the OCSP protocol server would prevent you from accessing that site.



On the other hand, if it fails trusting, then bad guys could DDoS the server to get their certificate trusted or could block the OCSP traffic in order to get it to be trusted.  So you're sort of messed up either way.  Also, it creates a substantial additional roundtrip time.  You've got to establish a TCP connection, bring up TLS because you need this to be a secure exchange, and then the communication.  So it creates an additional time overhead and slows everything down.  This is for every HTTPS connection that the browser would make to additional domains for the page.  So it's better, but it's still got lots of problems.



The next idea was, oh, okay, let's have the server which is serving the certificate "staple," as the term is, a recently received OCSP assertion to the TLS transaction.  So the idea being that the certificate lives a long time, but the OCSP assertion doesn't.  And so periodically the server, the web server that wants to serve high-integrity certificates, it will go and get the OCSP assertion and include that in the handshake.  That eliminates the overhead of the browser having to do it.  It allows a long-lived certificate and short-lived reassertions that there's been no expiration so far, to keep it valid.



And so that's sort of nice except there's still a problem because the OCSP can't be in the signed certificate because that's digitally signed, and you can't change the certificate.  So the OCSP staple has to be in the protocol, separate from the certificate.  But that means if bad guys steal the certificate, they just won't provide a stapled OCSP, and so we're back to the beginning again.



So the final solution was to embed in the certificate itself, the original signed certificate, a new flag, "OCSP Must Staple."  So now the certificate says the server serving this must provide a valid OCSP staple, or the certificate is invalid.  So that solves the problem of bad guys getting a certificate and just not including a staple, and also getting in the way of the OCSP and the need for the browser to go out and fetch its own OCSP information, thus slowing down all the transactions.



So all of that seems great, except where we are today is that neither of the two majority web servers on the Internet, which would be Apache with about a 46% share and Nginx with about a 20% share, neither of them have it correctly implemented.  They are both badly broken.  The idea should be that the server caches the most recent OCSP staple information and includes it with all the certificates that it hands out, like in the TLS handshake.  And then periodically, as it begins to approach expiration, it reaches out to the OCSP server to refresh its cached OCSP assertion.



It turns out that both Apache and Nginx are horribly broken.  If anything happens that causes an invalid OCSP to be received, or no OCSP to be received, rather than saying, oh, shoot, it's a good thing I started asking early, I'll wait a bit and try again, and again and again and again and again, hopefully finally achieving success before the existing staple expires, instead Nginx and Apache, if anything happens to their attempt to obtain a refresh, they invalidate the still-valid OCSP staple in their cache, thereby failing all connections to any server whose certificate has OCSP Must Staple in the certificate in order to succeed.  So, okay, that means nobody can even today actually use OCSP Must Staple until the majority web servers on the Internet get themselves fixed.  And people have tried to turn it on and immediately started having problems.



So right now - oh, and by the way, this problem was reported in 2014 to the Apache group, and it remains unfixed today.  So apparently they've got other things to do, or it just isn't high enough on their priority list, or they don't care.  I don't know.  But it's always been a problem, I mean, the problem of revocation has always been known, and we've never yet come up with a good fix.  We're getting closer.  But one of the enduring lessons of this podcast is these things which are broken or are trying to evolve just happen on a very slow time scale.



Firefox is considering giving up on OCSP for DV certs.  So this is the last shoe to drop on the OCSP side.  They filed a bug report, just because that's how they sort of communicate, saying that their telemetry indicates that fetching OCSP results is an important cause of slowness in the first TLS handshake.  Firefox, they write, is today the only major browser still fetching OCSP by default for DV certificates, meaning the lowliest common form, not even OV, which is Organizational Validation, but just Domain Validation.  It's like, yes, you've proven to me that you have this domain.



They said:  "Earlier we tried reducing the OCSP timeout to one second," meaning that they were going to wait less long for an answer, which would, if an OCSP result was slow to come, they wouldn't hang the whole connection.  They said:  "But that seems to have caused only a 2% improvement in 'SSL time until handshake finished' metric."  So this bug, as they call it, although it's not really a bug, it's a feature, is to disable OCSP fetching for DV certs.  "OCSP fetching should remain enabled," they write, "for EV certificates.  And OCSP stapling will remain fully functional.  We encourage everyone to use OCSP stapling."



So essentially they're just saying, you know, we're being hurt because we're providing - we're being hurt in comparative performance because we're the last browser to bother actually checking to see whether a DV certificate may have been revoked.  And so we're going to stop doing that.  Which is sad.  And they are explaining that stapling they will still honor, if a server staples.  And of course, as we just covered, that doesn't provide you with a firm guarantee.  But we're getting there.  But what we need now is the server technologies to properly implement the OCSP stapling so that certificates can then safely add the OCSP Must Staple, and then we'll finally have a sane and workable solution for revocation.  But it's difficult to get there.



BleepingComputer covered some interesting research.  Where is - I didn't follow the report down.  I've got the PDF:  ethz.ch.  I know them, but I can't - it's not coming to mind who they are.  But a bunch of researchers have discovered that the latest generation, which are the MLC, the multilevel cell SSDs, can be subject to physical adjacency disturbances similar to what we've seen with DRAM and the Rowhammer attacks.  And our listeners will remember that I deliberately purchased SLC, single-level cell SSDs for GRC just because I was suspicious of MLC.  It seemed like kind of a cheesy thing to do in order to squeeze more bits into the same space.  Yes, you get more density.  But you're going to lower your integrity.  And I don't need that much server space anyway.



Anyway, there are two attacks they've come up with, one called "program interference," a program interference attack, as they named it, occurring when data is deliberately written in specific ways and in MLC SSDs results in a 4.9 factor increase in the SSD's error rate.  This, they write, can corrupt the SSD's stored data and shorten the device's lifetime.  So this is bad.  This is like a statically appliable attack to deliberately damage an SSD.  The second attack they call "read disturb" occurs when a large number of specially patterned reads, which is very Rowhammer reminiscent, are performed within a short time.  The researcher said these read disturb errors will "corrupt both pages already written to partially programmed word lines in memory and pages that have yet to be written, thus ruining the SSD's ability to store data in a reliable manner in the future."



Now, I would counter this, and I did not have time to look at their paper in depth, but it is uncommon that software probably has the kind of access that would be needed.  It's not clear to me whether sector-level read and write access is granular enough to induce these kinds of problems.  But I wanted to put it on our radar that it looks like it's not just DRAMs that are vulnerable as a consequence of this rush to density.  Remember that it's the newer DRAM, the DDR3s and 4s that, as a consequence of the crazy density that they now have, that minimize the cell sizes in DRAM that subjected them to this noise induction from adjacent row interference.



Similarly, it's the crazy rush to density which caused manufacturers for competitive pressure reasons to need to put multiple bits of data into the same cell by creating multi levels of charge.  And then they need to discriminate among those levels of charge in order to re-extract the bits.  And so there are now two bits and even three bits per cell.  And that just reduces the error margin.  And then of course it makes them vulnerable to, now we know, this kind of attack.  And this kind of attack looks like a persistent attack, that is, a static problem, where Rowhammer is just a dynamic problem.



Also it came to my attention that Twitter has just discontinued their support of the Do Not Track browser preference.  They wrote:  "While we had hoped that our support for Do Not Track would spur industry adoption, an industry standard approach to Do Not Track did not materialize.  We now offer more granular privacy controls, and you can learn more about them outlined in the articles below."  And I looked, and they're completely, you know, I mean, like it's nothing.



And so what I think this really means, reading between the lines, is that Twitter has chosen to no longer respect the user's browser sending of the DNT, the Do Not Track query header.  They've chosen to ignore it.  And it's difficult to see how, if they were truly concerned about respecting privacy, they would choose to disable a privacy-enhancing feature that they had already implemented.  I mean, they had already done the work.  So why decide, oh, well, we changed our mind, we're going to take it out?



So what must actually be happening is that they want to be able to track users whose browsers are sending out the Do Not Track beacon, and this statement is their way of justifying that.  They're essentially saying "Other services are ignoring their visitors' explicit requests to not be tracked, so now we're going to ignore it also."  Like, okay.  Yeah.  Unfortunate.



I got a bunch of tweets, not surprisingly, from people telling me, informing me that the LastPass Authenticator would now be able to back up our two-factor data online.  Robert tweeted:  "@SGgrc @LastPass Authenticator can now back up your two-factor data online."  Samuel Movi said:  "@SGgrc I guess you won't be using your QR printouts much.  In the future LastPass will store two-factor codes alongside your passwords."  And Sean Stephens tweeted:  "@SGgrc Someone @LastPass is listening to Security Now!.  LastPass Authenticator now backs up to LastPass account for recovery and sync."



My response:  This is not a good thing.  This is bad.  Multifactor security requires heterogeneous deployment.



LEO:  Yes.



STEVE:  The idea is that my password and my time-based tokens must both be provided to log on.  But if the same service contains both my password and my master set of time-based token keys, a single compromise of that service renders my entire use of a second factor meaningless.  Also, I don't want an authenticator that is willing to export my time-based token keys.  Anyone who gains control of a device's UI could export the key set.  So the optimal solution is what I am doing and what I have recommended:  Choose an authenticator app, not LastPass's app - and I don't like LastPass's app either because the things are so huge you've got to scroll through in order to see a six-digit number.  I'd much rather have - I'm using Google's Authenticator, although there's another one that I haven't gotten around to looking at that apparently has an even denser display, which I would prefer.



So choose an authenticator app that deliberately does not allow for time-based token key export, like Google's Authenticator does not.  And you take responsibility for managing the time-based token keys yourself.  Yes, it's more work.  But it's also the only way to actually obtain the security offered by a second factor.  That's not something you want to relegate, and certainly not to the person who's storing all of your first factors.



LEO:  It's why I never use the LastPass Authenticator.



STEVE:  I will be printing out my QR codes.  I have a growing sheaf of printed out QR codes.  And when I set up a new one, I just go through, it takes about a minute to say, here, sniff this, sniff this, sniff this, and then I'm good to go.  And it's a one-way process.  It makes much more sense to do it that way.



Okay.  So yesterday was the 44th anniversary of the invention of the Ethernet.  And as I said, Bob Metcalfe tweeted that "Ethernet's 44th birthday is Monday.  I sketched this" - that is, the diagram on the first page of our show notes - "to include in a typed memo," he tweeted, "on May 22, 1973 at the Xerox Palo Alto Research Center."  And so I just wanted to note, to give Bob some major props because the Ethernet was a truly brilliant innovation.  At the time, solutions involved some sort of centralized controller to negotiate access.  There really it was no good definition of a network.  We were just sort of - we were pre-network.



And remember that the traditional mainframe and terminal model was that all the terminals would connect back to sort of a concentrator device, and so they would all have their own connections.  But this whole concept of an inter-workstation network was new.  Now, IBM had something known as the Token Ring.  And the idea there was that you would literally have a ring topology where workstations were connected, and the so-called "token" was like a baton.  And so a workstation would hand the baton to the next workstation.  If it had something it wanted to say or send, it would send it.  And then it would hand the baton to the next workstation.  If a workstation received this baton, and it did have nothing to say, it would immediately forward it to the next one.



So this token would circulate around the network.  And it was only the holder of the token could speak on the network.  And so that sort of solved the contention problem, well, did solve the contention problem, but actually at a cost of great complexity because then what happens if a workstation hangs, as they used to back then, when it had the token?  Now the token was lost, you know, like the baton being dropped.  And so you needed some provision for token regeneration.  You needed all sorts of systems in place to deal with what happens, you know, I'm not getting the token, who has the token.



Anyway, it really escalated.  Bob's brilliance was to conceive of a network where all the peers, where everybody connected was a peer.  And they were on a common backbone, a common ether.  That is, so everybody could hear everybody else.  And then he developed a brilliant and minimal simple logic.  And that is, anybody that wants to speak listens, because everybody can hear, listens until nobody is speaking.  Then they choose a random number which is how long to wait before speaking.  And if, when that time is up, nobody is still speaking, they speak.  And while they're speaking, they're also listening to see if anybody else happened by chance to start speaking at the exact same time because, before you speak, you make sure nobody is.



But there is a chance two people could both start at the same time.  So if that happens, they will both hear the other, or all three of them, or all four of them, however many happen to try to speak at the same time.  So they then choose another random number, which is how long to back off before retrying.  And that's it.  It allowed a simple logic to exist in every NIC, Network Interface Controller, and it solved the problem.  Everybody got to talk.  And they all managed locally when they talked with a simple set of rules that they would obey, and the system worked.



And that is the Ethernet we have today, whether it's a wireless Ethernet, which is WiFi, or it's a wired Ethernet, that system was so good that it won.  And the only problem, in the same way that the Internet guys who developed packet switching realized, okay, this is not perfect, but it's good enough, that is, we're not going to guarantee packets.  We're going to have this just sort of opportunistic routing, and on top of that will be a protocol that solves the problem that buffers may overflow, packets may get dropped, routers may go down and so forth.  Similarly, there's a problem with Ethernet.  And that is, as the amount of traffic on that particular Ethernet segment, that ether backbone, whether it's wireless or wired, as the amount of traffic increases, think about what happens.  At some point everybody has too much to say.  And so the incidence of collision, of packet collision, where two people are talking at the same time, increases.



And so the efficiency of Ethernet falls to the point of failing as you begin to approach the maximum, actually well before you approach the theoretical maximum bandwidth of the system.  So whereas IBM's Token Ring actually allowed you to closely approach much more of the maximum capacity, it did that at the cost of great complexity.  Bob's breakthrough was being willing to sacrifice getting every last bit of bandwidth out of the Ethernet in turn for vastly simpler operation.  And that was 44 years ago yesterday, May 22nd in 1973, as we were graduating from high school, Leo, that Bob came up with this.



LEO:  Yes, he did.  And a few years later Esther Dyson got everybody together.  I love these pictures.  Is that next?



STEVE:  Yes, yes.  A bunch of people tweeted them to me for a reason that you'll see.



LEO:  Yeah.



STEVE:  There's a young Bill Gates there, second from the left.



LEO:  Gary Kildall, John Sculley, I'm not sure who...



STEVE:  Gary Kildall.  Yup, Sculley second from the right.



LEO:  Is that - I'm wondering if that's Lee Felsenstein?  I'm not sure.  Guy in the Russian outfit is, not Mitch Kapor...



STEVE:  Bricklin.  Right?



LEO:  Dan Bricklin, that's right, yeah.



STEVE:  Dan Bricklin.



LEO:  He was funny.



STEVE:  He dressed up that way, and Esther loved that because...



LEO:  He was Davy Crockett, it looks like.



STEVE:  Well, because he was a pioneer.  And so he was dressed up as a pioneer, the idea being that we were pioneering the PC industry.



LEO:  This is Esther Dyson used to have an amazing conference, Release 1.0.  And these are some pictures.  There's Jobs and Sculley.



STEVE:  Look how young he was, how young Jobs was, and Sculley, yup.  



LEO:  This is fun.  Who is that?



STEVE:  And there's Esther, and there's Bill in the background.



LEO:  Bill Gates, yeah, very young also.  And wait a minute, who's that young fellow?  Who's that guy?  Dark-haired, mustache.



STEVE:  Yup.



LEO:  Steve Gibson.  This is 1984; huh?



STEVE:  Yup.  I was there with all those guys.



LEO:  Were you there as a pioneer?  What were you doing there?



STEVE:  I was on a panel, and I also was the keynote speaker on one of the mornings.



LEO:  Nice.  



STEVE:  So, yeah.



LEO:  Mitch Kapor.  Gary Kildall was there.



STEVE:  There's Mitch, yup.



LEO:  These are Esther's pictures.  This is a lot of fun.  This brings back some memories, some of these people.



STEVE:  Yes, back in the day.  



LEO:  Back in the day.



STEVE:  So I have this week's groan-worthy riddle, and I apologize in advance:  How did the WannaCry hackers get away?



LEO:  I don't know, Steve.  How did the WannaCry hackers get away?



STEVE:  They ransomware.  Ooh.  Sorry.



LEO:  Okay.



STEVE:  Okay.  So Richard Romick sent me a nice tweet.  He said:  "I have a quick SpinRite testimonial for you.  Sorry if I make any Twitter faux pas, as this is the first time I have sent a message."  And Richard, for what it's worth, you did a very nice job.  He said:  "It started when my WD My Book Live Duo NAS told me one of the drives were failing.  While I don't have any reason to mistrust WD, I felt it was a conflict of interest when their device says I need to buy a new WD hard drive.  The NAS requires a specific line of WD hard drives to operate normally," he added.  "Therefore, I decided to SpinRite it.  I grabbed an old ASUS desktop computer out of the closet, and happy to see that the motherboard reported SMART statistics to SpinRite.  I ran a Level 2 SpinRite scan on it.  The scan was all blue across the board, and the ECC statistics looked good, no red.



"However, whenever I stuck the hard drive back in the NAS and ran a long test, everything was good to go again.  I'm guessing SpinRite performed some refreshing of the data it found hard to read, which the NAS had interpreted as the drive failing."  I think that's exactly right, Richard.  "Thanks for the great product.  I'm now running both drives on Level 4 just to be even more sure."  So Richard, thank you for the note.



And one one-liner from Barry Coggins.  He wrote:  "Laptop was running hot with loud fan speed.  Ran SpinRite, and all is well and quiet again."  And of course we've seen that often.  One of the things that happens is that there's, like, people don't appreciate the amount of energy required to move the head.  The head is accelerated and decelerated very fast.  Well, that creates heat.  And one of the things that people have noticed is SpinRite sometimes causes their laptop to overheat because it's having to move the head as much.  I expect that we're going to see 6.1 doesn't do that because the heads should move much more smoothly and uniformly because it'll be reading 32MB blocks at a time, rather than 32KB blocks at a time.  So a thousand times the amount of data per read, which the drives are optimized for and should make things run, not only much faster, but also cooler.



But in this case, if the drive was having a problem, it was having retries and rereads, causing its head to generate much more heat or the drive overall to consume much more power than it would otherwise.  And running SpinRite fixed the read problems and cooled off the drive and allowed the laptop's fan to slow itself down and everything to run cooler, which of course we know is much healthier for computers.  They don't like heat.



And a couple of closing-the-loop comments from our listeners.  Someone whose handle is @mcepl said:  "Hi, Steve.  Let me react in longer form [so this was a DM] on SN-612 [last week's podcast]," he says, "and your reflection on blaming the victim.  I'm afraid that the advice you gave was so simplistic as to be dangerous.  Yes, I can imagine that some MRI machine, completely air-gapped, really can run whatever" - well, he said "archeological."  Maybe he meant that.  I thought he meant architectural, but maybe he means archeological - "system one chooses.  I understand that you, with some level of understanding of computer security, can run Windows XP on your machine; although, as I recall, even you are now on supported Windows 7, aren't you?"  No, I have that machine next to me.  I haven't switched to it yet because I haven't been willing to accept the downtime it would take to move.  I will eventually.



"However, when we are talking about 90% of NHS computers, then I assume neither air-gapping nor sophisticated use of computer can be assumed there.  However, my biggest problem with this whole affair that I have been shouting to anyone whom I could talk to is the use of general-purpose desktop-oriented operating systems for everything, including things that are clearly inappropriate, like arrival/departure monitors, Twitter status and so forth is crazy and should be stopped.  Yes," he says, "I work for RHT."  And that's Red Hat; right?  "But I would never think that even RHEL would be a good idea to be used there.  And it is way more modular, so it can be stripped down more.  You don't even need a  hard drive for such monitors, or they should be read-only.  Just receive message via network and display it.  Why do you need Internet Explorer, Notepad, and FreeCell for that?"



For what it's worth, I am somewhat sympathetic to this.  I do think that it's unfortunate that we see Windows error dialogues on marquees in Vegas and in the terminals in airports and popped up on ATMs.  Unfortunately, it's a function of the fact that it is very easy to develop for Windows because of the huge array of tools that are prevalent.  People are learning to program Windows.  You can program Windows in any language that ever existed.  So that's what ends up getting used.  But I 100% agree that there are probably much more appropriate and less expensive and vastly more secure operating systems available.  But they  would cost more to actually deploy and to get out in the field, so that's not what's being done.



LEO:  Actually, there was a rumor, I don't know if it was true, that the National Health Service in England was thinking of moving to Ubuntu, which...



STEVE:  That would be great.



LEO:  It wouldn't be perfect, but it'd be less of a target, I would guess, yeah.



STEVE:  Yup.



LEO:  Than old Windows.



STEVE:  Bob Beaudoin, I think how I pronounce that.  Sorry, Bob, if I mispronounce your name, Bob Beaudoin.  He said:  "Did you  hang onto the minted bitcoins?  Sure worth something now."  And I did a little bit of follow-up.  And of course that's correct.  They went north of $2,000 per bitcoin over the weekend.  And of course we've seen a 450% increase over the last couple years.  It turns out - here's the math, which is somewhat distressing.  Had any of us purchased $100 worth of bitcoins seven years ago, we would be sitting on $72.9 million today.



LEO:  What?  Whoa.  That means Satoshi is rich.



STEVE:  Yes.  Yes.



LEO:  Wow.  Wow.



STEVE:  Seven years ago $100 of bitcoin is equivalent now to just shy of $73 million.  Back then, let's see, the price was 0.003 cents on May 22, 2010.  Wow.  So just...



LEO:  That's amazing.



STEVE:  It is incredible.  Bitcoin trading hit $2,185.89 in the early hours of Monday morning, yesterday, hitting a fresh record high after it first cruised through the $2,000 barrier over the weekend, according to CoinDesk data.  Yeah, so I guess Mark Thompson, minting in his garage, is feeling pretty good right now.  Wow.



Oh, and Kostas Kristilas said:  "Steve, in show 612, also last week, you were talking about the Android O restructuring and its similarity to Windows NT's HAL.  You had some difficulty remembering the third architecture supported.  I'm pretty sure," and he's right, "it was DEC's Alpha."  And remember I said Intel and MIPS, and then Alpha.  And then he says:  "In addition, later on, it supported PowerPC and Itanium, the latter two in Windows 2000."  And then he says:  "Later on, that was ARM."



ETC Maryland said:  "@SGgrc Windows S is not better because it's closed.  It is still Windows and will not be secure.  Just because an ecosystem is closed doesn't equal secure."  And he says:  "If that was the case, then open source means less secure.  It doesn't."  And I would take issue with this.  I disagree.  I think that an operating system where the only applications that can be run on it are curated is absolutely more secure.  Not perfect.  We've seen examples where bad stuff gets through the Apple Store, or gets onto the Android, you know, the Google Play Store.  But as soon as that's known, both of those closed ecosystems are able to respond.  Contrast that to Windows, where anybody can and does download anything from anywhere.  I mean, that is a formula for disaster.



So I don't know if he misunderstood, but I think he's completely wrong.  I mean, you're trading off a huge amount of convenience, the fact that you can't download anything you want from anywhere.  But I would argue what you are getting in return is dramatically more security.  And so, yeah, it does turn it into a limited toy, as opposed to a radically open workstation.  I choose the latter.  But I think there's certainly a place for Windows S, for example, in the educational environment, where a browser and an editor, an office suite, just a few things is all you really need, and not the kitchen sink that we have with Windows.  I'm not going to be putting my apps there because I'm not going to be developing for the UWP system.  But it remains to be seen whether this succeeds or not, or whether Windows S is just another Windows Phone.



Somebody tweeted a question from 609:  "How are all these people able to unlock others' iOS devices when, after a few incorrect attempts, iOS enforces the passcode and wipes the device?"  And I tweeted to him, and I'll just share the answer here.  The device must support some sort of "fail count" in nonvolatile memory.  So the people hacking phones, like the Cellebrite folks that I assume he's talking about, take a snapshot of the nonvolatile memory before they make their first guess.  They then make the maximum number of failed guesses.  Then they restore the snapshot, thus resetting the fail count to zero, and then make another round of guesses.  Then refresh the snapshot again, make another round of guesses.  Essentially that allows them to reset the fail count each time.  And I'm absolutely sure Apple - and I haven't kept track of it, whether Apple already has implemented a fix for that, or whether it's coming.  But if it's not already in, in the hardware, you know it'll be in the next generation of hardware.



@sassymanjohnson is this Twitter handle, asks a question from Security Now! this week, which is last week, he said:  "How did folks get the source code to WannaCry?"  Because we were talking about people, like the MalwareTech guy, finding this domain name in the source.  Almost without exception, everybody is using a beautiful tool known as the IDA Disassembler.  If you just google "IDA Disassembler," you'll find it.  And it is a beautiful multiplatform tool for reverse-engineering the compiled machine code of these exploits.  So you drop it in there, this thing performs a static analysis, finding jumps and breakpoints, identifying the tops of subroutines, breaking the code apart into all of the pieces, the individual pieces that originally formed it, and then showing how they interconnect and intercommunicate and link together.  And, where possible, it finds regions of memory that looks like strings and shows them to you as a string of text rather than as a series of bytes.



So it does a whole lot of like the first level, first pass reverse-engineering.  And then somebody who understands this stuff is able to go in and look at what the code does, add labels, and essentially further populate it with information from their own inspection.  So IDA Assembler, that is the go-to tool for this kind of forensic reverse-engineering of code for which you do not have the source, you only grabbed a copy from a machine that got infected, and you want to know what it does and how it works.



And lastly, JC tweeted:  "I keep asking why NTP" - the Network Time Protocol - "cannot be hacked.  No response."  I don't know if he's asking me, or if he's just asking the ether.  But, he says, "We're at a level that in theory it could.  If not, why not?  Podcast, please."



And I'll just say, yes, it can be hacked.  NTP is both a UDP and a TCP protocol, but typically it's carried over UDP in the same way that DNS is because it's much faster.  So it's not encrypted.  It could be hacked.  Although it's not really clear what a hack of time would give you.  Many things are, like we were talking about certificates, if you lied about the date or time enough, then the certificate would be declared invalid, either not yet valid or now expired.  So that Kerberos technology is time-based, and there have been some hacks against it.



But anyway, it's certainly the case that you could play games with time.  But we haven't run across - we have run across exploitable NTP servers where the NTP service itself can be exploited, and we've discussed those.  But you could certainly see an NTP response going by carrying the number of milliseconds since January 1, 1970, and change that into something else, and the recipient would now believe it's the wrong time, but it's not clear that that gets you a huge advantage.  So, yes, now you have a response.  Can be hacked.  Maybe not worth bothering.



LEO:  Wasn't the, I mean, aren't NNTP servers used for reflection and amplification attacks?  I mean, the server...



STEVE:  That, too yes.



LEO:  I mean, the server itself isn't - has been hacked, yeah.



STEVE:  Yup.  That, too.



LEO:  Is that it?



STEVE:  We're done.



LEO:  Nothing more you want to say?



STEVE:  613.



LEO:  In the can, as they say.  We do this show every Tuesday about 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  If you want to come by on a Tuesday afternoon, it's a great way to spend the afternoon or evening, depending on where you are.  You could also join us in the chatroom at irc.twit.tv.  But Steve, you know, he lives at GRC.com.  That's his website, the Gibson Research Corporation.  Lots of stuff there, including, of course, SpinRite, the world's finest hard drive and maintenance utility.  And also this show.  You can get audio of the show and handwritten, carefully contrived transcripts by Elaine Farris at GRC.com.  We have audio and video at our website, TWiT.tv/sn for Security Now!.  And of course the best way is just, you know, subscribe.  Find your favorite podcast application and search for Security Now! and subscribe because you want to collect the set.  You want all 613.  You really - I'm not kidding you.  You really do.



STEVE:  That's right.



LEO:  And they're all available at Steve's site and my site.  And actually the feed itself only has the most recent 10 because otherwise the feed would be prohibitively large.  You wouldn't want it downloaded every hour or so.  We are a prime number, Steve, I'm being informed by Dr. Morbius in the chatroom. 



STEVE:  Nice.



LEO:  Yeah.  One more of those and you could have a key, a public key, a private key.  Thank you, Steve.  We'll see you next week on Security Now!.



STEVE:  Thanks, my friend.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#614

DATE:		May 30, 2017

TITLE:		Vulnerabilities Galore!

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-614.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we discuss a new non-email medium for spearphishing, Chipotle can't catch a break, social engineering WannaCry exploits on Android, video subtitling now able to takeover our machines, a serious Android UI design flaw that Google appears to be stubbornly refusing to address, Linux gets its own version of WannaCry, another dangerous NSA exploit remains unpatched and publicly exploitable on WinXP and Server 2003 machines, a look at 1Password's brilliant and perfect new Travel Mode, Google extends its ad tracking into the offline world, some follow-ups, miscellany, and closing-the-loop feedback from our terrific listeners - concluding with my possibly useful analogy to explain the somewhat confusing value of open versus closed source.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We have, as he says in the title, vulnerabilities galore to talk about, including WannaCry on Linux.  He'll also give you his reviews of a couple of, well, "Alien," the new "Alien" movie, "Alien Covenant."  He's rereading the Frontier Saga.  We'll find out why.  And "Twin Peaks."  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 614, recorded Tuesday, May 30th, 2017:  Vulnerabilities Galore!



It's time for Security Now!, the show where we cover your privacy and security online.  We also do a lot of - I think Steve does a lot of teaching, as well.  I know a lot of people learn from this show.  It's even used in the curriculum at some universities.  Steve Gibson is our host, the man of the hour, the security guru at GRC.com.  And hello, Steve.  It's good to see you.



STEVE GIBSON:  Hey, Leo.



LEO:  I'm proud to say good friend.



STEVE:  Yes.  Great to be with you again for Episode 614, as we cruise through Year 12 of this no-end-in-sight podcast.



LEO:  Would you like sometimes to do some prognostications of what we'll do 12 years from now when we're doing Security Now!, Episode 1228?



STEVE:  No.  Remember?



LEO:  Oh, you're quitting at a thousand.



STEVE:  I'm going out at three digits.  I can't go to four digits.  That'll break my whole system.  So we're not yet done.  We're not there.  666, that'll be, not coincidentally, that's the two-thirds point.  When we get to 666, at that point we have 333 remaining.  So anyway - besides, you're going to be on a permanent vacation by then anyway, so I don't think...



LEO:  Well, I don't know.  I mean, I've got bills to pay.  I don't know.



STEVE:  So this week we have, first of all, the title of this week's podcast:  Vulnerabilities Galore!  There wasn't anything that stuck out because there were just so many just headshakers here.  We're going to discuss a new non-email medium for spearphishing; the fact that the Chipotle restaurant chain just can't catch a break; social engineering WannaCry exploits, get this, on Android, which of course can't be infected with WannaCry, but nobody seems to really understand that; video subtitling now able to take over our machines; a serious Android UI design flaw that Google appears to be stubbornly refusing to address, although at the Black Hat conference next month the pressure's going to get turned up.



Linux now has its own version of WannaCry.  Another dangerous NSA exploit remaining unpatched and publicly exploitable on WinXP and Server 2000 machines, which of course aren't going to get back patched.  We're going to take a look at 1Password's brilliant and perfect, in terms of implementation, new travel mode.  Google has announced that - and this one I said, wait, can this be true?  Google is extending its ad performance tracking into the offline world.  So you just can't escape them.



We've got some follow-up, some miscellany, some fun stuff, and a bunch of closing-the-loop feedback from our terrific listeners.  Oh, and concluding with my possibly useful analogy which one of our listeners asked for to explain to a non-computer-savvy person who knows enough about the question of open source and closed source the somewhat confusing value of open versus closed source software.  I think I have a useful analogy that I came up with when responding to this person's question.  So I think a fun and interesting podcast.



LEO:  That's great.



STEVE:  And if you can get the Picture of the Week on the screen?



LEO:  I have it, yes.



STEVE:  Okay.  By the way, I forgot, I meant to mention before we started recording, but what the heck.



LEO:  Go ahead, do it now.



STEVE:  Andy was talking about the Art of Noise, and I just thought I would share that my absolute all-time favorite cut of theirs is called "Moments in Love."  And it is just - it is a fabulous piece of music.



LEO:  Oh, well, I have to listen to it.



STEVE:  "Moments in Love" by the Art of Noise is one of my all-time faves.



LEO:  Should have known you're an Art of Noise fan.



STEVE:  So our Picture of the Week is just wonderful.  This is the low-tech solution to apparently a somewhat publicly available emergency phone.  So this is a wall phone underneath a sign that says "Emergency Telephone" and then "Only 911 can be dialed."  And looking at it on the wall, you see sort of this band across the middle of it on the backside, and you think, what the heck?  And so the second picture is someone lifting it up and looking at it, and it's this piece of tinfoil which has been wrapped around the keypad with cutouts for the "1" button and the "9" button.



LEO:  You could dial, I should point out, 999, 991, 199, but they really want you to dial 911.



STEVE:  Yeah, they're trying to discourage you from exploring the other buttons.



LEO:  Which makes me want to, frankly.



STEVE:  Yeah.  We'd call this a very weak firewall.  Of course, if I were involved, I would have opened up the handset because inside there is going to be a circuit board where the little elastomer pads are pressed against, and you could just put insulating Scotch tape over all of the pads except the "1" and the "9" and then put the phone back together.



LEO:  Clever.



STEVE:  And that would kill all the buttons except "1" and "9."  But anyway, we have a lower tech solution which is really pretty funny.  I mean, this is a sad bit of...



LEO:  Is there anything it can't do?



STEVE:  Right.  So, okay.  This is just sort of a public service announcement.  Security research firms are reporting a shift from email as the target of spearphishing to Twitter and Facebook social media networks because there's a general feeling of community and trust and sort of a lower level of anonymity, the idea being everyone sort of knows you get spam, and anybody can send anything to anyone in email.  But there's sort of a greater sense of closure and trust and community within a social media circle.  And so some experiments have been done to test people.



And so what's happening is the message is finally getting through not to click on links in email.  I mean, it's a mantra.  And as we've talked in the past, corporations are educating their employees.  Some of them are employing outside firms to run benign probes of their employees from the outside to try to phish them into clicking a link and then sending them to the principal.  And so in an experiment that was conducted late last year, an automated program sent links through social media to 819 people.  Of those, 275 users opened the links, which is a little bit over one in three, which is a far greater percentage than we're now seeing in email.



LEO:  Wow.



STEVE:  So the point is people's guard is finally beginning to come up.  And so what's happening is phishing is becoming less effective.  So what do the bad guys do?  They go, hmm, where else, how else can we get links in front of people?  Oh, let's tweet them.  Let's post to their Facebook page.  And that's what's happening.  So again, as a public service announcement, I want to just put this on our listeners' radar that it isn't anymore the case that only email is a problem.  The bad guys have figured out that we're getting wise to them phishing through email, and so they're increasingly beginning to use alternative approaches, specifically Twitter and Facebook and other, I imagine, messaging apps if they can get to you somehow.  So not good.



I know you talked about it over the weekend, but I wanted just to cover it for the sake of, well, being topical, and that is that Chipotle, the restaurant chain, did formally announce that 



"most," which was an interesting adjective, of its restaurants - is that an adjective, "most" of its restaurants?



LEO:  Most.



STEVE:  I don't think it's an adjective.



LEO:  I don't know, what is it?



STEVE:  It's not an adverb.  I don't know, it's one of those things in the twilight zone - were infected with credit card-stealing malware.  The reason I'm suspicious about mostness is that apparently it was their central corporate chain processing system.  So why wouldn't it have been all of them?



LEO:  That would be all of them.



STEVE:  Yeah.  So here's the deal.  The credit card details of purchases made from March 24th through April 18th may have been exposed.  They said "most restaurants."  But again, since this was a breach of their central charge processing system, it's more likely that all branches were vulnerable, and some PR person put a little spin on the press release finally.  Now, as is the case with credit card theft, there's no recourse available to consumers.



Back before the Internet, well, not before the Internet, early in the days of the Internet, before I started just booking my own travel, I had a travel agent.  And normally I'm just going to Northern California once a year.  And her name was Judy Supple, and so I'd give her a phone call every year like around Thanksgiving, and she'd say, "Oh, Steve, same credit card?"  And I'd say, "No, I've lost that last one, too, so here's my new number."



The point was that this predated PayPal.  Security was worse, and my cards were being compromised constantly because I was out there on the front lines buying things.  But we didn't have Amazon to aggregate so many purchases.  We didn't have PayPal to aggregate so many purchases.  So much as I was trying not to spread my card number around, it got out.  And of course I'm eating in restaurants, too, so I never really knew, I was never able to figure out what the leakage point was.  But the point is that, and this is the case for credit card, but not for debit card, consumers are not liable for any charges.  So the only thing...



LEO:  That's true for debit now, too, by the way.



STEVE:  Oh, it is.  Oh, good.  Good, good, good.  So scrutinize your card statement...



LEO:  In the U.S.



STEVE:  Ah.



LEO:  We have international listeners, so in your country check the rules.



STEVE:  Good.



LEO:  Yeah.



STEVE:  So in the U.S., thank you.  What you need to do is, if you purchased from a Chipotle restaurant between the dates of March 24th and April 18th, your credit card information may have escaped.  And the problem is the other thing we've seen is that there is normally a multi-month delay before your information starts getting used.  They're aggregated; they're chopped up; they're processed; they're put on the dark web; they're bundled.  And it's not clear why it takes so long.  But experience shows that it does.



So unfortunately, if you did have a credit card transaction at Chipotle between March 24th and April 18th, your credit card information may be out there.  And so unfortunately there's no end date except the expiration of that card.  Just pay a little extra attention to your credit card statements.  And if you find something that you know you didn't purchase, which is what I used to do when I would - and normally the fraud detection stuff picks that up.  I was getting - I think they had me on speed dial back in the day because it was like, have you purchased a - I don't remember even now what - there were some really funny things.  It's like, uh, no, I'm not in Germany.  I've been in California the whole time.  Oh, well, maybe that's not you.  No, I'm sure it's not me, and please take it off of my card.



So anyway, so it's up to us to defend ourselves.  And unfortunately that's the state of the world.  And again, there isn't any point at which you're safe until the card that you use naturally expires, which at least changes the CVV code and then makes it much more difficult to use, presuming that the CVV was also captured as part of the purchase.  Which is not supposed to be, but sometimes it is.



From our "we should have seen this coming" department, even though WannaCry, that we've been discussing the last two weeks and of course was a worldwide phenomenon, 300,000-plus Windows machines compromised globally, entirely a Windows problem.  The recent high-profile worldwide attack has, believe it or not, spawned a bunch of fake Android WannaCry protection apps.  Which are completely unrelated in any way.  But it's in the news, and there's, you know, Android users who don't listen to this podcast are like, oh, maybe I need that.  I don't want to get that horrible thing I've been reading on the news and people are talking about on my phone.  So they're being downloaded.



The good news is they're not particularly malicious.  Predominantly they're adware leveraging the current WannaCry hysteria to get themselves installed on people's Android devices, where they then attempt to get the user to install additional unnecessary junk ware.  So anyway, just a heads-up.  It probably wouldn't catch any of our listeners, but our listeners have friends.  And you don't need anti-WannaCry protection for your Android phone.  There are some things you need for your Android phone we'll get to here in a second, but not WannaCry.



Under the topic of "here's one no one saw coming" is the Attack of the Subtitles.  Check Point Research, that we talk about every so often because they're a great security research team, did a blog posting a few days ago titled "Hacked in Translation:  From Subtitles to Complete Takeover."  And once again, it's been a theme for this podcast for the last couple years, once we saw this sort of - we were able to generalize a set of problems that kept recurring.  And that is how difficult it is for interpreters not to have mistakes in them.  Another interpreter is the processing of subtitle files for videos.



"Researchers at Check Point have discovered a new attack vector which threatens millions of users worldwide:  attack by subtitle.  By crafting malicious subtitle files, which are then downloaded by a victim's media player, attackers can take complete control over any type of device via vulnerabilities found in many popular streaming platforms."  And they looked closely at some of the biggest ones.  The biggest of all is VLC, that I use, and I imagine lots of our savvy Windows and Linux and Mac people, it's multiplatform, stands for VideoLAN, VLC. There's also Kodi (XBMC), Popcorn Time, and Stremio, or Strem.io.



Check Point writes in their coverage of this that there are approximately 200 million video players and streamers that currently run the vulnerable software, making this one of the most widespread, easily accessed, and zero-resistance vulnerabilities reported in recent years.  They've tested and found multiple serious subtitle-parsing vulnerabilities in every video player they've examined, starting with the four most prominent players that I mentioned.  They said that they have reason to believe similar vulnerabilities exist in other media players.  They did follow responsible disclosure practices and reported all vulnerabilities that they found and demonstrated exploits to the developers of the vulnerable media players.  Some of the issues were already fixed at the time that they reported them, while others were then investigated and fixed.



So VLAN is now at 2.2.6, and I just checked my version.  I was at 2.2.4.  So I and 170 million others are using VLAN.  So if anyone's got it, you just want to make sure you're up to 2.2.6.  So it's been updated.  Essentially all four of them have.  And I have the links to the latest versions in the show notes, if anyone is interested, or just I'm sure you can launch the player and have it check for updates and update yourself.  But this is an emerging threat vector.  The problem is we're protecting ourselves by knowing this, but most of the world won't know about this.  So hopefully these various things have auto update which is engaged and enabled and will work to protect their users.  I don't remember whether...



LEO:  Yeah, it auto updates, yeah.



STEVE:  VLAN does?  Okay, good.



LEO:  VLC, yeah.



STEVE:  VLC.  I tried bringing it up and looked at the About.  I didn't check to see if it would update itself.  And maybe it's just taking some time to get around.



LEO:  It does it all the time.  What it'll say is "I have an update.  Do you want to update?"  And you say yes.



STEVE:  Good.



LEO:  Please.  Now.  But, yeah, you might even want to manually do it if it doesn't offer you that.



STEVE:  Yeah.  I would say it's definitely worth doing.



LEO:  It's interesting that all these players have this problem.  So it must be a common DLL or something; right?



STEVE:  In the case of VLC, it's a plugin.  And I can't remember what the name of it was.



LEO:  Is it a codec or...



STEVE:  Well, they may have used common code.  So you're right.  What's probably the case is there was an open source, publicly available implementation, and they all grabbed that off of GitHub and then repackaged it for their own needs and with hooks for their own particular player.  And so as a consequence there is some commonality there.  Again, it's an interpreter, and it's just not hard to, as we keep seeing, to find mistakes in interpreters because, as is always the case, as soon as they get it working they're like, okay, it works.  But you never think of it as an entry point, as a threat vector foothold.



So the team that we've talked about before of researchers, both from University of California at Santa Barbara and the Georgia Institute of Technology, are back with a new and worrisome finding.  They call it Cloak & Dagger, and they've got a site, hyphenated, cloak-and-dagger.org.  Not dot com, dot org.  Cloak-and-dagger, with hyphens, dot org.  It's a very nice page that will explain what's going on.  And they're the people who will be showing this at this coming - in two months at the July 2017 Black Hat in Vegas.  They just won a Distinguished Practical Paper Award from the proceedings of the IEEE Symposium on Security and Privacy in San Jose earlier this month.



So, but what they found is worrisome.  From their disclosure, they said:  "Cloak & Dagger is a new class of potential attacks affecting Android devices."  And this is where I mentioned at the top of the show that they're having problems getting Google to understand, apparently, what's going on, and we'll walk through in a second the responsible disclosure back and forth.  If nothing else, these guys are really patient.  It's too bad Google's not putting themselves on their own Project Zero timeout because they'd be in trouble if they were.



They wrote:  "These attacks" - get this - "allow a malicious app to completely control the UI feedback loop and take over the device without giving the user a chance to notice the malicious activity.  These attacks only require two permissions that, in case the app is installed from the Play Store, the user does not need to explicitly grant and for which they are not even notified."  They wrote:  "Our user study indicates these attacks are practical.  These attacks affect all recent versions of Android - including the latest version, Android 7.1.2 - and they are yet to be fixed."



So under "Main Takeaways" they said:  "We uncover a series of vulnerabilities and design shortcomings affecting the Android  UI.  These attacks abuse one or both of the" - and there's two, there's a SYSTEM_ALERT_WINDOW, which is also known as "draw on top," meaning it's a privilege that allows something to pop up over the existing application.  And so what this does it is allows the underlying UI to be obscured, and these guys are able to get up to all kinds of mischief with that power.  And the second one is the BIND_ACCESSIBILITY_SERVICE, known as "a11y."



So they wrote:  "If the malicious app is installed from the Play Store, the user is not notified about the permissions, and they do not need to explicitly grant them for the attacks to succeed.  In fact, in this scenario the 'draw on top' privilege is automatically granted, and this permission is enough to lure the user into unknowingly enable [that second permission, the] a11y through clickjacking."  They write:  "The possible attacks include advanced clickjacking, unconstrained keystroke recording, stealthy phishing, the silent installation of a god mode app with all permissions enabled, and silent phone unlocking and arbitrary actions while keeping the screen off."  And then they have a full list of things below.



They wrote:  "These attacks are practical.  We performed a user study with 20 human subjects, and no user understood what had happened to them.  Most of the attacks are due to design issues."  And this is where they're having a problem with Google because it's like, that's not a bug, that's a feature.  Okay.  "And they are thus challenging to prevent," they write.  "In fact, one may say that some of these functionality work 'as intended.'  Nonetheless, this work shows that this functionality can be abused."  And, as they wrote:  "To date, all these attacks are still practical."



So I'm going to skip over some enumeration of the nature of the attacks because they just expand on what I already wrote.  But under their responsible disclosure timeline, which they were careful to share because it failed with Google, and they wanted the world to understand, look, we've been trying for more than a year.  They started last August and they're going to go public with this in July.  So it will have been a year that they've been trying to get Google's Android security team to understand this.



On August 22nd they opened several issues on the bug tracker for the Android Open Source Project.  On the 31st of August, so nine days later, the Android security team set the severity as moderate for one of the bugs, that is, the "draw on top" with unconstrained keystroke recording.  Then a month later, on September 30th, the Android security team marked one of the reported bugs, the other one, the a11y, where you could do unrestrained keyboard recording plus leaking a security PIN plus unlocking the device while keeping the screen off as "works as intended."  Yes.  That's a feature, not a bug.



Then two weeks later, on October 10th, these researchers followed up, pointing out that the accessibility services documentation states that this a11y should not be able to access passwords, and that a11y should not be able to unlock the device and perform arbitrary actions while keeping the screen off.  Yet it does.  Three days later, that is, October 13th, Android security team states that:  "The password will be repeated if the user explicitly turns that on in settings under Settings > Accessibility > Speak Passwords.  The option is off by default.  If the user explicitly enables this feature, it is not a security vulnerability."  So again they're saying, well, we don't think that's a vulnerability.



So the next day these researchers follow up by clarifying that their attack works even without this feature.  And they wrote:  "In fact, our report does not even mention it."  Four days later, October 18th, the Android security team marks this bug as "high severity."  Okay, it sounds good.  Making progress.  November 28th, a little over a month later, Android security team downgrades this bug as "not a security issue" and marks it as "Won't Fix (Intended Behavior)" because "limiting those services would render the device unusable."



Not quite a month later, December 19th, a draft of their IEEE paper that I refer to, which won their award, is shared with Adrian Ludwig, the director of Android Security.  Months go by.  March 15th of this year they follow up again, pointing out that the documentation states otherwise.  They say:  "We also follow up on all the other bugs we opened, asking for a status update."  Now we're up to the beginning of this month, May 3rd.  "Again, nothing comes back.  We follow up a second time, asking for a status update for the bugs we reported.  The next day, on May 4th, Android security team keeps our a11y findings as 'won't fix,' but they state they will update the documentation."  That's right, so don't fix the problem, change the documentation so that it agrees with the problem.  They wrote:  "We did not receive updates about any of the other bugs we reported."



Four days later, May 8th:  "We have a telephone conversation with the antimalware and a11y Google teams, during which we thoroughly discussed all the details of our research."  On the 19th of May, the a11y team confirms the a11y-related issues we reported as "won't fix."  Okay.  Three days later, May 22nd, and this is the last entry:  "This website and our research paper at IEEE S&P are made public."  So where we are today, every attack discussed in this work is practical, even with the latest version of Android.



So they then, in order to help better explain this, they have in their presentation some Q&A.  Let's see:  How can an app stealthily obtain two permissions?  They explain that.  Why is the "draw on top" permission automatically granted?  And they say, for example:  "The behavior appears to be a deliberate decision by Google and not an oversight.  To the best of our understanding, Google's rationale behind this decision is that an explicit security prompt would interfere too much with the user experience, especially because it is requested by apps used by hundreds of millions of users.  For example, Facebook requires this permission to implement the 'Android chat heads,' one of its very popular features."



LEO:  Not with me.  I could easily give that one up.



STEVE:  Ugh.  So, yeah, so in their Q&A:  Are these permissions shown to the user after the app is installed?  No.  How difficult is it to get the app with those two permissions approved?  Turns out they're often approved by default.  They said:  "A quick experiment shows that it is trivial to get such an app accepted on Google Play Store."  And this is a year after they told Google, or nearly a year.  They said:  "In particular, we submitted an app requiring both of these permissions and containing a non-obfuscated functionality to download and execute arbitrary code, attempting to simulate a clearly malicious behavior.  This app got approved after just a few hours, and it is still available on the Google Play Store."



Then they say in their Q&A:  What do you recommend to users?  So they say:  "We recommend users to check which applications have access to the 'draw on top' and the 'a11y' permissions.  Unfortunately, both permissions are considered 'special' and, for this reason, certain versions of Android may show 'no permission required' even if, in fact, the app has access to both the permissions required."  So even the UI in this case is defective in that, if somebody, if a responsible Android user wants to take responsibility, you can't find out.



They said in their paper:  "We provide instructions for several versions of Android."  Oh, and I should mention that this is just the top-level coverage.  In the link that I have in the show notes is a PDF for their whole paper, where they go into as much detail as anyone could want.  And they said:  "This work shows that the user should not consider their device's UI as a trusted source" - this is so sad.  I'll start again.  "This work shows that the user should not consider their device's UI," that is, their Android device's UI, "as a trusted source of information.  Thus, from a conceptual point of view, the user should rely on other means than the device's UI itself."  Okay, what?



LEO:  Geez, what?  Yeah, what's the other choice?



STEVE:  I know, I know.  "An alternative solution," they wrote, "is to use" - get this - "command line tools" - oh, yeah, that'll go over big - "such as adb, or to determine the permissions requested by each app through the Play Store website.  For example, to check the permissions" - get this - "of the official LastPass app, which requires both permissions, you can go to its Play Store page, scroll down, and click View Details under Permissions.  The 'draw on top' permission will appear under the Others > Draw over other apps label, while the 'a11y' will appear under Others > Bind to an accessibility service."



So none of this is easy.  Our non-tech-savvy people aren't going to get this, and they're going to be caught out.  And now this is all public.  I mean, this is no, you know, they kept this quiet until a week and a half ago.  Now it's all public.  Google is accepting apps that do this.  And they'll be showing this at Black Hat, and Google is apparently saying, no, this is what we want.



So finally, in the Q&A:  Why are these bugs not fixed yet?  And they wrote:  "Some of the issues uncovered by this work are design-related issues, not simple bugs, and thus it necessarily takes more time to fix them.  On the other hand, Google is saying 'won't fix.'  Moreover, these are not classic," they wrote, "low-level issues, but UI-related problems.  These issues may be more challenging to fix since changes may introduce backward compatibility issues."  Well, for example, apps like LastPass are...



LEO:  And Facebook, yeah.



STEVE:  Exactly, depending upon this behavior which now these guys have shown can be hugely abused.  And there's very little control over, like none right now, over which apps are able to acquire those permissions when they're submitted to the Play Store.



LEO:  I'm going to guess this is an accessibility issue, and that if they took these features out, that would reduce accessibility.



STEVE:  Certainly the second one is, although...



LEO:  Yeah, "draw on top" does, for sure, yeah.



STEVE:  Well, no.  The other one is explicitly an accessibility issue.  But the "draw on top" looks like it's an ability that allows pop-ups, like from Facebook, to pop up in order to prompt you for, like...



LEO:  That's a big vulnerability because you can clickjack; right?



STEVE:  Exactly.  So they said:  "These issues may be more challenging to fix since changes may introduce backward-compatibility issues and changes that are visible to end users," meaning a loss of existing functionality.  They said:  "Finally, some of the bugs were marked as 'won't fix,' and thus they will not be fixed."



And then, finally, the last question:  Are these attacks practical?  And they said:  "We believe so.  We performed a user study with 20 human subjects, and none detected to be under attack.  We report more details in our paper."  So let's just hope that the attention this is going to get will force some action from Google.  This sounds like a powerful feature that Android has to allow one app to present something to a user on top of another.  But these people demonstrated that this feature can be abused.  And, I mean, what we're going to now see, and I imagine we'll be discussing this in the coming months, is a bunch of actual exploits of this which will probably drive Google to take some sort of measure.  I don't know what it'll be.



LEO:  You'd have to - a website couldn't do this.  You'd have to install a malicious app; right?



STEVE:  I don't want to...



LEO:  You'd have to give the browser those two permissions that draw.



STEVE:  As I understand, though, in Android browser apps have a lot of functionality available to them.  So I'm reluctant to say that you couldn't just surf to somewhere and have that happen. 



LEO:  See, that would be a real problem.  If it's just an app, that's a little bit of a lesser problem.  You just tell people, you know, only install mainstream apps.



STEVE:  Yeah, the problem is, I mean, maybe - we were just talking about the need to audit features and permissions in apps in Android.  But what this says is that, even though this is something that apps can ask for, those are not features which are necessarily surfaced to any UI.



LEO:  Right.



STEVE:  And also there's this problem, sort of, of the chicken and egg.  That is, what they're saying is you can no longer trust what the UI shows you, meaning that a malicious app, once it gets in, could cover up your screen with whatever it wants in order to get up to mischief.



LEO:  LastPass, for instance, requires you go into accessibility settings, at least for that accessibility-based one, and give it explicit permission.  In other words, it's not a pop-up in the traditional permissions pop-up.  You have to actually go into - if you want "draw on top," you actually have to go into accessibility permissions and say, "I want this capability." 



STEVE:  And you know if you did turn it on, because what they're saying is...



LEO:  Well, I always turn it on.



STEVE:  ...it was turned - okay.  So they're saying that the apps, when downloaded, can ask for and implicitly have both of those turned on, and the user is never asked.



LEO:  Yeah, see I'm not sure because I do know that you don't get this "draw on top" stuff without going into accessibility.  And it does explicitly ask you, LastPass does, okay, here's what you're going to have to do to give us - I think this is the autofill capability that it wants.  And it says you have to do this if you want this.



STEVE:  What they're saying, for example, is that people who downloaded the Facebook app were never...



LEO:  No, no.  You aren't in Facebook.  Yeah, that's right.  But that's the chat heads.  That's "draw on top."



STEVE:  The "draw on top," right.



LEO:  I don't know if you need accessibility permissions in Facebook or not to get full functionality.  Yeah, Facebook doesn't ask you for a whole lot of interactivity because they can't.



STEVE:  Exactly.  We can understand why they wouldn't.  And it's just like, what?  I don't know how to answer this question.  



LEO:  Yeah, right.



STEVE:  Yeah, so maybe what the least we need is an auditing tool to carefully look at which, you know, to understand the risk and only allow trusted apps like the Facebook app, like LastPass, things where you could say these apps have a reason for needing this, rather than just everybody who asks for it gets it, which is the way it is now, and then can get up to some mischief.



So believe it or not, following on the heels of WannaCry is SambaCry.



LEO:  It was just a matter of time.



STEVE:  And, my friend, it is really bad.



LEO:  SambaCry.



STEVE:  SambaCry.  The Hacker News headline was "7-Year-Old Samba Flaw Lets Hackers Access Thousands" - and we're talking more than 24,000 - "Linux PCs Remotely."  Bleeping Computer's headline was - oh, wait, they had a bigger number.  I might be thinking - the 24,000 might have been something else because BleepingComputer wrote:  "Over 104,000 Samba installations vulnerable to remote takeover attacks."



Okay.  So what's going on?  Samba has a problem.  Their own security advisory, published last Wednesday, wrote:  "All versions of Samba from 3.5.0 on."  Okay, 3.5.0 was released since March 1st, 2010, so seven years ago.  So for more than seven years Samba is vulnerable to a remote code execution vulnerability - with some conditions, but unfortunately they're not very restrictive - "which allows a malicious client to upload a shared library to a writable share, then cause the server to load and execute it."  So that means, if you have a publicly exposed writeable share, what this means is that somebody finds it.  And of course it's easy to find because you scan for port 445.  And, oh, look.  If it's not Windows, it's Linux.



So if it's Windows, you use WannaCry.  If it's Linux, you use SambaCry.  Since the SMBD, that's the Server Message Block Daemon, or the Samba Daemon, typically runs at root, the uploaded and executed module runs in the SMBD context with the same full root privilege.  So what this amounts to is the ability for anyone that can write to a Samba share the ability to install, essentially upload their own loadable module into that directory and then make the Samba server run it as root.  A Shodan query of port:445 and then bang, meaning not, !os:windows shows approximately one million non-Windows hosts that have TCP port 445 open to the Internet, more than half of which exist in the UAE, the United Arab Emirates, that's 36%...



LEO:  Half of them?



STEVE:  ...more than half of those.



LEO:  That's weird.



STEVE:  And the U.S. has 16%.  However, it isn't clear how many of them are running vulnerable versions.  On the other hand, for the last seven years they've all been vulnerable.  The vulnerability be exploited with a single line of code.  A malicious client can upload and cause the SMB server to execute, as I mentioned, a shared library from a writeable share.  And exploit modules are already available in Metasploit to exploit this issue on Linux for ARM, x86, and x86 64-bit architectures.  Now, this bug has nothing to do with EternalBlue.  Whereas Eternal Blue, of course, on Windows was a buffer overflow exploit, the Linux SMB vulnerability leverages an arbitrary shared library load and execute.



So they're different beasts, but this is, I mean, I would argue even worse and more powerful.  And note it is totally wormable.  This requires no user intervention or interaction.  If your system gets scanned, this can be uploaded to it and executed.  And if it's a worm, then it starts scanning and takes off.  So the non-vulnerable versions have just been released, which is why this went public.  So 4.4.14 is not vulnerable; 4.5.10, not vulnerable; and 4.6.4, not vulnerable.  But these have only been released in the last week.  And as the security firm F5 put it, I liked their simple equation:  network + remote code execution + root = drop what you're doing and patch.  Meaning, really.



Okay.  So patches exist.  Debian, Ubuntu, CentOS and Red Hat have all taken immediate action to protect their users and have released patches to their supported versions.  And there is a security workaround available for people who cannot patch.  If you add - and I've got all of the details in the notes here.  If you add the parameter "nt pipe support = no" to the global section of the smb.conf [config] file and restart the SMB daemon, that'll shut this vulnerability down, if for some reason you are unable to patch.  But everybody who references this notes that it may disable some functionality for Windows clients.



On the other hand, I mean, obviously immediately updating your version, or maybe just taking it offline if you don't really need it is necessary.  And of course NAS vendors, Network Attached Storage vendors, have work to do because different brands and models that use Samba for filesharing - a lot if not all of them provide this functionality - will have to issue firmware updates to patch this flaw.  If the firmware updates for these appliances take the same time as they usually do, we'll have this bug around for quite some time.  And we know that these things, you know, there are lots of servers in the closet.  And Shodan says the attack surface on the public Internet is huge.



So this has just happened.  I imagine that next week we'll be talking about the SambaCry worm having gone, unfortunately, "viral" is the only term we have for it.  But, yeah, being actually active and probably fighting over Linux machines that are available publicly.  So, yikes.  You don't want to have a publicly writeable Samba share.  And maybe you think, oh, you know, it's safe because people can deposit things publicly, but we'll scrutinize them before we do anything with them.  No.  People can deposit an executable module and run it without any permission as root, if you have something that's from seven years old and haven't just fixed it.



So certainly all of our listeners who - and remember, the public exposure is only one aspect.  The Intranet side is equally bad because, just as WannaCry spread publicly, but also looked for internally, which is of course what brought down the whole U.K. medical system was the Intranet got scanned and taken down by that cryptomalware.  So, I mean, maybe the same thing will happen.  We'll be talking about it a week from now.  Looks really bad.



And here, I knew that there was 24,000 of something somewhere.  The NSA's Windows EsteemAudit exploit, which uses RDP - that's the Remote Desktop Protocol.  That's the whole, you know, it's used for getting the desktop remotely.  It is still unpatched, not surprisingly, for Windows XP and Server 2003.  So here we have a situation where this is another bad problem which isn't patched because that operating system of that era, the XP/Server 2003, are no longer being maintained.  Because the WannaCry was such a problem, as we know, Microsoft issued an emergency back patch for those.



Well, what Shodan shows is 24,734 presently vulnerable WinXP and Server 2003 systems on the public Internet.  And they're not going to get patched, I mean ever, unless Microsoft decides, gee, maybe we should fix this one, too.  So EsteemAudit is another very dangerous NSA-developed Windows hacking tool which was leaked as part of what the Shadow Brokers were doing.  It targets the remote desktop protocol, which runs a service at the famous port 3389 on Microsoft systems.  And in this case it's Windows XP and Server 2003 that have always been vulnerable and will probably always be vulnerable.  It can be weaponized into wormable malware.  And again, I think we're going to see that in the same way that WannaCry ransomware was, as we all experienced, turned into a worm and generated lots of headlines.



So the good news is, for people who are staying with, for whatever reason, WinXP and Server 2003 and have a profile that would make them exposed, I mean, you're crazy to have port 3389 exposed to the public Internet.  But, I mean, there is a logon username and password.  I would never trust that, but some people are going to.  There is an unofficial patch which EnSilo Security has published.  I have the link to the patch in the show notes.



They write:  "The patch for WinXP and Server 2003 supports silent installation and does not require a reboot.  This helps users avoid the required downtime typically associated with patch installations.  Upon patching, any attempt to use an EsteemAudit exploit to infect a patched machine will fail."  So again, Microsoft hasn't fixed it, probably won't.  But if any of our listeners or firms have XP and Server 2003 with 3389 exposed, I would say either - certainly publicly, but even on the Intranet, I mean, if you don't need it, then turn it off.  If you do need it, I would argue the only recourse you have at the moment is to use this unofficial patch which I have every reason to trust.  I mean, EnSilo is a well-known, trusted firm, and they've done this for all the right reasons.  So I would say take action.



LEO:  Steve Gibson, continuing on.  He's fully caffeinated now.



STEVE:  I am.



LEO:  You're like that inflatable pilot in "Airplane!."  We've just pumped you back up, and you're fully inflated.



STEVE:  Although they're not a sponsor on this podcast, I did immediately purchase the Oars and Alps...



LEO:  Did you?  You're going to like it.



STEVE:  ...face charcoal.  It just looks too freaky.  I have to see what that's about.



LEO:  It actually is great.  It really - my skin is really soft.  Well, that's another ad for another time.



STEVE:  And I used the TWiT code, and I just did want to mention that they support both PayPal and Amazon Pay.



LEO:  Oh, nice.  I love it.  I love that.



STEVE:  Along the lines of not spreading your credit card any further than you absolutely must.  I always look for that option.



LEO:  Yeah, me, too, yeah.



STEVE:  So bravo.  Speaking of bravo, I have a new acronym, TDF.  Stands for Traveler Data Frisking.



LEO:  I'm going to be going through some of that, some intrusive travel data frisking, I think, maybe.



STEVE:  I know.  Being frisked for your data is something that seems to be on the rise.  And so many of our listeners have asked, over the last couple weeks, since it was announced by AgileBits, what do I think about 1Password's Travel Mode?  And my answer is they did it exactly right.



LEO:  Oh, that's good to know.  That's great.



STEVE:  Yes, 100%.  Your various secrets are - so the way this works is you first partition them into vaults.  And the vaults can be individually flagged as "Safe for Travel" or not.  So as you approach customs or a particularly officious-looking TSA agent, you log into your 1Password.com account and activate Travel Mode, whereupon all of the non-safe for travel vaults are completely removed.  They are wiped and removed from your connected devices.  They're not obscured or hidden.  They are gone.  So then you present your device, and if they need to get a password from you, you go, okay, yeah, here's what I've got.  Whatever you need to do not to be harassed.  You travel through customs smiling and maybe trying not to bow too deep.  And if you are data frisked, the duly anointed officials will see only those secrets you previously decided to allow them to see.  Nothing more to see here.  Move along.



Once you're safely through inspection, you simply log back into your 1Password.com account, turn off travel mode.  Maybe you want to wait till you get to the hotel or whatever, depending.  Maybe, I mean, and maybe, in fact, you just - if these are things you don't need while you're traveling, then it's also just better for security to not have them on your devices, not only to deal with the officious-looking TSA agents, but just in general not to have them while you're traveling.



But if you need them, the point is you turn off Travel Mode, and all of those vaults are immediately repopulated on your devices and reappear, just as they were before.  So I dug in.  I read Rick Fillion's explanation of this.  He posted on AgileBits' blog under "Introducing Travel Mode:  Protect Your Data When Crossing Borders."  And I was very impressed with everything I saw.  So double thumbs up.  These guys did it exactly right.  And it looks to me like a great feature.  So now we need everybody else to copy them.



LEO:  Yeah.  LastPass.



STEVE:  Hello.  Listening?  Yeah.  Okay.  So this one I was immediately put in mind of Hudson from the second "Alien" movie, "Aliens," when he said, "Stop your grinnin' and drop your linen."  And I have to say I was put in mind of it because I rewatched all of the Alien movies in preparation for seeing "Covenant," which I did see on opening day.  We'll be talking about that in a minute - without any spoilers, of course - in our Miscellany.  But the reason is this was just a headshaker.  Google now knows, believe it or not, when its users go to the physical store and buy stuff.  Not online, but brick and mortar.



The Washington Post, as we well know, those of us who've been following politics, has been very busy the last few weeks.  In this case, they had some interesting coverage from Google.  They wrote:  "Google has begun using billions of credit card transaction records to prove that its online ads are prompting people to make purchases - even when they happen offline in brick-and-mortar stores, the company revealed" a week ago, which was a week ago Tuesday.  "The advance allows Google to determine how many sales have been generated by digital ad campaigns, a goal that industry insiders have long described as 'the holy grail' of online advertising.  But the announcement also renewed longstanding privacy complaints about how the company uses personal information.



"To power its multibillion-dollar advertising juggernaut," writes the Washington Post, "Google already analyzes users' web browsing, search history, and geographic locations, using data from popular Google-owned apps like YouTube, Gmail, Google Maps, and Google Play Store.  All that information is aggregated and tied to the real identities of users when they log into Google's services.



"Now, the new credit card data enables Google to connect these digital trails to the real-world [consequences, essentially the] purchase records in a far more extensive way than was possible before.  And in doing so, Google is again treading in territory that consumers may consider a little too intimate and potentially sensitive.  Privacy advocates said few people understand that their purchases are being analyzed in this way and might feel uneasy, despite assurances from Google that it has taken steps to protect the personal information of its users."  And again, I'm not saying anyone should be concerned.  I'm just saying we should have the knowledge that this is going on.  I just find it amazing and interesting.



"Google also declined to detail how the new system works exactly or what companies are analyzing records of credit and debit cards on Google's behalf," because they subcontracted this.  Google, the Washington Post reports, which saw just shy of $80 billion, "$79 billion in revenue last year, said it would not handle the records directly, but that its undisclosed partner companies had access to 70 percent of all transactions for credit and debit cards in the United States."  I imagine they're trying to get that last 30 at the moment.



"Marc Rotenberg, the executive director for the Electronic Privacy Information Center, was quoted in The Washington Post's story, saying:  'What's really fascinating to me is that'" - and not surprising, actually - "'is that, as companies become increasingly intrusive in terms of their data collection, they also become more secretive.'"  Yeah, no surprise.  "He has urged government regulators and Congress to demand answers about how Google and other technology companies are collecting and using data from their users."



Now, Google did say that it took pains to protect its users' privacy.  They said:  "While we developed the concept for this product years ago, it required years of effort to develop a solution that could meet our stringent user privacy requirements."  Okay.  So that all sounds good.  They said:  "To accomplish this" - and I'm not sure about this part - "we developed a new, custom encryption technology" - what? - "that ensures users' data remains private, secure, and anonymous."



Okay.  So this was probably written for a non-techie audience.  I would imagine what they actually used is well-known proven security primitives and designed some system that provided required anonymity and somehow kept the partners they're working with from knowing anything that they didn't need to in order to get this done.  So it would be interesting to find out a little more about this.  Maybe we will in the future.  But I just thought it was interesting that there is such a clear monetary incentive behind being able to prove the value of the ads that Google serves, that it's no longer enough to use, oh, look, the user clicked on the ad, or they clicked on the link, or they went to the site.



This has now been extended to the presumption that users were consciously or subconsciously influenced by the presentation of the ad such that later, when they were strolling the aisles in the supermarket, they purchased something that had been advertised.  And notice it's not, I mean, in order to tie this to an item, you need an itemized receipt for what was purchased.  Maybe only where, but clearly Google would like to know what, as well, in order to tie it to an ad.  So, really interesting.



I did want to mention that the FCC's public feedback system for their request for public comments on the proposed rollback of Net Neutrality is once again up and running and open to subjecting itself to spam and attacks and lots of belligerent users who feel strongly about this.  So GoFCCYourself.com once again successfully redirects to the submission page buried at the FCC, for any of our listeners who do wish to make themselves and their feelings heard.



Also I got a nice email from Clint Wilson, who is the DigiCert product manager that I've dealt with in the past when I've needed special favors, like I've talked about how I needed to, like, I needed a certificate with a midnight expiration that still understood - that had an SHA-1 signature, yet I needed the companion signature that expired on the normal time later, but that was SHA-256.  And so DigiCert was able to provide.



Well, we discussed last week how Stack Exchange used DigiCert to create that amazing certificate.  And there were some points that I was confused about, only because I just, in assembling the show notes, I didn't have a chance to dig into this on the fly.  I asked you at the time, Leo, to go check to see whether it was an EV certificate, and it wasn't.  Which made sense to me because EV certs cannot have wildcards.  I mean, I dearly wish they could because in that case I would have EV wildcard certs, which would make my life much easier.  But I didn't understand why then the EV root was used to sign a non-EV cert.



Anyway, Clint explained it all to me.  I got his full description in the show notes.  I won't drag everyone through it.  But essentially it boiled down to the fact that, again, demonstrating the kind of attention that they give to this - oh, he did say, I mean, there's lots of detail here which shows how much attention certificates can get from people who really understand them.  He said:  "Stack Exchange uses OV certificates" - that's the organization validation, not the domain validation - "because they need to use wildcard names in order to support non-SNI clients," so *.something.com, for example, "and relying third-party software.  EV certs" - and this is an exception I didn't know about - "are not allowed to include wildcard DNS names except" - get this - "for Tor Hidden Services .onion addresses," which can be wildcard and EV.  So wow, Clint, thank you for the clarification.



And he said:  "Stack Exchange's main certificate includes multiple wildcard names," as we saw last week, "to further support non-SNI clients," he says, "and to simplify some of the deployment requirements.  This is something I'd love to say is unique to DigiCert, but it has become relatively readily available from most commercial CAs in recent years," and he says, "though I believe we [meaning DigiCert] were among the first to offer these, notably to Wikipedia, Facebook, and other similar organizations."



And then, finally, this confusion about why it was an EV signature.  He goes into some detail here.  Essentially they have two roots.  They have the DigiCert SHA-2 High Assurance Server CA, and the High Assurance EV Root CA.  It turns out that really older software has almost vanishingly better support for the EV cert.  And he said down in the hundredths of a percent better, but nonetheless a little bit better.  And so they deliberately chose to sign the Stack Exchange cert, the single Stack Exchange mega wildcard crazy certificate with all of the additional domains with the absolutely highest coverage root, just so that they would have the least amount of problems.  So Clint, thank you for providing the clarification.  Much appreciated.  And it answered some mysteries.



Okay.  A bit of miscellany.  First I got a kick out of this.  Leo, you were talking - this was sort of a follow-up from your talking about the growing popularity of the Security Now! podcast last week.  Scott Foger, I guess is how you pronounce his name, his Twitter handle is @fogrito, so maybe it's Foger.  He said:  "My eight-year-old son..."



LEO:  You're getting them young now.



STEVE:  Eight years old - "...is listening to Security Now! podcast while swinging at the playground."



LEO:  Oh, wow.



STEVE:  He said:  "@SGgrc is his favorite.  Got to start them young."



LEO:  Well, get him a credit card so we can follow all his transactions from now on.



STEVE:  I did see "Alien Covenant" on its opening day.  And, you know, I like the Alien series a lot, the whole franchise.  I rewatched "Prometheus" just because it's the sequel to "Prometheus," and both of those predate, in terms of the actual Alien franchise timeline, the first movie, that of course was where we were introduced to the concept.  And then James Cameron's sequel, "Aliens," was arguably the best of the bunch, still just outstanding.  So I was not wowed by it.  But it was good.  So there's that.



I did want to mention "Twin Peaks," Leo.  Just last Sunday, day before yesterday night, was the second week.  And I don't know if they're going to continue doing two hours per week, but the first week, two weeks ago, was two hours, and then two days ago was another two.  And it is David Lynch's - like David Lynch on overdrive.  It may be a little too weird for most people.  I mean, "Twin Peaks" was weird already.  And in fact after the first two hours I tweeted my favorite line from that first installment, which was the Chief of Police, who is the same guy we originally had in the original "Twin Peaks" series. over the phone to the Log Lady, saying, "Once again, I find myself in complete agreement with your log."  Which is really not a statement.



LEO:  I don't even - I do not get the appeal of this.  But I'll grant it to you, yeah.



STEVE:  Anyway, it's quite bizarre.  But, yeah.  Let's see.  Where are we?  So I'm going to skip this one just because we don't have enough time.  Oh.  I keep promising that I'm going to stop talking about the Frontier Saga, but I feel like I need to do our listeners justice.  I am flooded by people whose life has been disrupted.  When I look at the comments in the mailbag, and it's "Darn you Gibson," and "Oh, my god, what happened to my life?" and such things.



Jason White tweeted:  "Thank you."  Well, that's nice.  He's not upset with me.  He said:  "The Frontier Saga is everything I could hope for in a sci-fi story - outstanding action, great characters, intriguing technology.  Oh and can I say again, action.  The primary battle in Book 3," he wrote, "is absolutely amazing.  I have an hour and a half commute several times a week, and this series is second on my list after Security Now!. Thanks for the recommendation."



Steve Doyle tweeted:  "Hey Steve, I just had to say thanks.  I absolutely love the Frontier Saga.  I've started listening a week or so after you mentioned them and am about to finish Book 10.  They have consumed all my free listening time."  He says:  "Pro tip for everyone who doesn't want to spend the money to purchase all the audiobooks.  I've been able to find them at my county library."



And I did respond to him, and so I'll just add, I am currently rereading the series because I've read them all, all 19.  And then I thought, okay.  While waiting for Book 20 - which, by the way came out two nights ago.  So Book 20, the fifth book of the second series, is now available.  Now I'm just going back through because the guy's style is so good, the characterization and the development of relationships, now that I sort of know the future, I'm rereading them to know who Tug and Dumar really are, to know about Jalea, to watch Josh and Loki develop, and to watch Cameron and Jessica grow to understand who Nathan is becoming.



So I know I sound like some sort of a cult worshipper here.  But, I mean, these are just -I'm coming to the position of feeling that it's the best stuff I've ever read in my life.  That is, this series, I mean, so many of the other things we've talked about are really good.  But this stands up there as, I think, as good as any of the other stuff.  Different.  A little more of a space opera.  But, boy, I mean, the feedback from our listeners is 100%.



And speaking of 100% feedback, Jeff from Columbus, Ohio, said:  "I <3 SpinRite."  And I thought, what the heck?  And I've just - I'm a little slow on the uptake.  I guess now I get it.  That's a heart on its side, <3, which is probably a common...



LEO:  Yeah.  Been around a few years, yeah.



STEVE:  ...icon.  But I never really spent the time to parse it.  Anyway, so "I <3 SpinRite."  He said:  "Just a quick note to say how much I love SpinRite.  I bought it after listening to my first episode of Security Now!, something like a decade ago."  Yes, that's about when it would be.  "Thankfully, I have never had to use your product.  But," he said, "there is no comparison," and presumably he means to anything else, "for keeping things running at peak performance.  Recently a 2TB backup drive was running slow at the start of each backup, transferring just bytes for several minutes before eventually returning to normal speed.  I knew SpinRite would fix it," he wrote.  "Sure enough, I just backed up 40GB in half the time it took me to type this note.  Thanks again for all you do.  P.S.:  If you need a beta tester, I'm your guy."



And so when I get back - when I get SQRL wrapped - which is what I spent the whole weekend on.  I was up till 2:30 in the morning Saturday night/Sunday morning on a real roll, working on SQRL.  I had actually intended to take some time off this weekend, but I was just getting too much done with SQRL, so I didn't.  Once that's done, I will immediately return to SpinRite 6.1.  And I will be developing it as I was before and as I have SQRL, in public view.  It's a little bit off the beaten path, that is, it's in the GRC newsgroups.  But we have this amazing group that are assembled there.  And it's so useful to me to have an interactive environment where I can put code up, and people can experiment with it.



Just last week somebody had installed an off-brand firewall add-on for Windows 10 which only affected Edge, not Chrome or IE.  But as a consequence of the fact that we're testing this as we're going, we quickly found - we realized what the problem was and were able to come up with a resolution.  So the result is that when it finally emerges, it's already well tested and bulletproof.  And the point here is that, once SpinRite 6.1 is happening, there will be all of this ongoing, sort of out of sight, but available to people who care enough.  And as I have said, as I have something which is fully functional, but doesn't have all of the polish on it, I will certainly make it more widely available to all of our listeners who want to play with something from the start.



LEO:  All right.  Let's continue on.



STEVE:  Okay.  So I ran across a term, I'm sure I had seen it before, but it just caught me off guard.  And I just love this term.  So somebody sent to me, and I'm not sure why I don't have his name here, but the subject was "Not all memory leaks are important."  So it must have been in the mailbag.



LEO:  Oh, I know what that is, an article.  It was in, like, the Hacker News or something, yeah.



STEVE:  Well, this one was anonymous, and so that's why I didn't have his name.  He said:  "Many years ago I worked on an SS7 surveillance system."  And of course we know SS7 is the international phone system glue.  "Many years ago I worked on an SS7 surveillance system," he says, "an equipment alarm monitoring app.  After many of our Unix machines got updated, they started rebooting every 60 hours or so.  It turned out that there was a bug that caused a memory leak on each unsuccessful connection attempt.  We didn't know that at the time," he says, in parens, "(it was a driver issue).  So we turned on logging in the app in the hopes of finding out what was wrong."  Then he says:  "Needless to say, the memory leak disappeared."  And here's the term I just love.  He says, parens, "(a heisenbug)."



LEO:  Yes.  When observed, it disappears.



STEVE:  When observed, yes.  A heisenbug.  Turn logging on, oh, now the problem doesn't happen.  Darn.  He says:  "In the spirit of 'not all memory leaks are important,' we just left the logging on and piped it into dev/null.  Problem mitigated."



LEO:  Fixed it.



STEVE:  Yup.



LEO:  I think the one I read they just doubled the memory, and then it just didn't matter.  It just didn't matter.



STEVE:  Right.  Well, and of course we talked in the last couple weeks about the memory leak on the missile, where as long as the leak wouldn't bring the...



LEO:  That's the one, right, right.



STEVE:  ...software down before the missile reached its target, eh.



LEO:  That was it.  Yeah, yeah, yeah, yeah.



STEVE:  Not a problem.  And how much cheaper it was to double the memory rather than bring the subcontractors back and try to have them scrounge around and fix the leak.



LEO:  Simple.



STEVE:  So I don't know what is happening, but I found in the mailbag three different people who were like, oh, my god, robocalls.



LEO:  Oh, yeah.  It's out of control, yeah.



STEVE:  There's just been an escalation of it.



LEO:  Yeah, it's just out of control.



STEVE:  So Jerome Shidel said:  "Steve, please help.  Last time I was at my mom's house, she is being bombarded by scammers and telemarketers with upwards of 20 to 30 a day calls coming from local" - well, yes, spoofed local - "and all over the country."



LEO:  Yeah.  That's the new thing is they use your area code so you'll answer, yeah.



STEVE:  Yup.  And now they actually use your prefix.  That's the one - because I've been watching this.  It just makes my blood boil, the abuse of my private phone line.  And I'm one of 12 people who still have a landline.  But still, anyway, so he says:  "She gets the mortgage, debt consolidation, and IRS is filing a lawsuit final notice scams.  She is on the DNC list."



LEO:  Yeah, doesn't matter, yeah.



STEVE:  And I can vouch for the fact it absolutely means nothing.  He says:  "Other than terminating landline service, what can we do?  Any advice would be great."



LEO:  Well, it happens on cells just as much.



STEVE:  Yes.



LEO:  They don't care.



STEVE:  Yes.  So the problem appears to be getting worse because I was surprised by how many people mentioned it.  I can vouch for the fact because I have Caller ID, and I look at the Caller ID display.  And as you said, Leo, first they began using my area code.  Then they began using my prefix.  And in fact I got caught out a few times thinking, whoa, is this a neighbor, because it happens that in our neighborhood everybody has, like, the same prefix in this region.  And so I thought, well, maybe it actually is somebody who knows me.  So now I don't - I'm not getting suckered by that.  So it's area code and then prefix, and so they make up a random last four digits.



I verified my - I have actually three landlines, two voice and one fax.  I still have a fax line, and Sue sends me things every so often, and we'll fax things back and forth, just to have documents.  I've been on the Do Not Call Registry since July 27th of 2003, coming up on 14 years.  And it makes absolutely no sense.  As you said, Leo, doesn't matter.  Cell phones are not immune.  I have a huge and growing list of blocked numbers on my iPhone, which I never even use.  So the other thing we know is it's not like they're actually getting your number from somewhere because that's the other thing I notice is, since I have three landlines, they will ring sequentially with the same nonsense.  So they're just running through, sequentially dialing everything.



Now, iOS apps can manage a Do Not Disturb whitelist.  But unfortunately it doesn't prevent the call from being accepted.  It's just able to, if Do Not Disturb is active, it immediately goes to voicemail.  And I just saw, as I was doing some updated research on this, the Republican National Committee is trying to get the FCC to change some qualification of what counts as a call because it turns out there is a way to cause a mobile device to immediately go to voicemail.  They're trying to get that ruled as a non-call so that essentially the RNC will be able to inject voicemail into your mobile phones without controls, without any recourse.



LEO:  Oh, god.  Horrible.



STEVE:  So what we need is a phone firewall.  Mark Thompson, who is a friend of ours, who is a techie, he set up a PC-based Asterisk system years ago, which I should mention is not for the faint of heart.  I mean, the problem is telephony in general uses a completely made-up vocabulary where none of the terms have any normal world meaning.  So you have to, like, learn a whole new language to know what a termination is and, I mean, I can't even - I remember looking at it, thinking maybe I should do this.  And it's like, oh, no.



LEO:  I did the same thing.  And there's distros where it's all set up.  It's still nontrivial, yeah.



STEVE:  Yeah, yeah.  So what he has is an extension number.  So when you dial Mark's line, an automated assistant, an automated attendant picks up and says, "Please dial the extension number of the party you wish to call."  And then that's all it does.  And if he hasn't told you ahead of time...



LEO:  That's a good idea.



STEVE:  ...then, yeah.  So the problem is...



LEO:  The problem is your doctor calls, he doesn't know your extension number, and you're going to miss that call.



STEVE:  Exactly.  Exactly.  And just a couple weeks ago I received a call from an attorney representing someone of whom I am their executor of their will.  A lot of people choose me for that for some reason.  So it was like, uh-oh.



LEO:  You're so trustworthy.  And you're going to live a lot longer than the rest of us.



STEVE:  Actually, in this case they said, "We trust you more than anyone in our family."



LEO:  Yeah, that's not unusual, actually.



STEVE:  So I said, well, okay, fine, I'm happy to be your executor.  Anyway, the point is that, if I had something like Mark has, they couldn't have gotten to me.  And I want people to get to me.  So what we learned with Internet firewalls is that you can, just as with packets, you cannot block malicious.  The only solution is to whitelist.  So this has been a growing problem for me.  And I would probably have written a solution for myself, if I hadn't been on a roll with SQRL, because what I realized was - and I'm just going to leave this out here for our listeners.  There is a simple solution.  And if somebody wants to create one, I will publish it on the podcast to our listeners.



The simple solution is an $18 USB voice modem from Amazon.  They're just little dongles.  In fact, here is one that I got.  I mean, it's got the RJ-11 connector on one end and a USB on the other.  The famous Hayes AT command set, there is something known as the voice extensions, also known as the TAM, T-A-M, feature set, stands for Telephone Answering Machine.  Part of what a voice modem can do is to deliver a prerecorded message.  It also understands Caller ID.  And so here's how this thing would work.  First of all, so you need to plug this into a computer.  And if I were doing it, I would just use a PC.  The problem is that's non-optimal.  What's optimal is the $10 Raspberry Pi Zero, which is all you need.  It's got a USB interface.  It's $10.  So that and an $18 voice modem.  So for less than 30 bucks you've got a solution, although I'm not a Raspberry Pi developer.  Which is why...



LEO:  And this isn't going to work for a cell phone, and fewer and fewer people...



STEVE:  Well, it's going to work for mine because I'm going to turn on call forwarding.



LEO:  Ah.  There you go.



STEVE:  So everything that comes into my cell phone - first of all, I was looking to see whether Verizon, because I'm a Verizon cell phone user, apparently per carrier in the settings for the phone sometimes there's forwarding available.  It looks like I have to, for a Verizon customer, you need to enable it through *72 or something, rather than just turning it on in the UI.  But it's funny, I've had this account since the beginning of time.  My total lifetime use of the phone is one hour and one minute.



LEO:  You don't call out much.



STEVE:  I don't ever talk.



LEO:  Yeah, it's a computer.



STEVE:  It's a pocket computer and text messaging.  So I'm never getting calls there.  So I'll forward it to home.  Anyway, the idea would be, if a call comes in, the phone rings.  This thing sees the Caller ID.  If it's one it knows, like my mom or Mark Thompson or you or my best friend Mark here, it says, oh, yeah, fine.  And it doesn't pick up.  It just lets everything keep ringing.  If it doesn't know the number, it answers and says:  "Hi there.  Thanks for calling.  I don't accept unsolicited telemarketing calls.  If you are an actual human being, please press zero before the end of this message."  And then it goes on a little bit longer to give them time.  And then, oh, and it says:  "Press zero before the end of this message, and then call back," blah blah blah blah blah blah.  Beep.



Well, any telemarketing software thinks an answering machine just answered.  So you never get a human listening to that, and they hang up.  I've been watching this behavior now growing for a year or two.  A real person will hear that and go, okay, press zero, hang up.  Now they're whitelisted, and then they call back, and they get through.  So unknown random numbers.  You don't have to blacklist everything because blacklisting doesn't work anymore because Caller ID is just random.  So a real person hears that, presses zero, and calls you back.  So people like your doctor and dentist and so forth, they're able to get through to you.  Yes, you have to jump through a hoop once.  But once they're on the whitelist, they're whitelisted.



Anyway, that's the solution.  There is an $80 device on Amazon that's the Sentry v3.1, which does this.  So if you're not a developer, if you don't want to wait for one of our listeners to do it, which would cost 30 bucks for a voice modem and a Raspberry Pi Zero, you could pay 80 right now.  I did send this back to the person who asked, and he said he just ordered it for his mom.  And it is a cute little thing.



LEO:  And does it say "Press zero if you're a human?"



STEVE:  Yes, yes.



LEO:  Oh, that's great.



STEVE:  It has a built-in message, and you can - but I would rather use my voice.  And so you can say "Press zero if you're a human, and then call back."  And so they press zero.  That whitelists them.  If by any chance someone does leak through, you're able to browse the whitelist and move them to the blacklist, or just delete them.  But it turns out, in my experience, whenever I answer and say hello, then there's a click click click click, and then someone from some other country gets on the line.  So it's clear that there isn't a human being.  What the auto robo dialing software is looking for, an end of the ring and then a hello, a short thing.  Any extended speech, followed by a beep, is regarded as an answering machine, and they just... 



LEO:  Makes sense, yeah.



STEVE:  It goes on to the next one.  So I think this would perfectly block.  It's not necessary to spend $80, and I would dearly love to have time to do this myself, but I don't have time.  So if any of our listeners says, hey, that looks like a fun - that's a cool project, the idea being that you could use your phone to digitize your message with the incoming voice modem, store that file, and then that's what you play back when it doesn't recognize Caller ID.  So it's a whitelisting, low hassle, and could potentially completely end this problem.  But there is something called the Sentry v3.1.  There's also a Version 1 and a 2, but they don't quite do all of this, available on Amazon.



LEO:  Good idea.  Really interesting idea.  I think that's a better solution.



STEVE:  Unfortunately, we need something because it's just getting out of control.



LEO:  Oh, yeah, it's terrible.



STEVE:  And it's not that, I mean, I look at Caller ID, and I just don't pick up.  But it just frosts me that, unfortunately, that the world is such that there's no control over this, and our privacy is being abused like this.



LEO:  Yeah, terrible.



STEVE:  So Wayne Patton sent me a tweet saying:  "@SGgrc Isn't printing out your second-factor QR codes," meaning for the time-based authentication that I've talked about, "sort of like writing down your passwords?  I'm probably missing something?"  And it's like, uh...



LEO:  It's exactly like writing down your passwords.



STEVE:  And I responded, yes, exactly.  I said, "And writing down passwords..."



LEO:  Nothing wrong with that.



STEVE:  "...is the officially recommended best practice."  I said, as Bruce Schneier once brilliantly noted, and I'm paraphrasing, "It's far more secure to use a password that you cannot remember.  So write it down on paper.  We already have plenty of systems available for managing bits of paper, like physical wallets."  And so the point is that, while a local physical attack does need to be considered, the big threat is the automatable remote network attack, for which strong passwords provide the best security.  So yes, Wayne, printing out your second-factor QR codes is exactly like writing down your passwords.  Which is best practice.



LEO:  And there's nothing wrong with that.



STEVE:  Simon Zerafa, who always manages to send me something interesting, sent OCSP, which we discussed at length last week, seems to be alive and well.  And he quoted somebody tweeting that Let's Encrypt handles 25 billion, with a "B," OCSP requests per month.  Get this.  That averages 10,000 per second.  And remember that Let's Encrypt had an outage a few weeks back.  And I hadn't dug into it very deeply.  But this affected them from May 18th through the 19th initially, and then it got worse.  From around 6:00 in the morning on the 19th until 10:00 in the evening it became a major outage, they wrote, of both their OCSP and the main ACME API used for certificate issuance, where approximately 80% of the requests were failing during that phase.  So I was curious to find out, like, to dig in a little bit more than I had bothered to, what was going on.  And I got a kick out of the problem.



They wrote:  "The initial cause was a code deploy aimed to fix a problem with slash collapsing.  The OCSP protocol specifies that requests can be made via POST or GET," which are the two oldest query types for HTTP queries.  "Since the request body is binary data, GET requests must be encoded in some ASCII-safe way."  Meaning to turn the binary into ASCII.  "The OCSP RFC 46" - Jesus, that's a low-numbered RFC - "back in 1999 predated by several years the common use of base64url encoding," which was first standardized in a much later RFC in 2003.  So it defined the GET request - so OCSP defined the GET request as the base64 encoding.



Now, I should step back and just say that was never correct.  That was always wrong.  And I'm frankly shocked that that hasn't been fixed yet.  So essentially what this means - here's the problem.  Base64 encoding takes groups of three bytes of 24 bits.  Right?  Because a byte is eight bits.  So three of those is 24 bits.  And it regroups them into four pieces of six bits.  So that gives you - so six bits is 64 possibilities.  So it turns out if you use uppercase alpha, lowercase alpha, and the digits 0 through 9, that gives you 62 symbols.  So we're two short.  Base64 chose plus (+) and forward slash (/) as the final two symbols to make up the 64-symbol alphabet that you get when you reparse 24 bits from three bytes into four six-bit symbols, so four characters from three bytes.



And the problem is that forward slash is not URL safe.  It is, as we know, forward slash is a component of URLs.  So, like, how could you possibly ever have that in a GET request?  I mean, for example, this is one of the advantages of letting SQRL mature awhile.  It always used base64url encoding because I've always known that you can't have forward slashes.  Base64url encoding changes those final two symbols.  Instead of using plus and forward slash, it defines them as minus and underscore, which are both URL-safe characters that won't be confused by anything parsing a URL.



So it turns out that underlay the problem that Let's Encrypt had because they implemented the spec as it was, and the nature of their implementation caused a bunch of forward slashes to occur, which ended up screwing up the parsing of the use of the GET version of the request.  So I just got a kick out of the fact that something that, like, should never have been problem bit these people because somebody wasn't paying attention to the fact that you can't use base64, straight original base64 encoding, for binary data in a URL.



In SQRL, the server provides what we call of course the "nut," because it's SQRL, which is actually a nonce which the server uses to produce a unique challenge which the SQRL protocol signs.  Well, that is in that little QR code; and, if you click on it, it's in the URL that you click on.  That's binary, so we base64url encode it to make it safe.  And that's the query that the SQRL client picks up and processes.  So it's not going to bite us, but it did bite Let's Encrypt and the OCSP mainstream protocol, surprisingly.



Christopher Meacham tweeted, he said:  "@SGgrc Despite #AntiTrump, the Mar-a-Lago vulnerability was not responsibly disclosed."  And he's referring to that ProPublica article we discussed where they trawled out in the bay and aimed an antenna at Mar-a-Lago and also a bunch of other of the well-known Trump properties.  And I'd have to say I agree with that, although some things are just being done in plain sight.  And it's not like it took a rocket scientist to think, wow, let's check the WiFi at Mar-a-Lago.  And as their article noted, there are probably a lot of Pringles cans being aimed at Mar-a-Lago at the moment.



Hipposec said:  "Would love to hear a tidbit on SN about becoming a security researcher, especially transitioning from hobby to career."  And I thought about that a bit.  And I thought, you know, the biggest and really only requirement is an understanding of low-level code.  If you think about that, what we are, I mean, all of the things we see, everything we deal with is the assembly code, one step up from the machine language, but typically down below the source code.  I mean, it is the case that open source, where you can see the problem, you can go, oh, wait a minute, they defined that as a WORD, but they used it as a DWORD, and that's not going to fit, so that's a problem.  Yes.  But normally the problems are found by actually looking at the machine language.



So I would argue that what all security researchers have, what we keep seeing almost everyone doing, is understanding the code that the chip itself is reading.  And so there's no hurry.  It takes a while to get the hang of that.  But there are so many free tools and so much information available on the Internet now to help somebody in that journey, that I think it's a cool thing to do.



But really you can't be a security researcher, well, I mean, you can.  Some of the problems that Tavis finds, for example, are his deep knowledge of JavaScript and the DOM, like recently the problem he found, for example, when he was taking a shower, with LastPass.  But again, really, really deep understanding is what you need.  And so that just, you know, it takes time and involvement, if somehow you can get that.



Jason Werner tweeted:  "Just wondering what warranted such a high opinion of Edge."  He says:  "Seems lacking in configurability and choice, let alone privacy."  And so I'll say to Jason and our listeners that the thing I skipped when I said we're running out of time was a comparison of the resource consumption.  And it was someone who - a well-known website opened five browser tabs in Firefox, Chrome, and Edge.  Firefox required something like 236MB.  Chrome required a little more than twice that, 500, I think, and something megs.  Edge required 1.4GB for the same five tabs.



LEO:  Oh.  Wow.



STEVE:  So, yes.  And you know me.  I'm a Firefox user, and I care about resource consumption, just because.  Nowadays - I heard you talking about, Leo, that 18-core i9 Extreme processor.  I don't know if you know it comes with a large jug of Freon, which you just pour slowly on top of the chip while it's turning on, and it just evaporates on contact with the top of that chip.  My lord.  So all we're doing now is just throwing more resources at things.  Anyway, my high opinion of Edge comes, not because it is not lacking in configurability and choice and privacy, I don't disagree with those things, but because it is a from-scratch rewrite of IE.  And they did a beautiful job.



I mean, maybe it'll get a little more svelte over time.  I wouldn't hold my breath.  But, I mean, as a mass consumer base browser that most people are going to be using by default on Windows 10, and with no choice on Windows 10 S, the good news is it's not a piece of garbage, the way IE had become.  Even though IE got better, too, later in its life, Edge, they did a beautiful - there's a lot of good technology in Edge.  And so I'm not going to ding them just because they're Microsoft.  It's a good browser, and it's the browser that Windows 10 users are going to be using.  Thank goodness, you know, it's not IE6 any longer, or there'd be hell to pay.



LEO:  It is also the case, people always talk to me, they call up, say my RAM is 100% use.  A good operating system is going to use as much memory as available to it because that speeds up performance.  So it's not necessarily a measure to see how much RAM it consumes.  It's to see, when you start running other processes, if it gives that RAM up, if it runs more skinnily.  I mean, if you've got 8GB of RAM, there's no reason for the only program running to not use 8GB.



STEVE:  Correct.



LEO:  Or 7, anyway; right?



STEVE:  Correct.  Yeah.  And as long as you've got a large bottle of Freon handy.



LEO:  And you need a memory manager that's going to be smart enough and a program that's going to be smart enough to release memory when it's needed by other processes.  But it speeds everything up to use all the RAM it can use.



STEVE:  Yeah.  My cranky finicky old 32-bit XP machine starts getting unhappy as I get up near 3GB.  And so I just hit up Firefox, I restart Firefox.



LEO:  Yeah.  There's a limit to how much you have, you know.  If you have, for instance, if you have a 32-bit processor, you can't address more than 4GB; right?



STEVE:  Right.  And in XP the LSASS process, which is then responsible for security stuff, it apparently is leaking memory.  So after a couple weeks I'll just reboot the system, and it's much happier after that.  But it's old and creaky.  I'll switch to it when I can take some time off, probably before I - maybe after the first release of SpinRite.



And our final question:  Mark Dean says:  "Analogy needed.  Hey, Steve.  I was wondering," he writes, "how you would explain this.  I have a non-technical friend who is against software companies releasing their code as open source.  He thinks that it is giving hackers access to just go into systems.  I tried to tell him that having the source code in and of itself does not mean hackers have the ability to hack into a system.  They may know more about the details of the system, but that's all," he says, parens, "(unless things like passwords are in the source)," which of course they're not in any well-written open source.  He says:  "I tried to use the analogy about encryption, where the algorithm is well known, but that doesn't mean the keys or values can be easily derived," he says, "but that probably confused him more."  And, yeah, I don't think that's a great analogy, either.  "Any suggestions on explaining how knowledge of the source doesn't automatically mean an easy hack?  Thanks."



And so I thought about that a bit, and I wrote back to him.  And I said, here's an analogy.  Not publishing the source code would be like not printing a map to get from point A to point B.  Having a map would allow people to more easily inspect the route and find a better solution.  But not having a map does not prevent anyone from driving the route to obtain the same information.  In other words, it is inherently impossible to keep the map's route a secret, just as it's inherently impossible to keep computer code a secret.  Anyone who's interested can obtain the same information, and bad guys will be motivated to do so.  But publishing the map makes it easier for others, without the strong nefarious motivation of exploitation, to casually glance at the map and suggest improvements and help out.  And also note that publishing the map also doesn't guarantee that good guys will bother to help.  But it does make it much easier and more likely that someone will.



So I kind of like that.  I came up with that in response to the question.  But to me it sort of - it embodies the concepts and the principles, and that is that, yes, the source code makes what it's doing more accessible.  But not having a source code doesn't keep it out, doesn't keep the same operation out of the realm of the bad guys.  And having the source code makes it much easier for good guys to help, if they're so inclined to do so, much like looking at a map would.



LEO:  It's good.



STEVE:  There's our analogy for the week.



LEO:  And security experts often agree that the weakest form of security is through obscurity.



STEVE:  Right.



LEO:  It can be a form of security, but it's not very strong, especially if you can disassemble the binary.



Well, we have completed this journey into the mind of the hacker.  And now we have to go back into the real world, I'm sorry to say.  We do this show with Steve every Tuesday, right after MacBreak Weekly, supposed to be about 1:30 Pacific, that's 4:30 Eastern time, 20:30 UTC, if you want to watch live and join us in the chatroom at irc.twit.tv.  Most people, though, just prefer to download a copy, and you can do that from Steve's site, GRC.com.  He not only has audio copies there, but he has transcripts, so you can read along or search the transcript.  That's a very valuable tool that he funds himself.  Thank you, Steve.



While you're there you can help Steve with his funding by buying a copy of SpinRite, the world's best hard drive maintenance and recovery utility.  And then, since you've paid for it, you can now browse the site and find all sorts of other cool stuff that's absolutely free.  GRC.com.  We also have audio and video at our website, TWiT.tv/sn.  But we also encourage you to subscribe.  Whatever you use to listen to podcasts probably has a facility so that you can automatically download it each week.  And that way you'll get up on a Wednesday morning and go, "Oh, look, brand new copy of Security Now!."  Plus, of all the shows we do, this is the one I think people want the whole set.  All 614 episodes.



STEVE:  They're all there, baby.



LEO:  Why not have them all?  And hand them down to your children, your eight year old.  When she stops swinging, she can become a true geek.  Thank you, Steve.



STEVE:  Thank you, my friend.  And enjoy your vacation.  I guess we will not have you...



LEO:  I won't be here next week, yeah.



STEVE:  ...for two weeks.



LEO:  Yeah, that's right.  I will be on vacation next Tuesday and the Tuesday following, but then I'll be back on the 20th.



STEVE:  Yay.  I know you're looking forward to it, so have a great time, and we'll be back with you in three weeks.



LEO:  Yeah.  And I'm sure we have somebody wonderful, probably Father Robert, but I'm not sure, hosting with you.  Maybe Jason.



STEVE:  Cool.



LEO:  Yeah.  Have a great time, and I will see you in two weeks.



STEVE:  Right-o, buddy.



LEO:  Bye-bye.



STEVE:  Bye.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#615

DATE:		June 6, 2017

TITLE:		Legacy's Long Tail

HOSTS:	Steve Gibson & Father Robert Ballecer

SOURCE:	https://media.GRC.com/sn/SN-615.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  This week we discuss an embarrassing high-profile breach of an online identity company, an overhyped problem found in Linux's sudo command, the frightening software used by the U.K.'s Trident nuclear missile submarine launch platforms, how emerging nations prevent high school test cheating, another lesson about the danger of SMS authentication codes, another worrisome Shodan search result, high-penetration dangerous adware from a Chinese marketer, another "that's not a bug" bug in Chrome allowing websites to surreptitiously record audio and video without the user's knowledge, the foreseeable evolution of hybrid cryptomalware, the limp return of Google Contributor, Google continues to work on end-to-end email encryption, a follow-up on straight-to-voicemail policy, "homomorphic encryption" (what the heck is that?), and "closing the loop" follow-up from recent discussions.



SHOW TEASE:  It's time for Security Now! with Steve Gibson.  And Steve is guarding the galaxy of security exploits.  OneLogin becomes no login.  Linux has a new privilege escalation hack.  Windows for Warships?  And your cell phone carrier will leave you baked, broke, and potentially busted.  Security Now! is next.



FATHER ROBERT BALLECER:  This is Security Now! with Steve Gibson, Episode 615, recorded June 6, 2017:  Legacy's Long Tail.



This is Security Now!.  It's a dangerous universe, and we're like the Guardians of the Galaxy except what we guard is far more important than a bunch of cosmic stones.  That's right, Steve G., he's the man behind Gibson Research, ShieldsUP!, and SpinRite.  He's the one person you can trust because he answers all your security questions with "I am Gibson."



I'm Father Robert Ballecer, the Digital Jesuit, in for Leo Laporte.  And let's get to the action.  Steve, it is so good to work with you again.  I love, I mean, I like having Leo here, but I love when he goes on vacation because it means maybe, maybe I get to play with Gibson.



STEVE GIBSON:  Well, it's fun to have a little variety.  And I am not Groot.  I am, as you said, Gibson.  I did see the second "Guardians of the Galaxy."  And it was cute, but I thought the first one was better.



PADRE:  I haven't seen any movies this summer yet.  Literally no movies.



STEVE:  Wow.  Well, you've been very busy this summer. 



PADRE:  I've been a little busy.  I do want to see "Guardians of the Galaxy."  I want to see "Wonder Woman."  I've heard great things about that.  And of course I need to see "Spider-Man:  Homecoming" because I just - that's my childhood right there.  Spider Man was my geek passion.



STEVE:  It looks great.  And I've enjoyed all the Spider-Man movies so far.



PADRE:  Steve, this has been an interesting week.  You know, nothing earth shattering, but quite a few things that have been building up on security breaches and bad practices that we've been covering over the last couple of years.  Where do you want to start?



STEVE:  Well, so the last couple weeks, of course, have been WannaCry and like some crazy high-profile things.  As you said, there was a lot of interesting news, but nothing particularly breathtaking.  So I decided to title this "Legacy's Long Tail" because that's a theme we keep seeing.  And there's one particular story that is - actually, I think the term they use in the U.K. is "gobsmacking," which is appropriate because this involves the U.K.'s four nuclear-tipped missile Trident submarines and a piece of news that, well, our listeners will know what it means when we get there.



But there was a bunch of stuff to talk about.  We're going to talk about an embarrassingly high-profile breach of an online identity company that's got hundreds of corporate users who all then have themselves a huge number, in the tens of thousands, of users of their services.  A somewhat overhyped problem found in Linux's sudo command.  I mentioned the frightening software used by the U.K.'s Trident missile subs.  Just a fun anecdote about how small emerging nations, or at least one in particular, although there was some news about a different nation, as well, preventing high school test cheating.  Another lesson that we - and we've had a surprising number of problems with SMS authentication codes.  We hit another example of that in the last week.



The founder of Shodan did a sort of a custom script to search for a particular type of presence on the Internet and was surprised by what he found.  The news of a high-penetration, very worrisome adware from a Chinese marketer that reminded me of my own experience 17 years ago, believe it or not.  Another "but that's not a bug" bug in Chrome, which allows websites to surreptitiously record audio and video without their users' knowledge.  A foreseeable evolution of cryptomalware.  The somewhat limp return of Google's Contributor.  I hope they're going to fix it in the future.



And just a couple little bits of news.  The fact that Google is continuing to work on end-to-end email encryption, which is a very elusive holy grail.  Everybody wants it, but I've always been lukewarm on it because email is just fundamentally not secure, and all efforts so far to layer security on top of it have only come at the cost of significant inconvenience.  So we'll sort of look at where that's going.  I wanted to follow up on a mention I made last week about the FCC's consideration of formally authorizing a so-called "straight to voicemail" policy and how worrisome that is.



And then I'm going to tease us a little bit about something, we may have mentioned it a couple times, but we've never dug into it, and I don't think we're going to have time to do it justice that week.  But that is homomorphic encryption.  We'll just sort of touch on that before closing the loop with a comment from one of our listeners also relating to a topic from last week.  So I think another great Security Now! with lots of interesting content for our listeners.



PADRE:  I like this kind of an episode because it's stretched out.  There are so many topics that have a little bit of the puzzle.  I mean, in case those people who have been watching Security Now! for years and years and years haven't figured it out yet, everything that Steve talks about kind of goes into the next topic and the next topic and the next breach and the next attack.  And I like the menu that you've set down.



STEVE:  Well, and we end up with - what our longtime listeners are seeing are sort of the emergence of some themes, where it's like interpreters are hard, and we keep seeing vulnerabilities in all kinds of different instances of interpretation, that sort of being like the next frontier.  We finally moved from macros and email onto other things.  And then of course another theme is that it's so difficult to get us to move forward, to abandon previous solutions in favor of better ones like how difficult it was to get away from SHA-1 signed certificates onto SHA-256, even though the better ones had been around for a long time.  It's like, yeah, but, you know, these are working, and we're busy.  We have other things to do.  And so there is this, I think it's interesting, sort of this emergent set of themes that everything fits into.  So, yeah, more of that this week.



PADRE:  All right, Steve.  So we've got a few things that we need to cover.  Do we want to bury the lead, or do we go with the big story of the week?



STEVE:  You know, I don't want to forget to share with our listeners your own project and discovery.  Let's start with that because I think it's so cool.  One of the things that, in talking about this notion of recurring themes, one of the things that is huge has been side channel attacks.  And we've talked about all kinds of different ones, like watching somebody entering keys on a keypad from a distance on their phone.  Even though you can't see the surface of the phone, you can see the relative motion.  And we realized that it's possible to disambiguate what they're doing from that.  Just recently there was news of the accelerometer being able to determine where you were tapping on the screen because in tapping it, no matter how rigidly you held it, just the tap resulted in pitch and yaw that was a function of where you were tapping and so forth.



So there's been lots of talk recently about side channel attacks.  And you actually, as you often do on Know How, demonstrated how one that we have discussed in the past sort of from a theoretical standpoint actually works.



PADRE:  Yeah.  This is kind of weird.  I heard about this, and I kind of dismissed it.  I thought, oh, this is one of those silly Internet things.  But there was a discussion of whether or not you could use a sensitive enough IR camera - and not a near IR (NIR) camera.  There's this thing called near IR which is basically just a CMOS or a CCD sensor that you remove the IR-cut filter, and maybe you put a few IR LEDs.  I mean, that does work.  When you're talking about home security cameras, you can use that reflected IR to be able to sort of see in the dark.  But a real IR camera sees emitted IR, not reflected IR.  And there's a bunch of really cool things you can do with that.  We've been using it on TWiET, we've been using it on Know How to do things like see whether or not a power cable is overloaded because, as it approaches its maximum rated value...



STEVE:  Nice, nice.



PADRE:  ...it shows up.  It starts emitting a lot of heat.  Same thing with power panels.  Or what I've used it for is I'll go to the datacenter, and you can actually look at the flow of heat, the emitted light, IR, from that heat as it flows up from the racks.  Shows you whether or not you've got a good placement of all your equipment because you want your hot stuff up top, and you want the cooler stuff down below.



Well, there was one other thing, and I wanted to try it with this.  This actually was just released a few hours ago.  This is the new FLIR ONE Pro.  FLIR, of course, is Forward Looking Infrared.  They're a company that has sort of made their name making military hardware.  Well, they came into the studio, and they talked about their new Lepton chip, which is one of these military sensors that they shrunk down, reduced the resolution a tiny bit, and they've given us this.



So this is a module, the FLIR One Pro module, that goes onto a USB-C equipped phone.  I'll show a couple of features on it right now.  But really quickly I want to show you what we're talking about, if you go to the side camera here, John.  So this is a remote control.  I'm using it as a stand-in for any PIN device.  So if you have to punch in a personal identifying number, well, there's something that you can do.  If you notice, right now this is seeing this remote, and it's all black.  But if I punch in my PIN, and then I do it, you can actually see where the residual heat watches are.



STEVE:  Nice.



PADRE:  And the cool thing is, if you take a picture of this, it lets you switch back and forth between visible and IR energy, and it will show you the heat decay.  So by calculating the heat decay, you can figure out which button was pushed when.  Now, I've tried this at supermarkets, where people push in their PIN numbers.  And of course I always ask them if I could do it.  They punch in their PIN numbers, and then I'll just take a quick scan of the pad.  And I'm able to, like 90% of the time, duplicate their sequence.  Again, it's one of these things where you're probably not thinking about it, but, yeah, you do leave fingerprints, even if you don't know it.  In fact, I've gotten into the custom now of anytime I'm using a PIN pad, I will actually put my hand over the entire pad to warm it.  I know that's silly, but now that I know that it's really easy to do this?  And by the way, that heat decay can stick around for a couple of minutes.  It's just one of the things I have to watch for now.



STEVE:  Very cool.  Another classic side channel attack.  In this case, our hot little fingers heat up the buttons, and the buttons are going to return to ambient temperature in the sequence in which they were pressed.  So, wow, very cool.



PADRE:  And actually I thought maybe this attack would work better in the winter because the heat would show up better.  Not so.  It actually works better in the summer because then the ambient air isn't there to pull the heat away from the buttons.  In the winter, when I was doing this in the cold, I could maybe get an image 60 seconds after someone's punched it.  In the summer, I just tested it when it was like an 87-degree day in San Francisco, I was able to do it five minutes afterwards.  So, I mean, it's pretty impressive.  And the sensor itself, this is not a CCD.  This is not a CMOS.  This is an actual custom piece of silicon that they created.  And I've got to say, it's a specialized product, but for $200 this is something that's going to be fun to have in my toolkit.



STEVE:  I was just going to ask you what the price is.  So 200 bucks, that's a good price for a FLIR camera.



PADRE:  Well, I remember the first time I ever played with a FLIR was on an Apache gunship.  They were doing one of these demonstrations.  And they had us put on the helmet.



STEVE:  Well, of course.



PADRE:  And the camera and the turret follows your head tracking.  And they would turn off all the lights, and you could still see, just zeroed in on heat signatures.  This is the same technology.



STEVE:  Very cool.



PADRE:  So I guess what you should do is don't touch anything ever.



STEVE:  Or, I guess, what, wear gloves.  You want to not transmit the heat into the device that you're touching.



PADRE:  Someone in the chatroom says that he carries around a bottle of Freon anywhere he goes, and he just sprays it down afterwards.



STEVE:  Or enter your PIN, get the acknowledgment, and then press a few more buttons.



PADRE:  Just keep going.



STEVE:  Yes, exactly.



PADRE:  Again, that's one of those other things I will confess to.  Whenever I'm typing in a PIN, I will do some fake PIN presses, and then the real PIN presses, and then more fake PIN presses.



STEVE:  And you might call that "PIN Haystacks," when you think about it.



PADRE:  Yes, actually, yes.  And I only do that because just this last week - I don't know if you do this, too, Steve.  But anytime I go to an ATM, I always give a yank on the card swipe, just to make sure no one put something over the top of it.



STEVE:  Yup.



PADRE:  And, finally, seven years after starting to do this, I yanked, and one came off.  So I've got a, what do they call them, swiping device.



STEVE:  Oh, my goodness.  Wow.



PADRE:  I just have to figure out how it works.



STEVE:  So our first bit of security news is, aside from this cool side channel attack, is that a major single sign-on provider, OneLogin [audio dropout].  It was last Wednesday evening around 9:00 p.m. Pacific time, they detected an odd usage pattern in a database instance.  They use Amazon's AWS cloud services for their infrastructure, and they saw that something suspicious was going on and shut down that instance.  Subsequently, looking at it, they realized their worst nightmare had occurred, and that is that for about seven hours somebody had been rummaging around in their database.



So starting at around 2:00 p.m. last Wednesday afternoon through 9:00 p.m. when they saw it and shut it down, somebody had apparently obtained their AWS keys, which meant they had decrypted access to their infrastructure.  So just to remind people, what a service like OneLogin does is they're sort of a commercialized version of "Sign in with Google."  That is, they are an OAuth provider.



So the idea would be that, rather than Sign in with Google or Log in with Google, it would be Sign in with OneLogin.  So their customers are corporations who want to offer to their employees or to their customers the convenience of single sign-on operation.  So when you go to the company's site, rather than "Sign in with Google," you see "Sign in with OneLogin."  So essentially they're an aggregator of the login technology, the login verification for all of their customers.  And they're logging their customers' customers, that is, their visitors [audio dropout], depending upon what their model is, they're logging them into those companies.  So there's, I mean, they have a huge responsibility, and a breach of their security is a disaster, not only for their direct customers, but for all the people who log into their customers' sites.  So this is not good.



Now, they got a lot of coverage in the press because this was a high-profile breach.  And the coverage was not all good.  Aside from the embarrassment, which is completely understandable, and one of the recurring themes on the podcast is security is hard.  And everyone can be forgiven, I think, for making a mistake.  How people respond is what's important.  So we've defended LastPass, that has been a recent victim of Tavis Ormandy's showers, because even if problems are found, in no instance were any customers damaged that we know of, and LastPass responds typically within hours.  So the problem here is - and responsible disclosure means you just tell people what happened.  So OneLogin didn't do that.  They were very circuitous in their disclosure.  They didn't publicly provide much detail.  Their response has been called "opaque" and "deliberately lacking in detail."



However, they provided their customers, to whom they feel, I guess, more obligation, email with much more information.  Of course that email didn't stay in the possession of the customers.  Quickly that got out to the press, that realized the extent of what had happened.  And in this email that they provided to their own customers, they said:  "The threat actor," as they put it, "was able to access database tables that contain information about users, apps, and various types of keys."  They wrote:  "While we encrypt certain sensitive data at rest, at this time we cannot rule out the possibility that the threat actor also obtained the ability to decrypt data.  We are thus erring on the side of caution and recommending actions our customers should take, which we have already communicated."



And then, in this private email to their customers, they were told basically start over.  That is, start from scratch.  Essentially, nothing that had been done could be trusted.  Meaning they were told to generate new API keys and OAuth tokens, create new security certificates as well as credentials, recycle any secrets that were stored in OneLogin's Secure Notes feature, and have every end user update their passwords and more.  So basically a complete start from scratch.  They were saying, as far as we know, the bad guys obtained our AWS keys and had access to the encrypted secrets that we, say they, were holding on behalf of all of our customers.  So, sorry about that.  Start over.



PADRE:  You know, Steve, one of the parts of the story that just kind of - it flusters me.  And again, it's because they've been so opaque with the information that they've released.  At first they said, okay, we've been breached, but we don't know how bad it is.  Then it was, it's probably all the customers in the United States, but we don't know what they've taken.  And then that memo leaked, and they realized, no, they took everything.  They had access to the complete database.  Now, check me on this.  So the attacker, whoever it was, got a copy of the API keys.  So they were able to directly access the storage basket that OneLogin was using for their database. 



STEVE:  Right.



PADRE:  They don't tell us exactly how they got access to the second part, which is either information or a tool to decrypt the database, because it was encrypted.  This was not left out in cleartext.



STEVE:  Right.



PADRE:  But OneLogin in essentially saying, yeah, we think that they've got a method for decrypting, which has driven a lot of speculation.  Was it a common encryption theme?  Was it something that was tied to the AWS key?  Were they just using Amazon encryption?  Or was this a proprietary encryption, that they left information in cleartext on the same server?



STEVE:  Well, again, there's no information because this is their proprietary technology.  What we do know is that, essentially, if they're an OAuth provider, they have the ability to essentially authenticate the people who authenticate to them on behalf of their customers.  So what this says is, again, basically start over.  They have to start over.



Now, as everybody who's listening to this podcast knows, I've invested a great deal of my time with the SQRL project in nailing down every last detail required to turn what is a simple and theoretically strong concept, that is, the original idea that I had, into a useful reality with SQRL.  So when I encounter stories like this, since SQRL is on my mind, and it's what I've been spending all my time on to push this thing over the finish line, I naturally test what I have, the model, against the same kinds of vulnerabilities.  And what's so fundamentally different, and what makes SQRL unique, is that it operates in an entirely different way.  The slogan I've coined to describe this is "SQRL gives websites no secrets to keep."  And so naturally, if a service has no secrets to keep, it has no secrets to lose.  It has no secrets at risk.



So in traditional login, and OneLogin and OAuth are still in the traditional model, they rely upon first entrusting a server with a secret.  Or hopefully a safer derivative of a secret, we all know, which is frequently a hash.  And then in subsequent visits the original provider of that secret chose the server again that we still have or remember that original secret.  So we're just saying, "I'm back.  Here's essentially what I gave you or a derivative of what I gave you.  It's me."  And so, again, the problem is that that remote service has to keep those secret because those can be used to impersonate someone.



What's different about SQRL and always has been is that it's based on a simple cryptographic proof of knowledge, so without ever sharing that knowledge.  What this means is that the site, what we give to a site, what the site holds for the user can only ever be used to verify the user's assertion of who they are.  It cannot be used to generate such an assertion.  So there is zero value in the theft of that information from the site.  So it's fundamentally a different approach.  And we're getting close to getting this thing out the door.  As our listeners know, it's been working more or less for a year.



And I'm just, again, I'm continually testing against the problems that I see in order to make sure that it meets the challenge.  And in fact there's already some features that are there I'll touch on a little bit later with a different story, again where I'm saying, okay, yeah, we got that covered, too.  At the moment I've just been rejiggering some of SQRL's antispoofing enforcement because spoofing is a problem that there still has not yet been any solution for.  And OAuth is as subject to it as anything.



And I just decided, and I talked about this last week, and I think the week before, that as a consequence of what Microsoft was saying they were going to do in Windows 10, which was to ban Edge web pages from being able to access the localhost services, we backed off a year and a half ago, about 18 months ago, summer before last, when that was apparently going to happen.  That ended up not happening because Microsoft, it turns out, could not close that down.  There are too many things that use browser pages that are able to access stuff in your computer through 127.0.0.1, the localhost IP.  And so Microsoft said, okay, there was just too much developer flak.



So before putting this thing to bed, we revisited this.  And by allowing the browser to communicate to the SQRL client running a localhost server on the machine, we have what's called "client-provided session," where the communication between the authenticating remote server and the SQRL client, it receives the session token, which it then hands back to the user's browser.  That inherently cuts any man in the middle out of the loop.  So it makes this solution completely unspoofable.  So I just thought that's too much of a benefit to pass up, even in the v1.0 release of SQRL.  So we brought it back with a vengeance, and it'll be there from day one.



PADRE:  We keep having these stories that show us more and more of the advantages of having some sort of authentication technology that doesn't require a secret to be kept remotely.  Because we've just, I mean, we know, I think we're wise enough now to realize anything that you give, a secret that you give to a third party eventually, at some point, is probably going to leak.  It's just - that's just the nature of the beast.



And I'm wondering, one of the things about this story is the fact that there are so many large companies that are customers of OneLogin.  They include Amazon, Microsoft, Cisco with their WebEx, Google Analytics, and LinkedIn.  We use services like this because we want to think that it's more secure for us to use a trusted third party, rather than us keeping all of these loose usernames and passwords.  But at some point the Internet and the cloud economy keeps pushing us to this idea of persistent authentication, where our identity is known, no matter where we are on the cloud, no matter what service we're using.  But do you think it's possible to have that persistent authentication if multiple third parties need to have a secret that they can't let out?



STEVE:  Actually, SQRL provides that, too.



PADRE:  See, that's where I'm thinking it's going to work.  I think SQRL is - it's not just a better way to do passwords.  It is the persistent authentication that we were looking for.



STEVE:  Correct, because SQRL can work in a strict two-party mode, where you authenticate with a site that you are wanting to log into.  But it also supports a three-party mode.  That is, for example, say that the federal government wanted to run an identity service so that you could log into the IRS and the DMV and a bunch of other government services, all with a single known identity.



So the idea would be that any of those government websites would show you the SQRL URL, but rather than the URL being to that site, it would be to - we'll just make up something and call it federated.identity.gov.  And so the idea is you're actually authenticating to that site, which means your identity is the same for all of the different government sites that you want to log into because, in the same way that OAuth arranges a third party, SQRL supports that concept, as well.  And so the idea would be that you actually establish your identity only once with federated.identity.gov, and then all government sites are able to refer to that singular identity and just create new sites as they want, without having fragmented identity.  So it isn't necessary.



And I think the big difference is, I mean, the biggest hurdle we have is the recovery problem because what you have with a third-party is you've got someone to go crying to if you forget your password.  And that is, so for example, if you forget your OneLogin identity, you do some sort of recovery with them, and in the process you recover that singular identity to all the services that use OneLogin.  But that, I mean, inherent in that is the vulnerability.  That's the single point of failure.



So what's different with SQRL is there is no one to go crying to.  There's no "I forgot my password" recovery because there isn't built into it a third party.  So instead we have, like, built into the SQRL system is a set of ways to allow the user to recover their own identity.  Even in the event of theft, you can take it back from somebody who has absconded with it.  So anyway, as soon as I get this thing launched, we'll have a lot of fun covering all the details and explaining how this system provides a robust, scalable identity solution for the Internet with zero vulnerability.



PADRE:  You know, Steve, it's interesting because what we're talking about right now is a quintessential function versus convenience issue.



STEVE:  Yes.



PADRE:  I can't tell you how many times I've been to a Vsites or a Black Hat talk or a Defcon panel, and we're talking about this sort of persistent strong encryption, strong authentication.  And then you get to the question of, well, what happens when Grandma loses her password?  The correct answer is it's gone.  That's it.  Whatever it was that you were using to authenticate yourself, if you lose that, you no longer can authenticate yourself.  But every commercial service out there right now it's, well, there is a way to recover.  And if there's a way to recover, it means you've now introduced a point of failure into your protection scheme.



STEVE:  Exactly.  Exactly.



PADRE:  But we're not willing to give that up.  I don't think I'm - even me, I'm paranoid, but I'm not willing to give that up because I know sometimes I do stupid things.  Maybe I'm going to be one of these people who changes my password because I took an  Ambien one night, and I went sleepwalking, and I just changed all my credentials.  You know, something as stupid as that makes people think, well, maybe there should be a last resort.



STEVE:  Well, we have one.  It's called the "rescue code."  I'll explain how it works as soon as we get there.



PADRE:  Absolutely.



STEVE:  So there actually is a means for recovering, as long as you do one thing once.  And, I mean, so exactly as you said, there is no way around that question of responsibility.  But essentially we have made this as recoverable as possible.  So I look forward to explaining it all in some detail.



We should mention that there was sort of an overhyped vulnerability that was found by the guys at Qualys Security in looking at the source for the processing of Linux's sudo command.  They found a really obscure bug.  So I call it overhyped because it's already been addressed.  It's one of those things where, okay, yes, Linux users should update.  Red Hat immediately pushed out patches for Enterprise Linux 6 and 7 and Server.  Debian has released fixes for its Wheezy, Jessie, and Sid releases.  All of the various Linuxes are going to be responding.



It wasn't a zero-day.  It's not known to ever have been exploited, and it's not a network vulnerability.  It's only a local privilege escalation where, by doing this bizarre series of things, and I'm not even going to bother trying to explain it because it'd just make everyone's eyes cross.  I've got a link in the show notes because OpenWall.com has the details listed.  But, I mean, you've got to go through a huge number of funky steps with a bunch of manipulations.  Essentially it ultimately allows someone without root privilege to get persistent root privilege by cleverly abusing a side effect of the way the "super user do," the so-called "sudo" command functions.  So I just wanted to let everybody know.



So Sudo v1.8.6p7 through 1.8.20 have been vulnerable, and the p1 variant of 1.8.20 and subsequent, presumably, are not vulnerable.  So just check in with Linux.  Again, it's local privilege escalation only, and really not something to worry about.  But definitely worth upgrading.  Oh, I shouldn't say "not something to worry about."  It entirely depends upon the environment of that machine and people, like the population of users who have access to sudo and the ability to do all this other bizarre stuff that's necessary in order to turn it into an exploit.  Just, you know, fix it and don't worry about it.



PADRE:  My understanding was that this had to do with the way that the vulnerable versions of Linux handle the terminal, TTY.



STEVE:  Correct.



PADRE:  Essentially it's field 2.  So field 2 they don't check for white space, so they don't, when they're parsing it...



STEVE:  Exactly.  You can have an embedded space and, yeah.



PADRE:  And it's supposed to, and it doesn't.  So what can happen is you can actually use that - it's basically a parsing attack.  You're attacking the way that Linux is parsing the individual commands on the terminal.



STEVE:  And what is a parser?  A parser is an interpreter.



PADRE:  Yeah, that's all it is.



STEVE:  So once again, we're interpreting something, and there is a mistake in the interpreter, which we keep seeing.



PADRE:  But the important thing is there is a lot of hay being made about this.  It is not a remote privilege escalation.  You actually have to be sitting at the machine.  In which case, if there was an attacker at your machine, you've got bigger problems than a terminal parsing issue.



STEVE:  And Padre, you also have to be a rocket scientist.  I mean, if you look at this stuff, I mean, like what you have to go through, it's like, okay, you probably already have root brain power if you're able to take advantage of this.



PADRE:  Last night when you sent me over the notes, I saw this, like, oh, interesting.  So I started up a terminal, and I started trying to do this.  And I got it working once, and I just gave up.  I'm like, this is way too complicated.



STEVE:  Speaking of which, we never closed the loop.  Leo mentioned that you were going to play with decrypting WannaCry...



PADRE:  Yes.



STEVE:  ...by trying to find the key that was persistent in memory.  Whatever happened with that?



PADRE:  So it does work.  I was able to get it to work.  It finds the two primes in memory, and then it recovers the public key, and then it can reconstruct the private key, and it does the decryption.  However, you have to get really, really, really lucky.  So this is one of these things where it's time sensitive because WannaCry doesn't delete the two primes from memory, but that doesn't mean that they're protected.  Those memory cells can be overwritten.  So if WannaCry does any further processes, or if you do anything to interrupt the process, so reboot the computer or kill it, it's gone.  It's entirely gone.  Even when it was a perfect 100% laboratory condition, where I had the WanaKiwi installed on the computer already, so it was ready to go.  I infected it.  I let the warning screen come up, went directly into a command shell, ran WanaKiwi.  I was only able to recover, what was it, like a third of time.  So it, yeah, it's not a great last-ditch solution.



STEVE:  And we talked about how WannaCry uses the Windows crypto functions, and there was a "destroy context" function which for a long period of time wasn't securely wiping the memory of the context before releasing it back to the system.  So the keys were just floating around out there.



PADRE:  Keys were just sitting there.  But again, the issue is, unless you've already got it on your computer - so you've got to have WanaKiwi on the computer ready to go.



STEVE:  Right.



PADRE:  Just the process of bringing up a browser and...



STEVE:  Too much churn.  It'll churn it up.



PADRE:  Yeah.  Yup, gone.  And you have to find both.  So if you find one, that's not enough.  You need both.  All right, Steve.  We've got OneLogin hacks.  We've got a Linux exploit that's not as bad as it was first reported.  What else do we got?



STEVE:  Okay.  Now, for this kind of story, I often tell Leo to make sure that he's well centered over the ball that he's sitting on because otherwise he may, when he hears this, he may fall off of his ball.  And this is just incredible.  So I'm reading along in a report in the Guardian.co.uk, a report titled "U.K.'s Trident nuclear submarines 'vulnerable to catastrophic hack,'" where a security thinktank does not believe that the fact that submarines are inherently, okay, the term is "air gapped," but in this case water gapped, should support the level of complacency that the bureaucrats appear to have.



In the show notes, for anyone, especially if you're in the U.K. - although I'm not sure it matters where in the world you are if there's an aberrant nuclear-tipped missile launch.  I do have the "Hacking U.K. Trident" PDF link in the show notes.  It's a 38-page report titled "Hacking U.K. Trident:  A Growing Threat," which states that the U.K.'s Trident submarine fleet is vulnerable to, as the Guardian quoted, "a catastrophic cyberattack" that could render Britain's nuclear weapons useless.  It warns that a successful cyberattack could "neutralize operations, lead to loss of life, defeat, or perhaps even the catastrophic exchange of nuclear warheads," and then they say, parens, "(directly or indirectly)."



Okay.  So that doesn't sound good; right?  The report says:  "Trident's sensitive cyber systems are not connected to the Internet or any other civilian network.  Nevertheless," the report reads, "the vessel, missiles, warheads, and all the various support systems rely on networked computers, devices, and software, and each of these have to be designed and programmed.  All of them incorporate unique data and must be regularly upgraded, reconfigured, and patched."  And apparently that has not been happening.  The U.K. has four nuclear missile-carrying submarines which are in the process of being replaced.  And the replacements are scheduled to go into service not for a while, in the early 2030s.  Okay?



So then, as I'm reading, I encounter what I described at the top of the show as, and I believe this is U.K. terminology, a "gobsmacking" paragraph:  "The report comes after the cyberattack last month that disrupted the NHS."  Remember how most of the U.K.'s hospital systems and medical infrastructure was completely shut down.  This report says "that disrupted the NHS, which uses the same Windows software as the Trident submarines."  I said, "What?"  Yes, the U.K.'s Trident submarines are running Windows XP.



PADRE:  You know, we could have just skipped all of that and just said, "Here's the gobsmacking part.  There's a bunch of nukes run by Windows XP."



STEVE:  Oh.  Page 23 reads:  "The Submarine Command System" - the SMCS.  We know that the military loves their acronyms.  "The Submarine Command System was first created for the Vanguard-class submarines as their tactical information and torpedo weapons control systems.  It has a long and complex pedigree.  Its updated versions are based upon a version of Windows XP and known colloquially as" - I can't even say this - "as 'Windows for Warships.'"



PADRE:  Oh, that's almost as good as Windows Bob.  Do you think Clippy is going to drop by the U.K.'s Ministry of Defense and say, "Hey, we noticed that you're running Windows XP.  Would you like to upgrade to Windows 10?" 



STEVE:  Oh.  "These have now been installed on all active Royal Navy submarine classes.  Both Windows-based and Linux-based operating systems hold the legacy of vulnerabilities from the original systems, even though they operate on obscure and classified equipment and run bespoke programs.



"In 2002" - so 15 years ago - "it was proposed to convert SMCS" - that's the Submarine Command System - "to run on standard x86 hardware redesigned specifically for naval command systems.  The plan" - and I'm reading from the report - "was to convert the SMCS infrastructure" - which used to be proprietary - "and applications to run on Microsoft Windows XP" - because what could possibly go wrong? - "and known as SMCS-NG (for Next Generation) or" - as it's now commonly known - "Windows for Warships.  This is based upon a variant of Windows 2000 and Windows XP."  So, yes, even really old XP.



PADRE:  So it's not even XP Embedded.  This is just sort of a stripped-down version of the operating system.



STEVE:  Yes.  "SMCS-NG was retrofitted into all Royal Navy submarines by December of 2008.  The software is supplied as a universal release configured for the sensor and weapons fit of each submarine."  The report finishes, and of course this is well known to our listeners:  "Windows has an entangled monolithic structure, as opposed to a modular architecture.  It is therefore impossible," they write, "to change the proprietary operating system by means of reconfigurations and third-party modules.  This structure of" - and they wrote "consumer-friendly."  And I thought, it's not so consumer-friendly if it lands a nuke in your neighborhood.



Anyway, "The structure of the consumer-friendly operating system exposes potentially vulnerable services" - potentially vulnerable, uh-huh - "services and features" - that brings a new meaning to the term "potentially vulnerable" - "that might not be required for the adequate functioning of the submarine."



PADRE:  Oh, I don't know about that.  I mean, I know I'd want my weapons officer to be able to play Minesweeper.



STEVE:  Who doesn't want Skype?  You want Skype in your Trident class nuclear armed submarine.



PADRE:  Okay.  But Steve, let me back off here for a second because we have covered this sort of story on This Week in Enterprise Tech on the TWiT.tv network when we were talking about U.S. military nuclear infrastructure, specifically how some of our infrastructure dates back to the '70s and '60s.  And we can look at that and say, oh, my gosh, that's so old.  They're using these huge floppy disks.  But...



STEVE:  No, please, please, please stay there.



PADRE:  Exactly.  At the same time it's like, yes, we know that that's antiquated hardware and software.  But it's not vulnerable.  I mean, it's not like a consumer operating system, where you can pick up something like Metasploit and put together an attack package in under five minutes and send it on its merry way.  I have no way to infect nuclear launch machines from the '60s that are still using vacuum tubes and tapes.



STEVE:  Correct.  And it's like, you know, we were all often told that the flight control computer in the NASA shuttle systems had less power than your typical calculator.  But similarly, yes, and nothing can go wrong.  There isn't an extra word of memory available to contain malware in these.  So they can't.  Unlike Windows, which is like, how much RAM do you need?  Oh, no problem. 



PADRE:  Well, but Steve, this is just an extension of the underlying issue behind what we saw with WannaCry, where there are just some systems that are very difficult to upgrade, very difficult to extract your programming, your code.  I mean, this is what happened to the National Health Service in Britain, where they were saying, look, we're not ready to upgrade to a new operating system.  All of our software has been written for XP.  And there are so many regulations about what we can and cannot do because it might endanger patient information that trying to upgrade to a new operating system isn't something that we can choose in even a decade.



STEVE:  In other words, legacy's long tail.



PADRE:  Yes.  Yes.  



STEVE:  It's just that, I mean, that is the problem, the long tail of legacy systems.



PADRE:  And it's easy to look at a story like this and say, well, this is stupid.  They need to upgrade.  But we're going to have the exact same conversation in 10 years because they will have spent billions of dollars to upgrade, and their new operating system will no longer be new.  



STEVE:  Right.  And, I mean, the problem with using commodity operating systems is that, if it's the same - one of the things, another recurring theme is the danger of homogeneity in our systems.  If we're all using the same stuff, then a problem is ubiquitously bad because it can infect everything.  If instead we had a much more heterogeneous, yeah, a heterogeneous world, then we would be siloed, and the danger from a threat would be much less widespread, much more contained.



Speaking of threat containment, last Tuesday, okay, so a week ago yesterday, the nation of Ethiopia turned off the Internet for themselves.  They shut down Ethiopia's access to the Internet.  Why?  Because the following day was high school test day for both the 10th-grade national exams and a little more than a quarter million 12th-graders taking their university entrance exams.  The problem is that, a year before, the exams had been posted to the Internet, which caused a huge upheaval.  So the Ethiopian government solution was, okay, we'll just shut down.  We will go dark.  That way, if anyone posts the exams to the Internet, the students will not be able to get them, and the tests won't have to be retaken and invalidated.



The report that I read said:  "Outbound traffic from Ethiopia was shut down around 4:00 p.m. U.K. time" - this was in the Guardian.co.uk - "on Tuesday, according to Google's transparency report, which registered Ethiopian visits to the Google's company sites plummeting over the evening.  By Wednesday afternoon, access still had not been restored."  Right.  They wanted to keep it down while the students were taking their tests because a year ago activists leaked the papers for the country's 12th-grade national exams, that is, the college entrance exams, calling for the postponements of papers due to a school shutdown in the regional state of Oromia.



Now, apparently, the government took the move to shut down Internet access as a preventative measure.  And it's worth noting that an emerging economy such as Ethiopia has a far lesser dependence upon the Internet than any post-emergent economy.  Obviously, the U.S. and all major economies now on the planet have become so dependent upon the Internet that our economies would collapse overnight if networking were to disappear.  So I just thought it was interesting that, yes, we'll just shut down the entire country's access in order to briefly mask any potential exposure of sensitive material during test-taking time.  I don't know if that will be an annual event going forward.



PADRE:  Yeah, it's interesting because this would be impossible in a modern Western world because there is no switch to turn off the Internet.



STEVE:  Right.



PADRE:  There's no big red button.  The infrastructure is so diverse, I mean, it's designed to be robust.



STEVE:  Yes.



PADRE:  But when you go to a developing nation, and actually in Ethiopia it's something like 78 or so percent of the country is connected wirelessly.  That you can control.  That you can actually shut down relatively simply.  You just talk to the carriers.  But any time you get a more advanced infrastructure, you just can't.  Still, it's kind of a novel approach.  I actually don't think it's really going to work because those who want to cheat will find a way to cheat.  People were cheating before the Internet.  People were cheating before mobile phones.  People will be cheating long after the Internet has been shut down because of test taking.



But, yeah, you're right, anytime you get to see a story like this it's sort of an interesting microcosm of what if because, you may remember, we've had people in this country who have said there should be an Internet kill switch.  And it's sort of like, oh, okay, can we actually think about all the logistics that would be involved in trying to do that?



STEVE:  And you know, I think everyone would acknowledge that we're now past that.  I mean, nobody in their right mind imagines that you could turn it off, and it wouldn't be the end of the world as we know it.  I mean, today it would be.



Speaking of the end of the world, Leo talked about this over the weekend, and I thought this was - I needed to bring it up just because it was an interesting story of woe.  And apparently there is a takeaway potential solution.  I meant to do this yesterday, and it just slipped my mind.  But on Medium.com an individual named Cody Brown told the story that was very upsetting to him, the loss of, which he watched occur, of $8,000 U.S. worth of bitcoin.  And I think it was actually a couple different cryptocurrencies, but a total present value of $8,000 disappeared in 15 minutes as a consequence of him being the target of an attack.  One of the other sort of principles that we're seeing emerge is that it is very difficult for a targeted attack to be thwarted.  That is, our systems are so porous that they can be set up to resist penetration, but that human factor still comes in.  That's the ultimate Achilles heel.



And it sort of gets back to what we were talking about, Padre, about if there is recourse, then that's the point of vulnerability, if nothing else.  So anyway, so what happened in this case was - and Cody lays this out in his blog posting.  It actually, the snapshot of the picture is our Picture of the Week because he took a picture of what his phone showed him.  He was just sitting around in the evening and up pops a message:  "Free VZW Message:  You're on the phone with Verizon and just authenticated with an alternative method.  Not you? Please call us at 800-922-0204 immediately."



Well, it wasn't him.  And he's like, what?  So he immediately calls the number.  And he gets the message:  "You have reached us outside of our normal business hours.  Please call back."  And then, you know, between Monday through Friday, whatever it is.  And he's like, uh, okay, wait.  So he starts scrambling.  He runs around, tries to find another number.  He sends some emergency tweets and a while later gets back a response.  Meanwhile, he sees a message indicating - he's watching powerlessly as his Google Mail account was taken over, and then the password was changed.  And then he starts getting notices from Coinbase that money is being transferred from his Coinbase account, the wallet that he has stored there, and $8,000 disappears.



So piecing this together, what he realized is that somebody, an attacker, decided to go after him, thus a deliberate focused attack.  They contacted Verizon and said, hey, I've got a new phone, and so I want to change my account over to this new phone.  And so the Verizon person said, okay, what's your password?  And the attacker, who of course didn't have it, said, uh, I forgot it.  And then the Verizon person says, okay, what's the PIN that you've registered with us?  Oh, says the attacker, it's been so long since I last used it, I forgot that, too.  And so the Verizon guy said, okay, so what is the answer to one of your security questions?  Oh, says the attacker.  Damn.  I don't remember.



And so the Verizon person says, well, do you have your billing information?  Oh, yes, I do.  Because that's nominally publicly available.  The person did some research into Cody, figured out maybe where he lived, some things about him, enough things, although none of them should have been used for account recovery.  The person ended up using some billing information in order to convince the Verizon person that they were Cody Brown, and they changed over to the new phone.  Now the attacker had the phone registered to that number, so all of the SMS-based second-factor authentication went to the attacker's phone, rather than to Cody's physical phone, and they then basically bypassed the second-factor, the SMS second-factor authentication and had their way with him.



So we've been talking now for quite a while.  I mentioned this first when I moved finally from Network Solutions over to Hover as my domain registrar, and I was very glad to see that Hover gave me the option.  It said, you know, do you want to use multifactor authentication?  I said yes because my domains are very important to me.  And they said, you know, they gave me a choice:  send an SMS message or use time-based token authentication.  And I immediately choose time-based because, as I have explained many times on the podcast, the problem with SMS, even if it was more secure than we know it is, is that it is fundamentally weaker because it requires a per authentication communication, rather than a one-time setup of a shared secret, which then is used to forever derive, you know, prove that you both know the shared secret based on an algorithm and time in order to produce the six-digit token that changes every 30 seconds.  So here again is a perfect example of the deliberate exploitation of SMS-based second factor and how it can be bypassed.



But the other part of this, again, where I'm like testing myself with SQRL, how do we prevent that, too.  And the good news is it's already in there.  In the show notes is a snapshot of SQRL's Settings & Options dialogue.  And there's two checkboxes which are not on by default.  That is, when you start using SQRL, they're not on, because I want to wait for people to understand how it works and get comfortable with it, and then they have the option of turning them on.  The first one is labeled "Request only SQRL login."  And the second one is "Request no SQRL bypass."  And I labeled it that way because those are flags which are provided as part of SQRL's interaction with the authentication site, with the server.



And so they're regarded as preferences, but they allow the user to express their request, their preference, to disable alternative forms of login, meaning username and password won't work anymore and so forth, and also to request that even SQRL not be bypassed.  That is, formally say I am going to take responsibility for my use of this system, and I want no recourse.  So in other words, if this was set, no person at Verizon would have the ability, that is, the technology would block them.  They would not have the ability to respond to an attacker under any circumstances saying, oops, I lost my SQRL identity, sorry, here's a new one.  They would say, I'm sorry, we don't have the ability to do that for you because you told us to turn that off.



So again, this is meant to be a true bulletproof solution.  And again, we're not forcing anyone to do this.  It's off by default.  But once you understand how this works, you see that it's working and you've been using it for a while, you decide, hey, you know, I get it, then you can turn this on.  And as you then log into sites with those flags set, that persistently sets, sort of leads that trail in the site that you visited, saying I'm going to take responsibility for my authentication to your service.



PADRE:  Right.  And this is something that we've known for a while, that the weak link is that human interaction who can decide arbitrarily to not follow the procedures.  In fact, you know, this is one of those cases where clearly Verizon is at fault here.  They did not follow any of the procedures.  Unfortunately, I don't think we actually have any rights to force them to follow the procedures that they say that they were going to do.



STEVE:  Correct.



PADRE:  Interestingly enough, a few years back I was doing a little bit of work for my other organization, and I was doing a mass swap of a lot of SIM cards for some of ours who were going overseas.  And I was dealing with one of the big four carriers.  And the first one that I did, all he asked for was the IMEI.  I didn't need the phone number.  I didn't need the account number.  I didn't need the account name.  Just the IMEI that it was currently on, and the IMEI that I wanted to switch to.  And it struck me, I was like, that is the worst authentication I've ever seen.  I can pull the IMEI off of any of a number of data sources.  And are you telling me that if I'm just pretending to be the IT person for this person, I can suddenly start getting all the traffic that he or she was supposed to get on their phone?



STEVE:  And it goes - IMEI goes through the air.



PADRE:  Yeah, exactly.  It's cleartext.  If you're playing man in the middle, you know exactly what is talking to you.  And that's one of the easiest to obtain pieces of information.  And the fact that they were willing to do something as radical as switching the service to a different phone, without even challenging one bit of information that I should know about the account, that just - I lost all confidence in their security procedures.  But on the other hand, the person was trying to be nice.  I mean, that's the thing.  They were a salesperson.  They're like, okay, of course, we want to give you a good customer service experience here.  We're going to make this as easy for you as possible. 



STEVE:  Right.



PADRE:  And I get that, and I appreciate that.  But then the IT person in me is saying this should not be happening right now.  And I did 20 of them in, like, 10 minutes, without ever mentioning a phone number or name.



STEVE:  Wow.  Wow.



PADRE:  This is actually - I put this hand in hand with something that one of my law enforcement friends was telling me, saying that mobile phones and mobile devices has been the biggest boon to law enforcement in the history of law enforcement because people just assume that they're secure when they're not.  And people just assume that they can put personal information on it, and it will be protected, and it won't be.



STEVE:  Well, and of course that's the argument against law enforcement saying they need access to decrypted data.  I mean, there's never been more access to data than there is today.  I mean, it's just - it's a cornucopia of stuff.  We have all of our IoT stuff, which is sucking in data.  And we've got commercial companies tracking us and profiling us and building up information.  And we know that in the U.S., at least, thanks to the Patriot Act, we have the government able to say that they have cause and need for the disclosure of some of that information, anything that a company has available to it.



So, yeah, I mean, the fact is, I mean, I understand the "going dark" problem.  We discuss it here all the time.  When I see another attack, another terrorist incident, as we had recently again in the U.K., and Theresa May gets up there and says "We need to be working with Silicon Valley, we need to be working with the technology companies, we cannot allow the bad guys to hide," I just, you know, we're marching towards a day where there's going to have to be some sort of compromise made, where Apple is not going to be able to say we have no access, we cannot get access to our customers' data.  I think we're going to end up seeing legislation with some sort of a multiyear horizon by which time these companies have to be able to make that available.  And we also know that the crypto horse is out of the barn.  I mean, once that happens, the bad guys will use crypto that doesn't have a means for a corporate sponsor to provide visibility.  So I think it's going to all be for naught.



PADRE:  The last time I came back into the country was just a couple of weeks ago.  And I have global entry, so I've got the little card that allows me to go quickly through.  But they can still pull you aside for secondary.  And so I get pulled aside for secondary, and so they wanted to see my phone.  And I have a different phone that I use when I'm international.  It's a burner phone.



STEVE:  Do you wear your collar when you're traveling?



PADRE:  I do, I do.



STEVE:  And even so.



PADRE:  Even so.  I think the collar really attracts them.



STEVE:  It could be a disguise, I suppose.



PADRE:  I think so, exactly.  But so the first thing they do is they said, "Well, your phone seems to have a fingerprint scanner, but can you open it with your fingerprint?"  I said, "No, it's a PIN."  And they said, "Well, why don't you have a fingerprint?"  I said, "I don't want people to be able to open it with just my fingerprint."  And so that was kind of a red flag.  And so I opened it for them.  And of course everything looks right.  It's got an email account.  It's got text messages.  They're not my main accounts.  They're sort of the burner accounts I've got.



STEVE:  Uh-huh.



PADRE:  And then they're looking through it for a few minutes, and the agent comes back and says, "Do you happen to have Dropbox and OneDrive?"  And I'm looking at him going, you're hoping I have the app on my phone so you can go through my personal documents.  I mean, that is horrible.  That is completely out of control.  But, yeah, you're right, this is a gateway.  And so if you give people an unwarranted way into everything that is connected to you, that cannot end well.  I don't know, Steve.



STEVE:  Wow.  I know.



PADRE:  I think I'm going to go back to writing in Latin on pen and paper.



STEVE:  Or I guess can you maybe just FedEx the phone to yourself before you get on the plane?



PADRE:  We're probably going to have to do that because once they ban laptops...



STEVE:  And just not have the technology.  Wow.



PADRE:  Well, it's interesting because on my way back, so I flew through Amsterdam, and they actually asked me when I was in Rome, they asked me to check my laptop because they were afraid that new regulations would come through while I was in the air.  It was like a 22-hour trip.  And they said, "We don't want you to have to hand in your laptop in Amsterdam."  And so I checked in my laptop.  But now they're saying they may not even allow that.  And I'm thinking, so what good is air travel to me as a technology person, if I'm not allowed to bring any technology with me?



STEVE:  Wow.



PADRE:  That's silly.  All right, Steve. So we've talked a little bit about exploits, exploits, and more bad news.  Can you give me something positive?



STEVE:  Not yet.  We've got that coming up.  But John Matherly is the founder of Shodan, that is the search engine that just - it's the gift that keeps on giving.  As we know, much as Google indexes websites and allows us to find stuff, Shodan has indexed essentially, or you might say that Google indexes port 80 and 443, that is, HTTP and HTTPS.  Shodan indexes everything else.  And so it's a search engine sort of slash database that scans the public-facing Internet IP space looking for things that answer all of the other ports.  We've got ports 1 through 65535.  And so, for example, it's the thing that, if some security camera runs a server on port 6273, you can say, hey, Shodan, what IP addresses have something listening on 6273, and it gives you a list.



Well, it turns out that it's also scriptable, and John wrote a script, the founder of Shodan wrote a script last week because he just kind of got curious.  He wanted to find out how many, if any, HDFS protocol servers were there.  HDFS is the Hadoop Distributed File System.  Hadoop is an open source Apache-based evolution of a project that Google originated many years ago to experiment with large dataset, large cluster computing in an open source environment.  So thus Apache-based, and it's like a big dataset processing facility.  And Hadoop is just sort of a fun word.  It's not an acronym.  It was actually named for the elephant toy belonging to the son of one of the project's early originators.  So the elephant's name was Hadoop, and that became the name of this system.



So John writes this script that queries his own Shodan backend database and discovers 4,487, right, 4,487 public IP instances of HDFS because it runs on a set of well-known ports - 50070, 50075, 50090, 50105, and so forth, like that.  There's, like, seven of them.  And then he, using his script, queries those 4,487 instances of HDFS for, like, what's there.  I mean, these are publicly facing, unauthenticated, no security databases exposing - and this is where I tell Leo to make sure he's centered over his ball - 5,120 terabytes.  That's 5.1 petabytes of big data.  It's just - oh, lord.  And so, okay.  So that begs the question to me, wait a minute.  No security?  No authentication?  How is that possible?



PADRE:  These are default installations.  These are all default installations.



STEVE:  Yes.



PADRE:  Oh, my goodness.



STEVE:  I dig into it a little bit, and I learn that setting up HDFS security is a bit of a nightmare.  You know, it's open source.  It's public.  It's oh, well, yeah.  Turns out that it requires far, far more configuration involving the Kerberos authentication service and servers.  It is not easy to do.  So it typically doesn't happen.  I imagine if it were simple to do, people would go, oh, yeah, why not?  But it's not.  It turns out it's really difficult.  The way the system was built, it is difficult to make them secure.  And so people are like, uh, well, okay, once anything important is in there, maybe we'll think about adding that.  And it never happens.  Unbelievable.



PADRE:  You know, this is a lot like - was it last week that a researcher found a bunch of AWS baskets that weren't encrypted and completely open.  And in fact one of those baskets contained a lot of information from a security contractor who was also dealing with big data and big data analytics.  Big data tools aren't new, and they install a lot like the tools that we've already used, which means that there is a way to just keep hitting Enter and get all the default settings.  But the default settings...



STEVE:  Yeah, yeah, yeah, yeah, yeah, yeah, yeah.



PADRE:  Those are for dev environments, not for production environments.



STEVE:  Right.



PADRE:  Oh, Steve, Steve.



STEVE:  I know.  Under the topic of "We should have seen this one coming," it turns out - and this one was a bit of a blast from the past.  Check Point discovered a startlingly high incidence of - they're calling it malware.  It was exactly what actually caused me to coin the term, as Leo often reminds people, that I coined the term "spyware."  As far as I know, 17 years ago that term didn't exist.  But let me back up a little bit.



So there is a Chinese digital marketer named Rafotech, R-A-F-O-T-E-C-H, who are behind the spread of a malware family which has been [audio dropout] Fireball.  It's apparently installed in - and Check Point's number was 250 million web browsers.  Rafotech's own PR claims 300 million.  So I think the 250 million is probably a good number - 250 million.  A quarter of a billion web browsers have this stuff.  It's ad revenue-generating zombies.  Rafotech- or Fireball-laced apps, Check Point says, have infected 20% of corporate networks around the world.



Check Point explained in their report published last Thursday that the malware hijacks browsers and generates revenue for this Beijing-based digital marketing agency.  They termed this "possibly the largest infection operation in history," adding that the Fireball infections could be turned into a distributor of any other malware family because this thing has the ability, if it chose, to download anything into these 250 million computers where it is resident, if it wished to.



So, okay.  This, as I said, reminded me of 17 years ago, back in March of 2000, a little over 17 years ago, after I had downloaded and installed the very popular at the time WinZip freeware.  I had been using Phil Katz's PKZIP utilities for a long time.  But Windows was happening 17 years ago, and I thought, okay, I'll take a look at WinZip.  So I download it, look at it, and it was like, eh, okay.  And it didn't really move me, so I uninstalled it.  And then sometime later I noticed something foreign running in my machine.



It turned out it was this stuff called Aureate, A-U-R-E-A-T-E, which was adware, which various freeware apps at the time were bundling.  And I coined the term "spyware" because, aside from just - the idea was that this WinZip, when it was onscreen, would have a window in it, much like browsers now present advertising.  This would be a window showing an ad hosted from the Aureate servers.  So this thing would be - so there was an Aureate DLL running persistently.  It would reach out and obtain ads from this Aureate ad network.



Turns out that the dialogue was in the clear.  I put a packet capture on it and found it was also logging all the software that I was using, everything I was doing on my computer, and sending that back in reports to the Aureate home, the Aureate mothership.  Thus spyware.



And so I wrote something called OptOut, which allowed you to surgically remove this from your machine.  And it turned out that a vast number of people had this installed in their machines without knowing it and that Aureate's specific instructions, even though I had uninstalled WinZip, they said when you uninstall the freeware which brought it, do not remove us because other freeware may be sharing the same installation instance, and we wouldn't want them to have their revenue system removed.  So it was insidious.



And of course that was 17 years ago.  Now here we are in 2017, and we have something very similar.  This is brought into people's machines with a bunch of freeware which is like, here, download this, and it'll do something for you.  It of course takes over people's browsers, installs a search surface in front of their browser that tells them they're going to get all kinds of benefits and things.  And in fact it's used to generate revenue for this company and to forward their search queries onto existing search services behind the scenes.  So I doubt that our users are being affected by this, but keep on the lookout because it is everywhere. 



PADRE:  I wouldn't be surprised because my father, who is tech savvy, has this happen to him quite a lot.  In fact, every time I go to visit him...



STEVE:  His browser's been commandeered?



PADRE:  I'm basically rolling back his computer.  I've got images.  I don't even count on uninstalling.  I just go back to an old image.  And it's always, "Well, I downloaded a screensaver that shows me pictures of my grandchildren."  And I'm like, "Okay, Dad, but there's other stuff that installs, and you can't do this."  He goes, "But it works."  And that's the thing.  They bundle with software that works.  So when you use it, your fears are assuaged.  You just go, okay, it must be legit.



There's a corollary to this.  There was a study that was done, I want to say three years ago, that they showed us at Black Hat.  They did a very convincing video.  They wanted to see if people's behaviors would change if the install packager told you exactly what was being installed, versus surreptitiously trying to install something on your computer.



STEVE:  And nobody cares.  I know.



PADRE:  No one cared.  They would just okay, okay, okay.  Now, occasionally someone who noticed there was an extra checkmark, do I want to install this, this, and this, and they'd go and they'd uncheck it.  But by far most people, regardless, I mean, it could actually tell them we will be installing software that will spy on you and redirect your browser, and people would still click through.  That's not fixable with technology.  That's a pay attention to what you're doing because this is important.



STEVE:  Yup.



PADRE:  Steve, is there any hope?



STEVE:  Well, not yet.  In fact, we have another instance - we've been talking recently about Google's problems with Chrome and things that they say are not bugs.  Now, in the same way that LastPass has been a victim to some degree of its own success, meaning that it's what Tavis is thinking about when he's in the shower, similarly Chrome is becoming its own victim of its own success.  It's now the majority browser on the Internet.  So of course it's what people are looking at more closely.  Modern web browsers, that is, those that support the features of HTML5, are able to use the standardized WebRTC features in order to create web page-based audio processing and audio/video teleconferencing apps and so forth, without the need for a third-party add-on.  So whereas once upon a time you needed to use a Flash-based app or Silverlight or a custom add-on to your browser, now it's in there.



The problem is that there is a system where the site must get your permission on a per-site basis to enable WebRTC.  So there was some attention in the spec paid to the implementation.  The problem is the permission is sticky, and it remains until explicitly revoked.  So it's one of those things where you'd want to perform an audit on your own from time to time of the set of apps that you have given, you have previously given permission to.  You'd want to curate that list.  And we've talked about this, how for example sometimes you want to go into your Twitter account and look at the different apps you have given access to your Twitter account to and say, you know, a few of them you'll still be using, but over time they just sort of fall by the wayside.  Yet unless we explicitly remove permission, it just doesn't go away.



So here's the problem in Chrome.  The Chrome UI, technically it's called the browser Chrome, so the Chrome Chrome, shows a red recording indicator, a red dot with a red circle around the red dot, on the tab of any page which has the WebRTC actively operating.  The problem is that sites are able to bring up pop-ups that don't have tabs, and they're not only pop-ups.  They can be pop-unders so that you don't even see that a website that you're viewing or a site that you have open in your browser has popped under the browser and turned on WebRTC streaming without your knowledge or permission, or that is to say, without your knowledge and current permission.  That is, at some point in the past you had to have enabled it.  But unless you explicitly disabled it, it still has permission.



And so a researcher went to Google with a proof of concept demonstrating that it was possible to generate a non-tabbed pop-under which could surreptitiously record audio and video with no visual indication whatsoever.  And oh, by the way, mobile version of Chrome doesn't even put the red dot on the tab.  So eight weeks ago, on April 10th, he opened the dialogue.  Google replied that it was not a bug because it's not a valid security issue, and so they have no plans to change this.  He did a complete presentation in Medium.com.  In the show notes I have his proof-of-concept demo where you can go there and see all this for yourself.  Basically it is a no-UI 30-second audio recording which you can then click a button to download the results and demonstrate that your Google Chrome browser is able to do this without any explicit indication that it is doing so.  And Google says, yeah, that's not a bug, that's a feature.



PADRE:  One of the projects I made a long time ago, and actually I never showed this on Know How, was a kill switch for all audio and video devices.  So it was an actual hard - not software, so you can't bypass it.  It is a hardware kill.  



STEVE:  Nice.  Nice.



PADRE:  So it will activate the capture devices, but they will display or show nothing because the actual physical device has been disconnected.  I think I need to bring that back.  I put that in a storage closet somewhere.  But I think maybe...



STEVE:  That would be a great topic.



PADRE:  Yup.  So are you saying, so I can pop-under the browser window, and then just avoid the whole thing of the microphone showing up on the tab? 



STEVE:  Correct.



PADRE:  Remember when we thought we had solved the pop-up and pop-under problem, like eight years ago?



STEVE:  Yeah.



PADRE:  Wait.  Didn't we solve the pop-over and pop-under problem?



STEVE:  Apparently not.



PADRE:  Steve, before we go on, you know what I need to have, because I need this every time we do this show together, I need you to talk about SpinRite.



STEVE:  Well, thank you.  I do have a fun little comment here, somewhere down here.  We're getting there.  I'll scroll down to it.  Ah.  It was two tweets.  Someone named Al Spaulding, who is a frequent Twitter acquaintance of mine, on the 3rd - so, what, three days ago - he sent two tweets.  He said:  "Steve, another SpinRite story."  He said:  "I started getting Outlook errors like 'can't load profile,' and eventually Outlook would not load at all."  He said: "Company IT looked at it and created a new profile."  He says:  "That worked for about 24 hours.  Then it all started again."  He said in a second tweet, that was the first one, followed up with:  "Finally ran SpinRite and no problems for five days now.  Thanks for a great product."  So Al, thanks for sharing that with me and our listeners.  Appreciate it.



PADRE:  Well, Steve, I've actually got a SpinRite story from the last week. 



STEVE:  Ah.



PADRE:  I did all the WannaCry segments for Know How.  And I use VMs in my lab at home because I have a nice VMware cluster.  There's no way I'm going to tear that thing out and bring it to the studio every time I want to do a demo.  So I actually have some physical lab kits which is basically a motherboard, some memory, and some SSDs that I swap in and out because I have the same image on all of them.  And I had two Samsung SSDs that stopped imaging.  They would no longer image.  And I thought, oh, maybe they've reached the end of their useful life.  Ran SpinRite, was it Level 2?



STEVE:  Probably, yes.



PADRE:  On those SSDs.  And now they're back up to full capacity and back up to full speed.



STEVE:  Nice.



PADRE:  So still works for me.



STEVE:  Yay.  I love it.  In fact...



PADRE:  I can't even say it's - it's not even my emergency tool anymore because it's one of the first things I do.  If I'm having problems with a storage device, just pop out SpinRite.  That's the very first thing that works.



STEVE:  It's just your go-to, yeah.



PADRE:  It's my go-to now.



STEVE:  Yeah.  In fact, all of us who have had experience with it, that's, I mean, that's what we do because it just works.



PADRE:  Just works.



STEVE:  Very cool.



PADRE:  Now, we can't tease the audience with "homomorphic encryption" and not deliver.  And I think it's time for some serious math.  So you want to drop the math on us?



STEVE:  Okay.  So we've never talked about homomorphic encryption.  And last week I shared the news, which was a little bit disturbing, that Google had come up with a way of further measuring the effectiveness of their online ads by entering into some sort of a data sharing arrangement with physical credit card processors such that they could determine our identity and associate our offline purchases with our online presence, the idea being that, if we were at ad sites - and of course this is sort of the holy grail for advertising.  If we're browsing around the web, looking at ad sites, being influenced by online advertising, yet not taking action online, but rather deferring the action to a subsequent physical purchase in the real world, how do they get credit for the influence of the ad?



Google wants to cross that last bridge.  And so it was somewhat worrisome that their reach was as far as it was.  I think I remember saying that they now had access to 70% of credit card transactions that would allow them to create this association.  So the question was how is this being done in a privacy-respecting fashion?  So homomorphic encryption is a way of encrypting data such that its encrypted form can still be used.  That is, normally we think of something that's encrypted as being data in flight, or maybe data at rest, that is, like stored.  But the point is you can't - you can never use it in its encrypted form because it's, as we know, it's just pseudorandom noise.  If it's properly encrypted, it's just nothing.  It's noise.  And so we have to arrange to decrypt it in order to bring it back to its original useful form.



Homomorphic encryption changes that.  It's a family of technologies that allows work to be done in the ciphertext form.  Wikipedia, for example, defines it as:  "Homomorphic encryption is a form of encryption that allows computations to be carried out on ciphertext, thus generating an encrypted result which, when decrypted, matches the result of operations performed on the plaintext."  Which is freaky.  So imagine that you encrypted something, and then you added something, like arithmetically added something, such that the result was still encrypted, but modified.  But when you decrypted it, the decrypted result also had the result of that addition.  Which is like, what?



PADRE:  Okay.



STEVE:  It's like, what?



PADRE:  So this is like an encryption chain.  So essentially it allows me to create an encryption and then keep adding to it.  So, for example, multiple services on the Internet that I might be using, they can add little bits and pieces, and they can have access to some of it, but not all of it.  Is that how that works?



STEVE:  Yeah.  So this comes from the blog posting of someone who listens to Security Now!, who heard me talking about this last week.  He actually is with a company that is involved in doing this.  And so here's what he wrote, and then I'll explain it a little more.  He says:  "We needed to share some data with another company, and this related to credit card transactions.  But we did not want to share the actual card numbers.  They're known in the industry as PANs, Primary Account Numbers.  What to do?  What we came up with was quite neat and can probably be used by others.  The external company collects the card numbers they want information on.  They encrypt these under RSA with a key they generate and never share."  And I should mention, I forgot, I've edited this a little bit for clarity and to use some terms that our listeners will be familiar with.



"They send these encrypted numbers to us.  We further encrypt them with our own RSA key and securely shuffle the order of the entries to completely confuse them so that they are now reordered" - they're cryptographically securely reordered - "and then return them to their originator."  So now they have a set of these credit card numbers, double-encrypted.  He says:  "Then we perform an extract of the relevant transactions and encrypt the credit card numbers with our RSA key and send these as well."



PADRE:  Okay.



STEVE:  Now the recipients of these can encrypt these keys with theirs because RSA is a commutative function.  So in other words, in the same way that A times B times C gives you the same result as A times C times B, that is, the order doesn't matter.  And so what these guys did was they took the original credit card numbers that were encrypted by someone else.  Then they got them and encrypted them with theirs, scrambled them to reorder them so that the originator could not associate them, and they gave them back to them.



Then they took a bunch of information that they wanted, encrypted the credit card numbers with their RSA key, and provided that to the original group.  They added their encryption to the already once encrypted by the other guys' RSA.  And that meant then that the numbers matched up.  That is, so that first company started with credit card numbers and received back doubly encrypted ones out of order.  Then when they similarly doubly encrypted the credit card numbers provided by the second company, they matched up again.  And so that allowed them to perform the data analysis without ever revealing the actual credit card numbers.  Those were blinded by this double-encryption process, which I thought was very clever.



PADRE:  And at the end of chain, when they decrypt, they get everything.  So they do one decryption, and they get everything?  Now, the question is, for me, does that change the original data that was inside of that original encrypt?  Or do they get the different layers, and then they can perform the operation?  I guess I'm a little confused about that.



STEVE:  So the better way to think of it is it's a little bit like a hash chain, except hashing is not commutative.  So if you took something and hashed it once and then - or like an HMAC.  So like a keyed hash.  If you used a keyed hash and hashed it, and then a different key and hashed it again, you'd get a result.  But if you swapped the order, you would not get the same result.  So hashing, keyed hashing is not commutative.  But RSA encryption is.



PADRE:  Okay.  Yeah, I get it.



STEVE:  And so it's not a reversible process in the same way that hashing is not reversible.  But it is a forward-moving commutative process.  So it doesn't matter which order you double encrypt.  Either order will generate the same result, in the same way that hashing is a forward-only process.  This is all forward-only, but it provides a clever technique for blinding someone to some data which you still want them to be able to compare.  So, very clever.



PADRE:  Very clever.  Someone was comparing it to the block chain, but block chain works a little bit differently than that.



STEVE:  Yeah, yeah.



PADRE:  But the same kind of idea where everyone can add their little bit and piece without having to see the entire chain of encrypted data.



STEVE:  Right.  And in this case the fact that the order in which they add it doesn't matter, that allows this clever hack.  And finally...



PADRE:  That's fun.  I want to play with that now.



STEVE:  Yeah.  And in fact I've got Stuart's entry in the show notes, and he shows the math, the detailed exponentiation RSA math, and demonstrates how mathematically you can show that what they're doing survives commutation.  So, very cool.



And finally - @Really_Evil_Rob is his handle.  I don't think he really is.  He sent me a note on the 2nd saying:  "@SGgrc I've been thinking about your system of printed QR codes for your authenticator tokens.  What about saving them to a USB flash drive?"  Now, just to back up a little bit, Father, to loop you in on this, I've been talking about how, first of all, time-based tokens are what we want, not SMS-based tokens.  And what we want is authenticator apps that refuse to export because they contain private keys which need to be kept secret.



So the thing you want, especially if you're going to go through the border crossing, or a particularly, as I put it last week, officious-looking TSA agent you're about to be confronted with, you don't want your authenticator app to be willing to export your keys because that means anyone could get them.  And then what they have is the keys to the kingdom.  They've got all of your future time-based tokens forever.  So what I explained was you first want to use apps that refuse to export.



And when you are establishing an account, as I did for example with Hover, invariably they'll show you a QR code, which you, if you were setting up your authenticator app, you would snap a picture.  I instead said, no, print the page.  Print it.  Put it on paper.  Because we don't, in this day and age, have just one device.  I've got a phone.  I've got iPads.  I might have a Windows or a Linux-based authenticator.  The point is they all support optical input.  And what you want them to do is not to support optical output, or even text-based output.  The idea being that every time you're setting up an account, you print the page.  And you end up with a sheaf of papers.  And so the idea was you store that offline.



And so Robert's question made me realize that I hadn't clearly articulated why I thought this was practical.  The reason it's practical is that it's the tradeoff in security and convenience.  And as we've said on this podcast, and as we're always saying, there is no question that there's always a security/convenience tradeoff.  So here, if you're setting up a new device - for example, I am excited about the new iPad.  One is on its way.  The sort of the mid-sized iPad Pro looks like the right one for me.  I did not - I haven't upgraded for years because it hasn't been right.  This new one will be.  I am a time-based token authenticator user.  I will be setting up a new iPad.  And I like Google's Auth app.  I don't like LastPass's because it's just too - the entries are too big.  I like smaller entries because I can get more on the screen.  And there's another one I haven't taken a look at yet.  But I'll be setting up this new Pad.



So I have printed out a sheaf of QR codes.  And I simply snap - and setting up the new one, I let it see them each in turn, and now it's set up.  And then I put them back in a drawer.  So, yes, I am responsible now for maintaining these securely offline.  But this is the point that I wanted to make to Robert and other listeners.  No, I don't want them on a USB flash drive.  I want them offline.  They are too valuable to me to have in any electronic form.  Electronic is not safe.  It was like you were saying, switching back to writing in Latin for your notes.  So this is a place where it wants to be offline because it's practical.  It's not like you need to refer to them every time you authenticate.  No.  It's only every time you are setting up a new device.  And that's something we do infrequently.



So you want the security of, I mean, the absolute security of it being offline, not on a USB drive where you're going to insert it into something, and that something could have malware on it, waiting for you to stick your drive in and then suck the contents off the drive.  Or malware running, and it encrypts the contents and demands ransom.  No.  This is a place where the tradeoff says, you know, take a picture when it's a QR code being presented by the site where you're establishing a new  time-based token and add it to your pile of offline stored papers, which you know where they are.  And the point is, yes, you need to provide some physical security to those.  But that's way better.  That's something we have control over to a much greater degree than anything involving online storage and technology.



PADRE:  You know, Steve, this is a little bit of the security of inconvenience.  We should get ourselves into the mindset that, when we have a little bit of inconvenience, especially when setting up a new device or a newly authenticated piece of gear that needs to use our credentials, if you do not experience that inconvenience, you don't have enough security.  It's too easy.



STEVE:  Well, it's like every website needing its own password.  That's incredibly inconvenient.  It's also way more secure.



PADRE:  Exactly.  Exactly.  But, I mean, the problem is our brains are not yet wired to have that reward.  We think of, wow, man, I shouldn't have to go through this many steps to set up a new phone, where it should be, oh, okay.  If it's this difficult for me, and I have all my credentials, it will be more difficult for someone who's trying to attack me, they're going to go after a lower hanging fruit than I.



STEVE:  Right.



PADRE:  That's a better way to think of it.  But we don't because we're lazy.  I'm lazy.  I'm incredibly lazy.  Steve Gibson, he is my security guru.  He is a man who I turn to anytime I need things like maths explained to me.  Thank you so very much for sharing your time with us.  I always consider it an honor whenever I get to co-host the show with you.



STEVE:  Hey, thank you for standing in.  And we get you next week; don't we?



PADRE:  Yes.



STEVE:  You're still here?



PADRE:  Oh, yeah, absolutely.  In fact, 30 minutes after we finish next week's show is when I head off to be in silence for two months.  So it will be the last show I do before I go in.



STEVE:  In that case,  we're going to talk you into silence next week.



PADRE:  Well, it should take me - hopefully we'll do a show that takes me two months to decipher.



STEVE:  Recover from.  Perfect.



PADRE:  Folks, that does it for this episode of Security Now!.  Don't forget that we are live here on the TWiT.tv network every Tuesday at 13:30 Pacific time.  Steve is always here to be able to inject you with some healthy paranoia - and yes, folks, it is healthy - and help you to understand the wonderful world of security.  You can find all our shows at the show page at TWiT.tv/sn, as well as iTunes, Stitcher, and wherever fine podcasts are aggregated.  You can also find high-quality audio downloads on GRC.com, which is also where you'll find everything about GRC.  That's SpinRite, that's ShieldsUP!, and everything about SQRL.  I'm Father Robert Ballecer, the Digital Jesuit, saying that if you want to keep your data going into the future, it's time for Security Now!. 



STEVE:  Thanks, Padre.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#616

DATE:		June 13, 2017

TITLE:		Things Are Getting Worse

HOSTS:	Steve Gibson & Father Robert Ballecer

SOURCE:	https://media.GRC.com/sn/SN-616.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  This week we discuss clever malware hiding its social media communications.  The NSA documents the Russian election hacking two-factor authentication bypass; meanwhile, other Russian attackers leverage Google's own infrastructure to hide their spoofing.  Tavis finds more problems in Microsoft's anti-malware protection; a cryptocurrency stealing malware; more concerns over widespread Internet-connected camera design; malware found to be exploiting Intel's AMT motherboard features; the new danger of mouse-cursor hovering; Apple's iCloud sync security claims; Azure changes their CA; a bunch of catch-up miscellany; and a bit of "closing the loop" feedback from our listeners.



SHOW TEASE:  It's time for Security Now! with Steve Gibson.  Malware that uses social media as a command-and-control node.  Microsoft has a new malware vulnerability in their malware vulnerability engine.  The fastest way to lose more than $30,000 in bitcoins.  And you know how you tell your friends and family just not to click on anything suspicious?  Yeah, that's not good enough anymore.  Security Now! is next.



FATHER ROBERT BALLECER:  This is Security Now! with Steve Gibson, Episode 616, recorded June 13th, 2017:  Things Are Getting Worse.



This is Security Now!, where the only thing to fear is fear itself, and of course everything else that could potentially destroy your digital life.  When the dark forces of insecurity gather up their exploits and breach toolkits, that's when we bring on this man.  That's right, Steve Gibson.  He is the big brain behind Gibson Research, ShieldsUP!, SpinRite, and SQRL, the man who can relieve you of your fear with a healthy dose of warranted paranoia.  I'm Father Robert Ballecer, the Digital Jesuit, in for Leo Laporte.  Steve, it's so good to see you, my friend.



STEVE GIBSON:  Well, likewise, for our second of our two weeks together while Leo's off gallivanting around the Galapagos, I guess.  So this was sort of a different week.  You know that last week's podcast we titled, I think it was "Legacy's Long Tail."  And looking over what we had to talk about this week, I just - I sort of had, metaphorically, my face in my hands and just decided, okay, we just have to call this one "Things Are Getting Worse."  Because I think the forces of security are losing this battle.



We're going to discuss a clever malware which is, I mean, and we're going to look at the technology of this because it's insidious.  It's hiding its intercommunications within social media posting comments.  We've got a leaked NSA document about the Russian election hacking and how they "bypassed" two-factor authentication where that was in place.  So, whoops, not quite providing the security that we were hoping.  Meanwhile, other Russian hackers are leveraging Google's own infrastructure  to hide their spoofed websites.



Tavis, our friend Tavis Ormandy of course with Google's Project Zero, has found additional problems with Microsoft's antimalware protection engine.  And we'll remember that in May there was an emergency out-of-cycle update to fix something that he had found because the last thing you want is the antimalware filter, which is ubiquitous across Microsoft's platforms, to have a remotely exploitable vulnerability, which is what he found and Microsoft immediately fixed.  Well, there's another one.



We also have cryptocurrency-stealing malware, more concerns over widespread Internet-connected camera design, malware found to be exploiting Intel's AMT motherboard features, which is a concern we've been talking about on this podcast now for a couple years.  There's this off-the-map processor which is in the chipset, but which is undocumented, or very, very underdocumented, and stays on even when the system is off.  And I know you with your other podcast focusing on enterprise computing have looked at this a lot, Padre.



PADRE:  Oh, yeah.



STEVE:  Then we also have a new danger of mouse cursor hovering.  No longer is it enough not to click on the link.  Turns out you can't even hover over a link.  Then there's Apple's somewhat questionable iCloud sync security claims.  The news of Azure changing their certificate authority to one that may not surprise our listeners of the podcast.  And we have a bunch of catch-up miscellany and some "closing the loop" feedback with our listeners.  So I think another jam-packed couple hours.



PADRE:  You know, Steve, one of these days I'm going to sub in for Leo, and you're just going to say, hey, you know what, we've solved everything.  There's no security.  Let's just all go home.



STEVE:  Goodnight.



PADRE:  Yeah, that will not be this week.	



STEVE:  Actually, here we are at Episode 616, in our 12th year,  And Leo and I, you know, when he initially proposed this, I had flown up to Toronto to do a day's worth of recordings for Call For Help.  And during the pause between shows, because we did like four in one day, we were just sort of kicking back while they rewound their VCRs.  I mean, things were going to tape back then.  And he said, "What would you think about doing a weekly podcast, maybe 15 minutes, on security?"  And I said, "A what cast?"  Because I had never heard the term before.  And I remember thinking, oh, please don't bring that up again, because it just sounded like more than I needed.



Well, needless to say, it's been a significant win for everybody.  Our listeners appreciate pulling the week's events together, finding the top ones.  And I need to remind everyone that my Twitter followers, and those who even don't follow, but who send me tweets of things that they see occurring that they would like this podcast to cover, that I go through the entire prior week pulling topics together and filtering through what I think are the most significant events.  So, I mean, it really is an interactive driven podcast, for which I am very grateful.



PADRE:  You know what's very kind of sad, Steve, is the fact that, as we move forward, we see new security vulnerabilities, new exploits, but the old ones never go away.  They're always sticking around in some form or another.  In fact, I just got rid of a Conficker infestation at the school that I live at.  And I'm thinking Conficker?  What systems do we even have that are still vulnerable to that?  And sure enough, there were some old student computers that were, and they were hammering the network.  And I'm thinking this is pretty much what happens in security, where all we do is add new ones.  We never get rid of the old ones.



STEVE:  Well, and back in the early days, when it was sort of quaint to capture packets that were like out on the public Internet, I coined the term "IBR," Internet Background Radiation, because I realized that this stuff was never going to go away.  There's all this good traffic, point to point, intentional, intended, deliberate traffic.  But there's also sort of this underlying hiss of background radiation which are packets coming from people's closets, increasingly now unfortunately IP-connected cameras; but VCRs, routers have been compromised.  So now we have all this IoT problem and growth from that.



But back in the day, as they say, it used to be servers that had long been forgotten that would get compromised and then would never get rebooted.  And so they'd get MSBlast-infected, and then that infestation or that infection would then start in turn scanning the Internet and so thus creating a background of radiation such that, if any vulnerable machine happened to pop up on the public IPv4 space, before long, if it had any vulnerabilities that hadn't been fixed, it would get commandeered.  And that's the world we live in today.



PADRE:  Five years ago, when I was still working with Interop, we owned a Class A.  It was the 45-dot Class A in IPv4 space.



STEVE:  Nice, nice.



PADRE:  And anytime we lit it up, at any show, almost immediately at least a gigabit of noise.  Remember, this is five years ago.  So that was a whole heck of a lot.  We had to block it upstream.  And that was just all the automated scanning because our range only came up during the shows.  But it was one of those things where you're like, wow, there's no humans involved here.  These are just machines that just keep scanning the same address ranges over and over again.



STEVE:  Well, and there was that CAIDA group, C-A-I-D-A.  They also maintained large blocks of space that had never had any IPs assigned.  And they [audio dropout] DDoS attacks by picking up the reflected packets because attackers were spoofing their source SYN packets.  They would be bouncing off of machines under attack.  And then those that happened, just because the source IPs were randomly generated, all of the ones that happened to fall within their unallocated block of IPs would come in with the IP of the remote target attack that was doing everything it could to respond to the inbound spoofed SYNs with SYN ACKs.  So they were able to give us our first look at the early days of DoS and DDoS attacks of given sites by looking at the traffic reflected from them into blocks of IP space, thus yet another form of Internet Background Radiation.



PADRE:  I love this.  Unfortunately, we could go forever.  We could just to the whole Internet networking nostalgia.  We don't want to do that.  We want to push forward.  Okay, Steve.  So what is this about malware on social media sites?  It sounds like it's using some sort of parsing to be able to take little bits and pieces of everything and assemble it into something bad?



STEVE:  It's actually very well done.  I did want to take a moment to mention our Picture of the Week.



PADRE:  Oh, yes, yes, please.



STEVE:  Somebody sent this, and I just got a kick out of it.  I don't know if they put this picture together, if they found it somewhere.  Anyway, but of course it follows from our discussion last week of Windows for Warships, which is unfortunately installed on the U.K.'s Trident class nuclear-armed submarines.  Anyway, this is a synthesized picture of the Windows XP famous rolling green hills with the fluffy...



PADRE:  Is this called - I think this is called Bliss.  Is this Bliss or Azure?



STEVE:  I think that's the Bliss theme.  I think you're right because it's one I'm familiar with.  Anyway, somebody, as they say, photoshopped very nicely a submarine into the rolling green hills.  Anyway, just did a beautiful job.  So I don't know if the person who sent it to me did this, or if they found it somewhere.  But it's just a perfect picture for our Picture of the Week.



PADRE:  XP for Nukes?  What could possibly go wrong with that?



STEVE:  Oh, lord.  I did see, I was just wondering whether it was official wallpaper or what.  And I did run across a follow-up article from two years ago, carried by theregister.co.uk, saying "Windows for Warships?  Not on our new aircraft carrier."



PADRE:  Oh, no.



STEVE:  Yes, apparently saner heads prevailed.  Okay.  So to your question about social media, the technology in this is one of the things that caused me to just say, okay, we're losing this battle.  Malware was found which was scanning - shoot, I can't remember the site that it was scanning - Snapchat and was looking at comments to a Britney Spears Snapchat posting.  The malware would scan through the comments, running a custom hash over each comment.  If the hash value matched 183 - so this is not a cryptographic hash because, as we know, a cryptographic hash would make it computationally infeasible to deliberately synthesize a comment that had a known hash output.  So this was a non-cryptographically strong hash.



But the point was it was good enough that the bad guys could make a given comment hash to a value that they wanted.  So it was successively stepped through these comments.  If the hash matched 183, it would then run a regular expression, a regex, on the comment to obtain the path of a bit.ly URL.  And the regex would essentially look for a number of short strings or characters and then extract the character following the one that matched the regex, and it would do it repeatedly through that string.  So the idea was the comment would contain a series of characters embedded which an unseen pattern match would cause to be successively extracted to create a bit.ly link.



So, for example, in this example, and then this actually occurred, someone named asmith2155 posted a comment, "#2hot make loved to her, uupss #Hot #X."  Well, it turns out that, unseen - because the \u200d is a unicode character which is called the "zero width joiner," normally used to separate emojis.  So it's a nonprinting, non-visible joiner which does pass through any filtering because you want to allow the emoji joiner to allow emojis to get posted.  So the bad guys know all of this.  So they put this \u200d successively in front of the printable ASCII that they want this regex to select out of the comment in order to synthesize a bit.ly link which then the malware fetches, which redirects it to the command-and-control server, which in this case resolved to static.travelclothes.org and then a slash and a URL to a PHP page, which was also used in the past as a watering hole command-and-control server by the group that was known to have produced this malware.



So standing back a little bit from this, the moral of the story is, as with other forms of steganography, which I would argue this is a form of, that is, as we know, steganography is the practice of putting something in public view which is obscured in some way that you can look at it and not see what's there.  So I would argue this is a form of that.  So as with other forms, it is not feasible to attempt to examine public media for its hidden meaning.  The only way to find this is to discover the endpoints that access that hidden content and then look at where they are looking in order to find this.  So this is just diabolical.  And essentially...



PADRE:  It's ingenious.  It's a little bit ingenious.



STEVE:  Oh, my goodness.  I mean...



PADRE:  I've got to have respect for that.



STEVE:  Yes.  And that's the point.  It demonstrates that, you know, I don't want to say we should give up.  But it's like, okay, we're not going to win this.  I mean, this is clever leveraging of very powerful technology which was put in place for a good purpose but is being abused.  And there's more of it being produced every day than the good guys, the white hat security researchers, are able to keep up with.



PADRE:  Now, where would I use this?  Would this mostly be sort of a command-and-control node?  Would this be a way for me to tell the malware what I want it to do?



STEVE:  Right, right.  So the idea would be you have malware deployed all over the Internet, and you want to be able to securely and anonymously send it instructions in a way that - but you don't want to embed the command-and-control in the malware itself because then...



PADRE:  Because that's how they've taken us down in the past.  If you've got a malware system, and you take down the C&C node, then it all ceases.  But in this particular case the command-and-control node is the Internet.



STEVE:  Correct.  Exactly.



PADRE:  You [crosstalk] take everything down.



STEVE:  Exactly.



PADRE:  Wow.



STEVE:  And so the malware is looking, is like out there reading social media, just like users do; but it's doing it with foreknowledge of the way that postings will be laced into the social media to communicate to it.  And so it's running a hash function over the postings, looking for those postings that hash to 183.  And if found, it then runs a regex over it to extract, I mean, very much the same way the spy novels have, you know, go to page 13 and go down five lines and over three words and take the first character of that word.  Then go here, here, here.  Anyway, and you assemble a message.  This is doing that.  And there's just no way to catch it except to, I mean, it is steganography in practice, no longer just in theory.  And sure, certainly it could be that photos could be posted where the least significant bits of one color or the intensity vector in the photo contain the information.  But this was actually found in the wild.  It is being done.  So not just [crosstalk].



PADRE:  The diabolical part of that is, I mean, look at the ways that we would use to break up a botnet.  Command and control, okay, so that won't work.  Also you could look for trigger phrases, and if you find any of those trigger phrases you can filter out the trigger phrases.  That won't work because you can make multiple phrases that will hash out to 183.  So, yeah.  This, again, I understand it's being used for nefarious things, but this is kind of brilliant.



STEVE:  And we can't take Instagram down.  I mean, you know...



PADRE:  We could, but we don't want to.



STEVE:  And that's the other thing that's diabolical is that, by putting this in a super popular public stream, we lose the ability to take the malware offline by preventing all instances from going to a single point.  All [audio dropout] are now in directing [audio dropout] through something that cannot be taken down.



PADRE:  Well, I think what we've learned here is that, if you want to be secure, it's actually quite simple.  All you need to do is don't look for anything concerning Britney Spears.



STEVE:  That's right.



PADRE:  That's it.  That fixes it.  Okay.



STEVE:  So there was an NSA report that got leaked which detailed the way the Russian attackers had attempted to attack the 2016 U.S. elections.  And the most interesting takeaway for me, I mean, first of all it was basically website spoofing.  But the question was what to do in the event of two-factor authentication because this is what two-factor authentication is supposed to prevent.  Well, it turns out, and unfortunately in the slide that I've got in the show notes - I did capture it, but in order to fit it onto the page, the original image that I had was already blurry, and I had to squeeze it down to 700 pixels to get it on.  But down in the lower right, under point 7, it says, "If 2FA enabled, also enter," and it says "phone number" and "legitimate verification code."



And then in the box to the right of that, in fact I did pull what it says.  So the slide says:  "If the victim had previously enabled two-factor authentication (2FA), the actor-controlled website would further prompt the victim to provide their phone number and their legitimate Google verification code that was just sent to their phone."  So, okay.  So here's the problem, is that the big problem that nothing has yet solved, and I will say with a single important caveat, a single exception, is man-in-the-middle spoofing.  What the Russian attackers had done was to create a fake Google site.  And so when users went to their site believing they were at Google, they were asked to log in.



So the user would enter their username and password, as many of us have at legitimate Google properties.  And then, but, okay, because we're at the attacker site, the attacker gets those and has automation that immediately goes to the real Google site and is prompted for the username and password.  So they forward, the attackers' automation forwards the username and password to Google and submits them.  Google immediately challenges the attacker automation, saying, oh, please enter the six-digit PIN we just sent you.  Of course, that PIN goes to the actual user.



So the attacker sends back to the user, who thinks they're at Google, but they're actually at Snoogle or who knows what else, sends them back exactly that, saying, ah, please enter the PIN that we just sent you.  So the user checks their phone, and it's like, oh, yup, I got the PIN, just as they would expect.  So they enter that six-digit code into the fraudulent site that in turn gives it to Google.  Now the attacker is logged in as the user, just having freshly authenticated themselves in the presence of two-factor authentication.



PADRE:  Wow.  I mean, okay.  I know that this can be done.  I understand the conceptual steps.  But again, that's smart because it does take a bit of timing.  You have to be able to know how to do this properly in order to make this work.  And I'm assuming they're only doing this for particularly targeted accounts.  They want to take over certain accounts because those accounts will give them access to other accounts; right?  Because, I mean, this is way too much work just for every possible account on your list.



STEVE:  Well, actually this can all be automated.



PADRE:  Oh, my goodness.



STEVE:  So the attacker server immediately forwards username and password.  It then looks at the page it receives and then presents that to the user as the next page.  So the point is it is always possible to insert a man in the middle and spoof the user who's not paying attention.



PADRE:  Right.  But, I mean, man-in-the-middle attacks were supposed to stop, I mean, two-factor authentication was supposed to stop man-in-the-middle attacks because you use a separate route.



STEVE:  Correct.  And doesn't because the point is you are - and notice that even the time-based token would not be a solution because in that case Google would say, you know, please use your Google Auth or Authy or whatever you're using in order to give us the - it's only going to be current for 30 seconds, as we know, but that's enough.  You enter it in.  They immediately receive it and forward it to Google, but before that six-digit code is expired, and they're logged in.  And I should say this was part, in this case, part of an elaborate spearphishing campaign.  And it was targeted at employees of one voting machine software company, and then a bunch of people in the DNC.  So in this case it was targeted.



But this scales with automation all the way up so that two-factor authentication unfortunately really doesn't protect us.  And this is why, as I had mentioned before, almost two years ago I brought my forward progress on SQRL to a halt because I hadn't solved this problem.  And it's also why - and it wasn't last week, Padre, but about a month ago I had said to Leo that I had stopped because somebody in the SQRL newsgroup where I've been doing all of this work said, you know, with the advent of OAuth, where users are used to logging in with another site's credentials, if SQRL presented a dialogue, said you are logging into Amazon when you're at EvilSite, then the idea was that we were presenting that as a caution to keep users from deliberately giving EvilSite their Amazon credential.



But in the wake of this growing popularity of OAuth, which is training people to essentially log in with another site's credentials, I realized that the prevention that we had was requiring too much from users.  So what I have just emerged from since we last did the podcast was about a month ago I said, "Oh, shoot."  Not exactly that word, but you get the idea.  "We have to fix this."  What happened a year and a half ago was I realized that the - I stopped everything because I felt like I didn't understand the problem well enough.  I didn't completely have a grasp of it.  So I spent several days with an engineering pad and pencil drawing pictures, like to really understand what causes this problem, what causes the man in the middle, essentially the man-in-the-middle problem, and what can be done to solve it?



And the root of the problem is that an attacker is able to insert themselves between the authentication and the server to which they are authenticating.  I mean, that's the crux of the problem.  And so what SQRL incorporated as a result of that work about 18 months ago was something we called CPS, Client Provided Session.  Because what normally happens is, when you authenticate with a remote site, it gives your browser a cookie which is now the token that represents you as the authenticated user so that your queries, which then bear that token, going back to the web server, keep you logged in, keep you authenticated.



So the problem with a man in the middle, if they're able to present themselves to that site, then they receive - it's their web session that gets the authentication cookie, and they are then logged in as you at that site.  So I thought, okay, that's it.  We have to have some way to cut that man out.  And it turns out we have a solution.  Until last month [audio dropout] not going to be mandatory.  It was not - because some things about Windows 10 scared us off back a year and a half ago when Windows 10 was still looming.  It turns out that Microsoft was unable to do something that they intended to, which was to cut off the Edge browser from servers running in the localhost, that is, in its own machine.  They had said that they were going to do that.  But it turns out there are too many instances where that's being done.



So this is the key is that with SQRL now, and this is now in the spec, has to be supported, so the failure of it sets off alarm bells because those bells should never go off, is that when SQRL uses your SQRL identity to authenticate with a remote server, that remote server no longer authenticates the web session because that's the danger here.  Instead, the server sends back to the SQRL client an authenticated token.  And so the secret is the SQRL client then gives that to the user's browser.



PADRE:  Oh, okay.



STEVE:  In other words, it cuts out the man in the middle.  And so what happens is when you are authenticating with SQRL, your browser initiates a page jump to the localhost, to the SQRL port on the localhost.  And it sits there waiting to load a page.  When you're finished authenticating, the SQRL client gives a redirect, a 301 found redirect - or is it a 302?  I don't remember.  Anyway, an HTTP redirect back to the browser.  And that redirect contains the authentication token so no bad guy ever gets it.  And so we have, as far as I know, the only absolutely spoof-proof authentication system that exists.



PADRE:  Right, because the bad guy's counting on you, the human, doing something silly, which is putting in the token into a site that you cannot trust.  If SQRL's taking that away from me, so I am no longer involved in entering that token, you can't spoof SQRL.  You can spoof the human; you can't spoof SQRL.



STEVE:  Exactly.



PADRE:  I like that.  Okay, no, that absolutely makes sense.  And that's, I mean, it just kind of makes me a little sad, Steve, because I thought multifactor authentication was a pretty good bet because that's something that I can actually explain to my parents and my non-tech-savvy family.  And now I have to explain, okay, yes, this is better than just having a username and password, but make sure you're actually entering in that authentication code only to an actual site, which they may or may not be able to tell.



STEVE:  So we could argue that multifactor authentication can improve some classes of attack.  That is, if the username and password list or database gets loose, then maybe it helps you because - or if the username and password is captured, statically captured.  The problem is that we are still victims of dynamic attack, where on the fly that second-factor is provided.  And if it's captured by a man in the middle, it's not protection.  And the other problem is that, if the username and password database includes the secret key for the one-time password, then you're still in trouble because they're able to, if they have that secret key, they also know what the proper six-digit code is.



PADRE:  Right.  Now, it's obvious the way that SQRL can solve this.  I mean, this is what's baked in.  Could you fix the current two-factor/multifactor authentication system?  Let's use Google's two-factor.  And you're using Google to authenticate other services.  If somehow the authenticator sent both the service in the cloud and you a code, so in that sense, even if you put the code in and it's intercepted by a third party, the one that's sent directly to the authorized service would not get to them, or they wouldn't be able to access, I mean, is that kind of close to what SQRL could do?  Or there's just no way to fix the current system?



STEVE:  So the secret here is that SQRL's authentication is driven by the domain name.  And that means it cannot be spoofed.  I mean, if an attacker uses a spoofed domain name for SQRL, it just generates a nonsense identity because, unless the domain name is authentic, it won't go to the right place.  And so an attacker has to give SQRL, even if they create a fake page, the SQRL code has to be to the real website because that's the URL that SQRL uses.  So SQRL has the actual web server's URL, gets the authentication token from it, and gives it to the user's browser, thus cutting any man in the middle out.



So the only way I could see another system could work is if you had something like this, if you had an unspoofable authentication.  The problem is, that means the authentication has to be tied to the actual domain name.  And SQRL's the only system that does that.



PADRE:  Right, right.  And unfortunately, since the current authentication schemes typically rely on certificates, not domain names, and we know that certificates can be spoofed and/or just handed out incorrectly, yeah, you can't really fix that.



STEVE:  Yeah.



PADRE:  Well, good, thank you.  I mean, that gives me my optimism for the week, Steve.  I really appreciate that.



STEVE:  So we have a little bit, yes.  Unfortunately, Tavis Ormandy has been busy, as usual.  As we mentioned last week, we suffered his showers while he was thinking about LastPass.  The good news is he's moved on.  He's done a really interesting project which he posted on GitHub.  I think he only has, like, six projects.  And I looked at them, and this is arguably the most interesting, I think, of those.  He likes Linux.  And he has mature and fast and powerful fuzzing tools on Linux.  But he wanted to pound on, that is, to fuzz, and as we know, "fuzzing" is the process of giving software unexpected input, looking for crash events.  And if one occurs, then to look at exactly what the execution path was as the software handled what malformed thing you gave it in order to see if you could leverage that into an exploit.



So, I mean, it's a powerful technique.  The problem is that Windows, I guess, either Windows doesn't have very mature tools, or Tavis just doesn't like them, or he likes Linux more.  So what he did was he said, okay.  I need a way of testing Windows DLLs under Linux.  So he wrote, and it's available publicly on GitHub, something he calls "LoadLibrary," which is actually an API in Windows that I use often.  It is a tool, a Linux tool that allows Windows DLLs to be loaded into Linux for examination and use.  He says it's not a WINE replacement, WINE of course being the entire and amazing, frankly, because SQRL runs under WINE and allows it to run for Linux and Mac, for example, an amazing Windows replacement.  It's not that.  It just allows a DLL to be loaded.  On the other hand, there are many useful Windows DLLs that have not been ported to Linux, and this allows you to use them, depending upon what dependencies they may have because you need to provide all of the other DLLs that that DLL may load.



Anyway, so he created LoadLibrary, and then he began poking at the mpengine.dll, which is the malware protection engine, which is the core of all of Microsoft's antimalware code, like Windows Defender and all of them, the malware protection engine throughout their system, their services.  And throughout May he was finding problems.  He found the MsMpEng Type Confusion.  That was the one that forced Microsoft to issue the emergency out-of-cycle patch because it was a remotely exploitable, wormable exploit.  He then found a UIF Decoder DoS problem and a Privilege Escalation throughout May.



Now he has found a very, in fact, he tweeted on June 7, he said:  "Sigh, more critical remote mpengine vulns.  Found on Linux then reproduced on Windows.  Full report on the way.  This needs to be sandboxed."  So I'm sure he's communicated with Microsoft.  I took a little screenshot of the top of the terminal window that he posted along with his tweet, showing that he had used LoadLibrary to load a testcase.exe, which it was then scanning.  And he hit an invalid pointer in the call to the memory allocation free function, which then I'm sure he pursued, and he figured out that, yes, he was able to deliberately cause an invalid pointer.  And, as we know, that's often the start of being able to leverage that into, apparently, a remote and networkable exploit.



So anyway, a number of things.  That means I don't know - today's Patch Tuesday, by the way.  And Microsoft did do their standard monthly rollup.  There were security updates to the kernel, to Microsoft Windows PDF, to the kernel mode drivers, to Uniscribe, Device Guard, IE, and also Edge and the Windows shell.  So a bunch of things.  I don't know, I just didn't have a chance because this just happened, whether they also fixed this.  Let's hope they did.  And I should also mention that Adobe, for those of us still using Flash and other, you know, Shockwave and so forth, lord help us...



PADRE:  I have Flash in a virtual machine because I will not run it on my computer anymore.



STEVE:  Good.  Twenty-one critical vulnerabilities.



PADRE:  Oh, good.  Is that all?



STEVE:  A little more than double what they did last month.  So a ton of problems fixed in the June Adobe patch, yeah.  The good news is Flash is, you know, it's legacy, but quickly going away because HTML5 now is offering so many features that people used to need to use Flash for.  And I keep, you know, from time to time I'll go over to TWiT.tv, and I'm unable to look at any of the videos there because they still use Flash.



PADRE:  Well, you can do Twitch.  If you go live.twit.tv and go to Twitch, that will work.  YouTube will work.



STEVE:  Ah, good.



PADRE:  But, yeah, I'm the same way.  When I'm crawling the web, anytime I find a site where a critical function will not work because I have Flash, not just disabled, but removed from my system, it always kind of makes me pause and go, okay, well, this is something that we need to work on.  This could be made better.  But Steve, let me ask you.  What is it about the malware protection engine that seems to make it such a big target?  Is it poorly written?  Or is it just that it has to have hooks into everything, and that makes it a juicy target?



STEVE:  It's the "I" word.



PADRE:  Oh.



STEVE:  It's an interpreter.



PADRE:  Oh, that's right.



STEVE:  And anytime you have an interpreter as, I mean, that's one of the things we are now seeing over and over and over.  This thing inherently has to examine what's coming across the network and try to find, like, understand it.  If it's an image, it's got to parse the image.  If it's an EXE, it's got to parse the EXE.  It's got to look inside it.  It has to decompress it if it's packed.  I mean, it's doing a huge amount of work.  And so, again I'm not laughing at Microsoft.  I recognize this is a hard problem to solve.  Unfortunately, it's a hard problem to solve.  And they haven't, I mean, it's very difficult to solve it perfectly, yet they have to be perfect.



And so the big problem is this is, in terms of attack surface, it is on the receiving end of the network connection and has to examine everything that comes in.  It has to also be inside of the TLS decryption because it needs to have the TLS tunnel removed so it's able to get at the EXE or the image or whatever it is that's inside there.  So, I mean, it is a ripe attack surface, and Tavis just found another bug in the interpretation of this content.



PADRE:  So what the malware protection engine needs is another protection engine that looks at anything it's parsing and interprets that to see if the first engine should be interpreting it.  Is that...



STEVE:  Well, and Tavis's comment was this should be sandboxed, which is interesting.  Probably for speed and so as not to cause a big problem, it's in the kernel, which means it's a kernel exploit ripe attack, which is doubly problematical.  And you wonder how you sandbox something that needs to be part of the OS, which is essentially where this thing runs.



PADRE:  You know, about 12 years ago...



STEVE:  I need to take a break - oh, go ahead.



PADRE:  I was just about to say that I used to have a hardware engine.  It was a USB key that you could plug into the side of your computer.  This was more than a decade ago.  This is when I was still living in San Jose.  And it basically intercepted - it was a man in the middle that would intercept all network traffic.  And it used its own operating system, mini operating system to scan all traffic and look for any threats.  They've gone out of business, but it was an interesting product in that it offered protection that didn't actually touch your operating system.



STEVE:  Well, and, you know, this is the promise of all of those secure routers that we're beginning to see.  I just saw that Norton Symantec is offering something, I don't remember what they called it.  It's a very pretty-looking geodesic blob on their page introducing it.  But I just shake my head because either it is going to be blind to all TLS traffic passing past it, or it's going to be proxying your TLS and giving all of your machines inside the LAN a certificate that they're going to be forced to trust in order for it to intercept your TLS.  And you don't want that either.  So, I mean, I keep seeing all these claims about, oh, a security-enhancing router.  It's like, you know, unfortunately, a decade ago, yes, you could do that.  Today no.



PADRE:  Not so much.



STEVE:  You want it to be blind to the traffic because, if it's not, then it's a huge security vulnerability.  



PADRE:  Right, right.  In fact, there was a technology that I saw about three years ago from one of the major chipset manufacturers who makes routers.  And they wanted to create a product for ISPs.  It needed to be tied together to an ISP because it would allow all the router endpoints to collectively look for vulnerabilities and exploits, and then everything would be blocked upstream.  So it would never even make it to the router.  They could just never get the ISPs to buy in.  It would add something like $5 to the cost of a router.  And, well, that's just too much.



STEVE:  Well, and for example, I fought a problem with SQRL because I cannot run a TLS server as localhost in the user's machine, much as I would like to, because it'd be nice to be able to have the browser make an HTTPS query.  But there's no way to protect the private key from theft if you have a server which is accessible.  The only way websites are able to offer secure TLS is that those sites are remote and protected so that no one can get the private key.



So I came up with a way of minimizing the impact of making an HTTP query from an HTTPS page, which is technically mixed content.  And it's one of the things I've been doing in the last month because I decided we have to be able to allow SQRL to be completely spoof proof.  That's just - it's too big a benefit to SQRL not to be able to enforce that.  And now we can.  But there are lots of hurdles to be overcome.  There are reasons this hasn't been done before.



PADRE:  So, Steve, I'm looking for, oh, I don't know, a couple of bitcoins floating around the Internet.  What could I possibly do?



STEVE:  So, boy, this was an expensive lesson for someone.



PADRE:  This is better than ordering a pizza with a thousand bitcoins.



STEVE:  Yeah, the report was, quote, "I copy-pasted a bitcoin address into Electrum and confirmed the bitcoin transaction.  A few minutes later I checked with the recipient to verify it had appeared in his wallet."  Now, we shouldn't [audio dropout] 13 bitcoins.  Thirteen bitcoins is not chump change any longer.  Thirteen bitcoins at this morning's value, when I looked it up and did the multiplication, is $35,555.  So he sends $35,555 to somebody else and confirms that it had appeared in his wallet.  It hadn't.  Somehow it was sent to the wrong address.  So this guy gets online, explains in some forums of knowledgeable people what happened.



He says, "I checked all browser windows, private messages, chat histories and do not know the address that grabbed the 13 bitcoins."  Well, when he shared this history of what happened with knowledgeable users on Reddit, they pointed out that the address was almost certainly changed on the fly by malware.  Specifically, there exists a clipboard contents-altering malware that has been around and known for a couple of years.  This malware surreptitiously continuously monitors the system clipboard for the appearance of a destination bitcoin address and, when found, immediately and silently replaces it with its own.



PADRE:  Oh, that's mean.



STEVE:  Oh.



PADRE:  That's [crosstalk].  So, wait, quick.  If he had been paying attention, would he have noticed when he copied and pasted into the window for actually sending the bitcoin that the address was different?  If he had actually looked?



STEVE:  Yes.



PADRE:  I mean, that's a tough sell because it is a convoluted string.  But...



STEVE:  None of us try to parse that.



PADRE:  No, never.



STEVE:  It just looks like gibberish.  It's like, okay, whatever.  You just sort of take it as like this noise.  And so, yes, you copy it, and then go somewhere else, select a field, paste it, and that paste was a different thing than what he copied.  And then he sent $35,555.  And who among us wouldn't do the same thing unless we were being really careful?  And as we know, it's gone.  That's, I mean...



PADRE:  Yeah, you don't get that back.  There is no way of recalling it now.



STEVE:  It's gone.  No.  It's, oh, wow.



PADRE:  That is just a dagger to the heart.  And here's the thing.  I mean, the block chain technology itself is sound.  So what they're doing is they're attacking the weakest points.  And the weakest points are - it's always going to be the client computer.  This, I would call this an advanced persistent threat because this is something that just sits on your computer.  It does absolutely nothing because it does not want to warn you about its presence.



STEVE:  Does not want to come to your attention; right.



PADRE:  Yeah, and it just looks at everything, everything that you're possibly putting into the copy and paste protocol.  And it looks for a bitcoin address that it replaces.  That's, again, kind of brilliant.



STEVE:  And think about a tiny bit of code.  So this little bit of code could be added to any freeware that the attacker wanted.  Imagine creating or modifying useful existing freeware, just to add a tiny little bit of code because, I mean, all of the hooks to monitor the global clipboard are readily available.  So all it does is it just polls the clipboard, looking for the appearance of something that is a bitcoin address and replaces it with its own.  I mean, it's tiny, yet somebody behind that just made themselves more than $35,000.



PADRE:  That's just maddening.



STEVE:  It's paying off way more today than it was a few years ago when it first occurred.  And the problem is the fact that it makes so much money means that there's huge financial incentive to do this more.



PADRE:  Right.  I actually have - I have, like, five bitcoins somewhere.



STEVE:  Nice.



PADRE:  I probably should go find them.



STEVE:  Nice.  Everybody who listens to this podcast knows that I have 50.



PADRE:  Oh, geez.



STEVE:  When I first was - we did a podcast on the block chain, where I went through and explained the brilliant technology that was the base of bitcoin.  And back then I set up on a machine, the one that I use for Skype, I think it was an i5 or, I mean, it was not a big, fancy - it was not a GPU miner or anything because back then bitcoins were easy.  And one hash completion was 50 bitcoins. 



PADRE:  Whoa.



STEVE:  That's how long ago it was.  And I woke up the next morning, and I had 50 bitcoins.  It was like, oh.  And I told everybody the next week, I said, hey, guess what, this little thing I ran overnight made 50 bitcoins.  Now, they weren't worth much back then.  They are today.



PADRE:  Are now.  I had a - this was before I was at TWiT.  I had a system that I was reviewing for Dell, and it was this multi-Xeon monster.  And so I'm like, okay, I'll try bitcoin.  And so I started mining, and I had left it running for maybe six or seven hours, and it had found five bitcoins.  I'm like, oh, okay.  Guess it's not that hard.  This was before I really understood the block chain technology.  And when I tried to do it a year later, I had an even more powerful system, and it just sat there spinning, spinning, and spinning.



STEVE:  And in fact there was a topic of discussion in the last week about how there's some malware which is commandeering Raspberry Pis for coin mining.  And I'm thinking, who cares?



PADRE:  Yeah.  You could get a million of those together.



STEVE:  A Raspberry Pi?  



PADRE:  It wouldn't matter.  Unless you're trying to mine a brand new cryptocurrency, no.  Raspberry Pi is just not going to cut it.  In fact, even the dedicated ASIC machines aren't really doing it anymore.



STEVE:  Correct.  You would have to be mining a brand new cryptocurrency and be the only person mining the brand new cryptocurrency in order for a Raspberry Pi to have a snowflake's chance of actually generating anything.  It's just like, no.



PADRE:  Yeah.



STEVE:  In fact, a friend of mine, Mark Thompson, who is a bitcoin miner, said that two of the main GPU makers - and I don't remember which two he said, but they're the big guys - are now doing a dedicated coin-mining chip.  So it's not just repurposing GPUs.  They're going to be doing mining silicon.



PADRE:  Well, I remember when Nvidia first started offering the GPU clusters.  So just like you could rent AWS, you could rent a cluster for a certain amount of time.  And it was two weeks after they made the announcement that the amount of processing power you needed to get a bitcoin became more expensive than the cost of the bitcoin.  And it just - it's phenomenal how fast that will rise.



STEVE:  And at some point you have to wonder why anybody would sell you GPUs to mine bitcoins when they could just plug them in themselves and mine them.  Actually, it turns out that in Arizona, where Mark is, it's only feasible to mine there because the power is so inexpensive compared to, for example, where I am in California.  You can't make money in California mining bitcoins because the energy requirement to run the GPU is higher than, you know, at the current difficulty of getting a coin you end up losing money no matter how hard you mine, no matter how fast and seriously you mine.  So you've got to do it...



PADRE:  You've got to remember Butterfly Labs.  So Butterfly Labs was the company that was making the custom ASIC machines.  And they did exactly what you said.  They would make the machines for the customers after taking their money and then hold onto them for a few weeks or months, mine with them, and then ship them out.  So they probably have a bunch of bitcoin, too.



STEVE:  So F-Secure is a great security firm that we talk about often.  The bad news is they took a careful look at a very popular OEM, Foscam, F-O-S-C-A-M.  And I got a kick out of the Foscam site.  It's www.foscam.com.  They are currently offering a Happy Father's Day giveaway.  And I'm thinking, yeah, that's right.  Give your old man something serious to worry about.  Oh, my goodness.  Eighteen different serious vulnerabilities in Foscam's IP Internet-connected cameras.  And unfortunately it's not just the Foscam brand.  Chacon, Thomson, 7links, Opticam, Netis, Turbox, Novodio, Ambientcam, Nexxt, Technaxx, Qcam, Ivue, Ebode, and Sab are all OEMs of Foscam and use the same technology.  So this is widespread.



The researchers at F-Secure documented 18 vulnerabilities that the manufacturer has not fixed despite being alerted to those problems several months ago.  So F-Secure responsibly disclosed their findings, told Foscam, hey, here's 18 problems, and you're not going to believe what some of them are.  All of the flaws were confirmed in a camera marketed under the Opticam i5 HD brand.  So there's yet another one.  And then a smaller number of those 18 were also found in the Foscam C2.  F-Secure's report noted that the weaknesses are likely to exist in, as I've said, many other camera models Foscam manufactures and sells under other brand names or OEMs to other people.



They wrote:  "The sheer number of vulnerabilities offers an attacker multiple alternatives" - it's not just here's the one way in.  It's pick your way in.



PADRE:  It's a menu of exploits.



STEVE:  "...in compromising the device," they wrote.  "Among the discovered vulnerabilities are insecure default credentials" - including a null password - "and hard-coded credentials, both of which make it trivial for an attacker to gain unauthorized access.  Other vulnerabilities allow for remote command injection by an attacker.  World-writeable files and directories allow an attacker to modify the code and gain root privileges.  Hidden Telnet functionality allows an attacker to use Telnet to discover additional vulnerabilities in the device and within the surrounding network.  In addition, the device's 'firewall' doesn't behave as a firewall and also discloses information about the validity of the credentials."



Dan Goodin, whom we often quote as a writer for Ars Technica, said:  "The flaws allow for a wide range of hacks, including using the Internet-connected cameras to participate with other infected devices in distributed denial-of-service attacks, accessing private videos, and compromising other devices connected to the same local network.  The vulnerabilities are compounded by the ability to permanently replace the normal firmware controlling the camera with malicious firmware that can survive restarts without being detected."  I mean, it just - it doesn't get any worse than this.



PADRE:  Right.  Because, I mean, essentially you're giving them the ability to rewrite the operating system with the camera to whatever they want.



STEVE:  Install their own, yes.



PADRE:  Yay.



STEVE:  So the standard wisdom is you must disable if at all possible Universal Plug and Play because that allows devices inside to statically map ports from the public Internet into your internal network.  [Audio dropout] must have Universal Plug and Play.  And in any event, now IoT devices have to be on an isolated network segment.  That is, if you want to play with this stuff, with light bulbs and cameras and baby monitors and microwaves and refrigerators and everything else, give it its own network segment and/or isolate the devices where you have important information, like your iOS and your Linux and your Windows and your Mac devices.  Put them on a separate network segment where those are unable to see each other, so that your IoT devices can have a field day, but only within themselves, and not be able to reach out and get into your main PCs.  It's just it's no longer optional.  This is the world we live in today, unfortunately,



PADRE:  You know, we created something like this for Know How just last week.  I can't remember who first proposed it.  We created a "three dumb router setup," and that actually would solve for this really, really well.



STEVE:  Yup, three dumb routers.



PADRE:  Segment everything.



STEVE:  Yup.  You really do need physical network isolation.  And three dumb routers is a nice way to do it.



PADRE:  And in fact Cisco two weeks ago, two or three weeks ago at their big IoT conference, they released a new IoT suite.  And the core technologies have existed for a while, but they put them together.  Essentially what it does, they've got a security engine that looks for devices that might be exhibiting owned behavior.  So they're starting to probe the network.  They're starting to exfiltrate data.  And what it will do is it dynamically segments those into their own VLAN.  So it completely isolates them.



STEVE:  Nice.



PADRE:  And then it warns the administrator.  And this is the sort of activity that we've been telling people to do for a while on TWiET.  It's this whole idea of you've got invaders in the walls, so you need to change your security to be able to deal with exploits already in your network.



STEVE:  I think you raise a really good point, which is we're in this purgatory, at the moment, between our routers not yet being universally smart enough like this to protect us, and IoT devices now having gotten smart enough to hurt us.  And so for while we're here, it's incumbent upon individuals who know enough to be proactive in going out of their way to enforce this kind of protection.  But the point you raise, I think, is a good one.  And that is that one can foresee a year, two, five, or 10 from now, that will have been taken care of.  Routers will have separate ports, or WiFi will exist in separate segments such that you can explicitly give some devices their own place to play, and it won't be up to users to do three dumb routers.  Instead, you'll just have one smart router.



PADRE:  Right.  And we're getting close to that.  In fact, we've got "Aspire" in the chatroom who is saying, well, unfortunately, something like a Chromecast doesn't work when you segment it.  It does on mine.  But mine's a bit more advanced because I've got a switch that supports dynamic VLAN.  So the way that it works on my network is everything has its own sandbox.  It can only see the Internet and itself.  It sees no other devices unless a device of a higher authorization, like all my administrative accounts, requests Chromecast support.  Like I want to send a command to the Chromecast, or I want to stream to the Chromecast.  Then just for the duration of that connection it puts them in a shared VLAN so that they can share traffic.  And the minute it's gone, it destroys the VLAN.



That was super enterprise-y 10 years ago.  The switch I'm using is actually 11 years old.  And I know that something like my Synology router actually does support VLANs and rules for VLANs.  So we're not too far off.  I mean, it's difficult to visualize right now, which is why I love your three dumb router setup because they're physical boxes.  But we can approximate it if you've got a smart enough engine that's looking at what devices exist on the network.



STEVE:  Right.



PADRE:  Well, okay.  That's kind of hope.  Right?  I mean, that's a positive thing?



STEVE:  Oh, I think it is.  I absolutely - and unfortunately it's being driven by necessity, where the necessity is we're in a very vulnerable position at this point.  But I'm glad that Cisco's stepping up.  And I imagine at some point before long we'll just be able to purchase those sorts of devices at retail.



PADRE:  Well, it's got to become table stakes.  Because unfortunately all consumer, and even SMB routers, are still operating on that flawed perimeter security model, the whole idea that we can build a wall, and you keep the bad guys on the outside and the good guys on the inside.  You can't do that anymore.  You just have to assume that something inside your network has been exploited.  And so if your security isn't segmenting and keeping that traffic away from good traffic, then it's not a good solution.



STEVE:  Well, and look at the enterprise situations where the exploited entity was an employee that clicked on a link, and now unfortunately, we'll get to this shortly, doesn't even - you don't even need to click.  You just need to hover, believe it or not.  And because they didn't have segmentation, the executive assistant was able to compromise the core network infrastructure and allow advanced persistent threats to exist in Sony or at RSA for a long period of time and really do damage. 



PADRE:  I have a friend who runs a fairly large network in Virginia.  And one of the things that he implemented, and at first all the IT people hated it, was he took away the superuser account.  So no one, nobody has this.  And they said, well, you know, sometimes we have to jump between network segments.  He said, well, you have all the authentication credentials you need.  You can do that.  And then as sort of a way to give back later on, he did authorize superuser account usage, but it would time out every five minutes.  So you could log in as a superuser, do what you had to do.



STEVE:  Nice, nice.



PADRE:  And if you were there longer than five minutes, it would time you out again.  And if you logged in more than twice consecutively, he got a message saying someone might be abusing the superuser account.



STEVE:  Nice.



PADRE:  And it's involved.  It's complicated.  It ticks people off.  But it's policy.  And following that policy, that's really going to help enforce security in large networks.



STEVE:  And, you know, that also sort of points to a broader concept, and that is monitoring.  That's the other thing we see.  The reason an intrusion detection system is useful is that it's not a firewall.  It's, I mean, it's an adjunct to a firewall.  It is monitoring usage.  It is monitoring what's going on.  And so that right-thinking admin said, okay, I'm going to help people do the right thing.  But if somebody does this twice in a short period of time, I'm going to get a notification.  That's brilliant because that means that he is watching the way the system is being used, making sure it's not being abused.  



PADRE:  And unfortunately he tried to do an automated system, and it kept coming back to this, like, no, you can't trust any automated system because any automated system is going to follow a set of rules, and you can game the rules.  It's more difficult to game an IT administrator who actually knows what he's doing.



STEVE:  Right.  And in fact I had to do something like that because I wrote, as we know, GRC's eCommerce system myself.  And I wanted to prevent unexpected abuse.  That is to say, I recognized that I could not predict all the things that someone could do.  So I said, okay.  I don't care what is going on.  After some fixed number of queries to the eCommerce system, that person is locked out.  Period.  I don't, I mean, I'm not going to try to figure out what they're doing.  I'm not going to say, oh, does this look good or bad?  If this count hits the limit, they're banned.  And that was the one bug I ever had in the system after I brought it online was that it turns out that I wasn't even discriminating between a successful purchase and if they screwed up their credit card number, or they didn't get their zip code right, or they couldn't remember what their PIN was, and they brought the count right up to one short of where they would be banned, and then they succeeded.  So that they, yah hah, they managed to purchase SpinRite.  Then the system wouldn't let them download it.



PADRE:  Oh.



STEVE:  Because that final event put them into the banned category.  And then when they tried to download, it said, I'm sorry, you've been blocked due to abuse of the system.  Contact GRC. 



PADRE:  I can imagine that customer service letter was probably a little annoyed.



STEVE:  Well.  And the moment it happened, it's like, oh, that's the one thing I should have allowed.  And so of course, when they successfully purchase, I zero their counter, and then they're good to go again.  So, but again, I recognized I could not predict what the nature of the abuse might be.  So I wouldn't care.  I would set a high limit.  But if they hit it, sorry, I don't know what you're doing, but it seems a little strange.  So...



PADRE:  That's what we've got.



STEVE:  So, and I'm sure you're dealing with this over on TWiET, Padre.



PADRE:  Yes, we are.  



STEVE:  This Intel advanced management technology, the hidden processor on the motherboard and the concerns that it raises.  Turns out that Microsoft, rather than just being theoretical, as it has been until now, and a concern, Microsoft found malware, advanced malware produced by a group they've named the Platinum Group.  It's a state-sponsored hacking group, so it is high-end.  They are abusing the Serial Over LAN, which of course the abbreviation is SOL, and that's a fitting abbreviation.  That's a part of the so-called AMT, the Active Management Technology, which in turn is [audio dropout] of Intel's ME, the Management Engine, which is the independent processor embedded within the Intel support chipsets for the higher end enterprise, the vPro, and some other high-end motherboards.



PADRE:  Right, the vPro series, yeah.



STEVE:  And so essentially what this means is that there is a, supported by the motherboard, OS agnostic, and even on when the power is off, the LAN interface is still up.  And there is an option, thank goodness, the one saving grace is that Serial Over LAN is not enabled by default.  So, but if it is enabled on the motherboard, there is malware using it.  And essentially what happens is it is a serial emulation over LAN.  So if something  bad has network access to the interface on the motherboard, would be, for example, a server, then they're able to pretend to be a serial interface which has been exploited by this Platinum written malware over a TCP connection in order to access the motherboard.



So the good news is Microsoft is not saying whether they have determined that malware is able to, other malware is able to turn this on, or whether it has to be done administratively by the enterprise setting up the system in the beginning.  The problem, and the fact that it is unfortunately a BIOS setting and not a physical jumper that needs to be physically changed, that's the one caveat.  And so you wonder if it's not possible for something to turn on and then use the Serial Over LAN in order to abuse it.



PADRE:  Precisely.  And the way that we've been covering it on TWiET is that this is part of the lights-out management solution.  So an administrator who might have to administer...



STEVE:  If you'll pardon the term.



PADRE:  Yes, exactly.  But, I mean, it allows them to get into machines, even if they're off.  It's a separate processor that's always running.  Now, if you are running a vPro processor, you do have to actually turn on AMT.  It has to be provisioned for any of this to work.  But as you mentioned, that is a soft setting.  And since the management engine runs separately, we don't actually know its full capabilities.  Intel has been very mum about that, for good reason.  But it's sort of this magical thing that we know is always running and has very deep access.  It can essentially bypass the operating system and any security precautions that you've put into place in your OS.  But when you get access to the Serial Over LAN, this is essentially an out-of-band management technology.  It allows you to use a serial console server to be able to access devices, again, even if their primary interface is misconfigured, broken, or otherwise off.



Now, the question, and I was going over a couple of forums talking about this, there are people who are saying that, even if you don't turn on the Serial Over LAN, the SOL feature, if you have provisioned AMT, you can use AMT to turn that on.  And you can do that even without turning on the computer, which is absolutely terrifying because...



STEVE:  I would believe it.



PADRE:  Yeah, because then you can use that.  You can use the Serial Over LAN interface to exfiltrate data.  And because it looks so different than standard TCP/IP traffic, the typical security engines that you might have watching your network will have no idea.  That's just - that's scary beyond belief.  Because it means that I now have to change the way my threat engines are looking at my network.



STEVE:  And you know, when I looked at this last time, I remember noting that they are supporting TLS connections, too.



PADRE:  Yes, yes.



STEVE:  That they've got certs built into the firmware, where it's protected, which allows them to have certs.  So that suggests that you wouldn't be able to inspect that traffic, either.  It would just look like innocuous encrypted TCP.  And so something goes, oh, well, okay, fine. 



PADRE:  "Chumley" wants to know why we're only mentioning Intel chips.  It's because this only exists in Intel's vPro chips.  This is an Intel-specific issue.  Now, one of the other things is Microsoft did respond.  So they've updated Windows Defender.  It doesn't eliminate the problem.  But what it will do is it watches that traffic.  And they have something that they've designed that they say can differentiate between legitimate AMT traffic and illegitimate AMT traffic.  And it probably has something to do with the actual amount of traffic that you're passing because, if it's true Serial Over LAN, it should be a very minuscule amount of traffic.  So the minute you start seeing it look like a Samba connection, that's probably an exfiltration.



STEVE:  Right.



PADRE:  Okay, Steve.  So let's get away from processors that run with the power of the dark lord and instead talk about this common held Internet myth, which is, well, as long as I don't click on it, I'm fine; right?  I mean, that's still true; right?



STEVE:  This is why I'm losing hope.



PADRE:  You can't lose hope.  I get my hope from you.



STEVE:  I'll still be here, but I don't know.  We have, it's been discovered in the wild, a PowerPoint-based link mouseover-based downloader, which leverages Windows PowerShell such that simply hovering over a malicious link is now sufficient to take over your computer.  This newly discovered attack does not rely upon macros, JavaScript, or VBA, Visual Basic for Applications, for its execution.  When the user opens the document they are presented with text that says "Loading ... Please Wait."  And that's displayed as a blue hyperlink to the user.  So it's like, oh, that's interesting.  Now, we're all kind of trained, even those in the know, to hover our mouse over a link to see if we get a little popup message about like what that is, or often that's the way our browsers will show us where the link is pointing to.  We're able to inspect, visually inspect the URL.



Well, it turns out that PowerPoint supports a hover event on links.  So when the user mouses over the text, which is, as I said, the way we typically check the destination of a hyperlink.  The underlying PowerPoint documentation will execute the hover action for the link, which executes PowerShell.  Now, it's the sequence, I mean, that's bad enough.  But it's like, okay, well, what can happen?  Well, somebody, the hacker who devised this, went through some serious work in order to knit together an exploit.  But this is one of the things that just sort of makes me think, okay, all hope is lost.



So the PowerShell command that is invoked by hovering over the link connects to the domain cccn.nl, which retrieves and saves a file named c.php to the disk, the user's drive, as ii.jse in the temp folder.  That file is then executed by wscript, the Windows scripting engine, and that drops a file named 168.gop.  JavaScript then executes the certutil.exe with the -decode parameter, giving it the 168.gop file as the thing to decode.  The result is saved in the temp folder as 484.exe.



Then 484.exe is executed, spawning mstsc.exe to allow remote desktop access to the system.  That fires up the RDP protocol to the system.  After that, the 484.exe was renamed and saved under AppData\Roaming\Microsoft\Internet Explorer\sectcms.exe by the mstsc.exe, where it gets re-executed from the new location.  And finally, a .bat file was written to the disk and executed using cmd.exe, which changes the attributes of that sectcms.exe program to hidden, read-only, and system.  It also deletes all of the intermediate files having extensions .txt, .exe, .gop, .log, and .jse from the temp folder, thus cleaning up after itself and removing the obvious tracks.



And all of that happens just from the historically safe practice of hovering over a link in a document that you have received and opened.



PADRE:  Steve, does this only work on the web-aware versions of Office with PowerPoint?  Because it would seem as if there needs to be some sort of handle for PowerPoint to be running in a browser in order for this to work.  Or is this just default on all installations of Office ever?



STEVE:  I believe that this is - this runs if you receive a PowerPoint document in email.  So it's the viewer, the viewer in email knows how to display a PowerPoint document.  And unfortunately PowerPoint is given the power to respond to a hover action.  And so that's the chink in the link, if you will, that allows then all of the rest of this cascade of actions to the unwitting user, who has been told, do not click any link.  But no one's ever had hover take over their computer until now.



PADRE:  Do not hover over a link.  Do not move your mouse.  Do not turn on the computer.



STEVE:  Or just give up and go home.



PADRE:  Just give up.



STEVE:  Ouch.



PADRE:  Is there a patch for this yet?  Is this something that Microsoft's going to patch up?  Or is this an unpatchable issue?



STEVE:  Not a bug, it's a feature.



PADRE:  Oh.



STEVE:  Yeah, I mean, Microsoft is very, very reticent to remove features because they have huge numbers of enterprise users that have incorporated those features into their daily usage.



PADRE:  I will say this.  I am currently running Microsoft Office 2007.  And my PowerPoint does not have hover management.  It's not new enough to know what to do when that happens.



STEVE:  And I'm on 2003, naturally, because...



PADRE:  Every time I do Windows Weekly, I get Paul Thurrott and Mary Jo Foley making fun of me.  But I'm like, what has changed?  I can still do everything I need to do.  I don't need to upgrade to 365.



STEVE:  No, no. 



PADRE:  And actually, yeah, this saves me.



STEVE:  2003 I acquired a license, back when the MSDN - because I'm a registered Microsoft developer, and so it used to be you had access to all those things.  And so that's a static license that allows me to use it, rather than the new dynamic process, where they're counting and decrementing every use.  So I've already got that.  That'll be my Win7 office suite.  And it works fine.



PADRE:  Now, do you still have your folder?  Because I got that action-packed folder that had all the software in sleeves.  I used to love that thing.



STEVE:  Yup.



PADRE:  I still have a copy of Windows XP, the Enterprise version of Windows XP.  I've been thinking about installing that.



STEVE:  I'm still using a copy of Windows XP.



PADRE:  Oh, that's right.  I use it for academic pursuits, when I want to get things owned really, really quickly.  But, yeah.



STEVE:  So I did want to mention, if we have any people who really want to dig in deeper, the link in the show notes takes what I just said - what I just gave everybody was a summary, believe it or not.  But there is a blow-by-blow, process-by-process, complete breakdown of the exploit of this hover-over-the-link attack, if somebody wants more.



Motherboard has an interesting coverage of the Worldwide Developer Conference last week and of some of the content that Apple was promoting.  Motherboard's title was "Apple Is Trying to Make Your iMessages Even More Private."  And it seems to me that "trying" is the operative word, and maybe a bit of misdirection.  During an interview that the well-known Apple blogger John Gruber, who blogs Daring Fireball is his site - Craig Federighi?  Is that how you pronounce his name?  I think it is.



PADRE:  Yeah, it sounds fair.  



STEVE:  Craig Federighi said that the company has figured out a way to do cross-iDevice syncing to the cloud while still remaining unreadable to Apple.  So the idea being that, if you delete something like an iMessage from one device, the new next-gen iCloud sync will delete that from all your devices, which has not been a feature that they've been able to support until now.  And Apple's touting the fact that they don't have the keys for doing this.



PADRE:  Right.  And that's actually what we want.  We don't want them to have the keys.  We want us to have the keys.  And the most reassuring question that you can ask of any vendor who's trying to tell you that they're protecting your privacy is what happens if I lose my keys/my authentication?  And they should say you're out of luck.  If they say that, then I can trust them.  If they say, well, there's a recovery process, I say, well, then you do have a copy of the keys.



STEVE:  And in fact you heard me last week saying that that's the way SQRL was designed.  And I'm sort of recognizing that SQRL may not be for everyone because it has made a different decision about key recovery than everybody else.  That is, it is secure, and so there is a small responsibility that we go out of our way to manage in a useful way.  But so my point is that I don't have a problem if it's not for everyone.  But for those for whom it is for, it is absolute protection.  And we do all kinds of things that I'll get into as soon as I have this thing published because then it will be more - I'll have more people's attention because it'll be like, wow, what is this?  How does this work?  But I did make that decision, that it's [audio dropout] party authenticator.  There's no one to go crying to.  But even so, we do give you get-out-of-jail-free cards.  You just have to put it somewhere. 



PADRE:  Right.  So let's take this and extrapolate it to the business side of Apple.  This is a great technology.  It's a great moment for security.  But at some point someone's going to say, "I paid so much money for my Apple devices, you can't give me a way to get my encrypted iCloud messages back?"



STEVE:  So Craig said, Craig Federighi said, quote:  "Our security and encryption team has been doing work over a number of years now to be able to synchronize information across your, what we call your circle of devices," he said, "all those devices that are associated with the common account, in a way that they each generate and share keys with each other that Apple does not have.  And so, even if they store information in the cloud, it is encrypted with keys that Apple doesn't have.  So users can put things in the cloud.  They can pull stuff down from the cloud.  So the cloud still serves as a conduit, and even ultimately kind of a backup for them, but only they can read it."



Now, Motherboard says:  "It's unclear exactly how Apple is able to pull this off as there's no explanation of how this works other than from those words from Craig.  The company didn't respond to a request for comment asking for clarification.  It's possible that we won't know the exact technical details," Motherboard writes, "until iOS 11 officially comes out later this year.



"Meanwhile, cryptographers" - to your point, Padre - "are already scratching their heads and holding their breath.  Kenn White, security and cryptography researcher, told Motherboard in an online chat:  'The $6 million question'" - maybe that's in bitcoin these days - "'is how do users recover from a forgotten iCloud password?  If the answer is they cannot, that's a major user experience tradeoff for security.  If you can, maybe via email, then it's end-to-end with Apple managed or derived keys.  If recovery from a forgotten iCloud password is possible without access to the keys on a device's Secure Enclave, it's not truly end to end.  It's encrypted, but decryptable by parties other than the two people communicating.  In that sense, it's closer to the default security model of Telegram than that of Signal.'"



And I say, as I have said to our listeners on this podcast, irrespective of that, I continue to contend that, unless the user is explicitly managing, that is to say, receiving and verifying their communicating co-parties' encryption keys, as for example is the case with Threema, Apple remains entirely free to insert an additional party line key into any or all communications.  I'm not suggesting they are doing so.  But they do have the capability of responding to wiretap warrants.  iMessage is many-to-many messaging, not just one to one.  And users have no visibility into precisely who those many co-parties are.  So, for example, I have a buddy that I send messages to.  Oh, look, it's magic.  No one is ever looking at keys.  Nobody's looking at authentication.  How does that happen?  Apple does that.  Apple manages those keys.  And we know that iMessage supports group messaging, where you could have a bunch of people.  And it's all magic.



Well, everybody I'm sending to has their own key.  So that means that my client must be individually encrypting those individual messages for each party in the message.  Nothing prevents Apple from sticking in an extra key, an NSA key, so that now my outgoing messages are being also encrypted for a key that the NSA holds.  Again, I'm not saying they're doing it.  I'm just saying, this is a classic case of convenience versus security  tradeoff.  Apple, we know, stands for security.  But the architecture and the design is not secure.  They can say, oh, it's end-to-end encrypted.  We can't see.  No, but you could if you wanted to.



PADRE:  Right.  And, I mean, that's the thing.  I like the fact that they seem to be security first here.  But the details are far too sparse.  I need to know exactly what you mean by "we don't have your keys."  Because they seem to be talking around whether or not they can recover, or whether or not they might be able to assist you with getting your data back after a major disaster.  That can't be ambiguous.  But Steve, there's another part here.  And again, this goes into your security versus convenience.  This would be a great feature if I could opt into it.  If you could have a user who could make an informed decision to say, if I lose my authentication, this data is gone, versus something that's just rolled out as the default in iOS 11.



STEVE:  Right.



PADRE:  But then again, aside from you and me and a good part of the Security Now! audience, I don't see any users volunteering the ability to lose their data.  To me, I mean, I have enough trouble convincing the people I live with, who are relatively intelligent, that they should be concerned about their digital privacy.  If I tell someone, well, this is more protection, but if you lose your username and password, everything that you've done before is gone, I don't know a single one of them who would accept that.



STEVE:  Yeah. 



PADRE:  And unfortunately they have to accept that.  If we have a low bar of security for many people, that means there's many people around us who get exploited.  And when many people around us get exploited, it increases the risks that we will get exploited.  That's just how that works.



STEVE:  Well, I can't wait until you and I have a chance to sit down, and I can go over all of the decisions that are part of the final SQRL result, and how it meets these goals, because it really does.



PADRE:  It's going to come down to you talking to your friends and saying, well, have you enabled this?  Because if you haven't enabled it, I'm sorry, you can't be part of my circle because I can't trust you anymore.



STEVE:  So one real quick comment.  I just noticed something that I just got a kick out of, and that is that Microsoft's TechNet blog - and this got by me.  This was in April.  They noted that the Azure TLS certificates were changing.  They wrote in their blog:  "We know security is a top priority for you, and so is uptime of your applications.  To give you additional assurance of the authenticity of Azure services, most Azure services get their SSL/TLS certificates from a known set of intermediate certificate authorities (CAs) that Microsoft operates.  Microsoft publishes details of these CAs in its Certificate Practice Statement.



"Some organizations," they write, "configure their applications with specific CAs, using a security practice" - that we've often discussed here on the podcast - "called 'certificate pinning.'"  And as we know, "pinning a certificate" means that you record the actual fingerprint, the cryptographic signature of the certificate, so that it's not just - you're no longer trusting the whole chain, you are saying, this specific certificate is what we're going to trust.



They write:  "Since CAs expire and get replaced, this practice requires that all applications be updated periodically to use the latest CAs.  If this is not done in time, the application may get interrupted.  To make this process easy for you, Microsoft publishes new CAs well in advance of using them.  The current intermediate CAs used by Azure are due to expire in May 2018."  So this was one year advanced notice.  "Microsoft published a new set of CAs last year," they wrote, "in the July 2016 revision of their CPS (Certificate Practice Statement).  Azure services will begin using these new CAs from July 27, 2017."  So coming up next month.  "If your organization configures your application with specific CAs, then you must ensure your applications are updated by July 27, 2017 to prevent interruption."  So next month.



Anyway, the point was I got a kick out of this.  They finally said:  "Microsoft Azure services were previously signed by either of two intermediate certificates.  They are adding four additional intermediates, and they have a new endpoint."  The original CRL, the Certificate Revocation List distribution point, was public-trust.com.  The new CRL distribution point, meaning the parent of their new certificates, it may not surprise our listeners to know, is Digicert.com, which is my chosen and favorite CA.  So bravo to DigiCert for a nice "get," as they say, being the root signer for Microsoft's Azure services.



PADRE:  And this is what it's going to take for other CAs to stop playing on the gray side.  It's going to, you know, you've got the large providers - Microsoft, Amazon, Google - getting in there and saying, if you're going to do this, we just won't use you.  And let's see how well your business is going to do if we no longer trust your certs.



STEVE:  Right.



PADRE:  So, yeah, kudos to DigiCert; kudos to Microsoft.  This is absolutely warranted, and it's absolutely overdue.  This probably should have happened years ago.



STEVE:  And if you play fast and loose like Symantec was caught doing...



PADRE:  Oh, gosh.



STEVE:  ...by allowing - basically thinking that you're printing money, and oh, look, let's give other organizations the ability for unmonitored printing in our name.  And, well, we know what happens is that Symantec is in trouble because they were playing fast and loose, and now Chrome and Firefox are looking hard at minimizing their trust.



PADRE:  Right.  There's still a timeout; right?  That time period hasn't expired yet?



STEVE:  Oh, yeah.  They're in trouble for a few years.



PADRE:  And there's a reevaluation that Chrome is going to do at some point in the next, is it nine months?



STEVE:  Yes.



PADRE:  So essentially they're in timeout, so they're expiring all their certificates.  They're in timeout right now for new certificates.



STEVE:  Yes.  They're only allowing short new issues.  Yup, they're in the doghouse.  They're way in the doghouse.



PADRE:  This is the security equivalent of go sit in the corner and face the wall.  Wear this hat.



STEVE:  I did find a nice note from a SpinRite user, James Campbell, with the subject "Thank you!"  And here's yet another use of SpinRite.  He says:  "I upgraded one of my NASes and decided to use the old one to set up a server at my church.  The problem was I had 13 hard disks laying around.  I knew that some of them were laying around because I outgrew their capacity, and I knew that some had failed.  But alas, Post-it notes don't stick forever."  So they became unlabeled drives with unknown background.



He said:  "So, SpinRite to the rescue.  Today, 13 disks fully tested, four with catastrophic failures that wouldn't even mount.  And the remaining nine have now all been fully verified by SpinRite.  The old NAS is now stuffed with four matching drives and has been installed at the church.  Thanks, Steve.  Jim C."  And Jim, thanks for sharing your success using SpinRite.



PADRE:  You know, I had something very similar to this.  I had a drive corruption because of - it was a brownout.  So power fail, then brownout, then power fail.  They don't - NASes hate that.  And so I had a Synology, one of their 12-bay or 16-bay?  It was large, a large one.



STEVE:  Nice, nice, yeah.



PADRE:  And when it went down and came back up, six of the drives were reporting to SMART, they said they just weren't going to spin-up.  Pulled all the drives.  Kept them labeled.  Ran SpinRite on all of them.  Put it back in.  And when it came up, it said two failed drives.  And I was able to get everything synced up before replacing the questionable drives one by one.  



STEVE:  Nice.



PADRE:  And again, that was on me because I should have - it's a remote location.  I should have had it set up so that it was notifying me when the SMART counters were getting out there.  But, yeah, it works.  I will say I tried doing the same thing with a ReadyNAS, which is the Netgear NAS.  That did not like it.  It's very, very picky.  Synology, fine.  ReadyNAS, not so much. 



STEVE:  Interesting.  Well, and I think that's one of the problems we're seeing, that RAID in general is trying to get smart and is beginning to have [audio dropout] with drives.  For example, there are some RAIDS now that look at the timing of the drive.  And if the drive seems to be taking too long, they decide, oh, there's a problem here, and go off.  Whereas some drives do take some time, but then come back with an answer, and the RAID has already given up on them.



PADRE:  Right.



STEVE:  So it's sort of, you know, there isn't a standard for this.  And people are making things up as they go along and causing themselves problems.



PADRE:  That's why there's only two drives, two types of drives I use in my NASes right now.  One is Western Digital Red because that's got the TLER control, so you can essentially say, don't figure it out, the NAS does everything, just let the NAS do it.



STEVE:  Time Limited Error Recovery.



PADRE:  Right.  And the second one is Seagate's IronWolf series, which is often because I've got a bunch of 8TB drives.  They also run cool.  I think they will do TLER reconfiguration, as well.  But, yeah, I've had so many drives from different manufacturers, including Seagate and Hitachi, that I put into NASes, and they just - they don't do well.



STEVE:  We've talked on the podcast about "Orphan Black."



PADRE:  Yes.



STEVE:  One of Leo's favorites.  Tatiana Maslany does this incredible job playing...



PADRE:  So many different parts.



STEVE:  ...multiple characters.  I just did want to note to our listeners, since we've talked about it through the years, that the fifth and final season has just started back up again.  And also that [audio dropout] of ours was "Sense8," and that the second season appeared a couple weeks ago on Netflix.  So if anyone had missed that occurring and liked the first season of Sense8, the second season is there.



And last week I meant to mention that the previous day, that is, Wednesday, which was Worldwide Developer Conference Day last week, the SQL to Monument Valley was released.  It was a favorite of ours three years ago.  And in fact it was the 2014 iPad Game of the Year.  Not free.  It's $5 for iOS and also an Android release coming soon, published by Ustwo Games.  For what it's worth, I grabbed it.  And I don't know if it was worth $5.  I went through the entire thing in two sittings.



PADRE:  Steve, tell me what you really think.



STEVE:  I mean, it's beautiful.  But it's sort of more of the same.  So it was like, eh, okay.  I mean, it was interesting and nice.  But again, I would like something that lasted a little more than two sittings.  But, you know, so there it is.



PADRE:  I think Monument Valley is coming to the Nintendo Switch.



STEVE:  Nice.  Makes sense. 



PADRE:  Yeah; right?



STEVE:  They have a much bigger staff.  They used to have eight people.  And what happened was, it was such a grind that they did a little bit of a release.  They released something called - what was it called?  It was an update to - oh, Forgotten Shores.  So they did an enhancement to the original Monument Valley, the Forgotten Shores extension.  But they then wanted to go off and do something else.  They're now 20 people strong, and every single one of those additional 12 people came to Ustwo Games because they had fallen in love with Monument Valley and wanted to do more.  So that's why they ended up saying, okay, fine, we'll do more.



PADRE:  Well, that's the best way to do it, yeah.



STEVE:  And 30 million downloads of the first version.  So, yeah, baby.



PADRE:  Do the math.



STEVE:  I don't mean to say it's not great.  But for me, it was just - it was like I think they were kind of right, after doing the first one, to go try and do something different because it was charming and new.  But the sequel was like, eh, okay, more of the same.



PADRE:  It's like the Matrix movies.  The first one was mind-blowing and had never been done.  And then they tried to put substance and philosophy behind it in movies two and three, and people just sort of said, no.  No, stop.  Stop it.



STEVE:  I have two "closing the loop" pieces from listeners.  One guy, and this was important - well, to me, at least - said:  "Question regarding SQRL."  This is Jonathan Lloyd tweeted:  "How might a user share their credentials for a single site with someone else?  Is this possible?"  He said:  "I ask because many websites don't have a robust permission system.  I still need to send or receive login info at least a few times per year.  In any case, I thought I'd ask.  I don't recall hearing this mentioned on SN before.  Love the show.  I try not to miss an episode."  



That's a great question, and we've addressed it.  The problem is, what if multiple people need access to a single site?  The way it's done today, as we all know, is, hey, what's your password?  Well, could anything be less secure than that?  The point is, because the binding of identity is weak in our current Internet ecosystem, the weak identity binding allows a person to share their weak identity, which is just represented as a username and password, with someone else, who then is able to essentially impersonate them to that website. 



Well, SQRL identity binding is tight.  It is the reverse of weak.  I mean, and in fact a SQRL identity cannot be shared without sharing all of your SQRL identity.  That is, it's, I mean, it is something you cannot share.  So part of the SQRL specification, it isn't a requirement, but it strongly urges what we call a many-to-one mapping, meaning that when a site brings up SQRL support, specifically because SQRL does not allow people to share identities in the same way that people can share passwords, what we're encouraging any site that supports SQRL to do is to allow multiple SQRL identities to be associated with an online account.  So that Mom and Dad, for example, can both use their individual SQRL identities to share access to their banking account.  Or Mom and Dad and the kids can all share access to Legoland.com or something.



And the idea is that you would have a couple users who would be admin or privileged, and some other users who would be guest users.  And so that it would be the privileged users that would be able to invite other users to join with their SQRL account.  And we worked out all the protocols and all of the handshakes.  So the point is, because SQRL's identity binding is so tight, I mean, it really is you, then the way sites have supported a non-many-to-one mapping is like, well, okay, just give them your username and password.  Well, SQRL fights strongly against that.  So instead we're going to strongly encourage SQRL-supporting sites to offer many-to-one identity mapping.  And I think that's, I mean, it's not just SQRL.  Sites ought to be doing this and, like, get with the plan because asking people to disclose their identities to others is fundamentally insecure and really bad security practice.



PADRE:  Right.  This is part of that people training.  It's part of any new technology or process.  People have to understand this doesn't work like it did when you could give someone your username and password, and they could get in as you.



STEVE:  Correct.



PADRE:  This is you.



STEVE:  Correct.



PADRE:  You cannot share this part of yourself.



STEVE:  Correct.



PADRE:  Okay.



STEVE:  And lastly, Carl Green tweeted.  He said:  "Regarding Travel Mode for 1Password password manager," he said, "question.  Can it be en/disabled" - meaning enabled or disabled - "from your iPhone?  Seems to me they would ask you to do that."  And the answer is, well, no.  We talked about 1Password's brilliant travel mode, where you divide your secrets into vaults, and then vaults can be individually tagged as safe for travel or not.  And then before embarking on travel, before you're going to go through customs or approach the mean-looking, or as I said to Leo, the officious-looking TSA agent, you're able to go to the  1Password.com site, and you log in there, and then turn on, you activate Travel Mode.



And what happens then is all of your 1Password-enabled devices everywhere have the vaults tagged as "not safe for travel" deleted.  So the point is they did this right.  They made the control of Travel Mode out of band, as in not something that the 1Password app itself has any awareness or knowledge of visibility of.  So there is no way for anyone looking at it to know that you are in Travel Mode, or that you have enabled Travel Mode.  All they see is a subset of your secrets, those that you have explicitly allowed them to see.  So great question, Carl.  And again, 1Password did it right.



PADRE:  Steve, I might be mistaken here, but it seems that we've actually gotten to the end of your notes.  I don't think we've ever done that in an episode that's been you and me.  We typically stretch things out a lot.



STEVE:  Well, actually I scaled this podcast for the two of us, Padre.



PADRE:  You don't have to pack it quite as full because, I mean, I love engaging you about these things because I...  



STEVE:  And I'll tell you, from the feedback I get from our listeners, they really enjoy you co-hosting.  So thank you.



PADRE:  I take delight in this.



STEVE:  Thank you very much.



PADRE:  Again, I love working with Leo.  I will work with him as long as he will let me work with him.  However, I do enjoy when he goes away on vacation because it means I get to sit down with you for a couple of hours.



STEVE:  And I did hear he's got six vacations planned for this year.



PADRE:  Oh.  I will be down with that, then.



STEVE:  I think we'll be seeing more of you.



PADRE:  Fantastic.  Folks, that does it for this episode of Security Now!.  Now, don't forget that we are live here on the TWiT TV Network every Tuesday at 13:30 Pacific time.  Steve will always be here to inject you with the latest serum that will inoculate you from the security threats of the day, or at least to give you that healthy paranoia that we talked about at the top of the show.  Don't forget you can find all of our shows at the show page at TWiT.tv/sn, as well as on iTunes, Stitcher, YouTube, and wherever fine podcasts are found.



You can also get high-definition, high-resolution audio versions of the show at GRC.com, which is also where you will find everything about GRC, SpinRite, ShieldsUP!, and of course SQRL.  Until next time, I am Father Robert Ballecer.  This man right here is - I'm going to call him Dr. Steve Gibson, Professor Steve Gibson.  And until next time, if you need to think about security, you need to think about Security Now!.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#617

DATE:		June 20, 2017

TITLE:		When Governments React

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-617.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm 



DESCRIPTION:  This week we discuss France, Britain, Japan, Germany, and Russia each veering around in their Crypto Crash Cars; WikiLeaks' Vault 7 reveals widespread CIA WiFi router penetration; why we can no longer travel with laptops; HP printer security insanity; how long are typical passwords?; Microsoft to kill off SMBv1; the all-time mega ransomware payout; Google to get into the whole-system backup business; hacking PCs with vape pens; a bit of miscellany; and a bunch of "closing the loop" feedback with our terrific listeners.



SHOW TEASE:  It's time for Security Now!.  Yes, I am back.  Thank you to Father Robert Ballecer for filling in for me.  We have a lot to talk about, including initiatives by the U.K., by France, and by Japan to infringe on our privacy even more than before.  Steve explains, next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 617, recorded Tuesday, June 20th, 2017:  When Governments React.



It's time for Security Now!, the show where we cover your latest security woes.  Always something to talk about with Steve Gibson of GRC.  Good to see you, Steve.



STEVE GIBSON:  Great to have you back, Leo, all rested and relaxed and raring to go.



LEO:  I thank Father Robert for filling in.  It's nice.  Now he's on vacation.  And I understand he's not allowed to talk on his vacation.



STEVE:  No.  In fact I sent him a note to follow up on, like hours after we finished recording last week, just to say thanks for standing in, and I got an auto-reply saying "I'm no longer receiving email."  It's like, whoa.  So, I mean, and he...



LEO:  Yeah.  How long is that going to be, Lisa, that Robert's - he's on his...



STEVE:  He said two months.



LEO:  He's in his Tertian - they call it his Tertianship.  Two months, wow.



LISA:  [Off mic]



LEO:  Who's going to sign it?



LISA:  A one- or two-star general.



LEO:  A two-star general's going to sign this.  Maybe - this just in, Steve.  We got an email from the National Security Agency.  And this is not a joke.



LISA:  [Off mic]



LEO:  Yeah, but it's been redacted; see?  "The NSA would like to import your Security Now! show onto its Intranet for general dissemination to our cybersecurity workforce."  You're not surprised, so you've obviously received the same email, Steve?



STEVE:  No, but I have a standing invitation from them to come and talk.



LEO:  They like you over there at the...



STEVE:  Yeah.



LEO:  They say, "While we believe under the Creative Commons license this is permissible," which it is, "there are elements within your website's term of use we cannot agree to and therefore precludes us from importing those shows.  Specifically, parts of the Disputes and Indemnities section are inconsistent with federal law."  I think we just pasted it from some website, so I'm not surprised.  "We would like to execute a Memorandum of Understanding between the NSA and TWiT to modify these sections in a manner consistent with federal law."  And apparently - then we said, well, okay, but who's going to sign it?  And they said, well, it'll be either a two- or a three-star general.



LISA:  One- or two-star.



LEO:  One- or two-star.  There'll be stars involved.  So is that okay with you, Steve?  We give Security Now! to the NSA?



STEVE:  That's absolutely fine.  I was hoping that maybe Donald would sign the order, but...



LEO:  It would be fun if - could you get the President to sign it?



LISA:  [Off mic]



LEO:  Okay, just asking, you know.  General Clapper will be signing it.  I don't know who'll be signing it.



STEVE:  Tell Lisa thank you and hi.



LEO:  Yes.  Steve says hi.  Okay.



STEVE:  Cool.



LEO:  No, I saw that this morning, I thought, that is great.  Steve will love that.



STEVE:  I do.  That's great.



LEO:  Yeah, yeah.  Well, you know what, it never fails to impress me the number of people who listen to the show and then the variety of places that they listen.  I think this has, at this point, become the premier show for security information.  And so everybody wants to know what's going on, and no one tells the story better than Steve.



STEVE:  Well, you know, I learned the lesson back when I was doing the Tech Talk column for InfoWorld that endurance is one of the defining factors.  After a couple years, I got a nice note from the then-editor in chief of InfoWorld, Jonathan Sacks, who said - he was responding to one particular column I had written about how optical magneto drives functioned, and the lesson I learned when as a child I dropped a powerful magnet, and it lost its magnetism.  Anyway, so I put it all together into a column, and he said, wow, I just, you know, I love what you're doing.



And I said to him at the time, I responded, and I said, "Gee, thanks.  You know, I kind of feel like sometimes my voice is going off into the wilderness, and nothing is happening."  And he said, "Steve, you have to understand, it takes time for people to get to know you."  And he was referring, of course, to the print version of InfoWorld, which is pretty much all we had at the time.  He said, you know, "They're taking you into the bathroom with them," which is why I wanted to clarify that, "and into bed at the end of the day.  And they have to form a personal relationship with a columnist."  And I think that it's very much the same with a podcast.  And so here we are approaching the end of year 12.  And endurance, as I said.



LEO:  Yeah.  It's true in radio, too, that the longer you last on a show, people just - you become like an old shoe.  And I'm the king of old shoes.



STEVE:  Oh, I've got so many old shoes.  They're much better than new shoes.



LEO:  Very soft and accommodating.  Yes.



STEVE:  So this is Episode 617.  And as I said to you, last week's title was "Things Are Getting Worse."  And looking over the news from this week, I thought, okay, I have to title this one "When Governments React."  So we're going to discuss this week France's, Britain's, Japan's, Germany's, and Russia's - the way I put it in my little summary, each veering around in their Crypto Crash Cars because they're reacting to the crypto problem on the Internet.  We'll also talk about WikiLeaks' Vault 7 reveal of a widespread CIA WiFi router penetration.  And, oh, my lord, the number of vulnerable WiFi routers and access points is what's sort of stunning about this.  It's not like one or two.  Also we got some information, thanks to some fresh reporting, about exactly why it is that we can no longer travel with our laptops.



Some more details about the HP printer security insanity that we originally covered in April.  Now we know the nature of the problem, and it's been exploited in the lab and explained to us.  So we're going to cover that.  A fun chart that was made from, I think it was 32 million passwords, analyzing, for example, in one case, what is the distribution of password length, which is sort of fascinating.  Microsoft has finally said, in the wake of the WannaCry, that they're going to kill off v1 of their SMB, the file and printer sharing protocol, which is what enabled WannaCry to propagate.



LEO:  Good, good.



STEVE:  Yes, although of course now we've got patches.  We also have the all-time mega ransomware payout, which is somewhat stunning.  Google has announced that they're going to get into the, it's not quite "whole system backup" business, but it's closer.  So it won't be competing with one of our sponsors, in fact a sponsor of this particular today's podcast, Carbonite.  But it's kind of related.  We'll talk about that.  And believe it or not, it's possible to hack a PC with, of all things, a vape pen.  We'll also cover a little bit of miscellany and some "closing the loop" feedback with our terrific listeners.  So I think another great podcast.



So our Picture of the Week is brought to us by the ever-clever xkcd.  And reading the caption is the way to explain this best over the air.  I titled it "Lunch Order."  And the caption reads:  "Everyone complains about autocorrect.  But we forget about the time it prevented a nuclear war."  And so we have some guy who's approaching the control console who says, "Sir, Strategic Command has sent us a lunch order."  And the supervisor says, "Don't they have anything better to do?"



LEO:  Not launch, lunch.



STEVE:  Exactly.  Wonderful.



LEO:  Lunch.  That's pretty funny.



STEVE:  Okay.  So in our coverage of how governments are reacting, the first is a combined "French-British Action Plan," as it's titled.  I've got the PDF to it.  It's just two pages.  But I'll just pull the highlights from it.  The link to the PDF is in the show notes.



So it says:  "Terrorists, and the people they influence, are using the Internet, websites, email services, and social networks to gather information, organize, spread propaganda and operating methods, send and receive instructions, and claim responsibility for their acts.  At a meeting in Paris on the 13th of June" - that is, just a week and a half ago - "Prime Minister May and President Macron agreed to a joint U.K./France initiative to ensure the Internet cannot be used as a safe place for terrorists and criminals.  They stressed that coordination with G7 and EU partners will be sought on these issues.  The following four points were agreed as priorities."



I'm not going to go into them in detail, but the first one was improve methods to remove illegal content from the Internet.  And so the idea of, by coming up with ways to prevent unwanted...



LEO:  Bomb-making stuff or...



STEVE:  Yeah, exactly.  So, for example, they said:  "While efforts have been observed from companies regarding removing terrorist content, we need industry to move from their current position of reactively removing content when it is notified to them, to proactively identifying content and preventing it from being made available on their platforms in the first place."



So this is getting kind of dicey because, as we'll see, essentially governments are, as we've been predicting, wanting to exert explicit control over what has historically been a communications commons, a global commons, where people had anonymity and the freedom to put up what they ever wanted to, and the presumption was people would use their best judgment in how to decide about the veracity of the information presented.  The second point was...



LEO:  By the way, you remember, you're old enough - we are old enough - to remember "The Anarchist Cookbook."  Remember?



STEVE:  Yes.



LEO:  In the '60s everybody wanted to take that out of every library - it was pre-Internet, obviously - because it had bomb-making recipes in it, and free speech won out.



STEVE:  Yes.



LEO:  And you've always been able to get "The Anarchist Cookbook."  And of course now you can get it on the Internet.  So this is nothing new.  Governments have been trying to do this forever.



STEVE:  Right.  I guess maybe what's different is, you know, we've talked about...



LEO:  We're more scared.



STEVE:  Well, we've talked about how law enforcement is complaining on one hand that they don't have the access that they want to the communications.  But on the other hand, they've never had more access in the history of man to online activities.  So I think it's natural for law enforcement to want everything that they can get, and it's probably also useful for there to be a compromise, for there to be some pushback so that it's just not like, I mean, I guess what I'm trying to say is...



LEO:  Well, this first part is they want to censor the Internet.  They don't want stuff to be on the Internet because you could use it, partly because of maybe bomb-making information, but partly because it would be terrorist propaganda designed to encourage the weak-minded to become terrorists.



STEVE:  Right.  Well, in fact...



LEO:  That's government censorship.  And in fact, it's, what do they call that, before the fact?



STEVE:  Oh, wait.  Just let me think.  It was, well, we're coming to it.  Actually it's been called "precrime."



LEO:  Yeah, yeah.



STEVE:  And that's, well, I can't...



LEO:  We'll get to it.  I don't want to - I'm sorry.



STEVE:  We'll be getting to it in a second.



LEO:  I'll shut up.



STEVE:  Oh, it's Japan.  Anyway, that's next.  So the second point you were just alluding to, quote:  "Support the efforts of civil society organizations to promote alternative and counter narratives."  So they're saying...



LEO:  I don't have a problem with that.



STEVE:  Right.  Pull down the bad stuff; put up the good stuff.  But then number three is what has been lurking around, and I think it's inevitable:  "Work together to ensure our countries can access data for investigative purposes."



LEO:  Yeah.  This is the snooper stuff.



STEVE:  Yes.  And so we have 3.1 under that:  "Seek to preserve the retention and access to traffic and location data.  Under current terrorist threat levels, the ability to retain data useful to investigations remains essential."  Second point:  "Enable subscription holders to be identified in all circumstances."



LEO:  Oy.



STEVE:  So now we're talking about loss of anonymity.



LEO:  Yeah.



STEVE:  They said:  "A single Internet Protocol address can be shared between hundreds of users accessing the Internet or social platforms via their smartphones.  The capability to identify specific users is important, particularly where suspects have accessed terrorist content."  So then they said, under their proposals:  "Share expertise and legislative experience regarding these issues, including with Europol, with a view to intensifying dialogue with the industry."  So the idea being they're saying now IP address is no longer sufficiently granular because of course everyone behind a NAT or behind a Tor proxy, or any other major proxy or VPN, has their traffic mixed together.  So they're beginning to say, you know, we want some way to penetrate identity beyond IP.



And then of course the inevitable 3.3:  "Allow access to encrypted content."  And they said:  "When encryption technologies are used by criminal groups and terrorists, it must be possible to access the content of communications and their metadata.  This is not about," they say, "backdoors or banning encryption, but ensuring governments and companies develop shared solutions to this issue."  Of course without suggesting how we achieve that miracle.  And so they said under their proposals:  "Share strategies on the challenge of accessing content from encrypted services, and coordinate our engagement with the major communications service providers."



So here again we're seeing this move towards the ability to move, at least in the U.S., we have a Constitution which protects us against unwarranted search and seizure, so you get a warrant in order to do that.  I'll be very surprised if that isn't what happens with the U.S.  And then this is a challenge globally because we're all sharing a single big network at this point.



Anyway, the second country, or the third country - that was France and Britain getting together.  Last week Japanese Prime Minister Shinzo Abe's government passed a controversial piece of legislation giving prosecutors - get this - the power to monitor and arrest people in the planning stages of crimes.  So this is what some coverage of this has called "precrime."



"After an all-night legislative session in Tokyo, lawmakers, who were deliberately delayed by the bill's opposition, finally voted to pass the so-called 'anti-conspiracy bill,' controversial legislation that gives prosecutors the power to monitor and arrest people in the planning stages of crimes.  The government claims this is needed to bolster counterterrorism precautions ahead of the 2020 Tokyo Olympics.  Under the bill, terrorist groups or criminal organizations could be punished for the planning of" - and I thought it was amazing that there was a number put on this - "277 different crimes" - apparently they're enumerated - "ranging from arson to copyright violation."



LEO:  Uh-oh.



STEVE:  Uh-huh.  "Critics of the legislation argue that the legislation is vague and could lead to the suppression of civil liberties and excessive state surveillance.  It's also seen by many as a preamble to Abe's ambition to revise Japan's constitution.  Commenting about this, a professor of political science at Sophia University in Tokyo was quoted:  'This fits Abe's agenda in the run-up to a prospective national referendum on constitutional revision and Japan's possible involvement in future wars.  Both of these would require new means to control unruly citizens who object to government decisions.'"



So, yeah.  And not surprisingly, Russia is moving forward.  We discussed, I think it was last year when Russia changed some laws to ban some classes of VPNs.  And you'll remember, Leo, that one VPN provider in particular, Private Internet Access, pulled their service from the country after they were raided and had some of their servers seized.  So now there's a new surveillance bill in the Russian parliament, promising to deliver "greater security" to the country.  But as with so many countries, the bill's effect looks like it's going to do the opposite, mandating new encryption backdoors and imposing new data retention requirements on ISP and VPN providers.  So this legislation is expected to take effect in 2018, next year.  And the new law - this is a little chilling - would require messenger users, that is, users of messaging apps, to verify their real-world identities using their phone numbers with Russian mobile phone operators.



LEO:  Oh, god.



STEVE:  So you will be [audio dropout] explicitly deanonymized, so you can no longer use anonymous messaging [audio dropout].  Your messaging identity has to be tied to your real-world identity.  So something that we've been taking for granted about the Internet and that no doubt a lot of Russian citizens appreciate will become unlawful next year.



And there's also some additional impositions imposed on VPN providers.  In Russia, broadband users, as we've been covering, have increasingly turned to VPNs to avoid the growing list of censored websites.  To help thwart such usage, the bill would not only impose steep fines on VPN providers who don't agree to block blacklisted websites, but would require ISPs to terminate [audio dropout] of those VPN providers who do not comply.



The legislation reads, quote:  "As it stands, the bill requires local telecoms watchdog Roskomnadzor to keep a list of banned domains while identifying sites, services, and software that provide access to them.  Once the bypassing services are identified, Roskomnadzor will send notice to their hosts, giving them a 72-hour deadline to reveal the identities of their operators.  After this stage is complete, the host will be given another three days to order the people running the circumvention-capable service to stop providing access to banned domains.  If the service operator fails to comply within 30 days, all Internet service providers will be required to deny access to the service and its web presence, if it has one."  So within Russian borders, basically they're legislating out of existence some of the fundamental operating flexibility and freedom that the Internet has provided.



And, lastly, Germany.  A follower of ours, Ian Beckett, often sends me photos of pages because it's just easier for him to do, and this one is an article from The London Times headlined:  "Germany to change law on encryption."  And the article reads:  "Laws to enable security services to see messages before they're encrypted by providers such as WhatsApp are being drawn up in Germany because of concerns over secret communications between Islamist terrorists.  Angela Merkel's government believes that the same balance of eavesdropping and privacy should exist in the digital age as in the analog era of letters and phone calls.  Ms. Merkel aims to put digital security on the agenda for the G20 summit that she's hosting in Hamburg next month.



"Theresa May [as we know] has also called for a global approach to regulating digital providers, saying during the election campaign that there should be no 'safe space' for terrorist ideologues.  Germany is known as one of the countries most protective of personal privacy because of the legacy of surveillance by the Nazi regime and the Stasi secret police of communist East Germany.  However, terrorism in Europe is fueling calls for change.



"British authorities were incensed that they could not access the last WhatsApp message sent by Khalid Masood, the Westminster attacker, minutes before he began his killing spree by driving into pedestrians and fatally stabbing a policeman.  The messaging company, owned by Facebook, said that its service" - meaning WhatsApp, of course - "that its service was so secure that no one but the sender and recipient could see a message, not even WhatsApp itself.



"'We want messenger services to have an end-to-end encryption so that the communication of respectable citizens is undisturbed and secure,' said [Thomas de Maizire], the German interior minister.  But 'Nevertheless, security authorities need the option of access under certain circumstances.'  That would allow the authorities to read a suspect's communications before it was encrypted, he said."



So standing back from all this, we should remember that, as we know, it's in the commercial interests of Facebook, WhatsApp, Apple, and so on to claim that they are unable to read messages because their customers state that they want security.  I would argue that evidence suggests that people want it, but they're not that concerned about it.  It's like, yeah, if I can have it, that's fine.  But if you give me an ice cream cone, I'll tell you my password.  So no biggie.  But remember that the actual tradeoff for the convenience of users not being burdened with explicit key management and endpoint verification, for example, as Threema requires its users to do, is that any of these providers can in fact tap into their service's communications.  So I'm not saying they can do this retrospectively at the moment, though that capability could be added.  But we do know that they could do it prospectively, as with a wiretap order, under a warrant.



So anyway, I think that the future is uncertain.  But as our listeners know, this is one of the reasons that I stopped work years ago on my own VPN solution, because from our own coverage of what we saw happening it looked like the handwriting was on the wall and that it was going to be impossible to have truly secure communications that were unbreakable.  Which is unfortunate.



LEO:  Let me, okay, I'm going to play a little devil's advocate.



STEVE:  Good, good.



LEO:  I mean, is there not a way to balance privacy with security a little bit?  So, for instance, there's nothing I'm sending in my emails that really need to be private.



STEVE:  Yup.



LEO:  And if it helped prevent another Westminster Bridge attack to have those rules, wouldn't that be kind of, I mean, there's conflicting needs, obviously.



STEVE:  Correct.  And I agree with you completely, Leo.  I use iMessage because I'm sending tweets to my friends about what's going on or, I mean, iMessages to my friends.  And, I mean, I understand the position of people who are strongly opposed to any kind of opportunity for surveillance.  I mean, I want to respect that.  But the fact is, no one using WhatsApp or iMessage or Facebook Messenger, no one using those tools actually has that.  So again...



LEO:  You're saying that because the companies that make those programs actually could, if they wanted to...



STEVE:  Yes.



LEO:  ...access the communications.



STEVE:  They are controlling the cryptography and the keys.  And iMessage is a multi-way messaging system.  So if someone said to Apple, "We must have this communications," then an additional key could be added, and the user would have - it's completely nontransparent.  



LEO:  However, Signal and Threema and a variety of other apps don't have that flaw and are in fact secure.



STEVE:  Correct, correct.



LEO:  And presumably bad guys know that.



STEVE:  Well, yes.  And notice that in this...



LEO:  Although, wait a minute, he used WhatsApp, so maybe he didn't know that.



STEVE:  Well, precisely.  And I would argue that somebody who really cares isn't going to be using one of these easy mass use.  Although there was the interesting wording here about "capturing before encryption." 



LEO:  Eh.



STEVE:  So that suggests that the legislators have been having hearings and are listening to people saying once it's encrypted, it can't be decrypted.  And they're saying, well, how about then before it gets encrypted?



LEO:  Before, yeah.



STEVE:  And we all know, as you're typing it in, and you're seeing it on the screen before you hit Send, it's sitting there in the clear.  So, I mean, and this is why we've said on this podcast, the only way to actually have true security is for two naked people to meet in the middle of Central Park under an umbrella, or throw an opaque bag over their head so no one can read their lips, and then whisper to each other.  I mean, if you're using technology, it provides lots of benefits.  But actual security is an illusion.  I mean, absolute security is an illusion.



LEO:  And then, on the other hand, you can say, well, the terrorists win because we have decided to abridge our own liberties to protect ourselves.  And really, I mean, as bad as this is, the risks of death or injury from terrorist attack are very, very, very small.  Vanishingly small.



STEVE:  Well, and Leo, I would also argue that governments want the ability to eavesdrop.



LEO:  For other reasons.  Well, you see when they mention copyright; right? 



STEVE:  Right.



LEO:  That they're not just protecting us against people who would harm us.  They're...



STEVE:  Well, and how many times have they marched out the child abuse and child pornography?  And so the point is some of these things are a means to the end that the government wants. 



LEO:  That's something you want to watch.  They're using fear to further their own agenda which has nothing to do with terrorism.



STEVE:  Although in the U.S. we have a Constitution that says, if a judge decides that there is probable cause for someone's phone to be tapped, historically law enforcement is able to do that.  And I argue I don't think that should change.  I mean, to me, that seems like it's a tradeoff that has worked.  And the challenge is that doing it without introducing extra vulnerability is tricky.  That is, that's why, for example, selectively adding a key to a dialogue under warrant seems like the right tradeoff, where an additional encrypted stream is captured, and only the matching key can decrypt it.  That seems controllable.  But if a system is in place where it's possible, for example, to put a tap in before the encryption, then you really - that is a backdoor.  You really are then opening it up to abuse.  And it's difficult for me to see from a technology standpoint how you keep the bad guys from being able to pry that open, too.  And so, again...



LEO:  That's another issue, absolutely, yeah.



STEVE:  Let's just move forward slowly on this and hopefully not have some bad legislation occur.



LEO:  It's also, you know, possible to get paranoid, over-paranoid about that stuff.  For instance, I was really worried about taking a laptop and phones outside the United States, for fear of what would happen as I crossed the border.  And of course nothing happened.  It was the easiest thing.  It took me 15 seconds to get back into the U.S. with my U.S. passport.  So, yeah, you know, you can get over-worried about this kind of thing, as well.



STEVE:  Well, in fact I know that the U.S. saw a dramatic drop in foreign tourorism [sic] during the whole...



LEO:  Tourism, not tourorism.



STEVE:  Tourism.



LEO:  I don't want to confuse the two.



STEVE:  A drop in tourism during the early days of the new Trump administration with the travel ban.



LEO:  Right, right, like 17% or something, yeah.



STEVE:  Yes.  Not that people couldn't come in, they just didn't know.  And so they didn't want to get on a plane if they were going to be sent home after reaching the other end of their flight.  So exactly as you say, just the uncertainty creates a chilling effect.



LEO:  And if I were a brown Muslim, I might have gotten a lot more hassle coming into the country, U.S. citizen or not.



STEVE:  Sad as that is.



LEO:  Yeah, unfortunately.



STEVE:  So WikiLeaks dropped another blob of Vault 7 leaked documents from the CIA, and we learned last week of a project called CherryBlossom.  And it's a little chilling.  This is a rather comprehensive WiFi router and access point hacking system, in place and used by the CIA.  And I would argue that perhaps the most breathtaking aspect is the breadth of exploitation possible.  I have a link to the PDF document of the affected devices in the show notes.  And although in the document they're in alphabetical order, I first snapped this that wasn't.  But it looks like maybe they're in most popular order:  Belkin, D-Link, Linksys, Aironet/Cisco, the Apple AirPort Express, Allied Telesyn, Ambit, AMIT Inc, Accton, 3Com, Asustek Co, Breezecom, Cameo, Epigram, Gemtek, Global Sun, Hsing Tech, Orinoco, PLANET Technology, RPT Int, Senao, US Robotics, and Z-Com, and many models of all of those routers.



As we know, many of these routers share common firmware across their model line, where they just have different numbers of ports and antennas and speeds and things.  But the core firmware is the same.  And so, I mean, I didn't count the line items.  I don't want to say all models of all of these routers, but anyone interested should go look.



With this latest batch of leaked Vault 7 documents are the details of what is basically WiFi device firmware hacking framework, which is being used by the CIA for monitoring Internet activity of targeted systems by exploiting vulnerabilities in these WiFi devices.  So this was reportedly designed in a joint effort by the CIA, with the help of SRI International, of all people - I was surprised by that, you know, that's Stanford Research Institute, which is a U.S. nonprofit research institute, I think they're located in Palo Alto - as part of its CherryBomb project.



So CherryBlossom is a remotely controllable, firmware-based implant for both wireless routers and access points, which exploits router vulnerabilities to gain unauthorized access and then replace the firmware with custom CherryBlossom firmware.  So the wireless devices are implanted with this custom CherryBlossom firmware.  And since many devices support over-the-network updates, physical access is not required.  Once implanted with this firmware, these devices then perform, not surprisingly, man-in-the-middle attacks to monitor and manipulate the Internet traffic of their connected users.



So in the documentation, which is extensive here - there's even an installation guide - it states that the CherryTree command-and-control server must be located in a secure, sponsored facility and installed on Dell PowerEdge 1850 powered virtual servers running Red Hat Fedora 9, with at least 4GB of RAM.  So the whole toolkit is laid out.



And these compromised routers and access points naturally are able to monitor network traffic to collect email addresses, chat usernames, MAC addresses, and VoIP numbers.  They're able to redirect connected users to malicious websites, or non-authentic websites, I guess I would phrase it, to inject custom content into the data stream to fraudulently deliver - well, now, in the bullet points here it says "malware and compromise the connected systems."  I would argue maybe it's "mal" depending upon your perspective.  Setting up VPN tunnels to access clients connected to Flytrap's WLAN and LAN for further exploitation, Flytrap being another one of the monikers in this system.  And also the ability to copy the full network traffic of a targeted device, essentially exfiltrated to some remote server for later analysis.



So again, what we're seeing here, sort of in all of this post-Snowden era, is that unfortunately our law enforcement agencies have lost control of apparently much of their secret toolset, or at least lost control of the documentation; and that in fact, as I said earlier, the fact that there are WiFi access points and routers, which used to be a rarity, I mean, it was like, well, do you have WiFi?  Now you don't even ask the question.  You open up WiFi on your phone, and you have to scroll through a list of access points that are within range.  So again, it's not like there's a lack of capability and a lack of a target-rich environment for law enforcement to access.  And I don't think that's going to change.  So it's difficult to see anyone complaining that they don't have the access that they want.



LEO:  By the way, Steve, as long as we're talking about Internet access, you are breaking up periodically.  Not so badly that I'd want to start over, but every once in a while I get a little blip, a little hit on you.  It could be us, could be you, don't know.  Don't know.



STEVE:  And how did that compare to what happened with Renee after about an hour?



LEO:  It's very similar.



STEVE:  Okay.



LEO:  And I guess FLOSS Weekly was having problems this morning.  So it may well be us.



STEVE:  Well, and remember - well, remember, too, that Microsoft has redefined the way Skype connects.  We used to be able to get a direct point-to-point connection.



LEO:  Oh, I know, yeah.



STEVE:  And they shut that down.  We're now routed through Skype servers.



LEO:  Huh?  FLOSS was Randal, okay.  So, yeah.  Yeah, it could be that.  But the weird thing is, there are some hosts we never have problems with.  So I just don't know what it is.  I really don't.  We've tried all sorts of things.  But anyway, just a note.



STEVE:  Yup.



LEO:  Nothing to do about it.



STEVE:  Okay, so I have this in my show notes:  "Why we can't have nice things."  And I said:  "Or take our nice things traveling with us."  We finally got some interesting details about those increasing restrictions which have been imposed on traveling with electronics.  And essentially what we learned was that Israeli hackers reportedly got into ISIS networks and found they were building laptop bombs.



Two reporters, David Sanger and Eric Schmitt, reported that top Israeli cyberoperators penetrated a small cell of extremist bombmakers in Syria several months ago, and that was how the U.S. learned that the terrorists were working to make explosives that could pass through airport X-ray machines and other screening by looking exactly like batteries for laptop computers.  And according to two American officials that the reporters used as anonymous sources who were familiar with the operation, the intelligence obtained was so complete that it enabled the United States to understand how the weapons could be detonated.  The information helped prompt a ban in March on large electronic devices in carryon luggage on flights from 10 airports in eight Muslim majority countries to the United States and Britain.



And of course it was also, unfortunately, part of the classified intelligence that the U.S. President Donald Trump is believed to have revealed to the two Russian officials, foreign minister Sergey Lavrov and the ambassador to the U.S., Sergey Kislyak.  It was the disclosure of that classified intelligence that reportedly upset, greatly upset Israeli officials because it revealed the fact that the security surrounding this very small cell of extremist Syrian bombmakers had been successfully compromised, thus putting future intelligence gathering at some unnecessary risk.



So in any event, we now know a bit more about what's going on.  And of course, as an engineer who travels with electronics, I've always been a little bemused, you know, because like for years the TSA agents would say, "Turn that on."  And it was like, what, really?  And the second the screen lit up they would say, okay, that's enough.  And it's like, so as an engineer, the whole issue that laptops are a risk if they won't power up, but they're not if they will, I mean, it just never made any sense to me because you'd need to be extremely incompetent and way less technical than anyone you could find to believe that such a weak verification would make any sense.  I just don't get that.



And in another little weird anecdote, I last week ordered a high-capacity capacitor.  It was a multi-farad supercapacitor.  And I didn't note that it had lithium as an ingredient.  I ordered it online from my favorite supplier [audio dropout].  And I got a phone call from them saying, "Hey, you know, you asked for this to be sent to you via priority mail, but this has lithium in it."  And I said, "It's not a battery.  It's not going to explode."  And she said, "Well, I'm sorry, but it's got lithium, and you can't send anything with lithium through the mail."  And I said okay.  So anyway, it's coming to me via, I don't know, FedEx or UPS.  Either of those two carriers are able to take it.



So again, this is because the word, it's got the "L" word in it.  Even though it doesn't have lithium-ion chemistry, it still spooks people, and so we're being put through unnecessary inconvenience as a result.  Although I'm not saying that lithium batteries don't explode.  They do.  But lithium capacitors do not.



Okay.  So earlier this year, it was in April, actually, couple months ago, we talked about the serious problem with printers, that in this case it was HP printers that are widely distributed around the globe and had some unknown vulnerabilities that HP was fixing in a firmware update.  There was no information, no additional background about that.  But after HP's detail-lacking April security bulletin, some guys at the security firm Tenable decided to take a closer look under the hood of HP printers.  So they purchased a pair of HP OfficeJet Pro 8210s.  They bought two so that they could leave one without updated firmware, and then update the firmware of the other one, and then do some comparison.



Well, it turns out that - get this.  In the printers, auto-updating of the firmware was disabled by default.  So immediately that suggests that most users of HP's printers that have been known vulnerable since April, and now we have a complete disclosure as a result of this research, will not automatically receive fixes for the problems that have been found.  We'll have a takeaway for the listeners of this podcast in a moment.  But I took a screenshot from Tenable's reporting, showing that the firmware in one of these printers was dated April 28th of 2016, so a year old.  And also showing down at the bottom "Do not check for updates" is the default setting.  So that means that, globally, assuming that this is the default setting for HP printers, at least of this firmware family, which we can assume, these machines will never be updated unless users are proactive, which is what I hope our listeners will be.



So the Tenable guys manually updated one of the two identical printers' firmware in order to have, as I mentioned, both before and after patched images.  But it turned out they didn't need that.  They first used NMAP to scan the printer for open ports and found an unsurprising set of ports - 80, 443, 8080, and 9100 - 9100 the so-called "JetDirect" port, which turns out to be hosting many problems.  And it is present on all HP printers.  It's sort of the default means for the HP printer driver to talk to a networked printer.



So by using a PC-based instance of the very powerful netcat utility, they experimented connecting to port 9100 and sending a bunch of different commands in.  And one of the things they played with was the old-school path traversal exploits, where you prepend a directory list command, for example, with /../../.., the idea being that, if a server isn't protected against that, you're able to walk back to the root or upstream of the directory which is your normal logged-in directory for that service, essentially in order to escape from what weak containment that server provided.  So by doing this they were able to explore within - and this is all [audio dropout] over the network.  They didn't have to open anything up or pry any chips loose or anything.  This is just talking to port 9100.  They found the linux/bin directory, demonstrating that it's possible to traverse into that printer's Linux directory.



Okay, then, using three PJL commands - this port 9100 supports several different protocols.  One of them is called the Printer Job Language.  And there's an FSQUERY, FSUPLOAD, and FSDOWNLOAD command which provides them with read/write access to the printer's file systems.  So then they demonstrated that using two of those, FSQUERY and FSUPLOAD, combined with a directory traversal, they're able to retrieve the contents of, for example, the printer's /etc/password file.  Which is, again, another standard Linux component.  And then, by using the FSDOWNLOAD command, they discovered that, after they had surveilled the exposed file system, they were able to add a static invocation of the netcat command upon boot-up.



Okay.  There's no reason that printer firmware should have the netcat command in its binary directory except that someone just didn't care.  They just took a standard Linux build and put it in this printer, which is insane because with netcat you can, I mean, there's no purpose for the developer to use netcat.  Maybe it would use it with a script to go fetch firmware updates.  But that's lazy and sloppy because netcat is, as I mentioned before, a very powerful tool, which among other things allows you to create servers, sort of ad hoc servers, just from a command.



So they added to the startup script an invocation of netcat that would create a fully remotely accessible command shell on any available port of their choose.  So after doing that, they then rebooted the printer.  It came up, ran the startup script.  Netcat was launched, listening for incoming connections on the port of their choosing, which would then allow anyone that knew where the port was to remotely connect to and have full shell access to the printer.



Now we know what was found two months ago, and we know that the world is full of remotely network-exploitable HP printers containing seriously vulnerable and exploitable firmware, which HP has no ability to remotely update, assuming that the default across a large population of those printers is the same as it was in this case, which is it will not update its firmware by default.  Which means that this install base of printers makes perfect hosts for the insertion of advanced persistent threat malware within both consumer and enterprise networks.  So if our experience teaches us anything, we know that these highly vulnerable printers will continue to exist on networks forever until they finally die and are decommissioned.  But to the degree that they are alive, they will never go away.



LEO:  And they could be used to, what, spread malware?  What kinds of stuff?



STEVE:  Well, they could be used as a persistent outpost for any sort of threat actor.  For example, it could reach out from inside the network to establish a connection to a remote server [crosstalk]...



LEO:  So in a foreign country don't, "Thank god, an HP printer."  Somebody could compromise it and then have full access to our network, essentially.



STEVE:  Correct.  And persistent access.  That is, even if you shut down all the power and then brought it all back up again, this thing would come back up and reinstantiate itself and reconnect to the remote command-and-control server.  



LEO:  It's all the Jetdirects? 



STEVE:  It's, I would say - so the takeaway for our listeners is everyone within range of this podcast should proactively update any HP printer firmware within their control and responsibility as soon as possible.



LEO:  Ugh.



STEVE:  So just fire up the printer manager that gets installed when you install an HP printer and just go and update its firmware. 



LEO:  Has HP pushed the update?



STEVE:  They can't because the...



LEO:  No, but, I mean, but if you asked for it, there is a fix?



STEVE:  Yes, yes, yes.  For the last two months.



LEO:  Okay.



STEVE:  And this is the problem is the printers aren't updating themselves.



LEO:  They're not updating; right.



STEVE:  And so the printers in the closets, the printers next to the water cooler, I mean, again, our listeners need to update their HP printers.  But unfortunately, not everyone listens to this podcast.  And there's going to be just a massive install base of HP printers.



LEO:  We know the NSA will be safe, and that's - thank god for that.



STEVE:  Ah, yes, we do.



LEO:  Update your printers, gentlemen.



STEVE:  I'm sure they are.  GitHub hosted a fun analysis.  They took the top 32 million passwords and ran some analysis of them.  I have [audio dropout] in the show notes and a link to the GitHub page that has two additional graphs.  But this is fun.  So this is the distribution of password population by length.  So, for example, and I don't even know who has a four-character password, maybe that's a PIN, but 0.265% is four characters.  0.612% is five.  2.47% is six characters.  Fewer, 2.06% is seven.  Okay, then the big one.



LEO:  We like even numbers.



STEVE:  More than any other.  Huh?



LEO:  We like even numbers.



STEVE:  Is eight.



LEO:  Yeah.



STEVE:  Oh, you might be right, is eight, which is 23%.  So 23% of the top 32 million passwords which were analyzed from this repository had eight characters.  And then the chart just sort of falls off from there:  16.74% have nine characters; 16.16 had 10; 11 characters were 12.89; 12 characters was 10.68; 13, 7.68;  14, 4.66.  So now with 14-character passwords we're down to less than 5% of them.  Impressively, there were some 15-character passwords.  Now, these are LastPass users or 1Password or, you know, something...



LEO:  Yeah, I do like 20 if I can, 24, 38, 70.



STEVE:  Exactly, exactly.



LEO:  You have 64-character passwords.



STEVE:  But still, 17 characters is as far as this goes.  And so we're back to 0.26%.  And as we know, unfortunately, there are many sites that will say, oh, sorry, you can't use more than 15.  And so it's not surprising that there are not that many that have such long passwords.  But so basically the sweet spot, such as it is, is eight characters.  And what this does tell you, you have to know that anyone brute-forcing is - because remember, all you get is a go/no go after you try, after you make a guess at a brute-force password.  So the brute-forcing people know this distribution well.  And that means they're going to focus on all possible combinations of eight characters as their first choice, and then go to nine, 10, 11, 12, 13, and so forth.  Which does not suggest that you should use four, five, six, and seven characters.



LEO:  No.



STEVE:  Because it just takes no time to blast through the total possible brute-forcing of such a small number of characters per password.  So it's certainly the case that longer passwords are better.  But it's interesting that almost a quarter of all passwords are eight characters.



LEO:  Did you see the next chart?  Nearly 90% of the passwords used only lowercase letters and numbers.



STEVE:  Yup.



LEO:  So you want to help, put a little punctuation in there.



STEVE:  Just throw a little something unexpected in there.



LEO:  Right.  Hit the shift key.



STEVE:  Makes a big difference, yes.



LEO:  Yes, hit the shift key.  Adding a single special character to a password composed of lowercase letters makes it theoretically 15 times less common.  So there.



STEVE:  Nice.



LEO:  That's where LastPass really saves you.



STEVE:  Yeah, it does.  So, okay.  We all know that the WannaCry worm malware - oh, and by the way, the creation of that has recently been attributed to a cyberwarfare group operating out of North Korea.



LEO:  Ah.



STEVE:  So attribution is always difficult.



LEO:  That's what we thought.



STEVE:  Yup, attribution is always difficult, but that's where things are pointing.  So WannaCry leveraged the no longer, and I would argue not for a long time, mainstream, hasn't been mainstream, v1 of Microsoft's Server Message Block, the SMB, also known as Samba, protocol.  And we also know how difficult it has always been to remove support for legacy protocols because there's always something somewhere that is no longer supported, cannot be updated, is mission critical, and only understands the legacy protocol.  Thus these things tend to never die unless they're just forced, finally, to die.  Microsoft has finally stated that, finally, in the future, even though they've been trying for five years, they're going to disable the support for SMBv1.



Ned Pyle, who is the principal program manager in the Microsoft Windows Server high availability and storage group, told the guys at BleepingComputer that plans to disable SMBv1 have been in the works at Microsoft for the past five years.  He said that the security issues of SMBv1 were the main factor in deciding to disable the protocol - yeah, after WannaCry forced you to back-patch Windows XP - but the fact that SMBv2 was released nine years ago was also a factor.  Meaning that we've already had the successor for nearly a decade.  Pyle also said Microsoft would prefer everyone use SMBv3, which is five years old, released in 2012, as the standard.



As we have similarly seen, for example, with the earlier insecure SSL versions, if SMBv1 is available and enabled, even if it's not in common use, if it's available and not disabled, attackers can force a downgrade.  They can perform a so-called "downgrade attack" from the use of the newer and improved protocols to the known-exploitable protocols.  Remember that one of the reasons we finally shut down all earlier use of SSL in favor of TLS was that, for a long time, for the sake of backward compatibility, even though we had newer versions of our secure HTTP tunnel protocol, TLS, servers still supported SSL.  But when offered a choice, clients were able to say, oh, no, no, we don't know the new stuff, in order to force a downgrade to the use of the older and typically less secure fallback protocol, and thus subject servers to victimization through their willingness to support the older protocol.



So anyway, given that running SMBv1 is no longer necessary for modern enterprise users, and having it around opens up a significant security vulnerability, this Ned Pyle says it's time for it to be put to rest.  He claimed the ubiquity of SMBv1 had made taking action more difficult.  But he finally confirmed that, when Windows 10 Redstone 3 is released, both in the user and the server variants, SMBv1 will be disabled by default for the first time in Microsoft's history, since it was created.  And we've had file and printer sharing since, what, Windows 3?  I mean, it's like, it's always been there.  So, yeah, sayonara.  And good riddance.



LEO:  Yeah.



STEVE:  Oh, my goodness.  And the biggest ransomware payday of all time.  A South Korean web host named Nayana, N-A-Y-A-N-A, was hit by the Erebus cryptomalware.  Although Erebus was originally targeted only at computers running Microsoft Windows operating systems, it was later modified to work against Linux systems.  Now, it's unclear how Nayana became infected with Erebus.  But an examination of the largely unpatched software the web hosting service appeared to be running allowed Trend  Micro to presume that the attackers exploited a well-known vulnerability.



Trend Micro wrote:  "As for how this Linux ransomware arrives, we can only infer that Erebus may have leveraged vulnerabilities or a local Linux exploit.  For instance," they wrote, "based on open source intelligence, Nayana's website runs on Linux kernel 2.6.24.2, compiled back in 2008.  Security flaws like Dirty COW," they wrote, "that can provide attackers root access to vulnerable Linux systems, are just some of the threats it may have been exposed to.



"Additionally, Nayana's website uses Apache v1.3.36 and PHP v5.1.4, both of which were released back in 2006."  So both 11 years old.  "Apache vulnerabilities and PHP exploits are well-known; in fact, there was even a tool sold in the Chinese underground expressly for exploiting Apache Struts.  The version of Apache Nayana used is run as a user of nobody," which is the UID 99, the nobody user, "which indicates that a local exploit, and subsequent privilege elevation, may have been used in the attack."



Okay.  So now, as for the ransom paid to obtain the decryption key for essentially their entire enterprise, the fee was high due to the fact that all of the data stored on 153 Linux servers and 3,400 customer websites had been encrypted.  Nayana's own blog posting stated that the initial ransom demand was for 5 billion won worth of bitcoin, which is roughly $4.4 million U.S.  Company negotiators later managed to get the fee lowered to 1.8 billion won and ultimately landed a further reduction to 1.2 billion won, or just over $1 million, which ransom they did pay.



In an updated post last Saturday, the Nayana engineers said they were in the process of recovering the data, cautioning that the recovery was difficult and would take some time.  So a $1 million bitcoin ransom payout, a big payday for the ransomware people.  And that's the sort of news you don't want to see going widespread because that really paints a big target on any other hosting providers whose security may not be up to par <shudder>.



I did want to note that Google Drive is adding a feature which, as I mentioned at the top of the show, does not compete, completely at least, with the feature set offered - actually its minimal feature set does not compete with the feature set offered by Carbonite.  But the Google blog states that on June 28th - which is, what, Wednesday a week from tomorrow - they said:  "We will launch 'Backup and Sync from Google,' which is a new tool intended to help everyday users back up files and photos from their computers, so they're safe and accessible from anywhere."  Which is a little bit of an oxymoron, but we'll discuss that later.  "Backup and Sync is the latest version of Google Drive for Mac and PC, which is now integrated with the Google Photos desktop uploader."



So essentially what this does is I have a picture of the UI in the show notes, which shows Desktop clicked showing that this particular desktop had 220MB on it; the Documents folder checked, and it had 712MB in this instance; the Pictures folder checked, and it had 2.2GB; and the Photo Library under that had 1.9GB.  And so this Backup and Sync tool will replace the current Google Drive uploader client for Mac and PC and will also be integrated into the desktop Google Photos uploader.



And so what's different about this is, if anyone has used Google Drive, rather than creating new Google Drive folders on a user's system, the tool allows users to select the standard system drives that their system already has - Desktop, Documents, Pictures, Photo Library and so forth - and then it will clone those chosen folders up to Google Drive and then keep them synced.  So the system is targeted toward, I would argue - because this will not recover a crashed system.  So it's targeted toward more typical consumers who just want a backup solution for a limited set of the system's entire content, and typically just their own work product because, as we know, Google gives you 15GB for free.  So if you have your typical storage on lots of machines, you will overflow that quickly, and then you would need to augment the free storage provision and start paying Google for the privilege of having all of this content synchronized.  But so it's a nice step forward.  And in eight days that should go live.



LEO:  Kind of like iCloud.  But it does have that advantage.  I'm trying to think, there's not a lot of services like this that let you choose the folders.  OneDrive, iCloud, and Google all use their own kind of special Dropbox, special folders.  It's kind of nice.



STEVE:  Yeah, as long as - from a security standpoint, I'm nervous about synchronizing to a cloud provider where I'm not providing, I mean, this is - we originally coined the phrase TNO.  And PIE, Pre-Internet Encryption, where we're encrypting our contents, and that encrypted blob is then being sent.  I mean, there are problems with doing that in a bandwidth-friendly fashion, which is why Google's able to do this.  That is, the initial cloning would require a large transfer of data.  I'm sure they're compressing and doing smart things [audio dropout].  But then they are just able to do incremental maintenance of that blob by looking for changes.



But again, I would say to users who are thinking about this, don't put the keys to the kingdom in there because we don't know how safe this ultimately is.  I mean, we're sure Google is good with crypto and doing what they can.  But again, you're trading convenience and not having to deal with this yourself for absolute security.  I'm not using it for my stuff, but I could see that it would be very useful for casual users who want to be able to synchronize the major folders where their own work product is, their documents and the desktop.  There I think it makes sense.



LEO:  All right, Steve.  We've still got - we've got a ways to go.  I'm looking at your show notes.  We've got business to do still.



STEVE:  Oh, we do.  This is from the "but of course it is" department.  



LEO:  But of course.



STEVE:  PC Magazine reports that it's possible and fairly easy to hack a PC with a vape pen.



LEO:  Now, that's just weird.



STEVE:  So, well, and in retrospect, it's not surprising.  Researchers at a recent London security show warned and demonstrated that USB-rechargeable e-cigarettes could be modified without much effort to infect a victim's PC with malware.



LEO:  Do you mind if I charge my vape pen in your PC?  I just need a few minutes.  It'll charge really fast.



STEVE:  Let me take a hit off your PC; then I can take a hit off my vape pen.



LEO:  Wow.



STEVE:  So remember, as we've covered here in the past, a USB device is automatically registered either as a USB keyboard, or I should say can be automatically registered as a USB keyboard or, more cleverly, as a network adapter which, when it is enumerated by the USB system, will be queried for its DHCP configuration information.  That allows it to declare itself to be the system's network gateway and thereby reroute all subsequent network traffic through itself.



So our podcast followers should already be highly resistant to allowing anyone to plug anything into their machines' USB ports, but enterprise environments may have less control over such interactions.  So the takeaway here is that any USB thingy, no matter how apparently benign and innocent, could be exchanging more than USB power with any USB port.  And as we know also, the solution, if you must "charge," is a USB condom.



LEO:  I carry it with me everywhere, yes.



STEVE:  Yup.  It's just so easy and simple.  I like this one, the, what's it called?  It's called the PortaPow, P-O-R-T-A-P-O-W.  It's a little red plug.  It comes in a two-pack.  And so if you have that around, and someone wants to stick their vape pen into your machine, you say, uh, hold on a second, let me just slip this little condom over your vape pen, and then they're welcome to suck power.  And this thing protects the data because only the - a USB is a four-wire interface.  It's got ground, it's got 5V power, and it's got a send and receive, send-and-receive lines.  So two of the four just are terminated in the condom so that only ground and power are able to go through.



LEO:  And knowing you, you took this thing apart to verify that; right?



STEVE:  Yup.



LEO:  Because people are going to say, well, who are these PortaPow people?  Why should I trust them?  Because Steve took it apart and knows that only two of the lines are connected.  Why did I know that you did that?  I just knew you did that.  That's great.  Well done.



STEVE:  Okay.  So a bit of miscellany, and then some "closing the loop" feedback with our listeners.  Last week I tweeted what I thought was a very on-point and humorous video about the problems of using chip cards.  And my tweet said:  "It's unfortunate that this humorous video about using chip cards is not that much of an exaggeration."  I mean, there was some exaggeration there; but again, my own experience in the U.S. allowed me to relate to this.



Now, I wanted to follow up and just mention that the majority of responses from my Twitter followers, who are mostly Security Now! followers, was, "What?  Huh?  Chip-and-pin works great here.  What's wrong with the U.S.?"



LEO:  Yeah.  Because we don't use chip-and-pin.



STEVE:  Exactly.  Which was interesting to me since my experiences, as I've said, have been largely similar to those in the video.  So the conclusion would be that U.S. implementations, because they're new and kind of not yet ready for primetime, are much more finicky than in those countries where credit card chips have had a lot of time to mature.  Because, I mean, I see, for example, many chip-enabled machines do not yet accept chips.  So you'll see one that's got the slot, and you stick it in, and then the person says, "Oh, no, no, you have to swipe."  It's like, oh, okay, fine.  And, I mean, sometimes there's duct tape over the slot.  Or you'll try to swipe the card, but then they go, "Oh, no, no, you have to use the chip if you have one."  It's like, okay, fine.  And lord help you if you insert the card before you're told to.



LEO:  You've got to leave it in there and, oh, man.



STEVE:  Yeah.  Or if you take it out, like, and it takes a long time in the U.S.



LEO:  Mm-hmm.  It's a lot longer, yeah.



STEVE:  It's like, what the heck is going on?  And what's really sad is that what we also know is that this doesn't actually provide measurably more security.



LEO:  Why not, Steve?  Why not?



STEVE:  Are you watching the video?



LEO:  I'm watching the video, yeah, yeah, yeah.



STEVE:  Yeah.



LEO:  She's having all the problems you would expect, yeah, with one of these things.  [Crosstalk] follow Steve's Twitter.



STEVE:  For anyone who doesn't follow, I've got the link to the YouTube video in this week's show notes.  And it is pretty funny.  But I did want to - I wanted to acknowledge all of our international listeners who think that there's something wrong in the U.S.  You're right.  There's clearly something wrong over here.  We haven't figured out how to do it.



Oh, and this week's Darwin Award winner:  NBC, actually, I noted from my - I used to be in Northern California.  So KRON is the local NBC affiliate station.



LEO:  Not anymore.  You haven't been here in a long time.  They were.



STEVE:  Ah, okay.



LEO:  Now they're just independent.



STEVE:  So anyway, they carried the story of a group of not-too-clever thieves who stole a bunch of GPS tracking devices from a tech manufacturer.  But they were not too difficult to track down.



LEO:  Why is that, Steve?



STEVE:  Because they were GPS tracking devices that they stole.



LEO:  So they just followed the device and...



STEVE:  That's right.



LEO:  Yeah, yeah.



STEVE:  I have two short, fun notes about SpinRite.



LEO:  Actually, it is from an NBC affiliate, not KRON, but WCMI TV Columbus.



STEVE:  Oh, interesting.  Maybe it was picked up...



LEO:  Now maybe they [crosstalk] KRON.



STEVE:  I think it was...



LEO:  That's what it was.  I see KRON on it.  Yeah, yeah, yeah.



STEVE:  Right.



LEO:  That's funny.  Okay.



STEVE:  So anyway, yes, the Darwin Award winners.  Do not steal something that can track you after you have stolen it.



LEO:  Whoops.  Whoops.  Whoops.



STEVE:  That's not good.



LEO:  $18,000 worth of GPS tracking devices.



STEVE:  So, yeah, I think that you don't want to steal trackers because trackers are trackers.



LEO:  They put it in a storage locker that contained a lot of other stolen stuff, including drugs.  So we won't...



STEVE:  Doh.



LEO:  ...be seeing much of them for a while.



STEVE:  So James Mudd, who is in Oxford in the U.K., said hey.  He said:  "Hi, Steve and Leo.  I really enjoy the podcast and look forward to it every week.  I've been listening for years and just started listening again from the beginning."  Well, you have 617 episodes to catch up on.



LEO:  That's a lot of listening.



STEVE:  He says:  "I often SpinRite drives," which of course is a verb now, "before moving them between systems and formatting them.  So I have a question.  Is it better to SpinRite the drive, then format?  Or format first, then SpinRite?"  He says, "I usually SpinRite first, somehow feeling that having data on the drive is better.  But logic tells me it shouldn't make a difference."  And James, I would agree with your logic.  I thought about this for a while, but I just really - I can't see either way.  I would say go for convenience.  If it's, for whatever reason, easier to SpinRite it where it is before you move it, then do that.  If it's easier to SpinRite it after you move it and establish it and format it, then do that.  So I would opt for whichever just makes the most sense from a convenience standpoint.  Logistically, or in terms of what SpinRite's doing, it doesn't matter at this point.



And secondly, Matthew Norton in Northwest Indiana.  Now, I don't think that SpinRite can take credit for every miracle which is coincident with its use, although we do see it often doing things that are unexpected.  In this case, the subject was "Another random thing that SpinRite fixed."  And he wrote:  "I'd been having an issue with my NumLock not turning on when I booted my computer, even though it was turned on in the BIOS and in Windows.  I did a SpinRite check of my drives, and ever since then the NumLock has turned on at boot.  Thanks for the great product.  Eagerly awaiting SpinRite's future.  Thanks for the podcast.  I've been listening since Episode 1."  Now with NumLock.  I can't explain that.  I mean, maybe...



LEO:  You fixed his NumLock.



STEVE:  Maybe.  Maybe something about the way the system was booting, there could have been a little glitch in the boot process that clicked the NumLock off.  Who knows?  But Matt, I'm happy that SpinRite was up to the [crosstalk].



LEO:  I think you should put this on your website in your ad copy.



STEVE:  Not guaranteed to fix NumLock.



LEO:  No, but might.



STEVE:  But you never know.



LEO:  Might fix your NumLock, yeah.



STEVE:  You never know.



LEO:  I think that'd be a good name for SpinRite 6.



STEVE:  If your Lock is Num, then this will fix it.



LEO:  Right.



STEVE:  So a couple feedback from our listeners.  Dan Sidor asked, he said:  "Ubiquiti EdgeRouter X looks great.  Do you also have experience with their UniFi access points?"  And I wanted to remind Dan that, yes, the Ubiquiti EdgeRouter X looks great.  But we recently covered the fact that all of - unfortunately all - of the Ubiquiti wireless products did need to get their firmware updated because there was a recent exploit that affected them all.  The good news was it did not affect the wired routers, which we like and have been actively suggesting our listeners use.  But all of the various wireless ones did have that problem.  So I would, I mean, their hardware is nice.  I know that you've been, at some point, Leo, you were a Ubiquiti WiFi user, I think; right?



LEO:  No, I never used their WiFi.



STEVE:  Oh, okay.



LEO:  But they were like the first to do mesh.  It was a commercial version.



STEVE:  That's right. 



LEO:  And it required - we might have tried it here.  It required at the time, no longer, a PC running Java to control it.



STEVE:  Oh, lord.



LEO:  Yeah.  They fixed that.  And actually they make - Ubiquiti now makes a wireless WiFi system called the AmpliFi that I have used, just briefly, for review purposes.



STEVE:  I like their naming - UniFi, AmpliFi, Semper Fi.  It's like, okay.



LEO:  Semper Fi.  Burke says we did try it at the Brick House.  But I think we ended up using Ruckus equipment, I believe, is what we use here.



STEVE:  Anyway, so Dan, I would just say proceed with caution.  Make sure that the firmware for it has been updated in the last couple months and that you're running that updated firmware because we were watching them at the time, and they were a little slow in reacting.  And so it was like, ooh, I hope - I was hoping it wasn't going to be affecting the wired products also.  So there's a bit of a caution there.



And then "Kevn" tweeted:  "What was that switch/router you and Leo recommended a few months back?"  And there it is, the Ubiquiti EdgeRouter X.  And I think I've got it linked to, in the GRC Linkfarm, if you just put "linkfarm" into Google now, for me at least the first link that comes up is GRC.  And I think I've got a link there to it.



LEO:  And it's not wireless.  It's a $50 just six-port simple little switch and router with a lot of sophisticated software.



STEVE:  Yes.



LEO:  It's very cool.



STEVE:  What I like about it is that, whereas our typical router has a router core connected to a switch, which creates the ports, like if it's a four-port router or a five-port router, where there's one main port, like a WAN port, and then four LAN ports, normally the typical router is a switch that runs the four LAN ports.  This router has individual interfaces, individual network interface controller (NIC) interfaces per port.  And that allows you to give each port its own IP range to block the interacting traffic among ports.  And there's a lot of power in the UI, even more power at the command line, that really allows you to do things like create an IoT segmented network and really get lost in feature land.  So a very neat little router.  And again, 49 bucks, and it's cute.  Even Elaine got one because she's been listening to this podcast and wanted to improve their security.



So the last couple weeks [audio dropout] talking about SQRL.  And it was in the context of various discussions about two-factor authentication and so forth.  And I got some feedback over in GRC's Security Now! newsgroup saying, okay, Steve, enough about SQRL.  So I wanted to recognize that feedback and assure people that this is not going to become the SQRL podcast.  But at the same time, as a consequence of my talking about it, it did raise a couple questions, that I want to answer briefly, from our listeners.



One question asked:  "Are there any safeguards in SQRL against DNS spoofing?"  And the answer is yes.  When I was talking in the last couple weeks about the increased emphasis we have just put in the last month, it's what I've been doing, essentially, is bringing the so-called CPS, the Client Provided Session, bringing it to the forefront and making it a mandatory feature.  It absolutely protects against DNS spoofing attacks when you are using SQRL on the same system you're logging in.  That is, remember, you could either use it in the same-device or cross-device login.  So yes to DNS spoofing.



And then another question:  "If the malware has taken control of your DNS, would your mitigation on man-in-the-middle attacks still work against SQRL?"  And again, yes.  So we are safe against any kind of DNS attack as a consequence of the way this works.



Another listener asked:  "Listened to last SN podcast about dynamic pass and two-factor authentication spoofing."  He said:  "One more reason to use a password manager.  It will not recognize wrong address."  And to that I respond, true, but password managers will be completely fooled by DNS spoofing attacks.  So as I noted in the previous two questions, SQRL cannot be, but password managers will be.  So if you go to the legitimate-looking website with a URL domain that exactly matches what the password manager shows, your password manager will populate those fields.  But if your DNS has been intercepted and spoofed, you could be taken to a different physical server than where you believe you're going, and your password manager will populate the fields.  And script running on the page, as we have already seen, is able to immediately capture that information, so could steal your identity for that site.



And, finally, Chris Taylor asks:  "Could a hacker kill SQRL localhost port and run their own hijacker?"  And the answer is yes, absolutely.  One of the things that it is important to understand is the attack environment.  And there is absolutely no practical protection for a system that has malware on it.  And to believe otherwise is just fooling us.  Windows is not a secure operating system.  None of our PC operating systems are sufficiently secure to allow us to create software that malware running on that machine cannot subvert.



So I did not intend to suggest that anything that we're doing here is proof against a local compromise of the system.  There just is no known for any of this technology.  But the big attacks are the network-based attacks, something remote, a website losing control of their database, of user accounts.  Or DNS spoofing or man in the middle or anything remote network.  And for all of those we have that nailed, but not for local.  And nobody does.  I would argue it's not possible to protect against a local, essentially an internal attack.



Someone whose handle is Le French Fab said:  "Hi, just listened to SN-616."  So that was last week.  "Do you think Apple's two-factor authentication is in any way better than the way used by Google/Microsoft?  Thanks."  And I thought about that, and I think it is.  Whereas the other companies are forced to use text messaging through the mobile cellular text system, Apple has the significant advantage of having their own proprietary, closed, and encrypted messaging system in the form of iMessage, which allows Apple to get those second-factor tokens to their intended recipients without fear of man-in-the-middle or other on-the-fly interception.  So it is a significant advantage.



Again, I still would argue that the use of TOTP, the time-based auth-style authentication, is by far more secure.  But if you need a per-authentication transmission, then Apple's ability to send that to you through their own encrypted proprietary messaging platform makes that much more secure than having it go and be received as a text message over the cellular system.



LEO:  Although what happens if it's your first Apple device, or you don't have any other Apple devices?  Because it uses another Apple device to authenticate.  What if I buy an iPhone, I don't have a Mac or an iPad?



STEVE:  So it doesn't send it to the same device?  The one that you've got...



LEO:  Well, that would be dopey.  Oh, let's authenticate you on this device.  Here's the code.  Why bother?  No, it doesn't send it to you on the same device.  It sends it to you on another Apple device.  I presume that they then offer it via text or some other...



STEVE:  They must, yes.



LEO:  ...bad channel.



STEVE:  In order to get you bootstrapped the first time.



LEO:  Yeah.  It relies on you having a bunch of Mac or Apple products.  It's very typical.



STEVE:  Yeah.  And in fact that's what happens for me is when I'm - in fact, it just happened when I was setting up the new iPad Pro, is that it said oh, you know, and I got little pop-ups everywhere saying that I was using my identify on a new device. It's like, yeah, okay, fine.



LEO:  But they do make you turn it on now, which you've got to commend them for that.



STEVE:  Yeah.



LEO:  You have to use two-factor now.



STEVE:  Oh, Bill sent a tweet saying:  "Yesterday" - I thought this was a great question.  "Yesterday a friend was asked by TD Bank if they could keep the recording of her voiceprint for security purposes."  And he asks...



LEO:  Hmm.  Schwab does that, too.  They have voice identification login.  



STEVE:  And he asks:  "How unique is it?"  And my response is, okay, it's not unique enough to be the sole identifier.  But it is good as an additional factor.



LEO:  Right.



STEVE:  So like a PIN, you know, a PIN can't identify you uniquely.  But it can be - it's a valuable additional - it's a disqualifier if it doesn't match.  And so if someone is pretending to be this friend of his, and it's a gruff male voice, it doesn't even - it's not even in the ballpark of sounding the same.  It's like, uh, we aren't really convinced that's you.  So I think...



LEO:  Yeah, it's me.  I have a cold.



STEVE:  Absolutely valuable as a disqualifier, but not as a qualifier.



LEO:  My mom, and I don't know how Schwab does it, but they said, "Can we do voice authentication?"  And I just said, "Don't do it, Mom.  I don't know what they're doing."  But you're right.  If it was a second factor, that'd probably even be better than texting you a number; right?



STEVE:  Yup.



LEO:  Yeah.



STEVE:  Again, as a disqualifier, but not a qualifier.



LEO:  Right.



STEVE:  I think that makes a lot of sense.



LEO:  Let me in.  Hey.



STEVE:  And again, not perfect because you could imagine surreptitiously recording a sample of the person's voice saying something that you want.  And in fact we were talking a couple weeks ago that we're now seeing AI-ish software that can listen to someone talk and then synthesize them saying anything that you want them to say.  So it's getting very spooky out there.



LEO:  What about people like me, whose voice is all over the darn place?



STEVE:  Yes, you're hosed, Leo.



LEO:  Okay.



STEVE:  We can't ever believe anything we hear someone claiming that you said any longer because the software is out there to synthesize it.  And me, too, for that matter.  With 616 podcasts, I've probably used every word in my vocabulary.



LEO:  Exactly.  There's no word they can't duplicate.



STEVE:  So listener Ian Beckett, who's often tweeting, said:  "Tell me why every password interface doesn't have a 'three times and you're locked out for X hours' as standard?"  Well, and of course we know the answer to that.



LEO:  Because people get locked out.



STEVE:  Yes.  People are like, uh, which password did I...



LEO:  No, it's monkey1234.



STEVE:  Wait, did I use my sister's name backwards last time?



LEO:  Yeah.



STEVE:  And unfortunately it would just be a nightmare for customer support.  I will mention that GRC's email server has a zero tolerance guessing policy.



LEO:  Oh, I like that.



STEVE:  After I watched spamming servers connect up and just run through a ridiculous list of names, I implemented some technology where one single mistake, and that attempted connection is blacklisted for quite a while.  In fact, we've never had any problem with receiving email.  As long as you send it to an account that exists, it comes right through.  But if you're a spammer, and you're just hooking up and guessing, sorry, that's just not going to work for you.  So there are places where that kind of "one strike and you're out" policy can work; but, unfortunately, not in the password space.



LEO:  A company that I still work for has - I don't want to give away any secrets.  But a company that I still work for requires every three months training in FCC rules.  And I have to log in, but they change the password every two months.  So that means I never actually - because I don't ever use it otherwise, I don't actually know my password ever.  And now I guess they're so strapped for tech support people that when you go online, and you call the tech support to reset your password, they say, "If you're calling to reset your password," which obviously is a common practice, "use our online service."  Except that the online service doesn't seem to know who I am.  So it says no, there's no user by your name.  So I don't know what to do.  Geez, Louise.  Anyway, this is a problem.  And we've already said changing your password by a calendar is not more secure, it's less secure; right?  



STEVE:  And the good news is that the NIST guidelines were finally revised to state that.



LEO:  Hallelujah.



STEVE:  So with any luck, that'll percolate out into the IT space.  And it used to be that no one could get fired for choosing IBM, even after they stopped offering...



LEO:  It's more secure, boss.



STEVE:  Yes.  And so now no one can get fired for following the official NIST guidelines...



LEO:  Good point.



STEVE:  ...which now state there is no security benefit, and it reduces security for compulsory password change for no reason other than, oh, it's been a while.



LEO:  Well, if they had any IT people, I could tell them.  But apparently there's no one there.  So I'm kind of, you know, I'm kind of stuck between a rock and a hard place.  I can't actually get into my account, and there's no one there to help me.



STEVE:  Wow.



LEO:  Yeah, yeah.  It's just - but you nailed it.  It's support costs that everybody's really concerned about; right?



STEVE:  Right.  Right.



LEO:  That's the ultimate issue.



STEVE:  So David Benedict took a little exception, I think, to my being harsh with Google and Android about Google's official policy that the "draw over" problem would not be fixed.  And he raised a good point.  He said:  "Google not fixing does not mean they won't scrutinize apps more than before."  And I would imagine, in fact, they will increase the depth of their app scrutiny in order to prevent abuse of draw over.  So David, that's a very good point.  Thank you for bringing it up.



And lastly, this week's laugh-out-loud tweet, courtesy of a retweet from our listener Morgan Speck.  He retweeted Lesley, whose Twitter handle is @hacks4pancakes.  She tweeted, or he tweeted, I don't know if that's, I don't know, Lesley said...



LEO:  That's a man's spelling, L-E-S-L-E-Y.  So we'll say it's a guy, yeah.



STEVE:  Okay.  He tweeted:  "Personally, I think this Windows 7 thing is overhyped, and I shall be switching back to XP since it has a regular security patch cycle once again."



LEO:  Sounds like you, Steve.



STEVE:  Last couple of months, Microsoft has been updating XP every month, so let's all go back.



LEO:  Were there more than just for WannaCry?  There've been some other patches?



STEVE:  Yes, the last two, the last couple months.  And then there was an emergency out-of-cycle patch also.



LEO:  Wow.



STEVE:  So, yep, we're getting our XPs all updated, thank you very much.



LEO:  I think we should send @hacks4pancakes some pancakes for that tweet.



STEVE:  And as they say, Leo, that's the show.



LEO:  It's a wrap, ladies and gentlemen.  Steve Gibson, he's at GRC.com.  That's where you can find information about SQRL.  You were on Daily Tech News Show with Tom, I saw, so you can listen to your explanation, yeah.



STEVE:  Yup, a little piece on Thursday, yup.



LEO:  But there's a lot more stuff at GRC.com, including Steve's bread and butter, which is SpinRite, the world's best hard drive maintenance and recovery utility.  If you have a hard drive, even if it's not a spinning one, an SSD, you've got to have SpinRite.  Go to GRC.com to get it.  Use ShieldsUP!.  That's free.  In fact, everything else is free.  He's got lots of great stuff over there, including this show, audio versions and human transcriptions by Elaine Farris.



STEVE:  Yes.  Heat willing.  We're hoping that Elaine is still online.  It's 111 where she is.



LEO:  Oh, my god.



STEVE:  So she did mention that she might have a problem.  So cut us both a little bit slack if she's a little bit later than usual.  We'll hope that things are going to work out okay.



LEO:  I had an interesting experience on Sunday.  It was so hot, over 100 degrees in Petaluma, that my solar panels stopped working.  You'd think this would be like a golden time for them, but no, no, it was too hot.



STEVE:  Yeah.



LEO:  So they just - they took a little break.  But they're back now.  You can also find the show on our site, TWiT.tv/sn for Security Now!.  But you know the best thing to do, and you'll be glad when you (like many of our listeners) start over at Episode 1 that you have them all, subscribe to the podcast.  It'll automatically download, and you can listen at your leisure.  Do what the NSA does.  Listen every single week.



STEVE:  I was just going to say, I wanted to sign off saying I am flattered and gratified that the NSA has asked for formal permission to copy the podcast into their Intranet.  Very cool.



LEO:  I love that.  That's awesome.  That really is.  It's very high praise.  And, you know, we talk a lot about the NSA, and we maybe mock them a little bit.  But I have to say they have the best cybersecurity recommendations on their website.  They have really good stuff.  They understand that their mission is also to protect American businesses and individuals against cybercrime.  And they do a very good job of it.  So there's some obviously very smart people at the NSA, and I'm at this point now gone too far kissing up to them.



STEVE:  And they clearly have good taste.



LEO:  They have excellent taste.  And I just want to thank them for letting me back into the country.  So, you know, it was amazing.  It was like I didn't do any of the things I thought about, wiping the thing, bringing burner phones and stuff.  And it's good I didn't because I just scooted right back in the country.  No reason to.



Mr. Steve Gibson, bless you.  We'll see you next week.  We do the show every Tuesday, 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  If you want to join us live, or even in studio, we can put you in here if you email tickets@twit.tv.  We have a recent graduate from computer science at Carleton, up in Ottawa, and he's visiting with his mom as a celebration of his graduation.



STEVE:  Very cool.



LEO:  And he's going into tech.  So that's a good thing.  He's going to be a full-stack programmer.  Tickets@twit.tv, we'd love to see you in the studio.  Thanks for joining us, and we'll see you next time on Security Now!.  Bye-bye, Steve.



STEVE:  See you, my friend.  Bye.  



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#618

DATE:		June 27, 2017

TITLE:		Research:  Useful & Otherwise

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-618.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we discuss another terrific NIST initiative, RSA crypto in a quantum computing world, Cisco's specious malware detection claims, the meaning of post-audit OpenVPN bug findings, worrisome bugs revealed in Intel's recent Skylake and Kaby Lake processors, the commercialization of a malware technique, WannaCry keeps resurfacing, Linksys responds to the CIA's Vault 7 CherryBomb firmware, another government reacts to encryption, the NSA's amazing GitHub repository, more news about HP printer auto-updating, a piece of errata, some miscellany, and some closing-the-loop feedback from our listeners.



SHOW TEASE:  Time for Security Now!.  Steve Gibson's here.  Lots to talk about, including some bugs in OpenVPN.  Nothing to worry about.  It's been patched.  And Steve may question a little bit the methodology.  It's all coming up next, plus our image of the week, which was the same image of the week a couple of years ago on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 618, recorded Tuesday, June 27th, 2017:  Research:  Useful and Otherwise.



It's time for Security Now!, the show where we cover the latest in security and privacy and how stuff works with Steve Gibson.  He's the majordomo at GRC.com.  Hi, Steverino.  Live long and prosper.



STEVE GIBSON:  Hey, Leo.  Yes, great to be with you again, Episode 816.  What am I saying, 816 - 618.



LEO:  Yeah, don't rush it because you've already said you're retiring at 1,000.



STEVE:  A little dyslexia there, 618.  I titled this one "Research:  Useful and Otherwise" because there were a couple really interesting, significant stories about some really interesting research and then one that's like, okay, what, you know.  And actually, so I think it's going to be a great episode.  We're going to talk about another terrific NIST initiative.  They're the ones who ran contests that selected the Rijndael block cipher as the AES standard.  And then later they did the same thing for the cryptographic hash function, choosing Keccak for the SHA-3 cipher.  Anyway, they're doing another one for a very interesting category that I'm glad for.



Many people ask about quantum computing and what that's going to do to crypto.  So our friend Dan Bernstein, who's a well-known cryptographer - in fact, it was his elliptic curve that was the original inspiration for the SQRL project that I embarked on.  He's examined the nature of RSA crypto, which we know is based on the multiplication of primes and the difficulty of then factoring them back out of the product of the primes in a quantum computing world.  Unfortunately, we have Cisco's, what I've described as "specious malware detection claims," which is the "otherwise" in this research.  It's like, oh, okay.  I've dug into it, and I don't know what's behind it, but we'll cover it.



And then the meaning of post-audit OpenVPN bug findings.  Also some worrisome bugs that have just been revealed in Intel's recent Skylake and Kaby Lake processors, what they mean and what our listeners can do.  The unfortunate commercialization of a malware technique.  The fact that WannaCry keeps resurfacing.  Linksys responding to the CIA's Vault 7 CherryBomb firmware that we recently discussed.  Another government reacting to encryption.  The NSA's amazing GitHub repository.  More news about HP printer auto-updating.  A bit of errata, some miscellany, and then, of course some closing-the-loop feedback from our listeners.  So I think another great and interesting podcast here.



LEO:  Indeed, indeed, as always.  We've got a live studio audience.  Bruno's visiting from Switzerland, James and Robert from Missouri.  So they're in the studio, too.



STEVE:  That's much better than a dead studio audience.



LEO:  Yeah.



STEVE:  More animated. 



LEO:  We've had the dead studio audiences.  It's no fun at all.  All right.  I see a Picture of the Week.  In fact, I saw it earlier on Twitter.



STEVE:  Yeah, so did everybody.  And the problem is - I put it up because I wanted to acknowledge everybody who sent it to me.  But when I looked at it, I thought, eh, this looks very familiar.  Now, I figured, had we shown it before, I would have read it to the people who are listening to the audio.  So I went to GRC.com/sn.



LEO:  Oh, you are so smart.



STEVE:  And I put in, "In this corner we have," and bang.  Two years ago in June, so in 2015, this was our Picture of the Week.



LEO:  And if you look at the date on it, I think it's from 2006.  Can that be?



STEVE:  Yes, it does have an '06 copyright on it, too.



LEO:  Wow.



STEVE:  And this is one of the things that we see on the Internet from time to time is these things sort of come back into vogue.  Someone rediscovers it and posts it, and it goes viral and so forth.



LEO:  Well, but also, 11 years later, it's still true.



STEVE:  It is, interestingly, still relevant.  So for those who don't see the picture and who may not have been listening for two years, it's a boxing ring.  And in one corner we have someone holding up a big data security sign with lots of equipment there to create all this secure infrastructure.  And the announcer holding the microphone says, "In this corner we have firewalls, encryption, antivirus software, et cetera.  And in this corner, we have Dave."



LEO:  And Dave's wearing a sweatshirt that says "Human Error."



STEVE:  Yeah.  And the point being, yes, you can bring all of this technology to bear, and Dave can still defeat all of your best intentions just by being himself.



LEO:  Another way you can tell it's dated, you see antivirus software.  We've been having a lot of discussions on the radio show.  People have been begging me to ask you, and I'm sure we've covered it on the show, your position on using antivirus software.



STEVE:  I've listened to it all over the weekend.  And you're 100% correct in everything that you said.  It is the case - and in fact, Tavis's most recent shower was it brought about his findings that even Defender has problems that have been fixed.  The one thing you didn't say, and it's arguably too technical for your weekend audience, is that in order for third-party AV to do what it must, it's necessary to hook deeply into the operating system.  And that fundamentally destabilizes it.  They do the best job they can, but they're coming along, and they're having to put their hooks in, literally, to the kernel.



And so the advantage that Microsoft's built-in system has, in addition to just being there and built-in and sanctioned and so forth, is it's been - the OS has been tested with it in place.  Microsoft isn't testing their OS with other third-party AVs in place.  The AVs are having to test theirs against Microsoft.  So it just seems all around better and more stable not to put a third-party AV platform in.  Which is exactly what you were saying.



LEO:  On Windows 8 and 10 you've got Defender.  It's not the best in the world.  Nobody says it is.  But the problem with antivirus software is it gives you a sense of false confidence because no antivirus software can protect you from everything.  In fact, most protect you from a fraction of the total threat.



STEVE:  Yes.  And I would argue that it's getting worse because, in this spy vs. spy cat-and-mouse game, the strategies to avoid detection are evolving, and the AV people are always responding.  They're not preemptive.  They're unable to detect something that doesn't exist yet.  So they're inherently lagging, and the malware people are just trying to keep a few steps ahead.  So unfortunately there isn't - as we have said, the only solution, I mean, this is the classic, do you blacklist or do you whitelist?  The only solution is to whitelist, that is, to only allow what you know to be good to run.



LEO:  And that's Windows 10 S, which I think is dubious security.



STEVE:  Well, which has just been defeated badly, yes.



LEO:  Right.  But the problem is they don't whitelist aggressively enough.  They allow Win32 subsystem.  They allow Office.



STEVE: Well, yeah, I heard you talking about Win32 subsystem over the weekend.  It has to be there for legacies.  There's too much stuff that still runs it. 



LEO:  See, that's what I'm thinking is I think the only thing that they really have it in there for is the centennial apps, which are wrapped Windows 32 apps that they can then put in the Windows Store.  They're not true UWP apps.  And the biggest one is the one they have to have, which is Office.  So you've got 32-bit Office, and so that's the vector.  And I just - anyway.



STEVE:  It'll be interesting to see how it goes.



LEO:  Yeah.  I just feel like you give up so much with Windows 10 S because you can't run a decent browser, and it's of dubious value in terms of security.



STEVE:  Yeah, well, and you will be giving up SQRL's ability to prevent...



LEO:  Oh, really, yeah, because SQRL's not in the store unless they...



STEVE:  Yes.  Well, SQRL's not in the store.  But also one of the questionable things they've done is they've prevented the browser from having access to the localhost IP.  And SQRL uses that in order to perform its client-provided session antispoofing protection.  And Windows 10 S is the only system on the planet that doesn't allow that to happen.  It still works, but you just lose that extra...



LEO:  But you do understand why they did that.  I mean, again, that's what you give up for the security.



STEVE:  Yeah.



LEO:  All right.



STEVE:  So the NIST, a branch of that, the Information Technology Laboratory, the ITL, announced an initiative that they've been working toward for a while.  In this initiative's introduction they said:  "The world is increasingly interconnected by myriads of devices."  Should "myriads" be plural?



LEO:  "Myriad" is one of them singular plural...



STEVE:  I thought so, "...by myriad devices working in concert to accomplish important tasks including," they write, "automotive systems, sensor networks, healthcare, distributed control systems, and the Internet of Things, also cyber-physical systems and the smart grid."  They write:  "But these devices may have resource limitations compared to common desktop computers.  For example, they may have significantly reduced power consumption, less computation power, and orders of magnitude less memory than desktop computers.  These constraints can make it difficult to implement modern cryptographic algorithms, most of which are designed for desktop or server environments.



"To address this issue, different cryptographic algorithms have been tailored for resource-constrained devices.  The academic community has performed a significant amount of work on this type of cryptography, called 'lightweight cryptography.'  This work includes efficient implementations of conventional cryptography standards and the design and analysis of new lightweight algorithms and protocols.



"In 2013, NIST's Information Technology Laboratory started a lightweight cryptography project to investigate the issues and then develop a strategy for the standardization of lightweight cryptographic algorithms. In 2015 and 2016, NIST held two lightweight cryptography workshops to solicit feedback on the constraints and limitations of the target devices, the requirements and characteristics of real-world applications of lightweight cryptography."



And now, recently, NIST decided to create, through an open process, a portfolio of lightweight algorithms.  ITL, which is this group within NIST, has published NIST Internal Report 8114.  And I have a link to the full report in the show notes for anyone who's interested, which is titled the "Report on Lightweight Cryptography," to "summarize the findings of this project and to outline NIST's plans for the standardization of lightweight algorithms.  Here, we present highlights from this report, especially the devices targeted by lightweight cryptography, how the algorithms were designed, and the standardization of lightweight cryptographic algorithms."  And of course I'm not going to go into all that detail.



But the beginning of their actual report says, which is sort of interesting because it helps to give us a better sense for exactly what this is, they say:  "This report provides an overview of lightweight cryptography, summarizes the findings of NIST's lightweight cryptography project, and outlines NIST's plans for the standardization of lightweight algorithms.  In particular, NIST has decided to create a portfolio of lightweight algorithms through an open process.



"This report includes a list of questions to the stakeholders of lightweight cryptography that will serve" - we need to somehow shorten that.  It's a little bit of a tongue twister.  Lightcrypt or something, who knows - "that will serve as the basis for determining requirements.  NIST will develop profiles based on community responses to these questions.  These profiles are intended to capture cryptographic algorithm requirements imposed by devices and applications where lightweight cryptography is needed.  Algorithms will be recommended for use only in the context of profiles, which describe physical, performance, and security characteristics."



So anyway, they then talk about the target devices.  They enumerate and clarify the performance metrics which are their goals, talk about the cryptographic primitives - and we've discussed them all on this show in the past - block ciphers,  hash functions, message authentication codes, and stream ciphers.  And the idea being they want to take all the lessons that we have from traditional full-strength heavyweight crypto and say, okay, if you've got a light bulb, and you want crypto in a light bulb, or especially if you have some little battery-powered tag somewhere, and you want cryptographic strength, what are the constraints in these applications, thanks to the nature of the device, and also remember the cost.  Cost is a factor, and larger chips cost more.



And so there's some pushback against the need to be able to accommodate the existing cryptographic primitives which just assume they're in a multi-gigabyte desktop environment or a server.  So, for example, things like reducing the block size of a block cipher.  We've talked about this before, how AES is deliberately 128-bit block because what the symmetric cipher does is map every single combination of 128 bits into a different and unpredictable combination of 128 bits, one for one, so that for every input combination of 128 bits, you get a different output 128 bits.  And that's reversible, so you can go the other way.  And what's so cool, I mean, almost magical, is that that mapping function is controlled by an input key.  And what AES gives us is officially three different key sizes which can be used, which allows you to have variable strength of key within a given block size.



And so it's important, I mean, one of the things that they clearly recognize is there are tradeoffs for making reductions for the classically established algorithms and standards.  But there are also things we can do to offset the tradeoff.  For example, in a block cipher, most block ciphers have this notion of rounds.  We've talked about this, and we did a whole podcast on AES, for example, where we actually looked in detail at an AES round, the idea being that each cycle mixes the bits in itself in a reversible way.



But if you just did that once, there just isn't enough mixture, enough mixing that a single round creates, making it easy to reverse the process or to figure out what the key must be.  So to overcome that, they do multiple rounds in order to - so each round is reversible, so the whole thing ends up being reversible.  But by the time you're done with, like, 14 rounds, you're not only exhausted, but you're so scrambled up, even though they're reversibly scrambled up, there's just no way to get back to the beginning.



So, but the point is, the design was made deliberately with the tradeoffs of a desktop environment in mind.  So, for example, in AES there are these things - throughout crypto there's a thing called an S-box with is essentially, it's a little fixed mapping box.  It takes some number of bits in, often eight, and it produces eight bits out.  So that's like a fixed key kind of mini cipher.  And so those S-boxes are scattered around.  But each S-box, because it's taking eight bits in and needs to map those to a different eight bits out, that's an expensive process in terms of processing time or hardware real estate, if you wanted to integrate this on a chip.



So, for example, if you went to a four-bit S-box, where it only needs to map four bits into a different four bits, that's incredibly simpler in terms of die-size.  The four-bit S-box is estimated to have a relative size of 28, just sort of a numeric integer, 28; whereas an eight-bit S-box is 395.  So the problem is the requirement scales exponentially with the bit length of the S-box.  So the point is there's a whole different problem than has been really addressed before in the notion of bringing high security to low-resource or resource-constrained environments.  And, for example, where you might make each individual round in a cipher less strong, you can end up achieving the same end strength by just doing more rounds.  And since light bulbs and IoT devices tend not to be performance constrained, the tradeoffs can be different.  You can say, okay, we've got more time to do this on our IoT device.  So we're going to, for example, crank the rounds up really high while reducing what each round does, which creates huge savings in chip real estate, RAM requirements, and so forth.



So anyway, I think this is a great initiative.  I mean, these guys, it was exactly this kind of open process which we need somebody to be the honcho of, somebody to manage and spearhead - as I said, that's where we got the Rijndael selection as the AES cipher, and we got the SHA-3 next-generation ciphers.  I think this is great.  And all for the best.  But I would note that having a suite of carefully considered lightweight cryptographic primitive building blocks does not automatically make IoT devices secure.  That is, having them isn't going to change things overnight.



What this sort of falls into the classification of necessary - so it's necessary, but not sufficient.  Having approved cryptographic standards which are appropriate and still secure, just the tradeoffs were made differently, removes one of the largest objections that IoT makers can present.  They could say, well, we'd like to have crypto, but it's going to make our stuff uncompetitive because it's going to raise the price.  And besides, we all know that consumers say they want security, but nobody's actually willing to pay for it.  Well, if we have a suite of lower, probably lower performance, but parity secure and appropriate primitives, then cost stops being an objection.  And although it's not going to fix today's devices, I love the fact that we'll have this in place for tomorrow's.  And then nobody can say that it's going to make these things cost more because it won't.  It'll be crypto for free, essentially, in the same devices.  So, nice piece of forthcoming research.



Another piece of cool research is this whole question of - the term is "post-quantum RSA," meaning, okay, we had sort of this uncomfortable sense that quantum computers are on the horizon.  They're getting more capable.  They're inching forward.  It's a huge topic of research.  The question is, how will the presence of quantum computing impact present-day encryption?  So a bunch of academics, Dan Bernstein being among them, I think there's three others, decided to really tackle this question.



So in the abstract of their paper, and I've got the link to the full PDF in the show notes for anyone who's interested, they say:  "This paper proposes RSA parameters for which key generation, encryption, decryption, signing, and verification are feasible on today's computers, while all known attacks are infeasible, even assuming highly scalable quantum computers."  Okay.  So this paper proposes RSA parameters for doing everything RSA needs to do.  And I'll look at this question of feasibility in a minute because what they had to do, okay, feasible, maybe.  But the point being that they've looked at the question of can we keep RSA, is it possible to keep it, even in the presence of highly capable quantum computers.



So they said:  "As part of the performance analysis, this paper introduces a new algorithm to generate a batch of primes. As part of the attack analysis, this paper introduces a new quantum factorization algorithm that is often much faster than Shor's algorithm" - Peter Shor is a mathematician who famously, about 23 years ago, back in '94, he described how future quantum computers, as we've assumed they were going to operate, could be put to the task of prime factorization.  And this is one of the things that upset everybody is it looks like, oh, crap, quantum computers are going to be much better factoring machines than existing sequential machines that we have now.  So Shor's algorithm is often cited as this classic example of, whoops, quantum computers are going to kill crypto.  But in the process, Dan and his group came up with something even better - often, as they write, much faster than Shor's algorithm and much faster than pre-quantum factorization algorithms.



Okay.  So as we know, it's been the longstanding intractability of the integer factorization problem, that is, the difficulty of determining the two prime number factors which comprise a public key.  And that is the entire basis for RSA asymmetric key technology.  In RSA, as we've discussed in the past, the public key is the product of two secret prime factors.  And the reason a public key can be made public, even though it's got the secrets in it, it's composed of two prime numbers that have been multiplied.  That's all that the public key is.  So the reason the public key can be made public is that, even knowing it, we do not currently know how to break it back apart into the two prime factors that were multiplied in the first place to obtain it.



So with factorization, this is a place where size matters.  We know, for example, the prime factors of 35.  That's not difficult.  Any grammar school kid can tell you the prime factors of the number 35.  But it's easy to do that only because there are not many possible prime factors smaller than 35, and not many possible solutions to it.  This is not true when the products of primes are several thousand bits long.  We know that primes are surprisingly common throughout the number space.  They do not get less common sort of counterintuitively since a prime by definition is a number not divisible by anything smaller than it.  You kind of think that, as they're getting bigger, there are just, like, so many numbers smaller than it, that it would be increasingly difficult to find them.  Turns out that's not the case.  There's lots of them forever.



So as a consequence, as the primes get really big, there are just too many possibilities.  And no one has found, to date, a highly efficient way on non-quantum computers to break apart a sufficiently lengthy product of two large primes.  Thus RSA stands, although we have been inching the public key size up.  We were at 1024 for a while.  Now we're at 2048.  And eventually we'll be at 4096, just because everybody likes powers of two.  And in fact it's the relative weakness of the factorization problem which accounts for the fact that, for example, an RSA public key needs to be 2048 bits long, whereas an elliptic curve public key, which uses a different hard problem, not prime factorization, it can be 256 bits long and with equal strength.



So the essential question this paper seeks to answer is, even assuming the presence of fast quantum factoring, and also while offering this new faster quantum factorization algorithm that they've called GEECM, under quantum factorization how does the difficulty of usage, that is, using a given RSA public key, how does the difficulty of using it scale relative to the difficulty of a practical attack using even quantum factorization as the prime factors' product length increases.



Okay.  So we know, for example, a quantum computer could probably cut through today's 2048-bit RSA public key without breaking a sweat.  So they wanted to understand how RSA scales.  That is, if we need to make factoring much more difficult because quantum computers can factor much better than current-day computers, and for some reason if we want to hold onto RSA, how does it scale?  So they ask the question, is it actually true that quantum computers will kill RSA?



And they write:  "The question here is not whether quantum computers will be built or will be affordable for attackers.  This paper assumes that astonishingly scalable quantum computers will be built, making a" - the term they use is "qubit," that is the term - "making a qubit operation, a quantum-bit operation..."



LEO:  Q-U-B-I-T, not...



STEVE:  Right.



LEO:  ...the biblical measurement.



STEVE:  Right, right, as in a quantum-bit operation as inexpensive as today's bit operation.  So they say:  "Under this assumption, Shor's algorithm easily breaks RSA as used on the Internet today."  Yup, dead.  "The question is whether RSA parameters" - meaning prime lengths - "can be adjusted so that all known quantum attack algorithms are infeasible while encryption and decryption remain feasible."  So that's what I meant when I said, you know, how does it scale?  Can we keep growing the public key size so that we could still use it, yet very fast quantum computers can't crack it. 



So they write:  "The conventional wisdom is that Shor's algorithm factors an RSA public key 'n' almost as quickly as the legitimate RSA user can decrypt."  Okay, so basically it really weakens the benefit.  The point is that using the key is about the same level of difficulty or speed as cracking the key.  So they say:  "Decryption uses an exponentiation modulo n; Shor's algorithm uses a quantum exponentiation modulo n.  There are some small overheads," they write, "in Shor's algorithm, but these overheads create only a very small gap between the cost of decryption and the cost of factorization.



"The main finding of this paper is that standard techniques for speeding up RSA, when pushed to their extremes, do create a much larger gap between the legitimate user's costs and the attacker's costs."  Meaning the cost to attack increases much faster than the cost to use.  So RSA could be kept alive, kind of on life support, if it was like, if we had no better solutions.  Now, it's worth noting we've already got post-quantum crypto.  We haven't talked about it yet on the podcast because qubits, there aren't a lot of qubits wandering around yet.  None of us have them on our desks or in our pockets or anything, but they're coming.  And so academia is ahead of this.  So this is not, you know, no one should misunderstand this as suggesting that it's practical, or we actually will be using RSA in a post-quantum era.  It's just sort of interesting research.  And I think useful research, too.



So they say:  "These extremes," that is, pushing RSA to its extreme.  "These extremes require a careful analysis of quantum algorithms for integer factorization.  As part of this security analysis, this paper introduces" - and then they talk about their own new faster quantum factoring solution.  And then finally they say:  "We report initial implementation results" -  so they actually, you know, the rubber meets the road here.  They built this stuff under assumptions about what would happen if, they said, okay, if quantum factorization ends up being essentially on par with current, what does it mean?  So they said, and I had to do a double-take on this because it's like, wait, what?



"And finally," they say, "we report initial implementation results for RSA parameters large enough" - that is, remember, long keys - "large enough to push all known quantum attacks above 2^100 qubit operations."  Right?  So like we're going to say that qubits are efficient, so we're going to force you to use a whole ton of them, 2^100.  So this is very much like the numbers we're used to seeing:  2^128, 2^256, okay, this is 2^100 qubit operations, making them again incredibly time consuming and thus impractical.  So they say:  "We report initial implementation results for RSA parameters large enough to push all known quantum attacks above 2^100 qubit operations."  And here it comes.  "These results include successful completion of the most expensive operation in post-quantum RSA, namely" - Leo, are you well centered over your ball?



LEO:  I am.



STEVE:  "...the generation of an eight-terabit public key."



LEO:  I like it.  Eight trillion bits.  So the big ones now are 4,096 bits.



STEVE:  Yeah, yeah.  We don't - we're using 2048, you know, 2,048, and that's just fine now.



LEO:  I think eight terabits, even quantum computing might be hard pressed.



STEVE:  Yeah.  So as I said, this is not meant to - we're not suggesting that this is practical.  But they're saying, okay, we just kind of wanted to go there.  We wanted to say, okay, we got fast quantum factoring.  How badly does this kill RSA?  And the point is, okay, if for some reason you absolutely positively must use prime factorization as the hard - you still have to use it as the hard problem to solve.  And nobody is suggesting that.  But if you did, well, an eight trillion-bit public key, that would do the job.  Of course, you know, schlepping that around, communicating - oh, and of course one bit goes bad, I mean, it's difficult in today's storage environment even to reliably store an eight trillion-bit public key.  It itself would have to have its own error correction, which would increase its size substantially, just to protect against internal bits changing within the key, just sort of spontaneously, for quantum reasons.



So, yeah, at this point, that's a problem.  There are all kinds of post-quantum crypto which is ready to go.  Lattice techniques are looking like they're becoming very popular, but there's a handful of them.  So I just thought this was fun, to say, okay, can we keep this thing alive?  Well, yeah.  Is anyone going to?  Uh, no.  We're going to be switching to different crypto when quantum machines happen.



Now, that was all kind of fun and useful research.  I was really interested initially to see how this claim, which was made last week in a public presentation by Cisco, could be substantiated.  They claimed, Cisco did, that they would soon be commercializing and making available to their enterprise customers, and I will end with a cautionary note here in a minute, the ability to detect malware within encrypted web traffic with 99% accuracy.  And I thought, okay, what?  So I did some digging.



Last October at the AISec 2016 Proceedings, which was the 2016 ACM Workshop on Artificial Intelligence and Security, two Cisco guys, Blake Anderson and David McGrew, delivered their research paper titled "Identifying Encrypted Malware Traffic with Contextual Flow Data."  Okay, now, first of all, that sounds okay.  Interesting, but it's not clear that that's the same as within encrypted web traffic.  So one of my complaints is that somebody at Cisco has decided they want to sexify this research and have everyone go, ooh.  But, okay, contextual flow data is not malware within encrypted web traffic.  It's behavioral, and it's metadata.



Anyway, so the abstract of this paper from last October said:  "Identifying threats contained within encrypted network traffic poses a unique set of challenges."  Yeah, amen, baby.  "It is important" - they didn't say that, I did.  "It is important to monitor this traffic for threats and malware" - again, no disagreement there - "but do so," they write, "in a way that maintains the integrity of the encryption."  Okay.  So we've of course often talked about the middleboxes, as now the term is, which is an enterprise-class proxy that itself is the encryption termination point.  So it's got a certificate, and it is synthesizing remote site certs on the fly and signing those.  And the reason everyone in the enterprise trusts the certs that it has just made and signed is that all the devices in the enterprise have had its public key stuck into their certificate root store specifically so that no alarms will get raised, and everybody trusts those certs.



So the point is Cisco is saying, okay, but we want to do this without decrypting the traffic, which immediately says, "Huh?  What?  Okay, you've got my attention."  They said:  "Because pattern matching cannot operate on encrypted data..."  Right.  As we all know.  Well, good encrypted data is indistinguishable from pseudorandom noise.  They say:  "...previous approaches have leveraged observable metadata gathered from the flow, e.g., the flow's packet lengths and inter-arrival times."



And remember we've previously encountered and reported on this podcast numerous side-channel attacks on encryption which leveraged content-dependent compression ratios.  That is, if the plaintext was compressed, then encrypted - which is the sequence you need because you can't compress encrypted data.  It won't compress.  So the problem was that the ratio of encryption is inherently a function of what data you're - I'm sorry.  Yeah, the ratio of compression is inherently dependent upon what data you're compressing.  And so encryption doesn't change the size.  Thus you can infer the compression ratio from the encrypted size.  And then, if you're really clever, you can, through lots of repeated test encryptions, you can reverse-engineer some unknown plaintext when you pad it with known plaintext.  That we've all covered in the past.



But they're saying, okay, we're not going to do that.  "In this work," they write, "we extend the current state of the art by considering" - and then they have invented a term called "data omnia" approach, which I guess means taking lots of inputs.  "To this end, we develop supervised machine learning models that take advantage of a unique and diverse set of network flow data features."  Okay, so, yes, data omnia, as they term it.  "These data features include TLS handshake metadata, DNS contextual flows linked to the encrypted flow, and the HTTP headers" - meaning nonencrypted headers - "of HTTP contextual flows from the same IP source address within a five-minute window.  We begin by exhibiting," and blah blah blah.



Okay.  So basically what they've said, what the marketing claim is absolutely bears no resemblance to what they're actually doing.  I was tempted to, like, dig into the paper further.  But I thought, you know, it was behind a paywall, didn't seem like it was worth $15 to go any further.



They do, however, say:  "We begin by exhibiting" - this is still in the abstract, so this is what's publicly available.  "We begin by exhibiting the differences between malicious and benign traffic's use of TLS, DNS, and HTTP on millions of unique flows.  This study is used to design the feature sets that have the most discriminatory power.  Then we show that incorporating this contextual information into a supervised learning system significantly increases performance at a 0.00% false discovery rate for the problem of classifying encrypted malicious flows. We further validate our false positive rate on an independent real-world dataset."



Okay.  So they have a 0.00% false positive discovery rate.  And the first that came into my mind yesterday as I was putting this together was, okay, but so does a can of dog food.  You can set any can of dog food down in the middle of a desk.  You can even plug it in, if you want to.  It doesn't matter either way.  It will also have a 0.00% false positive discovery rate.  Malware can be zipping past, and no matter what happens, it will never generate a false positive malware discovery.  What their paper is silent on, is my point, is nowhere do they talk about the false negative detection rate, which is to say, how much malware whizzes past which they miss.



So I did find - and I already had, actually - a full paper that the same authors produced earlier last summer.  So that allowed me to go into it.  That paper was titled "Deciphering Malware's Use of TLS Without Decryption."  And this paper, it looks like what they're now claiming is a result of the fact that they couldn't actually do what they were claiming to do.  And the paper that is fully public shows them trying to use TLS handshake metadata to infer the content of the subsequent connection.  And we know about TLS handshake metadata.  That's a client and a server negotiating cryptographic protocols - which security suites are available, which TLS extensions do they support and so forth.



So they did a lot of research.  I mean, and I'm not knocking them for doing research.  We need research.  And we need research to be published so that people can look at it and learn from it.  The problem I have is when the marketing people get a hold of it and decide that telling the truth isn't going to sell anything except dog food.  And so they end up saying something, they make a claim that is patently impossible and which no one can deliver.  So the original research showed that, if they took, I don't know, I didn't count, but it was like 10 or 15, somewhere like that, different families of malware, and they examined the TLS handshake metadata which those families of malware used, given a universe of known malware, they could discriminate with 90% accuracy which among that malware the handshake metadata was.



Okay, so that's an entirely different problem than finding malware in a sea of benign traffic.  So this is useful.  But I noted that they weren't using DNS metadata originally.  Now they are.  And of course using DNS, looking at the DNS queries that a network makes, will go a long way toward telling you if something is malicious because it's, you know, malicious domains are themselves, by their name, suspicious looking.  We see what command-and-control servers use, you know, 64 characters of gibberish.ru, for example.  Okay, that would raise some suspicion.  And DNS is not encrypted, as we know, so it's always available for inspection.



So anyway, the problem is that Cisco is claiming that they're going to be deploying this soon.  Cisco's Senior VP David Goeckeler said at a press event in San Francisco a week ago, last Tuesday, "We get both privacy and security," adding that the new technology, he says, can detect malware with 99% accuracy.  Okay.  Maybe they have a system that can do that, but it is not by looking inside of non-decrypted TLS traffic.  It is by looking at everything else.  And the fact is TLS, the actual metadata, we know from their previous work is providing almost no useful information.  I would imagine, just from the abstract in the paper which closely precedes Cisco's announcement that they have some massive breakthrough, is that they're probably looking at DNS largely, using that as most of the source of input to the system.



So anyway, my advice, if anyone is a Cisco customer with an enterprise profile, this has every earmark of being a low success rate heuristic which is attempting to do something certainly desirable and sexy, but basically impossible.  So don't rely on it exclusively at first.  Test it thoroughly in a network lab setting before signing any long-term contract.  I would say, you know, approach this one with skepticism because it just - it'd be nice to do.  Maybe by looking at everything else like DNS you could develop some useful heuristics.  And you don't want them false-positiving too much because that's going to, of course, create a problem.  And, boy, I would be a fan of not needing to decrypt, as everyone knows, our HTTPS traffic at the enterprise border.  It's just a cleaner solution all around not to have to do that.  So if they can pull it off, bravo.  But I would say approach with caution.



So the unfortunate title of this work is "The OpenVPN Post-Audit Bug Bonanza."



LEO:  Uh-oh.  Oh, dear.  Everybody uses OpenVPN.  Everybody.  Right?



STEVE:  Yes.  And I believe should.  Guido Vranken has a full report in his blog on WordPress.  I've got the link to it in the show notes.  But to set the context of this, I need to share his claim.  So he writes in the summary:  "I've discovered four important security vulnerabilities in OpenVPN.  Interestingly, these were not found by the two recently completed audits of OpenVPN code.  Below you'll find mostly technical information about the vulnerabilities and about how I found them, but also some commentary on why commissioning code audits isn't always the best way to find vulnerabilities."  And I'll certainly be responding to that, as well.



He says:  "After a hardening of the OpenVPN code as commissioned by the Dutch intelligence service AIVD and two recent audits, I thought it was now time for some real action," he says.  "Most of these issues were found through fuzzing."  He writes:  "I hate to admit it, but my chops in the arcane art of reviewing code manually, acquired through grueling practice, are dwarfed by the fuzzer in one fell swoop.  The mortal's mind," he says, "can only retain and comprehend so much information at a time; and, for programs that perform long cycles of complex, deeply nested operations, it is simply not feasible to expect a human to perform an encompassing and reliable verification."



He writes:  "End users and companies who want to invest in validating the security of an application written in an [and he has in quotes] 'unsafe' language like C, such as those who crowdfunded the OpenVPN audit, should not request," he says, "a manual source code audit, but rather task the experts with the goal of ensuring intended operation and finding vulnerabilities, using the strategy that provides the optimal yield for a given funding window."



Now, just know I disagree with pretty much everything he's said so far.  But there's value in this.  He finishes:  "Upon first thought, you'd assume" - oh, no, this is me now.  So okay.  So he says, you know, fuzzing rocks; code audits fail.  So upon first thought you'd assume both endeavors boil down to the same thing.  Oh, no, I'm sorry, this is him still talking, his voice.  "Upon first thought you'd assume both endeavors boil down to the same thing, but my fuzzing-based strategy," he writes, "is evidently more effective."  I will argue that in a minute.  "What's more, once a set of fuzzers has been written, these can be integrated into a continuous integration environment for permanent protection henceforth; whereas a code review only provides a 'snapshot' security assessment of a particular software version."  No argument there.  He finishes:  "Manual reviews may still be part of the effort, but only where automation (fuzzing) is not adequate."



Okay.  So first of all, just to make sure everybody understands, we all know what a code audit is.  Fuzzing, as we have discussed in the past, is the art and practice of just throwing nonsense at the API or the communications channel of something and seeing if it breaks.  I remember way back, years ago, eEye, the eEye security firm, E-E-Y-E, had a whole lab of Windows machines, and they were finding problems.  And so, like, suddenly one of the machines would just stop because their code had thrown some data at a function which it wasn't written to expect, and the system would crash.  And so as the fuzzer was doing this fuzzing, it was logging everything it was about to do so that they could rewind and repeat the problem and then go take a look manually at what the fuzzer had discovered.



Okay.  So my position is I strongly disagree that either approach is more or less valuable than the other.  The two approaches are different, and it's that difference that makes both important.  We know from so much coverage in the past on this podcast that there are definitely many classes of critical problems that fuzzing will not find, for example, subtle flaws in cryptographic implementations, secrets-based timing, and power changes that would enable side-channel attacks, unsafe assumptions about the use of fundamental cryptographic primitives, and I could go on like that all day, talking about what it is that a cryptographer sees when they look at this kind of code, that isn't about does it crash, it's about is it worth using, I mean, if when it's working is it secure.  So this notion that deliberate code auditing is not every bit as important as fuzzing is utter nonsense.



That said, as we've talked about, fuzzing is a very powerful tool for discovering an entirely different class of very important bugs that could, as Guido correctly asserts, easily slip past code auditors.  That's not the kind of bug they're looking for.  It's not the kind of bug they really can look for, which is why fuzzing makes absolute sense in conjunction with code auditing.  And as he points out, where he talks about a "given funding window," he's right.  Fuzzing is super cheap because it's automatable.  And it's super easy to do once it's been set up.



So, yes, I'm delighted that he did this.  I'm delighted that as a consequence OpenVPN is even more solid than it was after the code audit, which made it better.  And now we've got the best of both approaches.  But no one should think for a second that any money could have been saved by only using fuzzing instead of a careful code audit by cryptography-savvy coders and developers who know how to take a look inside of this and make sure that the things they can see were done correctly.  There are things they can't see, and that's where fuzzing can be very effective.  So hats off to Guido for doing this.  And I understand that he's proud of his results, and he should be.  But it's not the case that one replaces the other.



LEO:  Should we be concerned about OpenVPN?  I mean, did he find bugs that are, in fact [crosstalk]?



STEVE:  Yeah.  Oh, yeah, yeah, yeah.  I'm glad you asked, yes.  It has been fixed.



LEO:  Good, okay.



STEVE:  So, I mean, there were several crashes and something that may have been leverageable to remote code execution, so the typical kinds of things that are problems.  His fuzzer crashed OpenVPN, and he found four different problems where you could double-release some memory.  And although he didn't take it to an exploit, he said, "Oops," you know, "that's a bug in the code.  Let's fix that bug."  So the updated version of OpenVPN is already available.  And I'm sure everybody who has their repositories being checked for updates, I looked and I saw that the FreeBSD server at GRC has an OpenVPN update waiting for it.  So, yeah.  So it's important to update OpenVPN now because we just found some more problems.  But I didn't want anyone to come away thinking, oh, look, that audit was BS, and we should just turn everything loose to fuzzing.  It's like, no.  We need both.



Boy, and speaking of fuzzing, it turns out, and this is a problem, Intel's Skylake and Kaby Lake processors have been found to contain serious microcode bugs.  We know that low-level processor flaws are not without precedent.  But fortunately they are extremely rare; and our chip vendors generally, I would say, do an amazing job dealing with the stunning complexity of modern Complex Instruction Set, CISC, C-I-S-C, processors.  So far Intel has been relatively quiet about the problem, but this is in the news.  Reporters who have talked to engineers at Dell and Intel have been told that the problem exists, and there are fixes for it that are just waiting for confirmation before being pushed.  There is a microcode patch which is currently being quietly tested to make sure that it doesn't break anything else.



And naturally, being a processor-level bug, it is OS agnostic.  All operating systems and other software running, any software running under the OSes - and that would include VMs, potentially, so Windows, macOS, Linux, FreeBSD, et cetera - can be vulnerable.  A quietly published Intel chip errata reads:  "Under complex micro-architectural conditions" - meaning the micro architecture that is what the microcode drives which implements the higher level instruction set, which is what the code sees and runs - they write, "...short loops of less than 64 instructions that use the AH, BH, CH, or DH registers, as well as" - and by the way, that's the high eight bits, the AH, BH, CH, DH, the high half, the high eight bits of the A, B, C, and D registers - "as well as their corresponding wider register."  RAX which is the 64-bit AX, EAX which is the 32-bit view, or AX or AH may cause, they write, "unpredictable system behavior."  Yeah, as in the instructions don't work right.



They say:  "This can only happen when both logical processors on the same physical processor are active."  In other words, this is a hyperthreading problem.  As we know, Intel, before they went to multicore, they said, hey, you know, if we take one physical processor and allow it essentially to have two program counters, that is, so that we could be doing two things at once, we could get - we could, like, squeak out marginally more performance.  It wasn't, like, double.  And, frankly, it wasn't very much.  I don't remember now what the numbers were, like 10% more.  I mean, you know, it was free.  So it was like, okay, not bad, as long as it's free.  And that's what the so-called hyperthreading is, where essentially they create two virtual processors from one by allowing two things to happen at once.  Well, it looks like Skylake and Kaby Lake, but not, for example, Haswell, the predecessor family, have a microcode flaw in the implementation of hyperthreading which affects short loop repetitions of the registers of some instructions.



So, okay.  So there is a Debian page which has the best discussion of this.  I have the link in the show notes.  In fact, it's so important and so good that I made the first show-numbered bit.ly link in a long time:  bit.ly/sn618.  So that just bounces your browser, redirects your browser to this Debian.org page, where they've got beautiful coverage of this.  What happened was that some developers at the beginning of the year, back in January, were working on a compiler for the OCaml language, which is an obscure kind of crazy-looking - I took a look at it because, like, what's OCaml?  And I still don't know.  But it's amazing.  And as a side effect of the way it was driving the downstream GCC compiler, it set some constraints on GCC that were unusual, that caused it to produce problems.



And so these guys are thinking, oh, crap, our OCaml compiler isn't working.  And they looked at the code, and they said, uh, yes, it is.  The chip must be broken.  Which of course they couldn't believe because that just doesn't happen.  Turns out they were right.  And so their compiler was inducing this flaw in Skylake and Kaby Lake chips, which - and I'm not sure of the sequence of this.  They went back and they discovered that even as far back as the end of the first quarter of last year, this could be occurring on Skylake chips.  So Intel knows about it.  There is firmware kind of like somewhere.  And I imagine that what we'll end up seeing is a firmware patch that Apple will push out with macOS, Microsoft will push out to their users of the affected processors, and the various Linux and Unix and so forth, everybody's going to get this, end up getting this patched.  The good news is firmware is patchable.



LEO:  How does microcode get patched, though?  Because that's hardware.



STEVE:  No, it actually is patchable.  You're able to patch the microcode in Intel's chips.  It's been done before.  There was a firmware...



LEO:  It doesn't modify the chip.  It must, like, somehow interrupt the call or something; right?  Or no.



STEVE:  I've not looked into it.  They're calling it a firmware update.



LEO:  Interesting, huh.



STEVE:  So I don't know.  It wouldn't surprise me if they load something from a store at power-up and then allow it to be patched.  So anyway, I imagine sooner or later we're all going to be getting this.  The Debian page suggests - and I don't know that anyone has to take action on it.  First of all, if you don't - if you have Haswell or earlier, you're not affected.



LEO:  That's what they do, they fix the microcode.  That's why you have microcode.  It interposes between the actual physical hardware, and it lives in memory.



STEVE:  Yeah.



LEO:  It lives in special high-speed memory.  Well, that's interesting.



STEVE:  Yeah, yeah.  And we did, for anyone who's interested, we did a whole series years ago on modern processor architecture and how it all works.  And the incentive for microcode, because microcode began in the...



LEO:  [Crosstalk]; right?



STEVE:  Well, it began in the minicomputer era.  For example, the ARM chips don't need microcode because they are RISC processors, that is, they have such a simple instruction set that you can implement just in hardware.  But Intel took the earlier path.  They have a complex instruction set.  And so it's so complicated that you can't implement it reasonably in hardware.  So you have to create a computer within a computer.  Basically the microcode is interpreting the complex instructions to get the work done.  And so this is a benefit of, or kind of a mixed blessing, but in this case it is fixable.  And so you can patch it in order to change its behavior at a very low level.



LEO:  Early on in the 8086 they had - remember the floating point era.



STEVE:  Yup.



LEO:  It did floating point math wrong.



STEVE:  Oh, and you were able, I remember, you were able to bring up a Lotus or an Excel spreadsheet.



LEO:  You could demo it easily, yeah.



STEVE:  Yeah, and put in a couple special numbers, and [buzzer sound], it gave you the wrong answer.



LEO:  Yeah.



STEVE:  It's like, ooh, that's not good.



LEO:  Yeah.



STEVE:  Yeah.  Okay.  So I said at the top of the show that a malware technique had been commercialized.  And this is unfortunate.  Gizmodo picked up on this, and I did a little bit of deeper research into a company called NaviStone, N-A-V-I-S-T-O-N-E.  And they are, you know, they're everything I object to in terms of the way a company should operate.  They are just the king of slime.  Okay.  So you visit a website and begin filling in a contact form, or maybe the site's Create an Account form.  Or maybe start filling in purchase information.  Or maybe your own browser, which has been paying attention in the past, beats you to the punch on some fields and autofills a bunch of them for you, being your little handy-dandy assistant.



In the traditional world, where we all grew up, this was a passive process, meaning that your browser was showing you a page.  There was a form there.  And somewhere on the page, generally at the bottom below the fill-in area, there would be a Create an Account button, or a Submit button.  And the implicit and longstanding rule has been nothing happens with any of that information until and unless you press the button to explicitly and deliberately send that provided information off to the hosting web server of the website you're visiting.



And, Leo, you'll remember a few months back we talked about it, in fact you played with during the podcast, a malicious code on browser pages that was deliberately creating hidden unseen offscreen form fields which the browser's autofill automation would fill, even though you couldn't even see it doing it.  And of course it was doing it to help you, with well-meaning intent, but that malicious code on that page could suck those browser-filled unseen field contents off the page and spirit them away to parts unknown.



Well, on the heels of the adage "If it can be done, someone will do it," we have "If someone can make money doing it, they will."  And in this case we have this what seems to me to be an ultra slimy company named NaviStone.  Their public website says, you know, it claims - get a load of the - this is just amazing:  "Reach your anonymous website visitors with retargeting postcards and convert 50X more visitors than digital display ads."  They have also a slogan:  "Convert your anonymous website traffic.  Harness the power of consumer intent data.  Take your retargeting offline."



Then they say:  "Until today, your ability to retarget anonymous website shoppers was limited to low-impact, low-response digital display ads.  With the NaviStone turnkey postcard program you can reach out directly to anonymous website shoppers, tailor your marketing to the individual shopper based on their website behavior, mail personalized postcards within 24-48 hours after a site visit."  And then in their little "Who We Are" box - because I was just, I was thinking, oh, wow - they say:  "Traditional direct marketing contact strategies are driven entirely by past purchase behavior.  As that data ages, it becomes less predictive of future responsiveness.  At NaviStone we lead the vanguard in progressive website visitor tracking technology."  Yes, you can find it here, folks.



LEO:  I might a little bit disagree with you.  And the only reason I say this is it's very common - this is what it's mostly used for.  And you might have even done this.  You go to a site, and you load the shopping cart up, but then you don't execute.  You leave for whatever reason.  And in fact a lot of people do this on purpose because what you'll get later is an email saying, hey, we noticed you loaded the shopping cart, but you didn't buy it.  Maybe you'd like to get 10% off or 20% off on these items.  And that's primarily what this is used for.



STEVE:  So you're saying, if you are completely anonymous, and they don't know who you...



LEO:  Well, you're anonymous because you didn't submit.  So if I go to a website, let's say I go to Zappos.  By the way, it's very widely used.  If I got to Zappos, and I shop, and I load the shopping cart, and then for whatever reason I leave, maybe I just - I'm not done, I'm going to go back to the shopping cart, I haven't given them any information except the fact that I want to do this.  So the next time I go back to that site it can say, oh, I see you didn't submit.  Would you like a discount to encourage you to submit?  That's the kind of usage.



STEVE:  Ah.  And I don't have a problem with that at that one site.



LEO:  Well, that's, see, I don't think it's completely slimy.  I think that that's the intent of this.



STEVE:  Well, the - okay.



LEO:  The assumption you're making which everybody makes is, well, if I fill in a form on the website, the website has no access to it unless I post it, unless I hit the Submit button and do a POST.



STEVE:  Right.



LEO:  So what this is, is some JavaScript that watches the form being filled out and captures it without a post.



STEVE:  Correct.



LEO:  I don't know if that's completely slimy.  It certainly could be used in a slimy fashion.



STEVE:  Well, it sends it to a third party.



LEO:  Right.



STEVE:  So it's not going to the site you're visiting.  So the site you're visiting has contracted with NaviStone, and that...



LEO:  Very many, by the way, very many sites do this.  This is very common in ecommerce.



STEVE:  Okay.  And what they're saying is that you will then receive a postcard or an email, even though you have submitted nothing.



LEO:  Well, that's a little dubious because how are they going to get your email?



STEVE:  That's my point, is this is serious web tracking.  This is not just you went to this site, and then you come back, and they say, oh, look, we notice you didn't finish before.  Gizmodo tested it.  Gizmodo says three sites - Rockler.com, gift site CollectionsEtc.com, and clothing site Boston Proper, they said - "sent us emails about items we'd left in our shopping carts using the email addresses we'd typed into the site, but never submitted."



LEO:  That's how they got your email.  It was part of what was typed in, yeah.



STEVE:  Well, yes.  Although NaviStone also has your physical address.  And what they're selling on their site...



LEO:  And how do they have your physical address?



STEVE:  That's my point.  They're tracking, Leo.  This is actually real.



LEO:  No.  They have your physical address if you've entered it into the form.  That's where that comes from.



STEVE:  No.  They're a cross-site tracking system.  That's what they do.  They aggregate this information from across the web, following you around...



LEO:  Oh, yeah, yeah, yeah.  You may not have entered - okay, that makes sense.  Maybe you entered it on Zappos, but you got a postcard from Boston Proper because you'd entered it at one point.  But then how do they identify you?  They must have some sort of...



STEVE:  Well, of course.  If their JavaScript is also on all these sites, then your browser is carrying their cookie.  And so they're able to link you across all these sites.



LEO:  So that's why you turn off third-party cookies.



STEVE:  Right.



LEO:  Yeah.



STEVE:  So anyway...



LEO:  I think the problem is that ecommerce cart abandonment rate is about 70%.  So what I think completely legitimately companies like Boston Proper are just trying to do is, well, maybe that person wanted this stuff, but for whatever reason got distracted, or the browser crashed.  We'd like to remind them.



STEVE:  And I would have no problem if they kept it in the family.  I would have no problem if their JavaScript watched that and then saw you come back.  Instead...



LEO:  Oh, man.  Very few ecommerce sites are self-hosted.  You know, they use Shopify and other systems.



STEVE:  Right, right.



LEO:  I mean, that's pretty common.



STEVE:  And in fact there is a search service, sort of like or reminiscent of Shodan, that's called BuiltWith.  And we've talked about it before.  BuiltWith is a spider that goes around and looks at, like, it basically harvests the technology that sites use.  Gizmodo was a little curious, so they found more than a hundred sites using this NaviStone technology which aggregates that stuff.



LEO:  Yeah, yeah.  I bet it's many more than that.  If not NaviStone, it's somebody else.



STEVE:  It's a company very much like it, yeah.



LEO:  NaviStone agreed to stop doing it; right?



STEVE:  They backed off as a result of some of this coverage of one aspect of this.  But, I mean, it is their business model is to do this.



LEO:  Yeah.



STEVE:  And WannaCry is still making - is bringing tears to some people.  There were two instances that occurred in the last week.  The Honda Sayama auto plant in Japan stopped their production line of engines and two classes of Hondas, a van and something else, I don't remember, for two days, for nearly 48 hours last week when WannaCry somehow came back and found a couple of the machines that were critical to the assembly line's operation.  And then, so that was one bit.



And the other was people were sort of celebrating that Australia's stoplight ticketing cameras were taken down.  A network of 55 cameras in Australia got hit by WannaCry, apparently when a technician connected an infected USB drive to one of the devices, and then it quickly spread through the network, which was apparently running Windows OS.  So in all of these cases, the moral for us is, if you want your stoplight cameras or your nuclear submarines on a consumer operating system, well, you get what you pay for.  So, yikes.



Linksys has responded to the CherryBlossom CIA Vault 7 attack.  I'm glad for that.  They're a router that many of us have and use.  So I just wanted to give a heads-up that last week they, in fact they addressed it directly in an advisory dated 6/21, they said:  "Linksys is aware of the CherryBlossom project that was recently released by WikiLeaks' Vault 7 publication.  Based on the WikiLeaks report, customized firmware - and of course we covered this at length last week - was created for certain older Linksys routers without our knowledge or consent for the purposes of monitoring, controlling, and manipulating Internet traffic of a targeted user."



And so customized firmware, they wrote, can be loaded onto a router, either with physical access to the router, proximate access to the router via WiFi, or intercepting the device in transit to be delivered to a user.  Which we know apparently has been done in the past.  They said:  "If users believe their router firmware may have been compromised, Linksys" - and of course how would anyone really know that - "Linksys recommends that users download the latest available firmware" - and then it's just www.linksys.com/support - and update your router.



So, oh, and one thing I noted that I don't think I'd ever seen before.  I don't know if this is a recent occurrence.  But the advisory finishes, after listing all of the affected model numbers:  "We would also like to recommend the following changes after the factory reset is complete to further secure the router:  First, set a strong admin password, one that includes capital letters, numbers, special characters, and a password length of at least" - okay, well, they're saying eight characters; but, as we know, more is better here.  "Disable guest access if it is not in use.  And disable router features like WPS and UPnP," they say, "if they are not being used."



So yay to Linksys suggesting that disabling WPS we know we've long recommended, and Universal Plug and Play, which is of course a real problem in this world where people are getting IoT devices infected, and they can use UPnP to get access to the external world.



Last episode, which was titled "When Governments React," we looked at France, Britain, Japan, Germany, and Russia.  And now we have a sixth country.  Australia has just said that they intend to push for encryption backdoors at next week's Five Eyes meeting.  The Five Eyes alliance, as we know, is Australia, Canada, New Zealand, the U.K., and the U.S., which are a group bound by multilateral U.K./USA agreement for joint cooperation in signals intelligence, military intelligence, and human intelligence.  And it's always important for us to remember that any non-technical policy person using the term "encryption backdoors," you know, we have no idea what they mean, well, because neither do they, really.  So until we see actual legislation, all we can infer from this is that they're not happy.



In this case, the Australian Attorney General George Brandis is stating he'll be pushing for backdoors, whatever that means, at the upcoming meeting of the Five Eyes in Ottawa, Canada next week, where they will discuss tactics to combat terrorism and protect borders.  Australia has made it clear it wants tech companies to do more than to give intelligence and law enforcement agencies access to encrypted communications.  Brandis said, in a joint statement:  "I will raise the need to address ongoing challenges posed by terrorists and criminals using encryption.  These discussions will focus on the need to cooperate with service providers to ensure reasonable assistance is provided to law enforcement and security agencies."



So he has a reputation in Australia.  He's previously rationalized away potential objections to backdooring encryption, saying that it is people's tendency to overshare on social media, and that that indicates they don't care if the government or several governments have access to their private messages.  So, and there's some grain of truth to that, as I've been arguing lately.  We've seen lots of studies that suggest a person will give you their password in trade for an ice cream cone, and that everyone says they want encryption; but when it comes down to it, in order to be truly secure, people tend to be a little lazy, so not going all the way.



I had no idea that the NSA had a repository on GitHub.  And it's easy to find:  nationalsecurityagency.github.io.  And, wow, there are a bunch of neat things.  Just to give you just a sense for it, something called Apache Accumulo, a sorted, distributed key/value store that provides robust, scalable data storage and retrieval.  It adds cell-based access control and a server-side programming mechanism that can modify key/value pairs at various points in the data management process.



Something called CASA identifies unexpected and prohibited Certificate Authority certificates on Windows systems.  That sounds very useful.  Control Flow Integrity Research; DCP, a program that reduces the time span needed for making a forensic copy of hard drives for forensic analysis; and on and on.  So if anyone's interested, there's just a bunch of really interesting projects:  nationalsecurityagency.github.io.  So somebody tweeted that to me, and I took a look, and I was really impressed.  There's lots of stuff there.



I did get a lot of feedback from our listeners, who followed up on the question of HP printer firmware.  Many people tweeted photos, emailed photos, and just sent me notes.  So far not, out of maybe a population of 30 or 40 all over the place, not a single HP printer appears to default to firmware update automatic.  I think for whatever reason HP must have made the decision that, yes, our printers have firmware.  But when they leave the shipping dock, they work.  So you could update them if you want to; but they work, so leave them alone.



Unfortunately, as we discussed at length last week, yes, they work; but they've also got very significantly worrisome exploitable firmware because they've got a full Linux subset that can be taken over through various means.  So as I said last week, I think it's time everyone should take a moment to update their HP printer firmware, since HP is not doing it automatically.



Oh, and I said something wrong.  Ben, who tweeted as @Gingiraffe, said two things:  "First, Apple two-factor authentication doesn't use iMessage."  And "Apple two-factor authentication," he says, "does have some limited time-based one-time password integration available.  Love the show.  Cheers."  And the moment I read "Apple two-factor authentication doesn't use iMessage," it's like, duh, of course.



Now, which is to say the UI is not iMessage.  And what I should have said last week, which is where I completely agree I misspoke, is I said Apple had the advantage of not using the cellular network's existing text messaging platform to send their one-time password tokens because they've got their own system.  I said iMessage.  That was wrong.  I don't know that it doesn't leverage that communications flow.  It certainly doesn't have to.  And the presentation is different.  I don't know, because it is a six-digit code typically, at least in one of the types of prompts and challenges you can receive, maybe it is time-based, where they have a shared key with your device.  Who knows.  Anyway, thank you, Ben, for letting me correct that.



LEO:  Although as we pointed out, if you only have one Apple device, it will use text messaging.



STEVE:  Because it has no choice.



LEO:  It has no choice.



STEVE:  Right.  I got a nice note, or an interesting follow-up to a discussion last week from Grant Taylor, who tweets as @DrScriptt with two T's.  He said, and this sort of gave me a bit of opportunity to clarify something, he said:  "@SGgrc.  Running SpinRite before a format might speed up a format on an otherwise not completely happy drive."



Remember the question I answered was should you run SpinRite before or after.  And I kind of said, eh, I really don't think it matters.  So I would say that it's true except that the explosion in storage capacity we have witnessed over the last 10 years has changed the entire nature of - and I'm putting it in air quotes here - "formatting."



It was once the case that formatting a drive meant scanning and verifying the drive's storage surface, looking for known and already marked defective sectors.  Back then, drives were checked by the manufacturer for "known defects," as they were called, and those were specified and could be entered into the low-level format process to start those sectors off as flagged bad.  And then, additionally, the formatting software would just go out, and it would look at all the sectors, and also just kind of make sure they're all present and accounted for.  And then, after doing that, it would establish the file system on the drive with those defective areas never put into use, that is, pre-established in the file system as defective.  So sectors would fall into clusters, and the free cluster map for the file system would already take those out of service, so nobody could ever use them.



But as drives have gotten bigger, it has become really infeasible to do that any longer.  As SpinRite's own struggle to run in a practical length of time has revealed, drives are becoming large relative to any system's, and even the drive's, own ability to actually transfer all of the drive's phenomenal amount of [audio dropout] into the system.  And as I've discussed before, I'll be making a huge leap forward in SpinRite's performance with its next release through a combination of pure assembly language code, which will work directly with the hardware, no more BIOS, and leverage the hardware's maximum theoretical data buffer size of 32MB.  So this goes from 32K buffers, which we have now, to 32MB buffers, which is the most that the hardware can handle.  So SpinRite from v6.1 on will transfer data at the maximum rate, limited only by the drive's rotation; or, in the case of solid-state media, the raw speed of the interface.



But today's drive sizes mean that simple formatting, like what we do when we set up a new drive, is no longer practically able to check the drive's storage integrity.  Instead it must assume everything is okay.  In Windows we see the so-called "quick format," which makes that assumption.  It simply builds the file system's directory structure at the front of the drive and hopes for the best.  It skips any actual testing of the drive for the sake of practicality and expediency.  And so I would argue that running SpinRite sometime, whether before or after, is probably a good thing to do because otherwise it may be the case that some areas of your drive are never visited.  I mean, they weren't visited during formatting.  And unless you absolutely fill up your drive, they may stay largely unused for some length of time and not actually be available for use, I mean, not be good any longer by the time you finally get to it.



So again, nobody actually formats a drive any longer.  You just can't afford to.  They've gotten too big.  So you just say, okay, the drive says it's this big.  We're going to assume it's telling the truth and that there's all that unexplored territory out there.



And finally, I have a couple of closing-the-loop bits, closing-the-loop feedback from our customers.  Jeffry Erickson tweeted that "ProtonMail now has a free and paid VPN service."  And he said, you know, "Topic for SN?"  And, okay.  So VPN services, I think, are more or less generic and, as we know, are widely available.  That being the case, I think that certainly more choices and wider feature sets are better.  But rapid adoption of a new service just because it's new is probably not the way to obtain the greatest security guarantee.  A VPN service is one place where I think letting things settle down a bit makes a little more sense, all other things being equal.  If a new service has some specific feature that makes it especially beneficial for a specific user, like where they are, their terms, whether you're already a subscriber and they're making the VPN service available as part of your existing subscription or your relationship with them, okay.  But just standard security wisdom suggests that, you know, give it six months.



Again, there are so many solutions which are time-hardened that I don't know if there's a reason to jump on a brand new one except to notice that it's there, celebrate it, and then wait for the Wikipedia comparison of VPN features to add that service and get populated so that you can easily take a look at it and see if makes sense.  So I think it's a good thing.  I just don't know that jumping on something brand new, when it's something you're depending upon, makes sense unless you have a specific reason to do so.



Oh, and Leo, I heard you talking over the weekend, answering the question about Steve's favored imaging software.



LEO:  Yeah, and I think I got it wrong.  I said Drive Image XML.  That wasn't it?



STEVE:  Yeah.



LEO:  I just googled it.



STEVE:  Unfortunately, well, and you did say it's got this annoying generic name.



LEO:  Yeah, like Drive Imager or something.



STEVE:  Like it definitely does.  Or like Image for Windows.



LEO:  There it is.



STEVE:  Which is just a little - but you...



LEO:  I did buy it, and I use it.



STEVE:  And it's what I use.  I use it on everything.  It's from a company called TeraByte, T-E-R-A-B-Y-T-E.  It's very affordable.  It's $39 or something.  And they've got versions for Linux and DOS.  They've got packages.  It supports UEFI.  And what I like about it is that it also knows about NTFS.  So you can do single large images rather than the earlier systems that only knew about FAT32 and so they had to break them up into 4GB pieces, which once upon a time that was big.  Not anymore.  So Dennis, thanks for asking the question, and it's TeraByte Unlimited is the whole name of the company, Image for Windows, and also for Linux and DOS.  And they've got a bunch of neat utilities and add-ons and freebies, and they're just a great company.



LEO:  Windows 10 does come with its own imaging.  I think...



STEVE:  Yeah, I heard you mention that.  Do earlier versions not?  Is that a new - is that a Win10 feature?



LEO:  I don't know.  I'm going to have to ask some people.  It might be - that's a good question.  I don't know.



STEVE:  Because I've never been aware - I've always been aware of, of course, backup has always been part of Windows.  But that's file level.  And as you mention, what we want is something that can boot from a completely dead or an empty drive and then immediately, or given time, reinstantiate the file system, the OS, the other partitions, and everything.  And the cool thing about TeraByte's is, for example, you can click on the root of the tree of the drive, which is the drive, the partition table, the various weird partitions that Microsoft now creates.  You just say, yeah, give them all to me.  And it puts them all in one image.  And if you then give it a virgin drive and say, here, put that back, it'll just recreate.  Oh, and it's able even to dynamically resize the various partitions to fit the drive that it's going to.  So, I mean, yeah, really, those guys solved the problem.



LEO:  Yeah.  I think it actually started in Windows 7.  In fact, it's the older style of backup.



STEVE:  Interesting.



LEO:  And one of the nice things about the Windows version is that all you have to do is put an install disk in, which you've made, of course, because if you don't - you should do that before you do a drive image, do a recovery disk.  And then you can run it from the recovery disk.  So what you do is you, if you go to Windows key, type "backup," and then it says at the bottom, "looking for an older backup."  And this is the thing that I had to kind of figure out.  Go to Backup and Restore (Windows 7).  And then this has an imaging system built in.  So I think this is the Windows 7 imaging.



STEVE:  Oh, neat.  I will check that out.



LEO:  It allows you to go to a hard drive, DVDs, a network location.  And then you restore it by running the system installer, and you can just restore from the image directly.



STEVE:  Very nice.



LEO:  Which is about everything you'd want, I think, in a Windows [crosstalk].



STEVE:  I think yes.  And I would argue that, for anyone from Win7 on, that makes sense.



LEO:  Yea, yeah.



STEVE:  Nice.



LEO:  It's a little hard to find it.  It's not obvious that it exists.  Somebody had to tell me.



STEVE:  And now you've told all of our listeners, so that's good.



LEO:  Yeah. 



STEVE:  Our frequent contributor, Simon Zerafa, sort of sent a reminder that I wanted to pass on.  He says:  "Time to check for apps with access to important accounts.  Here's the list for Google.  Do the same with Twitter and Facebook."  And I did.  And sure enough, I found some old things that I'd forgotten I'd given permission to access Google.



So it's myaccount.google.com/permissions.  Again, just https://myaccount.google.com/permissions.  And just look, if you are a Google user, take a perusal over the list.  I took about half of them out.  I had, like, 15 things.  And it's like, oh, yeah, what's that?  I haven't used that forever.  And it's just good, just makes sense to clean that kind of thing up because no apps typically remove themselves.  I don't even know if they can, once given permission.  So there's also, of course, Twitter has that and Facebook has that.  So thank you, Simon, for the reminder.



And I love this.  This is actually the perfect conclusion for this week.  This is our "say it isn't true" horrifying observation of the week.  Someone tweeting as Klingonveckan said:  "Regarding SN-617," which of course was last week, "about eight characters being the most common password length.  I just have to bring up the fact, he says, that 'password' is eight characters."



LEO:  Not a coincidence.



STEVE:  It's like, oh, say it isn't true.



LEO:  Yeah, not a coincidence.



STEVE:  Yes, indeed.



LEO:  I think the chatroom might have observed that at the time, as well.



STEVE:  Ah.



LEO:  In fact, because of your comments last week, I made some new passwords this week, and I made sure they were all odd-numbered lengths.  And when the site said you can have up to 30, I would make it 29, right, because 30 would then be more common than 29.



STEVE:  Yup.



LEO:  So the idea is to kind of foil the statistics, I guess.



STEVE:  Very good.  Don't follow the pack.



LEO:  Yeah.  Be your own person.  Well, if you listen to this show, you probably are your own person, and you probably drive your friends and neighbors crazy.  But that's because you know more than they do, thanks to this guy right here, Steve Gibson.  Go to GRC.com to find Steve's stuff.  It's all there, GRC.com, including SpinRite, the world's best hard drive maintenance and recovery utility, his bread and butter.  Information about SQRL is there, Perfect Paper Passwords, Password Haystacks.  ShieldsUP!, lest we forget.  How many millions of people have used that now?  I mean, it goes up at an amazing rate.  I always, when I set up a router, that's the first place I go.  GRC.com.  He also has the show there, audio versions of the show, plus Elaine's transcripts.  There'll be one or two places in this show where she doesn't know what you said.  Just weird, just little dropouts.  But it wasn't bad.  It was better, for sure.



STEVE:  Good, I'll do it from now on.



LEO:  Yeah.  And - sorry, Elaine.  If you want audio or video, you can also get it from us, TWiT.tv/sn.  And if you have a podcast app that you favor, you can always search for Security Now! in that and subscribe, and you'll get each and every episode.  And that's always nice.  We will be back next Tuesday, 1:30, right after MacBreak Weekly, 1:30 Pacific, 4:30 Eastern, 20:30 UTC, if you want to tune in live.  Join us in the chatroom, irc.twit.tv.  You hear them referenced quite a bit.



STEVE:  I think actually in this case we will be back in two weeks, my friend.



LEO:  Oh, you know, you sent me an email, and I wanted to talk to you about that.  Yeah, next week is the Fourth of July.  And TWiT is dark.  But, I mean, if you want - I know you hate missing shows.  And I know your audience hates it when you miss shows.  So if you wanted to record the day after or the day before, something, we could do that.  Otherwise, enjoy a little break.



STEVE:  Well, I had a back-and-forth with Lisa, and she said, "No, we're taking the day off."  So she said, "Enjoy..."



LEO:  Yeah, we are.  You can't record on Tuesday.  Nobody will be here.  But if you wanted to, I could set it up for another day.



STEVE:  Let's take a day.



LEO:  Think of it as the all-star break.  Just relax.



STEVE:  We'll enjoy the Fourth of July, and we'll resume with lots of news in two weeks.



LEO:  Maybe nothing will happen.



STEVE:  That's right.  That's right.



LEO:  That'll happen.  Thank you, Steve Gibson, and we'll see you next time on Security Now!.



STEVE:  Okay, my friend.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#619

DATE:		July 11, 2017

TITLE:		All the Usual Suspects

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-619.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we have all the usual suspects: governments regulating their citizenry, evolving Internet standards, some brilliant new attack mitigations and some new side-channel attacks, browsers responding to negligent certificate authorities, specious tracking lawsuits, flying device jailbreaking, more IoT tomfoolery, this week's horrifying Android vulnerability, more Vault 7 CIA WikiLeaks, a great tip about controlling the Internet through DNS - and even more!  In other words, all of the usual suspects!  (And two weeks until our annual Black Hat exploit extravaganza!)



SHOW TEASE:  It's time for Security Now!. Steve Gibson is here.  Yay!  We're going to talk about all the things that happened in the last two weeks.  Yeah, we missed last week's episode because of Independence Day.  But we're back, and fortunately nothing went too horribly wrong.  In fact, lo and behold, Steve's got high praise for Microsoft.  They're going to do a really good thing.  He talks about his favorite new thingy and a SpinRite recommendation from 40 years ago.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 619, recorded Tuesday, July 11th, 2017:  All the Usual Suspects.



It's time for Security Now!, the show where we cover your security and privacy online with this fellow right here, the Explainer in Chief.



STEVE GIBSON:  Yo, Leo.



LEO:  Steven "Tiberius" Gibson.  You're wearing glasses today.



STEVE:  I am.  I've been wearing contact lenses for 45 years.



LEO:  You've been deceiving us.



STEVE:  I'll never forget, I was riding in my best high school buddy's parents' car.  He was driving.  This is before - we had our licenses, but we didn't have our own cars in high school.  And I turned to Scott, and I said, "So, should I get contacts?"  And without skipping a beat he says, "Absolutely."  And I said, "Oh, okay."



LEO:  Wow, Scott had an opinion, apparently.	



STEVE:  Yeah, he did.  And I've been wearing hard contact lenses, the original...



LEO:  Wait a minute.  You never got soft lenses?



STEVE:  Nope, RGP, Rigid Gas Permeable contact lenses for 45 years.



LEO:  Those you had to really get used to.  My first ones were glass.  And then I did get eventually the permeables, which were somewhat more comfortable.  And the good news is that you don't throw them out.  They last and last.



STEVE:  Yup.



LEO:  You know, I now wear daily - you might find daily wear more comfortable.  You don't ever clean them.  You just throw them out.



STEVE:  Yeah.  I'm just - I think I'm at the point now where, I mean, you know, I've given up on vanity.  I have no hair left.



LEO:  You look good in glasses.  You look more intelligent. 



STEVE:  Well, okay, I can use the...



LEO:  You were like a leading man before.  Now you're like a normal geek.



STEVE:  Oh.  Wait a minute.  That's what I prefer.



LEO:  A smart guy.



STEVE:  Not sure that I want to lose the leading man look, if I can hold onto that for another couple years.  Anyway, we are back after our missing week.  And the good news is the world didn't spin out of control.  And in one podcast I think we're going to be able to fit everything.  In looking over what happened, I thought, okay.  I'm just going to call this one "All the Usual Suspects" because that's what we have.



We've got governments regulating their citizenry, evolving Internet standards, some brilliant new attack mitigations and some new side channel attacks, browsers responding to negligent certificate authorities, specious tracking lawsuits, flying device jailbreaking, more IoT tomfoolery, this week's horrifying Android vulnerability, more Vault 7 CIA WikiLeaks, a great tip about controlling the Internet through DNS, and more.  So in other words, all the usual suspects.  That's pretty much the way this podcast has been rolling lately.



LEO:  Wow, wow, wow.



STEVE:  So, yeah, lots of fun and interesting news.  Oh, and we are starting our official two-week Black Hat exploit extravaganza countdown.  Black Hat is - I think it's like the 21st through the 27th at the end of this month, so a couple weeks from now.  And already we're beginning to see early reports.  We'll cover some of them this week.  No doubt we'll have some more next week.  And I would imagine in the weeks following Black Hat, lots of more details.  So in some cases, as is always the case, there's sort of some early leaks about what's going to be presented, but we lack some detail that we don't get until the actual presentation.  So it's a good time of the year for Security Now!.



LEO:  It is.  Thank goodness we've got security - now.  



STEVE:  So our Picture of the Week is just sort of a headshaker.  It's, you know, we've seen these sites, which are credible, which solicit people to put their email address into the site, and the site will then check all of the disclosed leaked username/password databases.



LEO:  Yeah, have I been pwned; right?



STEVE:  Exactly, exactly.  And the problem with that is that, I mean, it is a sketchy thing to do.  Now, arguably, people don't feel that their email address is super secret.  But it's not a big reach to imagine a dialogue like this one which was - it's a spoofed dialogue.  I hope so.  I hope this doesn't actually exist.  But it's a dialogue whose title says, "Is your credit card number in a hacker's database?"  And then actually the English is pretty good except for the very end.  So it then reads, "You can easily find out now!  All you need to do is enter its information here, and we will scan thousands" - thousands, Leo - "of hacker databases..."



LEO:  Thousands.



STEVE:  "...to see if any they have match yours."  So they almost pulled it off, but there was a little English tripping over the feet there at the end.  And then of course this has three fields, like THE three fields:  credit card number, expiration date, and zip code.  Actually, they didn't ask for the CVV, which I guess would have taken it to the final stage.



LEO:  Yeah.  What's your mother's maiden name again?



STEVE:  Yeah.  Just want to verify.



LEO:  Just want to make sure, yeah.



STEVE:  Oh, lord.  So anyway, let's just hope nobody is actually being presented with that and filling it out because it's clearly...



LEO:  The name of the window title kind of gives it away that it's a spoof.  However, I'm not going to say it out loud.



STEVE:  It does, indeed.  And I didn't.



LEO:  Yeah.  But I do feel like this probably does exist.  And somebody in the chatroom is saying, well, who would be dumb enough to fall for that?



STEVE:  No, and that's it.  That's exactly it.  Unfortunately, we have to recognize the size of the sphere of users of the Internet today.  And, I mean, Leo, you speak to them on the weekends often.



LEO:  I still do.  Well, but it's not just - and I don't want to say they're dumb.



STEVE:  No.



LEO:  It's people in a hurry, people not paying attention, people who are not wary and aren't, as we are, alert to the possibilities of fraud.



STEVE:  Who knows what HTTP stands for?



LEO:  Right, right.



STEVE:  We know what it stands for, but most people don't.



LEO:  We shouldn't discriminate against people just because they're not, you know, geeks.



STEVE:  Not us, exactly, no.  And I'm not.  And that's the point, is that we are a minority.  And there are going to be people who go, oh, I want to know if my credit card is in hacker databases.  What a nice, convenient service.



So, okay.  Top of the news is that last Thursday - I mean, we've got a lot of top of the news, but we'll start with the W3C finally decided, after much debate, to add the EME, which we've discussed in the past, the Encrypted Media Extensions, to the formal HTML5 spec.  Not everybody was happy.



LEO:  Unh-unh.



STEVE:  Or, as Techdirt's Mike Masnick wrote the next day:  "Tim Berners-Lee Sells Out His Creation."



LEO:  Yeah.  I was so surprised that it was Tim who did this, really.



STEVE:  Well, he's the director, and... 



LEO:  He was pretty outspoken that this, well, go ahead, yeah.



STEVE:  He was.  He was.  But, well, so there's two aspects of this that Mike focuses on.  And I'm with him on one of the two.  So he wrote, just to give you a sense for his position, he wrote:  "For years now, we've discussed the various problems with the push, led by the MPAA [Motion Picture Association], but with some help from Netflix, to officially add DRM to the HTML5 standard.  Now," he writes, "some will quibble even with that description, as supporters of this proposal insist that it's not actually adding DRM, but rather this 'Encrypted Media Extensions' is merely just a system by which DRM might be implemented."  Mike writes...



LEO:  Yeah, but why else would you do it?



STEVE:  Precisely.  And as he says, "But that's a bunch of semantic hogwash."



LEO:  Yeah, yeah.



STEVE:  "EME is bringing DRM directly into HTML," Mike writes, "and killing the dream of a truly open Internet.  Instead," he says, "we get a functionally broken Internet."  Which I'll argue with.  He says, "Despite widespread protests and concerns about this, the W3C boss and inventor of the web Tim Berners-Lee has signed off on the proposal.  Of course..."



LEO:  So disappointing.



STEVE:  Yeah, "given the years of criticism over this, that signoff has come with a long and detailed defense of the decision."  So there are many issues underlying this decision, but there are two key ones that I want to discuss here - first of all, whether EME is necessary at all, and whether or not the W3C should have included a special protection for security researchers because that's what's missing.  And as we've heard me say on this podcast so often, all of the evidence demonstrates the huge benefit to researchers being allowed to poke at security without fear of the DMCA being used to throttle that research.



So Mike doesn't want this at all, that is, doesn't want EME, Encrypted Media Extensions.  My feeling is it was probably inevitable.  The alternatives were no access, no web-based access to protected copyright content, or the requirement for a browser add-on plugin to implement proprietary protections.  And we're seeing...



LEO:  That's what we've been doing till now, by the way.  That's how it's worked up to now.



STEVE:  Right.  And we're trying to move away from the likes of Flash and Silverlight and so forth to more of a standards-based approach.  Or the alternative, of course, would be a separate custom application published by each provider to deliver their own content.  So to me, I mean, I don't use DRM.  I've never copy protected anything that I've ever published.  But my sense is that either, well, I think it's clear that the copyright holders are never going to give in, that is, they're not going to just say, okay, fine, and make their content available without protection.  So the alternative is to make it more cumbersome to have that content online.



So essentially what this does is it creates a standard container and a standard means of invoking what's called the CDM, which is the Content Decryption Module.  And so that remains a per-provider blob.  But the EME, the Encrypted Media Extensions, makes that a uniform deliverable which ends up providing a seamless experience to the web user who wants to access DRM-protected content through their browser.  So it expands the portal.  It probably makes it stronger in the long term.



But the biggest problem from my standpoint is that they have punted on the DMCA provisions.  That is, in fact, exactly what they wrote was:  "We recommend organizations involved in DRM and EME implementations ensure proper security and privacy protection of their users.  We also recommend that such organizations not use the anti-circumvention provisions of the Digital Millennium Copyright Act and similar laws around the world to prevent security and privacy research on the specification or on implementations.  We invite them to adopt the proposed best practices for security guidelines, or some variation, intended to protect security and privacy researchers."



But, I mean, and this was one of the big issues surrounding this, that they just, as I said, they just punted on.  They did not put that into the spec.  So it is certainly possible that somebody who in the view of the MPAA or the RIAA, whomever was the content owner, decided they didn't like somebody doing research into the security of their CDM, their Content Decryption Module, could thwart that research.



LEO:  And that's really where people feel that Tim sold out because he could have supported this very small exemption, and didn't even do that.  And I think that's the sellout.



STEVE:  Yes, yes.  And there I agree.  Again, I think what we're seeing broadly is this continual maturation of the Internet.  And to me, we are using currently separate applications, for example, on iOS devices, you know, HBO Go and HBO Now and Showtime's got its own module, and everybody has their own.  And this, moving forward, to me this feels like a means of integrating this and making the experience more seamless.  And the only way we're going to ever get that content in a web browser is if there's either a plugin or a standard.  And so, again, people don't have to view protected content through their browsers.  No one's making them do it.  But this at least provides a framework.  So I can see that.  But I really do wish that they had said, okay, we're going to do it, but we're going to protect researchers also.  And, as you said, Leo, they chose not to.



A very nice piece of new research has gone public in the most recent OpenBSD snapshot.  Version 6.1 of OpenBSD brings us KARL, K-A-R-L.  Now, we have ASLR, which is always a tongue-twister, the often-spoken-of on this podcast Address Space Layout Randomization.  And there's the kernel version of that, KASLR, Kernel Address Space Layout Randomization.  Now, exclusively for the time being because the Linux team are drooling over this, we have KARL, Kernel Address Randomized Link, a little awkward acronym, but KARL is what we have.  And it is very cool.  Under KARL, that is, from v6.1 of OpenBSD on, every freshly booted kernel will initiate a background process to randomly relink and rebuild a next version of that same kernel which, once finished, will automatically be used at the next system boot.  And I know from hearing you talk about it, Leo, when you were playing with, I think it was Linux...



LEO:  Yeah.



STEVE:  ...and recompiling the kernel from scratch.  And every time I do it I just shake my head.  Actually, for me it's FreeBSD, it's how can this possibly work?  Because the screen, I mean, it's like some crazy, I mean, they wouldn't even show it to you on a movie looking like that.  No one would believe that it's actually doing something.  It just scrolls endlessly, these ridiculously long command line streams of stuff going by.  And every one of those is a source module being compiled into an object module, a .o file.  And then, finally, they're all linked, all of those object modules are linked together to create an image, and that image is what's then stored on the non-volatile store, typically hard drive or SSD.  And then that's loaded into memory at boot.  That is the kernel.



Well, address space layout randomization, as we know, takes modules of the system.  Like in the case of Windows they're DLLs.  And so the entire kernel is a set of a few, not many, like maybe five or six depending, and device drivers also, but a few big blobs.  And with relatively low granularity, for technical reasons, they randomly place those in physical memory so that the address of the blob is not known to an attacker because we know that one of, you know, as we've tightened down the screws on exploits, we've made it more difficult to, for example, for an attacker to run code in a data buffer.  And we've done that by setting the data buffer to non-execute, the so-called NX bit, tells the chip you are not allowed to execute code.  You can treat it as data, but not as instructions.



So to get around that, attackers said, okay, they would use this so-called Return-Oriented Programming, ROP, where they would find existing little snippets of code in the kernel and jump to it in order to - it's like, okay, if you won't let me run my own code, I'll knit together existing code, which has to be marked as executable because it is, and perform my nefarious deeds that way.



So the problem is that, if any function in the kernel leaks its own location, that is, for example, say that there is a function which obtains some information which it holds in its own internal buffer, and it returns a pointer to the caller saying here's a pointer to the information you requested.  Well, that pointer will be to the buffer, which reveals the location of the buffer to the caller and so, in essence, that leakage of the position of that function.  And what that immediately does is then, for anyone who is knowledgeable about the way the OS is built, that reveals the relative position of all the functions and all the code in that one blob, in that whole module.



So what we've seen is exactly that kind of workaround for address space layout randomization, any clue that can be obtained.  And in fact, even if you don't have a clue, the granularity is typically one in 256.  So it's only a byte worth of granularity.  So if you have a scattershot attack where malware is just going to guess where something is, it'll be wrong 255 out of 256 guesses, but it'll be right one out of 256 guesses, and that may be all it needs.



So what KARL does only now on OpenBSD is that final stage of kernel building, where all of those .o, the individual small object files, which are normally independently compiled, they still exist.  But the final link phase is reperformed in the background with completely randomized order.  So the kernel that results is no longer broken into multiple pieces and relocated with low granularity.  Instead, it is a custom-built kernel with deliberately randomized offsets to every single object file that composes it.  So it's far more anti-exploit granularity, and it's available now in OpenBSD.



And the Linux team, as I said before, is jealous.  They're looking already at borrowing the idea.  So it may be that in the not too distant future this could become a very powerful attack-resistant technology, available for Linux family operating systems, which is of course the most popular open source OS on the planet.



LEO:  Yeah, and the good news is that linking step doesn't take that long.  That's not the stuff you see scrolling by mostly.  It's the compiling that takes forever.



STEVE:  Correct.



LEO:  So just relinking it, that's the only way you could even contemplate doing this.  It probably wouldn't be, especially just if you show it on the screen, it'd probably take a few seconds, I would think.



STEVE:  Well, and so what they've done is they spawn a background rebuilding thread.



LEO:  There you go, yeah.



STEVE:  And so if you happen to reboot before it's done, it just reuses the same one.  But as long as it's allowed to finish, then it stages that rebuilt one, which was rebuilt probably using completely system idle time so that it doesn't have any foreground impact at all.  It just uses that next time you reboot.  So essentially...



LEO:  That's clever.  I think that's a great idea.



STEVE:  Oh, it's brilliant, yeah.  I mean, it's like, okay, that's the way we're all going to be doing this in a few years because - if you can.  I don't know how Microsoft can do it.  There was some talk of adopting this in Windows.



LEO:  Oh, no.  You get a blob with Windows.  I don't think there's any object files on the drive.



STEVE:  Exactly.  That's the problem is Microsoft does not want you messing around with the Windows kernel.  But for the open platforms, I think this makes a lot of sense.  I'm bullish about it.  It's very clever.



So, as I said, Black Hat 2017 is approaching at the end of the month.  It was one year ago during the Black Hat 2016 conference that Apple's head of security, Ivan Krstic, announced with a great deal of fanfare that Apple would finally be joining all of the other large publishers, which at the time included Microsoft, Google, and Facebook, as well as countless smaller firms.  And in fact, Leo, down toward the end of this I've got - you'll see a www.bugcrowd.com link.  It's a really nice site showing all of the software publishers of all stripes that offer bug bounties - attention, acclamation, cash awards...



LEO:  They want money.



STEVE:  ...hall of fame and so forth.  So last year Apple said, "Us, too."  And in Ivan's Black Hat presentation they detailed, or he detailed, Apple's five broad categories of bugs and their exploits.  $200,000 would be paid in return for the disclosure of any flaws in iOS's secure boot firmware components.  $100,000 for a vulnerability allowing the extraction of confidential material protected by the secure enclave processor.  $50,000 for the execution of arbitrary code with kernel privileges, or for unauthorized access to iCloud account data on Apple servers.  And, finally, $25,000 for demonstrating access from a sandboxed process to user data residing outside of the sandbox.



So now here we are nearly one year later, approaching the subsequent Black Hat conference.  And by any objective measure, Apple's bug bounty program has been an utter failure.  Why?



LEO:  We kind of expected that.



STEVE:  We did.  In fact, we anticipated it a year ago when they announced this.  Motherboard did some research.  They promised a handful of well-known exploiters their anonymity, and they also spoke to some others who were bound by Apple NDA, the non-disclosure agreements.  They promised them anonymity in order to speak off the record.  Everyone told the same story.  Apple bugs are so rare and so difficult to find and are thus so valuable that no one who is looking for a payday from their efforts would turn an uncovered, high-quality exploit over to Apple.  Why?  Apple doesn't pay enough.



LEO:  Economics, yeah.



STEVE:  The going rate, get this, the going rate on the gray market for a multi-exploit iOS jailbreak is currently $1.5 million.  And even second-tier exploit purchasers will pay half a million U.S. dollars for similar exploits.  So it's like, eh, $200,000?  Thanks, Apple, but if I can get $1.5 million?  Yikes.



LEO:  Right.  Apple created this, really, by being so secure and such a good target; right?



STEVE:  Yeah.



LEO:  And they for a long time didn't have any bug bounties because they said we don't want to get in a bidding war.



STEVE:  Well, yeah, exactly.  And remember when we'd been covering the Pwn2Own competitions in Canada, sadly, my favorite browser, Firefox, kind of fell off the list of competition because it was regarded as too easy.  And so, no, sorry, we're not even going to pay anything for Firefox exploits.  Chrome, yes.  IE, well, Edge, yes.  Firefox, eh, don't bother.  So, I mean, it really is what we see is the case, that the harder it is to beat, the more you are rewarded if you beat it.  And so ostensibly the people who pay, you can imagine, I mean, the point is, imagine this.  A purchaser who pays a hacker $1.5 million, that's profitable for them.



LEO:  There's a reason they're paying that much.



STEVE:  Exactly.  They're able, I mean, they have purchasers who are going to make that a profitable acquisition for them.  Yow.  So that's the world we live in today.  Incredible.  And the good news is that BugCrowd.com site demonstrates that there is now an ecosystem.  Again, no one is paying this much money.  This is up, you know, top-tier, high-quality, reproducible.  This is what state actors and intelligence services, government level, who have a need to get into somebody's phone, they'll pay the money.  And of course, famously, Apple has refused to help in some cases.  For example, in the case of the FBI they refused to crack an iPhone open.  And so apparently the FBI was able to purchase that from someone for some serious coin.



LEO:  Looking at all these huge number of bug bounties, I'm guessing that people make a living doing this.



STEVE:  Yeah.  And if I didn't have anything better to do, I think it would be kind of fun to see if I could find exploits and then responsibly report them.  It's like, okay, you know, fix this.  And this is why we have to not criminalize that activity.  You could criminalize it being irresponsibly handled.  That's arguably hurting the ecosystem.  But don't criminalize responsible disclosure of problems found.  We have to keep that alive.



The wheels of browser behavior turn slowly, but they eventually do.  Two years ago, in July of 2015, two years ago, here we are in July of 2017, a significant breach of certificate authority policy was discovered by a user of WoSign's poorly designed free certificate service, who obtained a certificate for the entire GitHub root domain after only proving control over a subdomain.  We covered the story at the time.  And as we know, GitHub's architecture, by its nature, gives individuals control over their own GitHub subdomains.  That's what it's all about.



So in another implicit failure to abide by the CA rules, WoSign either did not detect or did not report and also did not revoke the misissued certificate.  So this demonstrated no effective auditing and disclosure was present or performed, and that's part of being a CA.  So this breach went undisclosed until a year later a British Mozilla developer discovered - actually it was 13 months later, and so it's 11 months ago that this had happened.  He posted this on Mozilla's security policy mailing list, saying, "In June of 2015" - referring to this original breach - "an applicant found a problem with WoSign's free certificate service, which allowed them to get a certificate for the base domain if they were able to prove control over a subdomain."



Well, the original discovery was made, and we discussed this at the time, made using ucf.edu certificates and was then retested by the person who discovered it using GitHub.  The person who discovered it responsibly reported the issue to WoSign, who revoked the reported certificate.  But that person never reported the ucf.edu certificates, only the GitHub, like the GitHub subsequent misissuance.  So a year later the originally misissued ucf.edu certificate had still not been revoked, further demonstrating WoSign's, as I put at the time, and again, "woeful" lack of responsibility.  So during subsequent investigations in collaboration with Mozilla, Google conducted a public investigation which uncovered additional cases of WoSign misissuance of certificates.



So last October Chrome 56 began gradually reducing its trust of certificates signed by WoSign and its StartCom subsidiary.  And so what they did was they blacklisted WoSign and StartCom, but then made whitelisted exclusions to permit certain sets of certificates, I mean, they didn't want to just lower the boom.  They announced this is going to be the policy.  And so they used  date-based trust to continue to trust, based on when certificates were issued, the ranges that they thought had probably not been exploited.  And but what they've done is they've said, but, you know, we have clear proof now that WoSign and StartCom are not honoring the obligations that come with essentially the ability to print money, as we've discussed in the past.  Yes, making certificates is charging for bit patterns, but with it comes responsibility.



So as of or with the upcoming release of Chrome 61, all of that conditional trust of WoSign and StartCom certs will be terminated.  Based on the Chromium development calendar, this change should be visible in the Chrome Dev channel within a few weeks, the Chrome Beta channel around the end of this month, July 2017, and then will be released into the Stable channel somewhere around the middle of September 2017.  No more WoSign, no more StartCom accepted by the majority web browser on the Internet today, which is Chrome.



And as we know, a similar march is underway for Symantec, who was found to similarly not be living up to their bit printing and charging responsibilities under the CAB guidelines.  So it always moves slowly, and I think it should.  No one wants to make a mistake here.  But there has to be enforcement of the responsibility that goes along with this.



So the most often-requested feature for Let's Encrypt is coming at the beginning of next year.  January 2018, Let's Encrypt will begin offering, for the first time ever, wildcard certificates.



LEO:  What?  Fantastic.  Because I had to spend some money, a lot of it, for wildcard.  That's great.



STEVE:  Yeah.  From its inception, Let's Encrypt has been carefully, and we've been following along, rolling forward, trying hard, desperately hard not to make any big mistakes with something as critical as fully automated certificate issuance.  They've held off on the much-requested feature of issuing a single certificate for all of a domain's subdomains, for example, *.example.com, or like *.github.com, where that certificate would protect any machines within that domain or any domains within that domain until they felt sufficiently safe, that is, until Let's Encrypt felt sufficiently safe and secure in offering wildcards.



So in January, this coming January, rather than requiring, as they do now and have, every subdomain to individually approve and obtain its own certificate, the single parent domain will be empowered to obtain a single certificate for all possible subdomains.  Since the only thing Let's Encrypt automation is verifying is domain control, these certificates will be and can only be the least rigorous form of DV (Domain Validation) certs.  But that's all any site needs, if it just wants to protect, encrypt, and authenticate its traffic.  So for the majority of sites that didn't need to bother, like may have wanted to have HTTPS connections, but didn't want to get on the paid-for bandwagon, and may have been using free services like WoSign and StartCom were offering, well, those won't work soon at all under Chrome.



Let's Encrypt is an increasingly useful option.  I still want Extended Validation.  There are people who want organization-level, OV (Organization Validation) certs.  For that, you still need to use a certificate authority like my favorite, DigiCert.  But there are applications where you just want to encrypt your traffic.  I mean, you can't securely log a user on, as we know, without HTTPS because, if you 'log on,' unquote, over HTTP, that state cookie, the cookie which is then associated with your logged-on-ness, is exploitable.  It's in the clear.  It's back in the Firesheep days that's what was going on.  So you can't have any kind of security, and the notion of being logged onto something unless you're able to create an encrypted tunnel between the user and the web server.  And that requires TLS, and that needs a certificate.  So bravo for this.  That's a nice move forward.



And I have to grumble again about headlines that have too much hook in them and not enough truth.  Because the headlines read:  "Researchers Crack 1024-bit RSA Encryption in GnuPG Crypto Library."  Except, no, they didn't.  The term "cracking encryption" means something, and that would be huge news if it were true.  But it's not.  What "cracking encryption" does not mean is arranging to obtain the private key by exploiting an algorithmic implementation flaw in a particular library.  Yes, that's not a good thing, but neither is it a crack of the encryption.  What did happen is that - and props to these guys because they basically pointed out and demonstrated a long-known problem that crypto library authors had been ignoring.  They can ignore it no longer.



So this international group of eight crypto researchers used a flawed implementation of RSA to powerfully demonstrate that the underlying Libgcrypt crypto library, which is used by GPG, needs to be fixed.  This implementation flaw was used to leak a private key - which is not a phrase you ever want to hear, you don't want your private key leaked - through a side channel.  It used the Level 3 cache.  And we talked about caching not too long ago, how L1 and L2 are in the chip or, like, in the core.  And Level 3 is the larger cache just before main memory.  So they used cache attack timing in a side-channel attack, which requires an attacker to run carefully designed and instrumented forensic software on the same machine where the private RSA key is being used, while it's being used.



So again, this is not a crack of 1024-bit RSA.  This is a known implementation problem was vividly demonstrated.  In the paper's abstract they wrote - and Bernstein was the lead author in this, Dan Bernstein, whom we talk about often, I know.  He's the creator of the curve, as I've said, that was the inspiration behind SQRL.  They write:  "It is well known" - and I'm not going to go into the details, but this gives you a better sense for it - "well known that constant-time implementations of modular exponentiation cannot use sliding windows."  Well, Leo, everybody knows that.



LEO:  I thought that was common knowledge.



STEVE:  And you have to be careful, as we know, which way you open the window because, if you open the window the wrong way, that's bad.



LEO:  And sliding doors, that's...



STEVE:  Oh, my god.  Yes, exactly.  They wrote:  "However, software libraries such as Libgcrypt" - shame on them - "used by GnuPG, continue nevertheless to use sliding windows.  It is widely believed, even if the complete pattern of squarings and multiplications is observed through a side-channel attack, the number of exponent bits leaked is not sufficient to carry out a full key-recovery attack against RSA.  Specifically, 4-bit sliding windows leak only 40% of the bits, and 5-bit sliding windows leak only 33% of the bits."  Which, you know, that would seem like a problem.



"In this paper," they write, "we demonstrate a complete break of RSA-1024 as implemented in Libgcrypt.  Our attack makes essential use of the fact that Libgcrypt uses the left-to-right method for computing the sliding window expansion.  We show for the first time that the direction of the encoding matters.  The pattern of squarings and multiplications in left-to-right sliding windows leaks significantly more information about the exponent than right-to-left sliding windows.  We show how to extend the Heninger-Shacham algorithm for partial key recovery to make use of this information and obtain a very efficient full key recovery for RSA-1024."  And then they note:  "For RSA-2048, our attack is efficient for 13% of possible keys."



So there you go, Leo.  You've got to be very careful of which way you slide the window, and you don't want to leave it open by mistake because you could lose some bits.  Anyway, so don't worry, RSA-1024 is intact.  And I am sure that this will represent a lesson learned by the Libgcrypt folks, who will probably fix their code, and we'll get an update...



LEO:  Close their sliding windows.



STEVE:  They're going to slide the window in the proper direction because they weren't.  And leave it to Dan to point that out.



LEO:  By the way, so I've been using 4096-bit keys.  When they say 1024, are they talking about key size?



STEVE:  Yes, the length of the private key [crosstalk].



LEO:  So if you used a 2048 or 4096, this wouldn't impact you no matter what lib you used.



STEVE:  Well, 13% of 2048-bit keys could be efficiently recovered.



LEO:  Okay.



STEVE:  And they didn't talk about 4096.  My guess is that the percentage of leakage would probably be fixed.  But you've got so many excess bits in there, Leo, that you could lose half of them, and nobody would - it wouldn't help anybody enough.



LEO:  I'm realizing that might have been a mistake because I'd have to use - I can only use the YubiKey 4 with that size of a PGP key.  They don't support the 4096.



STEVE:  Yes, luckily we're not at the 8Tb private key point at this point.  So I thought this was an interesting little piece.  The good news is a judge has his head on right.  There was a lawsuit which a bunch of people have been continually trying to bring against Facebook for five years.  The Guardian reported last Monday that a judge had dismissed a lawsuit, this time with prejudice, meaning that a subset of the claims cannot be brought up again, which was accusing Facebook of tracking users' web browsing activity even after they logged out of Facebook.  The plaintiffs in this suit alleged that Facebook was using the Like buttons found on other websites to track which sites they visited - duh - allowing Facebook to assemble detailed records of their browsing.  Yes.  That's what they're for.  That's what they do.  The plaintiffs argued that this violated federal and state privacy and wiretapping laws.



However, U.S. District Judge Edward Davila in San Jose, California, dismissed the case because he said that the plaintiffs failed to show they had a reasonable expectation of privacy or that they suffered any realistic economic harm or loss.  In other words, the plaintiffs were annoyed, but annoyance is not sufficient grounds for a lawsuit.  Davila said the plaintiffs could have taken steps to keep their browsing histories private, for example, using the Digital Advertising Alliance's opt-out tool or using incognito mode, which is now available in all major browsers.  And they failed to show that Facebook illegally "intercepted," which is apparently what they were alleging, or eavesdropped on any communications.



He wrote:  "Facebook's intrusion could have easily been blocked, but plaintiffs chose not to do so."  Davila also dismissed an earlier version of the five-year-old case back in October of 2015.  He wrote:  "The fact that a user's web browser automatically sends the same information to both parties" - meaning the site you're visiting and the site whose assets are also hosted on that page - "does not establish that one party intercepted the user's communication with the other."  He said:  "The plaintiffs cannot bring privacy and wiretapping claims again, but they could pursue a breach of contract claim if they chose."



So standing back from this, it sounds as thought the plaintiffs are unhappy that logging out of Facebook does not stop Facebook's tracking of their activities.  But I think they need a lesson in how the Internet works.  As we know, websites give a browser a unique token, a cookie, which is subsequently returned anytime that browser makes any request for any asset from that - in this case Facebook - domain.  But that's entirely separate from logging in.  Even if you never were to log into Facebook, just by touching the Facebook domain, which your browser would do anytime you visited any page on the Internet which contained a Like button, Facebook would still give your browser a cookie, which could then be used to begin assembling an anonymous profile of essentially you, anonymously, as your browser, because it gets uniquely tagged, even if they have no idea who you are.



Just going anywhere and picking up that cookie then allows that to begin.  You don't even have ever needed to visit Facebook ever, since any contact with any page makes that happen.  And then of course later, if you did visit Facebook and logged in and created an account and associated yourself with that Facebook session, then all of the pieces suddenly click together and fit together.  But all logging in is doing is telling Facebook that that cookie which has been wandering around and appearing every time your browser came into contact with any page having a Facebook Like button, all logging in is doing is setting a flag saying this user is logged in.  And when you log out, that flag is reset.  And the resetting of that flag just means don't let this person do anything on Facebook until, like, only give them the login page until they do log in, and then they have freedom to do whatever they want to on their page.  So anyway, I'm happy to see that this didn't go any further, and these people just need to know how the Internet works.



In our continuing "when governments react" watch, we have China upping the ante on VPNs.  They have instructed the three major carriers of content in China, which is China Mobile, China Unicom, and China Telecom, to formally block all use of VPNs by February of next year, February 2018.  And so this is sort of a - VPNs have been pushed against by the Chinese government to the point that already the Android Store has lost pretty much all of the Chinese-based VPN services.  They're just no longer available there.



And as we know, we've talked about the so-called Great Firewall a number of times in the past.  China has been blocking content that they don't want their citizenry to have access to, including Facebook, Twitter, YouTube, and Instagram.  News sources like New York Times and Wall Street Journal are blocked, as well as Google Scholar.  And as we covered on the podcast at the time seven years ago, Google's own reluctance to have its results censored led it to quit the country, and then the Chinese government subsequently banned most of Google's services.



So a way around all of this border filtering with the Great Firewall has been the use of an encrypted tunnel that would mask the accesses that somebody using the VPN was pursuing.  And so that had been an increasingly popular way, as people inside China wanted to get access to the whole Internet, to an unfiltered view of the Internet.  China has said, okay, we're going to make that increasingly difficult.  Aside from the policy angle, our question is, is it even possible?  Once upon a time, as we know, old-school VPNs used well-known ports.  They used specific ports, and that's how some corporations and public WiFi setups and so forth would deliberately block the use of VPNs when they wanted to prohibit it.



Modern VPNs, though, are able to tunnel over TCP or UDP and have complete flexibility of which ports they use.  Which makes filtering them much more difficult.  And, for example, it would be possible for a website to offer access to the rest of the web if it wanted to.  So, for example, somebody in China could access a non-blocked website into which they put the URL they want to visit, and that site presents them with the page content from the URL they provided.  You could sort of think of it, anyone who's ever used like the web archive has seen something like that, where you go to the web archive site, and you give it the URL you want to look up in the past, and it shows you the page that was archived.



Well, it could as easily be done with live content from the web on the fly.  Essentially, you're sort of using that intermediate website as a deliberate man in the middle to perform the accesses for you to content that would otherwise be filtered.  And, yes, now then you end up with a cat-and-mouse race where it's necessary to go stomping out all of the sites that are offering a service like that.  I just, you know, it just seems infeasible.  So I think what this means is that the convenience and ubiquity of using easier-to-use VPNs will probably get throttled.  But it just doesn't seem that there's a practical means for really blocking somebody who's determined to access external content to do so.



On the other hand, the convenience of having a Twitter app that just watches the stream, that you'll lose because you would really need a traditional VPN in order to do that.  But again, we're watching governments and state actors deciding that they want to profile their population's use of the Internet to suit their own needs, increasingly, as the Internet becomes more of a thing.  And believe it or not, drone jailbreaking is now a thing.



LEO:  Finally.



STEVE:  DJI, the maker, as we know, of what is arguably the best line of commercial quad copters, especially useful for professional aerial photography used in weddings and bike races and especially watching Apple assemble their solar doughnut, is locking down their drones against a growing army of DIY hackers who are arguing for freedom of flight.  This controversial DJI firmware has for some time been enforcing no-fly zones and also altitude and air speed controls, which upset drone owners who chafe at such limitations.  And online boards are full of reports where the restrictions appear to be applied arbitrarily, and in some cases improperly, apparently restricting flight for no good valid reason.



So in response, hacking DJI's drone firmware has been on the rise.  And DJI has started to fight back by pushing firmware updates to break the jailbreaks to Internet-connected drones, and they've been proactively removing previously available legacy versions of their own firmware from their servers to prevent people from getting access to it and retrofitting older firmware which can then be jailbroken and used.  So what of course has been spawned is an underground of firmware archives and so-called "chop shops" which are retrofitting full flight freedom firmware.



And the legal ground, sort of the legal position is a little bit unclear in the U.S. because, once again, the Digital Millennium Copyright Act is present.  The Librarian of Congress, which administers specific exemptions to the DMCA, has given so far wide latitude to tinkerers who seek to break through software locks for the sake of repair or restoring factory defaults.  So, for example, it's currently legal to hack into trailers, cars - I'm sorry, tractors, cars, and cell phones, but not legal to jailbreak videogame consoles.  But there's currently no specific exemption for drones, although I guess DJI would have to bring a lawsuit against its consumers in order to test this and establish some precedent.



But so now we have a - I doubt that, without really upping the stakes, DJI is going to be successful.  We've seen, for example, what Android mobile devices and iOS mobile devices have had to go through in order to sufficiently strengthen themselves to the point that it's becoming, you know, it's worth $1.5 million to get a jailbroken iPhone exploit.  So anyway, I just thought it was interesting that there's now another jailbreaking front in the case of people wanting drone freedom.



And, okay, in this week's horrific Android vulnerability, we have Broadpwn, which we don't know everything about, and we won't for two weeks, until Black Hat.  But we know a lot.  First of all, it's very worrisome, affecting, I mean, I don't even know the number, at least millions of Android and probably iOS devices.  Unpatched Broadcom WiFi chips, okay, so not cellular, the WiFi chip, which are used in both Android and iOS devices, are vulnerable to a bug that allows an attacker to execute code on those devices without any interaction needed from the user and, even worse, simply by being within radio range of any malicious WiFi access point.  You don't even have to join it.  You don't have to do anything.  Your mobile device just has to receive its signal.



A security researcher named Nitay Artenstein or "steen," I never know, no one knows, discovered and nicknamed the Broadcom firmware flaw Broadpwn, P-W-N.  It's got a CVE number, 2017-9417.  And he will be delivering the full presentation about Broadpwn at this year's Black Hat USA security conference at the end of the month.  He responsibly disclosed the bug to Google, who already included it in the Android fix which was available in last week's July 5th Android security update.  So for devices that are receiving Android security updates, you got fixed last week.



Although little public information is available yet, we know, and Artenstein has said, that Broadpwn affects millions of Android and iOS devices that use Broadcom's WiFi chips.  The flaw is present in the firmware for a huge and old line of Broadcom.  It's the BCM4300 family, or 43xx family of WiFi chips, which are included in an extraordinary range of mobile devices, including vendors Google with the Nexus line, Samsung, HTC, and LG.



And then, after being made curious about this, another Android security expert reverse-engineered last week's July security patch from Google to dig out more details about Broadpwn.  He determined that the bug appears to be a heap overflow in the Broadcom firmware.  The researcher said that the exploitation occurs when the user's device receives a WME, which is a quality-of-service information element, with a malformed length from any network.  And as I noted, exploitation does not require user interaction.  The victim needs only to enter into the WiFi range of an attacker's signal.  Artenstein has confirmed that even connecting to a malicious network is not necessary.  Not surprisingly, Google's security bulletin last week rated Broadpwn as critical.



This BCM43xx line appears to date back at least 12 years, to 2005.  It's unclear whether this flaw was present back then.  But as we've discussed on this podcast previously, this baseband OS is a concern because it doesn't get much attention.  We're always talking about iOS or Android, the visible OSes that we see and that our apps interact with.  But there is this underlay OS that runs the cellular and the WiFi that performs the communication.  And it's firmware-based, too.  And it tends not to change nearly as often as the OSes that run on top of it because it's kind of part of the lower level hardware offering.  You know, it comes from Broadcom with the chip.



So it's very clear that, if this flaw were to be weaponized by any global state actor and subject to any exploitation limitations, that is to say, we don't know for sure what exactly you can do with this.  So you can run code, what, on the Broadcom chip?  Well, what does that mean?  We don't really know yet.  But that means that virtually any unpatched Android device could be compromised simply by getting a malicious portable WiFi access point within WiFi range of any targeted Android user.



We keep covering these serious Android problems.  In older and unpatched devices, they are rapidly piling up, that is, these problems are.  And I would argue they've already far exceeded critical mass.  Although targeted exploitation is unlikely to affect many of us, opportunistic exploitation on college campuses and in public settings such as airports, hotels, or <gulp> the upcoming Black Hat conference could easily affect huge numbers of users, virtually anybody with an Android device with their WiFi radio turned on.



So the conclusion to be drawn from this as we step back a bit is clear.  Android devices that are not continually being rapidly and responsibly patched cannot be used securely.  And unfortunately, depending upon how far back this goes, there is a massive population of devices that are not being patched, that have been abandoned, but are still in use.  And if this is exploitable, that is, if the exploitation of the baseband OS firmware can be turned into something, it's a goldmine for the weaponization of this against old devices that are no longer being patched.  We have, as I said, no information at this point on the status of this for iOS devices.  We'll see what Artenstein says in two weeks during his Black Hat talk.



LEO:  And learn how he pronounces his name.



STEVE:  Yes.  I hope he says.



LEO:  Very exciting.



STEVE:  So our listeners know that I have a somewhat contentious relationship with Microsoft.  I'm annoyed when they do things, you know.  I mean, I wrote Never10, which I looked yesterday:  2,653,204 downloads.  And it's about 2,000 more a day.



LEO:  That's awesome.  What, still?



STEVE:  Yeah.



LEO:  Okay.



STEVE:  Yeah.  But I do want to give them props when they do something that I  think is really great.  Doesn't happen that often.  Yes, I know.  But they've done something that I think is fabulous:  a new form of very clever selective whitelisting will be coming to Windows 10.  It's called Controlled Folder Access.  It will be debuting in September's Fall Creators Update.  And Win10 testers now have access to a preview of the changes, which will include this new Controlled Folder Access, or CFA, feature.  CFA is designed - and this is what I think is just Microsoft nailed this one - to only allow specific apps to access and read and write to specific folders.  If enabled, the default list prevents unknown and unauthorized apps - read "cryptomalware" - from accessing the desktop, pictures, movies, and documents folders.



LEO:  It's funny because that's how iOS works; right?



STEVE:  Correct.



LEO:  Yeah.



STEVE:  But, you know, Windows has been just a free-for-all.  So in the new version that is coming, CFA can be enabled.  The dialogue is the Windows Defender Security Center.  There's a Protected Folders option and an "Allow an App through Controlled Folder Access."  So I think this is a brilliant tradeoff.  We've talked before often about how blacklisting is prone to failure.  I mean, because you're saying, oh, don't allow this, don't allow that.  So something just pretends to be something else.  So the only way to protect yourself is a Deny All and then Allow Selected policy.



What's not clear is how this will work interactively, that is, whether or not, first of all, it'll be enabled by default, whether it might start off being disabled and then be enabled in the future once Microsoft gains more experience with it, what programs will be permitted, et cetera.  But as we've been saying here again, whitelisting, a default block and selective permit is the only way to be truly secure.  And of course you have to also check that an application is actually what it says it is so that you just didn't have malware pretending to be WinWord and then allowing it access to your documents folder.



But bravo.  There is an impact on the user because they may be a little confused.  But I think you could work around that.  If they downloaded some new program that wanted access to their documents, then you'd have to permit, manually permit that program to have access.  But what that would mean is that malware could not surreptitiously slip into your system and then surprise you with a big scary red screen telling you what percentage of a bitcoin you have to pay in order to get your stuff back.



So bravo.  This is the kind of movement we need in order to tighten things down.  And I think what's clever about it is that it's constrained.  That is, it would break everything if Microsoft tried to establish wholesale whitelisting policy across the entire operating system.  Instead, they're just saying these are the folders where most of users' important stuff is.  And the universe of applications that need access to that subset of folders is reasonably small.  So we can constrain the access to that subset of the whole OS.  And the universe of apps that need access to it, we can constrain that, too.  So just, yay, props to Microsoft on that one.



In our This Week in Bizarre IoT Stories, we've had the smart water meter that contradicted somebody's story about the use of the hot tub in the middle of the night.  Now we have Engadget reporting a somewhat bizarre piece of IoT news, that a smart home voice response device - and by the way, it was originally misreported as being a Google Home device, but we still don't know what make and model it was - was responsible for ending a violent dispute by calling the police.  "Police in New Mexico reported that a smart home device intervened in a domestic violence incident by calling 911."



LEO:  Oh, come on.



STEVE:  I'm not kidding.  "When Eduardo Barros asked, apparently in a loud voice, 'Did you call the sheriffs?' as he threatened his girlfriend with a gun during..."



LEO:  Oh, "call the sheriffs."  I get it.



STEVE:  "...during a fight, the device interpreted it as a request to call emergency services.  The 911 call responders overheard the altercation and called both negotiators and a SWAT team, who arrested Barros over assault, battery, and firearms charges after a stand-off.  Barros's girlfriend was hurt in the altercation, although police contend that the situation would almost certainly have been much worse.  County Sheriff Manuel Gonzales believes that the command 'possibly helped save a life,' including that of the girlfriend's daughter, who was thankfully unharmed."



LEO:  We don't know what kind of device, though.



STEVE:  We don't.  We know it's not a Google Home device, so that...



LEO:  And Amazon has apparently said it's not them, either.



STEVE:  Interesting.



LEO:  I don't know what it is.



STEVE:  So I know from firsthand experience, and I'm sure you do, Leo, because you've got one in every room now, and even in your closet...



LEO:  Everywhere.



STEVE:  ...that unintended sounds, especially loud sounds, can cause the command discriminators in these devices to misfire.  For some time I had an Amazon Echo in my living room, listening with me to my whole room theater.  And from time to time, when nothing to my ear sounded like its wakeup trigger word, the device's blue ring would suddenly illuminate, scan around a bit, and then it would go back to sleep.  And I used to kind of look at it going, okay.  So it's entirely believable that this could happen upon hearing especially a shouted, you know, screaming voice, "Did you call the sheriffs?"  Especially if the girlfriend's name was you know who.



LEO:  Oh.



STEVE:  So, yeah.



LEO:  If your name is A-L-E-X-A, you're safe.  That's funny.



STEVE:  Also in two weeks at this increasingly anticipated Black Hat conference, we will have a full presentation of Skype & Type.  A group of researchers have developed and proven yet another side-channel attack.  Forbes reported in an advanced demonstration of the Black Hat presentation that, after sufficient training, which can happen in the background during and over a Skype call, a randomly chosen password was determined within seconds using only information gleaned over the connection from the unique and distinctive sounds and timing of someone typing on a keyboard.



So we've talked before about the keyboard as a mechanical device, vibrations from typing being picked up by a cell phone lying near the keyboard.  In this case the actual sound, especially my keyboard because it's an old clanky one, I imagine, the sound of the keyboard, even through Skype's compression and decompression of the audio, was sufficient to allow a random character password to be determined within seconds.  Full details in two weeks at Black Hat.



WikiLeaks and the Vault 7 document leakages have last week informed us of another CIA, alleged CIA project known as OutlawCountry.  It is a very powerful, but apparently also very targeted, remote monitoring kernel exploit affecting some Linux-based machines.  It reportedly allows CIA hackers to redirect all outbound network traffic on a targeted computer to CIA-controlled computer systems for exfiltration and infiltration of data.  It's a Linux kernel module loaded via shell access to the targeted system.  So they need to have some way, some other means of getting it in.  But once there it creates a hidden Netfilter table under an obscure name on a targeted machine.



However, it's not without limitations.  The leaked documents indicate that OutlawCountry v1.0 contains one kernel module for the 64-bit CentOS/Red Hat v6.x version of Linux, stating that "This module will only work with default kernels," suggesting that the tool may have been developed for specific targeted use, where the intended target was known beforehand.  But, yes, Linux systems, too, not just crazy SMBv1 exploits.



Somebody tweeted to me an interesting-looking Kickstarter project.  And it started off looking like yet another maybe overhyped custom router.  And we've talked about those before.  You know, the CUJO comes to mind because they're alleging the ability to do, that is, in CUJO, things they can't do in a post-HTTP world, in a TLS world where all traffic is encrypted.  First of all, this thing that was called Firewalla, F-I-R-E-W-A-L-L-A, I looked at that.  I didn't dig in too deep, but I looked carefully at what they were claiming.  And they did not appear to be overselling what it could do.  That's the good news.



But what I was reminded of was something I've been meaning to talk about before.  And two weeks ago, when I went off on my rant about Cisco overselling their "seeing into encrypted communications," which really looked like it reduced to using DNS metadata, I forgot to mention, and this reminded me, of something that some very good friends of mine and of the show have done.  And these are the Nerds On Site folks, who were one of our very early podcast sponsors.



LEO:  Dave's in the chatroom right now, I think.



STEVE:  Well, David Redekop is the person whose name is on the blog.  What these guys created is something called DNSthingy. 



LEO:  Good name.



STEVE:  It's a great name, DNSthingy.  DNSthingy.com is the site.  And they nailed it.  It is an affordable subscription service, less expensive than the other things that we've seen.  And what they've done is they've fully thought through the power of DNS.  That is, even in a land of encrypted communications, devices in your Intranet have to get the IP that they're connecting to.  As we know, by default, DNS is not encrypted.  So this allows them to exert very granular per-device controls.



So, for example, the kids' iPads or computers could have time restrictions put on them so that they just stop being useful at bedtime.  They can have family-safe content filtering put on them or not.  The whole Internet can appear to be shut down or not.  They have installable firmware for ASUS routers and a whole list of them that they support, or routers based on ClearOS or, my favorite, pfSense.  So as we know, you can take a very inexpensive little standard router PC, load it up with pfSense, add DNSthingy to it.  They've got a beautiful little dashboard for configuring it.



Anyway, we know who these guys are.  They're 100% aboveboard.  They're the real deal.  Great people.  And so I would commend our listeners to take a look, just check out DNSthingy.com.  One quick way to get a sense for it is to go to the blog and scroll back down through David's postings because it's active.  It's being maintained.  They're all fired up about it.  And I just think, I mean, it's the way to deal with a post-encryption world where you still want to apply per-device controls.



One thing you could do, for example, is constrain what your IoT devices can do.  You could say to your camera, you are only allowed, camera, to look up the IP of the service that supports you, and nothing else.  So if malware got in there, it couldn't go checking out command-and-control servers anywhere else.  Its DNS queries would get blocked.  Oh, and it does ad blocking, too.  So you can say I want specific devices to be ad blocked or the whole household to have ad blocking.



LEO:  Is it hardware?



STEVE:  No, it is software...



LEO:  It's an update to your router.



STEVE:  Right, exactly.



LEO:  Some routers do this, of course.  In fact, the Eero just added this feature to their routers.  So that's cool.  So if your router doesn't do it - although even routers with access control lists, not a lot of them have ad blocking.  That seems like a pretty - I don't think the Eero does ad blocking.



STEVE:  Well, and again, like they sat down and, as we know, they're tech-y.  And they said, okay, what's everything we can do?  So there's, like, time-based restrictions, per-device restrictions, and way more.  So it's very capable.  For a family router where Mom and Dad want to manage what the kids can do, you want content blocking.  You want time restrictions.  And, for example, you could also prevent somebody - I saw this at some point on the page or on the site.  You can prevent someone from changing their DNS to something else in order, for example - the example they used was forcing OpenDNS to be used and prevent someone from changing it to something else in order to get around OpenDNS's restriction.  So I've been meaning to talk about it, DNSthingy.com.  These guys did a great job.



LEO:  That looks cool.  They do relocation, too.  So you can access geographically restricted content.  That's kind of neat.



STEVE:  Yeah.



LEO:  Yeah, it's pretty sophisticated.  Wow.  Nice job, Dave.



STEVE:  Next week we're going to talk about something that Bruce Schneier referred to as, he said in his blog posting, "This is nice work."  And I'm just going to share a couple juicy bits because we'll cover it in detail next week.  But from the abstract they wrote:  "We present the password reset MITM attack and show how it can be used to take over user accounts.  The attacker initiates a password reset process with a website and forwards every challenge to the victim, who either wishes to register in the attacking site or to access a particular resource on it.  The attack has several variants, including exploitation of a password reset process that relies on the victim's mobile phone either using SMS or phone call."



They write:  "We evaluated [what they called] the PRMITM attack on Google and Facebook users in several experiments and found that their password reset process is vulnerable to this password reset man-in-the-middle attack.  Since millions of accounts are currently vulnerable to the PRMITM attack, we also," they write, "present a list of recommendations for implementing and auditing the password reset process."  We'll get into it in detail next week.  There's a lot there.  And as I have said recently, man-in-the-middle attacks and website spoofing, that remains the real challenge going forward to the Internet security because it's just so easy to get tricked.



There is a fabulous YouTube video for our little Miscellany here.  I gave it a bit.ly shortcut because it was so good:  bit.ly/sha-256.  So again, bit.ly/sha-256.  This guy does a brilliant job of helping us to visualize just what 2^256 means.  He only made one mistake, early in the video, because he said that 2^256 meant that you had to guess an average of 2^256 guesses.  Well, we know that's wrong.  If you've made 2^256 unique guesses, that would give you a 100% chance of guessing it because that's all there are.  So what he meant was 2^255 guesses would give you the average number required.



LEO:  Half the set.



STEVE:  That is half that set, exactly.  Other than that tiny mistake, he did a beautiful job.  And so if anyone wants to get a - it's fun.  It's only a little over five minutes long.  But really it characterizes and puts into beautiful concrete context what 2^256 actually means.  And that's important because that's how many bitcoin addresses there are, that's how many SQRL identities there are, and so on.  So really nice piece of work:  bit.ly/sha-256.



Also I wanted to note, and I've got the links here in the show notes, there are two Security Now! T-shirts that someone created.  One is a very nice TNO Trust No One T-shirt, and the other is a T-shirt based on the "S" in IoT joke, you know, the "S" in IoT stands for security.  Anyway, they are at Teespring.com, T-E-E-S-P-R-I-N-G dotcom.



I did have a little quick note about SpinRite.  Bill Taroli apparently was having a problem.  Looks like he's a Linux user using the Btrfs file system.  And so his first tweet said:  "Sigh.  One offline uncorrectable read error on my SSD."  So as we know, SSDs fail, too.  So he tweeted:  "Time for a Level 2 scan with," and then he said "@GibsonResearch @SpinRite," and then did a "#securitynow."



Then a couple tweets later, it was some time later, he tweeted again, replying to the previous tweet.  And he said:  "@SpinRite awesome!  SMART says drive is 63% useful life, but it feels so much faster now.  Full Btrfs defrag under 30% iowait now."  So Bill is obviously scrutinizing the operation of his system closely.  He's looking at the iowait time that his Linux Btrfs file system is experiencing when he's defragging it.  And after running SpinRite on his SSD, he saw the iowait time drop dramatically because just running a Level 2 scan with SpinRite over the SSD cleaned up the SSD and sped it up significantly.



And then I saw one more thing that really brought kind of almost a tear to my eye, Leo, and that was Chris Erickson tweeted, and I have the link in the show notes, but then there's also two pictures on the next page:  BYTE magazine, November 1988.  And the first picture is the cover of BYTE magazine showing Steve Jobs's new machine for the '90s, the NeXT, the NeXT computer.  And then, under reviews, next to the big word "BYTE," it says - it's reviewing a number of things, and in there is SpinRite.  And it was sort of, for me, a blast from the past.  But as you know, Leo, because we were active at the time, BYTE magazine, it was the magazine of record back in the day, you know, in the late '80s.  I mean, it was the bible.



And so Richard Grehan was the BYTE senior technical editor at large.  And he finished his review - this is just the last two paragraphs.  He said, he titled it:  "A Must-Use Utility."  "All I can say," he wrote, "about the Gibson Research people is that they did their homework.  The user interface is well thought out and easy to use; all interaction is via an easy-to-navigate windows system.  The package comes with" - now, he said a hard disk, but he meant a floppy disk - "and a 40-page user's manual that is more interesting for its historical content, how the authors of the package" - and we know that the author is singular - "made all their discoveries about hard disks, than any other information.  The program is so well put together," he wrote, "I found I seldom referred to the manual anyway."



He said:  "SpinRite is no 14-disk grand slam C compiler, but you shouldn't underestimate its usefulness.  If you have a PC with a hard disk drive that you spend most of your day relating to, and your heart sinks every time you see the drive's bad sector list, SpinRite is what the word 'must' was invented for."  And, oh, my god.  Can you imagine?  I mean, I just - I memorized that review.



LEO:  Does he say anything about how he's waiting for v6, or no?



STEVE:  No.  That was v1.0, I think, back then.



LEO:  Yeah, sure, yeah.



STEVE:  Yeah, wow.



LEO:  You must be one of the long, if not the longest running program still in existence.



STEVE:  Thirty-nine years, my friend.



LEO:  That's got to be a record.



STEVE:  It will be 40 years in 2018.



LEO:  Well, we're throwing a party for that one.



STEVE:  Yeah, wow.



LEO:  Wow.



STEVE:  Two quickie "closing the loops" from our listeners.  Ian tweeted from @ianc.  He said:  "@SGgrc Listener for the past year.  You often talk of prime numbers.  Other than basic knowledge of prime, why are they so powerful for crypto?"  And I'll answer quickly, just say that many types of cryptography are based on a problem known sort of generically as a trapdoor function, the idea being that it's something that's easy to do, but difficult to undo.  And the only reason that prime numbers ever come up is that the most venerable public key crypto, that is, asymmetric crypto, where you are able to openly publish one of the keys to use for verifying a signature or verifying a cryptography, but keep the other one secret, is that it is trivial to multiply two prime numbers.  And we have never yet found a fast way of demultiplying them, that is, of factoring the product of two primes.



That's the only thing, the only principle, the only reason primes are a deal is that it's easy to multiply, but the prime factorization problem has proved to be intractable for decades.  And it's the reason we were talking two weeks ago about the whole "what does quantum mean" and the horror of an 8Tb public key.  Essentially, a private key is one half of the public key.  And so you take the private key as one of the primes and another prime.  You multiply them.  And so essentially, because you can't pull it apart, you're hiding the private key in plain sight.  You're saying, here you go.  Here's your public key.  And what's cool about the RSA algorithm is it's able to use the unknown primeness of half of that public key to get some work done, but no one is able - it's only by knowing the half that is the private prime that you're able to take advantage of that.  And that part you keep secret.



So that's, I mean, it's elegant in its simplicity, and it's so cool that it has remained a secret for so long.  At some point we will talk in detail about elliptic curve technology, which is the newer system.  It's the one that SQRL uses and that is becoming increasingly popular.  The advantage of it is that, in order for prime factorization to be slow enough, the primes have to be so big that the resulting product of the two primes, the public key, is huge.  And so the elliptic curve problem is a much harder problem to solve.  Therefore, you get equivalent strength with a much smaller public key.  And that's one of the - it just makes it much more convenient.



And lastly, someone who calls himself Acid Trucks on Twitter said:  "@SGgrc What are your thoughts on using router features like parental controls to shield the Internet from IoT devices?"  And I, coincidentally, already answered that when I was talking about DNSthingy and some of the clever ways that you could use blocking DNS or parental controls to keep somebody, especially an IoT device, from having unfettered access to the Internet.  You just say no.  Any query coming from a given MAC address only has permission to access the IP at a given domain.  Otherwise, deny it. 



LEO:  That's interesting.



STEVE:  Yeah.



LEO:  So DNSthingy would do that.



STEVE:  Yeah.



LEO:  That's a simple way to do it.  Just say, hey, this device, unh-unh-unh.



STEVE:  Exactly.



LEO:  Nice.



STEVE:  You can only access the site that gives you the service and none other.



LEO:  Yeah, yeah.  Love it.  Steve, one more, you know, I'm going to make one more announcement.  Now, tomorrow is Internet Freedom Day, another big protest on the Internet to preserve Net Neutrality.  And you can find out more at BattlefortheNet.com.  In fact, I encourage you, go there.



STEVE:  Yay, yes.



LEO:  And send a letter to the FCC.  Call your Congress Critter.  A lot of companies participating in this, including, ironically, AT&T.  Well, we don't buy that, AT&T.  We know you're not a part of the fun.  And a lot of websites you'll know will either go dark or in some way indicate it.  Ours will with banners.  And as we did with the SOPA protest some years ago, if you tune in tomorrow, any of our streams, they'll be in black and white.



STEVE:  Nice, nice.



LEO:  With a badge just to remind you to call your Congressperson and email or write to the FCC and comment.  You know, the second stage of commenting I think begins tomorrow.  They didn't get all the comments they ought to have because...



STEVE:  God help them.



LEO:  ...they were DDoSed somehow by all the comments.  You think they could pull that twice?  I don't know.  Anyway, so tomorrow's the big day, July 12th.  And I know you care about that, and we all care about preserving Net Neutrality.  We would not be here if it weren't for the fact that anybody can participating in the Internet as an equal.



STEVE:  And corporate dominance is just - it's a problem.  We do need some regulation.



LEO:  Yeah.  And that's what we're saying.  Don't regulate the Internet, just regulate some ISPs that would like to regulate the Internet themselves.



STEVE:  Right.



LEO:  Unilaterally.  Steve Gibson's at GRC.com.  That's where you go to get SpinRite, for 39 years the world's best hard drive maintenance and recovery utility.  Wow.  And of course while you're there you could check out ShieldsUP! and all the other freebies he offers because he makes a living on SpinRite.  Including this show.  The audio's there, and transcripts, really good transcripts.  Elaine Farris does those all by hand, listening, typing, at GRC.com.  You can also get audio and video versions of this show at our site, TWiT.tv/sn.  Best thing to do, if you ask me, would be to subscribe so you get every episode.  You want the complete set, by all means.



You can even watch us do it live, every Tuesday, 1:30 Pacific, 4:30 Eastern, 20:30 UTC at TWiT.tv/live, or live.twit.tv.  I think both work.  I think even twitlive.tv or .com works.  And if you do that, do join us in the chatroom.  Nice people in there, including Dave Redekop.  If you can just go to irc.twit.tv, you can participate, participate with them.  What else?  Anything else I need to say?



STEVE:  We're approaching Black Hat.  I have a feeling we'll be discussing more pre-Black Hat news next week, and we'll be covering in detail as it happens.



LEO:  It's always our busy time.



STEVE:  We've got to, yeah, I think we're going to have a lot of fun in the coming weeks.



LEO:  Steve Gibson, thank you so much.  And we will see you next time on Security Now!.



STEVE:  Thanks, my friend.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#620

DATE:		July 18, 2017

TITLE:		Calm Before the Storm

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-620.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week, while waiting for news from the upcoming Black Hat and DEF CON conventions, we discuss another terrific security eBook bundle offer, a Net Neutrality follow-up, a MySpace account recovery surprise, another new feature coming to Win10, the wrong-headedness of paste-blocking web forms, Australia versus the laws of math, does an implanted pacemaker meet the self-incrimination exemption, an updated worst-case crypto future model, a surprising find at a flea market, another example of the consumer as the product, a SQRL technology update, and some closing-the-loop feedback from our terrific listeners.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here with lots of security news.  Next week, of course, we'll have Black Hat.  This week we're just getting ready for it.  We'll talk about all sorts of problems and patches, questions from our audience, and a whole lot more.  Here's a question:  Do ones weigh more than zeroes when it comes to hard drives?  It's coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 620, recorded Tuesday, July 18th, 2017:  The Calm Before the Storm.



It's time for Security Now!, the show where we protect you and your loved ones, your privacy, your security, help you understand how technology works, throw in a little sci-fi and games for fun, and that's all because of this guy right here.  This show really is Steve Gibson's show from GRC.com.  Hi, Steve.



STEVE GIBSON:  Leo, great to be with you again, as always.



LEO:  Nice to have you.  We should call this just The Steve Gibson Show.  I think it would have done better, frankly, had we.



STEVE:  We didn't know nearly 12 years ago - actually, nearly 13 years ago.  You realize we're coming up on, we're closing in on the end of year 12 at the end of next month, end of August is when that happens.  So, yeah, we didn't know how it was going to evolve.  And of course it's become, well, many people's go-to weekly hit on what happened during the week.



I titled today's episode, which is No. 620, "Calm Before Storm" because nothing earthshaking happened.  But toward the end of the month, as we mentioned last week, we've got Black Hat and DEF CON conventions occurring one after the other in Las Vegas.  And they always produce a bunch of really interesting news.  And so there is sort of a sense of - you know how all the water disappears from the beach, and people go, where did the water go?  Well, yes, that's because the tsunami is approaching.



LEO:  It seems so calm.



STEVE:  So we're going to discuss another terrific security-related eBook bundle from that Humble Bundle that we've discussed before.  I tweeted it to our listeners and already got a ton of good feedback, so I want to make sure that the rest of our listeners who are not Twitter users know about it.



I've got a little bit of a Net Neutrality follow-up from all of last week's events, just sort of a non-partisan take on what I think needs to happen.  There was a MySpace account recovery surprise, and not in a good way.  Another new feature coming to Windows 10 Creators Edition later this year, but it has appeared now in the latest builds.  We're going to talk about the wrongheadedness of paste-blocking web forms, which is something that's been going on for years.  Our listeners have, like, asked me what I thought, and I've just never gotten a moment to talk about it.  And so I thought, okay, it's reached critical mass.  We're going to do that.



Also we have Australia versus the laws of math.  Does an implanted pacemaker meet the self-incrimination exemption?  An update on the worst-case crypto future model, which occurred to me following some other news.  It's surprising what one can find at a flea market.  Another example of the consumer as the product, and a little quick SQRL technology update to tell our listeners where I am.  And then some closing-the-loop feedback from people who listen to the podcast.  So I think another great couple hours.



So our Picture of the Week, we've actually seen this before years ago, but it is one of these memes that just deserves to never die.  And the caption that I saw it with this time just really closes the deal.  So the caption reads:  "We have patched that vulnerability you reported."



LEO:  I should show the picture here.  Let me show it to you so, for people watching at home, you can see it.  Oh, that's a nice patch you did there.



STEVE:  Anyway, so for those who are listening, this shows a, I don't know, a walk path or a bike path.  It's a paved asphalt strip that's got lawn on both sides, extending off into infinity on both sides.  And in the middle of this asphalt path, it looks like a recently installed gate.  It's bright yellow.  And where the gate is the asphalt's removed, and there's a strip of concrete, so it looks like someone took out the asphalt, poured some concrete, anchored this yellow gate in.  And, I mean, you cannot go through that gate.  But it's completely open on both sides.  And frankly, the lawn doesn't look like it's in great shape on the sides, and you wonder if that's just because everyone goes around.  Anyway, it's just a classic, you know, "We have patched that vulnerability you reported."  Yes, you said we needed a gate, and we gave you a gate to block any foot traffic.



LEO:  No foot traffic allowed.



STEVE:  Or bicycles or whatever.  Ugh, wonderful.  Okay.  So I put this number one because this offering of security-related books is breathtaking.  The link is in the show notes.  The show notes are already posted on GRC.com/sn for Episode 620, so you can immediately get them.  But it's also HumbleBundle.com/books/cybersecurity-wiley.  And the lineup is amazing.  There was a previous, a couple years ago, I think it was O'Reilly who was making these available.  This, as the URL indicates, is Wiley as the publisher.



So we've got, okay, for a dollar or more, that is, for just a dollar, but you're invited to provide more if you wish, the book "Social Engineering:  The Art of Human Hacking."  Also you get "The Web Application Hacker's Handbook:  Finding and Exploiting Security Flaws, 2nd Edition."  And "Practical Reverse Engineering:  x86, x64, ARM, Windows Kernel, Reversing Tools, and Obfuscation."  And the fourth book, "Threat Modeling:  Designing for Security."  All four of those as eBooks in multiple formats with no DRM for a dollar.  But give them five or something.  I mean, and I should also mention that this is all a charitable donation, as well, and you can choose to which charity it goes to from a list lower down the page.



If you go to $8 or more, in addition to those four:  "Security Engineering:  A Guide to Building Dependable Distributed Systems, 2nd Edition."  Also, the second edition of "The Shellcoder's Handbook:  Discovering and Exploiting Security Holes."  "Cryptography Engineering:  Design Principles and Practical Applications."  "The Art of Deception:  Controlling the Human Element of Security."  And "The Art of Memory Forensics:  Detecting Malware and Threats in Windows, Linux, and Mac Memory."  Those five books, in addition to the first four, for $8.



And two of Schneier's books are in the final set.  There's an additional five for $15 or more:  "The Malware Analyst's Cookbook and DVD:  Tools and Techniques for Fighting Malicious Code."  "Unauthorized Access:  Physical Penetration Testing for IT Security Firms."  And one of the first two of Bruce's books:  "Secrets and Lies:  Digital Security in a Networked World, 15th Anniversary Edition."  Also "CEH v9:  Certified Ethical Hacker Version 9 Study Guide."  And, finally, the book - and I've got two copies behind me.  When I was looking at this, I turned around.  One is a hardback, and one is softcover.  And that is "Applied Cryptography:  Protocols, Algorithms, and Source Code in C, the 20th Anniversary Edition."  And of course "Secrets and Lies" and "Applied Cryptography" are both Bruce Schneier's books.  So all eBooks.  What is that?



So that's 14 eBooks, four in the first and then five in each of the two, for $15.  So anyway, I wanted to make sure our listeners know about it.  I've already gotten a ton of feedback from the people who saw my tweet about this yesterday as I was putting this together.  And a lot of people were very impressed with the lineup.  So it's for a good cause.  Apparently it's doing very well.  And I would say take advantage of it.  When I looked yesterday, it was 13 days and some hours remaining, so I think that is an end-of-July cutoff.  So it looks to me like it's good through the end of this month, the end of July.



Okay.  So as we know, Wednesday, July 12, last Wednesday, was the Internet-Wide Day of Action to Save Net Neutrality, which is not really a very catchy title, but that's what it was called.  And I saw, Leo, you guys went to black-and-white and had a red banner across the top of the screen.  And many companies did all kinds of things.  There were companies who were, like, putting up a sort of an emulated blocking banner to bring it to people's attention or to say this is what you might be greeted with if we don't have some regulation to prevent ISPs from pretty much doing anything they want to with the bandwidth that you're purchasing from them.



So yesterday, that is, Monday, just yesterday, major tech companies clashed with Internet service providers over whether this Net Neutrality law, this order that the Obama administration put into effect in 2015 which barred the blocking or slowing of web content, should be scrapped by the U.S. Federal Communications Commission, which is what the concern is now.  So the Internet Association is on the side of we need to preserve Net Neutrality and this existing legislation.  They represent the major technology firms, including Alphabet, which of course is Google, Facebook, Amazon, Microsoft, Netflix, Twitter, Snap, and many others.  All collectively urge the FCC to abandon its tentative plans to rescind the rules barring Internet service providers from hindering consumer access to web content or offering paid fast lanes.  And in the Internet Association submission they wrote that dismantling the rules "will create significant uncertainty in the market and upset the balance that has led to the current virtuous circle," as they put it, "of innovation in the broadband ecosystem."



And of course on the other side the major ISPs - AT&T, Comcast, Charter Communications, and Verizon - all urge the FCC to reverse the existing rules, which were enacted by the Obama administration, while vowing not to hinder Internet access.  That is, they were saying, we will voluntarily agree not to break the spirit of Net Neutrality, but we don't want to be forced to.  And one of them even went as far as to say, well, there are some instances where we think everyone would agree that it's a good thing.  So it's like, yeah, okay, right.



So the FCC, for their part, said that - oh, I'm sorry.  AT&T said that the FCC in 2015, when this was initially put in place, quote, "grossly exaggerated the need for public utility-style regulation, while ignoring its costs."  And so of course what has started all this is that, two months ago, the month before last, in May, the FCC voted two to one to advance the FCC Chairman Ajit Pai's plan to withdraw that previous administration's order reclassifying Internet service providers as if they were utilities.



The problem is, in my opinion, access to the Internet has evolved from what was originally sort of an optional luxury to being a near necessity.  And in every meaningful way, its provision is, I would argue, no different from electricity.  When I purchase electricity from, in my case, Southern California Edison, I pay the same price and the electricity flows just as well to my air conditioner as to my coffeepot.  And this is true, even if Southern California Edison happened to own their own air conditioning company.  They're not permitted to charge me a higher rate if I use a competitor's air conditioner.  But you can imagine that in such a situation they would love to, if they could, to subsidize their own private interests at the expense of the competition and, ultimately, at the expense of the public.



So the FCC Chairman argues that the Obama order unnecessarily harms jobs and investment and has not committed to retaining any rules, saying that he favors an "open Internet," of course meaning a completely unregulated Internet.  And as a proud capitalist myself, I would be happy to have that if we also had choice among providers, I mean true competition.  But as I have said when we've had problems with the bandwidth on this podcast, I have no choice.  I mean, there is absolutely no true practical choice for me in broadband providers.  In my area of Orange County in Southern California, Cox Cable is my sole source of broadband.



And of course this has been deliberately arranged over time.  And years ago we were covering the mergers of these major providers as they were purchasing each other, dramatically reducing effective competition in the market.  And some of the big ones were paused or backed off on or stopped.  But there's clearly a tendency for monopolies to form.  And I would argue that, unless you have true competition, then you have to have some regulation.  And there isn't today actual competition so that a consumer can easily jump from one broadband provider to another, at least not in all markets.  Not in mine.  So anyway, 12 state attorneys general, including from Illinois and California, have urged the FCC not to overturn the Obama rules.



More than, now, I don't know what this number means, but 8.4 million public comments have been filed on the proposal.  And the reason we don't know what it means is it would be nice to know whether that number was bot-filtered or not because we know that there were lots of shenanigans played with that site that wasn't preventing somewhat specious submissions from being made.



So my nonpartisan observation is that mostly I'm troubled that we're seeing such an expanded use of executive orders in place of congressional legislation.  It's true that Congress moves slowly and with great deliberation, when it moves at all.  But that's the way it was designed to function.  The inertia means that things change gradually, only after being examined and debated at length, and that they can be tuned and tweaked as needed over time.  But when issues like this become politically partisan, and when the party holding the office of the executive bounces back and forth between parties as this country experiments with different leadership styles, we wind up creating a climate of tremendous uncertainty, which is arguably the worst of all possible worlds because then no one is able to plan.  No one is able to rely on what the future is going to look like.



So in this case President Obama's administration put this in place by executive order, and President Trump's is considering removing it.  I hope that Congress finds the will to consider this, what I think is a crucially important issue, at length and will take it up once and for all and remove it from underneath the President's executive order pen, except to sign actual legislation into law that settles this once and for all because having this jump back and forth as a political football is just - it's, again, as I said, it's the worst of all possible outcomes.



LEO:  Actually, I don't think it was executive order, Steve.  I think it was the FCC that decided to use Title II regulation.  Obama encouraged them to, but he did not say so.  The FCC said so, and that is in their mandate.



STEVE:  So it's political appointments to the FCC, then.



LEO:  Well, it's appropriate for them to do it.  And the FCC, you may remember, tried to enforce Net Neutrality without Title II regulation, and it went to the courts because they were sued immediately by Verizon.



STEVE:  Right.



LEO:  And the judge said, look, you can't do this.  You don't have sufficient congressional mandate.



STEVE:  Ah, okay.



LEO:  And pointed them at Title II of the Telecommunications Act saying, however, if you...



STEVE:  Under Title II.



LEO:  If you were to declare Internet service providers as utilities, under Title II you could do this.  You haven't up to now.  You said they're telecommunications or something.  But if you were to declare them utilities, you could do this.  In effect, the judge said, "But if you wanted to, you could do it."



STEVE:  Yes, I do remember that.



LEO:  Yeah.  And then Tom Wheeler decided, after extensive comment, and that was really the thing that was interesting about this is the Internet weighed in heavily.  And while I think Wheeler was reluctant to use Title II, was convinced to do so by millions of comments from the Internet last year.  So that's why Ajit Pai can reverse it, because it's an FCC rule.  But you're right, though, and this is what AT&T, which kind of jokingly, in my opinion, said we support the Internet Freedom Day because we believe in Net Neutrality, too, but we believe Congress should do it.  And so they're saying the same thing as you.  And the reason is the telecommunications industry - actually, not even that.  If you just look at Comcast, Verizon, and AT&T...



STEVE:  Strong lobbying power.



LEO:  ...poured more than half a trillion dollars into lobbying in the last 10 years, $574 billion.  So they know that if they go to Congress, they will win.  So I agree with you.  I think really Congress is the right answer to this.  But at this point it is within the FCC's purview to choose Title II and, in this case, to stop using Title II to regulate it because Congress did give them that tool in the Telecommunications Act.



STEVE:  And so then I guess the conclusion is that money wins.



LEO:  Money wins in everything.  But it does, certainly in Congress, especially since the Citizens United decision has allowed a huge influx of dark money into politics, it's gotten worse.  And remember Larry Lessig, the Harvard constitutional lawyer, brilliant guy, created Creative Commons?  For a long time he was fighting against copy protection, DRM.  And then he gave up.  He said, you know what, I can't win this battle because, before we win this battle, we have to change how election financing works in this country.



STEVE:  Right.



LEO:  We have to get the influence of big money out of...



STEVE:  And return control to the electorate.



LEO:  To the people.  And in fact that was Larry's - Larry ran for President.  And his entire platform was elect me because you're never going to get elected representatives to do this, so elect me.  I'll choose a really great vice president.



STEVE:  Exactly.



LEO:  I will do this one thing and then resign.



STEVE:  Yes.



LEO:  Remember that?



STEVE:  I remember exactly that.  It was wonderful.



LEO:  He didn't get very far after that because of the power of money.



STEVE:  People were saying, "Larry who?"  



LEO:  The only thing I'd say about that is the only reason money is valuable in politics is to win votes.  And so ultimately it is still in the hands of the voter.  And we just have to vote.  And we have to make our voices heard.  Call your members of Congress.  That scares the pants off them.  That's far worse than anything, than Citizens United or the NRA or the Koch Brothers.



STEVE:  Yes, the idea that they might not get reelected.



LEO:  That's what scares them.  And the reason it works right now is because most of the congresspeople who support this are Republicans facing easy reelection, but difficult primaries should the Koch Brothers and others decide to finance opponents in the primaries.  They're worried about the primaries, not the elections.  They're in safe Republican seats.



STEVE:  Right.



LEO:  So if people - I think if people started to take it seriously and vote and make their voice be heard, there would be a shot at this.  That's the only way.  And I don't think we're going to change, have campaign finance reform.



STEVE:  And of course, as we've discussed, we finally have - it's taken us as techies this long to actually understand what the term means.



LEO:  Right.



STEVE:  You have been saying it's not the Internet that needs regulation, it's the ISPs.  And that finally clarified the issue in a way that, you know, "Net Neutrality" has got to be the worst term anyone has ever come up with for something that's arguably very important.



LEO:  It's a terrible term.  That's part of the problem; right?  What the hell does that mean?



STEVE:  Yeah, gosh.  And so here we have the world, or at least the U.S., going to be significantly impacted by the outcome.  And most people just, I mean, you know, we care, and our listeners care.  But we're not the electorate at large.  So, yeah, I don't know what chance it has.



LEO:  I agree.



STEVE:  In May 2016 we discussed, a little over a year ago, the fact that MySpace had lost control of 427 million passwords, which were being offered in aggregate to buyers for $2,800.  And at the time we said, okay, if you were ever a MySpace user, go do something.  Delete your account.  Change your password.  Back then, if you were using the same password for all of your logins, then remember what that MySpace password was and make sure you're not using it anywhere else.  And it was just like, it was a disaster.  Yesterday...



LEO:  By the way, I changed my MySpace password when that happens.  Didn't want anybody using my account.



STEVE:  No.  Yesterday, get this, a frustrated security researcher at a firm named Positive Technologies - the researcher's name was Leigh-Anne Galloway - finally publicly disclosed a troubling vulnerability she had uncovered after first responsibly disclosing the problem to MySpace nearly three months ago, in reaction to which MySpace was irresponsibly silent.  She never got any acknowledgement or response of any sort.  Gave them 90 days, nearly, and just said, okay, fine.  So she described, in the coverage of this, MySpace as being an enormous graveyard of personal data.



And in fact I was reminded of the term "zombie data," that is, like places where we have personal data that we've sort of wandered off from, and it's zombie data.  It is our data, and it's never going to die, but it's sort of just there and not being kept current.  So she noted that companies have a duty of care to their users, both present and past.  And in some coverage of this, Leigh-Anne told Motherboard, who reported on this, that when she discovered the flaw, she was horrified and shocked by the complete lack of due diligence on MySpace's part.



Okay.  So what's the problem?  Unlike nearly ever other password recovery system, which is at least anchored to a user-controlled email address, MySpace offers an account recovery process for people who have even lost access to their email account.  And I remember discussing this before, that is, this policy has been in place for a while.  It's a little unnerving because basically they're saying, oh, you've lost your email, like your email provider went away, or you forgot your email login or whatever.  So MySpace implemented a system that would even forgive you if you could not receive a password recovery email.



Now, at first glance, what they're doing doesn't look too bad.  I've got a link here in the show notes for anyone who's interested.  Since what MySpace presents you with is a comprehensive and somewhat intimidating form asking for a great many of an individual's details, with a whole bunch of them marked with asterisks saying "required information must be provided."



So the form states that all of the following information must be provided, which includes the email address associated with the profile - okay, so right off the bat it's like, wait a minute.  If you know your email address - so maybe it's email address, but you can't actually retrieve email from it.  Okay.  Your date of birth.  The zip code listed on the account.  Your full name listed on the account.  The city and state of the account owner.  And there's a bunch of other stuff which is optional.  But all those things, red asterisk-starred, must be provided.  However, what Leigh-Anne found is that it appears that some heuristic logic operating behind the scenes processes the form's data so as to minimize their support costs.



LEO:  [Chuckling]



STEVE:  I know.



LEO:  Just keep listening, folks.  You're not going to believe this.



STEVE:  So it likely has an "if any three or more are valid" in the whole...



LEO:  They don't have to be right.



STEVE:  ...in the whole form acceptance threshold.  So unfortunately, when you look this over, and consequently what Leigh-Anne discovered and reported, and which MySpace has ignored, and Motherboard then confirmed, was that anyone having only the MySpace user's name, the account username, and their data of birth, only those three pieces of information is able to establish themselves as the new owner of any existing MySpace account.



LEO:  So changing my password did nothing.



STEVE:  No.



LEO:  I should change my birth date, is what I should do.



STEVE:  In their reporting, Motherboard verified this, writing:  "Once we finished the recovery process, we had full access to two accounts.  We could write new posts, read old messages, and basically do whatever the account owner could."  And then they said in parens, "(Thanks again to the two brave volunteers who let us break into their old MySpace accounts.)"



LEO:  Unbelievable.  Unbelievable.



STEVE:  It is.  Now, that form, which is readily hackable even by its owner, does have a dropdown list box selection of whether you want to recover access or delete your account permanently.  And I've got links in the show notes here.  There's a different "Delete Your Profile" link and process.  So essentially this ups the ante.  We already recommended in the past that our listeners should proactively remove any residual MySpace accounts.



So if nothing else, this is a reminder that doing so then was an even better idea than we knew at the time because this has probably always been the way this worked.  And it just took somebody poking at it to see how little of the form's data actually had to be correct.  And Leigh-Anne discovered, uh, not much.  And why?  It's because they don't want to get support calls.  It's like, if you just kind of - fuzzy logic.  If you sort of seem like maybe you might be the person who used to be using this MySpace account, eh, that's fine.  We'll give it to you.  So, yeah.  Zombie data.  If you were ever a MySpace user, it's worth taking a minute to just wipe your data clear because otherwise somebody could easily decide they want to impersonate you.  And it's just not difficult to do.



And speaking of account recovery, the Windows 10 Fall Creators Update will be adding what is apparently a much requested feature.  They will be adding an "I forgot my password" option to the Windows 10 login lock screen.  So where you are prompted for a password, there will be a "I forgot it" link, which there has not been until now.  People who are configured to use Windows Hello mode or a PIN will also be able to access the new password reset option right from the lock screen.



Once the password reset process has been started by clicking that link, Cortana pops up and will guide the user through the reset process, which essentially amounts to providing some means of verifying yourself with a secondary email address, receiving a test SMS message, or using the Authenticator app in order to prove who you are; and, once verified, the user will be allowed to reset their password.  So Microsoft continues to listen to feedback and is adding the things that people want.  And I updated my Win10 machine last night to see if I could actually get that, and maybe I have to use the wrong password a few times for it to show up.  I was afraid of getting myself locked out or something.  So I didn't see it.  And I did get - I am on the Creator track, so I think I've got the latest.  But it may not have been there yesterday, but it is on the way.



And speaking of passwords, a listener's tweet reminded me that one thing we haven't discussed ever is the bizarre and sort of counter, what I think is a counterproductive practice, which some, I mean, brain-dead websites have been adopting now for several years of blocking form-fill automation.  And I've received tweets about this probably for several years.  Not in great number, or it would have made it onto the radar and onto the podcast before now.  But the problem, of course, is that this fights against password managers.



The idea is that there is technology on a website which is proactively blocking the automation of form fill, which our password managers use in order to conveniently provide a unique password for every site that we visit, assuming that that's the way we've taken the trouble to set them up.  And they're even manually - they're even blocking manual copy and paste.  So even if you weren't using a password manager, but say that, like, years ago you started a text file or an Excel file or something, where you just sort of had your own ad hoc database in order to help you remember which password you use for which site, where you're used to copying that from your little ad hoc database and then selecting the password field and hitting ctrl-v to paste, but this site won't let you do it.  So it essentially forces you to press keys on your keyboard - and you can sort of imagine Scotty on Star Trek saying, "How quaint" - in order to actually have to manually enter the password.



So this makes no sense to me, and I could not see any rational, right-thinking justification for the practice.  But it is somewhat widespread.  I mean, I'm not encountering it, but in researching it on the 'Net I found lots of instances of it, specific sites that were doing it.  And so I dug around to see whether there was some hidden benefit that hadn't occurred to me.



And I did run across a posting, a blog posting by Troy Hunt, who's a well-known, and we quote him on this podcast from time to time, security researcher who writes a widely read blog.  His short bio states that he creates courses for Pluralsight, is a Microsoft Regional Director and MVP who travels the world speaking at events and training technology professionals.  He examined this question a little over three years ago, so it's been around for a while, under the heading on his blog of "The Cobra Effect," which was a fun anecdote about the mistake that the British apparently made way back when they were in a conquering mood.  They encountered a problem on the subcontinent of India, and that was cobras, which were numerous at the time.  It turns out there were a lot of them wandering around and taking bites out of the British.



So ingenious as the Brits were, they decided to offer a bounty on cobras in the hopes that the indigenous inhabitants of India would round up the cobras in return for cash.  But of course this turned cobras into a form of currency that could be bred.  So cobra breeding became a thing.



LEO:  That's a hoot.



STEVE:  Oh, yeah.  And after the British saw the error of their ways and terminated the bounty, the excess cobras were all released.



LEO:  Were released into the wild.



STEVE:  Back into the wild.



LEO:  You get a net gain in cobras.  Whoops.



STEVE:  Yeah, that's what happens.  Which of course resulted in more cobras than there were originally.



LEO:  Oh, my gosh.



STEVE:  So of course the law of unintended consequences.  So anyway, but I digress.  Troy's analogy was meant to make the point that sometimes trying to fix something to make it better actually has the reverse effect.  And all of us who use password managers would argue that a site which fights against very secure, difficult to enter, and even hopefully unknowable passwords, is lowering your security.



And so, okay, now I understand why this didn't occur to me.  It turns out that the blocking of login automation is apparently, and this is what Troy determined, a completely wrongheaded attempt to prevent automated brute-force login attempts presumably being made by something that's using a web page's automation for brute-force login guessing.  Like some sort of a, I don't know, a form-fill-in robot which fills in the form and then submits the page over and over and over.  Okay.



But anyone who understands how the web actually works knows that the web browser page is merely a frontend which puts a pretty face onto an eventual HTTP form query, an HTTP POST verb query.  So no sane brute-force attacker is going to automate the form.  They're going to bypass that completely and directly submit the form's data to the remote web server for its approval or rejection.  So it's not the way brute-forcing works, which some lame person somewhere thought, oh, I'll be clever, and I'm going to prevent anyone from pasting data into the password field, which will prevent bots from doing that.  But that's not what bots do.  Anyway, another way to say this...



LEO:  Calm down, Steve.  It's okay.



STEVE:  It's not possible to robustly prevent brute-forcing in the web browser client because the browser is trivially bypassed by making direct server queries.



LEO:  I see that also you see places that won't let you paste into the field.  Do you think that's the same misguided, you know, just cut/paste.  



STEVE:  Yes, that's the same, yes.



LEO:  Yeah, well, that's not...



STEVE:  And apparently, like, when companies have been asked, they've said, oh, we'll lose our certification if we allow pasting into the password field.  It's like, what?  Anyway, it's something that's been around us for a few years, and I've been meaning to just say, okay.



LEO:  Stop it.  Knock it off.



STEVE:  To dig in - well, yes, yes, exactly.



LEO:  Please knock it off.



STEVE:  In the same way that the NIST has now changed their guidelines to say there is no point in forcing people...



LEO:  Yeah, there's a good one.



STEVE:  ...to change their password every month.  Please, there is no benefit to blocking pasting into password fields.  It doesn't prevent any actual attack on your server because it's only the web page which no bot is going to fill in. 



LEO:  Right, right.  It'll do a POST.



STEVE:  It's going to issue - exactly.



LEO:  It'll POST the data.



STEVE:  Exactly.



LEO:  It's much faster.  Why would - I'm going to type it in.  I have written a special automated typing system.



STEVE:  Oh, lord.  So yesterday, while I was putting the show together, going back through all the submissions from our listeners over the past week, I kept seeing this link.  And it was about some insanity in Australia.  And I thought, okay, we talked about this last week.  And so I kind of just kept pushing past it, saying, yeah, yeah, yeah.  Finally, someone sent me a tweet with a couple quotes which did stop me cold.  And I said,  wait, what?



So, okay.  The headline on this piece in the Guardian was of little surprise.  It read:  "New Law Would Force Facebook and Google to Give Police Access to Encrypted Messages."  Okay, yeah.  That's what we've been talking about.  The subheading:  "Under government plan, Internet companies would be obliged to give law enforcement agencies warranted access."  Okay, yeah.  Still no surprise.



Malcolm Turnbull said on Friday - and he's the guy that we quoted last week when we were talking about Australia weighing in, or maybe it was the week before.  The law would be modeled on Britain's Investigatory - that's the word I always have a hard time getting around - Investigatory Powers Act, passed last November, which gave intelligence agencies some of the most extensive surveillance powers in the Western world.



Under the law, Internet companies would have the same obligations as telephone companies to help law enforcement agencies, you know, for example, in telephone wiretaps back in the good old days.  Police would need warrants to access the communications.  Turnbull said the legislation was necessary to keep pace with advances in technology that could facilitate crime.  He said:  "We need to ensure that the Internet is not used as a dark place for bad people to hide their criminal activities from the law."



Okay.  Now, here it comes.  Here it comes.  He says, asked by reporters, the Guardian reports, how legislation would prevent users simply moving to encryption software not controlled by tech companies - you know, yay to the reporters for asking the question; right? - Turnbull responds that Australian law overrode the laws of mathematics.



LEO:  What?



STEVE:  Oh, yes.  He said, quote, and I'm not kidding:  "The laws of Australia prevail in Australia."



LEO:  I didn't know you could do that.  That's awesome.



STEVE:  Oh, they've got some powerful laws there, Leo.  He said:  "I can assure you of that."  I'm still quoting.  "The laws of mathematics are very commendable," he said.



LEO:  Yes, they're commendable.



STEVE:  Yes.  "But the only laws that apply in Australia is the law of Australia."  Now, I don't know about the law of gravity because that's handy to have.  Okay.



LEO:  What a moron.



STEVE:  Yes, the people handling the legislation for us.  Also, Turnbull denied that the government's plan involved the use of a backdoor into programs to allow access to encrypted messages on platforms such as WhatsApp and Telegram.  He said, and I'm quoting again:  "A backdoor is typically a flaw in a software program that perhaps the developer of the software program is not aware of, and that somebody who knows about it can exploit."  Still quoting.  "If there are flaws in software programs, obviously, that's why you get updates on your phone and your computer all the time.  So we're not talking about that.  We're talking about lawful access."



So, okay.  Translation:  Apparently, if it's a lawful backdoor, then by definition it's not a backdoor.  It's a handy new feature.  Then, pressed on whether the government's plans meant it would ask companies such as Facebook and Apple to keep a copy of encryption keys used by customers, Turnbull said:  "I'm not a cryptographer."  Uh-huh, yeah, surprise.  "But we are seeking," he said, "what we are seeking to do is to secure their assistance," that is, the assistance of actual cryptographers, although I would argue that it's less seeking their assistance then compelling their assistance.  Anyway, he said:  "They have to face up to their responsibility.  They can't just wash their hands of it and say it's got nothing to do with them."



So, yeah.  Attorney General George Brandis said the legislation would, quote, "impose an obligation upon device manufacturers and service providers to provide appropriate assistance to intelligence and law enforcement on a warranted basis."  Anyway, so again, I just - the idea that they're saying, sorry, the laws of mathematics will be bent to suit the laws of Australia, which will prevail.  It's like, okay.  Now I understand why our listeners thought I had to see this, and I thank them for bringing it to my attention because it was good for a good laugh.



And it's going to be really interesting to see how this all shakes out.  I think probably, what, next year I would imagine we'll begin to see this.  They're saying that this will be put in front of their Parliament by November of this year, and saying that it would allow courts to order tech companies to quickly unlock communications.  Of course, now, the technology we have in place now has been designed not to allow that.



And so there'll have to be some - the legislation will have to incorporate a period of time, well, I mean, we'll have to see how this all comes out.  But if it does happen, it'll have to incorporate some leeway to allow the technology to be changed to accommodate warranted intercept.  And we'll have to see whether it's, like, from some point forward, or past communications, after the fact, I mean, this is going to - it's going to be a mess because, without legislation, we've been covering for years the huge strides the crypto industry has made, arguably spurred by the Snowden revelations, adding things like perfect forward secrecy, where future disclosure of a key does not allow you, by design, to go back and decrypt previous conversations.  That's what perfect forward secrecy means.  And the best protocols we're using today now support that.



So, boy, you know, without any legislation, the technology has shot to a point where we're using a lot of technologies which are fundamentally hostile to the kind of legislation that everybody, all of the saber-rattling that we're seeing is saying that they want.  And it's quite clear that these legislators are not cryptographers.  So, yeah, we live through an interesting period now, Leo.



LEO:  You have to think he's - Turnbull's got to know better.  He's just being cynical.  He's appealing to know-nothings.  It's just not so different from the law of the land in the U.K.



STEVE:  Correct.



LEO:  So we knew this would slowly creep across the world.



STEVE:  Yeah.  And what interests us on this podcast are the details.  What is it?  What is the law going to say, and what are the technological implications of that?



LEO:  Right.



STEVE:  So, okay.  First it was a smart water meter.  Then a listening home audio device.  Now it's a suspect's own heart rate, as recorded by his implanted pacemaker.  Engadget's headline reads:  "Judge Allows Pacemaker Data to Be Used in Arson Trial," with the subheading, "The subject tried and failed to get the judge to disregard his own heartbeat as evidence."  Now, we covered this story at the time.  It was in the latter half of last year.  You'll remember this, Leo, because it was sort of bizarre even then.



Authorities in Ohio arrested a man named Ross Compton on the charge of arson and insurance fraud based on his pacemaker data.  Compton told the police that, when he saw his house burning on September 19 of last year, he packed his suitcases, threw them out his bedroom window, and carried them to his car.  However...



LEO:  How they got there.



STEVE:  Uh-huh.  Since he has a serious heart condition and other medical issues that would have made it extremely difficult for him to do all of that, also in the timeframe apparent, that was apparently, like, based on the 911 call and so forth, investigators were able to secure a search warrant for his pacemaker.  So there was reasonable suspicion that allowed them to get his pacemaker data.  Then, according to court documents, a cardiologist reviewed his heart rate, the pacer demand, and his cardiac rhythms before, during, and after the fire, saying, quote:  "It is highly improbable that Mr. Compton would have been able to collect, pack, and remove the number of items from the house, exit his bedroom window, and carry numerous large and heavy items to the front of his residence during the short period of time he has indicated, due to his medical conditions."



And as it turned out, that data became a key piece of evidence that allowed law enforcement to indict the accused, this guy Mr. Compton, although they also detected gasoline on his shoes and clothing.  So, yeah, how did that get there?



LEO:  He was shaky after the fire, and he had to fill his tank, I think.



STEVE:  He had some hokey story.



LEO:  Yeah.



STEVE:  So I should note that the EFF is not happy about this, naturally.  Stephanie Lacambra, an Electronic Frontier Foundation staff attorney, told SC Magazine in their coverage at that time that cases like this could be the canary in the coalmine concerning the larger privacy implications of using a person's medical data.



She explained:  "Americans should have to make a choice" - should not, sorry.  "Americans should not have to make a choice between health and privacy.  We as a society," she said, "value our rights to maintain privacy over personal and medical information, and compelling citizens to turn over protected health data to law enforcement erodes those rights."



Ross's attorney, so the defense attorney, tried to convince the court to disregard that evidence, arguing that it was obtained in an illegal search.  But the judge who heard the case didn't see it that way.  He has decided to allow the suspect's pacemaker results to be used as evidence against him in an upcoming trial.  However, something the judge said is troubling.  Engadget reported that Judge Charles Pater said he does not think the data's use has bigger privacy implications.  But his exact quote makes that rather murky.



He said:  "There is a lot of other information about things that may characterize the inside of my body that I would much prefer to keep private rather than how my heart is beating."  He said:  "It is just not that big of a deal."  And it's like, whoa, wait.  So he's saying that heart rate should be made publicly available, or available in this case, yet he is acknowledging there are other things about his body that he doesn't want to have available.



So it seems to me like, you know, I'll be surprised, for example, if this got appealed, if that on-the-record statement wasn't used to say, wait a minute, the judge is just making an arbitrary decision that pacemaker data is acceptable, but other things, like maybe your insulin pump data, for example, would not be.  I don't know.  It just seems to me that we need, again, some clear legislation about what is private and what's not.  And I am glad we have the EFF watching this stuff and often stepping in on our behalf.



Techdirt has a writer, Karl Bode, who posted yesterday under the topic of the misuses of technology.  And some of this is sort of already on the record for us.  But it triggered some additional thinking that I wanted to share with our listeners.  So I want to share his reporting and then further stretch our thinking about the possible future of Internet encryption.



So Karl wrote:  "The global war against privacy tools, VPNs, and encryption continues, utterly unhinged from common sense."  And, he writes:  "The assault on consumer privacy remains a notably global affair.  Reddit users noticed that India's fifth largest ISP, YOU" - as in Y-O-U - "Broadband, is among several of the country's ISPs that have been trying to prevent customers from using meaningful encryption.  According to the company's updated terms of service, as a customer of the ISP you're supposed to avoid using encryption to allow for easier monitoring of your online behavior."



Says the Terms of Service, quote:  "The customer shall not take any steps, including adopting any encryption system, that prevents or in any way hinders the company from maintaining a log of the customer or maintaining or having access to copies of all packages/data originating from the customer."  And then Karl writes:  "Of course, enforcement of such a requirement is largely impossible."  Okay, now, this is what I will address in a minute because it's actually and significantly incorrect, as a consequence of some additional thinking I've done.



But, he writes:  "YOU Broadband isn't just being randomly obtuse.  And while the ISP's Terms of Service is making headlines, this effort isn't really new."  He writes:  "Most Indian ISPs are simply adhering to a misguided, and still not adequately updated, set of 2007 guidelines" - so 10 years old - "imposed by India's Department of Telecommunications, demanding that ISPs try to prevent their subscribers from using any encryption with greater than a 40-bit key length, if they want to do business in India."



Quoting from the Terms of Service:  "The licensee shall ensure that bulk encryption is not deployed by ISPs connecting to landing station.  Further, individuals/groups/organizations are permitted to use encryption up to 40-bit key length in the symmetric key algorithms or its equivalent in other algorithms without having to obtain permission from licensor.  However, if encryption equipments higher than this limit are to be deployed, individuals/groups/organizations shall do so with the prior written permission of the licensor and deposit the decryption key split into two parts with the licensor."  Okay.



So then Karl says:  "Which in and of itself is rather hysterical, given that since 1996 or so most folks have considered a 40-bit key length to be the security equivalent of wet tissue paper."  In fact, Ian Goldberg, and we talked about this at the time, or talked about it in the past, won $1,000 from RSA for breaking 40-bit encryption in just a few hours way back in 1997, saying at the time:  "This is the final proof of what we've known for years:  40-bit encryption technology is obsolete."



So first of all, on the question of is this possible, well, what we know of the way web browsers and servers establish their connection is that by necessity there is a plaintext handshake which occurs between the client and the server, or any endpoints in a secure TLS connection, before the encrypted tunnel is brought up in order to begin encrypting traffic.  And we've often talked about how one of the reasons that browsers and servers have deprecated 40-bit keys and the use of 40-bit symmetric ciphers is that it was possible for an attacker to create a downgrade attack.  Because that handshake is, by necessity, in the clear, the client sends a list of all the ciphers it knows to the server.  And if a man in the middle were to edit that list, removing the higher security ciphers and leaving only the 40-bit ciphers, then the server would receive this sad list of available security and shrug to itself and go, well, okay, and agree to establish a 40-bit connection.



The client would think, wow - because the client wouldn't be aware that its handshake had been edited on the fly, it would think, wow, this must be a lame server.  I offered all these great ciphers, and the server apparently just can only do 40 bits.  Oh, well.  And so they would then negotiate a 40-bit connection which we now know, if that was captured, could in a relatively short amount of time be cracked.  And historically this was so-called "export-grade" encryption.  The good news is, for years we suffered under the previous use of export-grade encryption.  It just stayed around for the sake of backward compatibility to endpoints that couldn't do any better.



Finally, over the last few years, as we've moved forward, it has been dropped across the board.  So currently that could not be done.  But what could be done, if somebody wished to, would be that an ISP could first of all require their customers to use browsers with this degenerate form of encryption, or we know the ISP could themselves set themselves up as a so-called "middlebox" in order to intercept more secure connections, decrypt them on the fly, perform so-called "deep packet inspection," and then reencrypt as it goes to the server.



Now, until recently, I have been suggesting that what this would require is that we accept, "we" an ISP's customer, accept a certificate from the ISP because, for example, that's what the existing middleboxes do that are used in enterprise settings.  That middlebox has its own certificate, and all of the PCs within the organization have that cert added, the public key matching the private key that the middlebox is using, have that added to their trusted root store so that their connections can be intercepted and analyzed for malware and content protection in order to help secure the corporate Intranet against the fact that now most traffic is encrypted.



So that's the way those work.  But it's not - and so the other problem in terms of a practical solution is that how can an ISP ask all of their customers to do that?  And what about IoT devices, which may have a non-editable fixed firmware set of root certificates, if they're even using TLS.  Of course, if they're not, then it's not a problem.



But in thinking about this further, I realized that, for example, Google is able to mint their own certificates.  They're able to do that because they have a certificate which they got from Global Trust which is itself a certificate authority.  It's an intermediate certificate.  But unlike most intermediate certificates, it has the permission bits set because it obtained them from Global Trust to itself be a certificate authority and create its own certificates.



So one scenario here is that, in the future, ISPs might be established in the same way that Google has established themselves, such that an ISP would receive a certificate which is already trusted by all of the trust stores in all of our devices.  And unlike typical intermediate certificates, it would be a certificate authority certificate, allowing its interception hardware to seamlessly intercept traffic as it's crossing their borders.



Now, I hate that idea.  Don't misunderstand me.  I'm not suggesting this is a good idea.  But the technology exists to make that happen.  Now, what would be better if something like this occurs is that, instead, an ISP could be required to reroute specific customer traffic to a law enforcement hub.  That is, right now the ISPs have routers.  They're routing traffic around.  So imagine that, in responding to a court order, an ISP is told to route a certain customer's traffic to a specific destination.  Then law enforcement could have such a certificate allowing it to seamlessly and essentially transparently intercept traffic.



And there are still some downsides to that.  For example, we know that Chrome has pinned the known Google certificates.  So Chrome would set off alarm bells if there was a certificate from Google that this entity was trying to intercept and create on the fly.  So either those pinned certificates could be made as exceptions, or Chrome could have the law enforcement interception certificate added to essentially its permitted pinning, in order to prevent alarm bells from ringing.



So anyway, I just wanted to share some further thinking and evolution of ways that it might be possible for the structure that we have today to be matured in order to allow, at least in the U.S., with the constitutional provisions we have against unwarranted search, but the need to be able to provide search warrant supervised interception of specific traffic where a court has decided there is cause, there are technical means by which that could be done in a way that would still largely protect everyone's privacy and even protect the privacy of those who are being surveilled by centralizing that to a degree that, frankly, there are already commercial entities like Google doing exactly the same thing.  Google has the ability to mint certificates.  And that same capability could be provided to law enforcement under some proper terms and conditions.



A cryptography professor is wandering through a flea market and spots a typewriter for sale for 100 euros.  But because he's a cryptography professor, he immediately recognizes it for what it actually is - an original Model 1 German Enigma machine.



LEO:  Oh, no.



STEVE:  Being sold as a used typewriter.



LEO:  And not a very good one, at that.



STEVE:  No.  It's like, where do you put the paper?  How do you change the ribbon on this thing?



LEO:  OMG.



STEVE:  He buys it for 100 euros.



LEO:  Hell, yeah.



STEVE:  So he later sold it at auction.  And the only way I can explain the low price he got, although he made a pretty penny, is that maybe, I mean, the purchaser certainly knew what it was, and they made off like a bandit.  He sold it for 45,000 euros, the "typewriter" that he purchased in a flea market for 100 euros.  The reason, though, that was a good deal, although it must not have been in maybe super pristine condition because I think that matters.  Last month the famous Christie's auction house in New York sold an Enigma machine for $547,500.  So I have a feeling that was probably in mint condition, with all of its little light bulbs working, and probably fully functional.  But, yes, keep an eye out at the swap meet and the flea market and the garage sales for something that looks like a funky typewriter.  You never know what you're going to find.



LEO:  That's wild.  This was in Germany?



STEVE:  I think it was in - shoot, I don't think I had it here.  I've got the link to - the BBC News covered it.



LEO:  You've got to wonder how it got there; right?



STEVE:  Yeah, I know.  It must have just been in somebody's garage for, you know, it's like, oh, spring cleaning.



LEO:  But how did it get in their garage?  I mean, it wasn't - I don't think people just have, oh, yeah, I bought an Enigma machine today.



STEVE:  Yeah, that's true.  And there weren't - yeah, yeah.



LEO:  Wow.



STEVE:  They're rare.



LEO:  Isn't that neat.



STEVE:  Yeah.  So speaking of typewriters, this just sort of passed by.  I just wanted to note on the topic of that we are the product.  We consumers are the product.  The standard keyboard on the HTC 10 has begun showing ads.  Reddit had a long thread of mild annoyed or mildly infuriated people just sort of shaking their heads that now, above all the keys and other little UI features, there were ads beginning to appear on the HTC 10.  So, yes, the look of the...



LEO:  HTC said that was a mistake, that that...



STEVE:  Oh, really.  Oh, I'm glad you know.  Okay, good, good, good.



LEO:  Yeah.  Just that was something went wrong. 



STEVE:  That's freaky.  I mean, I saw the screenshot of it.



LEO:  Yes, no, you would [crosstalk].



STEVE:  And it was quite a convincing mistake.



LEO:  Yeah, well.



STEVE:  Good.  Okay.  So a SQRL update.  I have been working on SQRL's integrated install update and uninstall technology.  It is integrated into the single EXE, so essentially the first - and our listeners will be doing this before much longer.  It's getting close.  I did not use a separate third-party installer because, frankly, SQRL is, I think it's 287K or something, and the installers are multiple megabytes.  So the installer would be - oh, and it leaves debris behind and everything.  So it just doesn't do anything that is necessary.  So when you download it, you'll run it.  SQRL will pop up a screen, "I notice I'm not installed."  And you don't have to install.  You could say no, and it'll just run from where it is.  But if you install it, then you get some additional features.  You have to have admin privileges.



The installer that I have written is UAC friendly, so it understands about the split tokens we've talked about in the past that appeared with Vista, and then of course with Windows 7, 8, and 10, where you need to give permission in order to do something that's privileged.  By putting SQRL into the program files tree, it protects it from some level of abuse.  And it establishes it so that it puts an entry in the add/remove programs in order to make uninstall easy.  It gives it a presence in the Start Menu and so forth.



So I did something that was cool, though, that is an example of one of the advantages of doing my own technology for this.  And that is that of course the SQRL executable itself is signed with an Authenticode certificate that of course I got from DigiCert.  But that in itself, that is, just having the executable signed with a certificate doesn't itself provide the most security possible, since much of today's high-quality trusted software is signed with Authenticode certificates.  It raises fewer alarm bells when somebody runs something.  GRC's certificate has established a reputation.  So, for example, Never 10 is signed with my DigiCert Authenticode certificate.  And we've passed two million downloads.  So Windows knows about stuff signed by GRC and, similarly, by Authenticode certificates.



Any random attacker could sign their own malware.  So just the fact of it being signed actually doesn't provide much protection.  To prevent that from happening, after downloading an updated version of the SQRL client as a temporary file, SQRL's self-updater not only validates that the new download's Authenticode signature is valid, but also verifies that the certificate was issued to Gibson Research Corporation and that the issuer was DigiCert.  So the one step above doing that would be pinning to a specific certificate fingerprint.



The problem with that is that certificates expire, as we know, deliberately, every few years.  So pinning updates to a specific certificate would require the updates to contain the fingerprints of not-yet-valid future certificates and would force an update only for the purpose of updating the permission to update, essentially.  So instead, what I've essentially done is I've pinned the certificate to the "Issued By" and the "Issued To" fields in the certificate, and there's just no way that an attacker is going to be able to arrange to get any sort of a SQRL malware spoof client signed by a valid Authenticode certificate issued to Gibson Research Corporation by DigiCert.  That's not going to happen.



So anyway, and so the beauty of this is that you download the first copy of SQRL, and it will then keep a lookout for updates, notify you when there is one, and you press a button, and it updates.  What happens behind the scenes is that what is downloaded not only has to be over TLS, has to be from GRC, has to be from a known location there; but, once it comes down, then it has to be validly signed, and that certificate that it's carrying has to be from me and which I got from DigiCert.  And if all of that is true, then it renames itself as the new client, and that goes into use.



So a much stronger update technology than we normally see, and I just did it because I could because I was writing my own installer, updater, and remover.  The install and uninstall is all finished.  I'm just adding some little double-check dialogues to the updater.  Then that'll get done.  I'll turn it loose to the gang in the newsgroup to pound on.  And then I'm down to a few little things.  I've got a couple changes to the UI, and then I do have a to-do list that I've been accruing of just sort of mostly cosmetic stuff to deal with before I declare this thing finished.  So we're getting very close.



LEO:  Bravo.  That's awesome.



STEVE:  Yeah.  I'm excited.  Naturally.  And of course all of our SpinRite users are excited because once SQRL is behind me I return to work on 6.1.  Which reminds me, apparently I said SpinRite last week was approaching 40 years old.  I meant 30.  And I saw one tweet...



LEO:  Oh, that's less impressive.



STEVE:  Well, and I thought, how could I have said 40?  And so I saw one tweet from someone who said, I think you said 40, and it must have been 30.  And then I thought, I couldn't have said that.  But then several other people also caught it.  So clearly that was what I said.  I just wanted to correct the record.  Yes, 30 years old.



LEO:  Shows you how much credibility you have with me because I didn't doubt it for a minute.  He must have started in his 20s.



STEVE:  It also means that Sue has been with me for about 32 years because she has been with me from even before the days of SpinRite.



Speaking of SpinRite, I did see a nice note from Michael, who's in Glasgow, Scotland.  And the subject was:  "What's heavier, one ton of zeros or one ton of ones?"  I thought, what?  Anyway, he said:  "Hey Steve.  Longtime SpinRite user here.  It's an amazing product that I, like many of your listeners, treasure.



"I was thinking about Oxford's question from Episode 617, on whether it's better to SpinRite before formatting a drive or after.  The intuition he had about it probably making no difference is interesting.  You agreed with him, of course.  My question is, does SpinRite take the same amount of time in both cases?  Say that there's a 1TB drive chockful of years of data and," he writes, "detritus."



LEO:  Detritus, he means.



STEVE:  Detritus, that's right.  I always - I have a problem with that word, detritus, thank you - "but in good working order with no serious problems.  Will SpinRite finish working in the same amount of time as a blank, completely empty 1TB drive fresh out of the factory?  I've never tried it, but I guess it must."  He says:  "My gigantic folder structures and sizeable data files have never felt so light."  And so the answer is yes, until v7.



My plan is to do a series of point releases - 6.1, 6.2, 6.3, and I'm not sure how far it'll go.  Because it's taken me so long to get SQRL finished and to get back to SpinRite, I'm going to push out a 6.1 faster than I had planned, that is, I'm going to make the changes I need to very quickly in order to get a 6.1 out.  Then I'll do - and that will fully support direct hardware access to the hardware, that is, the native motherboard AHCI controller, or is it ACHI?  I can't - Advanced Host Controller, AHCI, Advanced Host Controller Interface.



I think 6.2 will then add support for the native USB controllers to allow attached storage to work much faster.  I did, before I paused work on 6.1 to work on SQRL, I did have the hardware running, the direct hardware interface running with a maximum size 32MB buffer, up from 32K, which is what SpinRite has always used traditionally because that's the max that could be allocated.  And it was running, it would be able to do, that is, the code I have was able to do half a terabyte per hour.  So it would do that one terabyte drive that Michael asks about in two hours.  But none of the SpinRite 6 series knows about the file structure.  SpinRite has always been a whole drive surface analyzer and data recovery tool.



With 7, I'm going to implement file level awareness to add things like rebuilding the file structure.  But what that will also mean is that it will be able to do selective file recovery and, for example, to verify the integrity of the data separate from the surface.  So for the moment, to answer Michael's question, it doesn't matter whether there is a file system on the drive or not, which is why many people run SpinRite on brand new drives before establishing, before even sticking it into a computer and establishing their operating system on it because it'll run on a blank drive just fine.  So anyway, Mike, thanks for the question and for bringing it up.



LEO:  All right.  On we go.  Steve Gibson, you've got some questions.



STEVE:  So, yeah, just a couple nice bits of feedback from our listeners.  Someone whose Twitter handle is @shadyhotdog, but the name he has registered is This Is Nighthawk!, he just said:  "DNSthingy is amazing."  He said: "Flashed my ASUS router with their firmware, and now I can do all sorts of cool stuff.  Thanks, @SGgrc."



LEO:  I think that's the Nighthawk who's a regular in our chatroom.



STEVE:  Oh, cool.  Yeah, of course, and that's the DNSthingy that we talked about last week from the Nerds On Site guys, David Redekop, who also hangs out in the chatroom, that uses DNS in order to perform all kinds of controls over people's local Internet.



Byron Lee wrote:  "Steve, my security-conscious friends won't touch my mobile Facebook Live video streams.  What's a good, simple alternative?"



LEO:  Ah.



STEVE:  And Leo, I was hoping you would have an answer to that, since I have no idea.



LEO:  I wonder why their friends won't, why his friends won't.



STEVE:  Yeah, he says "my security-conscious friends."



LEO:  Well, they just don't like Facebook, I guess.



STEVE:  Exactly.



LEO:  Well, there are certainly many ways to stream.  There's kind of, you know, we stream on Ustream.  That's free.  We stream on Twitch.tv.  That's free.  YouTube Live, that's free.  But I think if his friends don't like Facebook Live, they may not like any of those.  I'm not sure.  All four of those allow free live streaming.  I don't - it depends what his friends don't like.



STEVE:  Yeah.  I think maybe they're just anti-Facebook.  So you've just given us four good alternatives.  So I think that probably answers Byron's question.  Thank you.



LEO:  Periscope, that's another one.



STEVE:  Oh, yeah, of course, yeah.  Matt Warner said:  "Looks like Twitter disabled two-factor authentication using authenticators and now uses SMS only.  Is it better not to use two-factor authentication at all?"  And I would say absolutely not.  That is to say, it is better to use it.  I mean, the fact that SMS is not as safe or secure as authenticator-based, time-based one-time passwords or one-time tokens, doesn't mean that it's not still better than nothing.



So, yeah, it's a little worrisome that the six-character SMS is being sent every time you need to authenticate.  It would be nicer if that weren't the case.  But it sure beats, you know, but at least it's tied to your phone that's registered with your account and prevents an attacker from just being able to arbitrarily log in if they somehow guess your username and password.  So absolutely, use as much two-factor authentication as you can.  Any is better than none.



And Jerry Yu followed up on our discussion last week about that Broadcom exploit which was patched by Google earlier this month in Android, to note that it looks like it was also patched by Apple.  Remember I said we did not know where Apple stood.  It looks like it was patched in 10.3.1 because under the CVE number for that exploit, it reads:  "WiFi in Apple iOS before 10.3.1 does not prevent" - and that's the CVE-2017-6556 "stack buffer overflow exploitation via a crafted access point."  So that's definitely the problem we were talking about, and it looks like Apple did fix it in 10.3.1.  So that's good to know, that we don't have that baseband radio problem.



Michael Rickman asked if I saw this, and he sent me a link, which was to yet another all-in-one credit card project on Indiegogo.  And I just have to say, yes, and so far I think I've been burned by three of them.



LEO:  I warned you about Coin.



STEVE:  Yes.  Kickstarter or Indiegogo.  And, I mean, in fact I did finally get an email from one of the first ones where the guy, I mean, I sincerely think, I mean, I know that he did everything he could.  But I remember seeing a picture of his workshop, like he was in the garage winding some coils or something.



LEO:  Oh, boy.



STEVE:  And I remember thinking, oh, this does not look good.  So the problem is these electronic all-in-one credit card things, they are seductive, but it is very difficult technology.  And so this is not something I would argue anyone should fund. I'm sorry.  I don't mean to pour cold water on this.  But Indiegogo - and it's not universally the case that Kickstarter and Indiegogo are people in their garage.  But as you know, Leo, as I have experienced, many of these projects never ship.  And I will be very surprised if a small group are able to pull this off.  If it's going to be done, it's going to be done by somebody, by a much larger organization that can actually make this happen.



LEO:  This is the - now I want to say this is the one I want.  This is the first one I've seen that uses a chip, that uses an EMV chip.



STEVE:  I know.  Yup.



LEO:  See, that makes sense to me.  It's not a striped card, it's an EM - I know.  But you're right, it's hard to make.  They've got $2 million raised.  



STEVE:  Good.  Let somebody else pay for it.



LEO:  You've learned your lesson.



STEVE:  I'm not doing it again, no.  There's just too many things that can go wrong.  So as soon as it moves to "Yes, we've actually shipped it, and it works," then it's like, okay, sign - I only use one credit card anyway.  My best friend has a wallet that's probably throwing his back out, it's so thick, full of - because he travels for a business, and he's got hotel cards and airline cards and rental car cards and everything.  So, yeah, I could see where for many people collapsing them into a single card would make sense.  But, no.



LEO:  You have to charge this one every month, too, which is kind of weird.



STEVE:  Yeah, exactly.  I mean, there are just - there are downsides.



LEO:  Yeah.



STEVE:  So someone named, I don't even know how to pronounce this, Qrex.  It's probably something that was available on Twitter.  His name is Qrex, and his Twitter handle is @qrex.  He said:  "Confused re SN-619 HTML5."



LEO:  [Laughing]



STEVE:  He wrote - uh-huh.  He wrote:  "So privacy and security are good, unless it's media, and it's between a company and their subscribers?  Explanation?"



LEO:  That's a good point.



STEVE:  I know.  That one, it gave me some pause.  And so I thought, okay.  Here's how I - and really, I had to think about this for a while.  Privacy and security are good unless it's media, and it's between a company and their subscribers.  Okay.  Valid point.  The MPAA, let's remember, fought mightily to completely prevent the commercialization of consumer home recording.  When that inevitably failed, content providers attempted to thwart the consumers' legal right to make copies of their legally obtained media.



Early analog VHS tapes were protected with a system known as Macrovision, which deliberately messed up the automatic gain control, the AGC, of recorders, preventing the duplication of such protected content.  This also lowered the delivered quality of the result.  But the publishers didn't care.  It also didn't work to prevent copying.  Before long, Macrovision stripping boxes appeared which removed that protection.  So it increased the cost of everything, reduced the quality of the result, and didn't prevent anyone who actually wanted to make a copy from doing so.  Pirates continued to pirate.



In the U.S., as we know, our copyright laws provide exemption for fair use.  But media DRM does not honor that right.  The most famous examples are probably DVDs, whose encryption was broken long ago.  And early in this podcast we talked about the problem, the fundamental problem of a DVD player needing to decrypt the encrypted disk in order to show it to you.  The decryption has to happen there, in your living room.



So yes, it can be cracked.  And as of course we know, it was broken long ago, allowing owners now of those DVDs to legally, even if against the overtly restricted express wishes of the media's publisher to rip and decrypt DVDs for more convenient, and I would argue also more reliable, because we know DVDs don't last forever, storage and use.  The silver oxidizes over time, and a DVD can become unplayable.  But if you ripped it and stored it on a hard drive, it could last longer, potentially.



And of course more recently we have HDMI's HDCP, the High-bandwidth Digital Content Protection, which forces another significant compromise.  Once upon a time it was possible to instantly switch media sources and destinations in the blink of an eye.  But that smooth operation has been messed up by HDMI's HDCP.  Now, when switching, it's necessary to wait for at least several seconds for the endpoints to renegotiate their secure connection.  And when that fails, as it occasionally does, you have to switch away and back again and hope.  And HDCP limits cable length which can be used reliably because it's the most finicky protocol in the entire system.



And HDCP also has been completely bypassed.  The market is chockful of $39 Chinese HDMI HDCP decrypting capture cards, which contain all of the standardized and well-known HDCP keys and render the entire system ineffective.  If pirates wish to pirate, they can and do.  But in the meantime, everyone else, all legal and legitimate users suffer significantly lower quality experience because the MPAA once again won their legislative lobbying victory.



So will the web be any different?  Of course not.  It will be every bit as possible for pirates to decrypt and capture web-delivered media as it has been for them all along the way.  You can place your bets on that right now.  The people who realistically understand this know that we're going to get a fragile and unnecessarily complex system, which will be less comfortable and convenient for legitimate and legal use, while doing little to actually thwart those who are intent upon pirating the web-delivered content.



The bottom line is what the MPAA and RIAA want is simply impossible for technology to deliver.  Yet the money we have paid them allows them to continually screw up, with no apparent success, their media's delivery channel.  So, yeah.  This is a bit different from protecting our logon, password, and session cookies.  And that's our show.



LEO:  Wow.  Jam-packed with security goodness.  Steve Gibson is at GRC.com.  When you go there, check out SpinRite, the world's best hard drive maintenance and recovery utility.  That's his bread and butter because everything else there is free, including copies of this show, 64Kb MP3 audio, finely written transcriptions, so you can read along as you listen, and of course all that other great stuff he puts up there for all of Steve's wide and varied interests, including SQRL and Perfect Paper Passwords and Password Haystacks.  And there's a great password generator there, people keep reminding me, a great 64-character password generator.



STEVE:  Yup.



LEO:  It's all there, GRC.  It's fun to browse around, too.  It's like Grampa's attic.



STEVE:  Grampa, wonderful.



LEO:  If Grampa were a crazy security guru.  Hey, we're the same age.  I can say that.



STEVE:  Hey, no offense taken, my friend.



LEO:  Uncle Steve's attic, how about that?  We also have audio and video of the show, if you want to watch Steve in action and see his mug and the mug.  Just go to TWiT.tv/sn.  By the way, TWiT.tv/teespring for the T-shirt.  The mug will be added soon.  Steve gave his thumbs-up.  We put, I admit, peer pressure.  But that's, you know what, sometimes you have to.



STEVE:  It looks good.



LEO:  I think it looks great.  I can't wait to get one.  We'll send you a few, too.



STEVE:  Cool.



LEO:  If you go to TWiT.tv/sn you can also subscribe there, and that's what I recommend.  That way you'll get every episode.  There are quite a few, and many more to come.  Steve's pledged to stick with us till 1,000.  So a whole 380 episodes still to come.  TWiT.tv/sn, or use your podcast client, whatever you use to listen to podcasts, to subscribe.  We do this show every Tuesday, 1:30 Pacific, 4:30 Eastern, 20:30 UTC, right after MacBreak Weekly  So if you'd like to watch live, I encourage you to do so.  TWiT.tv/live has live video streams on the services I mentioned earlier, plus audio streams, and we're on TuneIn.



You can listen on your Amazon Echo, as well.  Oh, she wasn't listening, I guess.  Just actually you can watch live by saying "Echo" or whatever your trigger word is, "Watch TWiT Live on TuneIn," and it will play for you.  Sometimes you have to say "TWiT Live" for some reason.  I'm not sure why.  You can also say, "Echo, listen to Security Now! on TuneIn," and you'll get it that way.  And that way you'll get the latest episode.  That's kind of a fun way to listen, while you're cooking or whatever else you're doing.



Let's see.  What else?  Oh, if you're watching the live stream, I would invite you to visit the chatroom, irc.twit.tv, because you can play along with the home version of our show and all the crazy kids in there.  Thanks for joining us.  We'll see you next time - bye, Steve - on Security Now!.



STEVE:  Thanks, Leo.  Bye.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#621

DATE:		July 25, 2017

TITLE:		Crypto Tension

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-621.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  We start off this week with a fabulous Picture of the Week and, for the first time in this podcast's 12-year history, our first Quote of the Week.  Then we'll be discussing the chilling effects of arresting ethical hackers, the upcoming neutrality debate congressional hearing, something troubling I encountered at McAfee.com, an entirely new IoT nightmare you couldn't have seen coming and just won't believe, the long-awaited Adobe Flash end-of-life schedule, welcome performance news for Firefox users, the FCC allocates new sensor spectrum for self-driving cars, three bits of follow-up errata, a bit of miscellany, and then Crypto Tension - a careful look at the presently ongoing controversy surrounding the deliberate provisioning of passive eavesdropping decryption being seriously considered for inclusion in the forthcoming TLS v1.3 standard.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We're going to hear about a sad story of a young security expert who got busted in Budapest for finding all the flaws in the new subway system.  We'll also hear about a big fight going on right now in the Internet Engineering Task Force, and Steve will explain it all.  Plus iRobot that wants to sell your floor plans to the highest bidder.  What could that mean?  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 621, recorded Tuesday, July 25th, 2017:  Crypto Tension.



It's time for Security Now!, the show where we cover your latest security updates.  Of course this is the post-Black Hat, pre-DEF CON show, so you want to pay close attention.  Steve Gibson is here, our security guru.  Hi, Steve.



STEVE GIBSON:  Leo, great to be with you again, as always.



LEO:  Good to see you, good to see you.



STEVE:  This interesting time of the summer when we have our annual major Las Vegas events, all the hackers getting together.  Normally there are press releases sort of happening along the way, and then we do deeper coverage after.  For whatever reason, there doesn't seem to be a lot of upfront press coverage, so I imagine that we'll be talking about what was revealed, probably next week.



LEO:  One thing that I think Greg Ferro, who covers it, was talking about - maybe it was Roberto Baldwin, who is on his way to DEF CON.  He doesn't go to Black Hat.  Black Hat's more corporate; DEF CON's more fun.  He said that because the bug bounties these days are so steep, people just don't hold onto security exploits in the way they used to; right?



STEVE:  Yes, exactly.  So, yes, you can prance around onstage and have some glamour, or you could have a quarter million dollars.  Gee.



LEO:  Uh-huh, uh-huh.	



STEVE:  What do I choose?  But we do have a packed and interesting podcast.  We'll start off this week with a fabulous Picture of the Week that you already saw and got a big kick out of.  Then for the first time in this podcast's 12-year history our first Quote of the Week, which started off being just - I was going to cover it; and I thought, okay, I'm going to pull this out separately because this is just too fun.  Then we're going to discuss the chilling effects of arresting ethical hackers, something that happened in Hungary; the upcoming neutrality debate congressional hearing; something troubling I encountered yesterday when I went to McAfee.com; an entirely new IoT nightmare we couldn't have seen coming and won't believe; the long-awaited Adobe Flash end-of-life schedule, thank goodness; welcome performance news for Firefox users; the FCC allocating some new spectrum for self-driving cars; three bits of follow-up errata; a bit of miscellany; and then arguably the main topic of this podcast.



So unlike the recent ones, where we just haven't had - there's been so much news and no one thing really stood out?  This one does.  I titled the podcast "Crypto Tension" because we're going to take a look at the end of the podcast, a careful look at the presently ongoing controversy, and I can't believe I'm even saying this, surrounding the deliberate provisioning of passive eavesdropping decryption being seriously considered for inclusion in the forthcoming TLS v1.3 specification.  So, yikes.



LEO:  Yep, mm-hmm.



STEVE:  But first, a word from one of our several loving podcast sponsors.



LEO:  I wanted to mention, this is a terrible thing to do, but you liked "The Martian," right, Andy Weir's wonderful book and later movie?



STEVE:  Oh, loved it, yes, yes.



LEO:  Weir's been working on a new book.  We knew this.  We had him on "Triangulation."  We're getting him on "Triangulation" again in, I can't remember, soon.  And I just got a copy of his new book "Artemis."  And I was all excited until I saw, "On Sale November 14th."  So I don't want - I won't tease you.  And I will be reading this.



STEVE:  And "The Martian" was one of those where I knew the movie was happening, so I deliberately read the book first.



LEO:  Right.



STEVE:  And of course the book is always better.



LEO:  It really was, yeah, in this case.



STEVE:  Yeah, I mean, there's so much extra detail that a book is able to have that you just sort of have to do in broad strokes or broad brush with a movie.



LEO:  That's always the case.



STEVE:  Yeah.



LEO:  Plus I've always held, especially with science fiction, your mind is a much better set builder than any real Hollywood set builder.  And so these are much more vivid when you read them.  Now, the success of the "The Martian" movie was such that he's already sold this.  Movie rights have been acquired by 20th Century Fox, even though the book won't be out for months.  It's about - and actually it will be a great movie.  I don't want to give away too much.  I will read to you from just the back of the book.



STEVE:  Yeah, yeah, yeah.



LEO:  "Welcome to Artemis, the first and only city on the moon.  It's a tourist destination and an economic miracle.  It's a testament to our species' ingenuity, resourcefulness, and strength.  If you've got the money, you can stay at the Ritz-Carlton Artemis, which is on the Aldrin bubble.  If you're a little tighter, you could stay in the Conrad" - poor Pete Conrad, he got the cheap bubble.  Anyway, it sounds awesome.  I think that that's not a spoiler.



STEVE:  It does sound like a fabulous movie.



LEO:  What a premise; right?  What a premise.



STEVE:  Yeah.



LEO:  I won't read more, although there is more on the back of the book.  But I was so excited.  I just opened this, so I'm very excited.  I don't think Mark Watney is in this one, though.  But I'm sure Andy Weir's incredible sense of humor is.



STEVE:  I don't think Mark ever wants to leave the Earth again.



LEO:  Mark is staying put, staying put.  All right.  Enough of that.



STEVE:  Okay.  So our very first in 12 years Quote of the Week.  Todd Westby, who is the CEO of Wisconsin-based tech company Three Square Market, was explaining to a reporter from ABC News...



LEO:  Oh, dear.



STEVE:  ...about how 50 of the company's 80 employees had agreed to be "chipped."



LEO:  Oh, I saw this.



STEVE:  As is the slang term, have an RFID tag the size of a grain of rice implanted in that little webbing between their thumb and forefinger.



LEO:  We were talking about this at breakfast, Lisa and I.  She said, "Would you get chipped?"  I said, "Well," I said, "it's not a GPS.  It's probably RFID," which it was.



STEVE:  Okay, so, yeah.  So I just love the way he's tried to explain this, like sort of why this is a good idea.



LEO:  It's like chipping your pet.  It's the same exact technology.



STEVE:  Well, yes.  For our audience, though, he says to this reporter:  "There's really nothing to hack in it because it is encrypted just like credit cards are."



LEO:  Oh, please.  What?  What?



STEVE:  And then he says:  "The chances of hacking into it are almost nonexistent..."



LEO:  Oh, this guy doesn't know what he's talking about.



STEVE:  I know, "...because it's not connected to the Internet," he said.  "The only way for somebody to get connectivity to it is to basically chop off your hand."



LEO:  Oh, god, please.



STEVE:  Oh, that makes me feel so much better.  Those bullet points, that just sells the whole concept.



LEO:  Well, the thing that strikes me is that's, I mean, it's just sending out a number; right?  It's like a credit card number that's [crosstalk].



STEVE:  It is a transponder.  It is a passive transponder.



LEO:  So you put it near the reader, it'll send it a code.



STEVE:  You ping it with a little magnetic pulse that gives it the energy it needs in order to send back a fixed ID.



LEO:  And it's a hard-coded number.  That's my problem.  It's like a fingerprint.  I mean, you're not going to - it's not like a credit card, which you can change if somebody gets it.  It's more like a fingerprint, which you're out of luck if somebody gets it; right?



STEVE:  Well, it was described as a splinter, so it can be removed.



LEO:  I guess you could remove it and put it back.



STEVE:  If somebody regrets it.  He also said that his wife, his young adult children, and others would be getting, you know, probably Rover, as well, will be getting the microchip next week.



LEO:  Oh, lord. 



STEVE:  And, you know, what could possibly go wrong?  So again, I get it that people are split on this.  There could be - and he's saying, oh, you'll be able to just, like, put your hand up against a reader at the front door to enter and so forth.  I mean, I get that there's a convenience aspect.  My problem is, if it in fact produces a fixed ID, that's, as we know, far less secure than a rotating ID.  It can't...  



LEO:  I doubt that it does rotating.  It's too small; right?



STEVE:  Well, it could.  It can't be time based because there's no way for it to power a clock.  But it could be like those - remember the early VeriSign eInk cards, where every time you pressed the button you got a different six digits.  So there would be enough energy to increment a counter.  And with a little supercapacitor to keep it alive, blah blah blah.



Anyway, the point is that there are ways to make it better.  The problem with it being a fixed output is that then it's cloneable.  The problem is you might tend to over-rely on the security of something which is really not very secure.  This is not a secure technology.  It's a convenience.  But if you also just memorized a long passcode, that would be safer, and nobody would want to cut off your hand.  Which is a serious upside to not being chipped.  Anyway.



LEO:  By the way, this is old technology.  I remember Mexican diplomats who were subject to being kidnapped did this, not so much for credit cards, but just so that they would be identified when they got the remains.



STEVE:  Well, yeah.  And of course we're seeing an evolution of the technology.  The original RFID tags were like a large pill, I mean, like a really large capsule.  And so it was much more inconvenient to have that somewhere.  And they were, like, putting it in your forearm because you needed a bigger place.  But that little webbing in between your thumb and first finger, that seems to be now the place to lodge...



LEO:  I'm willing to bet a hundred bucks that it's exactly the same technology you do with Fido when you put a chip in his ear, that all it is is an ID number.



STEVE:  Yeah.  I think you're absolutely right.



LEO:  Yeah.



STEVE:  Yeah.  And by the way, they cost $300 each.  They're also not cheap.



LEO:  It's profit center.



STEVE:  That's right.  So speaking of something that is cloneable - we'll get to that at the end of this story.  Budapest's Hungarian Public Transportation Authority - whose name I cannot pronounce, but the initials, thank goodness, are BKK - turned in an 18-year-old ethical hacker after he notified them of a trivial exploit to their recently put online ticketing website.



LEO:  Oh, nice.



STEVE:  So, okay.  It's just so wrong.  So get a load of this.  The 18 year old, who has asked that his name not be made public, was poking at a newly available mobile online ticketing system which had just been put online.  So he just you know, wanted to see how they did.  And in other coverage of this that I read, the sudden appearance of this surprised people because this transportation agency had been trying to get e-ticketing going for years and had spent, I think it was something like 12 million equivalent U.S. dollars in apparently nothing so far.  So suddenly this thing appears.  Okay.  He discovered, this 18 year old, that after bringing up the BKK's website, he could simply press F12 to open the browser's built-in developer tools, modify the page's form submission code to alter the ticket's price.



LEO:  Oh, this guy was an elite hacker, obviously.  



STEVE:  Oh, that's right.  We've got to lock that guy up because you don't want to let him loose on society.  And because there was absolutely no client- or server-side ticket price validation in place, the BKK system blindly accepted the visitor-provided ticket price.



LEO:  Name your own price.



STEVE:  I mean, it was almost as if, I mean, it's the equivalent of putting a blank field on the page saying, "Fill in the price you would like to pay."



LEO:  How much would you like to pay?



STEVE:  And it issued a valid ticket at that reduced price.  That is, charged him - well, and so as a demo the young man says he purchased a ticket, normally priced at a U.S. equivalent $35, for just $0.20, to see if he could.  Now, he didn't even know it was going to work.  I mean, of course you wouldn't know looking at this.  You would hope it wouldn't work.  So he adjusts the form's statement of the ticket price from the equivalent of $35 to $0.20 as a simple proof of concept, and he never used the ticket in any way.



And of course, as I just said, when he initially made the trivial change to the web page, he didn't know if it was going to work.  It was a trial.  It was a test.  Well, after responsibly notifying the transportation authority of his finding, so that they could address this glaring deficiency in their brand new, just-put-online system, and never using the valid ticket, shortly afterward he was awakened in the middle of the night and arrested by the police.



LEO:  That's sad.



STEVE:  It is.  BKK Management then boasted in a press conference about, quote, "catching the hacker" - yeah, it was tough, we read our email - and declaring their systems secure once again because of this...



LEO:  Because we got the guy.



STEVE:  Yeah, we got him.  He's behind bars.  Bleeping Computer's reporting said that, since then, other security flaws - you can imagine, if it does this, what else must be there.  Other security flaws in BKK's system have since surfaced on Twitter.  And I put in parens here, yeah, no kidding.  I said I would hate to be BKK right now, if their publicly facing systems design is so slipshod.  Can you imagine what else must be wrong there?



And get this.  BKK has a $1 million annual contract with a local company, T-Systems, for the maintenance of its IT systems.  So somebody's got a sweetheart deal there with the government.  Okay.  But we actually do have additional results.  Since then, obviously, the news of this has drawn a lot of attention.  We have learned that the system stores its passwords in cleartext and emails them in the clear, if you ask for a reminder.  I forgot my password.  Oh, here it is.  Oh, thank you very much.  That's what - I forgot it was Aunt Bessie's maiden name.  Also, after logging in, visitors were able to get the data of other users, apparently simply through manipulating the page's URL.



LEO:  Oh, lord.



STEVE:  So your account name is showing in the URL.  And if you change it to somebody else, oh, look at that, now you're them.



LEO:  Oh, my goodness.



STEVE:  Reports have claimed that it's possible to access other users' profiles, which include their full name, their physical address, and an ID number which is either their national ID, their driver's license, or passport.



LEO:  And of course not easily changed.  No, wow.



STEVE:  Exactly.



LEO:  Wow.  Wow.



STEVE:  And if you put in "shop.bkk.hu" into the browser URL, nothing happens because the site never implemented an HTTP to HTTPS redirect to bounce the browser's default assumption of HTTP, which by the way I've commented previously ought to be changed now.  Browsers should try HTTPS or try both and choose the one that is secure, if they both work.  But that's not happening yet.



So if somebody hears or reads that, oh, the new e-ticketing system is shop.bkk.hu, and they go to their browser and type it in, they don't get to the page because they have to do https://shop.bkk.hu explicitly because these people didn't bother to open port 80 and bounce the user over to 443.  Also, tickets don't even work.  They don't display properly on iPhone's Safari browser.  So that's a problem.  And, finally, someone determined, I don't know how this could possibly have happened that they figured this out, another case of master hacking, that the admin password was adminadmin.



LEO:  No.



STEVE:  And logged in using that.



LEO:  Oh, whoever did this was a sixth-grader.  Wow.  Wow.



STEVE:  Oh, and, finally, it doesn't even work.  Getting back to the RFID tag problem, there's no tracking of ticket usage or cancellation of a ticket upon use.  So tickets were 100% copyable, and there were reports you hit the home button and power to take a snapshot of the ticket being displayed on your phone, and then email it to other people.  And...



LEO:  Everybody can use it.



STEVE:  Yes, exactly.



LEO:  One ticket serves all.



STEVE:  Some guys did an experiment, made a video showing the reuse of the same ticket through "ticket control," unquote, 10 out of 10 times, without being caught or raising any alarm, and the ticket was accepted.  So the BKK representatives in their announcement talked about how the system was under continuous attack, they said, of which none were successful.



LEO:  No.



STEVE:  So consequently there was no need to stop the system, and that everybody's data was safe.  Nothing to see here.  These are not the droids you're looking for.  Move along.



LEO:  Oh, wow, wow.



STEVE:  Wow.  So we know that criminal cyber hacking is a very real thing today.  Bad guys won't disclose the breaches they find.  They'll exploit them to the limit, doing real damage over time to their victims.  Compare that, contrast that to a teenager who verifies a problem and privately, quietly, and responsibly reports it to the affected company, thus allowing them, if these people had - well, I mean, now we realize it literally was the tip of the iceberg he found, but would allow them to fix the problem with no fanfare.  You know, I get it.  When you look at the laws, the laws are awful.  The laws that exist now are clearly wrong.  And it is a difficult problem because we know there is a wide gray area that separates the white hats and the black hats.  And you and I have talked about instances, Leo, in the past where a hacker kind of does some things that are questionable.



LEO:  Gray hat, yeah, yeah.



STEVE:  Yeah.  And then kind of says, oh, but I really didn't mean it, or I was just checking, it's like...



LEO:  I was pen testing, yeah, yeah.



STEVE:  Yeah, yeah, yeah.  And it's like, well, were you really?  So I get it that there's a gray area.  But what we have currently are very overbroad laws which are still on the books and label anything that some authority in power doesn't like, basically.  I mean, that's almost the way the law is written.  If you don't like this being done to you, then it's illegal.  But the problem is we need to provide, I mean, it's so crucial that an 18 year old somehow be able to not do damage to a potential victim, but help them, be allowed to help them.  They're paying a million dollars to some ridiculous company who has allowed this to happen, and here's an 18 year old who, for free, says you might want to fix this because anybody can set their ticket price.



LEO:  Everybody's embarrassed.  They're just embarrassed, that's all.



STEVE:  Yeah, exactly.  It is, you're right, it is a bureaucratic kneejerk reaction by people who don't understand any of the technology.



LEO:  Right.  They're just embarrassed.



STEVE:  And so we have to take the control of this out of their hands.



LEO:  We hope cooler heads prevail, and this poor kid gets out of jail.  Hungary is sliding badly into authoritarian government, like others in this world.



STEVE:  Which will remain nameless for the moment.



LEO:  Yeah.  But so I don't have the highest hopes.  This is kind of a symptom of authoritarian government.



STEVE:  And in a sort of related case of, whoops, that's really not what we meant, and I heard you guys talking about this on Sunday, whether or not ISPs like it, Title II is still in effect.  And Verizon was recently caught deliberately violating it.



LEO:  Yeah, mm-hmm.



STEVE:  Now, they're claiming, after the fact, that they were...



LEO:  Just a test.  It was just a test.



STEVE:  Now, but they didn't announce it beforehand.



LEO:  It was a test to see if we'd get caught.



STEVE:  Yes.  They didn't say, oh, we'll be running some tests next week.  In fact, it even took them a while to generate their corporate response to being outed.  They said they were testing performance optimizations for video content on their network, whatever that means - actually, I think we know what that means, slow down video - and the effect was clear.  Netflix, YouTube, and other video streaming services were being throttled.



This was not disclosed by Verizon until after it was discovered and became public.  Users who were achieving 30Mb download on an LTE connection of non-video content were measuring a reduced Netflix data rate of a flat 10Mb.  I mean, that's what Net Neutrality is trying to prevent.  Which is to say that Verizon was clearly conscious of the source of the data and was limiting its speed on their network because they could.



Now, also people during this time did some experimenting.  Using a VPN unthrottled the connection, since Verizon was then unable to peer into the traffic and did not know that it was coming from Netflix or YouTube or wherever, and thus could not throttle it based upon its source.  As we covered last week, one of the major ISPs said that they wanted their 2015 Title II classification as a common carrier repealed - and I love this - and that, when that was done, they would honor Net Neutrality voluntarily.



LEO:  Yeah, we'll do it, we'll do it.  Just make sure we don't have to do it.  That's the problem.  Yeah, we don't want to have to do it.



STEVE:  Exactly.  And here we appear to have Verizon breaking Net Neutrality while still under Title II, which makes that unlawful.  So what possible hope is there, if ISPs are released from the legal obligation to treat all traffic equally?  And we know that ISPs don't want users to use VPNs.  You were also talking about that on Sunday.  But if content and source-based traffic shaping, as it's called, I mean, that is the technical term, "traffic shaping," is applied to consumer data streams, an increasing use of VPNs is foreseeable.  Consumers have a tool, and it's very difficult to block because now VPNs can run over port 80 and port 443 and look just like encrypted web traffic.  So here's another interesting battle looming.



And speaking of Net Neutrality, I wanted to note that there was some nice coverage by Jon Brodkin in Ars Technica, and I'm so glad that this hearing is going to be on a Thursday and not on a Tuesday.  The biggest websites and the biggest Internet service providers are being summoned to Congress to testify about Net Neutrality.  The Chair of the House Energy and Commerce Committee, U.S. Representative Greg Walden, who's a Republican Representative of Oregon, said he's scheduling a full committee hearing titled "Ground Rules for the Internet Ecosystem" that will be set for Thursday, September 7th, so a ways away still.  I'm sure we'll be reminding our listeners.  This is one committee hearing I'm going to be watching.



This Tuesday morning, during an FCC oversight hearing, he said:  "Today I'm sending formal invitations to the top executives of the leading technology companies including Facebook, Alphabet, Amazon, and Netflix, as well as broadband providers including Comcast, AT&T, Verizon, and Charter, inviting each of them to come and testify before our full Energy and Commerce Committee."  Now, of course we know the question is - and this is the point you brought up, which was exactly right, Leo, last week.  The question is whether this is all just political theater, and whether the lobbyists may have already won this battle, and testimony is only being taken for face-saving purposes.



But also, as we said last week, for better or for worse, what we need here is clear and clean law, rather than the vicissitudes of presidential appointee mandates which are changing everything.  And Walden did say, he said he wants Congress to step I, and said both ISPs and websites should weigh in first.  He said:  "It's time for Congress to legislate the rules of the Internet and stop the ping-pong game of regulations and litigation."  Yay.



LEO:  Yeah, I think that is the right answer.  I really do.



STEVE:  Yes, yes.  "Given the importance," he said, "of this public policy debate and the work we need to do as a committee, it is essential that we hear directly from the country's top Internet and edge provider leaders who frequently speak out publicly about rules of the Internet.  It's time they came before us" - but haven't they before? - "and directly shared," he says, "their positions and answered our questions."  That's why this is going to be must-see TV.  "With more than a month's advance notice," he said, "I'm sure they can arrange their schedules to accommodate our invitations."  And I'm sure they're going to want to because this is high stakes.  I think this is important.  And, as I said, thank goodness it's on a Thursday.



Okay.  One more, and then we'll take a break, our second break.  Or our second sponsor, our first break.  My browser complained yesterday when I visited McAfee's website.  So there was a little bit of news that I don't know if we covered a few months back.  TechCrunch had a nice piece of reporting at the start of this past April.  They wrote:  "If you were on the Internet in a certain era, you remember McAfee."  And of course none of us have forgotten John.



TechCrunch writes:  "It was the defensive line between you and the rest of the Internet, reminding you with incessant pop-ups that you were not hacked, not quite yet, but only if you renewed your subscription right away.  Intel bought the firewall company in 2010" - and I was thinking, wow, I didn't realize it was that far back, seven years ago - "for an eye-popping $7.68 billion and billed it as Intel Security, and the name McAfee became more closely associated with the company's founder, a man who retired to Belize only to be accused of his neighbor's murder."  And then, as a little bit of trivia, Johnny Depp will apparently be playing John McAfee in an upcoming film.  That should be interesting.



LEO:  Perfect casting, yeah.



STEVE:  Yeah.  But then TechCrunch writes:  "But things didn't work out with Intel" - and then they said, parens - "(or Belize, either, for that matter).  So the unit formerly known as Intel Security will be" - and this is them writing in April - "McAfee once again.  Today, Intel is officially inking a deal that will spin McAfee out, with the asset management firm TPG taking a 51% stake" - so a majority share - "in the company for 4.2 billion," so a little more than it was purchased for in terms of percentage back in 2010 by Intel.  So it didn't jump up in value much.  Intel will retain the balancing 49% share.



Now, here's where McAfee stands.  McAfee currently secures two thirds of the world's 2,000 largest companies and grew its revenue 11% in the first half of last year, in the first half of 2016.  So it's a going concern.  Okay.  Yesterday I go to McAfee.com, right, the enterprise security firm.  And I'm greeted with an across-the-page fixed-position floating bar of text stating that, quote, "Your browser is blocking some features of this website."  This is on McAfee.com.  "Please follow the instructions at" - and then there's a URL - "support.heateor.com/browser-blocking-social-features to unblock these."



So I'm thinking, what?  And I, like, double check.  Am I at McAfee?  And, yeah.  So, like, okay.  I did a little digging and made sure this Heateor deal was legitimate, and I went there, clicked the link that was on the McAfee.com site.  So at the top of the page at Heateor, and this is a tongue-twister, it reads:  "Sassy Social Share, Super Socializer WordPress."



LEO:  Oh, lord.



STEVE:  From McAfee.  Headline:  "Why Is My Browser Blocking Social Features of the Webpage?"  This is dated March 17 of this year, 2017, so it's recent.  "Your browser," I'm reading, "might be blocking Social Features of the webpage you are facing issues with" - okay, the enterprise security firm securing two thirds of the top 2,000 enterprises - "related to loading social content," says this.  And I put in parens here, "Oh, no!"



"If you are using Mozilla Firefox browser [uh-huh, yes] and it has Tracking Protection feature enabled [of course it does] you may have issues," this page reads, "in getting content loaded from social media websites such as Facebook, Twitter, et cetera.  These features include social share counts, social avatars, social comments, and social login."  Yes, all those social things we so desperately need from our McAfee Enterprise Security provider.



And then the page says:  "To get the social content unblocked, you need to disable Tracking Protection of Firefox by following the steps mentioned below."  And then they explain how I go to the location bar and enter about:config, and then "This might void your warranty" page may appear.  Say "I accept the risk..."



LEO:  Oh, my god.



STEVE:  ...to continue.  Then search for "trackingprotection."  Then double-click on privacy.trackingprotection.enabled to set it to FALSE.



LEO:  Oh, please.



STEVE:  Anyway, enough said.  Who knows?  This may have come from an ad.  I mean, it's probably some widget somewhere that some person, I won't use a more descriptive adjective, put there.  Okay. 



LEO:  Is it still there?



STEVE:  Try going to McAfee.  Well, you'd have to have Firefox...



LEO:  I have Firefox.  But I should disable tracking, huh?  Where is it?



STEVE:  Yup.  And then it came up.  And it eventually went away because I was trying to make a screenshot of it, and I had already captured the URL, so I was able to see what it was.  But, yeah, it was...



LEO:  Is that the settings, or do I do that in...



STEVE:  About:config.



LEO:  About:config, okay.



STEVE:  And then put into the search bar "trackingprotection."  And that should eliminate all - it should whittle down all of the possible things...



LEO:  I am getting "This might void your warranty."



STEVE:  Yeah, there you go.



LEO:  But that's coming from Firefox.  That's them saying, you know, if you're messing with this, you could be - let's see.  Track.  Safe browsing.  Tracking protection is not enabled, so I'm going to enable it; right?  Now it's TRUE.



STEVE:  Right.



LEO:  All right, now let's go - let's have some fun.  Let's go to...



STEVE:  McAfee.com.



LEO:  ...McAfee.com.  Oh, yeah, tracking protection.  No, that's good.  Next, okay, that's just Firefox saying what's going on on the page.  That's fine, got it, okay.  Ah, you know what, they must have taken that off.



STEVE:  Scroll down a little bit.  See if you see something not moving on your page.



LEO:  I think they must have gotten a few complaints.  They do have, you know, social sharing widgets at the bottom, like I would share, oh, I was just on the McAfee page.



STEVE:  Oh, you know what it was?  I followed a link there.  This is not - the home page is not the page I was at.



LEO:  Oh, okay.



STEVE:  And unfortunately I don't know what the page was.  It didn't occur to me that it...



LEO:  So I'll browse around.  I'll see if I can get it to fire off again, yeah.  Yeah, they want you to share this on Facebook.  What they need is a like.  They probably want to have a Facebook Like button.  That's probably what it is.  You know, thumbs up, I like this page.  I like it.



STEVE:  Yeah.



LEO:  All right.



STEVE:  So I found the page.  I realized Firefox had the whole history.  And there's a shortcut link that - McAfee's own sort of equivalent of bit.ly.  And so you'd be able to type it in.  It's not coming back up for me, though.  But it did disappear after it had been there for a while.  So https://mcafee.ly.



LEO:  Ah, McAfeely.



STEVE:  Forward slash, and then numeral 2, lowercase u, uppercase A, uppercase A, 8, uppercase S, uppercase X. 



LEO:  Okay.  This is millions of Android devices hit with copycat malware.  



STEVE:  Yup, that's the page.  And are you - and so you do not see it.



LEO:  Oh, I do see it.  



STEVE:  Oh, there it is.



LEO:  Your browser is blocking some features of this fine website.  Please follow the instructions.  I don't blame you for being baffled because who wants to go to H-E-A-T-E-O-R, Heateor.com?



STEVE:  Yes.  And it's darkened the page.  And if you scroll, it's fixed.  And so it's just sitting there like...



LEO:  Oh, this is terrible.



STEVE:  This is McAfee.com.  Two thirds of the top 2,000 enterprise companies.



LEO:  So this is, I would guess, you see the social sharing block on here, it has LinkedIn, Facebook, Twitter, Google+, RSS.  I would guess that they want you to be able to share.  Oh, and they also support Facebook comments.  So maybe that's the issue.  I don't know.  Wow.



STEVE:  Yeah.



LEO:  I think that is so irresponsible to say, you know, why would you want to block tracking?



STEVE:  Well, except - yes, exactly.  They're taking you to a link telling you to void your Firefox warranty and turn off tracking protection.  It's like, okay.



LEO:  I don't want that stuff, yeah.



STEVE:  That's good security.  Thank you, McAfee.  Maybe John is around more than we think.



LEO:  Well, you know, I've recommended against this company for years.  It's not - it's dubious value.



STEVE:  Okay.  So here's a headline that just takes your breath away.  "Roomba maker..."



LEO:  Oh, yeah.



STEVE:  "...preparing to sell maps of your home to advertisers."



LEO:  A new form of revenue.



STEVE:  You just can't make this up.  Yes, it's funny, too, because Mark Thompson, a good friend of both of ours, was testing the roaming features of various floor vacs, and he actually showed me the - I don't know how he even did it.  But he was somehow, like, tracking the pattern of motion of various models and demonstrating, like he had screenshots of the entire path that a Roomba took through his home versus something else.  And it was surprisingly competent in its navigation.  I was very impressed with what I saw.



But so yesterday iRobot CEO Colin Angle announced plans to sell maps of users' homes to advertisers.  In 2015, iRobot started selling Roomba models capable of mapping homes, so the vacuums would know where they should go and stop bumping into furniture and other things.  Until now, these maps have been kept and used only internally on the device to aid its navigation and its understanding of the environment.  But iRobot realized there was a monetization possibility there and now plans to upload the maps of its customers' homes to its servers, from where they will be sold to online advertisers, apparently not endpoint advertisers but companies like Amazon, Apple, and Google.  As I understand it, and as Bleeping Computer reports, the primary buyers aren't regular ad companies, but makers of smart home voice assistance like the Amazon device, the Apple device, and the Google device.



The idea is, and it's a little unclear to me, that these companies would buy this data, that is, the mapping data of the homes their devices are in, because you can imagine there's probably a relatively good correlation between people who have those and also have Roombas roaming around.  They would buy the data and combine it with the telemetry they already obtain from their devices to build more sophisticated user profiles that they in turn - like maybe square footage.  Bigger houses are more valuable and so forth, more TVs in them, who knows.  In turn, they can sell, down the road, the plan is, to classic advertising companies or offer...



LEO:  This is made up.  Wait a minute.  Slow down.  This is what Bleeping Computer is assuming.  This is not what's going on.



STEVE:  Correct, correct.



LEO:  So I'll give you an example.  Wouldn't it be nice if your stereo knew how to shape your acoustics?  This is, by the way, what Apple's Home Hub will do, based on furniture positioning in your house.  It's not, I mean, really, is there a big privacy violation knowing where your divan is in the house?  And, by the way, Roomba says we don't do it currently.  We're thinking, we're seeing if there's interest in it, and we will of course seek proactive approval before we do it.



STEVE:  Right.



LEO:  But I don't - I think that, you know, you can think of all sorts of, I guess, all sorts, a few nefarious uses for this.  But I can think of some very useful uses for this.  Or even just what size should a sofa be?  If we're going to make sofas, what size do people want sofas to be?  Are we making them too big?  Can we get around the sofa?



STEVE:  And again, I always come down, you know, you and I do differ on the privacy aspects.  But my position has always been constant.  As long as the user is informed, I have no problem with this.



LEO:  Right, right.



STEVE:  If it's proactive, and they are giving permission for the floor plan of their home to be exported and out of their control, then I don't have a problem with that.



LEO:  I had the same reaction when I read it, like what?  But I'm trying to think of, yeah, of course you would - and as we know, companies often bury this stuff in Terms of Service.  And we've got to pay attention to this and make sure the Roomba does ask for permission.  And, by the way, the company's started selling military robots.  So who knows?  This might be an urban assault feature.  I don't know.  But as long as you're informed and you can opt out of it - I think the default is to opt out.  But again, that's what we prefer.



STEVE:  Well, no.  They want - this is the problem, Leo.  There is pressure, now that they turn it into profit, there is pressure for them to get it from their users.



LEO:  Right, sure.



STEVE:  So this tilts the balance in a way that's uncomfortable.  And we don't know how the maps are going to be protected.



LEO:  Right, right.



STEVE:  Maps are going to be now - the maps of your home will be out of your control in the cloud, subject to search warrants, so that the government can say we want to get a map of this person's home.



LEO:  But what would they do with that?



STEVE:  I'm just saying this is what happens.  All of your other hosts are where I am on this, Leo.  I've listened to them say...



LEO:  No, I know they are.  But I think there's a certain amount of techno panic, like oh, they can do this.  Oh, my god.  But I would like people also to think about, well, okay, I understand just kind of the default kneejerk response is nothing about me can be discerned by anybody.  But what's the, I mean, really, what are they going to do with this that's going to harm you?



STEVE:  I don't disagree.  My whole deal is make sure people know because it'll be very interesting to see, when this happens, how iRobot handles it because they just need to do it responsibly.  None of us...



LEO:  Well, frankly, they've announced that they're going to do this.  So clearly they're not doing it in the shadows.  They say they're going to do it; right?



STEVE:  Correct.  Correct.



LEO:  They didn't hide it.



STEVE:  Well, okay, except the CEO announcing it is very different from their consumers knowing it.



LEO:  Right.



STEVE:  Those are two different things.



LEO:  No, no, they need to - I agree.  They need to very clearly specify this, yeah.



STEVE:  Right.  Yeah, so anyway, I love it as content for this podcast because what we're seeing is we're seeing this creeping technology Internet-connectedness of virtually our entire, what was our private life, with devices that are listening to us and looking at us and recommending what we should wear and adjusting themselves to the size of our room, thus knowing the size of our room, and what's the contents of our refrigerator.



Again, I'm not saying that it's clear, but this is sort of this creeping exportation of or exfiltration of every manner of information.  And it does get interesting when you start aggregating it.  I mean, apparently that's what these home technology providers want to do, Amazon and Google and Apple.  They want to, ooh, look, here's some additional interesting information we will now be able to purchase, and we'll know who it is because they all have relationships with the users whose homes their devices are in.  So they'll associate the person's home floor plan with their devices.  And again...



LEO:  I can see why Amazon might want that for more effective array mics, things like that.



STEVE:  And actually you came up with some good use cases, like, oh, look.  Couches are generally too large for people's homes; okay?



LEO:  Right.  I'm challenged to think of a nefarious use case.  I mean, you came up with one, which is, well, then we know you have a fancy home.  But it's just a map of the floor plan.  It's not - I guess they know what your square footage is, but they can go to Zillow and find that out.



STEVE:  Very good point.  Very good point.



LEO:  I don't know - yeah.  You know, in general I'm not too worried about stuff like that.



STEVE:  All I want to do is just kind of keep an eye on it.  From my standpoint, I mean, I have split levels, and I don't have anything crawling - and lots of little rugs.  So it's like a death trap for a Roomba here.  So I'm not being mapped, and I don't have any of these things listening to me.  I don't need them.  It's not a thing that I do.  But just, you know, interesting evolution of our environment.  And again, the idea that something that began mapping a few years ago, so that it could do a better job of mapping, and on that definition there's been some definition drift.  Now it's, oh, look, we just realized we could upload these floor plans to our servers and sell them.  So, okay.



The end of life for Flash has actually been scheduled, finally.  And 2020 cannot come soon enough.  Google has a blog.  Mary Jo covered this in her column on ZDNet.  And of course I refuse to run Flash anywhere.  It's one thing to require it for video playback.  I get that.  Although, as we know, it's been possible to play video with pure HTML on a browser for so long now that there's really no excuse for requiring it.  My site has been playing video Flash-free for years.



But I'm also confounded by non-video sites that have Flash helpers of one sort or another where, for example, my browser complains that a page is apparently attempting to run Flash for non-video reasons.  I, of course, just say no and never wonder any further because I don't want Flash to run because we know the most likely scenario, and we've covered this on this podcast for years, is that an embedded advertisement is the culprit, and accepting third-party advertisers' Flash script is the last thing you want to do.  Flash ads have been, and we've covered them for years, the primary vector of malvertising infection.



And wouldn't you know it.  While putting this show together, I go to the Hacker News, which is TheHackerNews.com, a great site.  They have super nice coverage, and their banner is The Hacker News:  Security in a Serious Way.  And my browser drops down a little bar saying "Allow http://thehackernews.com to run Adobe Flash."  And it's, what, you know?  So something there, and I don't know if it's their widget or a third-party widget that they've linked to, but my browser is saying this site wants to run Flash.  How do you feel about that?  And again, it's not providing any value to me, so of course I say no.



Okay.  So this morning, this Tuesday morning, Google's Chromium team blog headline was, in homage to what I loved that Douglas Adams called the "fourth book in the 'Hitchhiker's Guide to the Galaxy' trilogy," Google's Chromium blog was "So long, and thanks for all the Flash."  They said:  "This morning, Adobe announced their plans" - oh, it must have been yesterday morning - "to end support for Flash late in 2020," so three years from now.  "For Flash developers this will mean transitioning to HTML, as Chrome will increasingly require explicit permission from users to run Flash content" - now, that's from now until then - "until support is removed completely at the end of 2020."



This is Chromium's blog.  "HTML is faster, safer, more power-efficient than Flash, and works across desktop and mobile.  Three years ago," they said, "over 80% of Chrome's daily desktop users visited sites containing Flash.  Today, only 17% of users visit sites with Flash."  So in three years there's been a drop from 80% to below 20, down to 17.  And they said, "And we're continuing to see a downward trend as sites move to HTML."  They wrote:  "We strongly encourage sites that still rely on Flash to make the move to HTML as there will be an increasing number of restrictions on Flash leading up to the end of support."  So as Google always does when they do a campaign of this sort - we saw this with the SHA-1 certs; we've seen this with them working against the irresponsibility of some certificate authorities in the past - they do this in stages.



Mary Jo wrote that Adobe finally has drawn a line in the sand, noting that Flash will no longer be supported after 2020.  Microsoft officials - so this is Mary Jo reporting on Microsoft - said that they'd do their part to wind down Flash support in the company's Internet and Edge browsers, so that Flash support will be entirely removed from Windows by the end of 2020, as well.



She said:  "Flash in Edge already is only click-to-run," she said, "as of the Windows 10 Creators Update.  Today Microsoft posted," she wrote, "its timeline and plan for getting rid of Flash over the next three years."  So through the end of 2017 and into 2018, Microsoft Edge will continue to ask users for permission to run Flash on most sites the first time the site is visited and will remember the user's preference on subsequent visits.  So that sounds like a proper tradeoff, not to overly harass people who really do still want to run it and need to.  IE will continue to allow Flash with no special permissions required during this time.



In mid to late 2018, so around this time next year, they write, "we will update Microsoft Edge to require permission for Flash to be run each session."  So they'll take away the sticky memory of your previous decision, and IE will still continue to allow Flash for all sites throughout 2018.  Around this same time in two years, mid to late 2019, they write, "we will disable Flash by default in both Edge and Internet Explorer.  Users will be able to reenable Flash in both browsers.  When reenabled, Microsoft Edge will continue to require approval for Flash on a site-by-site basis."



By the end of 2020, finally, in sync with Adobe's final cutting off all additional support, they say, "we will remove the ability, completely remove the ability to run Adobe Flash in Edge and IE across all supported versions of Microsoft Windows.  Users will no longer have any ability to enable and run Flash."  And then Mary Jo concluded, saying that Google, Mozilla - so also Firefox - and Apple are also committing to dropping Flash support by 2020 and probably do something similar.



So yay, you know, HTML can do everything now that Flash was needed for.  And there was arguably a good need for it.  Back in the day, it was the way we first did video and first did lots of things that HTML could not do.  Now it's just built-in native.  And it's become much more of a security concern than it is a benefit.  And as Google said, it takes up space, consumes power, blah blah blah.  And of course I guess that must be Apple on the Mac because iOS has always been Flash-free from day one, yeah.



LEO:  Right.  No, it's Safari, yeah.



STEVE:  And I just - I love this.  A number of our listeners, a bunch of our listeners, I think, are still using - have not let go of Firefox, as indeed I have not.  There's good news I just wanted to briefly share to those of us who are sticking with it and who enjoy organizing with tabs, as we know I do.  I've actually - I used to be keeping about 212 tabs open, from literally as far back as when I had paused the work on the next version of SpinRite in order to switch to SQRL.  They were all still there, all the tabs I had open at the time, because that's just sort of how I manage my stuff.  You should see my desktop.  Or, no, actually you shouldn't.



Anyway, June's NetMarketShare shows Firefox commanding now only a 12% share of the browser universe.  And as we know, Chrome has been eating everyone's lunch.  Chrome is nearly 60%.  They're at 59.5% of the market share.  And I guess a lot of that might be, what, Android and Chromebooks.  But a lot of Windows and Mac users are also Chrome users.  So it is a popular add-on browser.  And certainly, whenever you go to Google, if you hit the home page, they're trying to get you to use Chrome, if you're not there with a Chrome browser.  I see that all the time.  So they're trying to push their browser.  And we know it's a good browser.  It's got a great security model.  My problem with it is it is seriously resource intensive.  I mean, it gobbles memory.  So I just - I can't run it.



LEO:  You find that Firefox is more efficient now?  It used to be a memory hog.



STEVE:  Yes.  And so here's what's happened.  If you're interested, Leo, the link under this, the TechRadar link has some graphs showing the evolution of Firefox in terms of performance and memory over time.  Mozilla's so-called "Quantum Flow" project is bearing fruit in the next release number 55 of Firefox.  When loaded down, get this, with a massive test case of - and this even makes me tremble.



LEO:  I did read this.



STEVE:  1,691 open tabs.  I can't even imagine it getting off the ground.  1,691 open tabs.  Okay.  But it actually does.  The current version of Firefox number 54 required four minutes to start and consumed 2 gig of system memory, which actually is immense, seems low to me, but anyway.  By comparison, the next release, Firefox 55 with Quantum Flow technology, started the same daunting test set of tabs, not in four minutes, but in 15 seconds, and consumed less than half a gig of RAM.  So dramatically faster and dramatically reduced.  I think there's a second graph somewhere I saw.  It must have been on some other...



LEO:  I thought I saw that, too, yeah.



STEVE:  Yeah, some other reporting of this.  So anyway, that's all I had to say.  I get it that we're now the minority browser.  I love my Firefox.  I love my tabs on the side.  If Chrome - and I'm a little constrained because I'm still in a 32-bit OS, so I'm capped at 3 gigs.  I just, as we know, the next machine I build is either 64 gig or 128 gig of RAM, so it will be Chrome friendly.



LEO:  I thought you built your last machine.



STEVE:  Oh, I did, but I'm not using it yet.  It'll take me too long to switch over, and I've got to get...



LEO:  You actually built that machine, but you're still not using it?



STEVE:  Yeah.  Oh, because it'll take a chunk of time to get it set up and configured.



LEO:  It's not as a server.  It's your desktop; right?



STEVE:  Yeah, yeah.  But remember, it doesn't run any 16-bit code, Win7 doesn't.  And I've got legacy stuff.  Like Brief, I can't use Brief under Win7.



LEO:  No, yeah, you can't.



STEVE:  Which is my code editor.



LEO:  You need to just set up a virtual machine with DOS 5 on it.



STEVE:  And now we're starting to understand why it will take me time, which is why I haven't made the switch yet.  Okay, so...



LEO:  Actually, be an interesting project to have everything run in a container isolated within that system.  I think you could do that.



STEVE:  Well, and Qubes, the Qubes OS.



LEO:  Qubes OS, oh, yeah, that's how it works.



STEVE:  Very much like that.



LEO:  But I bet you you could have Windows 10 running and have Docker containers, or maybe Hyper-V containers, but I think Docker containers for all the different apps you want, including DOS.



STEVE:  Yes.  And that system is built.



LEO:  It'd be all isolated, sandboxed.



STEVE:  That system is built to be a VM host.  That's why I have, you know, someone says "64 gig?"  It's like, yes, so that I can have many large gig OSes running at the same time.  So that's the plan.  And that will be my last system.  But I'm not using it yet.



LEO:  But is it assembled?



STEVE:  It's right next to me, yeah.  It's over there.  Remember I showed pictures, and I talked about...



LEO:  It's just dark, huh?



STEVE:  Yeah, I just, I haven't, I mean, it hasn't been updated in like, I don't know, 18 months, ever since I - I just, you know... 



LEO:  Now, are you going to - you're not going to put Windows 10 on it, then.



STEVE:  Oh, it's got 7 already set up.



LEO:  Seven, okay.



STEVE:  I will never go to Windows 10.  And I will clarify a statement I made in the errata, I think it is, in a minute, about Windows 10.  But first, the FCC has just approved a sizable new chunk of radar spectrum for use by vehicular environment sensing radar.



LEO:  Ooh.



STEVE:  Yes.  This will enable the use of reduced cost and increased precision sensors in our next generation of autos.  As we know, many consumer vehicles already use radar.  Even if they're not going to drive themselves, they use it for collision avoidance, automatic lane-keeping and so forth.  But right now, the Washington Post writes, vehicular radar - oh, and my god, the pun of the year.  I take my hat off to Brian for this.  I'll get to it in a second.



But he writes:  "Vehicular radar is divided into a couple of different chunks of radio spectrum.  Last Thursday, the Federal Communications Commission voted to consolidate these chunks and added more to allocate additional bandwidth to vehicular radar.  FCC Commissioner Clyburn said:  'While we enthusiastically harness new technology that will ultimately propel us to a driverless future, we must maintain our focus on safety, and radar applications play an important role.'"



So then Brian Fung wrote, again, this is just such a good - this is the best pun.  He said:  "Thursday's decision" - that's last Thursday's decision - "by the FCC lets vehicle radar take advantage of the spectrum ranging from 76 GHz to 81, reflecting an addition of four extra gigahertz."  Oh.  Because of course that would be an awkward phrase if you didn't realize, of course, that radar is a reflective technology.  Radar works by reflection.  So anyway, Brian...



LEO:  Reflecting it at - you think he meant that?



STEVE:  Oh, he had to.  "Reflecting an addition of four extra..."



LEO:  Extremely nerdy.



STEVE:  It's wonderful.  You wouldn't say "reflecting an addition of four extra gigahertz" unless you knew what you were saying.



LEO:  Wow.



STEVE:  And the fact that radar is reflective, anyway, it's like, oh, bravo.  Anyway, what we have had before is 24 GHz.



LEO:  I think his math is off, though.  But other than that - is that five?



STEVE:  Oh, you're right, it is.



LEO:  Okay.



STEVE:  That would be five.



LEO:  He got the pun, but he didn't get the math.



STEVE:  Yeah.  Or, well, actually four extra, I'm not sure what the range at 24 gig is.  Because it didn't show a range.



LEO:  Oh, so they might have lost, yeah, so they probably lost some, yeah.



STEVE:  Yeah.  But what's important is the band.  What I wrote is, I said, I'll note that the increase in radar frequency - because we're going from 24 gig up to 76-81.  The increase in radar frequency is significant.  From an engineering standpoint the move from 24 to 76-81 significantly increases the resolving power of the radar, and the higher frequency means smaller and more efficient devices.  We're all familiar by analogy with audio speakers, famously known as woofers and tweeters.  We know that to produce low audio frequencies in free air requires a large diameter speaker cone, commonly known as a woofer, but that higher frequencies can be efficiently generated with smaller speakers.



This analogy holds at microwave frequencies, where this factor-of-three upward jump in frequency means a factor-of-three decrease in wavelength, which allows for many more array sensors within the same area.  So this is a win both for radar assist and for future autonomous driving.  So yay to the FCC for formally making this new chunk of spectrum, very high-frequency radar, available.  It will give our cars better eyes.



LEO:  Good, good. 



STEVE:  I did want to remind our listeners, I got a ton of appreciative feedback from our listeners about the Humble Book Bundle that we talked about last week.  I just wanted to give another reminder in case somebody said ooh, and then like forgot about it.  I gave it this week's bit.ly link to help anyone get there:  bit.ly/sn-621.  So this episode number, 621, all lowercase, sn-621.  That will take you there.  Just under six days remaining.  It was six days at 11:00 a.m. this morning Pacific time, so by the time you're hearing the podcast next week it's over.  So I just did want to let everyone know.  And a listener built a Humble Bundle Downloader that makes it easy to grab all of the bundle assets and other materials that are associated.  It's at GitHub.  I imagine you could just google or search "humblebundle-downloader" at GitHub, and I also have the link in the show notes for anyone who's interested.  So thanks for that.



LEO:  Yes, indeed.



STEVE:  So first piece of errata.



LEO:  Before you do that, I might have an errata.  You never showed the Picture of the Week.



STEVE:  Oh.



LEO:  And it's so good, I didn't want it to slide by.



STEVE:  Oh, I'm glad you reminded me.  Yes, yes, yes, thank you.



LEO:  Not to mention timely, yeah.  



STEVE:  Thank you.  Let's do that, and then we'll take our last break.



LEO:  Okay.  Okay.



STEVE:  Yes.  Oh, thank you, Leo.  You loved it, and I loved it, but nobody else got to hear about it.



LEO:  It was just the two of us, yeah.



STEVE:  That's right.  So this is so perfect.  This is a picture, several people sent it to me, and I got a couple of them really compressed, but I got a good one.  This is a placard standing up on the little sponge mat on the counter at the UPS Store in Las Vegas.



LEO:  This week, yeah.



STEVE:  This week, yes.  And this placard, it's got The UPS Store, and then their logo printed in color.  It says:  "Due to the DEF CON Hacking Convention, we will be accepting email print jobs with attachments only.  We will not accept USB prints or any links."  And then, down below, "We apologize for the inconvenience."



LEO:  I'm not sure that's actually better, but okay.



STEVE:  Well, and they had to have been bitten in the past.



LEO:  Maybe that's it, yeah.



STEVE:  It had to have happened.  And so they're like, okay.



LEO:  Oh, those hackers.  Oh.



STEVE:  I mean, maybe they're proactive, and they're just really on the ball, and they realize the danger.  Or maybe some scary-looking people came in.



LEO:  So funny.  So funny.



STEVE:  Oh, wonderful, wonderful.



LEO:  Don't fall for those spurious links.  You might be pulled to a McAfee page or something worse.



STEVE:  Okay.  So it turns out a report that everybody covered, including we here, was completely specious.



LEO:  Oh, no.



STEVE:  Yes.  No audio device called the police when that guy was threatening his wife with a gun.  Wired magazine, fortunately, followed up on what turned out to be a widely reported, entirely erroneous story about an unnamed home audio device which we'll remember was originally believed to be a Google Home unit, and then people believed it was the Amazon Echo, autonomously responding to a very loud and fraught domestic dispute by phoning the police.  Except it never happened.



LEO:  Yeah.  I was a little suspicious, I think you'll remember, yeah.



STEVE:  Yes.  I mean, but there wasn't, well, and the reason everyone believed it is it's what the authorities said.



LEO:  Right.



STEVE:  So Wired wrote:  "Despite what you may have heard, an Amazon Echo did not call the police earlier this week, when it heard a husband threatening his wife with a gun in New Mexico.  On Monday, news reports took Bernalillo County authorities' version of those events credulously, heralding the home assistant as a hero.  The alleged act also raised an important question:  Do you really want to" - and they're not coming down on either side of this.  "Do you really want to live in a world where Alexa listens to your conversations and calls the cops if she thinks" - ooh, I said the word, sorry.



LEO:  That's okay.  She's calling the cops right now.



STEVE:  "If she thinks things are getting out of hand.  The good news is that you don't live in that world.  Amazon's device" - and I've replaced her name with that word.  "Amazon's device can't, and did not, call 911.  Google Home can't do it, either. No," writes Wired, "voice-assistant device on the market can.  That doesn't invalidate the core question, though, especially as Amazon's Echo, Google Home, and their offshoots increasingly gain abilities and become more integral to everyday life.  How intrusive do you want to let these devices be?" asked Wired rhetorically.  "Should they be able to call the police?  Maybe not even just when specially prompted, but because they may have heard, for instance, a gunshot."



Okay, so "The Bernalillo County incident almost certainly had nothing to do with Amazon's Echo, but it presents an opportunity," Wired writes, "to think about issues and abilities that will become real sooner than you might think."  And so anyway, "The Sheriff's Department reported specifically that, when a man drew a gun on his wife in a home where an Amazon Echo was placed, he said to her, 'Did you call the sheriffs?' and the Echo misinterpreted that, they said, as a command to call the sheriffs, who then showed up at the front door.  The authorities later clarified that someone in the house could be heard in the 911 recording yelling the A-word, 'Call 911.'"



LEO:  Ah.  They'd already called 911.



STEVE:  Well, so they're now saying somebody was heard instructing the Amazon Echo to do that.  Then Wired says:  "That could not have happened, either.  Amazon's Echo requires first a wake word to activate."  As we all know, the default is the A-word, but you can also customize it to Echo, Amazon, or Computer.  "And while they can make calls, an Alexa-powered device can only call another Alexa-powered device.  Not only that, but it can only call other Alexa devices" - god, I keep saying the word, sorry, because I'm reading the text that I wrote - "can only call other Echoes..."



LEO:  Just say Echo, yeah.



STEVE:  "...that have enabled calling and have been added to your contact list.  Most importantly, these exchanges don't take place over the public switched telephone network, the worldwide network as we know that allows wireless and land phones to actually make calls.  In other words, the sheriffs would have needed an Echo device of their own for that to ever work, one that the couple in the domestic dispute had in their contact list," which frankly seems really unlikely in this case, in the case of that household.  "Later, the police said that the Echo device was used in combination with some kind of home phone or cellular," I mean, I don't think these guys know one end up from the other.



LEO:  They don't know what's going on.



STEVE:  Yeah.  And so they're, like, they keep changing their story.



LEO:  "Oh, I think it did."



STEVE:  So we don't know what happened, but I did want to correct the record.  It wasn't the case. 



LEO:  The Roomba did it.



STEVE:  It might have been sweeping in the room and looked up.  Oh, my goodness.  Okay.  So some people were confused...



LEO:  Oh, there's my front door.  Hold on, let me check this.



STEVE:  Some people were confused about the example I used of how ISPs could acquire certificate interception capability when I used Google's ability to mint their own certificates as an example.  So I just wanted to correct the record and make sure that people did not think that I was suggesting that Google was minting certificates for domains they don't control.  Absolutely not.  That was never my intention.  And I'm sure they never have and never would.  They're the good guys.  But they are minting their own certificates, and that means they could.  And so I wanted to make that distinction very clear.



So that an ISP, if it were decided that that was what we wanted, and this was the technological architecture that I wanted to explain, an ISP could be given on-the-fly certificate-signing capabilities for domains they don't control, namely the ones their customers are visiting.  So I just wanted to make a very clear line there, that I used Google as an example of somebody whose certificate is signed by a CA where the certificate that was signed has CA authority, and an ISP could get such a certificate.  They don't have them now, we assume and hope.  But it's possible.



So I only meant that to explain the technology and that it wouldn't require, as I was suggesting previously, that all of our devices accept a root cert from our ISPs.  There's a much worse and more powerful, but more practical from a technology standpoint, solution.  Also, I thought that iOS's update 10.3.2, which we talked about last week, fixed that Broadpwn bug.  Remember we talked about how Google's patch fixed it in Android.  That was the baseband firmware problem found in Broadcom's WiFi chip that allowed anybody using basically a QoS-style packet, just you don't even have to log onto the access point, you just had to be in contact with its radio, and your vulnerable device could be owned, that is, could have code executed on it.



Anyway, it turns out that there was a fix related to the baseband firmware in 10.3.2, but not that one.  It was the subsequent 10.3.3 iOS release, which just came out, which fixed that and a whole bunch of other problems.  So I wanted to correct the record.



Two bits of crazy miscellany.  One is that I've seen the Season 2 trailer of the next, well, Season 2 of Netflix's "Stranger Things."  So the good news is we're going to get one.



LEO:  Yeah, yeah.



STEVE:  We knew we were.  And the boys are back.  And this is one of those where you want them to create a lot of content quickly so they don't grow up because they're at the perfect age right now.  And I don't want them to be teenagers because it just won't work as well as it does with them being in elementary school.  So we are going to get a second season.  Not till October, but the first trailer is out.



And we talked at length last week about the various sites which are blocking our password managers and just cutting, copying, and pasting into forms.  There is an extension available for both Firefox and Chrome, which I can't give the name of on the podcast because the second word is the F-bomb.



LEO:  Oh.



STEVE:  So Don't "F" with Paste.  And so expand the "F" to the word we all know it stands for:  Don't F**k with Paste.  That is an extension available for Firefox and Chrome and maybe elsewhere.  I know of those two because I've had users who are listeners tell me they're using it for both of those different browsers.  So apparently there is a way to overcome that annoyance with that extension, which un-effs what the website has done, which is handy.



LEO:  Don't "F" with Paste.



STEVE:  So, and I have one of those real heartwarming SpinRite data recovery stories.  We've been talking a lot about the technology of SpinRite recently and what it does and data recovery and SMART data and how SMART data is only useful when the drive's under load, and numbers of ECC and all that kind of stuff.  Steven Almas sent me just one of those where I'm just like, I'm so glad it does these sorts of things.  He wrote:  "My neighbor, who is a single mother, asked for my IT help.  She had an external hard drive with 60,000 photos of her family and children, which was her only copy."  And you know where this story is going.



LEO:  Oh, oh.



STEVE:  "The hard drive did not mount on any computer, and she was very upset."  60,000 photos.  He said:  "I suggested that we purchase SpinRite and try that out.  Since I work in IT, I took a dedicated machine and ran SpinRite on her external hard drive, and SpinRite was able to recover all but 13 of the 60,000 photos."  He said:  "She now has a comprehensive backup solution in place, and we are forever grateful to Steve Gibson and SpinRite."  And he signed off saying, "Thank you from another Steve."  So Steve, thanks for sharing it.  And again, that's just so cool, 60,000 family photos of her and her children that would have been lost.



LEO:  Wow.



STEVE:  And, yes, SpinRite's $89.  But you have it for life.  I'm never letting it die.  As soon as SQRL is put to bed, I will be right back to it.  And as I have said, everyone who has a version of 6 will be able to play with the next release before its formal release, and there will be no charge for the updates that I do in succession to 6, creating .1, .2, and .3 and so forth.  And then my plan is to take it down completely and rewrite it from scratch, adding a whole ton of new features to it.  And again, of course, even people who have SpinRite 1 are able to upgrade to 6 today.  So I will be honoring upgrades moving forward.  So anyway, Steve, thanks for sharing that.  And, boy, that's so cool, to be able to recover, what would it be, 59,987 photos out of 60,000, all but 13.



LEO:  And the 13 he couldn't recover, that's just because those were the damaged sectors, or some [crosstalk] photo lived on it; right?



STEVE:  Well, yeah.  What this says is, and we see this with external drives, they get kicked around.  



LEO:  Yeah.



STEVE:  They fall off the table.  They get knocked over if they're standing up on edge.  They tend to get beaten up.  So my guess is that there was probably growing, accruing damage over time, and the drive was struggling.  And again, because there was no maintenance being performed, I mean, had she already had SpinRite, this would have - there would have been zero loss because, if you run SpinRite occasionally, it proactively, as we know, it maintains this and prevents it from happening.



So I'm glad you brought that up, Leo, because the fact that there was not even complete recovery says there was a lot of damage.  And so SpinRite did everything it could and no doubt struggled, even on those final 13 photos, but just finally said they're just gone.  The data is not here.  The head was bouncing around on top of them when it fell off the table or something.  And so, I mean, I really see that with removable drives because people don't appreciate them.  When it's in a computer, even in a laptop, the inertia of the laptop tends to protect the drive, which is shock-mounted inside, to varying degrees.  There isn't much room for much shock mount.  And so laptops are a problem, too.  But the problem is, if it's a little removable drive, it's just - it's under real G-shock threat.



LEO:  Mm-hmm.



STEVE:  Okay.  Two quick pieces of closing the loop.  Actually, one of them is quick, and one is not.  But we have time.  Ned Griffin and several others said @SGgrc - so it was a tweet.  He said:  "Steve, did I hear you say 'my Win 10 machine'?  Never thought I would hear you are using a flying turd OS."  Which of course I did famously characterize Windows 10 as at some point in the future, I mean, at some time in the past.  And so, yes, I have a Windows 10 machine, and I have been talking about Creators Update and the changes that are coming and so forth.  And I recognize I have my own bias, but that can't be allowed to influence unduly my reporting, for the purpose of this podcast, what's going on with Windows 10.



So I've got a Win 10 machine.  I updated just because I wanted to experiment and see that "I forgot my password" link that we were talking about coming to the login lock page.  And of course I'm also - I need to make sure that SQRL works seamlessly and correctly under Windows 10.  So it does.  But I only know that because I've been testing it all along the way.  So, yes, I'm sitting in front of XP.  But I do have all versions of Windows around me because I'm a developer, and I need them. 



Also, a couple of our listeners, and this is an ongoing flux in my Twitter feed from our listeners, is when they see a site with obviously bad security practice, they just shoot me a tweet saying, hey, just thought you'd be interested in knowing.  So, for example, while I was pulling this show together, I ran across someone who sent me a note saying that the gov.uk website allows a maximum of 12 characters and no symbols.  And then someone else sent a login page screenshot that showed a range.  I think it's maybe like eight to 15.



And so I just wanted to take a moment to put this into perspective.  This is sort of an opportunity.  This is well known to a lot of our listeners, but I know we have people who haven't been listening forever.  So I wanted to remind everyone that, while these sorts of limitations pose a definite concern, because they give us cause for concern about the underlying security practices of the site's design and technology, in and of themselves they are insufficient evidence either way.



That's one of the things we need to remember.  We're just seeing part of the front of a much deeper process.  The part we're seeing does not give us confidence, but it also doesn't tell the whole story.  For example, a site could allow a super long password, no limit, type until you're exhausted, but then store it directly in plaintext, which the user submits.  So if their database were to leak, and we have covered account database leakage for years here, it would be game over for all of a site's users, despite the unlimited password complexity offered upfront.  So there's an example of it really doesn't matter how much space they give you.



And on the flipside, a site could restrict their input password to, say, just eight characters, then pair that with a per-account large random nonce, which we know prevents a single attack against all of the site's passwords.  If then that nonce is combined with what the user provided and is run through, as would be possible, a monstrous memory-hard, acceleration-resistant, like five-second-long hash process to make every brute force guess, whether offline or online, impossibly slow and costly.  So that would turn eight characters into impossible to practically break.



So again, we're only seeing the front, and the back could render the most complex password unsafe, or the least complex password arguably more safe, by making its guessing extremely difficult.  So anyway, I just sort of thought, because I'm constantly seeing people saying this, it's like, yes, it's certainly the case that any site telling you to fix the limit to a certain thing, that's a worry because that suggests, strongly suggests, that they have allocated that much space in their database.  And we all know that, if you are hashing a password, any password as you should, then a hash by its nature turns a variable-length thing into a fixed-length result so that it doesn't matter how long the input password is.  The result is always the same length, and so it can be stored in a fixed-length database field.  So definite cause for concern, but not in and of itself clear evidence one way or the other.



Okay.  Now, this is just sad.  The title of the podcast I named "Crypto Tension."  And this is the TLS v1.3 Explicit Wiretap Controversy.  And before I got myself read into this enough, I was a little concerned because at first blush it looked like our friend Matthew Green, the well-known cryptographer, was endorsing what he's calling "Data Center Use of Static Diffie-Hellman in TLS 1.3."  That's the title of the IETF draft of his analysis of what's called "Static Diffie-Hellman."  I'll explain, of course, all about what this is.  So in his paper, and I've got the link in the show notes for anyone who wants the whole thing because I'm just snipping out a couple of the best parts.



The abstract reads - and this is Matthew Green wrote this.  He's the sole author on this:  "Unlike earlier versions of TLS, current drafts of TLS 1.3 have instead adopted ephemeral-mode Diffie-Hellman and elliptic-curve Diffie-Hellman as the primary cryptographic key exchange mechanism used in TLS."  Okay, now, what he's saying there is that, unlike the earlier ones, which did allow some of these ciphers, but were primarily using RSA, TLS 1.3 is going to be primarily using elliptic-curve Diffie Hellman and ephemeral Diffie-Hellman for its key exchange mechanism.  So he writes:  "This document describes an optional configuration for TLS servers that allows for the use of a static, meaning an unchanging, Diffie-Hellman secret for all TLS connections made to the server.  Passive monitoring of TLS connections can be enabled by installing a corresponding copy of this key in each monitoring device.



"While ephemeral elliptic-curve Diffie-Hellman," ephemeral EC Diffie-Hellman, he writes, "is in nearly all ways an improvement over the TLS RSA handshake, it has a limitation in certain enterprise settings, specifically the use of ephemeral PFS" - and that's, of course, the abbreviation for Perfect Forward Secrecy that we've talked about many times - "cipher suites is not compatible with enterprise network monitoring tools such as intrusion detection systems that must passively monitor Intranet TLS connections made to endpoints under the enterprise's control.  This includes TLS connections made from enterprise load balancers at the edge of the enterprise network to internal enterprise TLS servers.  It does not include TLS connections traveling over the external Internet.  Such monitoring is ubiquitous and indispensable in some industries, and loss of this capability may slow adoption of TLS 1.3."



Okay.  So let's understand.  He, as a cryptographer, he's not taking a position.  He's very involved in the TLS v1.3 working group.  There has been a huge controversy, and I'll get to that in a second, about this, about this request which is coming from the enterprise side ostensibly, to break a set of important security guarantees which were going to be incorporated into TLS 1.3 for the purpose of monitoring, for the purpose of being able to passively decrypt TLS-encrypted traffic. 



So I'm going to skip a bunch and just say that, in this document, the fourth point, and I've skipped over one, two, and three, of security considerations, he says:  "We now consider the security implications of the change described above."  So again, he's just saying, okay, if this is going to happen, let's make sure we don't do something wrong.  Like, let's understand what this means.



He says:  "The shift from fully-ephemeral elliptic-curve Diffie-Hellman to partially static Diffie-Hellman affects the security properties offered by the TLS 1.3 handshake by eliminating the perfect forward secrecy property provided by the server.  If a server is compromised, and the private key is stolen" - and I'll just add, or if it is given - "then an attacker who observes" - that is, passively eavesdrops - "any TLS handshake, even one that occurred prior to the compromise" - or the gift or the servicing of a search warrant - "will be able to recover traffic encryption keys and be able" - that is, in the past - "and will be able to decrypt traffic.



"Thus the modification described in Section 4," Matthew writes, "represents a deliberate weakening of some security properties.  Implementers who choose to include this capability should carefully consider the risks to their infrastructure of using a handshake without perfect forward secrecy.  Static secret keys," he says, "should be rotated regularly."  Yeah.  And of course they should be generated randomly per handshake.  That's the way it is now.  We're talking about breaking that on purpose.



So a great summary was produced by Stephen Checkoway, who's an assistant professor in the Department of Computer Science with the University of Illinois at Chicago.  And I'm going to read this fast.  But I can't skip it because he just - it's beautifully done.  And with the background I just gave you, this will make more sense:  "As the TLS 1.3 standardization process (hopefully)," he says in parens because it's been nine years, "comes to a close, there has been some drama on the TLS Working Group mailing list and at the recent IETF 99 meeting in Prague regarding the use of TLS 1.3 in enterprise networks.  This is a surprisingly contentious and important topic that I suspect many people who don't follow protocol development closely may have missed."  Thus you're getting it on this podcast.



"Transport Layer Security," he writes, "is, without exaggeration, the most important security protocol in use on the Internet today.  It is the successor protocol to the older SSL protocol and is used to cryptographically protect a wide variety of Internet communications including online banking, a significant fraction of email traffic, more than half of all web browsing" - and of course we know that number is going up fast - "and an ever-increasing amount of normal Internet activity.



"TLS is standardized by the Internet Engineering Task Force (IETF) which is organized into a set of working groups.  Each working group has a charter which describes its mission.  The TLS Working Group is currently charged with designing the fourth iteration of the TLS protocol, TLS 1.3.  This multi-year process takes place primarily on the TLS mailing list, as well as in regular, in-person meetings.  The 99th IETF meeting just concluded.  Sounds pretty dry.  What's the drama about?" he says.



"Much of the work is pretty dry and technical.  One of the working group's goals for the TLS 1.3 is to produce a more secure protocol than prior versions, which have had a series of subtle problems."  As we know.  We've covered them.  "To that end, the working group has removed a number of cryptographic options that reduced the security.  This removes options like cipher suites, sets of cryptographic algorithms that work together to secure the traffic that do not provide forward secrecy.



"To quote Wikipedia, 'A public-key system has the property of forward secrecy if it generates one random secret per session to complete a key agreement without using a deterministic algorithm.  This means that the compromise of one message cannot compromise others as well, and there is no one secret value whose acquisition could compromise multiple messages.'"  Perfectly phrased.  "Forward secrecy also generally requires the session key to be destroyed once the session ends to prevent an adversary from decrypting traffic afterwards.  So not only can you not decrypt traffic afterward, you cannot decrypt traffic that occurred beforehand."  They're talking about killing that.  Calm down, Steve.



"Forward secrecy is a very desirable property," he writes - Stephen writes.  Was it Stephen Checkoway?  Yeah.  He writes:  "Forward secrecy is a very desirable property in a cryptosystem. As I recall, when removing the non-forward-secret cipher suites was proposed on the mailing list for TLS 1.3, there was broad consensus.  At some point, late into the TLS 1.3 design process, some enterprise network operators began to realize that this would reduce their ability to inspect traffic in order," they claim or they say, they state, "to troubleshoot problems within their networks and started asking the TLS working group to restore some of the removed cipher suites or provide some other mechanism to support their internal network requirements."



He says:  "The most recently proposed mechanism uses what's called 'static Diffie-Hellman'" - and this of course is what Matthew's first paper that I talked about was looking carefully at - "and works by," he writes, "reusing encryption keys.  Interestingly, a form of this is used today as a minor optimization and isn't technically forbidden by TLS 1.3.  Initially, the working group refused to consider any proposal which would hurt or remove forward secrecy.  Recently, as the TLS 1.3 standardization effort has begun to draw to a close, the enterprise network operators have become more vocal.  On the mailing list and at in-person meetings, three viewpoints have emerged.  The debate between those with conflicting points of view has been vigorous," he says, "and in terms of the sheer number of words written, quite lengthy.  What exactly do the enterprise folks want?



"In a nutshell, these enterprise operators want the ability to decrypt the traffic that is inside their own networks.  Let's call this the enterprise viewpoint.  Now keep in mind any network traffic that is inside their network was either, A, generated from inside their network, in which case the enterprise's own computers created the plaintext in the first place; or, B, the traffic was sent from the Internet to one of the enterprise's computers.  In either case they already have the ability to do whatever they want with the plaintext," meaning because they're either endpoint where the decryption occurs naturally, "including storing all of it and examining it at will.



"If they already have access to the plaintext, why do they need changes to TLS 1.3 to enable them to get plaintext?  The question is key to the whole debate.  The enterprise viewpoint holds that operators need to be able to decrypt traffic from packet captures from various vantage points within the network.  For example, they would like to decrypt traffic before and after a load balancer, web server, or database server in order to pinpoint which part of the network infrastructure is causing problems.  On the mailing list and in person, they have been adamant that decryption from packet capture rather than, say, endpoint logging is the only way they can perform this sort of network debugging at the scale they need, given the fragility of what appear to be mind-bogglingly complex network architectures."



He says:  "It seems pretty reasonable to support this use case.  What's the problem with accommodating their request?  After all, this will only be for use inside their own networks.  On the one hand, this is reasonable and is completely supported today using TLS 1.2.  Indeed, one of the suggestions has been for network operators to continue using TLS 1.2 inside their networks if they need this capability.  On the other hand, there's no technical way to confine proposals to enable decryption to a particular network or data center."  Right.  You know, once the data's encrypted, that's the way it stays until it reaches its destination.  So you couldn't use anywhere TLS 1.3 if you used 1.2 anywhere.  That is, on a given connection.



"There are two major concerns raised by those opposed to breaking or degrading forward secrecy.  Let's call this the forward-secret viewpoint.  One concern raised by those with the forward-secret viewpoint is that proposals such as the static Diffie-Hellman approach mentioned above will enable wiretapping which would violate the IETF's Policy on Wiretapping.  Although that may be true - and this is hotly contested - some other technical mechanisms have been proposed which would make such wiretapping externally visible.



"The second concern is both more subtle and, I think, more compelling.  TLS, and SSL before it, has a history of supporting weak cryptography, and this support has come back to bite us several times."  And of course on this podcast it's an often-encountered theme and problem.  "The best of example of this is the export cipher suites.  These used cryptographically weak algorithms, but were at one point in time the only cipher suites that could be legally exported from the U.S.  Two decades after the use of export cipher suites should have ended, researchers showed how to abuse support for these deprecated algorithms in modern TLS libraries to perform man-in-the-middle TLS connections.



"The forward-secret viewpoint holds that the TLS Working Group should not standardize any weaker form of TLS; and, if this makes some network operators' jobs harder, tough.  That's two viewpoints, enterprise and forward-secret.  What's the third?  Let's call the third viewpoint the "pragmatic" viewpoint.  This viewpoint holds that, whether or not enterprise network operators really need the decryption capability, some of them really want it; and, since they really want it, they're going to do something to get it.  It's strictly better for the mechanism to be designed in public" - thus Matthew's contribution - "following normal IETF procedures, than to be cobbled together by people whose focus is on operations and not necessarily on security.  It's worth noting that at least one of the authors of the static Diffie-Hellman proposal mentioned above firmly holds the pragmatic viewpoint."  And he didn't name him, but I wouldn't be surprised if that was Matthew Green.



"Which viewpoint," he asks and finishes, "is correct?  Before I say which viewpoint I think makes the strongest case, I want to point out that I'm sympathetic to all three.  The network operator's job is just not an easy one, or so I assume.  It's definitely outside," he writes, "my particular area of expertise.  If they say they need plaintext in order to do their job, I don't think I'm in a position to contradict them.  The pragmatic viewpoint is quite compelling.  All else being equal, I'd much rather have the IETF design a standard mechanism to support the network operators' needs than have a hodgepodge of homegrown, difficult to use, non-interoperable, and potentially insecure solutions.



"But as they say, all else is rarely equal.  The Internet is for end-users, not for network operators.  The protocols we design today will, for better or worse, be in use for decades.  End-users have been paying the price for our mistakes and past compromises on security.  As protocol and implementation deficiencies necessitate new network hardware and software, the network operators have paid their own price.



"To rebut the enterprise and pragmatic viewpoints, I need not take a security-maximalist view.  The sense of urgency from the operators and the pragmatists is, I believe, unwarranted.  Yes, switching to TLS 1.3 will prevent operators from doing precisely what they're doing today; however, there is currently no need to switch.  TLS 1.2 supports their use case; and TLS 1.2, when used correctly, is secure as far as we know.  Of course, the network operators won't receive the benefits of mandatory forward secrecy, but that is precisely what they are asking to give up in TLS 1.3."



Finally:  "Designing secure protocols is hard."  Yeah.  I've been working on one for a few years.  "To date, our best efforts have not been as successful as we would like.  In my view, the only option we have is to design the most secure protocols we can to achieve our stated objectives.  We may still get it wrong, of course.  My hope is that, in 20 years, we won't, once again, be dealing with security issues we know about today.  Instead, I hope we'll be dealing with a whole new set of security issues."



So I finished on the dot of 4:00, and I'm glad I got that out because this is an important thing that is happening.  We will keep a watch on this.  I don't know how it's going to come down.  The good news is browsers - what this means is that the key being sent by the server as part of the negotiation would not change from one connection to the next on the same server, which means browsers would be aware if this was being done to their user and could raise a flag.



The other thing that's possible, because it certainly won't be the case that the earlier versions of TLS won't also be supported, is that browsers could choose not to advertise their support of TLS 1.3.  That is, typically you're able to selectively disable these various protocols, either in the browser or in the OS.  And so users could decide, uh, I'm not using this spyware TLS.  I'm going to only say that I know about 1.2, which would be, in a sense, it would be a version downgrade, but unfortunately a security upgrade, where you'd be saying, nah, I don't want 1.3; 1.2 is just fine.  I want my perfect forward secrecy cipher suites.



So anyway, really interesting that here at the very end, as they're getting ready to cross the finish line, with 1.2 being nine years old now, that suddenly these guys are saying, wait a minute, we need to spy on our own traffic.  And it's like, okay.  Well, we were going to fix it so no one could.  Now we're not sure what to do.



LEO:  Yeah, wow.



STEVE:  Interesting.



LEO:  Well, how long do these meetings go on for?



STEVE:  Oh, well, most of it is in mailing lists.



LEO:  Oh, okay.  They are actually - they did just - because one of our correspondents on Sunday, as you know, was there, Greg Ferro.



STEVE:  Yep.



LEO:  So I know they were having a physical meeting.  But, yeah, of course the mailing list will continue to debate this.



STEVE:  Yeah, I was glad he corrected himself.  He said that the URL was in the handshake.  And of course I immediately, wait, what?  No, it's not.  But then he did say that there were some aspects of the [crosstalk].



LEO:  There is some leak of data.  Yeah, he did say "URL."  But partial, and he said "partial," yeah.



STEVE:  It's the domain name.  We already know that in the SNI, Server Name Indication extension to TLS, you declare what host you're asking for.



LEO:  Interesting.  So that's the one thing that's leaking.  People know you've gone to Google, but not what you're doing there.



STEVE:  Precisely.  



LEO:  Steve Gibson, he does it again, ladies and gentlemen.  Thank you so much, Steve.  Everybody should go right now to GRC.com and buy a copy of SpinRite, if you don't already have one.



STEVE:  Keeps me afloat.



LEO:  And we want to keep him afloat, god knows, keep him in coffee.  All you have to do is go to GRC.com.  And while you're there, of course, there's plenty of other things to do, lots of freebies.  Steve gives everything else away, including his work on SQRL, Perfect Paper Passwords, ShieldsUP!, Don't Shoot the Messenger, DCOMbobulator, I can go on and on and on.



STEVE:  Even the upgrades to SpinRite.



LEO:  Yeah, those are free, too.  And of course this show's absolutely free.  In fact, Steve spends some of his money doing this show on transcripts, for one thing.  He gets great transcripts written of each show, so you'll find those there, along with the audio, 64Kb audio.  We have audio and video at our site, TWiT.tv/sn for Security Now!.  And you will find it, of course, wherever you subscribe to podcasts.  In fact, I encourage you to subscribe because this is one you want the whole set.  You want every episode of Security Now!.



Let's see.  Anything else?  We do the show Wednesday, 1:30, I mean, sorry, Tuesday, 1:30 Pacific, 4:30 Eastern, 20:30 UTC, so you can watch live if you want.  We stream it live.  Just, you know, it's the behind-the-scenes version of the show.  That's pre-transcript and all of that.  What else?  You can join us in the chatroom.  There's always a lively conversation behind the scenes at irc.twit.tv.  Thanks, Steve.  Have a great week, and I'll see you next time on Security Now!.



STEVE:  Fantastic.  We will be post-Vegas DEF CON and Black Hat, and I imagine we'll have some fun coverage of that.



LEO:  You didn't talk about Fruitfly.  That's going to be revealed tomorrow for the Mac.



STEVE:  Correct.  I actually, well, Leo, we went two hours and 13 minutes.



LEO:  I know, it was plenty, I know.  And we did talk about it on MacBreak Weekly, and I'm sure there'll be more to talk about after their presentation.



STEVE:  And actually I had a whole bunch of tabs, and I saved them for next week because I just - I tried to figure out how long this was going to go, but I thought this discussion of TLS 1.2 was really important.



LEO:  Yeah.  All 1,691 tabs saved.



STEVE:  Right.



LEO:  Now they'll load faster, too.  Thank you, Steve.



STEVE:  See you next week, buddy.  Bye.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.


GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#622

DATE:		August 1, 2017

TITLE:		Hack the Vote

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.GRC.com/sn/SN-622.mp3

ARCHIVE:	https://www.GRC.com/securitynow.htm



DESCRIPTION:  This week we look at the expected DEF CON fallout including the hacking of U.S. election voting machines, Microsoft's enhanced Bug Bounty Program, the wormification of the Broadcom WiFi firmware flaw, the worries when autonomous AI agents begin speaking in their own language which we cannot understand, Apple's pulling VPN clients from its Chinese App Store, a follow-up on iRobot's floor plan mapping intentions, some news on the Chrome browser front, the 18th Vault 7 WikiLeaks dump, and some closing-the-loop feedback from our terrific podcast followers.



SHOW TEASE:  It's time for Security Now!.  Much more news from Black Hat and DEF CON.  Steve's going to look at the voting machine hacks and a few other scary things like Broadpwn, which had the potential to infect one billion iPhone and Android devices.  That and all the security news coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 622, recorded August 1st, 2017:  Hack the Vote.



It's time for Security Now!, the show where we cover your security and privacy online, on the Internet, on the hackable voting machines, everywhere.  Here he is, the Commander in Chief, Mr. Steven Gibson of GRC.com.  Hello, Steve.



STEVE GIBSON:  Great to be with you again, as always, my friend.  Yeah.  So August 1st, and we are closing in on the end of year 12.



LEO:  Holy moly.



STEVE:  Our first podcast, which I will forever regret not numbering at zero - of course then we'd have the whole problem of 622 actually being our 623rd podcast, so that would get tiresome after a while.  But, yeah, it was toward, I think it was like, don't remember now, the 17th or something.  So a couple weeks from now we will be into the 13th, the lucky 13 year of the podcast.



So of course, as we've been predicting for a couple weeks, we are now on the backside of sort of the doubleheader of conferences in Las Vegas, Black Hat and DEF CON.  And as expected, we've got some final results.  Some of them...



LEO:  The results are in.



STEVE:  The results are in, and duck and cover.  So we're going to take a look at the expected DEF CON fallout, including the hacking of U.S. election voting machines, thus the title of this podcast, "Hack the Vote."  Microsoft also last week announced their enhanced bug bounty program.  We learned at DEF CON of the wormification - there's a lovely term - the wormification of the Broadcom WiFi firmware flaw, which as we know we were covering last month, through the month of July, as Google and Apple both got themselves up to date, but we'll talk about the consequences of that.  Then some interesting miscoverage, but really a classic instance of inflammatory headlines about the press's coverage of Facebook's autonomous AI agents beginning to speak their own language.



LEO:  Was it Google or Facebook?	



STEVE:  It was Facebook.



LEO:  Oh, okay.



STEVE:  And anyway, I drilled down and found the actual root story, and it was like, okay.  Well, I mean, it's definitely interesting, but it's not nearly what the headlines were, like oh, my god.



LEO:  They're communicating amongst themselves.



STEVE:  We don't know what they're saying.



LEO:  Ohhh.



STEVE:  We also have the brouhaha, which I think is, well, we'll talk about it, about Apple pulling their VPN clients from their Chinese App Store.  A follow-up on iRobot's floor plan mapping intentions.  Some news on the Chrome browser front.  The 18th Vault 7 WikiLeaks CIA dump.



LEO:  Uh-oh.



STEVE:  Three new pieces of news from there.  Some closing-the-loop feedback from our terrific podcast listeners.  So I think yet another great podcast.



LEO:  Well, I'm so thrilled.



STEVE:  Oh, and a wonderful Picture of the Week.  Just one of those, you just can't make this stuff up, Pictures of the Week.



LEO:  Oh, oh, I like it.  I like it.  And it ties into a product of your own, as a matter of fact.



STEVE:  That's why so many people brought it to my attention.  I got a bunch of retweets.  It's like, okay.



LEO:  Literally.  Literally retweeting.



STEVE:  Literally retweet.



LEO:  All right, Steve.



STEVE:  So our Picture of the Week is, for those who are listening, there's an installation on a wall which was - clearly some effort was put into it because a conduit was run up the outside of the wall, where an electrical receptacle was put.  And then this big black Model QB-4 Ultrasonic Bird Repeller was screwed to the wall and attached to this source of AC power.  I mean, so this was meant to do its job.  And the punchline on this which is so funny is that there is a bird nest, with bird, built on top of this thing, just sitting there peacefully, keeping her eggs warm, and apparently not the least bit perturbed by the QB-4 Ultrasonic Bird Repeller.



LEO:  Pretty funny.  Pretty funny.  Love it.  Love it.



STEVE:  Ineffective countermeasures is the story.  So last week, during the recently completed DEF CON cybersecurity conference, several hackers managed to hack into multiple U.S. voting machines, in some cases in as little as a few minutes, some cases taking an hour and a half or so.  For the first time ever, but likely not the last because it was such a success, DEF CON hosted what they called their "Voting Machine Village," during which the conference's tech-savvy attendees tried and succeeded to hack many commercial voting machine systems and help catch vulnerabilities in them.



This year, the first year this was done, but the planners already intend to make this thing a regular feature, provided 30 different pieces of voting equipment used in U.S. elections, including the Sequoia AVC Edge, the ES&S iVotronic, the AccuVote TSX, the WinVote, and the - is it Diebold?  Yeah, Diebold or Diebold?  How do you say that?



LEO:  Diebold.



STEVE:  Diebold ExpressPoll 4000 voting machines.  The executive summary is that every one of the 30 machines that the hackers poked at were, to varying degrees, hacked.  So not a single one of them resisted attack.  Now, in fairness, these weren't hands-off, over-the-air attacks.  There were some of those, but in some cases they required physical access to the machine, but like a USB port or something, very much like a laptop.  And many of the attacks which we're familiar with, that we've talked about over the years, work on these machines because, as we know, unfortunately, they are internally typically well-known architectures where a screen and some custom software were slapped on top, you know, much like the machines that run the nuclear submarines in the U.K.



LEO:  Yeah, well-known architectures like Windows XP and Windows CE, yeah, those well-known architectures.



STEVE:  Yeah, yeah, exactly.



LEO:  Yeah.



STEVE:  So the DEF CON hackers took complete control of an e-poll book, which is one of the Diebold devices.  In fact, I just saw some follow-up news saying that I think it was 650,000 personal information of voters was found on one of those.



LEO:  Wow.  Like they bought it on eBay, and it still had the stuff on it.



STEVE:  Yes, exactly.



LEO:  Oh, come on.  Geez Louise.



STEVE:  I know.  This, I mean, it's just - it's as bad as everything we've been covering about IoT...



LEO:  Yeah, times 10.



STEVE:  ...applied to voting machines.



LEO:  Yeah.



STEVE:  Yes, where we really would rather that they weren't this vulnerable.



LEO:  Right.



STEVE:  So that particular e-poll book is currently in use in dozens of states where voters sign in and receive their ballots.  So that's not per se a voting machine, but it's like a pre-voting staging device.  They also discovered and exploited significant security flaws in the AccuVote TSX, which is currently in use in 19 states; the Sequoia AVC Edge, used in 13 states; and another hacker broke into the hardware and firmware of another Diebold product, the TSX voting machine.



So, although somewhat less surprising, the WinVote voting machine had long been removed from use due to its vulnerabilities, so that wasn't a clear and present danger.  But those problems were again confirmed, and it was once in wide use while being horribly insecure.  They found that a remote access vulnerability in the WinVote OS exposed real election data that was still stored in the machine.  And another hacker hacked into the Express-Pollbook system, that's that Diebold system, exposing the internal data structure via a known OpenSSL vulnerability, allowing anyone to carry out remote attacks.  So remote attacks also are possible.  These do not require local physical access.



Jake Braun, who is a cybersecurity expert at the University of Chicago and who convinced DEF CON's founder Jeff Moss on the idea of creating this Voting Machine Village, said, quote:  "Without question, our voting systems are weak and susceptible to attack."  He said:  "Thanks to the contributors of the hacker community today, we have uncovered even more about exactly how."  There will be a more formal report forthcoming.



I have a link in the show notes here about the place on GitHub where Joseph L. Hall is assembling the Voting Village report.  Right now they have just like rough working notes, sort of the output of the various hackers sort of dumping their content out.  There will be a formal report.  And I did find coverage of this on The Hill website, so of course that's DC's, one of DC's reporting arms, and that suggests that this is getting the kind of attention that we need it to get.  So that's great.



Another person, Harri Hursti, who's the cofounder of Nordic Innovation Labs, and he was also one of the events co-organizers, said that the Village was announced at the last minute, but people were active in the forums, looking to understand the problems.  "The changes have to start somewhere," he said.  "This year it's in this room.  Next year it will be in a bigger room."  So that's just - that's all for the best, as we know.



Eric Hodge, who's the director of consulting at CyberScout, a consultant for Kentucky's Board of Elections, said:  "The best possible outcome is that the village results in a book of vulnerabilities to share with the FEC, the states, and other firms like ours."  I would take issue with that, and I will in a second.



So DEF CON's Voting Machine Village was the first time most researchers had ever had access to voting machines.  That's what has to change, okay, so think about that.  This was the first time most researchers ever had access to voting machines.  This is because they are considered proprietary, and their manufacturers protect them and don't allow anyone to poke at them.  Well, every lesson we have learned through the years of this podcast has taught us that that is a strategy that is doomed to failure.  That's what has to change.



There's no way that any state or county government should be allowed to spend taxpayer money on machines which have not been independently audited by security researchers.  Rather than treating their machines like proprietary closed boxes, voting machine manufacturers should gleefully turn their machines over to every independent security researcher they can find for the purpose of hardening their security offerings.  Then purchasers should have candidate purchases again independently vetted by still other independent researchers.



In other words, the lesson we keep learning is that, even with the best of intentions, mistakes are made.  And right now we're in a completely uncontrolled environment where the manufacturers are boasting about their military-grade security and using unearned reputation, basically.  I mean, everyone knows Diebold.  They're a well-known company.  But they're obviously not able to produce secure voting machines.  They shouldn't be purchased without some reason to believe that they're secure.



And of course I'm reminded of Steve Ballmer's pre-WinXP release where he was prancing around the stage screaming into a microphone about how XP was the most totally secure operating system ever created before its release.  And as we all know, shortly after its release XP was a security disaster.  So as I've often said, the security of a system must be designed in, but any actual systems-delivered security can only be proven afterwards.



So I'm so happy that this was done, that it's gotten a lot of press coverage, that there's egg on the face, deservedly so, because every one of these manufacturers was of course claiming impenetrable security, but never being willing to put it to the test.  So that was tested.  Not a single machine was found to be secure.  So after I put the show together I saw some news about some forthcoming potential IoT legislation.  I think that's all for the best.  We need something similar for voting machines.  Otherwise, everyone talking about "the integrity of our system," it's just empty air.



Now, that said, there is an advantage to, and I've also often said this, to a heterogeneous environment, that is, we don't want everyone in the country, at every state of government, using a single machine.  That's very dangerous.  So the fact that we have many systems, many companies creating widely differing machines, that's better.  And we also want to do things like avoid hooking them all together into one big network.



LEO:  One of these machines was WiFi-enabled, which is obviously a terrible idea for a voting machine; right?



STEVE:  Yes, yes.



LEO:  I mean, you don't even need physical access.



STEVE:  Yes.



LEO:  That is said to be the strength of the U.S. voting system is that every registrar in every county has a choice, and so there is no homogenous system.  So that's good.



STEVE:  Right, yeah.  And again, the problem is that we see big companies purchasing each other or big companies purchasing smaller ones and reducing choice and reducing competition, but also reducing heterogeneity.



LEO:  Right.



STEVE:  And that's a strength that we have now.  We need not to lose that.



LEO:  What you really want is every voting machine needs to have a paper trail so that, in case of concerns, you can have an audit.  The problem is electronic voting machines that have no paper trail.



STEVE:  Right.



LEO:  Then the machine is the only reliable source, or unreliable source, of the vote.



STEVE:  Right.



LEO:  If you have a paper trail for every vote, you can do spot checking; and, in case of trouble, you can do a full recount on paper.



STEVE:  Yeah.  And I think, I mean, were I architecting these, I would think, I mean, not having looked at this closely, it would seem to me that having a machine produce - you know what a fan I am of paper.  You know, SQRL prints its secrets on paper because they're offline inherently, and you can't attack them by WiFi or a USB dongle.  So imagine if the machine were just a transcriber of a friendly UI which then printed out, on paper, a barcode summary of exactly what that voter did.  And then in a separate stage you collect all of those machines' spools of paper and feed them through...  



LEO:  Precisely.  Exactly.



STEVE:  ...a master tabulator.



LEO:  Right.



STEVE:  And then you can do it as much as you want.  So you have several stages of checkpoint that way.



LEO:  I'm going to point you to - we did a Triangulation on this before the election this year, or last year, I guess, to VerifiedVoting.org, which is a really great organization that talks about all of this.  And of course they talked about the hacking conference.  And there is, under the - I'm not sure where it is on here.  But there is a proposal for a way to do this that would be highly secure, and it solves a lot of these problems.  These people have been thinking about this for a long time.  And so, you know...



STEVE:  Good.



LEO:  This is, frankly, a solved problem by some very smart computer scientists and mathematicians and election folks.  But, you know...



STEVE:  Inertia.  



LEO:  Inertia, yeah.  And money, frankly, because it costs money to do this.



STEVE:  Yeah.  And in fact, in my example, if the format of the barcode which was printed out was standardized, and it absolutely should be, then you have a standard point in between the machine that takes the votes and produces the barcode, and on the backend the tabulator that optically ingests all of those in a continuous paper strip and tabulates.  So you're not reducing competition, you're creating a standard protocol that allows machines on both ends to be independently designed and created and sold in a competitive marketplace, but with a common standard interchange format between the front end and the back end.



LEO:  Yeah.  And you trust - I would trust Caltech and MIT, and they have something they call the Voting Technology Project that addresses a variety of ways, not just technologies to take votes, but maybe even better ways to vote because of course there's other proposals about weighted voting and things like that, that are interesting.  But this is a good site.  It's vote.caltech.edu.



STEVE:  Nice.  So we did get news of a scary present and not-to-be-fixed vulnerability.  Okay.  So this is an SMB, and we know that stands for Server Message Block.  That's a.k.a. Microsoft's file and printer sharing protocol.  Eight years ago, back in '09, which we talked about at the time, there was an attack tool known as "Slow Loris," which operated over HTTP web connections.  That was a server-side resource depletion attack which exhausted a server's incoming connection-handling capacity while at the same time requiring very little bandwidth from the attacker.  So unlike today's traditional DoS and DDoS attacks, which use bandwidth flooding to just saturate the links inbound and prevent legitimate traffic from being able to compete with the attack traffic, then and now, even one attacking machine could bring a big remote website to its knees, preventing legitimate visitors from obtaining access.



So what was revealed at last week's DEF CON was a similar, but new, server-side resource depletion attack which it used that troublesome version one of the SMB protocol, which as we know is still supported in Microsoft's OSes, despite being long obsolete.  And we know Microsoft has said they're finally going to deprecate it.  It will be disabled in this forthcoming Windows 10 Creators Update release.  So on the newest machines it will finally be going away.



But it turns out that doesn't solve the problem.  Sean Dillon of RiskSense was among the first researchers to analyze the EternalBlue exploit, which was that leaked NSA SMBv1 exploit which, as we know and covered at the time, was used to spread the WannaCry ransomware.  It was during his analysis of EternalBlue that he discovered another, that is, this related issue.  He wrote:  "While working on EternalBlue, we observed a pattern in the way memory allocations were done on the non-paged pool of the Windows kernel.  The non-paged pool is memory that must be reserved in physical RAM.  It can't be swapped out.  That's the most precious pool of memory on the system."  He said, "We figured out how to exhaust that pool, even on servers that are very beefy, even having 128GB of memory."  And he said:  "We can take such a server down with a Raspberry Pi."



LEO:  Ah, ah, ah, $35 computer.



STEVE:  A Raspberry Pi and 20 lines of Python.



LEO:  Oh, wow.  So, Python, is there anything it can't do?  No.



STEVE:  Like Slow Loris before it, this new attack they dubbed "SMBloris" in homage to Slow Loris.  It leverages a memory-handling bug that could be exploited by attackers to shut down big web servers with small computers.  However, it was initially believed that attackers could only exploit the SMBloris vulnerability if the target machine had SMBv1 exposed to the Internet.  But that now appears - well, or internally, like on an Intranet, where it's much more likely that machines will have SMB exposed.  We would expect corporate firewalls, and certainly most ISPs today are still blocking the SMB ports on behalf of their subscribers.  But on an Intranet, in a large enough network, people could get up to some mischief.



Anyway, it turns out that that was hopeful thinking, that is, that it was only SMBv1.  The Register later updated their initial coverage to say, quote:  "According to Microsoft's SMB Supremo," as they called him, "Ned Pyle, SMBloris affects all versions of SMB, not v1 as first thought," because Microsoft said it all happens so early on in the connection.  So this problem will not be going away, even after the forthcoming Win10 Fall Creators Update, which disables support for SMBv1.



So the protocol technically, sort of the sub-protocol within SMB, is called NBSS, which is the NetBIOS Session Service.  Every connection to it, when tweaked in this special way, is able to cause the allocation of 128K of page-locked non-swappable kernel memory.  That is, so all of the protocol support is down in the OS.  So this is not allocated in userland.  This is in the kernel.  So the kernel says, oh, here comes an incoming SMB connection, and it has just notified us that it has 128K of data it's about to send.  So the kernel preemptively obtains a 128K block of memory which it locks in RAM, so it's non-swappable in the kernel, so that it can then asynchronously - it then acknowledges that it's ready to receive that per the protocol, and that memory is then retained while awaiting the 128K of follow-on memory.



So all of our technically astute listeners know where this is going.  That memory, that follow-on data is never delivered.  It's just a claim for future delivery which the protocol is then forced to preallocate.  Now, connections which are not active will get shut down after 30 seconds.  But it turns out that's enough time.  With more than 64,000 TCP client-side ports available, an attacking machine can force the allocation.  A single attacking machine at a single IP, using the available client ports, with 128K per port, can force the allocation of 8GB.



And since the protocol is available both over IPv4 and IPv6, both can be used simultaneously, bumping the forced allocation to 16GB.  And if a second source IP is available, the memory commit could be doubled again to 32GB of RAM, using just two IPs, and so on.  Once the attack has triggered memory saturation, the server will crash, and a system reboot will be required in order to resume normal operation.  So anyway, Sean Dillon was the person at RiskSense who found this and who delivered the presentation last week at DEF CON, saying that a Raspberry Pi could take down the beefiest server using 20 lines of Python code.



So we know that many systems have SMB exposed to the Internet.  Thus the disaster of WannaCry a couple months ago when this EternalBlue was weaponized, after being discovered from the NSA leak, into an exploit.  And so it's without question that those machines, which still have SMB exposed, and in this case not only v1, but now we know any version of SMB, are probably going to be crashing because there will be hackers that want to play with this, and it's too easy to find SMB exposed on the Internet.  It's just not difficult.  This has been a problem.  This has been the way SMB memory allocation has worked.



Sean said:  "I think Microsoft's problem" - because Microsoft, I should say, has said this is not going to be fixed.  Microsoft has taken the position that this is a configuration issue.  Now, that was arguably the case when v1 was believed to be the attack vector.  It'll be interesting to see if they still take the same position.  But as Sean said, quote:  "I think Microsoft's problem is that it would not be easy to fix.  It's the way they've done SMB memory allocation for over 20 years.  So everything relies on the fact that the client says 'I have a buffer that I'm sending that's this big.'  So the server reserves that much memory so it can handle the anticipated incoming data."



He says, "What we did with this attack was to simply say, 'I have a huge buffer,' and never send the data.  There are many components," he writes, "of the protocol support that rely on the fact that the buffer is already allocated, and the size is already known."  Which is to say it's not practical to change the implementation, for example, into an auto-sizing buffer that grows as data is actually received.  The way it's been done is pretty much the way it has to be.  So what we have is an old bug that's present in every version of Windows, which means that attackers will likely attack those machines that have SMB enabled publicly and maybe get up to some mischief in Intranets where it is enabled internally.



For our listeners, we have multiple lines of defense.  First of all, as I mentioned, ISPs are still blocking ports 137, 138, and 139, which are the oldest implementation ports for this; and 445, which is the newer version of Windows port.  Those are typically blocked.  Any incoming traffic with those ports as its destination is just dropped at the ISP edge.  And it's because that wasn't originally done, and because of the exposure of this, as I've said, that was the motivation, the impetus for me writing the ShieldsUP! system all those years ago because everybody had those ports exposed.  There was no firewall in Windows in the beginning.



So we have ISPs as the first line of defense.  We have NAT routers as the second line of defense.  A properly configured NAT router will similarly drop that traffic at your border.  And all of our modern OSes from XP Service Pack 2 and later have a firewall which is present and enabled by default.  So I don't think this is a problem for us.  But it's certainly a problem, I mean, it could be a problem in anyone's Intranet, even in a LAN, if something bad got into your system.  But again, it's not a data exfiltration bug.  It's just to crash your machine.  So I could see that people would get up to some mischief, but it's not clear what more they could do.



LEO:  All right.



STEVE:  So I have a link in this next piece, Leo, that you're going to want to take a look at.



LEO:  All right.



STEVE:  Microsoft has announced a new Windows Bug Bounty Program.



LEO:  I'm still emptying the trash.



STEVE:  Last Thursday Microsoft announced a new Windows Bug Bounty Program that will pay researchers - get this, but only for the biggest ones - up to a quarter million dollars.



LEO:  Wow.



STEVE:  For finding and disclosing security vulnerabilities.  The bug bounty program...



LEO:  That's the Hyper-V bounty.



STEVE:  Exactly.  The biggie is the Hyper-V.  If there are any problems there, they really want to know.



LEO:  Right.



STEVE:  It will focus upon a few key areas.  And on the page that I've linked to and in their description there's a little menu of what you get paid for which class of bugs.  There's Hyper-V problems, mitigation bypass, Windows Defender Application Guard, Microsoft Edge, and then all features made available through the Windows Insider Program.  The payouts from Microsoft depend upon where a vulnerability is found and how severe it is, and ranges from a low of $500 for a vulnerability in Edge, all the way up to...



LEO:  Wait a minute.  So that tells me that there's a lot of those.



STEVE:  Yeah.  



LEO:  Is that, I mean, can you judge by that? 



STEVE:  I think you're right.  I think that you're exactly right.  And that's the first thought I had when I heard that they were upping the ante is that, okay, now they're sort of sure that the easy-to-find bugs have been shaken loose, so they're saying, okay, we're willing to pay more for increasingly bad ones.  But I think you're right.  The fact that Edge only gets you 500 bucks says, okay, those may not be that difficult to find.



So in their announcement, what was really interesting also, they did something a little differently than others have.  They said any critical or important class remote execution, privilege elevation, or design flaw that compromises a customer's privacy and security will receive a bounty.  But they also said they would pay a maximum of 10% of the highest amount discoverers would have received if the discovery was fresh.  In other words, even if you find - if you report...



LEO:  An existing one.



STEVE:  ...an existing redundant problem, you can still get 10% of what the first reporter of that problem got.



LEO:  That's pretty good.



STEVE:  So they're actively working to encourage researchers to disclose everything they know of instead of perhaps sitting on vulnerabilities because they're not sure if they're new or not.  So bravo to Microsoft.  That's, you know, we need that.  Again, all the lessons we're learning is that it's only by proactively attacking presumed secure systems that they could be proven secure.



Ooh, boy.  DEF CON introduced us to Broadpwn, B-R-O-A-D-P-W-N.



LEO:  This one is bad.



STEVE:  Yes.  Which wormifies the Broadcom WiFi firmware bug that we talked about a month ago.  As we know, both Google and Apple both have issued patches last month.  But at the time that this was, well, prior to its disclosure, but at the time it was discovered, an estimated one billion devices were vulnerable to what was disclosed last week at DEF CON.  And of course this is always the way these things go.  It starts with a device crash.  Then the vulnerability which crashed the device is carefully examined and weaponized into something far more functional and useful to attackers.



A researcher by the name of Nitay Artenstein at Exodus Intelligence was the person who took this to the next level.  So I said DEF CON, but it looks like it was actually at Black Hat, the previous conference of the two.  He demonstrated a proof-of-concept attack that exploited the vulnerability which, as we know, had by then been patched, both in Android at the beginning of the month and in two patches.  In fact, the second patch was our errata from last week saying, well, we thought it was the earlier patch at 10.3.2, but now it looks like it was 10.3.3.



Anyway, so he demonstrated a proof-of-concept of this BCM43xx family of WiFi chips which had long been manufactured by Broadcom.  His attack fills the airwaves surrounding any compromised device with WiFi probes requesting low level, meaning WiFi protocol level, no user alert or action required.  This thing slips in down in the kernel, actually even below the kernel, in the baseband radio in the Broadcom chip, making connection requests to any and all nearby mobile smartphones which are also Broadcom BCM43xx equipped, which they all are, basically, because that's the chip of choice that everyone was using.  A billion devices in service.



When specially devised requests are then sent from the infected device, which infects all other devices within radio range that have not been patched and have this problem, the attack rewrites the firmware that controls the chip.  The compromised chip then itself begins sending the same malicious packets to all other vulnerable devices, setting off a potential flash worm chain reaction.  I mean, whoa.



So he wrote:  "Broadpwn is a fully remote attack against Broadcom's BCM43xx family of WiFi chipsets which allows for code execution on the main application processor in both Android and iOS.  It is based on an unusually powerful zero-day that allowed us to leverage it into a reliable, fully remote exploit."  He said his attack worked on a wide range of phones including all iPhones since the iPhone 5; Google's Nexus 5, 6, 6X, and 6P models; the Samsung Note 3 devices; and the Galaxy devices from S3 through S8.



So this is another perfect example and substantiation of what we've been saying on this podcast now for several years, which is it is utterly unsafe to be using any iOS or Android-based device that is not receiving regular security updates.  And note that Android is no longer alone here since Apple also eventually abandons their older, yet still functional iOS devices once that older hardware can no longer run the latest version of iOS.  So, I mean, and again, that means the device is old, but it does say that even old problems are still problems.



So for any of our listeners who really want the nitty-gritty, I've got a link in the show notes to Nitay's detailed blog posting at ExodusIntel.com.  It's 2017, July 26, just titled "Broadpwn."  I'm sure you can find it; or the link, as I said, is in the show notes.  And it's, I mean, he takes it all the way down.  I mean, for anyone who's interested in how this is actually done, he laid it out.  So it's a beautiful presentation, more than we need to get to or is practical over a mostly audio podcast.  But the details are all there.



And again, we know there is presently a huge number of phones that have that chip, have the vulnerable firmware, and have not been patched.  Probably predominantly Android phones, just because they are so quickly abandoned by their cell provider.  But you don't even need cell access, just a WiFi radio turned on.  So again, I wouldn't be surprised if in the next coming few weeks we're covering the actual outbreak of a Broadpwn flash worm which has just torn through the Android ecosystem of older phones that did not get patched during Google's patch, or any patches sent out from Google to responsible providers of non-Google devices, who then pushed the patches out to their users.  Anyway, boy, needs to happen.



Okay.  So in this week's crazy headline news, for example, Fox News' headline was "Facebook Engineers Panic, Pull the Plug on AI After Bots Develop Their Own Language."  Okay, that was the most over-the-top one.  The Facebook engineers did not panic.  But even BGR, Boy Genius Report, said:  "Facebook AIs develop own language and are immediately shut down."  Okay, well, that wasn't - that's not true, either.  But at least it doesn't, I mean, it sort of implies panic where none existed.



Many news outlets covered the story and also picked up on each other's coverage, seemingly amping it, like one-upping each other as they linked in a chain.  I tracked down the original coverage, which was much drier and far more accurate, over on FastCoDesign.com.  And the sober headline was:  "AI Is Inventing Languages Humans Can't Understand.  Should We Stop It?"  And it was a long and thought-provoking piece.  I'm just going to summarize it a bit here.



So what they wrote is:  "Researchers at Facebook realized their bots were chattering in a new language.  Then they stopped it."  But they didn't stop it because they were afraid of it.  They stopped it because they considered it a programming error that they had not explicitly constrained the dialogue to be English.  So, for example, I'll get into this in a little bit of detail in a second, but there was a deliberate negotiation protocol where two independent AI agents talked to each other.  And they were named, appropriately, Alice and Bob.  And so, for example, Bob said:  "I can can I I everything else."  Which is not perfect English.  Alice responded:  "Balls have zero to me to me to me to me to me to me to me to me to."  Which, again, not English.  But what's interesting is that they understood each other.  That passage...



LEO:  Like twins.



STEVE:  Exactly.  Perfect example, Leo.  Now, "That passage looks like nonsense, but this 'nonsense,' in quotes, was the discussion of what might be the most sophisticated negotiation software on the planet, negotiation software that had learned and evolved to get the best deal possible with more speed and efficiency and perhaps hidden nuance than we are able to perceive.



"The conversation occurred," as I said, "between two AI agents developed inside Facebook.  At first, they were speaking to each other in plain old English.  But then researchers realized they'd made a mistake in programming.  Dhruv Batra, at Facebook AI Research" - that's an acronym, Facebook AI Research, FAIR - "is a visiting research scientist from Georgia Tech.  He said:  'There was no reward for sticking with the English language as the AIs conversed.'  The two AI agents were competing to get the best deal, which is an effective strategy for sharpening the operation of AI by pitting them against each other in what is known as a 'generative adversarial network.'  In this case, neither was offered any incentive for speaking as a normal person would.  So as they grew, they began to diverge from English, eventually rearranging legible words into seemingly nonsensical sentences, but sentences they each understood."



Batra, speaking to a now-predictable phenomenon that's been observed over and over and over, said:  "AI agents will drift away from understandable language, inventing more efficient code words for themselves.  So, we then wonder, should we let our software do the same thing?  Should we allow AI to evolve its dialects for specific tasks that involve speaking to other AIs?  To essentially gossip out of our earshot?  Maybe.  It offers us the possibility" - and this is coming from the text at FastCo Design - "a more interoperable world, a more perfect place where iPhones talk to refrigerators that talk to your car without a second thought.  The tradeoff is that we, as humanity, would have no clue what those machines were actually saying to one another.



"Mike Lewis, who's a research scientist at FAIR, said that Facebook ultimately opted to require its negotiation bots to speak in plain old English.  He wrote:  'Our interest was having bots who could talk to people.'  And Facebook isn't alone in that perspective.  Microsoft has also indicated that it's more focused on human-to-computer speech.  Meanwhile Google, Amazon, and Apple are all also focusing incredible energies on developing conversational personalities for human consumption. They're the next wave of UI, like the mouse and keyboard for the AI era."



So another issue, as Facebook admits, is that it has no way of truly understanding any divergent computer language.  Batra says:  "It's important to remember there aren't bilingual speakers of AI and human language.  We already don't generally understand how complex AIs think because we can't see inside their thought processes.  Adding AI-to-AI conversations to this scenario would only make that problem worse."



So it's interesting.  What we're finding is that, when we create sufficiently powerful AI, that inherently means that they have wide latitude and huge degrees of freedom.  And when they're allowed to talk to each other, I mean, what's really freaky is this was - remember "Colossus:  The Forbin Project"? 



LEO:  Loved that movie.



STEVE:  That wonderful, it's like, from the '60s or '70s?  



LEO:  Yeah, yeah.



STEVE:  I mean, it was an old movie.  I can't remember who the star was.



LEO:  You're on the same page.  Somebody in the chatroom says "new version of the Forbin Project."  That's awesome, yeah.



STEVE:  Yeah, where Colossus determined that it had a counterpart in Russia, and it insisted that a communication link be established.  And they began talking and quickly evolved their own language, which the scientists at each end were completely unable to understand.  And they started with a mathematic basis and then evolved just completely out of anyone's ability to interpret it.  And what's bizarre is it turns out that's exactly what happens.  If we don't constrain two AIs that are flexible enough to have that capability, they will diverge and come up with a better language.



LEO:  They're talking to one another right now.



[Clip in background]



MALE VOICE:  "Colossus:  The Forbin Project."



LEO:  It was nobody famous in this.  I don't think there's anybody whose name you would recognize today.  But that was a great movie.



FEMALE VOICE:  It's making you a prisoner.



LEO:  Uh-oh.



MALE VOICE:  Shock.  Horror.  Suspense.  Created with all the technological brilliance of "2001:  A Space Odyssey."



LEO:  Not really.



MALE VOICE:  Colossus is the ultimate in sophisticated computers.



MALE VOICE:  I'm going to try to convince the computer that you're my mistress, and that that's why I have to be given the opportunity to see you regularly in private.  That way we can...



STEVE:  That's one of the Bond actors.



LEO:  Oh, is it?  A bad - Bond bad guy?



STEVE:  Yeah.



LEO:  No, really?



MALE VOICE:  Four times a week.



LEO:  So he's a prisoner of Colossus; right?  Colossus apparently can speak, but when it talks to humans it has to type.



MALE VOICE:  When do you think you'll be able to [indiscernible]?



MALE VOICE:  Colossus sees all, senses all, knows all, controls all armaments and all defenses.  When this emotionless creation becomes the master of man, the result is catastrophic.



[End Clip]



LEO:  Well, now I'm going to - I know what I'm watching tonight.



STEVE:  It's a great movie, Leo.



LEO:  I haven't seen it in ages.



STEVE:  I'm going to have to watch it again, too.  I mean, it is old, but it was really well done.  And in fact, earlier in this podcast, years ago, I mentioned that I had heard there was going to be a remake.



LEO:  Oh.



STEVE:  And I don't know what happened to it, but I hope it happens because it would be...



LEO:  It came out in the 1970s, and Eric Braeden is the guy's name.  But I...



STEVE:  Oh, I thought he was - okay.  Not famous.



LEO:  Not famous.



STEVE:  But anyway, great movie.  Great, great, great movie.



LEO:  Yeah.



STEVE:  "Colossus:  The Forbin Project."  So over the weekend Apple pulled many of its iOS VPN clients from its Chinese App Store.  And not unexpectedly, the makers of those VPN applications were up in arms over Apple's, quote, "capitulation," unquote, to pressure from the Chinese government, as they put it, claiming that this was a human rights issue, and were disappointed in Apple.  But for what it's worth, I mean, yes, it would be nice if Chinese citizens had more freedom.  But Apple is a super successful - actually, I don't know if you saw the numbers, Leo, but it just broke its records that just came out after MacBreak Weekly.  China is Apple's largest market outside the U.S.



Apple, as we know, is a publicly held commercial for-profit company whose device application model is that of a closed, customer-protecting, application ecosystem.  And so, yeah, that means there's a tradeoff.  If you want an open ecosystem, there are plenty of them freely available.  Get a PC or jailbreak an Android phone and sideload whatever you want and take your chances.



LEO:  That's a good point.  You don't have to use an iPhone.  That's a good point, yeah.



STEVE:  No.  Yeah.  Apple knows quite well that, if they're going to operate in China, they can do so only with the permission and approval of the Chinese government.  And if I were in China, I would definitely want the significantly enhanced security of using an Apple iOS device rather than any other non-Apple solution, knowing that there is extreme curation of the apps that are allowed to run on that device.  So that would be my choice.  So, yes, it's closed; but it's also closed as much as possible to malware.  And for the moment we also believe it's likely closed to eavesdropping.  So it's like, yeah, again, you could get a - if you wanted to do VPN-enabled surfing, just get a cheap Android phone and use that.



LEO:  Or a PC, although China's also banned VPN apps on PCs.  VPN services, I should say.  So you can use one, but it's run by the Chinese government, which tells me that it's not completely secure.



STEVE:  Yeah.  So iRobot did walk back their plans to sell their users' homes' floor plans, following a bit of a firestorm of customer... 



LEO:  They said they never intended to do it.  Not walked it back, but they said they never intended to do it.



STEVE:  I know.  Following a bit of a customer firestorm of outrage over the news, iRobot's CEO Colin Angle now says that his comments to Reuters were misinterpreted, and that iRobot has no plans and would never make such data available to third parties.  So in seeing that, I thought, I wonder, you know, I was kind of curious, wondering what was misinterpreted.  So I went back to our coverage last week and found Colin's quote from Reuters, where he said:  "There's an entire ecosystem of things and services that the smart home can deliver once you have a rich map of the home that the user has allowed to be shared."  So, okay.



LEO:  Yeah, that doesn't say anything about selling it.  And they said we intended - if we share it, we'll share it with other IoT, what was it, IoT devices.  They changed, well, I don't think they changed anything.  I don't think what they said was incompatible with what he says.



STEVE:  I agree.  So maybe it was just - okay.  So in the worst, what we got was a clarification and a clear statement.  And my sense was that Colin may have just sort of been winging it when he was talking to Reuters and just sort of said, oh, yeah, you know, they might have said, "So, what plans do you have for additional revenue?"  He might have felt a little challenged by that and just spit out something that he quickly decided, like, oops, you know, that's really not any firm plans we have.  And he also did say, "We have never had any discussions with any third parties about map sharing."  So if he gave them the wrong impression, it was probably just something that he said during the interview which he then decided was not such a good idea.  And if he was toying with the idea, I think it's clear what his users think about it.



A couple Google Chrome browser improvements.  Chrome will be, with Chrome 63, which is due in December of this year, so by the end of 2017, they are tightening the screws on embedded iframes, which is really good news.  Iframe is short for inline frame, which is essentially - it's a feature that's always been present in HTTP, or I should say in HTML, which allows a web page to embed essentially another web page inside of itself, that is, in a frame.  So the hosting page defines a rectangular region and then puts a URL in that frame which the web browser, after loading the hosting page and seeing that an iframe has been defined, the web browser says, oh, I need to now go get the content to fill that frame, which itself is a complete HTML page from somewhere else, that is, it doesn't have to be - it can be from the same domain, but cross-origin iframes are allowed.  So this has historically been an outsized vector for malware and browser exploitation that we've often talked about on the podcast.



So beginning at the end of this year, with Chrome 63, iframes will be restricted by default in what content they may contain, with restricted permissions needing to be then explicitly allowed.  There will be a new term added to the iframe clause.  And I didn't check to see if this was W3C already, that is, is it part of the spec which Chrome will be adopting?  Or which way is this flowing?  I don't know.  But I imagine, if not already, it will become a standard.  There will be an "allow" term added to the iframe link where the values can be geolocation, microphone, camera, speakers, MIDI, or encrypted media.



And if any of those are not available, their respective content type will not be provided, will not be allowed to be presented to the user through the iframe.  So they will be denied by default, and embedding pages will need to explicitly enable them.  So that's perfect because iframes are one of the ways that ads are hosted.  And, for example, it's the way ads are allowed to run Flash content by default is typically in a frame and, clearly, a way that some other third-party site could turn on the camera or could turn on the microphone or could annoy you with noise from MIDI or from speakers.  So those things will be shut down by default.  And if this becomes a standard, if it's not already, then that's a nice step forward in our security.



And the second piece of good news on the Chrome front, we talked some time ago about how some changes in Chrome had made it surprisingly difficult to inspect the certificate of a site that you were visiting.  We know that the "chrome" in Chrome, as it's called because by tradition all of the window dressing around the page is called the "browser chrome," which no doubt is where Chrome got the name for itself, they had changed - Google had changed it in Chrome, or the Chromium Project had, to make it more difficult to find and inspect the certificate.  We talked about this at the time.



Well, they've changed it to make it very easy.  It is still opt-in.  But if anyone with Chrome goes to - and there's a link which is worth reading out because it's not easy to find.  If you just go to chrome://flags, that will take you to a page with, I mean, it's as bad as Firefox's about:config.  I mean, there is so much stuff there, it's like, yikes.  Lots of flags.  Anyway, the one you're looking for is at - so //flags/#show-cert-link.  It's down near the bottom, so you could also scroll all the way down to the bottom and then up like about a page, and you'll find show-cert-link, and then it's just a little tiny link there that will say "enable."  And so you want to click that, and then you'll be prompted to reload the browser.  You need to reload the browser.  And once you do that, when you then click on the padlock shown in the URL bar, that will drop open.  Right now, without having done that, it drops open a long list of characteristics of the site you're visiting.  Once you have enabled the show-cert-link, the first thing...



LEO:  This is only in Chrome 60, I've got to point out.  Most people...



STEVE:  Oh, yeah, yeah, I forgot to say, yes, thank you.



LEO:  If you don't have Canary, you don't have it.



STEVE:  Which is - and I did update on my Win7 machine in order to see exactly what it was doing, in order to explain this.  And so Chrome 60 is the current one.  So if you go to About Chrome, and it sees you're on 59, it'll give you an update right then.  So I think you should be able to get that.  Anyway, the good news is, once you've done this, the first thing on that dropdown list is a button to allow you to inspect the certificate, which now makes it super easy to do so.  So yay to Chrome for giving us that tweak.  And again, that's a power user feature.  We know that most people probably won't care.  But for those of us who do, having it just right there, two clicks away, is extra nice.



So WikiLeaks' 18th Vault 7 dump contained news of three new tools that the CIA uses for hacking and implanting stuff on Mac OS X and Linux - the first two for Mac OS, the third one for Linux.  Achilles is a tool used to subvert Mac OS X disk images.  It allows CIA operators to combine their own malicious trojan apps with a legitimate Mac OS app into a hybrid disk image .DMG file.  The tool which binds these pieces together is just a BASH shell script, which gives the operators opportunity to run the appended malware as desired.



And what's interesting is, when the unsuspecting targeted user downloads an infected disk image on their Apple computer, opens and installs the software, the malicious executables run in the background.  But Achilles then detaches itself so that afterwards all traces of the Achilles tool are removed securely from the downloaded .DMG file.  So the file then is a checksum match.  It is a perfect match for the original unmodified .DMG.  So if somebody didn't check before, for example, checksum verification, but thought, oh, maybe I should check afterwards, well, it'll match, and it's too late because Achilles will have already delivered its payload and removed itself from its install image.



So this is not of any particular high-tech-ness interest.  It's just some interesting technique that the CIA, presuming that these documents are legitimate and were in fact leaked, as we believe, it's informative that this sort of targeted attacking is going on, and that Mac OS X is the intended target.



A second tool, SeaPea, P-E-A, is a stealth rootkit for Mac OS X systems; and, as such, like any rootkit, it would provide CIA operators with a stealthy tool with launching capabilities that hides important files, processes, and socket connections from users so that even somebody looking for any debris in their system, trying to audit their connections through netstat to see what their network system is doing, it cleans up all of those presentations.  We talked about rootkits years ago in great detail because there were a lot of them around at the time.



So SeaPea requires transient root access for it to be installed on a target Mac, making it, again, not leveraging any exploits or zero days or anything, but being a targeted attack tool.  And it will then remain resident and cannot be removed until the next version of the Mac OS is updated, which would flush it out because it would just remove it from the system.  But so a stealthy rootkit targeted at Mac OS X that, if all of this is to be believed, the CIA is able to arrange to get into someone's machine where they need access, and then it hides itself.



And finally, Aeris, A-E-R-I-S, is an automated implant targeting Linux machines.  It's written in C, designed to open backdoors in not only Linux, but some Unix systems.  In Linux it's able to operate on Debian, CentOS, and Red Hat; and then also both FreeBSD Unix and Solaris Unix OSes.  It provides build scripts which allow CIA operators to generate customized effects, depending upon their needs; supports automated file exfiltration, reconfigurable beacon interval and jitter, HTTPS and SMTP protocol; and, I got a kick out of this, all with TLS encrypted communications with mutual endpoint authentication, providing fully secure end-to-end encrypted exfiltration with a structured command-and-control system that's similar to what is used by several Windows implants.



LEO:  So it sounds like these would be used, if you got access to a network, and you were sitting at your console somewhere else, that you would then install these onto those systems, exfiltrate, delete them, and get out.  You're not going to leave them there for days and weeks and months.  They'd be discovered.  This sounds like more this is like the kind of tool that's used once you've hacked a system, right, hacked into a network.



STEVE:  Yeah.  I guess the idea is that they're not leveraging any mysterious problems.



LEO:  Right.  These are just tools.



STEVE:  Exactly.  The rootkit would remain hidden, so nobody looking for it would be able to see it.



LEO:  Right.  So SeaPea might be more - but you have to have root access to put it on in the first place.



STEVE:  Correct.  And you would see its traffic.  So if a lot was going on, then if you were monitoring traffic, then that would expose it to external surveillance.  But I think what we're seeing here is deliberately targeted attacks.



LEO:  Right.



STEVE:  So they're, as I described them, workmanlike tools which are in this toolkit.  And so if there's someplace where the CIA has, like, brief access or temporary access or maybe gets somebody who is trusted by the system owner to, like, insert this USB drive and do us a favor on behalf of the U.S. government, then that could happen.  So again, the 18th release of WikiLeaks goodies.



And I did find a very nice blog posting, actually, so sort of refers to me and SpinRite and the podcast, with a little, you know, not written directly to me.  He called it a "Universal Fix for Windows KSOD."  And I didn't know what that was.  I understand it was black rather than blue, so he used the "K" of "black" rather than, you know, because both "blue" and "black" start with a "B."  So it's not the Blue Screen of Death, it's the Black Screen of Death, but KSOD.



He said:  "Ever had your Windows installation inexplicably die, leaving your computer unusable without a fix?"  He said:  "I have, more times than I'd like to count.  The last time this happened was yesterday, when Windows 7 would not boot" - I'm sorry - "when Windows 7 would only boot into a black screen with a movable cursor, also known as the blacK Screen Of Death."  He wrote:  "It was a serious case, considering none of the safe modes or repair functions in the Windows boot options would work.  Each option would universally end in either a blacK Screen of Death or the classic Blue Screen of Death after hanging on aswRvrt.sys during safe boot.  After exhaustively eliminating all possible regular fixes that were available on the Internet, I decided it was time for the big guns:  Steve Gibson's SpinRite.



"Prior to trying SpinRite, I first tried Kaspersky's Rescue Disk 10, which was entirely useless," he wrote, "for my case.  After booting from the rescue USB dongle, I would always get a 'Missing Operating System' error in the boot screen.  Not reassuring."  He said:  "I have long been a fan of Steve Gibson's Security Now! podcast, which is why I knew of the tool.  I knew the tool would be one of the few things that might do the trick, so I gave it a shot.  After an hour running SpinRite 6, and a few reboots later, my Windows 7 installation was working perfectly, as if nothing had ever happened.  SpinRite saved the day."



And then he said:  "TL;DR."  He said:  "SpinRite saved my machine from a perpetual and otherwise unbeatable KSOD scenario.  And my guess is that, if you are having KSOD problems, then SpinRite is one of the few things that might help you, too."  So, wow.  Thanks for the nice blog post.



LEO:  Very nice, yeah.  Okey-dokey, Steve.  What else do we want to talk about today?



STEVE:  So we've got a couple of closing-the-loop items.



LEO:  All right.



STEVE:  From our terrific listeners.  My mention of S3 and how I get a $2.83...



LEO:  Speaking of Amazon services, as a matter of fact.



STEVE:  Yes.  Got a lot of interest from our listeners.  Logan Rogers tweeted:  "Are you just using S3 Bucket from @awscloud for your backup that you talk about on Security Now!?  Looks like cheap cloud backup."  And then Ugly Bob said:  "I'm curious as to what software you use to back up to AWS?"



LEO:  We used to use Jungle Disk.  Remember the good old days?



STEVE:  Yup, yup.  And in fact my bookkeeper's machine is still being backed up by Jungle Disk.  I get a report by email on the weekend of the...



LEO:  So they're still around?  Or you're just using the old...



STEVE:  Yup.



LEO:  Oh.



STEVE:  Yeah, I think Rackspace bought them.



LEO:  Oh, that's right.



STEVE:  And I had a perpetual license from the beginning, and they're still honoring it.



LEO:  Oh, that's neat.  They're still around.



STEVE:  So I'm very impressed, yeah.  So what I use is a, naturally, a command-line utility.  It's at s3.codeplex.com, and it's just called S3.  And it is a wonderful little interface to Amazon's S3 and EC2 web services.  The description at CodePlex - by the way, CodePlex is going away.  So if anyone is interested, you might want to go to s3.codeplex.com and grab it.  It describes itself as a "Windows command-line utility for Amazon's S3 and EC2 web services that requires no installation."  It's a single .EXE file.



LEO:  Your kind of software.



STEVE:  Yup, with no DLLs.  It only requires .NET 2, so will work on a plain vanilla Windows 2003 installation or, what was that, Windows 7 was the parallel non-server version.  So it just runs.  And actually I'm running it on XP, so it works there, too.  Although I probably have .NET added to it.



Under features they said:  Efficiently uploads and downloads large numbers of files, or whole directories, between Amazon S3 and Windows PCs.  Everything in one .EXE.  Nothing to install or configure.  Just download it where it's needed and run.  It doesn't require anything except .NET 2.0, which you probably already have on our machines.  Works well in an automated backup solution or as an ad-hoc system admin tool.  Can split large files into chunks for upload without creating any temporary files on disk.  Fast parallel file transfers.  Actually, it says "(Coming Soon)," and I wouldn't hold my breath because I think 2010 was the last version of it.  Can use HTTP HEAD command to quickly determine which files don't need to be uploaded because they haven't been updated using the /sync option.  Support for EC2 operations.  Free and open source, and nothing to pay.  No paid versions.



So what I do is I have a batch file.  And when I download the audio and recompress it for Elaine, I simply type "send," space, and then "sn-622" is what I'll be doing.  And that batch file both sends a copy to GRC and also sends a copy to Amazon.  So I'm just, you know, it goes off, and it's trouble-free.  Anyway, there was so much interest shown by our listeners that I wanted to let them know what I'm using.



There's also, for Firefox users, and I don't know about Chrome, there's a really neat S3 add-on called Open S3Fox.  And that is a full S3 bucket browser which you're able to configure that allows you to use a web page like an app and poke around within your S3 buckets.  So it works great for me.  And as I said, boy, is it inexpensive.



Steve Whisenhant said:  "I drive Uber, and I'm considering enabling hotspot on my phone as a perk for riders.  What are the risks to me, assuming WPA2 and password posted in the car?"  So, boy.  I would be worried because we know that there is, for example, on an iOS device, I trust Apple, I trust their security, but they do have protocols that bridge the phone's iOS to WiFi-connected iOS devices.  Again, I'm sure Apple has done everything they can to make it secure.  But the idea of deliberately allowing unknown riders to essentially attach to your phone's tether, WiFi tether, I mean, I get what a bonus it would be to riders.  But, boy, that would make me nervous.



The alternative, of course, is to get a cellular standalone hotspot.  PC mag did some coverage at the beginning, or earlier this year, titled "The Best Mobile Hotspots of 2017," where they talk about a range of them and have in-depth reviews and so forth.  The downside, of course, is then you have to purchase that and then pay for a service separate from your phone.  The upside is there's no connection to your phone.  So I'm trying to think whether you could isolate.  It might be possible to use one of the mobile WiFi repeaters.  We've talked about them before, like you use in a hotel, where you use the hotel's WiFi, but then it creates an access point to your devices and is a NAT router.  That would at least provide some level of security, although you're not really providing NAT routing protection in the direction you want because you'd want your phone to be behind the NAT, rather than your customers or your riders behind the NAT.



So, Steve, I don't think there's a clean answer.  Again, I would trust Apple to have made this secure.  But if there was a mistake that was made, you're potentially allowing someone to hook to your phone.  Probably not a big deal.  Probably not worth worrying about.  But at least maybe worth being aware of.



Also I got a kick out of this.  Someone whose Twitter name is Trust No-One...



LEO:  Great name.



STEVE:  @twust.  He said:  "@LastPass two-factor authentication is only as strong as a cleartext link emailed to you when you click 'I lost my two-factor authentication.'"  



LEO:  This always worries me about two-factor because, no matter what you have, there's always, "Did you lose your device?"  And then there's backup processes.  And some of these just aren't very secure; right?



STEVE:  Exactly.  I mean, my notes...



LEO:  Which means that a bad guy will use that.



STEVE:  Well, exactly.  And so what I wrote here was the important lesson here is the weakest link principle:  No security system is stronger than its weakest link, as we've often said.  So, consequently, no authentication system can be stronger than the strength of what's required to bypass it.  And of course, as I've said, this is the tradeoff that SQRL makes deliberately in a different direction.



And as it's getting close to happening - I'm shaking out the final details of the install, update, and uninstall systems with the guys online - I'm recognizing that it's not ever going to replace usernames and passwords.  I doubt that it will because it does make that tradeoff.  It says, if you're willing to be ultimately responsible, you can have ultimate security.  But I'm sure there are a lot of people who don't want - who the idea of having no other recourse except what SQRL provides, and when you and I, Leo, cover this in detail, which we will when it's available for everyone to play with, everyone will see that I've gone to tremendous lengths to create all kinds of recovery systems.



But ultimately, ultimately it's up to the individual to be somewhat responsible.  And I think some people won't bother.  They're just like, eh, I'd rather have someone I can call and cry to.  So again, not for everybody.  But it would be cool if it were an option because when used it, as far as we know, cannot be bypassed.  And in fact you're even able, once you're comfortable with it, to set some switches, send a beacon to the sites you then visit saying, "I want you to please disable alternative login and all account recovery."  So if this all happened, then there's just nothing a bad guy could do to get you.  So I think it's worth having this out there as an example of how it can be done.  And maybe it'll become more than an example.



LEO:  Well, and I should point out that, at least on LastPass, you get to choose what backups you want, what multifactor authentication systems are allowed.



STEVE:  Nice.



LEO:  And so, you know, I have a YubiKey as a default, but I've disabled everything except YubiKey and Google Authenticator.  So I would presume...



STEVE:  That's perfect.



LEO:  Yeah.  If I lost my YubiKey, I'd have a backup; but it would be something secure as opposed to email.



STEVE:  Yeah, and it's worth - it's not something that we all do.  But our listeners might pretend to, like, not know their authentication code and try to perform a reset, how difficult it is, because that's what a bad guy is going to do.



LEO:  Right.  Yeah, and some of these systems are terrible.  I mean, they're just really terrible.  PayPal's is awful.



STEVE:  Uh-huh.  Well, I know.



LEO:  So it's just - I can't remember what it was because I have the dongle; right?  But then when you get to...



STEVE:  Well, they disabled it.  They stopped supporting...



LEO:  Trying to remember what it was.



STEVE:  ...the token and began using SMS.  It's like, uh, what?  Because it used to be expensive.  They were doing it through VeriSign, which charged them per authentication.  It was free to end-users, but not to the companies who were using the service.



LEO:  I've seen, it's probably not PayPal, but I've seen some services use secret questions as their fallback.



STEVE:  Ugh, yeah.



LEO:  Which means the secret questions are the most - that's the weak link.



STEVE:  Yup.



LEO:  And that's a pretty weak link.



STEVE:  Yup.  So Rex Moncrief cited an article in ZDNet which was titled "This Android Spyware Can Record Calls, Take Snapshots, and Video Targets."  And then it also says "Gmail, LinkedIn, Snapchat."  So he sent the link and asked the question:  "Any feedback on now this 'retrieves' data from Threema?"  And Threema was among the apps that this malware is reputed to be able to compromise.  And the answer, of course, is it does it before it's encrypted.  And I thought - I liked the question because it reminds us that, as strong as end-to-end encryption can be - and I believe Threema is ultimately strong because it gives the user responsibility for validating the identity of the endpoints, which you have to have.  If you don't have that, then you're trusting, inherently trusting an authentication mechanism other than yourself.



But the point is both endpoints have to be uncompromised.  So it's the man in the middle, it's the traffic interception that is being prevented with encryption and authentication; but not the person typing into a keyboard where there's been a hook placed in the keyboard, monitoring the keystrokes that the person is then securely encrypting before it goes over the air.  So again, if you compromise the endpoints, all bets are off.  It doesn't matter how secure your link is because you're able to grab the data before it happened.  And that's what this malware does, which of course is then able to compromise everything.  And it has specific hooks for specific applications, which is why there were some that were enumerated.



And, finally, Chris Shaw asked, I thought, a great question, which has never come up.  He asks:  "Does printing a PDF to a PDF strip malicious content from the file?"  And it does.  It hadn't occurred to me.  But that is a great way.  It neuters all of the active stuff because what you're printing is a rendered image of the PDF, which is then described as a new PDF of that image.  But going through the printer rendering stage, very much sort of like I was describing with the voting machines, where we forced a standardized paper trail to link both ends, this is sort of similar.  All the form content, any macros, any funny business that might have existed in the first PDF is flattened into a simple visual description of what is seen on the page.  That's what the printer renders and then redescribes in a PDF.  So great question, Chris.  And, yeah, that's one way.



In fact, I do it all the time.  I own a copy of Adobe's full Acrobat.  And sometimes I'll, like, want to remove some content from a PDF or highlight something.  And I've noticed that, when I redisplay the page, the original content flashes briefly, or it takes a while for the highlights to appear because they're overlaid, you know, they're being rendered on top of the underlying PDF.  Which annoys me.  So I will then print my marked-up PDF, which creates a PDF that is now solid.  It no longer has descriptions of those individual things.  It describes the final result, which then displays instantly with no secondary render due to the history of the previous PDF's markup.  So same kind of idea.  So again, great question, Chris.



LEO:  I am going to do a follow-up.  I didn't want to do a follow-up on this PayPal thing till I'd fixed it.



STEVE:  Ah.  Okay.



LEO:  So if you have misplaced your PayPal security key or two-factor authenticator, they want to do it via, as you pointed out, SMS.  You said, "I forgot."  It said, okay, well, now you have a couple of choices.  We can call you, or you can answer these security questions, which are mother's maiden name and last four digits of your social.



STEVE:  [Sighing]



LEO:  Now, that was the default.  I'd obviously never looked at this.  So I have changed it to other security questions and nonsense answers.



STEVE:  Other data.



LEO:  But I'm glad I did that, and I invite everybody to do that.  If you're assuming, oh, PayPal's secure because I've got a password and two-factor, look at the fall-through.  And it wouldn't just be PayPal, be everybody else.  Look at the fall-through.  And, wow, now I have some homework to do.  But, wow, that was not a good fall-through.  I'm lucky I didn't get hacked.



STEVE:  No.  Publicly available information.  



LEO:  Yeah.  And that was - I don't believe I'd set that up.  I'm not stupid.  And even whenever I set this up, 15 years ago, I wouldn't have used mother's maiden name and last four digits of the social.  I would have come up with - they do insist on security questions.  But I would have done what I recommend everybody do, which is answer them with nonsense.



STEVE:  Exactly.



LEO:  Another passphrase which you store in LastPass.



STEVE:  Yeah, and unfortunately don't use 0000 as your last four digits of the social because that's what the bad guy will test.



LEO:  Yeah, right away, yeah.



STEVE:  So they don't even need the information, then.



LEO:  Because I don't think I would have, yeah, I'm pretty sure that's the default.



STEVE:  Has to be.



LEO:  So check your PayPal accounts.  And you might want to, if you haven't ever set up secret questions, you might want to do so.  Holy cow.



STEVE:  Good.  Good tip.



LEO:  Okay.  Yeah, and I didn't want to say that until I'd fixed it.



STEVE:  Cool.



LEO:  All right.



STEVE:  And that's our podcast, my friend.



LEO:  Yes, sir.  We do this show at 1:30 Pacific, 4:30 Eastern, 20:30 UTC on Tuesdays.  And you can stop by and join us.  Erica came all the way from Chicago to sit in the studio and watch this show.  Thank you for being here, Erica.



STEVE:  Yay, cool.



LEO:  You can do that by emailing tickets at TWiT.tv.  We do this show in my office, which has only about four or five seats.  So that's really important, if you want to come to a show that's done out of my office, to email tickets@twit.tv.  We have a lot more room in the big studio, but...



STEVE:  Standing-room only.



LEO:  We have had people stand.  Or sit at my feet.  Which is fun.  If you want to watch live, you can go to TWiT.tv/live and watch the live stream.  And if you do that, please join us in the chatroom.  Great people in there.  Well, mostly.  All you have to do is going to irc.twit.tv.  You can use a browser, or I figure if you listen to this show you probably have an IRC client lying around.  Irc.twit.tv, we'd love to have you in there.



For everybody who has a job, and their job is not 9:00 to 5:00 Monday through Friday, or is Monday through Friday 9:00 to 5:00, you might want to then download an episode.  You can listen at your leisure.  Steve's got not only audio, the most popular format, but he also has transcriptions you can read as you listen.  And for some people that's how they learn.  Megan was just saying she learns by reading.  That's how you do it.  Go to GRC.com.  While you're there, pick up SpinRite, the world's best hard drive recovery and maintenance utility.  Take a look at the other freebies Steve gives away out of the goodness of his own heart.  Keep up on what's going on with SQRL, lots of other things.  You can also get copies of audio.  And we even have video, for reasons no one understands, here at TWiT.tv/sn.



STEVE:  You're forward-looking, Leo, so video.



LEO:  Well, it seemed like a good idea at the time.  You resisted it, and you were probably right.  But now you know I'm wearing a tie, so there you go.  You can also subscribe.  That's the best thing to do.  We have audio and video subscriptions available.  Wherever you get your podcasts, just look for Security Now!.  And we will be back here next week talking about probably the most important topics we do anywhere on TWiT, your privacy and security.  Thanks, Steve.



STEVE:  Thanks, my friend.  See you next week.  Bye.



Copyright (c) 2017 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details: http://creativecommons.org/licenses/by-nc-sa/2.5/.






