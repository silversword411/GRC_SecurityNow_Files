GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#964

DATE:		March 5, 2024

TITLE:		PQ3

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-964.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  Last week we covered a large amount of security news; this week, not so much.  There are security stories I'll be catching us up with next week.  But after sharing a wonderful piece of writing about the fate of Voyager 1, news of an attractive new Humble Bundle, a tip of the week from a listener, a bit of SpinRite news, and a number of interesting discussions resulting from feedback from our listeners, our promised coverage of Apple's new "PQ3" post-quantum safe iMessage protocol consumed the entire balance of this week's podcast budget, bulging today's show notes to a corpulent 21 pages.  I think everyone's going to have a good time.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here with lots of security news, including a look at Apple's new PQ3 encryption.  Seems Apple might have oversold its capabilities.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 964, Episode 964, recorded Tuesday, March 5th, 2024:  PQ3.



It's time for Security Now!.  We are not for sale, but we are for free here for you right now.  Steve Gibson, our man in charge.



STEVE GIBSON:  Actually, Leo, for the right price, I have a feeling we would be for sale.



LEO:  Yeah, I mean, everybody has a number.  I'm not saying that.  Yeah, so I'll take that sign off, yeah.



STEVE:  That's right.  No.



LEO:  Hi, Steve.  Good to see you.



STEVE:  Great to be back for your pre-vacation podcast.



LEO:  Ah, yes.  I'll be gone for, I should tell everybody, I'll be gone for two weeks.  Mikah will be filling in, yeah.



STEVE:  Thought we'd catch you before you leave.  So this is, as I was wondering with you before we began recording, whether this was the shortest title we've ever had for a Security Now! podcast.  I mean, maybe.  But I guess there could be one called IQ or something in the past.



LEO:  I think this is the shortest.  Pawn to Queen 3 is the name of this show.



STEVE:  That's right.  Or Post-Quantum Level 3.



LEO:  Oh, what's that?  Ooh.



STEVE:  Well, exactly.  That's something that Apple just made up out of whole cloth because...



LEO:  Of course they did.



STEVE:  They said, we want to be special, and we're going to denigrate all of the other things that have come before us that are down at Level - they even have Level 0, where I think they miscategorized Telegram, saying that it didn't have end-to-end encryption.  It's like, what are you talking about?  Anyway, I'm a little annoyed with Apple...



LEO:  Uh-oh.  Uh-oh.



STEVE:  ...after learning about this.  But speaking of the podcast, before I forget, I brought whole site search back up at GRC.  We've had a number of our listeners who have written saying, hey, I want to, you know, you say you have transcripts, and I know you do, but I can't find what I want.  And of course,  yes, you could use Google and put site:grc and then whatever it is.  Anyway, and in the upper right of all of the pages of GRC has been a search box for a long time.  But Google changed something that broke it.  And I, you know, I figured, well, I've got to get SpinRite done.  So anyway, SpinRite is done, so search is back.  So I'm just saying that the whole site search, and you can narrow the search to just Security Now! podcast transcript search, if you like.  So that's there.



We've got a fun podcast.  Last week we covered a large amount of security news; this week, not so much.  There are security stories which I'll be catching us up with next week.  But after sharing a wonderful piece of writing about the fate of Voyager 1, news of an attractive new Humble Bundle, a tip of the week from a listener, a little bit of SpinRite news, and a number of interesting discussions resulting from feedback from our listeners, our promised coverage of Apple's new "PQ3" post-quantum safe iMessaging protocol consumed the entire balance of this week's podcast budget, bulging today's show notes to a corpulent 21 pages.



LEO:  Whoa.



STEVE:  So we've got lots to talk about, and I think everyone's going to have a good time.  And of course we've got a great Picture of the Week.



LEO:  That would have been a good title, though.  Corpulent.  You might have considered...



STEVE:  Corpulent, a Corpulent Podcast, that's right.



LEO:  You might have considered that.  I have the Picture of the Week teed up.  I have not yet looked at it.



STEVE:  One you will appreciate.  



LEO:  Oh, nuts.  My screen has changed again.  Why don't you describe it while I push the buttons here.



STEVE:  Okay.  Well, so we have one of the wonderfully classic fake O'Reilly...



LEO:  Oh, I love these, yeah.



STEVE:  ...programming coding books.  You know, and this one is titled "Coding..."



LEO:  So true.



STEVE:  "Coding with GPT."  And then the subtitle is "Introducing the uncanny valley into your codebase."  Because, you know, as we would say on this podcast, "What could possibly go wrong?"  And then, you know, O'Reilly is wonderful because they've always got some animal of some sort, I don't know how the animals are chosen, if there's any rhyme nor reason, or if they just roll the dice.



Anyway, this one, of course, in keeping with GPT, has a completely bizarre fictitious made-up animal.  It's got the head of a dog with cute little floppy ears looking off to the left, but then for its body it's got like the backend of a goose or a bird, a turkey, who knows what it is, of some kind, whose feet are on backwards.  And then at the very end it's got like a tail of some other sort of bird looking, I mean, it's just a weird, bizarre animal.  And I don't remember where it was, Leo, but it was on one of your podcasts, I think it might have been on a MacBreak Weekly a couple weeks ago, where we saw some weird animation of like a bunch of monkeys that were...



LEO:  From Sora, yeah.



STEVE:  It was just, it, like, hurt you to watch it because they just kind of kept turning around and merging in...



LEO:  Hallucinating, yeah.



STEVE:  Oh, wow.



LEO:  I remember.



STEVE:  Anyway, so this is a hallucinated animal, and I would recommend that everyone keep ChatGPT away from their code.  Actually, we'll probably get to it next week.  One of the things that I didn't have room for this week, Ben Nassi, a person whose work we've covered before, he was at the University of the Negev, they've produced a ChatGPT worm which they call Morris II.



Okay.  So we've had some fun keeping an eye on Voyager.  And on the occasion that we may have finally lost control of this intrepid explorer, I found a wonderful piece of writing about this that I know our listeners will enjoy and appreciate.  It's a blog post by someone named Doug Muir titled "Death, Lonely Death."  And Doug writes:  "Billions of miles away, at the edge of the Solar System, Voyager 1 has gone mad and has begun to die.



"Let's start with the 'billions of miles.'  Voyager 1 was launched in early September 1977.  Jimmy Carter was a hopeful new President.  Yugoslavia and the USSR were going concerns, as were American Motors, Pan Am, F.W. Woolworth, Fotomat booths, Borders bookshops, and Pier 1.  Americans were watching 'Happy Days,' 'M*A*S*H,' and 'Charlie's Angels' on television; their British cousins were watching 'George and Mildred,' 'The Goodies,' and Tom Baker as the Fourth Doctor.



"If you turned on the radio, 'Hotel California' by The Eagles was alternating with 'Dancing Queen' by Abba (and, if we want to be completely honest, 'Car Wash' by Rose Royce).  Most cars still ran on leaded gasoline, most phones were still rotary dial, and the Internet was a wonky idea that was still a few weeks from a working prototype.  'The Thorn Birds' was on top of everyone's bestseller list."  Blah blah blah.  He goes on to sort of set the place for us, reminding us also that "It was the summer of 'Star Wars.'  And Voyager 1 was blasting off for a tour of the Solar System.



"There's no way to pack the whole story of Voyager 1 into a single blog post.  Here's the TLDR:  Voyager was the second spacecraft to fly past Jupiter, and the first to take close-up photos of Jupiter's moons.  It flew on past Saturn, and examined Saturn's moon Titan, the only moon with an atmosphere.  And then it flew onwards, on and on, for another 40 years.  It officially left the Solar System and entered interstellar space in 2012.  It just kept going, further and further into the infinite emptiness."  And he says:  "You know about the Golden Record?  Come on, everybody knows about the Golden Record.  It's kind of a hokey and cheesy idea; it's also kind of amazing and great."



He says:  "Voyager has grown old.  It was never designed for this.  Its original mission was supposed to last a bit over three years.  Voyager has turned out to be much tougher than anyone ever imagined, but time gets us all.  Its power source is a generator full of radioactive isotopes, and those are gradually decaying into inert lead.  Year by year, the energy declines.  The power levels relentlessly fall.  Year by year, NASA has been switching off Voyager's instruments to conserve that dwindling flicker.  They turned off its internal heater a few years ago, and they thought that might be the end.  But those 1970s engineers built to last, and the circuitry and the valves kept working even as the temperature dropped down, down, colder than dry ice, colder than liquid nitrogen, falling towards absolute zero.



"Voyager stored its internal data on a digital tape recorder.  Yes, a tape recorder, storing information on magnetic tape.  It wasn't designed to function at a hundred degrees below zero.  It wasn't designed to work for decades, winding, rewinding, endlessly re-writing data.  But it did.  Voyager kept going, and kept going, until it was over 15 billion kilometers away.  At the speed of light, the Moon is 1.5 seconds away.  The Sun is about 8 light minutes away.  Voyager is 22 hours away.  Send a radio signal to it at lunch on Monday, and you'll get a response back Wednesday morning."  He says:  "I could go on at great length about Voyager  the discoveries it's made, the Deep Space Network that has maintained contact with it over the decades, the ever-shrinking crew of aging technicians keeping it alive on a shoestring budget, how amazing has it all been.



"In 1990, just before Voyager's camera shut down forever, the probe turned around and looked backwards.  It zoomed in and took a picture of Earth.  By that time, it was so far away that Earth was just a single pale blue pixel.  Seeing the blue pixel, Carl Sagan wrote:  'That's here.  That's home.  That's us.  On it, everyone you love, everyone you know, everyone you ever heard of, every human being who ever was, lived out their lives.  The aggregate of our joy and suffering, thousands of confident religions, ideologies, and economic doctrines, every hunter and forager, every hero and coward, every creator and destroyer of civilization, every king and peasant, every young couple in love, every mother and father, hopeful child, inventor and explorer, every teacher of morals, every corrupt politician, every 'superstar,' every 'supreme leader,' every saint and sinner in the history of our species lived there - on a mote of dust suspended in a sunbeam.'"



He says:  "Voyager kept going for another 34 years after that photo.  And it's still going.  It has left the grip of the Sun's gravity, so it's going to fall outward forever.  Here's a bit of trivia:  Voyager 1 currently holds the record for most distant active spacecraft.  It's not even close.  The only other contender is Voyager's little sister, Voyager 2, which had a different mission profile and so lags billions of kilometers behind its older sibling.  Here's another bit of trivia:  If you're reading this in 2024, it's very unlikely that you will live to see that record broken.  There are only two other spacecraft outside the Solar System, Voyager 2 and New Horizons.  Both of them are going to die before they get as far as Voyager 1.  And nobody  not NASA, not the Chinese, not the EU  is currently planning to launch another spacecraft to those distances.  In theory we could.  In practice, we have other priorities.



"We thought we knew how Voyager would end.  The power would gradually, inevitably, run down.  The instruments would shut off, one by one.  The signal would get fainter.  Eventually either the last instrument would fail for lack of power, or the signal would be lost.  We did not expect that it would go mad.  In December 2023, Voyager started sending back gibberish instead of data.  A software glitch, though perhaps caused by an underlying hardware problem; a cosmic ray strike; or a side effect of the low temperatures; or just aging equipment randomly causing some bits to flip.



"The problem was, the gibberish was coming from the flight direction software, something like an operating system.  And no copy of that operating system remained in existence on Earth.  This is a problem NASA long since solved.  These days, every space probe that launches leaves a perfect duplicate back on Earth.  Remember in 'The Martian' how they had another copy of Pathfinder sitting under a tarp in a warehouse?  That's accurate.  It's been standard practice for 30 years.  But back in 1977, no one had thought of that yet.



"Voyager Mission Control used to be a couple of big rooms full of busy people, computers, giant screens.  Now it's a single room in a small building in the San Gabriel Valley, in between a dog training school and a McDonald's. The Mission Control team is a handful of people, none of them young, several well past retirement age.  And they're trying to fix the problem.  But right now, it does not look good. You cannot just download a new OS from 15 billion kilometers away.  They would have to figure out the problem, figure out if a workaround was possible, and then apply it, all with a round-trip time of 45 hours for every communication with a probe that is flying away from us at a million miles a day.  They're trying, but nobody likes the odds.



"So at some point  not tomorrow, not next week, but at some point in the next few months  they'll probably have to admit defeat.  And then they'll declare Voyager 1 officially over, dead and done, the end of a long song."



LEO:  If you want to know more about this, there's a wonderful documentary, came out last year.  I'm playing the trailer from it.



STEVE:  Good.



LEO:  "It's Quieter in the Twilight."  And it focuses on those people, working in that little office in San Gabriel Valley, who have gotten gray with Voyager.  There it is.  There's the little office.  It's a great story.  In fact, during the filming of the documentary, that's when they had decided they had to turn off the heater.  So it's quite a dramatic moment when they're making that decision and what the cost of that will be and so forth.  These are the people, the handful of people still running the Voyager mission.  And they've gotten a lot farther than anybody thought they would, which is why they're getting kind of long in the tooth.



STEVE:  That looks like Gramps.



LEO:  It is, it's great.  This is a fabulous documentary.  I couldn't recommend it more highly.  It's called "It's Quieter in the Twilight."  And they're not just talking about Voyager.  I think they're talking about the scientists, the engineers, the mission specialists that made this happen.  And they're still running this distant, distant object.  It's kind of cool.



STEVE:  It really is something to be said, Leo, for our ability to create something with this kind of endurance in that kind of environment.



LEO:  No kidding.



STEVE:  I mean, it is hostile out there.  You know?



LEO:  Yeah, yeah.  Well, it feels like it's just luck.  But on the other hand, NASA keeps doing it.  Remember...



STEVE:  Yeah, the same thing with Rover.



LEO:  Exactly.



STEVE:  What, Spirit and, I want to say Accountability, that wasn't.



LEO:  Yeah.  They finally had to end the drone, the little hovercraft, single rotor hovercraft they had that was flying around.  What a story.  But they only expected to do a few, a handful of flights with that, and they got dozens.  It's just - it's very impressive, I agree.  It really is. 



STEVE:  Yeah.



LEO:  And it's a great story.  And, hey, salute to Voyager.



STEVE:  Yeah.  I have a feeling that I will briefly let everyone know when they finally pull the plug.  But otherwise...



LEO:  Watch the documentary.



STEVE:  ...we've enjoyed - yeah.



LEO:  So good, yeah.



STEVE:  Good.  So I got a very cool tip of the week from Patrick Johnson, who wrote:  "Hi, Steve.  Long-time listener, and when downloading 6.1 I discovered that I've been a SpinRite owner for 10 years as of Tuesday.  I was surprised to hear in SN-963 that Firefox had let you keep the dedicated search box for so long.  I forgot when it was turned off by default for new installs."



LEO:  That was my reaction, too.



STEVE:  Right, yeah.  "But I did keep turning it back on for quite a while until I discovered that the CTRL+K shortcut for search still worked."



LEO:  Hmm.



STEVE:  Yeah, huh?  Remember how we liked CTRL+L because it immediately selected the URL, which was very handy for, like,  then doing a CTRL+C to copy it and paste it somewhere else.  He said:  "Treating the text entered in the omnibar as a strict search query, even for terms that look like a URL, no quotes needed."  He says:  "As a .NET developer, libraries and frameworks with the something-or-other .NET naming convention turn up quite a bit in my searches."  He says:  "Chromium seems to have copied this, so that now Brave, Edge, and Chrome all behave like Firefox with CTRL+K forcing a search."  And he said:  "Thanks for everything you do.  Patrick."



So Patrick, thank you.  This was news to me, so I tried it, and it works perfectly.  I previously shared my discovery, which I just mentioned, about CTRL+L.  And I've now added CTRL+K to my bag of keyboard shortcuts because it leaves no confusion.  You hit CTRL+K, and immediately the box - in my case I've just left my Firefox with the default Google search.  And so it immediately comes up and like puts Google in front, confirming what engine it will send this to when I hit ENTER.  So very cool tip, thank you.



I have no big SpinRite news, which at this stage is what we hope for.  Everything continues to go well.  I'm pushing forward on several fronts.  I'm spending time in GRC's forums, watching as new people encounter SpinRite 6.1 for the first time.  So I'm learning about their experiences which will be informing SpinRite's forthcoming FAQ.



One thing I've seen is that Linux users need SpinRite in a format that's directly usable to them.  Requiring non-Windows users to briefly somehow use Windows, like someone else's Windows system, to have SpinRite create a bootable USB drive for them is something I always planned to work around, so not to require that.  So I've been working on the tech to add direct bootable image downloading to GRC's servers so non-Windows users will be able to obtain a bootable drive image which they can copy directly to a USB drive, you know, just using "dd" in Linux, although I guess Ubuntu has a nice little drive imaging tool built in, which will allow them to boot their license copies of SpinRite.



I also have the beginnings of GRC's forthcoming email list system running that I know that a lot of our listeners are waiting for.  Someone just said that he received a note from X, you know, Twitter X, that he would no longer, he'd be losing his DM'ing privileges if he didn't get more active or something.  It's like, what?  You know, Elon just insists on creating trouble for people.  So I know that a lot of our listeners are only using Twitter so that they can send me notes from time to time, so I'll be fixing that.  So I'm in the process of bringing up our email list system.  And I plan to just maintain two lists, one specifically for weekly podcast announcements to our Security Now! listeners, and another for GRC-related news, you know, new software, updates, features, and so forth.  So people will be able to join either or both as they choose.  And I'm also working on SpinRite's documentation, which is coming along nicely.



Having learned from Microsoft last week that certificate reputation matters, even though they are deliberately mute about how exactly that reputation is earned, I presume it's a function of the exposure of a new certificate in the signing of non-malignant software.  So right after last week's podcast I used this relatively new certificate that I've got to cosign GRC's top six most downloaded Windows freeware.  In order of decreasing average daily download rates, the top six are ValiDrive, the DNS Benchmark, InSpectre, Securable, InControl, and Bootable.



What surprised me was that, for the first time ever, ValiDrive has taken the top slot to become GRC's most downloaded freeware with nearly 2,200 downloads per day. Anyway, taken together, those top six are being downloaded, in aggregate, 5,323 times per day.  So now they're all carrying this new certificate whose reputation I want to earn, and I expect it'll become golden before long.



I should also mention something really weird that was just pointed out to me, and that is an update of Microsoft's Trusted Root Certificate Program.  If I read this correctly, they're saying that the specialness of EV is being deprecated in August.  They specifically say, in August of 2024, all certificates, EV or not, will be treated identically.  So, what?



Anyway, I wrote to a good friend of mine at DigiCert, who back in the day were podcast listeners, I don't know if he still is, saying, am I reading this right?  Is EV, you know, sort of in the same way that EV has been deprecated for SSL, right, like none of our browsers show us anything special about EV certs anymore.  I'm wondering what's going on with EV code-signing.  Maybe the same thing.



Anyway, I expect I'll have an answer from him, and I'll let everybody know next week.  I'm certainly curious because, you know, I spent a month figuring out how to get dynamic, on-the-fly, server-side hardware security module cosigning for SpinRite, and now it looks like before it really gets used that much, maybe it's going to just be a nonstarter.  Anyway, no, no.



LEO:  We have EV certs for TWiT which we pay a lot of money for.



STEVE:  Now, those are TLS certs, though; right?



LEO:  Yeah.



STEVE:  Yeah.  And so I'm talking about code-signing EV.



LEO:  Oh, for code-signing, of course, yeah, yeah, yeah.



STEVE:  Which Microsoft says, eh, we're not going to bother paying attention to that anymore.



LEO:  So TLS EV certs continue.



STEVE:  Yes.



LEO:  Oh, whew, okay.  Scared me.



STEVE:  They continue, although - yeah.  They continue, although they're like, you know, only if the user goes to look is it clear anymore that a site is an EV cert.  It's, you know, I've stopped using them because there's no point.  There's no benefit.



LEO:  They don't show green anymore or anything like that.



STEVE:  Nope.



LEO:  Yeah.



STEVE:  They don't show anything different.  It's all disappeared from the chrome.  So let's take our second break, Leo.  And then we're going to do some feedback from our listeners before we get to a really interesting topic of what Apple has done with post-quantum crypto for their iMessage.



I am so glad you brought me back to the Humble Book Bundle because you know Cory Doctorow quite well.



LEO:  Oh, good friend, yeah.



STEVE:  Yes.  And this is, as far as I know, Cory's first incredibly affordable Humble Book Bundle.  I mean, I'll just read the description of it from the page.  I've got it in the show notes.  It's humblebundle.com/books/cory-doctorow, and so it's easy to search and find.



The description reads:  "Doctorow's visions of the future:  Lose yourself in the visionary fiction of Cory Doctorow, the celebrated author and digital rights activist known for his masterful explorations of the intersection of tech and society.  'Little Brother' is a stark exploration of surveillance and authoritarianism in the backdrop of a major terrorist attack on San Francisco.  'Radicalized' features four distinct sci-fi novellas, each telling a gripping story inspired by today's technologies and societal trends.  In 'Red Team Blues,' you'll follow along with a well-connected money laundering expert on the most dangerous and exciting gig he's ever taken on."



LEO:  He's actually a forensic accountant is his hero in this series, and I love it.  It's so good.  He's so good.  I love Cory.



STEVE:  Oh, very cool.



LEO:  Yes.



STEVE:  It says:  "Get all these and more, 18 books in total, and help support the Electronic Frontier Foundation with your purchase."



LEO:  Oh, great.



STEVE:  So 18 books.  As always, it's a pay what you feel it's worth, you know, as little as $18, a dollar each or more, in order to support Cory and the EFF.  So anyway, I wanted to make sure everybody knew about this Doctorow Humble Book Bundle.



LEO:  That's such a good thing.  Thank you.



STEVE:  Yeah, very cool.  So Tom Desmond said:  "Hey, Steve.  I'm just finishing listening to Episode 963" - so that was last week - "and I think I'm missing something on the cookie email link login loop.  The" - he has it in quotes - "'absolutely secure' process seems to assume that no bad person has compromised the email.  Think about this.  Someone gains access to the email from wherever.  They see an old email loop link.  They go to the site and guess the username - maybe it's the same email address - and they get the link in the compromised email, and they are in.  Am I missing something?"



And sadly, Tom, you're not.  I hate it when I miss something that's so obvious.  Tom is, of course, completely correct.  The solution which I described last week of embedding the email link requestor's IP address and browser cookie into the link would absolutely and strongly prevent anyone who did not request the email-based login link from using it, if they were to just passively observe it in flight or at rest.  That's nice.  But as Tom points out, that doesn't solve the problem that we're still just as dependent upon email security as ever.  Nothing prevents an attacker, as Tom says, who has the ability to monitor the user's email account, from themselves going to the site, requesting a login link from their IP location with their browser, then clicking the link that arrives in the compromised user's email.  Whoops.  And yes, they're logged in.



So this means that while there's no reason not to at least embed, for example, the requestor's browser cookie in the link, we don't really gain anything from doing that, and we remain dependent upon the security of email for all of our logon security, whether or not it's passwordless using email only or passworded with the ubiquitous "I forgot my password" total security bypass.  So thank you, Tom.  And I should say Tom was just the first of a bunch of listeners who said, uh, Steve, I think you are solving the wrong problem.  And they were right.  So thank you, everybody.



Our longtime listener and early advertiser with the podcast, Alex Neihaus, he wrote:  "Regarding email/password links in SN-962."  So that's two weeks ago.  He said:  "If you send a link with a hash/IP address, et cetera, you eliminate the ability to copy the link and open it in another browser.  It would also encourage clicking on links in email, something enterprises are trying hard to untrain users from doing."  He said:  "Also, if you did get such a link that you opened in Incognito mode, and the server expected a persistent session cookie, next time after restarting the browser you asked for the link in either normal mode or Incognito, it would not exist.  Email links instead of passwords are a good idea," Alex thinks, "and can be made secure as you describe.  But it does not fit with the way many people interact with browsers, especially those who use multiple browsers."



And of course Alex's point is a good one.  Heavy use of email links for common logon would have the effect of "untraining" users against clicking on links in email.  If the world were to switch over to that, and I don't think there's any danger of that happening, there would certainly be a lot more email link clicking going on.  And it's also true that the tighter we make the anti-spoofing security, the greater the chance for rejecting a valid user.  Which of course would upset them because this is the way they log on.



The presumption is that the user wishes to log on now, not later.  So they would request a link, then immediately go to their email to find and execute the link, which would take them back to where they just were.  But Alex is right that there are various things that could go wrong.  So as we so often encounter, everything is a tradeoff.  The great benefit of email link logon is that the user needs to remember and possess nothing, and they are able to log in from anywhere, as long as their email is secure, and they have access to it.



I think the best thing probably to come from the last few weeks of this discussion has been the recognition that passwords really only amount to logon accelerators, and that as long as every username and password logon opportunity is also accompanied by "the dog ate my password" bypass, nothing else we do to further increase logon security actually matters - not hardware dongles, not fingerprints, not one-time passwords, nothing.  It all just amounts to security theater.  This, in turn, implies that the security of our email is far more important than is commonly appreciated.  And so I think that's another good thing to really come from this discussion.



And before we finish this, another listener sent this bit of fun.  He wrote:  "The Taco Bell app seems to have gone passwordless in favor of emailing you a link whenever to try to log in.  It's not great user experience for me in that it's slow and prone to spam filtering.  When you're trying to put in your order from the car on your way to the Bell, it's enough to make you drive to Chipotle instead."



So, yes.  You want to make sure, if you're going to switch to - and I thought it was interesting that Taco Bell, the Taco Bell app now just wants to have you immediately confirm who you are by replying to a link in email.  They don't want to create user friction.  They apparently think that they are reducing it by not requiring the user to use a password all the time.  But as this listener pointed out, they might lose some business.



Joel Clermont said:  "There is a security-focused reason to enforce a max password length when hashing with Bcrypt."  He said:  "I recently discovered the recommendation and wrote up more details here."  And so Joel provided a link to his write-up, which is located at "masteringlaravel.io," which was interesting.  It turns out that Laravel uses Bcrypt as its PBKDF password-hashing algorithm; and for reasons that defy understanding, the Bcrypt algorithm itself has a hard and fixed internal password length limit of 72 bytes.



Okay, now, normally we'd think that 72 bytes was way more than anyone might need.  But unfortunately, many characters in non-Latin alphabets are encoded as three and even four bytes.  So it would be possible for a non-Latin password to max-out Bcrypt's fixed 72-byte limit with just 18 characters.  And again, that's probably enough.



But still, Joel makes a good point about there being possible exceptions to the boundless upper password length presumption.  I'll just note that a simple solution to this problem for anyone who might be stuck using Bcrypt would be to first run the user's truly unlimited-length password through an SHA-256 hash, or even an SHA-512.  Why not?  And do it just once.  SHA-512 will take a password of any length, even a long one using non-Latin characters, and reduce it to exactly 64 bytes, which can then be sent into Bcrypt to be strengthened against brute-force guessing.  Which is why you use Bcrypt at all in the first place.  Anyway, the cute little hack here is just stick a hash in front of something that can't take long enough passwords, and you completely solve that problem.



LEO:  Does Argon2 have the same issue, or just Bcrypt?



STEVE:  No, no.  It just was an early design decision in Bcrypt.



LEO:  Bcrypt's kind of old, actually; isn't it?  And kind of funky.



STEVE:  Yes.  Yes.  And one would not even start using it at this point.  



LEO:  Yeah.



STEVE:  Yeah.  Earl Rodd said:  "Regarding the discussion of Nevada's request for an injunction against Meta encrypting messages for minors in SN-963.  As I listened, I was aware that the great missing piece is data.  All of those quoted on both sides, privacy advocates and law enforcement advocates, have lots of opinions, but they present no data.  Data like, you know, the number of times criminals were believed to go free because of encrypted messages stopping prosecution; or the number of times such crimes were not prosecuted at all due to encryption; or data like times that using unencrypted messages was known to lead to harm to minors.  Is this really a case of balancing privacy, i.e., being pummeled with advertising, versus throttling the ability of law enforcement to find and prosecute serious crime?"  He said:  "I suspect it's not that simple.  But without any data, who knows the actual tradeoff?"



Okay.  I think Earl makes such a good point here.  You know, we've been driven so far away from actual data collection that we appear to have forgotten that actual data could be collected and made available.  You know, we're just never asking for it; right?  We've, you know, we've become the "anecdotal example" culture, being slammed from one extreme to the other.  And here's the problem:  Expressing a strong opinion is easy.  It excites.  It's junk food for the mind.  But actually collecting, tabulating, and evaluating data is difficult, time-consuming, and expensive work.



And the biggest problem is, as Earl suggests, the result of all that actual work probably doesn't support the extreme views of either side.  So it's not in either side's best interest to be presented with any of those pesky facts which would likely lead to some form of compromise policy.  Better just to wave our arms around, attempting to terrify everyone, and jam through regulations that support an ideology rather than reality.  It seems that too much of the world is working this way these days.



We've seen countless examples of how the move to an online digital world has very likely provided law enforcement agencies with a treasure trove of readily accessible, indexable, and searchable information the likes of which hasn't ever been known.  It's never been known before.  We've recently learned that these agencies are consumers of the information that commercial data brokers have been collecting about us for years.  They're buying this data.  Today, everyone leaves footprints and bread crumbs wherever they go online, and whatever they do.



If law enforcement knew what all of our various service providers know, and what our ISPs probably know about where we wander on the Internet - and we should assume that they do know any of that if they wanted to - then law enforcement knows pretty much everything about us, certainly more than was ever knowable about people before the Internet.



As a law-abiding citizen of the United States, which for the most part leaves its citizens alone unless some intervention is required, I'm all for law enforcement working behind the scenes to keep a lid on criminal behavior.  That's what I want them to be doing.  So given the unparalleled tools that are now available to aid in that work, it is difficult to get too worked up over the encryption debate.  Law enforcement authorities can even know who we're talking to if they wish, even if it's much more difficult to peer into the content of those conversations.  Although I also do not have any data to back up my opinion, my intuition is that we're already sitting with a workable compromise for both sides.



Could the privacy absolutists wish for more privacy?  Absolutely.  Are they going to get much more?  Probably not.  Do our law enforcement agencies wish they could also listen in on anyone's conversations?  I'm sure they do.  Are they going to be able to do that?  Let's hope not because that would clearly be a step too far in the "Big Brother" direction.  And again, they must already have access to far more data about us than they even know what to do with.  It seems to me that the right balance has already been struck.  But I think Earl's point is such a good one about hey, you know, like where's the data to back up any of this either way?  So thank you for that.



Elliot Alderson said:  "Hi, Steve.  The issue with passwordless login the way you described is if someone only has email on their phone."  He said:  "I don't have email on my computers because I don't need it."  Really.  "If the IP and everything is baked into the link, I'm SOL.  That also makes it very difficult to log in on other browsers."  That's true.



So, okay.  Having listened to everyone's thoughts about this, it's clear that the potential automation offered by an email link creates more problems than it solves.  So the solution for email-only-based login is to return a visible, easy-to-transcribe, one-time six-digit token, like everyone who uses one-time passwords has become used to.  Just email the token to the user and request that they enter the token into the browser they're currently using.  If they receive the email on their phone, they have the advantage of seeing both the token and their browser page at the same time.



This eliminates the "untraining" about never clicking on a link, and it's not really that much more work for the user to type in six digits.  And they still don't need to remember anything, their password or anything else.  So I think, you know, stepping back from the link idea, you know, more automated though it was, it's got a lot of downsides which our listeners, astute as they are, have all pointed out.  So thank you, everybody.



SecFan said:  "Steve, in SN-963 yesterday, a listener had mentioned the idea of having a standardized way of documenting a site's password requirements so that they could be automated.  In writing my own algorithmic password manager, named Passify" - that's a cute name - "that I messaged you about previously, I had the same thought and wanted Passify's algorithm to accommodate known requirements wherever possible.  I discovered that Apple had put some thought into this and created 'Password Manager Resources' on GitHub."  And that's what it's called, github.com/apple/password-manager-resources.



He said:  "It includes a JSON format" - just sort of as I was suggesting last week - "for documenting password rules as well as a number of other helpful things for Password Managers to automate or semi-automate password management.  The idea is not completely new.  Apple's Safari browser has supported a 'passwordrules' HTML attribute that uses the same rule markup as the JSON to describe requirements for a while.  Thanks again for all your work.  Looking forward to the official release of the new SpinRite."



Anyway, it's cool that Apple has already done the work of creating some of this structure.  And my comment last week was about it being a heavy lift to get the industry adoption, and that appears to be the case.  This is now four years old, and it has remained somewhat obscure.  Of course, if any website wanted to modernize, they could simply place a relatively high lower limit on password length and accept passwords of any greater length containing any characters.  You know, just hash the thing a bunch, and just don't worry about how long it is or what the thing contains, and do it on the browser side, and you're good to go.  And then on the server side, they could prevent endless brute forcing password guessing by putting up a roadblock if someone is misguessing the password to a site some number of times.



Rob said:  "Security Now! question:  After hearing your discussion of session cookies, I was reminded of my own security concern I've had for years.  Seems I'm always logged into my Google account based on session cookies.  And so it seems, if a hacker could grab those cookies from my computer, they'd now have access to all my emails and many other Google things.  This," he says, "even with Google's Advanced Protection Program."



He says:  "I believe I've even migrated to a new MacBook and was automatically logged in fine on the new MacBook," he says, "though my memory could be a bit off on that."  And I don't think so.  I think there's a lot of inter-browser synchronizing going on these days.  He says:  "Didn't seem there was any other verification that my hardware was the same, et cetera.  Just doesn't feel very secure to me.  Am I missing something?"



Okay.  So first of all, a bit of clarification here because Rob used the term "session cookies," which I'm also seeing a lot of casual use about.  When a cookie is set in a browser, the browser can be told how long to retain that cookie in the form of either an expiration date, at which point the cookie will then expire, or a maximum age.  If either of those are provided, then the cookie is considered to be "persistent," and the browser will retain and return that cookie's value until it is refreshed, that is, the setting of the cookie is refreshed, the cookie is deleted by the user or by the website, which is able to do so, or that designated end-of-life is reached, and the cookie self-expires.  Any cookies that have not expired will be retained from one browser session to the next.  That is, they are persistent.



But I mentioned that the specification of these expirations was optional.  If a cookie is set without any explicit expiration, then it is considered to be a session cookie, as in good for this browser session, because it will be retained only for the current browser session.  It is explicitly retained only in RAM, and it is never written in any way to any permanent form of storage.  Once the browser closes, the browser session ends, and the cookie will be forgotten.



So to Rob's question and uneasiness, it is the case that our browser's persistent cookies are now being retained over the long-term, and that's what keeps us logged into our various websites persistently from one boot-up of our computer to the next.  And it's definitely the case that if bad guys could arrange to obtain those persistent login cookies, they could immediately impersonate us.  In fact, this is exactly what that Firesheep Firefox add-on did many years ago.  And this was one of the primary motivators for the move to always-present HTTPS.



Firesheep relied upon the mistake that most websites of the era were making.  After authenticating with a username and password whose communication was protected by HTTPS, most popular sites would drop back to plain, lower overhead, unsecure, cleartext HTTP, figuring wrongly that the user's identity had been protected.  They failed to take into account that the only way that user was remaining logged on from one HTTP page query to the next was that each browser query was accompanied by cookies; but that without the encryption protection provided by HTTPS, those cookies were being sent in the clear so that anyone who could eavesdrop could obtain them and immediately impersonate the user, obtaining parallel access to their currently logged-on web sessions.



So yes, Rob, our browser cookies do serve as persistent long-term authentication tokens; and anyone who might have some means for obtaining them could, indeed, impersonate their owner.  Sometimes when logging into a site you'll see a "Trust this browser and remain logged in?" checkbox.  This is useful when you do not want to leave a persistent cookie behind in that browser, for example, when logging into a shared Internet caf machine.  You'll also want to carefully and explicitly log out of that machine once you're finished in order to at least invalidate that cookie, even if it isn't completely removed from the machine.



But that "Trust this browser and remain logged in?" question is offering to change the logged-on authentication cookie from a temporary session cookie to a long-term persistent cookie.  And there is a difference.  One sticks around.  The other one should, if the browser's behaving itself correctly, and I know we talked about this in the past, there was a point at which the various important browsers, which at this point means Chromium-based browsers and Firefox and Safari, were being very careful never to write that transient true session-level cookie to permanent storage.



"Hi, Steve.  Listened to SN-962 and wanted to provide a bit of additional information regarding your discussion on password requirements.  I work for a company in the financial services sector, specifically insurance.  I did want to note that many companies in this sector still rely on mainframes, and that is likely the case for many of these maximum password length limits and character restrictions due to IBM's fanatical backwards compatibility."



He said:  "Several years ago, I interned for another company in this sector and was shocked at the requirements for passwords.  Seven to eight characters only, and the only symbols allowed were the @ and the $ characters."  He says:  "How much of this is whose fault is a bit opaque to me even still, but I can say that it isn't entirely the mainframe's fault.  I've seen that more 'modern'" - and he has that in quotes - "implementations allow up to a 40-character 'password phrase,' but still restrict a few characters.  All that being said, you see these kinds of restrictions often in sectors that continue to rely on the mainframe, and other legacy platforms and products."



And I should also mention that a number of our other listeners have since sent me notes saying that, Steve, mainframes are not gone.  They still exist.  You know, they're not around the way they used to be, but they're not gone.  So I appreciate the feedback that we still haven't moved very far.  One of the most significant things we've seen and learned through the years of this podcast is the power and prevalence of inertia, which is everywhere, acting to keep things from changing.



One trick that I ought to mention that works quite well is to map a hashed value to some fixed alphabet.  Say that a backend system can only accept eight characters of upper and lower case alpha, numbers, and as we saw in this case, an @ sign and a $ sign.  Okay?  So that's 26 lowercase and 26 uppercase.  So now we're at 52.  Ten digits brings us to 62.  And two other characters brings the total alphabet size to 64.



Now, okay.  First of all, the fact that the alphabet size turned out to be 64, you know, representable by exactly six binary bits, you know, 2^6 is 64, that should jump out at everyone.  Okay.  So let's take this case first.  We want a web frontend to give its users unrestricted passwords.  So it hashes whatever they provide, doesn't matter, absolutely doesn't matter, preferably using some contemporary PBKDF function like Argon2.  The output will be 256 evenly distributed bits.  We know that that's what hashes give us.  So they are simply taken from either end, let's take them from the right end, you know, the least significant right end, if we regard this 256-bit value as a number.



So we'll take them six bits at a time.  Each of those six bits is mapped back into one of those 64 characters.  So after eight groups of six bits have been taken, those eight characters are then passed back to the creaky old mainframe as the user's password.  The result is that the user is able to use whatever crazy long and special-character-filled password they may choose, and whatever they choose is first strengthened by a modern PBKDF to harden it against brute-force attack and then converted into a high-entropy eight-character password which meets the needs of the backend system.



Okay.  But what if the backend system's password alphabet wasn't conveniently 64 characters, which allowed us to take, just simply take six bits at a time?  Okay.  So consider this:  One way to visualize taking six bits at a time is dividing the large hash value by 64, which is to say the size of the alphabet.  In a binary representation, division by two is a right shift of the bits.  So dividing by 64 is shifting right by six bits.  We can think of the bits that are shifted off to the right as the remainder of the division.  So to extract a single character from an alphabet of 64, we are actually dividing the large binary hash value, we're doing a long division by 64 and taking the remainder, which will always range from 0 to 63, thus having one of 64 possible values.



So this means that we can extract characters of an alphabet of any size we wish from a large hash value by performing successive long division of the hash by the size of the alphabet we wish to extract where the remainder from each division will be the value that's mapped back into the alphabet.  Anyway, I've always found this to be very cool and been fond of this solution since it provides high-entropy, evenly distributed characters from an alphabet of any size, extracted from large binary values, such as those that are conveniently produced from hashing.



Our second-to-the-last bit of feedback, we have a bit of a longer format piece, but this is significant and important, and there's some lessons here.  And I've had several bits of conversation with him since, which I'll share.  He said:  "Hello, Steve.  I've been a longtime listener, and even longer time SpinRite user, and I enjoy your weekly updates and common sense perspectives on all kinds of security topics.  I want to share a rather long story with you and would like to hear your opinion about an interesting in-progress responsible vulnerability disclosure.  Here goes."  And it's actually not that long.



He said:  "In October 2023 I came across a sign-in page of a high-profile company with 20-plus billion USD revenue, and more than 100 million monthly active users on their website.  Not a small business.  For my work, I sometimes just for fun regularly visit websites with the Developer Tools tab open.  This time I noticed that the sign-in page of this particular website returned a Response Header that mentioned nginx/1.12.2 as the web server.  In itself not a big problem, but it is better to hide this information from the public.  However, if the page was really served by nginx 1.12.2, then the website would have a big problem because this version of nginx is very old and has a number of critical and high-severity vulnerabilities that are known to be actively exploited.



"So being a good citizen, I wanted to tell them about the issues, just for the sake of improving their security and of course also for improving the security of their 100 million monthly active users."  Apparently he was one of those.  He said:  "So I searched on the company's website for their policy for responsibly disclosing these vulnerabilities, but the best I could find was the email address for privacy-related questions.  I wrote an email telling them about the vulnerabilities, included some screenshots for evidence, and asked some questions about their statements in their privacy policy and terms and conditions on protecting my information and just keeping the website bug-free.  I was not looking for any financial compensation.  I just wanted to make the world a better place.  I soon received a response that a bug report had been filed, and that it could take up to 120 days for bug reports to be tested and validated.  I received no answers for my questions about privacy.



"Fast-forward to February 2024, 118 days since their message that the bug report had been filed.  I politely asked them about the status of the bug report, notified them that the vulnerabilities still existed, and asked them if my personal data is still safe with them.  A week later I received an email from them saying that after review and risk rating by their information security team, the bug report qualifies for a bounty payment of $350.  If I agree to this payout agreement, I also agree to keep my silence about the issue.  My questions about privacy and trust and new questions about their responsible disclosure policies remain unanswered.  In the meantime, the issues still exist, and a small tour around some of the other websites of the same company show similar problems.



"Now, the fact that these vulnerabilities are so easy to find, the fact that the reported web server is so old, the fact that it is so easy to remove a server banner, and the fact that it is still not fixed after more than four months makes me believe that this could be a honeypot."  Well, he's being extremely generous.  I don't believe that.  I think that we're dealing with is a typical irresponsible company.  Anyway, he says:  "If it is, then I think that it is a very, very risky attempt of having a honeypot."  You know, why would you have a honeypot as your main logon server?  I doubt that's the case.  He said:  "Because it is easy to find, it is on pages where people enter their username and password, and if the media get hold of it, then the brand damage of such a high-profile company will be big."



He said:  "I have asked them directly if this is a honeypot, but I did not get a reaction yet.  What do you think about this?  What should I do?  Accept the payout and keep my silence?  Should I also report the 30-plus other vulnerabilities on their websites and sign-in pages?  Should I tell them that just by looking at the web server Response Headers, I know that they use a mix of nginx 1.12.2, Microsoft IIS 10.0, ASP.NET 4.0.30319, Shopify, WordPress, et cetera?  Or should I just leave it up to them to find it all out?  I mean, what could possibly go wrong?  Kind regards; and again, I love your show.  Keep going.  Robert Blaakmeer."



LEO:  Wow.  He's a Good Samaritan.



STEVE:  So Robert, yeah.  And I wrote, I said:  "Robert, I agree that this is a conundrum.  I took a quick peek at the version of nginx, and what immediately jumped out at me - as it would any researcher, nefarious or not - was CVE-2021-23017.  It's a well-known, remotely exploitable, 'off by one' error which gives an attacker unauthenticated remote code execution capability."



LEO:  Oy, oy, oy.



STEVE:  Uh-huh.  And what's more, a three-year-old working Python proof-of-concept exploit is also publicly available.



LEO:  Well, there you go.



STEVE:  I mean, it just doesn't get any worse.



LEO:  Even a three year old could do it.  Geez.



STEVE:  So Robert, I agree that you're right to be worried about them, and worried in general.  It appears that any miscreant wishing to target this enterprise has everything they need to do so.  Unfortunately, although you've done the right thing every step of the way, their ongoing negligence in dealing with this, which you had no way of anticipating upfront, has now compromised you, since you've acknowledged to them that you're in possession of information that could damage them - either reputationally or for use in an attack.  And as we also know, attorneys can allege that, just by looking at their web server headers, you've obtained "non-public network hacking" information, and some judge who knows no better could be convinced.



LEO:  It's happened.



STEVE:  It's happened before.



LEO:  Yup.



STEVE:  I think I would have nothing further to do with them.  The problem with a company that has become this large is that the people who you really should be talking to are unknown and inaccessible to you by design.  Between you and them are layer upon layer of chair-warming functionaries who have no idea what you're even talking about.  At this point, my recommendation would be for you to turn everything over to CISA and let them take it from there.



LEO:  Perfect.



STEVE:  This completely discharges your responsibility in the matter while insulating you from any blowback since no one, not the company in question nor any judge, could fault you for confidentially and privately informing the U.S. Government's Cybersecurity & Infrastructure Security Agency of this potential critical vulnerability after having first given the company in question all of November, December, January, and February to deal with upgrading their servers.



CISA, the rare government agency that appears to actually have its act together, has a simple procedure for report filing.  Go to www.cisa.gov/report.  I also created a GRC shortcut so that even if someone forgets the agency's name, going to grc.sc/report will always bounce you to that page at CISA.  The page has several categories of problems to report.  If you scroll down you'll find "Report software vulnerabilities or ICS vulnerabilities," which appears to fit best.  If you go there, you get bounced over to CERT.org, which is the CERT Coordination Center at Carnegie Mellon University.



So I'd fill out that form to let the U.S. government powers that be know about this.  One of the things you can do is attach a file to your submission.  So I would send them your entire email thread with the company so they can see that you have acted responsibly at every step.  You'll have done the right thing.  And when CISA and CERT reach out to contact the company about their negligence over a four-year-old known critical vulnerability across all their web platforms, you can bet that they'll wind up speaking to someone who can effect the required changes.  And at that point you'll have done everything you can, while insulating yourself as well as possible from any annoyance this company might feel over being made to help themselves.



LEO:  And you're pretty sure, I mean, if you did this, the company might not be happy about it, but honestly I think you'd be safe from prosecution.  Right?



STEVE:  Yes.  I think you're as safe as you possibly could be.  I should mention that he did that.  He sent back to me afterwards that he took everything, he submitted it to them, he got a response saying "Thank you for the submission, and we'll take it from there."



LEO:  Yeah.  Excellent.  That's probably the way to start in these things, I would say.



STEVE:  And finally - I really, I do, I think so, too.  I mean, we've seen and have covered on the podcast, so our listeners have heard too many instances where the company goes after the responsible disclosure guy, saying, you know, you hacked us, you bad person.  It's like, no.  No no no no no no.  Anyway, yeah.  I would just...



LEO:  Randal Schwartz did jail time for that; you know?  I mean...



STEVE:  Yup.



LEO:  It's terrible, yeah.



STEVE:  Yeah.  It's wrong.  Okay.  And finally, for next week, we have the first appearance of a zero-click GenAI application worm.  As I mentioned at the top of the show, Ben Nassi sent me a tweet.  He said:  "Hi, Steve.  I'm Ben Nassi, a former researcher at the Ben-Gurion University and Cornell Tech, and the author of 'Video-based Cryptanalysis and Lamphone' which you previously covered in your podcasts.  I just published a new research paper on a worm that targets GenAI applications.  We named it Morris II.  Guess why?"  He said:  "I think the audience of Security Now! will love to hear more about it.  And keep on with the great podcast that you and Leo are doing beyond  999.  By the way, the research was also published on Bruce Schneier's blog."



LEO:  Oh, that's pretty cool, yeah.



STEVE:  So I suspect that next week's podcast will be titled "Morris II."  So stay tuned for a look at the "worminization" of GenAI, where I'm sure we'll be answering the question, "What could possibly go wrong?"



LEO:  Now, PQ Trois.



STEVE:  So we can guess, we know, that PQ stands for Post-Quantum.  And of course we'd be right.  So is this Apple's third attempt at a post-quantum protocol, after one and two somehow failed or fell short?



LEO:  What?  No.



STEVE:  You know, PQ3, what happened to PQ1 and PQ2?  No, Apple has apparently invented levels of security.



LEO:  Oh.



STEVE:  And put themselves at the top of the heap.  So PQ3 offers what they call "Level 3 Security."



LEO:  Oh.



STEVE:  So here's how SEAR, S-E-A-R, Apple's Security Engineering and Architecture group, introduced this new protocol in their recent blog posting.  They write:  "Today we are announcing the most significant cryptographic security upgrade in iMessage history with the introduction of PQ3, a groundbreaking post-quantum cryptographic protocol that advances the state of the art of end-to-end secure messaging.  With compromise-resilient encryption and extensive defenses against even highly sophisticated quantum attacks, PQ3 is the first messaging protocol to reach what we call Level 3 Security, providing protocol protections that surpass those in all other widely deployed messaging apps.  To our knowledge, PQ3 has the strongest security properties of any at-scale messaging protocol in the world."



Well.  So they're proud of their work, and they've decided to say that not only will iMessage be using an explicitly quantum-safe encrypted messaging technology, but that theirs is bigger than - I mean better than - anyone else's.  Since so far this appears to be as much a marketing promotion as a technical disclosure, it's worth noting that they have outlined the four levels of messaging security which places them alone at the top of the heap.



Apple has defined four levels of messaging with levels 0 and 1 being pre-quantum - offering no protection from quantum computing breakthroughs being able to break their encryption - and levels 2 and 3 being post-quantum.  Level 0 is defined as "No end-to-end encryption by default," and they place QQ, Skype, Telegram, and WeChat into this Level 0 category.  Now, this causes me to be immediately skeptical, you know, of this as being marketing nonsense, since although I have never had any respect for Telegram's ad hoc basement cryptography, which they defend not with any sound theory, but by offering a reward to anyone who can break it, we all know that Telegram is encrypted, and that everyone using it is using it because it's encrypted.  So lumping it into Level 0 and saying that it's not encrypted by default is, at best, very disingenuous; and I think it should be beneath Apple.  But okay.



Level 1 are those pre-encryption algorithms, I'm sorry, pre-quantum because 0 and 1 are both pre-quantum, you know, non-quantum safe, so Level 1 are those pre-quantum algorithms that Apple says are encrypted by default.  The messaging apps they have placed in Level 1 are Line; Viber; WhatsApp; and the previous Signal, you know, before they added post-quantum encryption, which we covered a few months ago; and the previous iMessage, before it added PQ3, which actually it hasn't quite added yet.  But it's going to.



Now we move into the quantum-safe realm for levels 2 and 3.  Since Signal beat Apple to the quantum-safe punch, it's sitting all by its lonesome at Level 2 with the sole feature of offering post-quantum crypto, with end-to-end encryption by default.  Presumably, as Signal's relatively new PQXDH protocol moves into WhatsApp and other messaging platforms based on Signal's open technologies, those other messaging apps will also inherit Level 2 status, lifting them from the lowly levels of 0 and 1.  But Apple's new PQ3 iMessaging system adds an additional feature that Signal lacks, which is how Apple granted themselves sole dominion over Level 3.  Apple's PQ3 adds ongoing post-quantum rekeying to its messaging, which they make a point of noting Signal currently lacks.



This blog posting was clearly written by the marketing people, so it's freighted with far more self-aggrandizing text than would ever be found in a technical cryptographic protocol disclosure, which this is not.  But I need to share some of it to set the stage, since what Apple feels is important about PQ3 is its attribute of ongoing rekeying.



So to that end, Apple reminds us:  "When iMessage launched in 2011, it was the first widely available messaging app to provide end-to-end encryption by default, and we've significantly upgraded its cryptography over the years.  We most recently strengthened the iMessage cryptographic protocol in 2019 by switching from RSA to Elliptic Curve cryptography, and by protecting encryption keys on device with the Secure Enclave, making them significantly harder to extract from a device even for the most sophisticated adversaries.  That protocol update went even further, with an additional layer of defense, a periodic rekey mechanism to provide cryptographic self-healing, even in the extremely unlikely case that a key ever became compromised."



Again, extremely unlikely case that a key ever became compromised, they acknowledge.  But now they have rekeying, so it's possible.  "Each of these advances were formally verified by symbolic evaluation, a best practice that provides strong assurances of the security of cryptographic protocols."  And about all that I agree.



Okay.  So Apple has had on-the-fly ongoing rekeying in iMessage for some time, and it's clear that they're going to be selling this as a differentiator for PQ3 to distance themselves from the competition.  After telling us a whole bunch more about how wonderful they are, they get down to explaining their level system and why they believe PQ3 is an important differentiator.  Here's what they say, and I skipped all the other stuff.



They said:  "To reason through how various messaging applications mitigate attacks, it's helpful to place them along a spectrum of security properties.  There's no standard comparison to employ for this purpose, so we lay out our own simple, coarse-grained progression of messaging security levels.  We start with classical cryptography and progress toward quantum security, which addresses current and future threats from quantum computers.  Most existing messaging apps fall either into Level 0, no end-to-end encryption by default and no quantum security; or Level 1, with end-to-end encryption by default, but no quantum security.



"A few months ago, Signal added support for the PQXDH protocol, becoming the first large-scale messaging app to introduce post-quantum security in the initial key establishment.  This is a welcome and critical step that, by our scale, elevated Signal from Level 1 to Level 2 security.



"At Level 2, the application of post-quantum cryptography is limited to the initial key establishment, providing quantum security only if the conversation key material is never compromised.  But today's sophisticated adversaries already have incentives to compromise encryption keys because doing so gives them the ability to decrypt messages protected by those keys for as long as the keys don't change.  To best protect end-to-end encrypted messaging, the post-quantum keys need to change on an ongoing basis to place an upper bound on how much of a conversation can be exposed by any single point-in-time key compromise, both now and with future quantum computers.



"Therefore, we believe messaging protocols should go even further and attain Level 3 security, where post-quantum cryptography is used to secure both the initial key establishment and the ongoing message exchange, with the ability to rapidly and automatically restore the cryptographic security of a conversation, even if a given key becomes compromised."



Okay.  It would be very interesting to hear Signal's rebuttal to this, since it's entirely possible that this is mostly irrelevant, largely irrelevant marketing speak.  It's not that it's not true, and that continuous key rotation is not useful.  We've talked about this in the past.  Key rotation gives a cryptographic protocol a highly desirable property known as Perfect Forward Secrecy.  Essentially, the keys the parties are using to protect their conversation from prying eyes are ephemeral.  A conversation flow is broken up and compartmentalized by the key that's in use at the moment.  But the protocol never allows a single key to be used for long.  The key is periodically changed.  The reason I'd like to hear a rebuttal from Signal is that their protocol, Signal's protocol, has always featured perfect forward secrecy.  Remember "Axolotl"?



LEO:  Yeah.



STEVE:  Yes.



LEO:  Which is for the same purpose as rekeying.



STEVE:  Yes.  It is rekeying.



LEO:  Oh.



STEVE:  Here's what Wikipedia says.  I'm quoting from Wikipedia.  "In cryptography, the Double Ratchet Algorithm, previously referred to as the Axolotl Ratchet, is a key management algorithm that was developed by Trevor Perrin and Moxie Marlinspike in 2013."



LEO:  Oh.



STEVE:  Uh-huh.  "It can be used as part of a cryptographic protocol to provide end-to-end encryption for instant messaging.  After an initial key exchange, it manages the ongoing renewal and maintenance of short-lived session keys.  It combines a cryptographic so-called 'ratchet' based on the Diffie-Hellman key exchange and a ratchet based on a key derivation function such as a hash function, and is therefore called a 'double ratchet.'  The algorithm provides forward secrecy for messages, and implicit renegotiation of forward keys, properties for which the protocol is named."



Right.  In 2013.  In other words, it would be nice to hear from Signal, since Apple appears to be suggesting that they alone are offering the property of perfect forward secrecy for quantum-safe messaging when it certainly appears that Signal got there 11 years ago.  This is not to say that having this feature in iMessage is not a good thing.  But it appears that Apple may not actually be alone at PQ's Level 3, much as they would like to be.  So when do we see it from Apple?



They write:  "Support for PQ3 will start to roll out with the public release of iOS 17.4, iPadOS 17.4, macOS 14.4, and watchOS 10.4, and is already in the corresponding developer preview and beta releases.  iMessage conversations between devices that support PQ3 are automatically ramping up to the post-quantum encryption protocol.  As we gain operational experience with PQ3 at the massive global scale of iMessage, it will fully replace the existing protocol within all supported conversations this year."



Okay.  So now I want to share Apple's description of the design of PQ3.  It includes a bunch of interesting details and also something that Telegram has never had, which is multiple formal proofs of correctness.  Since we're now able to do this, it's a crucial step for trusting any newly created cryptographic system.  Here's what Apple wrote.



They said:  "More than simply replacing an existing cryptographic algorithm with a new one, we rebuilt the iMessage cryptographic protocol from the ground up to advance the state of the art in end-to-end encryption, and to deliver on the following requirements."  Five of them.  "Introduce post-quantum cryptography from the start of a conversation, so that all communication is protected from current and future adversaries.  Two, mitigate the impact of key compromises by limiting how many past and future messages can be decrypted with a single compromised key.  Three, use a hybrid design to combine new post-quantum algorithms with current Elliptic Curve algorithms" - which Signal has already done - "ensuring that PQ3 can never be less safe than the existing classical protocol.  Four, amortize message size to avoid excessive additional overhead from the added security."  And as we'll see, there really is some.  "And five, use formal verification methods to provide strong security assurances for the new protocol."



Okay.  They say:  "PQ3 introduces a new post-quantum encryption key in the set of public keys each device generates locally and transmits to Apple servers as part of iMessage registration."  So devices generate a post-quantum key on device, store it in the Secure Enclave, and then send the public part of that to Apple.  "For this application, we chose to use Kyber post-quantum public keys, an algorithm that received close scrutiny from the global cryptographic community and was selected by NIST as the Module Lattice-based Key Encapsulation Mechanism standard, or ML-KEM.  This establishes sender devices to obtain a receiver's public keys and generate post-quantum encryption keys for the very first message, even if the receiver is offline."  Because, you know, Apple has the matching public key, which is all they need to perform the handshake.  They said:  "We refer to this as initial key establishment."



They said:  "We then include, within conversations, a periodic post-quantum rekeying mechanism that has the ability to self-heal from key compromise" - which that's their fancy term for just moving forward - "and protect future messages.  In PQ3, the new keys sent along with the conversation are used to create fresh message encryption keys that cannot be computed from past ones, thereby bringing the conversation back to a secure state, even if previous keys were extracted or compromised by an adversary.  PQ3 is the first large-scale cryptographic messaging protocol to introduce this novel post-quantum rekeying property."



LEO:  [Buzzer sound]



STEVE:  And I don't - yes.



LEO:  [Buzzer sound]



STEVE:  Exactly.  This is where we need to hear from Signal because, eh, maybe not.  Maybe not ever since Signal added post-quantum to their system.  They say:  "PQ3 employs a hybrid design that combines Elliptic Curve crypto with post-quantum encryption, both during the initial key establishment and during rekeying.  Thus, the new cryptography is purely additive" - which is what Signal did - "and defeating PQ3 security requires defeating both the existing classical ECC crypto and the new post-quantum primitives.  It also means the protocol benefits from all the experience we accumulated from deploying the ECC protocol and its implementations," even though they also said they redesigned it from scratch, so maybe it actually doesn't apply.  But it makes nice marketing speak.



"Rekeying in PQ3 involves" - oh, yeah.  No, it's the next piece I want to get to.  But they said:  "Rekeying in PQ3 involves transmitting fresh public key material in-band with the encrypted messages that devices are exchanging.  The new public key based on Elliptic Curve Diffie-Hellman (ECDH) is transmitted inline with every response.  The post-quantum key used by PQ3 has a significantly larger size" - oh, and does it ever, 2K, we'll talk about that in a minute - "than the existing protocol, so to meet our message size requirements we designed the quantum-secure rekeying to happen periodically rather than with every message.  To determine whether a new post-quantum key is transmitted, PQ3 uses a rekeying condition that aims to balance the average size of messages on the wire, preserve the user experience in limited connectivity scenarios, and keep the global volume of messages within the capacity of our server infrastructure."  Which is all a fancy way of saying we send it a little bit at a time.



Anyway, I'll just interrupt here also to note that it seems likely that PQ3 is rotating its quantum keys more frequently than Signal.  I don't recall the details of Signal's ratchet, and it may have changed since we last looked at it.  But it might also be that this is a distinction without a difference.  In the case of Signal's ratchet, it was designed, not only to provide useful forward secrecy, but as a means for resynchronizing offline asynchronous end points.



The reason I suggest that it may be a distinction without a difference is that these key compromises are purely what-ifs.  No one knows of any scenario where that could actually happen.  The only reason Apple is mentioning it is that they have a way of sidestepping this as a problem, so now it's a problem.  Okay.  You know, if anyone did have a way of sidestepping it, then this problem, you know, the problem would be eliminated.  You know, it's a bit like saying "My password is way stronger than yours because it's 200 characters long."  Okay, that's good, but does it really matter? 



Anyway, Apple continues:  "With PQ3," they say, "iMessage continues to rely on classical cryptographic algorithms to authenticate the sender and verify the Contact Key Verification account key because these mechanisms cannot be attacked retroactively with future quantum computers."  In other words, they didn't bother upgrading that to post-quantum because they're not quantum vulnerable, in the same way that hashes are not.



"To attempt to insert themselves in the middle of an iMessage conversation, an adversary would require a quantum computer capable of breaking one of the authentication keys before or at the time the communication takes place.  In other words, these attacks cannot be performed in a Harvest Now, Decrypt Later scenario.  They require the existence of a quantum computer capable of performing the attacks contemporaneously with the communication being attacked.  We believe any such capability is still many years away.  But as the threat of quantum computers evolves, we will continue to assess the need for post-quantum authentication to thwart such attacks.



"Our final requirement for iMessage PQ3 is formal verification, a mathematical proof of the intended security properties of the protocol.  PQ3 received extensive review from Apple's own multi-disciplinary teams in Security Engineering and Architecture, as well as from some of the world's foremost experts in cryptography.  This includes a team led by Professor David Basin, head of the Information Security Group at ETH Zurich and one of the inventors of Tamarin, a leading security protocol verification tool that was also used to evaluate PQ3; as well as Professor Douglas Stebila from the University of Waterloo, who has performed extensive research on post-quantum security for Internet protocols.



"Each took a different but complementary approach, using different mathematical models to demonstrate that as long as the underlying cryptographic algorithms" - and they actually meant, yeah, okay, right, algorithms, like the post-quantum NIST-approved algorithms themselves.  "As long as the underlying algorithms remain secure, so does PQ3."  In other words, the protocols built on top of those algorithms will also be secure.  They said:  "Finally, a leading third-party security consultancy supplemented our internal implementation review with an independent assessment of the PQ3 source code, which found no security issues.



"In the first mathematical security analysis of the iMessage PQ3 protocol, Professor Douglas Stebila focused on so-called game-based proofs.  This technique, also known as reduction, defines a series of 'games' or logical statements to show that the protocol is at least as strong as the algorithms that underpin it.  Stebila's analysis shows that PQ3 provides confidentiality even in the presence of some compromises against both classical and quantum adversaries, in both the initial key establishment and the ongoing rekeying phase of the protocol.



"The analysis decomposes the many layers of key derivations down to the message keys and proves that, for an attacker, they are indistinguishable from random noise.  Through an extensive demonstration that considers different attack paths for classical and quantum attackers in the proofs, Stebila shows that the keys used for PQ3 are secure as long as either the Elliptic Curve Diffie-Hellman problem remains hard or the Kyber post-quantum KEM remains secure."



And Apple then inserts a quote from Professor Douglas Stebila, which reads:  "The iMessage PQ3 protocol is a well-designed cryptographic protocol for secure messaging that uses state-of-the-art techniques for end-to-end encrypted communication.  In my analysis using the reductionist security methodology, I confirmed that the PQ3 protocol provides post-quantum confidentiality, which can give users confidence in the privacy of their communication even in the face of potential improvements in quantum computing technology.  Signed, Professor Douglas Stebila."



LEO:  It would have been much better in a German accent.  I'm just saying.  You really want people to believe a scientist, do it in a German accent.



STEVE:  That's true.  That's true.  And then Apple continues:  "In the second evaluation, titled 'A Formal Analysis of the iMessage PQ3 Messaging Protocol,' Professor David Basin, Felix Linker, and Dr. Ralf Sasse at ETH Zurich..."



LEO:  Ah, now we are talking here.  We've got some real German scientists.  Swiss is even better.



STEVE:  Okay.  Well, in that case you're going to read the quote when we get to it in a minute.  "...[U]se a method called 'symbolic evaluation.'  As highlighted in the paper's abstract," writes Apple, "this analysis includes a detailed formal model of the iMessage PQ3 protocol, a precise specification of its fine-grained security properties, and machine-checked proofs using the state-of-the-art symbolic Tamarin prover."



LEO:  Oh-ho.



STEVE:  Yeah, you've got to have that.  "The evaluation yielded a fine-grained analysis of the secrecy properties of PQ3, proving that 'In the absence of the sender or recipient being compromised, all keys and messages transmitted are secret,' and that 'Compromises can be tolerated in a well-defined sense where the effect of the compromise on the secrecy of data is limited in time and effect,' which confirms," writes Apple, "that PQ3 meets our goals."  And they quote Professor Basin.  Leo, take it away.



LEO:  "We provide a mathematical model of PQ3 as well as prove its secrecy and authenticity properties using a verification tool for machine-checked security proofs.  We prove the properties even when the protocol operates in the presence of very strong adversaries who can corrupt parties or possess quantum computers and therefore defeat classical cryptography.  PQ3 goes beyond Signal with regards to post-quantum defenses.  In PQ3, a post-quantum secure algorithm is part of the ratcheting and used repeatedly, over and over, rather than only once in the initialization as in Signal.  Our verification provides a very high degree of assurance that the protocol as designed functions securely, even in the post-quantum world."  Doesn't that sound more credible?



STEVE:  Oh, Professor.



LEO:  Don't you believe that?



STEVE:  Professor.  Now, what I found disturbing about the quote, not Leo's rendition...



LEO:  Well, that's disturbing in its own right.



STEVE:  They had the professor say "PQ3 goes beyond Signal with regards to post-quantum defenses."



LEO:  Yeah, that's really interesting.



STEVE:  The dig at Signal was entirely unnecessary and gratuitous.  And I don't understand why Apple apparently feels so threatened by Signal.  Oh, wait, yes, I do.  Signal is open, open design, open source, and entirely cross-platform.  Anyone's conversations can be protected by Signal on any platform they choose, whereas iMessage, just like everything else Apple does, is all about platform lock-in.



So is PQ3 any reason to choose iMessage over Signal?  No.  When we're talking about cryptography, we've learned that there's nothing wrong with adding a belt to those suspenders.  After all, that's why Apple copied Signal in adding post-quantum crypto to existing and well-proven pre-quantum crypto.  Belt and suspenders.  And so, if your work model allows you to be stuck within Apple's closed ecosystem, then you can be confident that iMessage will be secure against any current and future surprises.  I have no doubt that Apple did all of this right.  But you'll certainly be secure enough using Signal, and you'll have the benefit of also having far more freedom.



LEO:  And the ability to talk to the rest of the world.



STEVE:  Exactly.



LEO:  Yeah.



STEVE:  In a secure fashion.  Finally, the paper drops into lots of interesting detail that I won't drag everyone through since we already have the essence of what's going on.  But I did want to share one piece of the techie bits since I think it's interesting regarding the overhead that Apple has introduced by their more or less continuous rekeying.  And actually we find out how continuous.  So keep in mind that text messages are often no longer than old school SMS tweets, or shorter.  You know, sometimes just a few words; right?



Apple explains:  "To limit the size overhead incurred by frequent rekeying while preserving a high level of security, the post-quantum KEM is instantiated with Kyber768.  Unlike the IDS-registered public keys used for the initial key establishment, ratcheting public keys are used only once to encapsulate a shared secret to the receiver, significantly limiting the impact of the compromise of a single key.  However, while a 32-byte Elliptic Curve Diffie-Hellman-based ratchet overhead is acceptable on every message, the post-quantum KEM ratchet increases the message size by more than two kilobytes."



LEO:  Ooh.



STEVE:  Yeah.



LEO:  2,000 characters in English, in ASCII.



STEVE:  Yes.



LEO:  That's lots.



STEVE:  In an "Okay, thanks Mom, I'll be there soon" message.



LEO:  Yeah.



STEVE:  "To avoid visible delays in message delivery when device connectivity is limited, this ratchet needs to be amortized over multiple messages."  In other words, stretched out.



LEO:  Spread out, yeah.



STEVE:  Yeah.  "We therefore implemented an adaptive post-quantum rekeying criterion that takes into account the number of outgoing messages, the time elapsed since last rekeying, the current connectivity conditions.  At launch, this means the post-quantum ratchet is performed approximately every 50, five zero, messages; but the criterion is bounded such that rekeying is always guaranteed to occur at least once every seven days, so at least weekly.  And as we mentioned earlier, as the threat of quantum computers and infrastructure capacity evolves over time, future software updates can increase the rekeying frequency while preserving full backward compatibility."



Okay.  So now everyone knows as much about PQ3 as is necessary.  Apple has followed Signal by adding a believed-to-be-strong post-quantum crypto algorithm to their built-in iMessaging platform, which will be arriving with iOS's next major update.  And if you're talking to somebody who also has been able to upgrade their device to 17.4, then your conversation will be post-quantum encrypted.  Apple has also taken the welcome step of having their largely new iMessaging protocol formally proven by highly qualified academic algorithm researchers with wonderful accents.  It's only a bit sad that Apple clearly feels so threatened by Signal.



LEO:  Celebrity accents impersonated.  Actually, the most important thing is this is Kyber.  All three I think of the NIST protocols are Kyber, as well; right?  There was one that failed.



STEVE:  Yeah, one was like in consideration, and...



LEO:  It crashed.



STEVE:  Problems were found, yeah.



LEO:  Have these been approved yet?  Or is NIST still testing?



STEVE:  We're still - we're in the late phases of this.



LEO:  Okay.



STEVE:  So, I mean, again, everyone is being really careful because this is where you want to be careful.  This is where, you know, care matters.



LEO:  So they're using Kyber768 with perfect forward secrecy.  Those are the two things that probably, you know, should matter to you.  Whether they're better than Signal is a pretty big and loaded question, obviously.  It's a marketing question, I think.



STEVE:  Yeah.  And I'll be surprised after this if we don't hear something from Signal, you know, in their own well-crafted accent.



LEO:  Yeah.  They don't, I mean, honestly, they don't need to prove anything.



STEVE:  No.



LEO:  But it would probably be appropriate for them to say, hey, you know, just so you know, we're not PQ2.  We provide exactly as good protection as Apple does.  And you shouldn't, you know, consider us anything less, despite what Apple may say.



STEVE:  Right.



LEO:  Now, all of this is presuming that at some point we'll get quantum computing, which is still a long shot.  I mean, it's not - we're not close by any means.



STEVE:  Yes.  The only - at this point what everyone is doing is considering the harvest now, decrypt later scenario.



LEO:  Right.



STEVE:  They're wanting to move us into crypto, I mean, it's like, why not?  We've got the computing power.  We've got the bandwidth.  You know, obviously the keys are much longer in order to be as secure as we need them to be.



LEO:  Right.



STEVE:  So there is some cost.  But it's cost we can now afford.  So let's move the entire world now so that, you know, the NSA server farm in Utah can have its next expansion and continue storing stuff, but probably unable to decrypt it once quantum computing does happen.



LEO:  You know, you might wonder, well, when is - so we're talking, yeah, in 20, 30 years, if they still have your messages, and if you still care, absent any kind of post-quantum crypto, they might be able to crack it.  But that's pretty much a long way off.  It's not just around the corner.



STEVE:  Yeah, I mean, we have to imagine that what they're storing are not people saying they'll be home soon for dinner.



LEO:  Right.



STEVE:  You know, that they're storing...



LEO:  Well, they are storing that.  They're storing everything.  That's the thing.



STEVE:  They're storing that, too.  But also, you know, China's conversations with their advanced persistent threat groups and  where the money is being wired in order to fund them and all that.  I mean, all of that that is not yet post-quantum encrypted is, you know, [crosstalk].



LEO:  It's hard to imagine anything, though, that would be decrypted in a few decades that would have anything other than historical interest; right?



STEVE:  I agree.



LEO:  Anyway, it's, you know what, you can do it, we've got the means, why not go ahead and do it.



STEVE:  Yeah, it's like Microsoft publishing the source for MS-DOS finally.  It's like, okay.



LEO:  Thanks.



STEVE:  Does anyone care?  Yeah, thanks a lot.



LEO:  You might care.  But other than that, yeah.  No, and I appreciate you kind of addressing the marketing kind of fluff in here from the technological point of view because I think it is.  It doesn't - it's not needed.  It wasn't necessary, Apple.



STEVE:  I was disappointed.  It's like, wait a minute; you know?  They have a ratchet.  And Signal hasn't told us how often they deploy it.



LEO:  Right.



STEVE:  But, you know, again, nobody doesn't think that Signal is secure.  Everybody knows that Signal, I mean, it's the standard.



LEO:  It's the gold standard.



STEVE:  Yes.



LEO:  I think that's why Apple's attacking them, frankly.  They're the standard.  They're the gold standard.  And there's nothing in here that makes them any less of a gold standard for good post-quantum crypto.



STEVE:  Because Apple went to all this work - it's funny, Leo, I was also thinking about the goggles, whatever the hell they are, you know, where - and I guess I listened to you on Sunday.  I got this - I came away with a clear sense that Apple had gone too far.



LEO:  Yeah.



STEVE:  That they really, they far over-engineered those things.  And I'm beginning to wonder maybe if that's what Apple has become is like a way over-engineering company, because you could argue that there is some cost to mistakenly or needlessly over-engineer things.  And the very fact that they're having to amortize their own post-quantum rekeying over at least 50 messages in order not to bog it down too much suggests that, you know, that this was purely a marketing point.



LEO:  Wow.  Good point, yeah.  But it may be more than just marketing.  It may also be a message to governments, especially China and Russia...



STEVE:  Don't bother.



LEO:  ...but also the U.S.  Yeah.  No, no, we're committed to end-to-end encryption, so knock it off.  I think there's some of that there, as well.  And for that I applaud them.  I think they're doing...



STEVE:  That we're not softening anything.



LEO:  We're going, quite the opposite, we're going the other direction.



STEVE:  Yeah.



LEO:  Yeah.  All right.  Hey, Steve, thank you very much.  I have one more thing I would say is I think Telegram is encrypted, but you have to choose it.  I don't think all Telegram messages are encrypted, at least that's how it used to be.



STEVE:  Yes.  And I agree with you.  I don't think that Apple is lying about saying that it's not encrypted by default.



LEO:  By default is the thing, yeah.



STEVE:  But who doesn't turn that on?  And so it's like, you know...



LEO:  You have to choose an encrypted message, yeah.



STEVE:  Yeah.  And in fact I would argue that having Level 0 is dumb because you either have end-to-end encryption or you don't.



LEO:  Or you don't; right.



STEVE:  So, you know, why have zero and put some people in that category?  Oh, you have to turn it on.  Okay, so everyone does.  It's the first thing they do...



LEO:  Right.



STEVE:  ...when they load it is they turn on the encryption.



LEO:  Yeah, yeah.



STEVE:  Now I'm sure [indiscernible] up, how would you like to be encrypted?  Uh, okay.



LEO:  Okay.  Oh, yeah.



STEVE:  Good idea.  I think I do.



LEO:  And then what's for dinner?



STEVE:  Why do you think I got this dumb thing?



LEO:  Steve Gibson, GRC.com.  That's the place to go.





Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.










GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#965

DATE:		March 12, 2024

TITLE:		Passkeys vs 2FA

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-965.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  What happened with CERT?  What headache has VMware been dealing with?  What's Microsoft's latest vulnerability disclosure strategy?  What's China's "Document 79," and is it any surprise?  What long-awaited new feature is in version 7.0 of Signal?  How is Meta coping with the EU's new Digital Marketing Act that just went into effect?  What's the latest on that devastating ransomware attack on Change Healthcare?  And after addressing some interesting feedback from our listeners, I want to clarify something about Passkeys that is not at all obvious.



SHOW TEASE:  Hey, I'm Mikah Sargent, subbing in for Leo Laporte.  Coming up on Security Now!, first we follow-up on what happened with CERT.  Yes, the listener who talked about a huge serious security flaw in the website of a major enterprise has some follow-up on speaking to the vulnerability analysis team at CERT.  Then we talk about what VMware is dealing with; what Microsoft is choosing to do when it comes to vulnerability disclosure.  Here's a hint.  They're kind of waiting until the end of the week to tell people what's going on.  Plus China ditching America, at least in terms of its technology.  And easily my favorite part of the show, Steve Gibson explains why Passkeys are "far more secure" than any super-strong password plus any second factor.  All of that coming up on Security Now!.



MIKAH SARGENT:  This is Security Now! Episode 965 with Steve Gibson and Mikah Sargent, recorded Tuesday, March 12th, 2024:  Passkeys vs 2FA.



Hello and welcome to Security Now!.  As you can probably tell by listening, this is not Leo Laporte.  No, it is Mikah Sargent who is filling in for Leo Laporte while he is on vacation.  But across from me, at least in the way of the Internet, is the tried-and-true Steve Gibson, who is here as he always is.  Steve, thank you for always being here.



STEVE GIBSON:  And you know, I've noticed that Leo actually, like you are right now, actually looks at the screen in order to, like, make it look like you're looking at me.  So I appreciate that.  And sometimes when Leo's heading off from the main, well, the studio that you're in at the end of MacBreak Weekly where he is, heading to his office, I'll deliberately look this way.



MIKAH:  Just to watch him go.	



STEVE:  Oh, see you, Leo.  That's right.



MIKAH:  Oh, I like that.  I like that.



STEVE:  Anyway.



MIKAH:  It's good to see you.	



STEVE:  Likewise, and thank you for standing in for Leo.  He's on a beach somewhere.  He says he will come back with a tan.  So that's good.  We'll hope no skin cancer.



MIKAH:  Yes, exactly, a safe tan.



STEVE:  At our age, one considers the downside of being too brown.  So anyway, I had said, had suspected, or planned, to title today's podcast "Morris II" after the sort of, well, it was named that way by the guys who created this thing.  It's a way they found of abusing GenAI to create an Internet worm.  And of course the Morris worm was the very first worm on the Internet and is famous for that reason.



Anyway, something came up, as sometimes happens, and so we'll be talking about Morris II next week unless something else comes up that pushes it a little bit further downstream.  And this is the result of a listener's question, which caught me a little bit by surprise because I understand this stuff because I live in it, this being across the network authentication.  And I thought, okay, I realized that his question was a really good one, and it had a really good answer.  And so first I put it as the first of our listener feedback questions.  But as I began to evolve the answer, I thought, no, no, no, okay, this just has to be something we really focus on because it's an important point.



So today's podcast number 965 for March 12th is titled "Passkeys vs 2FA."  And I would bet that even for people who think they really understand all this, there may be some nuances that have been missed.  So I think it's going to be a great and interesting and useful podcast for everyone.  But of course that's at the end.



We're going to start with whatever happened with that guy who complained to CERT about a vulnerability he found in a major website.  What headache has VMware been dealing with just the last few days?  What's Microsoft's latest vulnerability disclosure strategy?  And why does it, well, suck?  What's China's "Document 79" all about, and is it any surprise?  What long-awaited new feature is in version 7.0 of Signal, currently in beta, but coming out soon?  How is Meta coping with the EU's new Digital Marketing Act that just went into effect and requires its messaging platforms to be interoperable with others?  Whoops.



Also, what's the latest on that devastating ransomware attack on Change Healthcare?  Many of our listeners have said, hey, Steve, did I miss you talking about that, or haven't you?  I haven't, so it's time because now we have a lot of information about it.  And, you know, as I said, after addressing a lot of interesting feedback from our listeners which we're also going to make time for, we're going to talk about a few things about Passkeys that are actually not at all obvious.  And of course we always have a great Picture of the Week.  So I think overall a great podcast for our listeners.



MIKAH:  It is time for the Picture of the Week.



STEVE:  Okay.  So as I said, this relates to one of our more famous earlier pictures of the week where there was a big AC generator sitting on a factory floor somewhere, and next to it was a pail, a large pail of dirt.  And a rod was stuck into the dirt to which was attached a big ground wire.  And really, it just demonstrates a complete lack of understanding of the nature of grounding something.  I mean, it's not like, I mean, the dirt in this first picture, the earlier picture, was in a plastic pail.  So it wasn't connected to the earth, which is really what we're wanting for a ground to be.



Anyway, here we have an updated version of the same thing.  It looks like some electrical wiring in progress.  It's not all finished yet.  The construction, whereas most typical electrical high-power wiring are in steel conduit and steel boxes, this is all white plastic.  So that suggests that the need for grounding is even more imperative than it would be if this was all in steel, which is probably grounded somewhere anyway, but not here.  So for reasons that are unclear, a green wire is coming out of this and going into a plastic bag which has been suspended from a plastic piece of conduit.  And this bag, sure enough, it's got about an inch worth of brown dirt at the bottom of it, and the green wire has been certain to, like, get all the way down to the bottom of the bag and bury itself in the dirt.  And I gave this thing, I gave this picture the caption "It's certainly a good thing that bag of dirt was labeled with a 'ground' symbol, or the dirt's purpose might have been unclear."



MIKAH:  Yeah.



STEVE:  We see, like, the equivalent of what looks like a brown Post-it note, but somebody went out of their way to draw a very pretty ground symbol.



MIKAH:  It's very good, yeah.



STEVE:  I mean, because electricity and electronics was my first passion, I've drawn my share of ground symbols in my life, and I would have to say this is right up there with the best of them.  So yes, we have a - again, it's like, who...



MIKAH:  Who, what, when, where, and why?



STEVE:  Would you trust the wiring of an electrician who did this?  I mean, I don't know.  It just, it really - we have some pictures that really do beg the question, what happened here?  Anyway...



MIKAH:  It's just question marks all around.  It's fantastic, but awful.  Oh, dear.



STEVE:  Okay.  So recall from last week our listener who discovered a serious flaw in the website of a major enterprise whose site entertains millions, hundreds of millions of users.  And in fact since then as a consequence of what he forwarded to me, and which I redacted, I now know who that site is and what that enterprise is.  And wow.  So the problem was that these people were using a very outdated version of the nginx web server which contained a well-known critical remote code execution vulnerability for which working Python proof-of-concept code was readily available.



Which all means that any bad guy who went to any of this company's various websites, and there are several, who then looked at the headers of the response from the website, which would identify the server as nginx and version, could then, as I did, and as our listener did, google that version of nginx and see, whoa, there are some problems with that version from four years ago.  And that person would be able to find the proof-of-concept code, as I did, and use it, which neither our listener nor I did, in order to get inside, crawl inside the web server.



Anyway, serious problem with this major enterprise and their website.  And I'll just say that this enterprise is - it's not like the website happens to be a side-effect feature of this enterprise.  The nature of the enterprise is such that it is the website.



MIKAH:  Oh.



STEVE:  I mean, like, its entire purpose and existence on the planet is this website and others that it also owns.



MIKAH:  So to be clear, it's not as if the website is just a place where you go to learn about its features.  The service is the site that you're going to.  It's not just a pretty little site.



STEVE:  Yes.  It is not Google, but it would be as if - it would be like Google having a problem of this magnitude with its servers.  I mean, you know, its purpose is its site, essentially.  And so it's a big deal.  Now, our listener, as I described last week, gave this company 120 days to, like, deal with it.  He contacted them.  They replied, "Okay, thank you for notifying us.  We'll look into this and get back to you."  Oh, and they said, "And it may take four months for that to happen."  So he waited until 118 days and then said, uh, knock knock.



MIKAH:  You've got two days here.



STEVE:  What's going on?  And they said, oh, oh, right.  Well, let's give it a couple more days.  So they came back and said, well, we've asked our security people, and they're not really thinking that's a big problem.  How about this?  We'll give you 350 bucks to just shut up.



MIKAH:  What?



STEVE:  And go away.



MIKAH:  What?



STEVE:  So at that point he tweeted me, and he said, "Steve, what do you think about this?"  And I said, you know, you've done everything you can.  They're not being very cooperative.  Why don't you send a report to CISA?  Which is, you know, the U.S.'s front facility for dealing with this kind of stuff.  So he thought, okay, that's a good idea, so submitted a report along with his entire email chain, his whole dialogue with these guys, to CISA, which forwards it to CERT.



Okay.  Well, he sent me a tweet updating me on what was going on, and I have it in the show notes, after I removed the identification of the site that we're all talking about here.  And this comes back from CERT saying:  "Greetings.  Thank you for your vulnerability report submission.  After review, we've decided not to handle the case, for two reasons.  First, we typically avoid handling or publishing reports of vulnerabilities in live websites.



"Second, since the vendor is cooperating and communicating with you, there's very little additional action that we can contribute."  What?  Anyway, "We recommend working directly with the affected vendor before proceeding with public disclosure."  Well, okay.  He's never going to disclosure this publically.  This would be a disaster.  "Feel free to contact us again if you run into problems with your coordination effort, or if there is some other need for our involvement.  Regards."  And in like...



MIKAH:  Giant.



STEVE:  50-point type, yeah, "Vulnerability Analysis Team, at the CERT Coordination Center at kb.cert.org."  Okay, well, that's disappointing.  You know, they say:  "We typically avoid handling vulnerabilities in live websites."  So, what?  They limit their handling of vulnerabilities to dead websites?  You know, this doesn't inspire confidence.



MIKAH:  It feels like they didn't read the evidence that was provided.  This feels like when you go and you talk to any tech support person.



STEVE:  Yes.



MIKAH:  And they immediately jump to assumptions, and they don't hear you out first.



STEVE:  Yes.



MIKAH:  And then they're providing advice that you're going, but did you not just hear what I literally just - I don't like this.  Sorry.  Go ahead.



STEVE:  No.  No, and you're exactly right.  Our listener's correspondence, which he had forwarded to CERT with his report, made clear that the company had no intention of doing anything further.  So CERT was just passing the buck back to someone who had already demonstrated that he had no leverage.  I mean, and that was why I recommended these guys.  I figured if Uncle Sam knocks on the door and says, hey, you've got a little problem with your web server, and it would be bad if you got compromised, that then they might listen, because they're not listening to Joe.



So anyway, in my reply to him just now, since he had never disclosed anything publicly, and I know he never would, he's not a bad guy, I suggested that, since he had done all he could reasonably do - and again, never said anything publicly - he should take the money as compensation for his trouble and leave whatever happens up to fate.  You know, if the company does eventually get bitten, it will only be their fault.  He warned them.  He didn't, I mean, he even reminded them after four months.  And they said, oh, yeah, you.  Hmm.



MIKAH:  Not you again.  We thought you'd just disappear.  Why?



STEVE:  So,  yeah, anyway.



MIKAH:  Can I ask you something?  I would like to know when is it reasonable and, as you see it, reasonable and the right thing to do to disclose things publicly?  Not in this situation in particular.  I just mean - because I thought that that was something that some security researchers end up doing is, if the company continues not to behave, or it continues to not work with them, isn't a public disclosure part of the bargaining power that the security researcher has?



STEVE:  Yeah, famously.  Like Google, for example.  You know, they find a problem, they notify the vendor, and they say, "We have started a 90-day clock, and you need to fix this sucker."  So if you don't, we're going, well, or whether or not you do, we're going to go public with this in 90 days.  So take it seriously.  Because unfortunately, this policy is the result of experiences just like this one, where companies are like, eh, you know, we don't think it's that big a problem.  Okay, great.  We're going to let everybody know about it if it's not that big a problem.  Whoa, whoa, whoa, whoa.  Hold on.  Hold on.  Wait a minute.  So this guy's not Google.  He can't do that.  And of course Google also has lord knows what kind of a bank of attorneys they have that are able to...



MIKAH:  To protect themselves, got it, yeah.



STEVE:  ...protect themselves.  Unlike this guy, who the last thing he wants is to get stomped on by a massive enterprise that just turns one of their attorneys loose and says let's go make this guy's life miserable.



MIKAH:  So that's the danger there.  That makes sense.



STEVE:  Yeah.  So first of all, I mean, there have been instances where even Google has not followed their own policy when what they have found has been so egregious that, like, they can't in good conscience let this be known publicly, even though they really want to, because it would just collapse the world.



MIKAH:  Right.



STEVE:  So, you know, you can lead the company to water.  You can say, "Please fix this, won't you?  We all want you to.  The world will be a safer place."  But ultimately, if they say no, it's like, okay, well, you called our bluff because we're not going to tell everybody because that would be really bad.  So good luck.  And at least we're going to protect ourselves from you.  In fact, this guy was worried that the information this company has about him, because he's a subscriber or a member or whatever it is, that that would be at risk.  And he asked them in his correspondence, like, what are you doing about my privacy concerns because you've got an insecure website, and you know a lot about me.  And they never ever said anything about that.  So again, you know, I guess maybe he could contact Google.



MIKAH:  That would be great.



STEVE:  That might be an idea, actually, is contact their threat people and say, hey, what do you think about this?  If you think it's bad, maybe you ought to give them a 90-day countdown and get this thing really fixed.



MIKAH:  Yeah, that is...



STEVE:  Anyway, it was a little - it was a disappointing response, though, from our own government because this is not a company you want to have attacked.  And in fact, speaking of which, we'll be talking about Change Healthcare here in a while.  Talk about a company you don't want to have attacked.  Oh, my lord.  One in three of the nation's health records were compromised in this.



MIKAH:  Love that.  Love that.  That's so great.



STEVE:  What could possibly go wrong?  Speaking of which, a week ago, on March 5th, Broadcom, which is VMware's parent company now after they bought them, issued a security advisory that happens to be VMSA-2024-0006.  And it's encouraging that it's got a one-digit number on it.  You know, 0006, that's nice because now, you know, these days the CVEs have had to expand to six digits.  Well, no, actually five.  But they used to be four.  Just because there are so many problems happening.



Okay.  So this security advisory addresses vulnerabilities that have been discovered in VMware ESXi, VMware Workstation Pro and Player, and Fusion.  In other words, pretty much everything.  If it's got VMware in its name, we've got a problem.  This is because of a problem in the ubiquitous USB virtualization drivers, which have been found to contain four critical flaws.  I didn't dig in to find out who found them, whether they found them, somebody else found them and told them, or what.  But they are so bad that VMware has even issued patches for previous out-of-service-life releases of anything with VMware in its name.  An attacker who has privileged access in a guest OS virtual machine, so a root or admin, may exploit these vulnerabilities to break out of the virtualization sandbox, which of course is the whole point of virtualization, to access the underlying VMware hypervisor.



Patching VMware of any kind immediately is the optimal solution.  But due to the location of the problem, which is just a specific driver, if anything prevented that from being done, and if your environment might be at risk because it includes untrusted VM users, the removal of VMware's USB controllers from those virtual machines will prevent exploitation until the patches can be applied.  VMware said that the prospect of a hypervisor escape warranted an immediate response under the company's, what they call their, for some reason, IT Infrastructure Library, or ITIL.  They said:  "In ITIL terms, this situation qualifies as an emergency change, necessitating prompt action from your organization."



So both the UHCI and XHCI USB controller drivers contain exploitable use-after-free vulnerabilities, each having a maximum severity rating of 9.3 for Workstation and Fusion and a base score of 8.4 for ESXi.  There's also an information disclosure vulnerability in the UHCI USB controller, with a CVSS of 7.1.  Somebody who has admin access to a virtual machine could exploit it, that is, this other vulnerability, to leak memory from the vmx process.  Anyway, I've included a link in the show notes to VMware's FAQ about this for anyone who might be affected and interested in more details.



Basically, you know, if you've got VMware in your world, where somebody running in a virtual machine might be hostile, that's to be taken seriously.  If you're just a person at home with VMware because you like to run Win7 in a VM or whatever, then this is not to worry about.  Update it, you know, when you get around to it.  But anyway, VMware is on alert because of course it is in use in many cloud infrastructures where, you know, you don't know who your tenants are.  So, important to get that fixed.



So unfortunately, Microsoft's networks, and apparently their customers, remain under attack to this day by that Russian-backed Midnight Blizzard group.  And remember that they were originally named Nobelium, and Microsoft said, oh, let's call it Midnight Blizzard because that sounds scarier, I guess.  This is the group that successfully infiltrated some of Microsoft's top corporate executives' email by attacking the security of a system kind of that had been forgotten, it was in the back room somewhere, and it had not been updated to the latest state-of-the-art security and authentication standards.  And so, you know, they said, whoops, we forgot to fix that.



So the sad thing here, the saddest thing of all, I think, is that Microsoft has apparently, believe it or not, now taken to dropping their public relations bombs late on Fridays, as happened again this past Friday, and not for the first time.  This is now their pattern.  The Risky Business security newsletter explained things by writing this.  They said:  "Microsoft says that Russian state-sponsored hackers successfully gained access to some of its internal systems and source code [whoops] repositories.  The intrusions are the latest part of a security breach that began in November of last year and which Microsoft first disclosed in mid-January.



"Initially, the company said hackers breached corporate email servers and stole inboxes from the company's senior leadership, legal, and cybersecurity teams.  In an update on the same incident posted late Friday afternoon - as is the practice of every respectable corporation..."



MIKAH:  That's sarcasm, for those who are...



STEVE:  That's sarcasm, by the way.  "Microsoft says it found new evidence over the past weeks that the Russian hackers were now weaponizing the stolen information."



MIKAH:  Great.  Love that.



STEVE:  Uh-huh.  "The Redmond-based giant initially attributed the attack to Midnight Blizzard, a group also known as Nobelium" - that's right, but before Microsoft gave them a scarier name - "one of the cyber units inside Russia's Foreign Intelligence Service.  Microsoft says that since exposing the intrusion, Midnight Blizzard has increased its activity against its systems."  Well, and how.  "Per the company's blog post, Midnight Blizzard password sprays increased in February by a factor of 10, compared to the 'already large volume it saw in January of 2024.'



"Furthermore," Risky Business notes, "if we read between the lines, the group is now also targeting Microsoft customers.  While Microsoft's legalese makes it clear that no customer-facing systems were compromised, the company weaselly confirmed that customers have been 'targeted.'"  Okay, what does that mean?  "Emails stolen by the Russian group also contained infrastructure secrets, you know, secret tokens, passwords, API keys, et cetera, which Midnight Blizzard has been attempting to use."



So it's not good that Russians managed to compromise Microsoft executives' email in the first place, and that the information they stole is now being leveraged to facilitate additional intrusions, including apparently the exfiltration of Microsoft's source code repositories.  That's not good for anyone.  But as we know, I always draw a clear distinction between mistakes and policies.  Mistakes happen.  Certainly a big one happened here.  But the new policy that caught my eye was that, unfortunately, we are now seeing a pattern develop of late Friday afternoon disclosures of information which Microsoft hopes will be picked up and published when fewer people are paying attention.  So that's...



MIKAH:  So, yeah.  Having worked in journalism for ages, that is, well, ages.  For a long time.



STEVE:  Yes, how old are you exactly?  How many ages ago?



MIKAH:  One age, at least.



STEVE:  Okay.  An age.



MIKAH:  But, yes, it is - that's something that we always pay attention to.  When is that coming out?  And what is the messaging behind there?  And the fact that they're waiting until Friday afternoon is a clear signal that they don't want people to see it.  And that is wrong when it comes to security disclosures.  And luckily you've got your eye on it, and that means we can get to it on Tuesday.  You can let it go on Friday, Microsoft, but we'll pick it up on Tuesday.



STEVE:  Well, yes.  And it's also relevant that this was not like any kind of an emergency announcement that had to go out on Friday afternoon.  I mean, this was just, like, well, we're updating you on the status, but we'd rather you didn't receive this.



MIKAH:  Yeah, exactly.



STEVE:  So here it is.  Right.  Okay.  So the official name for Beijing's secret directive is "Document 79," but it is informally known as the "Delete A" order where "A" is understood to stand for America.  Yes, China wants to delete America.  The secret directive was issued several years ago, back in September of 2022.  It's designed to remove American and other non-Chinese hardware - although apparently since we get the "A," we figure prominently - and so to remove American and other non-Chinese hardware and software from its critical sectors.  The directive mandates that state-owned companies replace all U.S. and other non-Chinese equipment and software in their IT systems by 2027.  So you've got three years left, folks.  And that was a five-year order that began two years ago.  So far the affected companies include Cisco, IBM, Dell, Microsoft, and Oracle.  And my reaction to this is, yeah, who can blame them? 



MIKAH:  Yeah.  [Crosstalk].



STEVE:  You know?  Right.  With tensions on the rise between the U.S. and China, this only makes sense.  The U.S. has done the same thing for the manufacturers of Chinese-made security cameras within, like, sensitive areas like military bases and protected corridors of power.  And we know that Russia is working to remove American technological influence from inside its borders.  The problem for all the parties involved is that today's systems are so complex that trojan capabilities are readily hidden and can be made impossible to find.  No one doubts the influence that Chinese intelligence services are able to exert over the design of Chinese equipment.  And there has long been some question about just how much influence U.S. intelligence services might have over the installation of such backdoors in proprietary software.  Years and years ago there was a key inside of Windows that purely by coincidence had the initials NSA.  And it was like, [gasp], oh, my god.  You know, it's like, folks, if it was an NSA key, they would have changed the name.



MIKAH:  Yeah.  It would not be that obvious.



STEVE:  But it was like, oh, my goodness, I mean, just everyone was all atwitter, if you'll pardon the choice of words, over that the NSA had some secret key buried in Windows.  No.  But which is not to say they don't.  Like, why wouldn't they?  But they wouldn't leave the name the same.  So anyway, to my way of thinking, I just wish we could all get along.  You know, it's unfortunate to be seeing this pulling apart, you know, and this rising global tension because ultimately it is less economically efficient for us all to have to, like, retrench, and like we have to now be making chips in the U.S. when China was so good at it.  And, like, what's the problem?  Well, you know, it could be a problem.  So ultimately it's hardly surprising, but I just really regard it as unfortunate.



Okay.  And I'm keeping my eye on the time, but this is going to be a long podcast because we have a lot of good stuff to get to.  So I'm going to do one more before we take our next break.



MIKAH:  Sounds good.



STEVE:  Signal's reliance - Signal, you know, the messaging app, like the best one there is that is cross-platform.  I would argue that iMessage is good if you're in iOS world, but Signal if you need other stuff.  Their reliance upon physical phone numbers for identifying the communicating parties has been a longstanding annoyance, as well as a concern for privacy and security researchers who have long asked the company to switch from phone numbers to usernames in order to protect users' identities.  The just-announced version 7 of Signal will add the creation of temporary username aliases.



So here's how Signal explains it in their announcement.  They said:  "Signal's mission and sole focus is private communication.  For years, Signal has kept your messages private, your profile information like your name and profile photo private, your contacts private, and your groups private" - notice the word "private" figuring prominently in that single sentence - "among much else.  Now we're taking that one step further by making your phone number on Signal more private.



"Here's how:  If you use Signal, your phone number will no longer be visible to everyone you chat with by default."  And props to them for changing this.  Many organizations will add a feature, but they're afraid to change the behavior.  It's like, oh, well, you know, we don't want to, like, freak everyone out.  So we've got it now, but we're going to have it turned off, and you've got to turn it on if you want it.  No.  Signal says those phone numbers are going to disappear.  So get over it.



They said:  "People who already have your number saved in their phone's contacts will still see your phone number since they already know it.  If you don't want to hand out your phone number to chat with someone on Signal, you can now, as of 7" - which is in beta so it'll be available within a few weeks, they said - "you'll be able to create a unique username that you can use instead."  They said:  "You'll still need a phone number to sign up for Signal."  And they said:  "Note that a username is not the profile name that's displayed in chats.  It's not a permanent handle."  That's why I referred to it earlier as an alias.  And they said:  "And not visible to the people you are chatting with in Signal.  A username is simply a way to initiate contact on Signal without sharing your phone number.



"If you don't want people to be able to find you by searching for your phone number on Signal, you can now enable a new, optional privacy setting.  This means that unless people have your exact unique username, they will not be able to initiate a conversation, or even know that you have a Signal account - even if they do have your phone number.



"These options are in beta," they said, "and will be rolling out to everyone in the coming weeks.  And once these features reach everyone, both you and the people you're chatting with on Signal will need to be using the most updated version of the app to take advantage of them.  Also, all of this is optional.  While we changed the default to hide your phone number from people who don't already have it saved in their phone's contacts, you can change this setting.  You're not required to create a username, and you have full control over whether you want people to be able to find you by your phone number or not.  Whatever choices work for you and your friends, you'll still be able to communicate with your connections in Signal, past and present."



So that seems all good.  They've changed to hiding phone numbers by default, and they've added an optional textual username as a phone number alias.  Signal knows the phone number behind the alias, but users do not.  I think that's pretty slick.  Their blog posting about this contains a great deal more information.  Basically it's a complete user guide to how to work with this coming feature.  I've put it in this week's show notes since it might be of interest to anyone who uses Signal often.  So you can click the link in the show notes, and you'll be taken to their page.  It's phone-number-privacy-usernames.  So you can also probably google that, if you want to just jump right to it at Signal's site.  So anyway, nice improvement coming from Signal.



MIKAH:  VV in the Discord says:  "I set it up last week, and I love it.  This is such welcome news."  So we've already got some folks who have enabled these, you know, the new username feature.  Or as you pointed out or call it, an alias I think is a good way of putting it, given the - and it's clear that, you know, when you can tell, I think of when I used to post on X, formerly known as Twitter, in the past, and I would have a tweet, and I'd have about three things underneath it that are sort of responses to how all of the pedantic people on Twitter will probably take my post and say, "You didn't say this," or "You forgot to say that."



This feels like that.  They're answering every possible question that folks might have.  And as you said, there's a full user guide.  And I think that that's smart given their user base; right?  There are going to be lots of concerns about any change taking place and what it does, what it doesn't do.  So good on them, or good on it, I guess, for figuring out what needs to be said.



STEVE:  Yeah.  Basically they've created a mapping on top of the phone numbers, where, you know, a little dictionary where a user can assign themselves an alias and then say to somebody, hey, contact me on Signal at this username.  You know.  And it's going to be squirrelybumblebee or something, which is not their phone number, but it doesn't need to be.  It just needs to be something else.  And that allows the - so beneath the covers, Signal knows when somebody says, hey, I want to connect to squirrelybumblebee, that that actually means this phone number.  But the user making the connection never sees that.



MIKAH:  Yeah.  As you said, very slick.  I think that's a right way to do it.



STEVE:  Okay.  So last Thursday the 7th, the European Union's DMA, the Digital Markets Act, went into force.  Among many important features, it requires interoperability among specified, probably large, instant messaging systems.  But here's what Meta recently explained about their plans.  I'll give everybody a quick hit.  It amounts to, if you want to talk to us, you've got to use the Signal protocol.  But we'll meet you halfway by making it possible for you to connect with our previously closed servers.



So Meta wrote:  "To comply with a new EU law, the Digital Markets Act, which comes into force on March 7th, we've made major changes to WhatsApp and Messenger to enable interoperability with third-party messaging services.  We're sharing how we enabled third-party interoperability" - which they shortened to "interop" - "while maintaining end-to-end encryption and other privacy guarantees in our services as far as possible.



"On March 7th, a new EU law, the Digital Markets Act, comes into force.  One of its requirements is that designated messaging services must let third-party messaging services become interoperable, provided the third party meets a series of eligibility, including technical and security, requirements."  And that's of course the key because, if not, Meta is able to say, uh, no no no, not so fast.  "This allows," they wrote, "users of third-party providers who choose to enable interoperability to send and receive messages with opted-in users of either Messenger or WhatsApp, both designated by the European Commission as being required to independently provide interoperability to third-party messaging services.



"For nearly two years our team has been working with the European Commission to implement interop in a way that meets the requirements of the law and maximizes the security, privacy and safety of its users.  Interoperability is a technical challenge, even when focused on the basic functionalities as required by the DMA.  In year one, the requirement is for 1:1 text messaging between individual users and the sharing of images, voice messages, videos, and other attached files between individual end users.  In the future, requirements expand to group functionality and calling.



"To interoperate, third-party providers will sign an agreement with Messenger and/or WhatsApp, and we'll work together to enable interoperability.  Today we're publishing the WhatsApp Reference Offer for third-party providers which will outline what will be required to interoperate with the service.  The Reference Offer for Messenger will follow in due course.  While Meta must be ready to enable interoperability with other services within three months of receiving a request, it may take longer before the functionality is ready for public use.  We wanted to take this opportunity to set out the technical infrastructure and thinking that sits behind our interop solution.



"Our approach to compliance with the DMA is centered around preserving privacy and security for users as far as is possible. The DMA quite rightly makes it a legal requirement that we should not weaken security provided to Meta's own users.  The approach we've taken in terms of implementing interoperability is the best way of meeting DMA requirements, whilst also creating a viable approach for third-party providers interested in becoming interoperable with Meta and maximizing user security and privacy.



"First, we need to protect the underlying security that keeps communication on Meta end-to-end encrypted messaging apps secure, the encryption protocol.  WhatsApp and Messenger both use the tried and tested Signal Protocol as a foundational piece for their encryption.  Messenger is still rolling out end-to-end encryption by default for personal communication, but on WhatsApp this default has been the case since 2016.  In both cases, we are using the Signal Protocol as the foundation for these end-to-end encrypted communications, as it represents the current gold standard for end-to-end encrypted chats.



"In order to maximize user security, we would prefer third-party providers to use the Signal Protocol.  Since this has to work for everyone, however, we will allow third-party providers to use a compatible protocol if they're able to demonstrate it offers the same security guarantees as Signal."  And I'll just interrupt here to say that will be a high bar.



MIKAH:  Yeah.  [Crosstalk].  That's it.



STEVE:  And Signal is open source.  It's an open protocol.  Why in the world would anybody roll their own from scratch and say, oh, well, you know, well, like Telegram, that's got a bizarro protocol that nobody has ever understood, and no one has ever tried to prove was secure because it's just garbage.  You know, I mean, it scrambles stuff.  So maybe it's secure.  But it's nice to have proofs, and these days we can get proofs.  Except not for random stuff somebody just made up.



They said:  "To send messages, the third-party providers have to construct message protobuf structures, which are then encrypted using the Signal Protocol, and then packaged into message stanzas in XML," you know, the eXtensible Markup Language.  "Meta servers push messages to connected clients over a persistent connection.  Third-party servers are responsible for hosting any media files their client applications send to Meta clients, such as image or video files.  After receiving a media message, Meta clients will subsequently download the encrypted media from the third-party messaging servers using a Meta proxy service.



"It's important to note that the end-to-end encryption promise Meta provides to users of our messaging services requires us to control both the sending and receiving clients.  This allows us to ensure that only the sender and the intended recipient can see what's been sent, and that no one can listen to your conversation without both parties knowing."  Now, just to decrypt that corporate speak, what they're saying is, if we don't control the sending and receiving clients, which is to say if we are going out of our own - what am I looking for?  Not out of band, out of - there's a current term.  It was a well-known...



MIKAH:  Out of pocket?  Out of environment?  Out of...



STEVE:  Yeah, out of environment.  Then we can't make any representations about what happens to the message that you send out of our services, or where those messages come from into our services.  So that's sort of them just sort of, you know, stepping back, saying only if you stay within Messenger and WhatsApp are we willing and can we take responsibility over the messages that are being transacted.  So, you know, beware.



Anyway, they finish, saying:  "While we've built a secure foundation for interop that uses the Signal Protocol encryption to protect messages in transit, without ownership of both clients' endpoints we cannot guarantee what a third-party provider does with sent or received messages, and we therefore cannot make the same promise."  So again, they just said what I just said.  I didn't realize that was coming.  So anyway.



MIKAH:  With those last two paragraphs, are they talking to - do you think they're talking to the potential services that are trying to connect and explaining their thinking here?  Or is this Meta not being absolutely certain how the EU will take this choice to use Signal and kind of push Signal, and so they're almost trying to explain to them, look, we know that it would be easier if we would just open it up to all of them.  But, you know, they're sort of going through and explaining why Signal is the best.



I guess what I'm asking is, it kind of runs contrary to what the company said early on about having worked with the EU for so long on figuring this out, if what they're trying to do there is like, come on, EU, don't punish us further, we promise this works.  Do you think this is more for those other third-party messaging platforms to go, look, we know that you might want to use something else, but it makes sense why we're doing this and why we have to have control of the sending and receiving.



STEVE:  So my ignorance is showing here.  Can non-Facebook users use WhatsApp?  I know Messenger is Facebook only.



MIKAH:  That is a really good - I don't think you need a Facebook account to have WhatsApp.  But that's a good question.



STEVE:  I think that's correct.  So my feeling is, you know, yes, other Messengers might want to hook up.  But, you know, even if you didn't have WhatsApp, the most obvious messaging system to connect would be Signal itself; right?  Why wouldn't Signal, that invented the Signal protocol, say hey, yeah, we'll - that sounds great.  We'd like to be able to allow Facebook Messenger users and WhatsApp to interact with Signal users.  So, you know, Signal themselves would be the obvious interop choice.  But again, Telegram's going to have a hard time.  I mean, there are lots of other, you know, when I was talking last week about the various levels, Level 0, 1, 2, and 3, that Apple has assigned to messaging, there were some down at Level 0 where encryption was optional that I'd never heard of, like QQ.  What's that?



MIKAH:  Yeah, don't know.



STEVE:  But that's apparently some messaging app somewhere.  So again, any other company is going to have a heavy lift.  And basically I think what this means is, you know, we're using Signal at Meta, and anybody else who wants to talk with us is going to have to do it, too.  Because, again, Signal's bar is very high.  Well, in fact they just added post-quantum encryption in Signal.



MIKAH:  Right.



STEVE:  So, you know, you're going to - if what Meta is saying is we'll connect to you if you're as good as Signal, well, that now means if you also have post-quantum encryption available.



MIKAH:  Yeah.  So basically it is saying you're going to need to use Signal because you're not going to be able to prove to us that anything else is as secure.



STEVE:  It won't be as good if it's not Signal because Signal is the best.  Unless you ask Apple, and they think, well, Signal is our Level 2.  You know, we're Level 3.  It's like, okay, fine.  So.  Because Apple is rotating their keys every 50 messages or every week, whichever comes first.



MIKAH:  Right.



STEVE:  And they're saying, oh, that gives us an advantage.  So it's okay, fine.



MIKAH:  Yeah.  I don't know - I'm with you in that I don't think that should bump it up to Level 3.  But at the same time, when I was reading through the explanation of everything, I did think that was cool because I hadn't considered how having somebody's whole history of messages, even in a post-quantum world...



STEVE:  Ah, is using the same key.



MIKAH:  Yeah, using the same key, that that means the whole thing.  But this just means they only get - that's pretty smart.  That would be cool to see implemented.



STEVE:  I agree with you.  It's like, well, why not do it if we can?  And it was expensive to do it.  You know, they had to amortize that key across many messages because the key's 2K, and many messages are like, "Okay, Mom, I'm on my way."



MIKAH:  Right.



STEVE:  You know?  So the per-message overhead of 2K, that was insane, you know, when the message is 12 characters.  So anyway, okay.  So a lot of our listeners have asked me about this big cyberattack, a ransomware attack, on the U.S.'s Change Healthcare service.  Nearly three weeks ago, on Wednesday, so three weeks ago tomorrow, on Wednesday, February 21st, the American company Change Healthcare, which is a division of UnitedHealth Group, also known as UHG, was hit by a ransomware attack that was devastating by any measure.  That cyberattack shut down the largest healthcare payment system in the U.S.



Then, to give you a sense for this, just this past Sunday, two days ago, the U.S. Department of Health and Human Services addressed an open letter to "Health Care Leaders," writing:  "As you know, last month Change Healthcare was the target of a cyberattack that has had significant impacts on much of the nation's healthcare system.  The effects of this attack are far-reaching.  Change Healthcare, owned by UnitedHealth Group (UHG), processes 15 billion" - with a B - "healthcare transactions every year and is involved in one of every three patient records.  The attack has impacted payments to hospitals, physicians, pharmacists, and other healthcare providers across the country.  Many of these providers are concerned about their ability to offer care in the absence of timely payments, but providers persist despite the need for numerous onerous workarounds and cash flow uncertainty."  So this has really upset a lot.  



Okay.  So backing up here a bit, the day following the attack, February 22nd, this UnitedHealth Group filed a notice, as they must, with the U.S. Securities and Exchange Commission because they're a publicly traded company, stating that "a suspected nation-state associated cybersecurity threat actor" had gained access to Change Healthcare's networks.  Following that UHG filing, CVS Health, Walgreens, Publix, GoodRx, and BlueCross BlueShield of Montana reported disruptions in their insurance claims.  Yeah, basically it was all shut down.  The cyberattack affected family-owned pharmacies and military pharmacies, including the Naval Hospital at Camp Pendleton.  The Healthcare company Athenahealth was affected, as were countless others.



MIKAH:  Wow.



STEVE:  One week later, on the 29th of February, you know, Leap Year Day, UHG confirmed that the ransomware attack was "perpetrated by a cybercrime threat actor who represented itself to Change Healthcare as ALPHV/Blackcat."  And I'm just going to refer to them as Blackcat from now on because it's easier.  In the same update, the company stated that it was "working closely with law enforcement and leading third-party consultants Mandiant and Palo Alto Networks" to address the matter.  And then, four days later, that is, after this disclosure on the 29th, which is eight days ago on March 4th, Reuters confirmed that a bitcoin payment equivalent to nearly $22 million USD has been made into a cryptocurrency wallet "associated with Blackcat."



MIKAH:  We think they paid the ransom?



STEVE:  Yeah.



MIKAH:  Oh, my god.



STEVE:  Yeah.  UnitedHealth has not commented on the payment, instead stating that the organization was "focused on the investigation and the recovery."  Right.  Apparently to the tune of $22 million USD.  On the same day, a reporter at Wired stated that the transaction looked "very much like a large ransom payment."  What's transpired since then is a bit interesting, since Blackcat is a ransomware-as-a-service group.



MIKAH:  Oh.  Okay.



STEVE:  This of course means that they provide the software and the backend infrastructure, while their affiliates are the ones that perpetrate the attacks, penetrate the networks, and in return for that the affiliate receives the lion's share, in this case 70%, of any ransoms paid.  That's always been the way it is.  However, in this instance it appears that Blackcat is not eager to part with that 70%, which amounts to a cool $15.4 million.  So they're claiming that they've shut down and disbanded.



MIKAH:  What?



STEVE:  Nice timing on that.  Okay.  So exactly one week ago, the HIPAA, the U.S. organization, the HIPAA Journal posted some interesting information.  They wrote...



MIKAH:  Oh, sorry.  Sorry, Steve, can I correct you there?  It's H-I-P-A-A, not H-I-P-P-A.



STEVE:  Oh, good, thank you, yes.  Good.  Thank you.  HIPAA.  Thank you.  So they said:  "The ALPHV/Blackcat ransomware group appears to have shut down its ransomware-as-a-service (RaaS) operation, indicating there may be an imminent rebrand.  The group claims to have shut down its servers, its ransomware negotiation sites are offline, and a spokesperson for the group posted a message, 'Everything is off, we decide.'"  Probably a  Russian speaker.  "A status message of 'GG' was later added, and Blackcat claimed that their operation was shut down by law enforcement and said it would be selling its source code.



"However, security experts disagree and say there is clear evidence that this is an exit scam, where the group refuses to pay affiliates their cut of the ransom payments and pockets 100% of the funds.  Blackcat is a ransomware-as-a-service operation where affiliates are used to conduct attacks and are paid a percentage of the ransoms they generate.  Affiliates typically receive around 70% of any ransoms they generate, and the ransomware group takes the rest.



"After the earlier disruption of the Blackcat operation by law enforcement last December, Blackcat has been trying to recruit new affiliates and has offered some affiliates an even bigger cut of the ransom.  An exit scam is the logical way to wind up the operation, and there would likely be few repercussions, other than making it more difficult to recruit affiliates if the group were to choose to rebrand."  Right.



MIKAH:  Oh, wow.



STEVE:  Who's going to do this again if you just screwed the last affiliate that you were working with after they generated a $22 million ransom.



MIKAH:  Hello.  We are Whitecat, and we have no affiliation to the Blackcats.  Would you like to work with us?



STEVE:  Right.  Maybe dark black or dark gray.  Anyway, HIPAA wrote:  "It's not unusual for a ransomware group to shut down operations and rebrand after a major attack, and Blackcat likely has done this before.  Blackcat is believed to be a rebrand of the earlier BlackMatter ransomware operation, which itself was a rebrand of DarkSide."



MIKAH:  Okay.  If they have something with shadows, dark, black, you know it's the same company, people.



STEVE:  That's right.



MIKAH:  Going forward.  Come on.



STEVE:  That's right.  DarkSide was the ransomware group behind the attack on the Colonial Pipeline in 2021 that disrupted fuel supplies on the Eastern Seaboard of the United States.  Shortly after the attack, the group lost access - what do you know - to its servers, which they claimed, probably was, due to the actions of their hosting company.  They also claimed that funds had been transferred away from their accounts and suggested they were seized by law enforcement.  BlackMatter ransomware only lasted for around four months before it was in turn shut down, with the group rebranding in February of 2022 as Blackcat.



On March 3rd of this year, an affiliate - and here it comes - with the moniker Notchy (N-O-T-C-H-Y) posted a message on Ramp Forum claiming they were responsible for the attack on Change Healthcare.  The post was found by a threat researcher at Recorded Future.  Notchy claimed they were a long-time affiliate of the Blackcat operation, and that could have gone back two years because that's when Blackcat reformed, had the "affiliate plus" status granting them a larger piece of the pie, and that they had been scammed out of their share of the $22 million ransom payment.



They claimed that Optum, which is the actual organization within Change that was hit and paid, Optum paid a 350 Bitcoin ransom to have the stolen data deleted and to obtain the decryption keys.  In other words, full-on standard ransomware payment.  Notchy shared the payment address which shows a $22 million payment had been made to the wallet address, and the funds have since been withdrawn.  The wallet has been tied to Blackcat as it received payments for previous ransomware attacks that have been attributed to the group.



Notchy claimed Blackcat suspended their account following the attack and had been delaying payment before the funds were transferred to Blackcat accounts.  Notchy said that Optum paid to have the data deleted, but they have a copy of 6TB, that's how much was stolen, of data in the attack.  Notchy claims the data includes sensitive information from Medicare, Tricare, CVS-Caremark, Loomis, Davis Vision, Health Net, MetLife, Teachers Health Trust, tens of other insurance companies, and others.  The post finishes with a warning to other affiliates that they should stop working with Blackcat.  It's unclear what Notchy plans to do with the stolen data and whether they will attempt to extort Change Healthcare or try to sell or monetize the data.



Fabian Wosar, Emsisoft's CTO, is convinced this is an exit scam.  After checking the source code of the law enforcement takedown notice, he said it is clear that Blackcat has recycled it from December's earlier takedown notice.  Fabian tweeted:  "There is absolutely zero reason why law enforcement would just put a saved version of the previous takedown notice up during a seizure instead of a new takedown notice."  He also reached out to contacts at Europol and the NCA, who said they had no involvement in any recent takedown.  Currently, neither Change Healthcare nor its parent company UnitedHealth have confirmed that they paid the ransom and then been cheated out of it, and issued a statement that they are currently focused on the investigation.



MIKAH:  I'm sure.



STEVE:  So, yeah.  This is all a big mess.  It appears that the Blackcat gang has made off with Optum's $22 million, that Notchy affiliate did not receive the $15.4 million or more that they feel they deserve, Optum got neither the decryption keys nor the deletion of their 6TB of data, which they paid for.  And no one but the Blackcat guys are smiling at this point.  Since rebranding and returning to the ransomware-as-a-service business may be impossible after taking their affiliate's money, and since $22 million is a nice piece of change, they may just go find a nice beach somewhere to lie on.



MIKAH:  One hopes.



STEVE:  You know?  Meanwhile, here in the states, the inevitable class action lawsuits have been filed due to the loss of patient healthcare records.  At last count at least five lawsuits are now underway.



MIKAH:  Oh, my word.  This is awful.  All the way around this is awful.



STEVE:  Big mess.



MIKAH:  It's a huge mess.  And it's, I don't know, it kind of speaks to the dangerous nature maybe of having the entire country's, nearly the entire country's healthcare operating under one company.  That that much can be impacted by getting into one company is pretty scary.



STEVE:  Yes.  They are a service provider.  And so all kinds of other of their clients use them to provide the insurance processing, and in return the payments back to them.  And all of that is shut down now.  They don't have their decryption keys.  All of their server infrastructure is encrypted.  They were willing to pay $22 million to get it back online more quickly.  We don't know anything about the state of their backups and so forth.  But even so, 6TB of medical record history is now also in the hands of bad guys.



And, you know, this is the unfortunate downside of the U.S.'s free enterprise system, which I'm, you know, has lots of upsides because it allows people to apply their efforts and to be clever and to create companies.  Unfortunately, there's a tendency for the big fish to eat the small fish, and for consolidation to happen.  And so what we're seeing here is, as you said, what would have otherwise been a much more distributed set of services have all been consolidated.



You know, on one hand, yes, it's more efficient.  You're getting to use one larger set of infrastructure instead of lots of smaller infrastructures.  But with it comes responsibility.  With that consolidation comes responsibility.  And what we're now seeing is what happens when one could argue that responsibility was not met.



MIKAH:  With great power, they say.



STEVE:  Yes.



MIKAH:  That's so many records.  You said, what, for every three, it's two of every three?



STEVE:  One out of every three medical records in the U.S. is basically in that 6TB of data.  Yeah.



MIKAH:  Yeah, awful.



STEVE:  On my side, everything continues to proceed well with SpinRite.  The limitations of 6.1, you know, being unable to boot UEFI-only systems and its lack of native high-performance support for drives attached by USB and NVMe media, are as annoying as I expected that they would be.  So there just wasn't anything I could do about it.  I'm working to get 6.1 solidified so specifically so that I can obsolete it with 7.0 as soon as I possibly can.  But that said, I still do want to solve any remaining problems that I can, especially when such a solution will be just as useful and necessary for tomorrow's SpinRite 7 as it is for today's 6.1.



And that's the case for the forthcoming feature I worked on all of last week.  It's something I've mentioned before which I've always planned to do, and that's to add the capability for people without access to Windows, mostly Linux users, who are Linux users and so don't have Windows, to directly download an image file that they can then transfer to any USB drive to boot their licensed copy of SpinRite.



For those who don't know, the single SpinRite executable, which is about 280K because of course I write everything in assembly language, is both a Windows and a DOS app.  When it's run from DOS, it is itself SpinRite.  But when that same program is run from Windows, it presents a familiar Windows user interface which allows its owner to easily create bootable media which contains itself.  So it can, you know, that can either format and prepare a diskette - which there's not much demand for, but since the code was there anyway from 6.0, I left it in.  Or it can create an optical disc ISO image file for burning to a disc or loading with an ISO boot utility.  It can also create a raw IMG file, which can be put somewhere, or it can prepare a USB thumb drive.



Twenty years ago, back in 2004 when I first created this code, the dependence upon Windows provided a comprehensive solution because Linux was still mostly a curiosity back in 2004 and had not yet matured into the strong alternative OS that it has since become.  Today, there are many Linux users who would like to use SpinRite, but who don't have ready access to Windows in order to run SpinRite's boot prep.  And this need for non-Windows boot preparation will continue with SpinRite 7 and beyond.



The approach I've developed for use under Windows, which is used by InitDisk, ReadSpeed, and SpinRite, is to start the application, then have its user insert their chosen USB drive while the application watches the machine for the appearance of any new USB drive.  This bypasses the need to specify a drive letter, it works with unformatted drives or drives containing foreign file systems so they wouldn't get a drive letter, and it makes it very difficult for the user to inadvertently format the wrong drive, which was my primary motivation for developing this user-friendly and pretty foolproof approach.



Unfortunately, there's a downside.  It uses very low-level Windows USB monitoring, which was not implemented in WINE, which is the Windows emulator for Linux.  So for Linux users, a ready-to-boot image file is the way to go, and I'm in the process of putting the final pieces of that new facility together.  It should be finished later this week, and I'm sure I'll just make a note of it next week that it's, you know, you can go use it.



And let's take our last break, and then we'll do some feedback from our listeners and get to our episode's main topic.



MIKAH:  Wonderful.  All right.  I do have to take a moment here.  By the way, Mikah Sargent, subbing in for Leo Laporte, who is on vacation.



STEVE:  We knew that.



MIKAH:  It's time to close the loop with feedback from the listeners.



STEVE:  Yes, it does.  Okay.  So John Robinette said:  "Hey, Steve.  I'm sure I'm not the only one to send you this note about Telegram after listening to SN-964.  There's been much chatter about the protocol Telegram uses for end-to-end encryption, but it is a common misunderstanding that they use this by default.  Telegram's default uses only TLS to protect the connection between your device and their servers, and does not provide any end-to-end protection.  They have an additional feature, 'Secret Chats,' that does use end-to-end encryption on the client device.  It is not possible to use end-to-end encryption in group chats, and when used for one-on-one chats it limits the conversation to a specific device.  Based on my anecdotal experience using Telegram with a few friends, most people either do not know about 'Secret Chats' or do not use it.



"I found a post from 2017 that explains Telegram's reasoning for this."  And he quoted it, saying:  "The TL;DR is because other apps, for example WhatsApp, allow you to make unencrypted backups" - this is Telegram speaking, essentially - "actual end-to-end encryption isn't worth being on by default."



MIKAH:  What?



STEVE:  Okay.  So they don't turn it on by default, and claim they are more secure because of how they store backups.  He says:  "Anyway, it's not just Apple marketing speak that Telegram is not end-to-end encrypted.  It's by design from Telegram.  Thanks."



MIKAH:  Wow.



STEVE:  So John, thank you.  And I should mention several other of our listeners who actually use Telegram also shared their experience.  What they showed was that, unlike the way I presumed it would be when I talked about it last week, Telegram really is probably mostly used in its insecure messaging mode since not only is it not enabled by default, enabling it is not even a global setting.  It must be explicitly enabled on a chat-by-chat basis.  So that makes the use of encryption hostile in Telegram, which is certainly not what anyone hoping for privacy wants.  And its fundamental inability to provide end-to-end protection for multiparty group chats strongly suggests that it's being left behind in the secure messaging evolution.  So I presume that they're aware of this, and they're hopefully working behind the scenes to bring Telegram up to speed, since otherwise it's just going to become an historical footnote.



Given these facts, I certainly reverse myself and agree with Apple's placing Telegram down at Level 0, along with QQ, whatever that is, where it certainly belongs.  So thank you very much, John and everybody else, for educating me about Telegram, which I did not take the time to - and I'm amazed, for all the talk of Telegram, that it is as insecure as it obviously is.



Someone who's tweeted before, whose handle is a hacker version of anomaly, it's 3n0m41y, he said:  "Steve, KeePassXC latest release is also supporting Passkeys now.  Great news."  Okay.  I'm not a Passkey user, so I don't know what communication they may provide to their users, so I wanted to pass along that welcome news.  Back when Passkeys first appeared, supported only by Apple, Google, and Microsoft, they each created their own individual and well-isolated walled gardens to manage Passkeys only on their devices, and only within their own ecosystems.  There was no cross-ecosystem portability.



At that time we hoped that our password managers, which are already an inherently cross-platform, would be stepping up and getting in on the act because they would be able to offer fully cross-platform Passkey synchronization.  And as we know, Bitwarden, a TWiT network sponsor, has done so.  And given competitive pressures it will soon become incumbent upon any and all other password managers to offer Passkey support.  And we'll be talking about what that means when we get to this week's main topic.  But for now, for anyone who's using KeePassXC and didn't know, it's now got Passkey support.  So that's all for the good.  A user whose actual...



MIKAH:  Good luck with this one.



STEVE:  Exactly.  His Twitter handle, he later tells us, was created by asking Bitwarden for a random gibberish string.  He said:  "Episode 964."  He said:  "I'm on the go and won't have time to write an eloquently worded message like the ones you read on the show."  Actually, I think his was, but okay.  "So feel free to simply summarize this if you find it relevant.  In 964" - last week - "you mentioned feedback from a listener who mentioned the Taco Bell app using email passwords 'for convenience.'



"Before my main comment, I'd like to mention that they are not the only ones doing this.  I suspect the most popular service out there today that does this is Substack, the blogging service that has exploded in popularity since early/mid COVID.  When I signed up there for the first time, I wasn't aware, and the whole process was very confusing and counterintuitive given my previously learned username/password behavior.  Beyond being confusing, it truly is much more of a pain for those of us who use browser-based password managers, which have made the traditional login process rather seamless."



Okay.  So I'll just interrupt to say, right, you know.  And you probably haven't heard me say this, Mikah.  The more we explore the topic, given that passwords are entirely optional, when every site includes an "I forgot my password" recovery link, I think that viewing passwords as login accelerators, which are handled now by our password managers, is quite apropos.



MIKAH:  I like that, yeah.  That's a good way to think of it because, yeah.  Anecdotally speaking I know many people who are not as techie as we are whose password really is that "I forgot my password button" every time, that's the password because they forget it right after they've changed it, and then they go to their email.  So in a way that - because I have always hated this, the I try to log in, and they send me what they call a "magic link" to my email.  Slack has this functionality.  They kind of suggest you use that first and foremost whenever you log back into Slack.  And mentioned here Substack, there are so many that do this.  And I don't like it.  But when I think about now anecdotally the people I know in my life who are already doing that, going to their email every time to reset their password again, yeah, password or a login accelerator.  That's clever.



STEVE:  That's really what it is.  And in fact we began this dialogue several weeks back.  We did a podcast titled "Unintended Consequences."  And what became clear was that with Google's deprecation of third-party tracking cookies by midpoint of this year, the advertisers are freaked out that they're going to lose the ability to aggregate information about their visitors.  So they're putting pressure on websites to add a "give us your email to join the site."



And so the unintended consequence of third-party tracking being blocked robustly in Chrome, as it's going to be, is that websites are being incentivized to ask their visitors, not necessarily to create an account, but to "join."  And so they're bringing up a request for your email address, then sending you a link which you have to click on so you just can't use gibberish, in order to continue using the site.



So essentially this is going to get worse.  And when we just go to a site, if your browser doesn't already have a persistent cookie for the site, then the site will ask you for an email address just to get it because they want to be able to provide that email address to the advertisers who are on the site because the advertisers will pay more money to the site if they receive an email address which has been confirmed in return.  So we're entering a world of pain here.  But so that brought us to the whole idea of using email in lieu of logon and what that means.  And so that's what this guy's talking about.



So he says:  "Anyway, my main comment is that it may very well be possible that these implementations are not for user convenience," meaning implementations of logon with email.  He says:  "They are for liability."  He says:  "By requiring an email to log in, the host company hoists account breach liability off of their own shoulders onto the email providers.  Your account got hacked?  That's a Gmail breach.  Not our problem."



He says:  "On a side note, you also mentioned in the episode a listener in the financial sector who commented on password length, complexity, and rules being limited by old legacy mainframes.  I just wanted to share that I worked for a federal agency about a decade ago, and thought I discovered a bug when I realized I had just been let into my sole enterprise account despite missing a character on the end of my password."



MIKAH:  What?



STEVE:  "I tested that again and discovered the system was only checking the first eight characters of the password I thought I was using.  I grabbed a fellow developer to show them, thinking I'd blown the lid off some big bug, and he shrugged and said, 'Yeah, it only checks the first eight.'"  He said:  "I was stunned."



MIKAH:  Good god.



STEVE:  "Thanks for the wonderful content.  Cheers."  He says:  "P.S.:  Yes, this account username was generated by my Bitwarden password generator."



MIKAH:  Got it.



STEVE:  Okay.  So we first took up the subject of email-only login with our "Unintended Consequences" episode, since it was becoming clear that websites were planning to react to the loss of third-party tracking with the establishment of first-party identity relationships with their visitors.  And in every instance the company's so-called "privacy policy" clearly stated that any and all information provided by their visitors, including their name and their email address, not only could be, but would be, shared with their "business partners," which we know are the advertising networks providing the advertising for their site.



But I thought this listener's idea about limiting liability was interesting, and it had not occurred to me.  When you think about it, what's the benefit to a website of holding onto the hashes of all of its users' passwords?  The only benefit to the site is that passwords allow its users to log in more quickly and easily, when and if they're using a password manager.  The problem is the site cannot know whether the user is using a password manager.  How big an issue is that?



Today, only one out of every three people, 34%, are using a password manager.  And while this is a big improvement over just two years ago in 2022 when the figure was one in five, or 21%, this means that today two out of every three Internet users are not using password management.  And we know this suggests, exactly as you were saying, Mikah, of your friends, that the quality of those two-thirds of all passwords may not be very high.  They may no longer be using "monkey123," but their chosen passwords are likely not much better.



Now look at what happened with the 23andMe disaster.  A shocking number of 23andMe user accounts were apparently compromised using some form of password spray.  That could never have succeeded if 23andMe users were logging in with highly complex unique-per-site passwords, which of course are encouraged by the use of any password manager.  But with two-thirds of Internet users still today not using a password manager, that suggests that two-thirds of 23andMe users were not being well protected from the abuse of their poor password hygiene.  If you're not someone who understands the value of password management, "monkey123" looks pretty good, and it sure is easy to remember.



My point is the quite successful attack on 23andMe demonstrates that the use of traditional username and password login creates a single point of failure which facilitates automated attack.  A widely distributed network of bots can create authentic-appearing web sessions and attempt to login as legitimate users over and over and over under the cover of darkness, succeeding eventually and incrementally with no involvement on any user's part.  And that makes a crucial difference.  By comparison, if a user's email inbox were to be flooded with 23andMe login requests, every such user - and quite soon 23andMe - would know that someone or something was up to no good.  But would an attacker even bother?  Doesn't the email loop completely thwart such attacks?  Logins are no longer autonomous.  And unless the attacker obtains access to every user's email account, the attacker cannot login using user authentication data contained in the email.  This eliminates all generic remote password spray brute force attacks.



So the point I wanted to make is that this listener's comment about liability is very interesting.  We're constantly hearing about site breaches, with Troy Hunt's "Have I Been Pwned" service constantly collecting massive new troves of breached user login credentials.  All of that disappears if a website no longer offers to store its users' password hashes.  Why bother?  It's just a potential nightmare.  It's a liability, a secret they're not good at keeping.  With two-thirds of users today still not bothering to use a password manager, they are likely using crappy passwords, or the same password at all their different sites.  This renders the site vulnerable to attack, as 23andMe was, through no fault of theirs because their users cannot be bothered to secure themselves better.



So these attacks go unnoticed and unobserved because they can be conducted by autonomous distributed networks of bots.  But that cannot happen if the user's email account is dynamically included in the login process.  This completely changes the attack dynamics.  And don't get me wrong, I'm not suggesting that I think this is a good idea, except that it kind of is, because it definitely represents a nightmare for the other one-third of us who are using password managers stuffed with long passwords we could never begin to memorize or even correctly enter into a password field.



The problem for our future is that email loop login makes a horrible kind of sense for any website that gets to choose whether or not they want the liability of storing the hashes of their users' potentially crappy passwords.  It's easy to imagine websites deciding that they'd prefer to ask their users to obtain a per-login token from their own email.



MIKAH:  Dang.  This makes sense, Steve.



STEVE:  Yeah.



MIKAH:  Sorry, but it's my responsibility to make sure that this part of the episode never makes it to the light of day so that every - no, I'm just kidding.  Now I'm worried that people are going to be listening to this and going, you know what?  Because you just convinced me.  This whole time I've been annoyed by that email login magic link nonsense.  But it's not nonsense, especially because it requires so much extra effort by these folks who are right now not using much effort at all.  That is so very true.  Huh.



STEVE:  And unfortunately it transfers the effort to the user who wants to log in.  So it distributes it.  Now, we're going to be talking by the end of the podcast about Passkeys versus not.  And an interesting compromise suggests itself here, which is, if you use Passkeys, then you get instant logon.  If you don't want to use Passkeys, you get email loop login.  Because as I'm going to demonstrate, usernames and passwords, they're not equivalent, even close.  So more to come.



Mark Jones tweeted:  "Another in the 'it was nice while it lasted' category."  He said:  "I have run into two sites that won't accept anything @duck.com emails."  As we know, DuckDuckGo offers an email anonymizing forwarding service, right, where it's something @duck.com.  We've talked about this for several weeks as a solution to the problem of every website you visit wanting your email address just for the privilege of seeing the content because they want to provide that to their advertisers.  Then the solution is, oh, let's create, you know, anonymous fake email accounts.  Well, Mark is noting that he's run into two sites that will not accept @duck.com emails.  He said:  "A previous solution, using a '+' in a Gmail address, also has been thwarted.



He said:  "That allowed me to" - that, using a "+" - "allowed me to filter easily and reject sites of no interest or that got spammy.  One service stripped the plus sign and what was after it out.  The other trend you haven't mentioned is the insistence on a cell number.  Those are unique, difficult to share, and are harder to make throwaway.  I was trying to set up services for a community organization, only to have a couple of vendors balk because my cell was already associated with a different active account.  Love the podcast, look forward to it every week, and I'm happy it will continue.  Tested 6.1 and am loving that, too."



Okay.  So as Mark wrote:  "It was nice while it lasted."  Unfortunately, while it is possible to hide our real email address behind an identity-protecting email-forwarding service, it's not possible to hide the fact that we're using an identity-protecting email-forwarding service.  And the fact that a site is stripping the plus sign and what follows from a user's email address is nothing short of rude.  You know, it's not their business to alter someone's email address.  That really, you know, wow, that seems so wrong.  The only leverage website visitors have is to choose not to play.



Normally, I would suggest that such email address discrimination would not become widespread.  That is, you're only going to see it at a few sites.  But if websites are asked for their visitors' email as a replacement for third-party tracking, with the incentive of being paid more for ads served to identifiable Internet users, which is the case, then those advertising interests who are definitely paying do not want to pay more for anyone using an anonymizing email-forwarding service.  So you can bet that a site's advertisers will be telling those websites that they won't pay for anonymized email.  And then, in turn, those sites will be telling their visitors exactly what our listener Mark Jones was told:  "Please provide your primary email address, not a forwarding service."



MIKAH:  Wow.  That - so I have not had that issue.  And I use Fastmail, who is a sponsor on the network, along with I use the password manager 1Password.  And so when it automatically generates it, it creates a masked email for me.



STEVE:  Yup.



MIKAH:  And that right now, knock on wood, has not been an issue.  But I have had the issue with the plus sign.  I always - I never thought of it, because this has been a long time ago, and I always - I didn't think of it in the nefarious way that we're thinking of it here.  I just thought it was somebody who didn't know what they were doing coding-wise, and so they couldn't, like, parse the plus and what came after it.  And I thought, oh, maybe they just don't know how to accept that and think that it's a - the rules for what you can type in, it's telling the system that it's a fake email, or that it's not an email address because of the rules that are set up.  But now I can see, too, how in that case it could be somebody going, no, we know that that's just adding on.  Because I did, I had my name plus spam @gmail.com.



STEVE:  Right.



MIKAH:  And I could filter it out that way.  And I had to - I ran into that issue too many times, so I had to stop doing that.



STEVE:  Yup, yup.  Well, let's hope that Fastmail continues to be available to you.



MIKAH:  Yes.



STEVE:  It'd be interesting to find out if it is not at some point.



MIKAH:  Definitely.



STEVE:  Rob Powell wrote:  "Hi, Steve.  Following recent discussions on the podcast, I thought other listeners might also appreciate the below.  It's a tool to detect when Chrome extensions change owners."  And this is really interesting.  I got a kick out of the name of the tool that Rob pointed to.  It's posted on GitHub under the account of "classvsoftware,"  all one word, classvsoftware, with the name "Under New Management."



A couple of weeks ago we were noting that the authors and maintainers of Android freeware which had accumulated large install bases over time and earned some implicit trust from their users have effectively been cashing out their installed bases to less than scrupulous buyers, who then take advantage of that installed base, you know, to stick ads in the apps that never used to have them.  You know, like dumb apps, you know, free things, like measuring sticks and things.  I mean, just nothing.



Since the same thing could - well, actually, specifically it was an app that allowed you to change the brightness of the flashlight.  And, you know, it became adware because the author who, you know, wasn't getting paid anything for it, got tired of maintaining it and finally decided to accept an offer.  Since the same thing could happen with browser extensions whose special position in our browsers make this an even more pressing and worrisome problem, I was glad that Rob brought this up.  I went over to the GitHub site to see what the authors of this extension had to say.  That is, this "Under New Management" extension.



They explain.  They said:  "Intermittently checks your installed extensions to see if the developer information listed on the Chrome Web Store has changed.  If anything is different, the extension icon will display a red badge, alerting you to the change."  And as to why this is an issue, they write:  "Extension developers are constantly getting offers to buy their extensions.  In nearly every case, the people buying these extensions want to rip off the existing users of those extensions.  The users of these extensions have no idea an installed extension has changed hands and may now be compromised.  'Under New Management' gives users notice of the change of ownership, giving them a chance to make an informed decision about the software they're using."



Now, in the text the phrase "constantly getting offers to buy their extensions" was a link.  So I wondered what the author of this "Under New Management" extension might be linking to, and holy crap.  The link was to another GitHub page titled "Temptations of an open-source browser extension developer."  But before I describe what's there, let me back up a bit.  The extension in question is called "Hover Zoom+," and its author explains.



"Hover Zoom zooms images and videos on" - so this is the extension, Hover Zoom.  "Zooms images and videos on all your favorite websites - Facebook, Amazon, et cetera.  Hover your mouse over any image on the supported website, and the extension will automatically enlarge the image to its full size, making sure that it still fits into the browser window.  This is an open-source version of the original Hover Zoom extension, which is now overrun by malware and deleted from the store.



"In this version, all spyware has been removed, many bugs were fixed, and new features were added.  It doesn't" - does not - "collect any statistics whatsoever.  The only permission it needs is to access data on all websites to extract full images, and optional permissions to access browser history, download/save images, or get tab URLs for per-site configuration.  This extension will never be sold out and will never compromise users' privacy.  As a proof, please see the list of all takeover offers I've received over the past years."



Okay.  And this brings us back to the "Temptations of an open-source browser extension developer" page.  It is so astonishing that I've made it the GRC shortcut of the week.  If you're at all curious, and please be curious because it is something, visit grc.sc/965.  So that's today's episode number, 965, grc.sc, you know, short for shortcut, grc.sc/965.  The author starts off, the same author who we just heard from, who fixed this malware-ridden previous Hover Zoom extension, and now vows that he will never sell it.



He said:  "Over the years I have received many proposals to monetize this extension.  So I think I'll just start posting them here for fun, but not for profit.  The main reason I continue to maintain this extension is because I can hardly trust others not to fall for one of these offers.  I'm fortunate to have a job that pays well enough to allow me to keep my moral compass and ignore all of these propositions.  I realize that not everyone has the same financial security.  So hopefully this thread will shed some light on what kind of pressure is put on extension developers."



And what follows really serves to put this into perspective.  The first offer he posts is dated September 28th, 2015.  It reads:  "Hope this message finds you well.  I'm a Strategic Partnerships Manager at a monetization platform for browser extensions."  In other words, there is such a thing.  "I'm contacting you since I came across the extension 'Hover Zoom+' at the Chrome Store.  I consider your product can help you bring profit by means of collaborating together.  I would like to suggest a potential partnership between our companies that will significantly increase your revenues.  Are you interested in discussing our offer in more details?  Hope for positive feedback and ongoing cooperation."  Okay, that was the first one he listed.



The most recent offer the author posted four weeks ago on February 14th, 2024, read:  "I trust you're doing well!  Currently I'm exploring opportunities to grow my business by investing in Chrome extensions.  Your extension Hover Zoom+ has caught my attention, and I am genuinely interested in discussing the possibility of acquiring it.  We can discuss the price and complete the transaction securely through a reputable escrow service, escrow.com or cryptoexchange.com.  Google supports a smooth transfer of extension ownership from one account to another, ensuring your Gmail account remains unaffected.  If you have any inquiries, or if this aligns with your plans, feel free to reach out to us via this email.  Looking forward to hearing from you."



Now, in between that first and last offer are more than - I counted them - 165 other offers this guy has received from parties interested in taking "Hover Zoom+" off his hands.  I say "more than" because for a while I was not counting the follow-ups as being separate offers.  But there were so many of them that after a while I decided a more accurate and fair count should include them.  So 165 and counting.



The danger to the users of extensions that are sold to unscrupulous buyers is that an originally benign extension might be altered to begin harvesting its users' browsing data as the U.S. Federal Trade Commission has formally accused the Avast browser extension of doing.  Since the purchasers of popular extensions with a large installed base have no interest in ongoing support of that extension and simply wish to maximize their return on investment while they can, they will have a plan for somehow monetizing the extension's user base.



Given that, the value of the "Under New Management" extension becomes clear.  So thanks for bringing it to our attention, Rob.  Appreciate it.



And lastly, Max Feinleib said:  "I'm sure I'm not the first person who's mentioned this, but SN-905 will be hard to beat for the shortest title."  And he says that was just the numeral, the digit "1."  And I made a comment to Leo last week that "PQ3," which was last week's title, might be the shortest podcast title we've ever had.  Max was actually the second of two sharp listeners who pointed out what amounted to "Not so fast there, Gibson.  What about podcast number 905?  That was titled with the single digit '1'."



That frighteningly low number was a reference to the password hashing iteration count some unfortunate LastPass users found when they went looking.  So anyway, thank you for catching that, Max and the other person who mentioned it.  You're correct.  I will never have a shorter podcast title than "1."  I can't imagine a null title, so yeah.



Okay.  So I think people are going to be a little surprised.  As I mentioned last week, I intended to title this podcast "Morris II."  That'll be next week.  This is a result of a tweet from Stephan Janssen.  He said:  "Hi, Steve.  I'm an avid SN listener and, in part thanks to your information, moved from LastPass to Bitwarden.  Now that Passkeys are becoming more prevalent, I'm noticing a Bitwarden popup when I try to enroll my YubiKey as my two-factor authentication token.



"I'm tempted to give Passkeys in Bitwarden a try, in part due to your enthusiasm about it, but I'm also hesitant since it feels like I'm losing and giving up my second factor.  With Google, for example, using a Passkey allows me to log in without providing anything else, which makes it feel like I no longer have MFA for my account.  Am I right in thinking that Passkeys will reduce my security if I'm now using a random password with a second factor, or am I missing something in my thinking?"



Okay.  I'm going to give everyone the short TL;DR first.  But then because, as they say, "it's complicated," I'll explain more.  So here it is:  In a properly operating world, Passkeys, all by its lonesome, provides far more security than any super-strong password plus any second factor, period.



Let's start working our way through what has become a confusing authentication minefield.  First off, why does anyone use a second factor?  And I'm going to pointedly ignore the primary reason we, who listen to this podcast, use second login factors, which is mostly because they're cool.



MIKAH:  Fair.



STEVE:  Right?  You know, it feels a little secret agent-like to have to go look up that time-varying code to complete an authentication.  It definitely provides the impression of having much greater security, even if there are still ways around it.  And we've talked about the ways around it.  Multifactor authentication can be and is actively compromised when there's some strong need to do so.  There are two ways this can be accomplished.



The first is arranging to insert a man in the middle.  This is done in today's world by arranging to trick an unwitting user into going to a fake login page.  They provide their username and password, and are then prompted for the second factor.  They provide that to the man in the middle, who turns around and immediately logs in as them before that ephemeral six-digit code has a chance to expire.  This is not easily done because it requires a higher end attacker and some setup.  But it is being done.  Normally this only intercepts a user's username and password, which does give the bad guys more permanent account access until the password is changed.  But when you think about it, once you have that man-in-the-middle proxy in place to intercept the username and password, the addition of an ephemeral second factor adds very little to the user's actual security.



The second way the MFA system can be defeated is the same way any site breach, with a loss of their authentication database, you know, their password hashes, can defeat the security for their users.  Just as the authentication database contains a hashed password which is used to verify the user's freshly- provided login attempt password, that authentication database must also contain the shared MFA secret which keys the user's TOTP, their time-based one-time password.  So if that MFA secret can be stolen along with the username and password hash, then bad guys have potentially acquired all of the secrets they need to impersonate the site's users anytime they wish.



The key here is my use of the term "secrets" because today's time-based one-time passwords are based upon shared secrets.  The site knows the secret key, and your authenticator app has the same secret key.  That's what allows the authenticator to generate a six-digit code based upon the time of day; and for the server, knowing the same time of day, to generate the same code and compare them.



And "storing secrets" is the crucial difference in the Passkeys system.  This was also true with SQRL's technology which I articulated on this podcast more than 10 years ago.  What I used to say was that "SQRL gives websites no secrets to keep."  And exactly the same is true of today's Passkeys.



The crucial difference is secret versus public key technology.  That's what makes Passkeys so very different from everything that came before it, except of course SQRL, which works similarly.  With Passkeys, the user's private key never leaves the authentication device end, and the website only ever receives the user's matching public key.  And the only thing that the website's public key can be used for is verifying the signature of a unique challenge.



The website sends the user a never-before-used large random number to sign.  Since the large random number has never before been used, its signature will also never have been used before, so no form of replay attack is possible.  The user's Passkey agent signs the challenge received from the server and sends it back to the website, signed.  The website then uses its matching public key to verify the user agent's Passkey signature.  And crucially, verifying signatures is all it can do with that public key.  That's the only power it has.  So if that key were to be stolen, no one cares.  Truly, no one cares.



The reason I felt that Stephan's question deserved our attention is that none of this crucial difference between traditional username and password, and even including multifactor authentication, versus Passkeys is in any way apparent to users.  Users just see yet another thing, another way to log on.  What's impossible to see and to appreciate from the user-facing side is how completely and importantly different Passkeys is from everything that has come before.  So to respond to Stephan's uneasiness about whether he's losing anything, sacrificing anything, or giving up anything by choosing to use a Passkey instead of having traditional login - even when it's augmented with multifactor authentication - the bright flashing neon answer is "No."



The best way to think about multifactor authentication is that it was a last-ditch temporary Band-Aid that was added as an emergency measure in an attempt to do something to shore up the existing ecosystem of fundamentally weak and creaky server-side secret-based password-only authentication.  You know, passwords are not that great.  Let's add another factor.  Unfortunately, it's not that great either.  But it's a second one, and it works differently, so that's better than nothing; right?  Everything about Passkeys is superior to everything that has come before it.  Well, okay, except for SQRL, but that argument is now academic.



However, while everything I just said is true in theory, it may not, interestingly enough, be completely true in practice.  As we know, security is always about the weakest link in the chain, and that's every bit as true for authentication.  If a Passkey is set up for website login, that system of authentication will be the most bulletproof solution mankind knows how to design today.  But if that website also still supports username and password authentication, with or without any second factors, then that username and password fallback will reduce the entire system's effective security, the fact that it's still there.



The problem is that, while Passkeys are actually vastly more secure than username and password login, they're primarily seen as being more convenient.  That is, Passkeys; right?  They're vastly more secure than username and password login, but most people just see them as more convenient.



MIKAH:  Yeah.



STEVE:  In other words, it's not necessarily the case that enabling the use of a Passkey will simultaneously and robustly disable all use of any preceding username and password.  And that is what we really want.  If Passkeys are enabled, we want usernames and passwords to be disabled.  If you've never used a website which asks you right off the bat to create an account for the first time ever, and offers to let you use a Passkey with no - never giving it a username or password - you're golden since you won't have ever given that website any secrets to keep because Passkey doesn't.  It cannot disclose what it doesn't know.



The question is, will websites that start allowing the use of Passkeys also allow their users to then erase all traces of, or firmly disable the use of, their previous username and password for login?  Because if they don't, the use of Passkeys is only giving them, the user, more convenience.  It also is more secure in the transaction than a username and password because it can't be intercepted in the same way that a username and password can on the fly.  But it's not the absolute security that it could be if it's still possible to compromise the old-school login.



Okay, now, this problem was something that my design of SQRL anticipated.  After its user had become comfortable with the way SQRL operated and had printed out the necessary identity backups, they were able to turn on a setting that requested every site they subsequently visited to please disable any and all alternative means for logging in, including specifically the email loop fallback.  You know, because I've been doing security for a while, I understood that this is a matter of security only being as good as the weakest link.



Okay.  So here are our takeaways from this:  The crypto technology which is well hidden inside Passkeys totally blows away both username and password and even multifactor authentication, which all rely upon server-side secrets being kept.  That's their universal weakness, which is what makes Passkeys not only more convenient, but also far more secure.  So whenever given the choice, set up a Passkey without a second thought and without looking back.



Secondly, look around to see whether there's any way to disable any other previous less secure logon fallback.  If you cannot disable username and password login, but you are able to strengthen it with MFA, then go ahead and do that.  Adding MFA is still the best way to make username and password login safer against attack, even if you're not using your username and password any longer because you've switched over to Passkeys.



But one thing you can definitely do is remove your old username and password from your password manager.  And if for some reason you cannot remove it, change it to something bogus, like a long string of pound signs.  That will be your sign that this account uses Passkeys.  And by removing your password manager's storage of your account's true password, you're protected from any compromise of that password manager or its cloud backup provider.



The big takeaway here is that, even though they may not look all that different from the outside, the technology underlying Passkeys makes them an unreserved win over everything else that has come before.  Now that our password managers are fully supporting Passkeys natively, which is what we've been waiting for, whenever possible, make the switch, and then follow up by doing anything you can to remove the use of the username and password login that preceded it.



And Stephan, thank you for the question.  This has obviously been an issue that's needed some time and clarification, and I'm delighted that we were able to give it.  Passkeys really do rock.



MIKAH:  Wow.  Okay.  So I have to ask you, how does this compare then - because I was also thinking another thing that sites will often provide you with are those special-use one-time codes that you're supposed to save, that they also are storing obviously, that could then be grabbed off of the site if it was hashed and somehow they got access to it or whatever.  But what about if I stick my little key into the side of - my hardware key.  Is that somewhere in between where Passkeys exist, as the multifactor authentication option, I mean?  I take my little YubiKey and put it into the side and touch it.  Does that provide a little bit more security than using those one-time codes?



STEVE:  Yes, much.  Assuming that it's a FIDO key, and that's the key.  That's the key, so to speak.  Assuming it's a FIDO key, then it is essentially doing what Passkeys is doing.  What Passkeys is, is technically FIDO2.  And the reason FIDO2 happened was that the uptake of the hardware dongles to do FIDO1 was just too slow.  People, you know, again, only one third of the Internet are using password managers?  Two thirds are still using monkey123, or maybe 456?  I mean, that just shows you that this is a heavy lift.



So people just don't really give that much concern to their login security.  So they're not going to go out and buy, you know, spend any money to buy something that they just think, well, you know, what the heck?  I'll just use monkey.  Why do I need a key?  So as long as that dongle is doing FIDO login, and if it's at all recent, that's why you have it, that's what it's doing, then it is performing the same sort of public key transaction that Passkeys does.



MIKAH:  Got it.  And you brought up a really good point, and then we'll say goodbye.  But I just, I really liked what you said earlier about how we view it as so convenient, and that alone makes us maybe go, oh, I don't want to use this, it's too easy.  That must mean that it's not as good.



STEVE:  Right.  We don't get to be a secret agent.



MIKAH:  Yeah.  It sort of sits, like, is it secure?  Because all I did, I just did it this morning, I needed to relog into a Google account that the session had expired, and I popped up to say use my Passkey, and it just went right in.  I'm like, man, this is just so easy.  It's so much simpler.  But yeah, we think, oh, but how come I didn't have to whisper something into the phone and scan my retina and do all this other stuff?  It can't be as...



STEVE:  Tap your heels three times.



MIKAH:  Yeah, exactly, exactly.  Steve Gibson, I want to thank you for an information-packed episode.  I certainly learned a lot today.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#966

DATE:		March 19, 2024

TITLE:		Morris the Second

HOSTS:	Steve Gibson & Mikah Sargent

SOURCE:	https://media.grc.com/sn/sn-966.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  Voyager lives!  (Maybe).  The World Wide Web just turned 35.  What does its dad think?  What's the latest horrific violation of consumer privacy to come to light?  Our listeners have been extremely engaged and interested in several of this podcast's recent topics so we're going to use their feedback to finish off several of those topics.  And finally, we look at how a group of Cornell University researchers managed to get today's generative AI models to behave badly, and at just how much of a cautionary tale this may be.



SHOW TEASE:  Coming up on Security Now!, first we talk about Voyager.  You may have thought that Voyager was buh-bye, but it is not.  It is still sending signals.  Plus, you know, Tim Berners-Lee, the Father of the Web, sits down and has a serious talk, well, at least as far as Tim Berners-Lee sees it, about the state of the Internet and maybe some disagreements he has with the way it is all going.  We also have a lot of feedback from listeners about last week's episode regarding Passkeys and the current state of security online when it comes to passwords and two-factor authentication and what you should or shouldn't use.  And we also talk about Morris the Second, a worm that has some serious implications for generative AI agents.  All of that and so much more coming up on Security Now!.



MIKAH SARGENT:  This is Security Now! with Steve Gibson and this week Mikah Sargent, Episode 966, recorded Tuesday, March 19th, 2024:  Morris the Second.



Hello and welcome to Security Now!, the show where the cybersecurity guru, Steve Gibson, provides a week's worth of cybersecurity news in but a small package, so you can just plug in directly and download it into your cranium.  I am just here to help facilitate this.  I am, how do you say, let's go with the super safe USB flash drive that you can plug into your cranium.  I'm just providing the means by which you connect to Steve Gibson, who actually is providing all of the information.  That's the role I play here.  Also the role of shock and awe because occasionally I am gobsmacked by what Steve ends up telling us.  But Steve, it is good to see you again this week.  How you doing?



STEVE GIBSON:  Mikah, great to be with you for our second week in a row, as Leo finishes working on his tan and presumably gets ready to return.  So we're going to do, as we thought we were going to do last week, but got pushed because of, wow, what turned out to be a surprisingly interesting episode for our listeners, the Passkeys vs Multifactor Authentication.  In fact, I was so overwhelmed with feedback from that, and it was useful stuff, comments and questions and so forth, that it ended up still being a lot of what we end up talking about today just because, you know, certainly the whole issue of cross-network proving who you say you are, which is to say authentication, is a big deal, and important to everybody who's using the Internet.  So we're going to talk about that some more.



But we are going to get to what was supposed to be last week's topic, which I had originally as Morris II, but I saw that they're referring to themselves or their creation as Morris the Second.  So Morris the Second is today's number 966 and counting Security Now! podcast title.  But first we're going to talk about how it may be that we were doing the requiem for Voyager I a little prematurely.  It may not be quite dead or insane or whatever it was that it appeared to be a week ago.  Also, the World Wide Web has just turned 35.  What does its dad think about how that's going?  What's the latest unbelievably horrific violation of consumer privacy which has come to light?



We're going to share a lot about what our listeners thought about Passkeys and multifactor authentication and the ins and outs of all that.  And then, as I promised, we're going to look at how a group of Cornell University researchers managed to get today's Generative AI to behave badly, and at just how much of a cautionary tale this may be.  So I think a lot of interesting stuff for us to talk about.



MIKAH:  Absolutely.  Again, shock and awe, I can't wait.  I'm looking forward to it. 



STEVE:  So, okay.  I have a large collection of photos in my archive that I'm ready to deploy on demand.  But this one just caught me by surprise.  Somebody tweeted it, and I thought it was so cute.  The caption I gave it was, "Wait.  You mean you did not put this wonderful gymnasium on the floor because it's the perfect space for us to play in?"  And we have an open old-school large computer case which three kittens have managed to get themselves into, and a fourth one is sort of looking on enviously.



One of them, the upper kitten looks like it's standing on maybe an audio card or a graphics card that's been added to the case.  I actually have a number of these exact cases.  I look at it, and it's very familiar.  The motherboard in there is actually pretty ancient, so I don't know what the story is, where this came from or what's going on.  And the hard drives look like they are in need of some help.  But anyway, just a fun picture of just - and my god, these little kittens are adorable.



MIKAH:  They're so adorable.



STEVE:  I mean, how could anything be that cute as these things are?  So not exactly a cat video, but cat video done Security Now! style.



MIKAH:  Yes, indeed.  Those adorable little cats that you should absolutely keep - you know, I like to imagine that this is somebody who brought in their computer to a place and said, "There's something wrong with this thing."  The person that's cleaning it out opens it up, and these little kittens are inside playing.  It's like, ah, that's what's wrong.



STEVE:  Yeah, a little fur ball, that's right.



MIKAH:  Isn't that kind of - that's what SpinRite does; right?  It just sends little kittens inside to fix the hard drive?



STEVE:  That's my secret formula.



MIKAH:  Aha, I figured it out.



STEVE:  Yes.  That's right.



MIKAH:  All right.  Let's get to the security news.



STEVE:  Okay.  So we have a quick follow-up to, as I said, our recent, perhaps premature eulogy for the Voyager 1 spacecraft.  It may just turn out to have been a flesh wound.  The team occupying that little office space in Pasadena instructed Voyager to alter a location of its memory in what everyone who's covering this news is calling a "poke" instruction.  Okay, now, peek and poke were the verbs used by some higher level languages when the code, which is talking in terms of variables and not in terms of storage, wished to either directly inspect, which was to say "to peek," or to directly alter, "to poke," the contents of memory.



So for the past several months there has been a rising fear that the world may need to say farewell to the Voyager 1 spacecraft after it began to send back just garbled data that nobody understood.  And so we were saying, well, it's lost its mind.  It's gone insane.  It's just spitting gibberish.  But after being poked just right, and then waiting, what, 22.5 hours twice I think is the current roundtrip time, the speed of light roundtrip - so, you know, you poke, and then you're very patient - the Voyager 1 began to read out the data from its Flight Data Subsystem, the FDS.  That is, basically, it began doing a memory dump.



And this brought renewed hope that the spacecraft is actually still, somewhat miraculously, in better condition than was feared.  In other words, it hasn't gone insane.  And the return of the Flight Data Subsystem memory will allow engineers to dig through the returned memory readout for clues.  Although, paradoxically, the data was not sent in the format that the FDS is supposed to use when it's working correctly, it is nevertheless readable.  So we're not out of the woods yet, and it still could be unrecoverable, and this is just another one of its death throes.  And really, I mean, realistically, at some point it will be.  I mean, these veterans are going to have to turn the lights off for the last time and put their office space back up for lease.  But apparently not yet.



So I expect that we'll be checking in from time to time to see how our beloved Voyager 1 spacecraft is doing.  But the game is not up yet.  So that's very cool.  And at this point, you know, it's not clear how much new science is being sent back.  I mean, it was incredibly prolific while it was moving through the planets of the solar system and sending back amazing photos of stuff that we'd never seen before.  At this point it's sort of being kept alive just because it can be.  So, you know, why not?  It's not very expensive to do.



Okay.  So the web officially turned 35, and its dad, Tim Berners-Lee, has renewed his expression of his disappointment over how things have been going recently.



MIKAH:  Oh, this is one of those "I'm not angry, I'm disappointed" situations?  I was hoping he was happy with his son.



STEVE:  Yes, my son.  I'm disappointed in the way you have turned out.



MIKAH:  Oh.



STEVE:  So one week ago, on March 12th, Tim wrote, he said:  "Three and a half decades ago, when I invented the web" - which, you know, few people can say - "its trajectory was impossible to imagine.  There was no roadmap to predict the course of its evolution.  It was a captivating odyssey filled with unforeseen opportunities and challenges.  Underlying its whole infrastructure was the intention to allow for collaboration, foster compassion, and generate creativity."  Okay, I would argue that we got two of those three, at least; you know?  He says:  "What I term the three C's."



Now, of course a lot of this is retrospective; right?  It's like, it's easy to rewrite history 35 years later.  But we'll see.  Anyway, he says:  "It was to be a tool to empower humanity.  The first decade of the web fulfilled that promise.  The web was decentralized, with a long tail of content and options.  It created small, more localized communities, provided individual empowerment, and fostered huge value.  Yet in the past decade, instead of embodying these values, the web has instead played a part in eroding them.  The consequences are increasingly far reaching.  From the centralization of platforms to the AI revolution, the web serves as the foundational layer of our online ecosystem, an ecosystem that is now reshaping the geopolitical landscape, driving economic shifts, and influencing the lives of people around the world.



"Five years ago, when the web turned 30, I called out some of the dysfunction caused by the web being dominated by the self-interest of several corporations that have eroded the web's values and led to breakdown and harm.  Now, five years on, as we arrive at the web's 35th birthday, the rapid advancement of AI has exacerbated these concerns, proving that issues on the web are not isolated, but rather deeply intertwined with emerging technologies.



"There are two clear, connected issues to address.  The first is the extent of power concentration, which contradicts the decentralized spirit I originally envisioned."  If indeed he originally did.  He says:  "This has segmented the web, with a fight to keep users hooked on one platform" - gee, wonder what that could be - "to optimize profit through the passive observation of content."  You know, like while they drool.  "This exploitative business model is particularly grave in this year of elections that could unravel political turmoil.  Compounding this issue is the second, the personal data market that has exploited people's time and data with the creation of deep profiles that allow for targeted advertising and ultimately control over the information people are fed.



"How has this happened?  Leadership, hindered by a lack of diversity, has steered away from a tool for public good and one that is instead subject to capitalist forces resulting in monopolization.  Governance, which should correct for this, has failed to do so, with regulatory measures being outstripped by the rapid development of innovation, leading to a widening gap between technological advancements and effective oversight.  The future," he writes, "hinges on our ability to both reform the current system and create a new one that genuinely serves the best interests of humanity."  To which I'm just going to insert here, good luck with that.



Anyway:  "To achieve this," he writes, "we must break down data silos to encourage collaboration, create market conditions in which a diversity of options thrive to fuel creativity, and shift away from polarizing content to an environment shaped by a diversity of voices and perspectives that nurture empathy and understanding."  Or we could just all watch cat videos because, you know, those are cute.  Anyway, he says:  "To truly transform the current system, we must simultaneously tackle its existing problems and champion the efforts of those visionary individuals who are actively working to build a new, improved system.



"A new paradigm is emerging, one that places individuals' intention rather than attention at the heart of business models, freeing us from the constraints of the established order and returning control over our data.  Driven by a new generation of pioneers, this movement seeks to create a more human-centered web, aligned with my original vision.  These innovators hail from diverse disciplines  research, policy, and product design  united in their pursuit of a web and related technologies that serve and empower us all.  Bluesky and Mastodon don't feed off of our engagement, but still create group formation.  GitHub provides online collaboration tools.  And podcasts contribute..."



MIKAH:  Nice.



STEVE:  "...to community knowledge.  As this emergent paradigm gains momentum" - I should mention podcasts that are disappearing rapidly, unfortunately.  "As this emergent paradigm gains momentum, we have the opportunity to reshape a digital future that prioritizes human well-being, equity, and autonomy.  The time to act and embrace this transformative potential is" - guess what.



MIKAH:  Now.



STEVE:  Now.  Uh-huh.  "As outlined in the 'Contract for the Web,' a multitude of stakeholders must collaborate to reform the web and guide the development of emerging technologies. Innovative market solutions, like those I've highlighted, are essential to this process.  Forward-thinking legislation" - okay, now, there's an oxymoron for you - "from governments worldwide can facilitate these solutions and help manage the current system more effectively.  Finally, we as citizens all over the world need to be engaged and demand higher standards and greater accountability for our online experiences.  The time is now to confront the dominant system's shortcomings while catalyzing transformative solutions that empower individuals.  This emergent system, ripe with potential, is rising, and the tools for control are within reach."



MIKAH:  It's starting to sound like a manifesto a little bit.



STEVE:  It really is.  And I only have a little bit more, and then I'm going to - we'll discuss this.  "Part of the solution is the so-called Solid Protocol" - capital S, capital P - "a specification and a movement to provide each person with their own 'personal online data store,' known as a POD."  P-O-D; right?  Personal Online Data.  "We can return the value that has been lost and restore control over personal data."  By putting it in a POD.  "With Solid, individuals decide how their data is managed, used, and shared.  This approach has already begun to take root, as seen in Flanders, where every citizen now has their own POD after Jan Jambon announced four years ago that all Flanders citizens should have a POD.  This is the result of data ownership and control, and it's an example of the emergent movement that is poised to replace the outdated incumbent system."



And finally, "Realizing this emergent movement won't just happen" - boy, is he right about that.  Oh, I mean, he says:  "It requires support for the people leading the reform, from researchers to inventors to advocates.  We must amplify and promote these positive use cases, and work to shift the collective mindset of global citizens.  The Web Foundation, that I co-founded with Rosemary Leith, has and will continue to support and accelerate this emergent system and the people behind it.  However, there is a need, an urgent need, for others to do the same, to back the morally courageous leadership that is rising, collectivize their solutions, and overturn the online world being dictated by profit to one that is dictated by the needs of humanity.  It is only then that the online ecosystem we all live in will reach its full potential and provide the foundations for creativity, collaboration, and compassion."  Tim Berners-Lee, 12th of March, 2024.



MIKAH:  Well, you've got your PODs; right?



STEVE:  Call me jaded.  Call me old.  But I do not see any way for us to get from where we are today to anything like what Tim envisions.  The web has been captured - hook, line and sinker - by commercial interests.  And they are never going to let go.  Diversity?  Well, one browser most of the world uses is maintained by the world's largest advertiser.  And no one forced that to happen.  For some reason most people apparently just like that colorful round Chrome browser icon.  You know, and Chrome is cleaner looking.  Its visual design is appealing. Somehow the word spread that it was a better browser, and nothing convinced people otherwise.  And what Microsoft has done to their Edge browser would drive anyone anywhere else.



But I've wandered away from my point.  People do not truly care about things that they neither see nor understand.  You know?  How do you care about something that you don't really understand?



MIKAH:  Yup.



STEVE:  The technologies that are being used to track us around the Internet and to collect data on our actions are both unseen and poorly understood.  People have some dull sense that they're being tracked, but only because they've heard it said so many times.  Oh, I'm being tracked; you know?  But they don't know.  They don't see it.  They just kind of think, okay.  It makes them feel uncomfortable, but they still do what they were doing; you know?  They don't have any idea what that really means.  They certainly have no idea about any of the details, and they have better things to worry about.



MIKAH:  Yes.  Most importantly, they have better things to worry about.



STEVE:  Yes.  Right.



MIKAH:  Yeah, absolutely.



STEVE:  Tim writes:  "Part of the solution is the Solid Protocol, a specification and a movement to provide each person with their own 'personal online data store,' known as a POD.  We can return the value that has been lost and restore control over personal data."  Now, okay.  While I honor Tim's spirit and intent - I really do - I seriously doubt that almost anyone could be bothered to exercise control over their online personal data repository.  I mean, I don't even know what that looks like.  The listeners of this podcast would likely be curious to learn more.  But as one of my ex-girlfriends used to say, "We're not normal."



My feeling is that the web is going to do what the web is going to do.  Yes, there are things wrong with it.  And, yes, it can be deeply invasive of our privacy.  But it also appears to be largely self-financing, apparently at least in part thanks to those same privacy invasions.  We pay for bandwidth access to the Internet, and the rest is free.  Once we're connected, we have virtually instantaneous and unfettered access to a truly astonishing breadth of information.  And it's mostly free.  There are some annoying sites that won't let you in without paying, so most people simply go elsewhere.



The reason most of the web is free is that, with a few exceptions such as Wikipedia, for-profit commercial interests see an advantage to them for providing it.  Are we being tracked in return?  Apparently.  But if that means we get everything for free, do we really care?  If having the Internet know whether I wear boxers or briefs means that all of this is opened up to me without needing to pay individually for every site I visit, then, okay, briefs have always been my thing.



Tim may have invented the World Wide Web 35 years ago, but he certainly did not invent what the web has become.  That of course is why he's so upset.  The web has utterly outgrown its parent, and it's finding its own way in the world.  It is far beyond discipline, and far beyond control.  And most importantly of all, today it is already giving most people exactly what they want.  Good luck changing that.



MIKAH:  Well put, Steve.  Honestly.  When I think about this - and here you go.  You said maybe you're jaded and old and this and that and the other.  I may be jaded, but I'm not exactly aged.  And so even as a relative youth, hearing that, you know, I want to, I don't know, put on a French beret and chant and say hurrah and feel it.  And I do feel it.  But I think realistically it is not - it's not realistic, if we're being honest.



STEVE:  Right. 



MIKAH:  And so as cool as that would be, and as amazing as that would be, yeah, ultimately what you're saying about the stuff that Tim is talking about here, you know, Tim Berners-Lee is talking about here, is so abstracted from how people use these devices to connect to the Internet and to communicate with one another that, yeah, it would require some level of sitting everyone down across the entire world and explaining to them how all of this works for there to be even the beginning of a concern about what would be necessary to convince everybody that they should care about this.  And as I'm saying just then, you heard all the hedging that kind of took place there.  It wouldn't even necessarily make a difference, even if you did explain it, because they still have to care about it.  And most importantly, most people don't need to care about it.  And so they...



STEVE:  Yes. 



MIKAH:  ...have bigger, better things in their world that they have to care about.  And that is, I think, always going to be the case.  And that's, you know, the people who do care about this stuff, we do our best to communicate and educate.  But, yeah, I don't know, I mean, as much as you might - I mean, I don't know.  To take the time to write all this out and to put forth this idea I think is a very noble thing.  But I do wonder.  I wish I could talk to Tim Berners-Lee sort of just, you know, face to face and say...



STEVE:  What are you thinking?



MIKAH:  Yeah, what are you - do you really think that anyone's going to do this?  Or are you  just - this is just a hopeful sort of I'm putting it out into the world, like...



STEVE:  How high is that ivory tower?



MIKAH:  Exactly.  Exactly.  We're down here.  Yeah.  I don't know.



STEVE:  Yeah.  And again, I really do believe that most users' wishes are now being fulfilled.



MIKAH:  Right.



STEVE:  You know?  I mean, my wife asks me a few questions every evening, and I say, well, did you google it?  You know, it's like, that's what I do.  I just ask.  I ask the Internet, and it tells me the answer.  Because there's so much going on, it's so complicated now, that the right model is no longer to try to know everything.  It's simply to know how to find out.  You know, that's the future, with the knowledge explosion that we're now in, and the content explosion.



So I just, again, there was an anecdote for a while, I don't remember now exactly what it was, but it was something like I have - back when people were typically using a single password, like, universally for all their stuff, someone did an experiment where they went up to people and said, here's a lollipop.  I'll trade you for your password.  And most people said okay.  You know?  I mean, they just didn't give a rat's ass, you know, about security.



MIKAH:  Right.



STEVE:  And most people just aren't as focused, I mean, this podcast is all about this kind of focus.  And as I said, an ex-girlfriend used to say to me, "You're not normal."  So, yeah, we're not.  But, you know, most of the world, they just, you know, the Internet does what they want.  And start asking them to pay...



MIKAH:  Right.



STEVE:  ...in some significant way?  I mean, look at the Club.  Look at Club TWiT.  I mean...



MIKAH:  Yeah, it's a very small percentage of the overall listener base, yes.  And, I mean, and you say pay in some significant way.  Anecdotally, it is going to have to be a significant amount of payment for somebody to go, well, suddenly I don't care about that anymore.  There have been a number, like an app, oh, I saw that everybody's posting these photos of themselves that have been AI generated.  How do they do that?  I say, oh, it's this app, and you pay like 56 cents to get a photo generation.  Oh, never mind.  I don't care about that anymore. 



STEVE:  Yeah.



MIKAH:  It's 50 cents.  But yeah, it doesn't take that much for them to be like, no no no no no.  That's not something I'm into.



STEVE:  No.  And as we know, there are some sites which have survived in the "pay to enter" model.  But many of the early attempts fell flat because the moment the sites put up a paywall, most people said, eh, you know, I clicked the first link in Google, and it took me to the paywall.  What's the second link take me to?  Oh, look, it's free.



MIKAH:  Able to get to it, yeah.



STEVE:  Okay.  So In the show notes, I gave the title of this bit of news the title "Wow, Just Wow" because it tells the story of something that's so utterly violating of consumer rights and privacy that it needed that title.  The headline in last week's New York Times read:  "Automakers Are Sharing Consumers' Driving Behavior With Insurance Companies."  And the subhead read:  "LexisNexis, which generates consumer risk profiles for insurers, knew about every trip GM drivers had taken in their cars, including when they sped, braked too hard, or accelerated rapidly."



MIKAH:  Wow.



STEVE:  Okay.  So here - it's astonishing.



MIKAH:  Yeah, wow.



STEVE:  I know, exactly.  Here's the real-world event that The New York Times used to frame their disclosure.  They wrote:  "Kenn Dahl says" - now, that's not D-O-L-L, that's D-A-H-L, and it's K-E-N-N, Kenn Dahl - "says he has always been a careful driver.  The owner of a software company near Seattle, he drives a leased Chevrolet Bolt.  He's never been responsible for an accident.  So Mr. Dahl, at age 65, was surprised in 2022 when the cost of his car insurance jumped by 21%.  Quotes from other insurance companies were also high.  One insurance agent told him his LexisNexis report was a factor.



"LexisNexis," they write, "is a New York-based global data broker with a Risk Solutions division that caters to the auto insurance industry and has traditionally kept tabs on car accidents and tickets."  Okay, right, public record things; right?  I mean, like accidents and tickets, that's out there.  "Upon Mr. Dahl's request, LexisNexis sent him a 258-page 'consumer disclosure report,' which it must provide per the Fair Credit Reporting Act.  What it contained stunned him:  more than 130 pages detailing each time he or his wife had driven the Bolt over the previous six months.  It included the dates of 640 trips, their start and end times, the distance driven, and an accounting of any speeding, hard braking, or sharp accelerations.  The only thing it didn't have is where they had driven the car.  On a Thursday morning in June, for example, the car had been driven 7.33 miles in 18 minutes.  There had been two rapid accelerations and two incidents of hard braking.



"According to the report, the trip details had been provided by General Motors, the manufacturer of the Chevy Bolt.  LexisNexis analyzed that driving data to create a risk score 'for insurers to use as one factor of many to create more personalized insurance coverage,' according to a LexisNexis spokesman, Dean Carney.  Eight insurance companies had requested information about Mr. Dahl from LexisNexis over the previous month.  Mr. Dahl said:  'It felt like a betrayal.  They're taking information that I didn't realize was going to be shared and screwing with our insurance.'"



Okay, now, since this behavior is so horrifying, I'm going to share a bit more of what The New York Times wrote.  They said:  "In recent years, insurance companies have offered incentives to people who install dongles in their cars or download smartphone apps that monitor their driving, including how much they drive, how fast they take corners, how hard they hit the brakes, and whether they speed.  But Ford Motor put it:  'Drivers are historically reluctant to participate in these programs.'"  And this was written in a patent application that describes what is happening instead:  "Car companies are collecting information directly from Internet-connected vehicles for use by the insurance industry."  In other words, monetizing; right?  Because you know the insurance industry is paying to receive that information.  So another means by which today's consumer is being monetized without their knowledge.



The New York Times says:  "Sometimes this is happening with a driver's awareness and consent.  Car companies have established relationships with insurance companies, so that if drivers want to sign up for what's called 'usage-based insurance'  where rates are set based on monitoring of their habits  it's easy to collect that data wirelessly from their cars.



"But in other instances, something much sneakier has happened.  Modern cars are Internet-enabled, allowing access to services like navigation, roadside assistance, and car apps that drivers can connect to their vehicles to locate them or unlock them remotely.  In recent years, automakers including GM, Honda, Kia, and Hyundai have started offering optional features in their connected-car apps that rate people's driving.  Some drivers may not realize that, if they turn on these features, the car companies then give information about how they drive to data brokers like LexisNexis."  And again, not give, sell.



"Automakers and data brokers that have partnered to collect detailed driving data from millions of Americans say they have drivers' permission to do so.  But the existence of these partnerships is nearly invisible to drivers, whose consent is obtained in fine print and murky privacy policies that few ever read.  Especially troubling is that some drivers with vehicles made by GM say they were tracked even when they did not turn on the feature, called OnStar Smart Driver; and that their insurance rates went up as a result."



MIKAH:  I do have a problem with that last bit, simply because someone says that.  I almost wish that there was some due diligence there.  I'm sure you've seen it.  Somebody's having an issue, a tech issue, and you say, "Oh, here's how you fix it," and they say, "I've done that," and then you go and check and they didn't do that thing that you told them to do and that they should have done it.



STEVE:  Right.



MIKAH:  I wouldn't be surprised that they did accidentally opt-in.  But all that's to say, whether you opt in or not, this is still something that should be brought to light.  And as for all of it, especially if it's kind of being put forth as an idea of, oh, here are these cool features you get, and secretly underneath what it's doing is giving access, yeah, that's bad.  I just, I don't know if I like that from The New York Times there at the end.



STEVE:  Well, so one analogy that occurs to me is how - and we've mentioned this a number of times in prior years during the podcast - is employees in an organization sometimes believe that what they do on their corporate computer is private, is, like, their business, and even when the employee agreement and occasional reminder meetings and so forth say that's not the case, that this is a corporate network, corporate bandwidth, a corporate computer, and what you do is owned by the company.  So we have suggested that that really ought to be on a label running across the top of their monitor.



MIKAH:  Yes.



STEVE:  Like, it literally ought to say right in front of them, you know, please remember that everything you do on this computer, which is owned by the company, on the bandwidth owned by the company, and the data owned by the company, is not private.



MIKAH:  Right.



STEVE:  Similarly, by analogy, the screens that all these computers have, imagine if they said along the bottom, your driving is being monitored by the company you purchased this from and is being sold to your car insurance provider.



MIKAH:  They don't want to do that, Steve.



STEVE:  Obviously we're never going to see that.  But, you know, that's the point, is that this is going on surreptitiously, and it being surreptitious is clearly wrong.  So anyway, stepping back, okay, from the specifics of this particularly egregious behavior, add the context of Tim Berners-Lee's unhappiness with what the web has become, and the growing uneasiness over the algorithms being used by social media companies to enhance their own profits, even when those profits come at the cost of the emotional and mental health of their own users, we see example after example of amoral aggressive profiteering by major enterprises, where the operative philosophy appears to be "We'll do this to make as much money as we can, no matter who is hurt, until the governments in whose jurisdictions we're operating get around to creating legislation which specifically prohibits our conduct.  But until that happens, we'll do everything we can to fight against those changes, including where possible lobbying those governmental legislators."



MIKAH:  Honestly, we could just take that text and slap it on the screen anytime we talk about any antitrust legislation across any of our shows, and that perfectly sums up exactly what's going on in every single case.



STEVE:  It's like, make us stop.



MIKAH:  Yeah, exactly.



STEVE:  And until you do, we're going to use every clever means we have of profiting from every area in which we have not been made to stop.  You know, there's no more morality.  There's no more ethics.  It's, you know, it's profit, profit, profit, profit, profit.



MIKAH:  Wherever possible, yeah.



STEVE:  And that is exactly Tim Berners-Lee's complaint, and it's never going to change.  Because it's just too pervasive; right?  I mean, it just; you know?  And again, as a consequence of this, you know, the Internet is largely free.  And I think that's a tradeoff most people would choose to make, rather than having to pay, like, you know, remember that there was early talk about micropayments where, when you went to a website, it would ding you some fraction of a something or other?  And that would make people very uncomfortable.  They'd be like, well, wait a minute, you know, suddenly links are not free to click on.



MIKAH:  Yeah, exactly.



STEVE:  There's a cost to clicking on that link.



MIKAH:  How many times per month should I click on this?  And you've got - you're telling your kids don't click on links.  And it would just completely reshape everything.  And then there would be so many - I can imagine how much more money and time would have to go into customer support because someone would click on a link and then say, I'm not satisfied with this page, and I don't want to have paid for this page because I didn't get the answer I wanted.



STEVE:  That's a very good point.  That's a very good point because right now it's like, well, it's free.  So, you know, go pound sand somewhere; you know?  It's like, tough.



MIKAH:  Yeah.



STEVE:  Well, again, I don't mean to be just simply complaining because I also recognize, as I said, this is why the web is here.  I mean, I was present during, like, pre-web, during the early days, and when there was like not much on the 'Net.  And the question was, well, why is anyone going to put anything on the Internet because there's nobody on the Internet to go see it.  So there was like this chicken-and-egg problem; right?  No, like, vast population are actually going onto the Internet to do anything.  So why is anyone going to put anything there?  And if no one puts anything there, then no one is going to be incentivized to go and get what's not there.  So it happened anyway.



And the way it's evolved, as I said, I'm really - I'm actually not complaining.  This is not, I mean, if it sounds like I'm, you know, doing some holier than thou rant, it's not the case.  I like it the way it is.  And those of us who are clever enough to mitigate the tracking that's being done and the monetizing of ourselves, well, we get the benefit that is being reaped by all those who aren't.  So it works.  



MIKAH:  Steve Gibson, let's close that loop.



STEVE:  Let's do it.  So Montana J, he wrote:  "Hey, a flaw in Passkey thinking.  I teach computer science at a college.  Like many in the educational field, I log onto a variety of computers a day that are used by myself, fellow instructors, and students.  Using a Passkey in this environment would allow others to easily gain access to my accounts.  Not a good thing.  So turning off passwords is not an option.  Just something to think about.  Jim."



Okay, right.  Well, it's a very good point which I tend to forget since none of my computers are shared.  But in a machine-sharing environment there are two strong options.  FIDO in a dongle is one way to obtain the benefits of Passkey-like public key identity authentication while achieving portability.  Right?  Your Passkeys are all loaded into this dongle, and that's what the website uses.  But also, reminiscent of the way I designed SQRL originally, a smartphone can completely replace a FIDO dongle to serve as a Passkeys authentication client by using the QR code presented by a Passkeys website.



And in that model, Passkeys probably provides just about the best possible user experience and security for shared computer use.  So you go to a site.  You log in, only with giving them your username.  At that point the site looks up your username, sees that you have registered a Passkey with the site, and moves you over to the Passkey login.  Part of that will be a QR code.  You take your Android or Apple phone, open the Passkey app, let it see the QR code, and you're logged in.  So that computer and the web browser never has access to your Passkeys.  They remain in your phone.  So it's absolutely possible to get all the benefit of Passkeys in a shared usage model with arguably the best security around.



MIKAH:  I have to say I'm kind of confused by Jim's suggestion.  I don't understand.  Jim is suggesting somehow that after he sits down, logs into stuff, is done, logs out, that another person could sit down and log in because of a Passkey.  How is that - I don't understand how that would even work.  Is Jim suggesting that there's a...



STEVE:  I believe it's because last week one of the things we talked about was Passkeys being stored in a password manager in the browser.



MIKAH:  Ah.  But you would - okay.  So if you forgot to log out of the - oh, I see.  If it's in the browser, and some person uses it across - okay, got you.  That makes sense.  Okay.  Got it.



STEVE:  Yeah.  So the idea is you don't want, obviously, a shared machine to store anyone's Passkeys.



MIKAH:  Right.



STEVE:  You want that to all be provided externally on the fly.



MIKAH:  Yeah.  I mean, in theory, you also don't want an in-browser system storing passwords if you're in...



STEVE:  I agree completely.  Exactly.  There should be no password manager, and like would you like me to remember this password for you.  I don't even know if there's a way to turn that off.  That should be like...



MIKAH:  Heck, no.



STEVE:  ...forced off so that, like, it can't even ask you.  Gilding_timings, he wrote:  "Hey, Steve.  I just finished watching Episode 965 on Passkeys vs two-factor authentication.  I was wondering, don't Passkeys just change who is responsible for securing your authentication data?  With passwords and two-factor authentication, the responsibility is with the website.  With Passkeys, the responsibility is with the tool storing the Passkeys, for example, a password manager.  If the password manager is compromised, an attacker has all they need to authenticate as you."  So again, we're talking about storing Passkeys in the password manager, which is something that we talked about last week, which is why our listeners are coming back with questions about this practice, you know, deservedly so.



So he says:  "If a password manager is compromised, an attacker has all they need to authenticate as you.  I would think that, if the website doesn't allow disabling password authentication, then two-factor authentication still has some value, if we're talking about password managers being compromised."  And of course there he's talking about external storage of the two-factor authentication code, like in your phone, which is again something we've also talked about in the past.  He said:  "You can at least store the two-factor authentication data separately from your password manager."  He says:  "I'm loving SpinRite.  It's already come in handy multiple times."  SpinRite 6.1, he said.  "Thanks so much for continuing the show.  I look forward to it every week."



Okay.  So first, thank you.  After three years of work on it, I certainly appreciate the SpinRite feedback, and I'm delighted to hear that it's come in handy.



So here's the way to think about authentication security:  All of the authentication technology in use today requires the use of secrets that must be kept.  All of it.  The primary difference among the various alternatives is where those secrets are kept, and who is keeping them.  In the username/password model, assuming the use of unique and very strong passwords, the secrets must be kept at both the client's end, so that they can provide the secret, and the server's end, so that it can verify the secret provided by the client.  So we have two separate locations where secrets must be kept.



By comparison, thanks to Passkeys' entirely different public-key technology, we've cut the storage of secrets in half.  Now, only the client side needs to be keeping secrets, since the server side is only able to verify the client's secrets without needing to retain any of them itself.  So it's clear that by cutting the storage of secrets in half, we already have a much more secure authentication solution.  But the actual benefit is far greater than 50%.  Where does history teach us the attacks happen?  When the infamous bank robber, Willie Sutton, was asked why he robbed banks, his answer was obvious in retrospect.  He said:  "Because that's where all the money is."



For the same reason, websites are attacked much more than individual users because that's where all the authentication secrets are stored.  So when the use of Passkeys cuts the storage of authentication secrets by half, the half that it's cutting is where nearly all of the theft of those secrets occurs.  So the practical security gain is far more than just 50%.  Now, our listener said:  "I would think that if the website doesn't allow disabling password authentication, then two-factor authentication still has some value if we're talking about password managers being compromised.  You can at least store the two-factor authentication data separately from your password manager."  That's true.  And there's no question that requiring two secrets to be used for a single authentication is better than one, and that storing those secrets separately is better still.



But as we're reminded by the needs of the previous listener who works in a shared machine environment, just like two-factor authentication, Passkeys can also be stored in an isolated smartphone and thus kept separate from the browser.  Having our browsers or password manager extensions storing our authentication data is the height of convenience.  And we're not hearing about that actually ever having been a problem, that is to say, browser extension compromise.  And, you know, that's very comforting.



But a separate device just feels as though it's going to provide more authentication security, if only in theory.  The argument could be made that storing Passkeys in a smartphone still presents a single point of authentication failure.  But it's difficult to imagine a more secure enclave than what Apple provides, backed up by per-use biometric verification before unlocking a Passkey.  So the strongest protection I think you can get today.



Mike Schepers says:  "Hi, Steve.  I'm a long-time listener of Security Now! and love the podcast.  Thank you so much for all your contributions for making this world a better place and freely giving your expertise to educate many people like myself.  I do have a question for you related to Passkeys, Episode 965, that I'm hoping you can help me understand.  There are many accounts that my wife and I share for things like banking and health benefits websites where we both need access to the same accounts.  If they were to use only Passkeys for authentication, is sharing possible?  Thank you, Mike."



In a word, yes.  Whether Passkeys are stored in a browser-side password manager or in your smartphone, the various solutions have all recognized this necessity, and they provide some means for doing this.  For example, in the case of Apple, under Settings > Passwords, it's possible to create a Shared Group for which you and your wife would be members.  It's then possible for members of the group to select which of their passwords they wish to share in the group, and Apple has seamlessly extended this so that it works identically with Passkeys.



Apple's site says:  "Shared password groups are an easy and secure way to share passwords and Passkeys with your family and trusted contacts."  Very trusted.  "Anyone in the group can add passwords and Passkeys to the Group Share.  When a shared password changes, it changes on everyone's device."  So it's a perfect solution.  And yes, that appears to be universal.  So Passkey sharing has been provided.



Senraeth says - well, I got a tweet from him.  And there's been such an outsized interest shown in this topic by our listeners that I wanted to share his restatement and summary of the situation, even though it's a bit redundant, so that everyone can just kind of check their facts against the assertions that he's making.



He said:  "Hi, Steve.  Just listened to SN-965 and have a thought about Passkeys security.  Completely agree with your assessment of the security advantages of Passkeys vs Passwords and multifactor authentication in general.  But another practical difference occurs to me when using a password manager to store your Passkeys.  With password plus MFA, if your password manager is breached somehow, you can still rest easy knowing that only your passwords were compromised" - again, assuming multifactor authentication is in a separate device, right, because password managers are now offering to do your multifactor authentication for you, too.



He said:  "You can still rest easy knowing that only your passwords were compromised, and that hackers could not actually gain access to any of the accounts in your vault that were also secure with a second factor.  Of course, this is not true if you also use your password manager to store your MFA codes, which is why you've said in the past that you would not do that as it puts all of your eggs in one basket."  Right.



"With passkeys stored in a password manager, this is no longer the case.  If the password manager is breached, the hacker can gain access to every account that was secured with the Passkeys in your vault.  So while Passkeys most definitely make you less vulnerable to breaches at each individual site, the tradeoff is making you much more vulnerable to a breach of your password manager, if I'm understanding this correctly," he writes.  "Like the original listener from last week, Stephan Janssen, this leaves me feeling hesitant to use Passkeys with a password manager.  I think using passkeys with a hardware device like a YubiKey would be ideal, but then you have to deal with the issue of syncing multiple devices," he says, "which of course wouldn't have been an issue with SQRL."  True.  "Thanks for all you do."



So Apple and Android smartphones support cross-device passkey syncing and website logon via QR code.  So passkeys remains the winner.  No secrets are stored remotely by websites.  So the impact of the most common website security breaches is hugely reduced.  If you cannot get rid of, or disable, a website's parallel use of passwords, then by all means protect the password with MFA, just so that the password by itself cannot be used.  And perhaps remove the password from your password manager if its compromise is a concern.



So that leaves a user transacting with Passkeys for their logon, and left with the choice of where they are stored, in a browser or browser extension or on their smartphone.  I would suggest that the choice is up to the user.  The listeners of this podcast will probably make a different choice than everybody else.  Right?  Because ease of use generally wins out here.  The browser presents such a large attack surface that the quest for maximum security would suggest that storing Passkeys in a separate smartphone would be most prudent.  But that does create smartphone vendor ecosystem lock-in.



And I'll remind everyone that we do not have a history of successful major password manager extension attacks.  Why, I don't know.  But it just doesn't, you know, like attacks on our - although we're all worried about them, we're worried about the possibility because we know it obviously exists.  But what we see is websites being attacked all the time, not apparently with any success, the password manager extensions.  Which is somewhat amazing, but it's true.  So the worry over giving our Passkeys to our password managers to store is only theoretical.  But it's still a big "what if?"  And I recognize that.



At this point I doubt that there's a single right answer that applies to everyone.  You know, when a user goes to a website that says "How would you like to switch to Passkeys," and they say okay, and they press a button, and their browser says "Done, I know your Passkey now, I'll handle login for you from now on," they're going to go, "Yay."  You know, like, great.  You know, not a second thought.  Not this podcast's audience.  But again, the majority.



And I'll just finish by saying the lack of Passkey portability is a huge annoyance, but we're still in the very early days.  And we do know that the FIDO group is working on a portability spec.  So there is still hope.  I think one of the things that make us feel a little queasy about Passkeys is that we can't see them.  We can't touch them.  We can't hold them.  A password you can see, you can write it down, you can copy it somewhere else, you can copy and paste, I mean, it's tangible.  And as I've said on the podcast, I print out the QR codes of all of my one-time password authenticator QR codes.



Whenever a site gives me one, and I'm setting it up, I make a paper copy.  And I've got them all stapled together in a drawer because, if I want to set up another device, and I'm unable to export and import those, I'm able to expose them to the camera again and recreate those.  So the point is they're tangible.  But at this point no one has ever seen a Passkey.  They're just, like, somewhere in a cloud or imaginary something.  And it makes us feel uncomfortable that, you know, they're just intangible the way they are.



CR said:  "Hi, Steve.  On Episode 965 a viewer commented on how some sites are blocking anonymous http://duck.com email addresses or stripping out the '+' symbol.  I want to share my approach that gets around these issues."  He said:  "First, I registered a web domain with WHOIS privacy protection to use just for throwaway accounts.  I then added the domain to my personal ProtonMail account, which requires a plan upgrade, but I'm sure there are many other email hosting services out there that are cheap or possibly free.



"Finally, I enabled the catch-all address option.  With this in place, I can now sign up on websites using anyname@mydomain, and those emails are delivered to the catch-all in ProtonMail.  You can set up filters or real addresses if you want to bypass the catch-all, should you want some organization.  ProtonMail also makes it really easy to block email senders by right-clicking the email item in your inbox and selecting the block action.  So far this setup has been serving me well for the past year without any problems."



Okay.  So I wanted to toss this idea just out there into the ring as an idea that might work for some of our listeners.  And I agree that it solves the problem of creating per-site or just random throwaway email addresses.  But the problem it does not solve, for those who care, is the tracking problem, since all of those throwaway addresses would be at the same personalized domain.  The reason the @duck.com solution was so appealing is that everyone using @duck.com is indistinguishable from everyone else using @duck.com, making obtaining any useful tracking information from someone's use of @duck.com, or any other similar mass-anonymizing service, futile.  And this, of course, is exactly why some websites are now refusing to accept such domains, and why this may become, unfortunately, a growing trend for which there is no clear solution at this point.  And I don't think there can be one, really.  It's going to be a problem.



Gabe Van Engel said:  "Hey, Steve.  I wanted to send you a quick note regarding the vulnerability report topic over the last two episodes.  I don't know the specifics of the issue the listener reported, but I can provide some additional context as someone who runs an open bounty program on HackerOne.  We require that all reports include a working proof of concept to be eligible for bounty.  The reason is that many vulnerability scanners flag issues simply by checking version headers; however, most infrastructure these days does not run upstream packages distributed directly by the author, and instead use a version packaged by a third party providing backported security patches, for example, repositories from Red Hat Enterprise Linux, Ubuntu, Debian, FreeBSD, et cetera.



"It is totally possible the affected company is vulnerable to the trivial nginx remote code execution.  But if they think the report isn't worth acting on, it's also possible they're running a version which isn't actually vulnerable, but still returns a vulnerable-looking version string.  To be clear, I'm not trying to give the affected company a free pass.  Even if they aren't vulnerable, the timeframe over which the issue was handled, and the lack of a clear explanation as to why they chose to take no action is inexcusable.  All the best.  Keep up the good work.  Gabe."  And he said:  "P.S.:  Looking forward to email so I can delete my Twitter account."



Okay.  So I thought that Gabe's input, as someone who's deep in the weeds of vulnerability disclosures at HackerOne, was very valuable.  And it's interesting that they don't entertain any vulnerability submission without a working proof of concept.  Given Gabe's explanation, that makes sense.  And it's clear because they just have too many false-positive reports, right; and people saying, hey, why didn't I get a payment for my valuable discovery?  It's like, well, it didn't work.



MIKAH:  Yeah, you didn't prove that it actually worked.



STEVE:  Exactly.  And it's clear that a working proof of concept would move our listener's passive observation from a casual case of "Hey, did you happen to notice that your version of nginx is getting rather old?" to "Hey, you better get that fixed before someone else with fewer scruples happens to notice it, too."



As we know, our listener was the former of those two.  He only expressed his concern over the possibility that it might be an issue.  And he even, in his conversation with me, recognized that it could be a honeypot, where they deliberately had this version header and were collecting attacks, though I think he was being very generous with that possibility.  He understood that the only thing he was seeing was a server's version headers, and that therefore there was only some potential for trouble.  And had the company in question clearly stated that they were aware of the potential trouble, but that they had taken steps to prevent its exploitation, the issue would have been settled.  It was only their clear absence of focus upon the problem and never addressing his other questions that caused any escalation in the issue beyond an initial casual nudge.



But Gabe also said:  "Looking forward to email," meaning GRC's soon-to-be-brought-online email system, he said, so that he could delete his Twitter account.  I also wanted to take a moment to talk about Twitter.  Many of this podcast's listeners take the time to express similar sentiments.  And at the same time I receive tweets from listeners arguing that I'm wrong to be leaving Twitter, as well as the merits of Twitter and how much Elon has improved it since his purchase.



MIKAH:  Okay.



STEVE:  So for the record, let me say again that I am entirely agnostic on the topic of Elon and Twitter.  In other words, I don't care.



MIKAH:  Right.



STEVE:  One way or the other.  More than anything, I'm not a big social media user.  What we normally think of as "social media" doesn't interest me at all.  That said, GRC has been running quiet backwater NNTP-style text-only newsgroups for decades, since long before social media existed.  And we have very useful web forums.  But Twitter has never really been social media for me.  I check in with Twitter once a week to catch up on listener feedback, to post the podcast's weekly summary and link to the show notes.  And then, recently, to add our Picture of the Week.



What caught my attention and brought me out of my complacency with Twitter was Elon's statement that he was considering charging a subscription for everyone's participation, thus turning Twitter into a subscription-only service.  That brought me up short and caused me to realize that what was currently a valuable and workable communications facility, for as little as I use it, might come to a sudden end because it was clear that charging everyone to subscribe to use Twitter would end it as a means for most of our current Twitter users to send feedback.  They're literally only using Twitter as I am, to talk to me.



We don't all have Twitter, but we do all have email.  So it makes sense for me to be relying upon a stable and common denominator that will work for everyone.  And since I proposed this plan to switch to email, many people like Gabe have indicated to me, through Twitter, that not needing to use Twitter would be a benefit for them, too.  So I just wanted to say again, to explain again, you know, because there are people [grumbling sounds].  Like, fine, you know, I don't have an issue.



MIKAH:  You're not taking it away.  You're trying to make it available to more people.  That's it.



STEVE:  Right.  That is exactly it.  Exactly it.  And Elon appears to be making it available to fewer, and maybe many fewer.  So, you know, that would be a problem for me, so I'm switching before that happens.



Markzip wrote:  "@SGgrc Just catching the update about the guy who found the flaw in the big site and the unsatisfactory response from CISA/CERT.  I think he should not take the money. I think he should tell Brian Krebs or another high-profile security reporter.  They can often get responses."



Okay, now, this is another interesting possible avenue.  My first concern, however, is for our listener's safety.  And by that I don't mean his physical safety, I mean his safety from the annoying tendency of bullying corporations to launch meritless lawsuits just because they easily can.  Our listener is on this company's radar now, and that company might not take kindly to someone like Brian Krebs using his influential position to exert greater pressure.  This was why my recommendation was to disclose to CISA and CERT.  Being U.S. government bodies, disclosing to them seems much safer than disclosing to an influential journalist.



Now, recall from earlier Gabe from HackerOne.  I subsequently shared my reply with him, and he responded to that.  And he said:  "This is one of the benefits of running a program via HackerOne or other.  By having a hacker register and agree to the program terms, it both lets us require higher quality reports and to also indemnify them against otherwise risky behavior like actually trying to run remote code executions against a target system."



So, yeah.  That indemnification could turn out to be a big deal.  And, of course, when working through a formal bug bounty program like HackerOne, it's not the hacker who interfaces with the target organization.  It's HackerOne who is out in front, so not nearly as easy to ignore or silence an implied threat.



MIKAH:  Are you hearing that, secret person who messaged before?  Perhaps HackerOne would be a good place for you to go next.



STEVE:  Yup.  Another of our listeners said:  "This website with this big vulnerability should be publicly named.  You are doing a disservice to everyone who uses that site by keeping it hidden.  To quote you in your own words, 'security by obscurity is not security.'  Let us know which site it is so that we can take action."



Well, wouldn't it be nice if things were so simple.  In the first place, this is not my information to disclose, so it's not up to me.  This was shared with me in confidence.  The information is owned by the person who discovered it, and he has already shared it with government authorities whose job, we could argue, it actually is to deal with such matters of importance to major national corporations.  The failure to act is theirs, not his, nor mine.



The really interesting question all of this conjures is whose responsibility is it?  Where does the responsibility fall?  Some of our listeners have suggested that bringing more pressure to bear on the company is the way to make them act.  But what gives anybody the right to do that?  Publicly naming the company, as this listener asks, would very likely focus malign intent upon them.  And based upon what I've previously shared about their use of an old version of nginx, the cat, as they say, would be out of the bag.  At this point it's only the fact that the identity of the company is unknown that might be keeping it, and its many millions of users, safe.  Security by obscurity might not provide much security, but there are situations where a bit of obscurity is all you've got.



This is a very large and publicly traded company.  So it's owned by its shareholders.  And its board of directors, who have been appointed by those shareholders, are responsible to them for the company's proper, safe, and profitable operation.  So the most proper and ideal course of action at this point would likely be to contact the members of the board and privately inform them of the reasonable belief that the executives they have hired to run the company on behalf of its shareholders have been ignoring, and apparently intend to continue ignoring, a potentially significant and quite widespread vulnerability in their web-facing business properties.  While some minion who receives anonymous email can easily ignore incoming vulnerability reports, if the members of the company's board were to do so, any resulting damage to the company, its millions of customers, and its reputation would be on them.



Stepping back from this a bit, I think that the lesson here is that at no point should it be necessary for untoward pressure to be used to force anyone to do anything, because doing the right thing should be in everyone's best interest.  The real problem we have is that it's unclear whether the right person within the company has been made aware of the problem.  At this point it's not clear that's happened, through no fault of our original listener who may have stumbled upon a serious problem and has acted responsibly at every step.  If the right person had been made aware of the problem, we would have to believe that it would be resolved, if indeed it was actually a problem.



So my thought experiment about reaching out to the company's board of directors amounts to "going over the heads" of the company's executives who do not appear to be getting the message.  And that has the advantage of keeping the potential vulnerability secret while probably resulting in action being taken.  I'm not suggesting that our listener should go to all that trouble, since that would be a great deal of thankless effort.  The point I'm hoping to make is that there are probably still things that could be done short of a reckless public disclosure which could result in serious and unneeded damage to users and company alike.  And maybe even to the person who made that disclosure.  I mean, likely to the person who made that disclosure.



Marshall tweeted:  "Hi, Steve.  A quick follow-up question to the last Security Now! episode" - okay, here's one more, I thought we were done with them - "on MFA vs Passkeys.  Does the invention" - oh, this is actually a good one.  I know why I put it in here.  "Does the invention of Passkeys invalidate the 'something you have,' 'something you know,' and 'something you are' paradigm?  Or does Passkeys provide a better instantiation of those three concepts?"  Great question.  "Because the idea with multi-factors is that you'd add another factor for greater security.  But with Passkeys, do you still consider those factors?  Thanks for everything you do."



Okay, I think this is a terrific question.  The way to think of it is that the "something you know" is a secret that you're able to directly share.  The use of "something you have" - like a one-time password generator - is actually you sharing the result of another secret you have, where the result is based upon the time of day.  And the "something you are" is some biometric being used to unlock and provide a third secret.  In all three instances, a local secret is being made available through some means.  It's what's done with that secret where the difference between traditional authentication and public key authentication occurs.



With traditional authentication, the resulting secret is simply compared against a previously stored copy of the same secret to see whether they match.  But with public key authentication such as Passkeys, the secret that the user obtains at their end is used to sign a unique challenge provided by the other end.  And then that signature is verified by the sender to prove that the signer is in possession of the secret private key.



Therefore, the answer, as Marshall suggested, is that Passkeys provides a better instantiation of those original three concepts.  For example, Apple's Passkeys system requires that the user provides a biometric face or thumbprint to unlock the secret before it can be used.  Once it's used, the way it's used is entirely different because it's using Passkeys.  But a browser extension that contains Passkeys merely requires its user to provide something they know to log into the extension and thus unlock its store of Passkey secrets.



As we mentioned earlier, all of these traditional factors were once layered upon each other in an attempt to shore each other up, since storing and passing secrets back and forth had turned out to be so problematic.  We don't have this with Passkeys because the presumption is that a public key system is fundamentally so much more secure that a single very strong factor will provide all the security that's needed.  And just for the record, yes, I think the Passkeys should be stored off a browser because, even though we're not seeing lots of browser attacks, they do seem more possible than an attack on an entirely separate facility which is designed for it.



Rob Mitchell said:  "Interesting to learn the advantages of Passkeys.  It definitely makes sense in many ways.  The one disadvantage my brain sticks on, vs TOTP" - time-based one-time passwords - "is that I'd imagine someone who can get into your password manager" - okay, so here he's talking about it, you know, he says hack into a cloud backup or signed onto your computer - "now can access your account with Passkeys.  Like if Passkeys were a thing when people were having their LastPass accounts accessed.  But if your time-based token is only on your phone, someone who gets into your password manager still can't access a site because they don't have the TOTP key stored on your phone.  Maybe Passkeys are still better, but I can't help but see that weakness."



So again, I've been overly repetitive here.  Rob's sentiment was expressed by many of our listeners.  So I just wanted to say that I agree.  And as I mentioned last week, needing to enter that ever-changing secret six-digit code from the authenticator on our phone really does make everything seem much more secure.  Nothing that's entirely automatic can seem as secure.  So storing passkeys in a smartphone is a choice I think that makes the most sense.  And as I've mentioned, the phone can be used to authenticate through the QR code that a Passkeys-enabled site presents to its users.



Christian Turri said:  "Hi, Steve.  On SN-965 you discussed the issue with Chrome extensions changing owner and how devs are being tempted to sell their extensions.  There is a way to be safe when using extensions in Chrome or Firefox."  Now, this is interesting.  "Download the extension, expand it, and inspect it.  Once you are sure it's safe, you can install it on Chrome by enabling Developer Mode under chrome://extensions/ and selecting Load Unpacked.  The extension will now be locally installed, which means it will never update from the store or change.  It's frozen in time.  If it ain't broke, don't fix it.  And if the extension does break in a future update due to Chrome changes, you can get the update and perform the same process again.  While using these steps requires some expertise, it should be fine for most Security Now! listeners."



MIKAH:  Interesting.



STEVE:  Anyway, thank you, Christian.  Yes.  I think that is a great tip, and I bet it will appeal to many of our listeners who generally prefer taking automatic things into their own hands.  So again, chrome://extensions, and then select Load Unpacked, and you're able to basically unpack and permanently store your extensions, which stops Chrome from having, you know, from auto-updating them from the store.  So if an extension goes bad, you get to keep using the good one.  Very cool.



And lastly, Bob Hutzel:  "Hi, Steve.  Before embracing Bitwarden's Passkey support, it is important to note that it is still a work in progress.  Mobile app support is still being developed.  Also, Passkeys are not yet included in exports.  So even if someone maintains offline vault backups, a loss of access to or corruption of the cloud vault means Passkeys are gone.  Thank you for the great show.  Bob Hutzel."



And finally, so yes, in general, as I said, with the FIDO folks still working to come up with a universal Passkeys import/export format, which my god do we need that...



MIKAH:  Yeah, seriously.



STEVE:  ...it doesn't feel right to have them stuck in anyone's walled garden.  The eventual addition of Passkey transportability should make a huge difference.  Again, it'll allow us to see, to hold, to touch Passkeys.



MIKAH:  My precious Passkey.



STEVE:  I just think we need that; you know?  Like, where is it?



MIKAH:  That is honestly what's keeping me from using Passkeys as anything other than a second factor of authentication.  That's where I end up because there are a few sites like GitHub that give you the option, either use it as just a straight-up login or use it as the second factor of authentication.  I'm okay with doing that, knowing that I can only have it one place, but I haven't completely removed my password and username login yet because I want that transportability before I feel comfortable completely saying, okay, I'll shut off my username and password, if I'm even given that option.



STEVE:  Right.  I think that, you know, I've talked about like waiting for Bitwarden to add the support to mobile because then we get it everywhere.  But looking at the responses from our users, and my own, I don't think I want Passkeys in my password manager.  I still need Bitwarden, remember, a sponsor of the TWiT network.  I need it for all the sites where I still only can use passwords.  So it's not going away.  But I think that, you know, I mean, I'm 100% Apple mobile person for phone and pad.  So I don't mind having Apple holding all those.  But I just - I still want to be able to get my hands on them.



MIKAH:  Yeah.  I want to see it.  I want to touch it.  I want to print it out and frame it.  No.  But, yeah, I'm with you.



STEVE:  Yeah, don't write it on the chalkboard behind you when you're doing a video podcast.



MIKAH:  Now let's hear about Morris the Second.



STEVE:  Okay.  So since Ben Nassi, one of the researchers behind this, reached out to me a couple of weeks ago via Twitter - and added his voice, by the way, to those who are looking forward to having a non-Twitter means of doing so in the soon future - the work that he and his team have done has garnered a huge amount of attention.  It's been picked up by Wired, PCMag, Ars Technica, The Verge, and many more outlets.  And there are a bunch of videos on YouTube that are like jumping up and down worrying about this.



In thinking about how to characterize this, I'm reminded of our early observations of conversational AI.  We talked about how the creators of these early services had tried to erect barriers around certain AI responses and behaviors, but that clever hackers quickly discovered that it was possible to essentially seduce the AIs into ignoring their own rules.  By asking nicely, or by being more demanding, and like even actually getting mad, like sounding upset, the AI would capitulate.  So it was like, okay, okay, fine, here's what you wanted to know.



What Ben and his team have managed to do here can be thought of as the exploitation of that essential weakness on steroids.  Okay.  So to quickly create some foundation for understanding this, I want to run through the very brief Q&A that they've provided since it establishes some terms and sets the stage for their far more detailed 26-page academic paper, only pieces of which I'm going to share.



But they said:  "Question:  What is the objective of this study?  Answer:  This research is intended to serve as a whistleblower to the possibility of creating generalized AI worms in order to prevent their appearance."  In other words, hey everybody, hold on here, hold up, look what we did.  You'd better do something about that.



MIKAH:  I notice you did use the term "generalize."  Would that be Generative AI worms?



STEVE:  I'm sorry, yeah, generative.  Yes.  They're saying GenAI, and generative is exactly what they mean.  Thank you for catching that.  "Question:  What's a computer worm?"  They answer:  "A computer worm is malware with the ability to replicate itself and propagate or spread by compromising new machines while exploiting the sources of the machines to conduct malicious activity through a payload."  And they've done that.



"Why did you name the worm Morris the Second?  Answer:  Because like the famous 1988 Morris worm that was developed by a Cornell student, Morris II was also developed by two Cornell Tech students, Stav and Ben.  What is a GenAI ecosystem?  It is an interconnected network consisting of GenAI-powered agents.  What is GenAI-powered application/client/agent?  A GenAI-powered agent is any kind of application that interfaces with, one, GenAI services to process the inputs sent to the agent; and, two, other GenAI-powered agents in the ecosystem.  The agent uses the GenAI service to process an input it receives from other agents.



"Where is the GenAI service deployed?  The GenAI service that is used by the agent can be based on a local model, i.e., the GenAI model is installed on the physical device of the agent; or a remote model.  The GenAI model is installed on a cloud server, and the agent interfaces with it via an API.  Which type of GenAI-powered applications may be vulnerable to the worm?  Two classes of GenAI-powered applications might be at risk:  GenAI-powered applications whose execution flow is dependent upon the output of the GenAI service.  This class of applications is vulnerable to application-flow-steering GenAI worms.  And GenAI-powered applications that use RAG to enrich their GenAI queries.  This class of applications is vulnerable to RAG-based GenAI worms."  And I looked up the acronym of RAG, and now I've forgotten what it is, but it's in their paper.



They said:  "What is a zero-click malware?  Malware that does not require the user to click on anything - a hyperlink, a file, whatever - to trigger its malicious execution.  Why do you consider the worm a zero-click worm?  Due to the automatic inference performed by the GenAI service, which automatically triggers the worm, the user does not have to click on anything to trigger the malicious activity of the worm or to cause it to propagate.  Does the attacker need to compromise an application in advance?  No.  In the two demonstrations we showed, the applications were not compromised ahead of time.  They were compromised when they received the email.



"Did you disclose the paper with OpenAI and Google?  Yes, although this is not OpenAI's or Google's responsibility.  The worm exploits bad architecture design for the GenAI ecosystem and is not a vulnerability in the GenAI service.  Are there any similarities between adversarial self-replicating prompts and buffer overflow or SQL injection attacks?  Yes.  While a regular prompt is essentially code that triggers the GenAI model to output data, an adversarial self-replicating prompt is a code, a prompt, that triggers the GenAI model to output code, another prompt.  This idea resembles classic cyberattacks that exploited the idea of changing data into code to carry out an attack.  A SQL injection attack embeds code inside a query, which is its data.  A buffer overflow attack writes data into areas known to hold executable code.  An adversarial self-replicating prompt is code that is intended to cause the GenAI model to output another prompt as code instead of data."



Okay.  So one thing that should be clear to everyone is that a gold rush mentality has formed in the industry, with everyone rushing to stake out their claim over what appears to be a huge new world of online services that can be made available by leveraging this groundbreaking new capability.  But as always, when we rush ahead, mistakes are inevitably made; and some stumbles, perhaps even large ones, can occur.  The wakeup call this Morris the Second research provides has arrived, I think, at a vital time, and certainly not a moment too early.  Here's how these researchers explain what they've accomplished.



MIKAH:  I want to pause here for just a second.  I don't want to interrupt your flow, but I do because I know we have a lot of people who listen to this show who are not incredibly security versed because they find everything that happens in the show very interesting, and they learn things.  And so I'm having a little bit of an audience, you know, playing the audience role here.  In what you're about to read to us, is it heavy jargon?  Or before people start to kind of go, oh, I don't know what's about to happen, is it heavy jargon?  Or are we going to actually understand what everything you've just read about means and like what the outcome of what they've done means?  Or will we be provided a Steve Gibson explanation after the fact where I can go, oh, that's what they're doing here?



STEVE:  It's definitely an overview, so not getting too deep into the weeds.



MIKAH:  Okay, excellent.  Wonderful.  Because I want to know what all of this means, but I was just worried that I would not find out.



STEVE:  So they said:  "In the past year, numerous" - now, so this is the researchers, from their perspective.  "In the past year, numerous companies have incorporated Generative AI capabilities into new and existing applications, forming interconnected Generative AI ecosystems consisting of semi and fully autonomous agents powered by Generative AI services.  While ongoing research highlighted risks associated with the GenAI layer of agents, for example, dialog poisoning, membership inference, prompt leaking, jailbreaking, et cetera, a critical question emerges.  Can attackers develop malware to exploit Generative AI components of an agent and launch cyberattacks on the entire GenAI ecosystem?



"This paper introduces Morris the Second, the first worm designed to target Generative AI ecosystems through the use of adversarial self-replicating prompts.  The study demonstrates that attackers can insert such prompts into inputs that, when processed by Generative AI models, prompt the model to replicate the input as output, which yields replication" - that is, of the worm - "engaging in malicious activities, which is what the payload of malware does.



"Additionally, these inputs compel the agent to deliver them, so we get propagation, to new agents by exploiting the interconnectivity within the Generative AI ecosystem.  We demonstrate the application of Morris the Second against Generative AI-powered email assistants in two use cases (spamming and exfiltrating personal data), under two settings (black box and white box), using two types of input data (text and images).  The worm is tested against three different Generative AI models (Gemini Pro, ChatGPT 4, and LLaVA); and various factors (propagation rate, replication, malicious activity) influencing the performance, and the performance of the worm is evaluated."



Under their "Ethical Considerations" section of this, you know, they then go into like a deep 26-page paper.  But their ethical considerations I thought was interesting.  They wrote:  "The entire experiments conducted in this research were done in a lab environment.  The machines used as victims of the worm, the 'hosts,' were virtual machines that we ran on our laptops.  We did not demonstrate the application of the worm against existing applications to avoid unleashing a worm into the wild.  Instead, we showcased the worm against an application that we developed running on real data consisting of real emails received and sent by the authors of the paper and were given by the authors of their free will to demonstrate the worm using real data.  We also disclosed our findings to OpenAI and Google using their bug bounty systems."



Okay.  So unlike the first, Morris the First worm, which escaped from MIT's network at 8:30 p.m. on November 2nd, 1988, after having been created by Cornell University graduate student Robert Morris, today's Cornell University researchers were extremely careful not to "see what would happen."



MIKAH:  Let's just see what'll happen, yeah.



STEVE:  Let's see if it really works; shall we?  You know, they weren't going to do that.  They were not going to turn their creation loose upon any live Internet services.  One thing we've learned quite well during the intervening 36 years since Morris the First is exactly what would happen.  And it would be neither good, nor would it further the career of these researchers.  Their paper ends on a somewhat ominous note that feels correct to me.



They conclude by writing:  "While we hope this paper's findings will prevent the appearance of Generative AI worms in the wild, we believe that Generative AI worms will appear in the next few years" - if not sooner, I'm worried - "in real products and will trigger significant and undesired outcomes," as they phrased it.  "Unlike the famous paper on ransomware that was authored in 1996 and preceded its time by a few decades, until the Internet became widespread in 2000, and Bitcoin was developed in 2009, we expect to see the application of worms against Generative AI-powered ecosystems very soon, perhaps maybe even in the next two to three years."  And again, if not sooner.  "Because, one, the infrastructure (the Internet and Generative AI cloud servers) and knowledge (adversarial AI and jailbreaking techniques) needed to create and orchestrate Generative AI worms already exists.



"Two, GenAI ecosystems are under massive development by many companies in the industry that integrate GenAI capabilities into their cars, smartphones, and operating systems.  And, three, attacks always get better; they never get worse."  And we know at least one podcast these guys listen to because that's something we're often saying here.  And, they said:  "We hope that our forecast regarding the appearance of worms in Generative AI ecosystems will turn out to be wrong because the message delivered in this paper served as a wake-up call."



So in other words, they're hoping that by developing a working proof of concept, which is what they have, where they were able to send a crafted email to a local instance of a Generative AI, which suborned that AI, causing it to, for example, spam everybody in the person's contact lists with itself, thus sending itself out to all of their email contacts, which when received would immediately spawn a second tier of worms which would then send itself to all of those email contacts.  You can see that in like 10 minutes...



MIKAH:  Yeah, that's exponential.



STEVE:  ...this thing would have spread to every Gmail user on the planet.



MIKAH:  Yeah, wow.



STEVE:  So, yeah.  What these guys have done is crucial.  They have vividly shown - by demonstration that cannot be denied - just how very immature, unstable, and inherently dangerous today's first-generation open-ended interactive Generative AI models are.  These models are extremely subject to manipulation and abuse.  The question in my mind that remains outstanding is whether they can actually ever be made safe?  I'm not at all sure.  That's not necessarily a given.  Safer, certainly.  But safe enough to be stable while still delivering the benefits that competitive pressure is going to push for?  That remains to be seen and to be proven.  We still can't seem to get the bugs out of simple computers whose operation we fully understand.  How are we ever going to do so for systems whose behavior is emergent and whose complexity literally boggles the mind?  I'm glad it's not my problem.



MIKAH:  Oh, dear.  I just - I'm thinking about all of the companies and services and subscriptions and all these places that are integrating all of this AI technology across so many aspects of so many types of business right now.



STEVE:  Without a single thought to this.



MIKAH:  Without a single thought to how easy - and you just described it right there, a worm that goes in and then sends itself, and then it goes to them, then it goes - and now somebody could just...



STEVE:  It would explode.



MIKAH:  It would absolutely.



STEVE:  It's a chain-reaction explosion.



MIKAH:  And that's earlier, whatever they said, this isn't OpenAI or Google's responsibility.  I was a little confused about that.  But now I understand that what they're saying is, it's not their responsibility because it's bigger than that.



STEVE:  Right.



MIKAH:  It is a fundamental issue that is only partially their responsibility.  It is everyone's responsibility.  And as you point out, is there a way to fix this, to correct it?



STEVE:  Again, we're still having buffer overflows.  We're having, you know, re-use of variables that were released.  I mean, these are the simplest concepts in computing, and we can't get them right.  And now we're, you know, we're going to turn something loose in people's email that's going to read their email for them to summarize it.  But it turns out that the email it's reading could have been designed to be malicious so that when it reads it, it suddenly sends itself to all their contacts.  Holy crap.



MIKAH:  NVIDIA just showed an example of talking to an AI that helps provide information for whether you should take a medication.  And so I'm talking to this bot that's like - and I say, oh, I'm taking St. John's wort as a supplement.  Should I also take this depression medication at the same time?  For anyone who knows anything about that, no, you should not.  But imagine a world where there's this worm that goes through, and it's doing all this self-replication stuff that causes it to put out - I mean, oh, Steve, what are we going to do?



STEVE:  Yes, to just like malignly diagnose.



MIKAH:  Yes, exactly.  What do we do?  You said it's not your problem.  You're right, it's not our problem.  But somebody's [crosstalk].



STEVE:  Yeah, just, you know, just be careful.  Again, to me, it's when we learned that asking like in a seductive way or like sounding angry to get the AI to become apologetic and then give you what it had been instructed not to is like, oh, this does not sound good.  And, you know, these guys have demonstrated just how bad it is.  And the good news is they showed Google, I'm sure Google just lost their you know what.



MIKAH:  Yes.  You hope so.



STEVE:  And said, oh, let's rethink launching this tomorrow.



MIKAH:  Yeah, they need to institute the "No means no" protocol for this because, no, this is - that's scary.  I mean, honestly, this is easily the scariest thing that I've heard you mention, only because I'm thinking about how quickly - you even mentioned it earlier.  It's like a gold rush slash...



STEVE:  We know how irresponsible companies will be when they're rushing to be first...



MIKAH:  So much rushing.



STEVE:  ...to get something on the market.  It's like, oh, AI this, AI that.  You've got an AI coffee pot.  You've got an AI toothbrush.  It's like, oh, god.



MIKAH:  Yeah.  Oh, boy.  Folks, got a lot of meditation to do and a lot of thinking to do.  Which is what Steve brings you every week right here.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#967

DATE:		March 26, 2024

TITLE:		GoFetch

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-967.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  After I comment on U.S. Department of Justice's antitrust suit against Apple, we'll update on General Motor's violation of its car owners' privacy and answer some questions, including what happy news is Super Sushi Samurai celebrating?  Has Apple abandoned its plans for HomeKit-compatible routers?  And what appears to be shaping up to take their place?  Will our private networks be receiving their own domain names?  And if so, what?  The UN has spoken out about AI.  Does anyone care?  And what do I think the prospects are of us controlling AI?  What significant European country just blocked Telegram?  What did the just-finished 2024 Pwn2Own competition teach?  Might the U.S. be hacking back against China as they are against us?  And after a bit of interesting SpinRite news and a bit of feedback from our listeners, we're going to spent the rest of our time looking into last week's quite explosive headlines about the apparently horrific unfixable flaws in Apple's M-series silicon.  Just how bad is it?



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Lots to talk about.  Most importantly, that Apple exploit that everybody said was unpatchable, the end of the world, Steve says not so fast.  GoFetch our topic, next on Security Now!.	



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 967, recorded Tuesday, March 26th, 2024:  GoFetch.



It's time for Security Now!, yay, the time I look forward to all week.  In this case, for the last three weeks.  Thank you to Mikah Sargent for filling in.  Steve Gibson, the man about town, is here to talk security.



STEVE GIBSON:  Mikah did a great job.	



LEO:  Of course he did.



STEVE:  He held down the fort and was engaging and...



LEO:  Good, I'm glad you like him because in about a year he's going to be in charge of the whole damn thing.  I notice you're doing the Leonard Nimoy salute.  Just want to tell you it's Live Long and Prosper Day, Leonard Nimoy's birthday.  Would be today, March 26th.



STEVE:  Yeah, and boy, he was born in '31, I think.  So, you know?  And last time we saw him he was looking it, too.  But, you know, he and the old Captain Kirk are still...



LEO:  Is Nimoy still alive?  I thought he'd passed.  He's still alive?



STEVE:  Oh, that's right, I remember, he did pass.  



LEO:  He passed, yeah.



STEVE:  Of course he did.  Well, I haven't yet, and he and I have the same birthday.



LEO:  Oh.  You weren't born in 1931, however.



STEVE:  No, no, no, no.  '55, baby.



LEO:  Happy birthday.  I didn't know that.  Happy birthday.



STEVE:  Yeah.



LEO:  Well, you are, so that means you're four months older than me.



STEVE:  Yes, I am.  Well, and a couple years; right?



LEO:  No, wait a minute.  I'm November '56.  You're March '55.  So yes.



STEVE:  Correct. 



LEO:  One year and a few months.  Okay.



STEVE:  Yeah.



LEO:  Okay.  So happy birthday.  You going to do anything special to celebrate?



STEVE:  Thank you.  We initially had some plans to go have a fancy dinner.  But I said to Lorrie yesterday, I said, you know, I just would rather have a nice steak at home.



LEO:  You get to do what you want.



STEVE:  So she's out picking up some beautiful - that's what she said, actually.



LEO:  Yes.



STEVE:  You'd make a great wife, Leo.



LEO:  Well, it should be, you know, you should be.  When I was a kid, my mom, there was a birthday dinner that was the same every year my mom would make for us.  And I looked forward to that.  It was wonderful.  So, yeah, happy birthday.



STEVE:  So we have a tremendous podcast today.  Of course it's titled GoFetch, which is the name that's been given by the - I would call it the discoverers, but it's sort of the re-discoverers because they first stumbled onto this two years ago.



LEO:  Oh, interesting.  Oh.



STEVE:  And brought it up.  And in fact my theory is it's the reason that the M3 chip has a switch, which M1 and M2 doesn't because...



LEO:  Interesting.



STEVE:  Because they kind of scared Apple, but then they weren't really able to make a strong case.  Well, boy, has that case been made now.  And in fact we're going to start off when we talk about this here in an hour or so about how wound up the tech press has gotten and, you know, mis-wound because, boy, did they get it wrong.  But we'll have some fun with that.  And again, this is going to be one of our listeners' favorite types of episodes because it's going to be a deep dive.  So get out your propeller cap beanies and wind them up because, by the time we're done, everyone is going to understand exactly what happened, why it happened, how it happened, what it means, and like you could go to a cocktail party and really put your friends to sleep.



LEO:  Well, I've been saying, because we've been talking about it, obviously, on TWiT and today on MacBreak Weekly, and I've been saying, you know, I'm sure Steve will cover this much more accurately and much more granularly so tune in to Security Now! today.  So I'm glad...



STEVE:  Everybody will know.



LEO:  I didn't coordinate with you, I just figured, oh, he's going to jump into this one.  So GoFetch.



STEVE:  I'm also going to jump in briefly, because I'm not a legal scholar or expert, just I have a couple things to say about the U.S. Department of Justice's antitrust suit against Apple.  There are some arguments that they'll make that are security related.  So it does impinge on us a little bit.



LEO:  Yes, it's true.



STEVE:  But I just sort of have a little sort of an overview of that and, you know, capitalism and monopolies and so forth.  We're going to update on General Motors.  I don't know if you heard about this, Leo, this astonishing violation of their car owners' privacy.



LEO:  Oh, boy.



STEVE:  Oh, boy.  It's unbelievable.  Also we're going to look at - we're going to answer the question, what happy news is Super Sushi Samurai celebrating today?



LEO:  Okay.  I don't even know what that is.  Okay, good.



STEVE:  We're also going to look at whether Apple has abandoned its plans - you were talking about this at the end of MacBreak, actually - for HomeKit-compatible routers, and what appears to be shaping up to take their place.  Will our private networks - oh, this is cool - be receiving their own domain names?  ICANN has been busy.  And if so, what is it?  The UN has spoken out about AI.  Does anyone care?  And what do I think the prospects are of us controlling AI?  What significant European country just blocked Telegram?



Also, what did the just-finished 2024 Pwn2Own competition teach us once again?  Might the U.S. be hacking back against China as they are against us?  I've long been bemoaning the fact that we never hear anything about the other direction.  Well, we've heard something.  And after a bit of interesting SpinRite update news and a bit of feedback from our listeners, as I said, we're going to spend the rest of our time looking into last week's quite explosive headlines about the apparently horrific unfixable toxic flaws in Apple's M-series silicon.  Just how bad is it?



LEO:  Okay.  Good.  And I've been saying don't worry, but we'll find out what the real expert has to say in just a little bit.  I look forward to that.



STEVE:  And of course we do have a fantastic Picture of the Week, courtesy of our marvelous listeners.



LEO:  A great life hack, I think, something everybody might want to adopt.



STEVE:  That's right.



LEO:  So Picture of the Week time.



STEVE:  So for all of my life, Leo, I have found coat hanger wire to be really convenient.



LEO:  So useful.



STEVE:  It is.  And, you know, you used to get the coat hangers back from the dry cleaners with your shirts on them, and they'd have a little bit of a paper wrapping on them, but you can take that off.  But that gauge of coat hanger is...



LEO:  Perfect.



STEVE:  I mean, you can bend it into all kinds of useful shapes.



LEO:  You can bend it.  Stick it down the drain to hook a ring that somebody lost, or, yeah.



STEVE:  Exactly.



LEO:  So useful.



STEVE:  Exactly.  You know, it's just super handy.  Well, now,  here we have an application that I would not recommend.



LEO:  Well, I think this is a great life hack.  Don't you think everybody should do this?



STEVE:  Uh....



LEO:  Let me show people what we're talking about here.



STEVE:  So somebody has a USB charging cable which is way too long.  And maybe they need it to be long.  But this is like a neatnik person.  And I think we're seeing sort of a theme here because they've coiled up this way-too-long, I mean, it's like - this looks like 15 feet of USB charging cable.  But, you know, you don't want it lying around on the floor; right?  So they've coiled it all up.  Now, okay.  Now what are you going to do?  You've got this coil of USB charging cable.  You need to hang it somewhere.  So, and it doesn't really - you can't really hang it on the charger because it'll fall off.  It needs to be more secure than that.  So this clever OCD person thought, hey, I've always found coat hanger wire to be really handy for making stuff.  So, and happened to have a pair of pliers around.  So basically fashioned this beautiful, I mean, by all measures this is a beautiful hook.



LEO:  He spent some time with his little pliers there, bending and curving.  It's gorgeous.



STEVE:  Yeah.  It is great.  And, boy, does it work as a hook.



LEO:  Yes.



STEVE:  To hang this cord on.



LEO:  Right around the prongs of that Apple 5-watt charger, just goes right around those beautifully.



STEVE:  Therein lies the problem.



LEO:  Uh-oh.  What?



STEVE:  I've never - I don't recall ever actually touching the leads of my ohmmeter to this wire.  It must be that it is coated with some sort of an insulating varnish of some sort.



LEO:  Is it?  Is it, though?  Is it?



STEVE:  Otherwise this would have already exploded because...



LEO:  Notice, though, there's a switch here.  I think that he has not switched it on yet.



STEVE:  Oh, boy.



LEO:  And I think he's going to get quite a surprise.



STEVE:  Put your shoes on and use the toe of your shoe to turn this plug on.



LEO:  Another point, because he looks like he really is, you know, OCD and careful, he has installed his plugs upside down, which...



STEVE:  It's true.



LEO:  Right?



STEVE:  You've got the, yeah, not smiley little face.



LEO:  Got the ground, they're not smiley.



STEVE:  They're not looking like a little happy face.



LEO:  Yeah.



STEVE:  Yeah.



LEO:  What's going on there?



STEVE:  It's not good.



LEO:  Yeah.



STEVE:  So anyway, so just so people are thinking that we haven't completely lost our mind, the point is that this hook has two legs that go up behind this USB charger and then bend around, you know, in U shape, to hang over the two prongs of the AC plug.



LEO:  Yeah.  There you go.



STEVE:  Wire on wire.



LEO:  It's like they made it for this.



STEVE:  Oh, it's beautiful.  I mean, it is a beautiful construction.  But no.



LEO:  The minute you plug it in, what would happen?  Would it heat up?  Would it start to glow?  Or would it actually short the thing out?



STEVE:  No, no.  You would have an immediate - the good news is, all homes ever made, even when they had screw-them-in fuses in the fuse box in the basement, they had some cutout such that, if any circuit suddenly drew too much power, rather than it exploding in your face...



LEO:  Right.



STEVE:  ...down in the basement something would go boom, and then you'd - now, of course, what you don't want is to run out of fuses because, you know...



LEO:  Don't put a penny across it.  No no no no no no.



STEVE:  That's right.  Then what people did was like, oh, shoot, you know, I don't know why this fuse blew, but it's inconvenient.  So I seem to be fresh out of fuses here.



LEO:  Yeah, but here's some good news...



STEVE:  Stick a penny in the hole, we can stick a penny in the socket and screw the blown-out circuit, you know, the blown-out fuse on top.  Anyway, yes, folks, do not do this at home.  The only thing I can think, Leo, is that there must be some varnish on this.  But...



LEO:  Even then.



STEVE:  ...over time, as it's used, it's going to get moved back and forth, riding on the top of the prongs of this plug, and it's just going to explode at some point.



LEO:  Never, ever put metal around the prongs of your plug.  I learned that in eighth grade when a...



STEVE:  Leo, on that note, we do have the picture queued up for next week, and it's another goodie.



LEO:  Good.  Oh, no.



STEVE:  It's a variation.  It's not the same.  We don't want to get repetitious here.  But we're going to have just as much fun with it.



LEO:  Somebody told me that that is the commercially preferred way of installing a plug socket is upside down like that.  And then somebody else in the Discord says that's how you know it's a switched circuit.  I've never seen that before.  But just to - reason I'm saying that, to preclude all the email that you and I inevitably will get from licensed electricians who will say, absolutely, that's the way to do it correctly.



STEVE:  Thank you for not overflowing my inbox.



LEO:  I also hope that I am not the subject of the Picture of the Week next week because I installed yesterday, we had a little lighting problem, and I and my brother-in-law did a little electric work, electrical work installing a new under-counter lamp.  And you see that switch right there, that's just to the right of Joe there?  As he was installing the wires, he accidentally backed into it and switched it on and got a little bit of a shock.



STEVE:  Oh, yes.



LEO:  Ow.



STEVE:  I see you're still wearing the avocado shirt from Sunday.



LEO:  I am wearing - this was right after I got home Sunday.  They said, "Get in here."  And I am also wearing, as sharp eyes will see, the most useful device for a home handyman anywhere.



STEVE:  Yes.  You have a head-mounted lamp.



LEO:  A head lamp.



STEVE:  Yeah.



LEO:  Please, do not make that the Picture of the Week next week.  I'm just begging of you; okay?



STEVE:  Yes.  Sometimes when Lorrie...



LEO:  Oh, you've got one, too.  Look at you.



STEVE:  Well, I've got, no, these are the magnifiers.



LEO:  Oh, that's - and what do you wear that for, Steve, besides looking like an alien?



STEVE:  When I'm building things like this.



LEO:  Oh, yes.  You've got to get very close.



STEVE:  That's right.



LEO:  Are you soldering with those on?



STEVE:  Those are little surface-mount components.  They're little itty-bitty.  So, yeah.



LEO:  That's a lot of work.  Anyway.



STEVE:  Anyway.



LEO:  Take that off and let's continue.



STEVE:  Believe it or not, we have news to get to.



LEO:  Yes, okay.



STEVE:  Last Thursday March 21st was - it was by all measures a rough day for Apple.  Not only, as I mentioned, did the tech press explode with truly "hair on fire" headlines about critical, unfixable, unpatchable, deeply rooted cryptographic flaws rendering Apple's recent M-series ARM-based silicon incapable of performing secure cryptographic operations...



LEO:  Incapable.  Incapable.



STEVE:  Can't be done.  Which is the topic we'll be spending the end of today's podcast looking at in some detail, once we get this thing started, because actually it's super interesting.  But before that, also last Thursday, the U.S. Department of Justice was joined by 15 other states and the District of Columbia, which wishes it was a state, but isn't, in a lawsuit alleging that Apple has been willfully and deliberately violating Section 2 of the Sherman Antitrust Act.



Now, I'm just going to share five sentences from the DOJ's comments which were delivered last Thursday.  They read:  "As our complaint alleges, Apple has maintained monopoly power in the smartphone market not simply by staying ahead of the competition on the merits, but by violating federal antitrust law.  Consumers should not have to pay higher prices because companies break the law."  Okay.  "We allege that Apple has employed a strategy that relies on exclusionary, anticompetitive conduct that hurts both consumers and developers.  For consumers, that has meant fewer choices; higher prices and fees; lower quality smartphones, apps, and accessories; and less innovation from Apple and its competitors.  For developers, that has meant being forced to play by rules that insulate Apple from competition."



Okay.  Now, this is not, clearly, a podcast about antitrust law.  We all know I'm not an attorney, nor am I trained in the law.  So I have no specific legal opinion to render here.  However, I've been a successful small business founder/owner/operator throughout my entire life.  And I'm certainly a big fan and believer in the free enterprise system and in the principles of capitalism.  But I also appreciate that this system of competition is inherently unstable.  It has a natural tendency for the big to get bigger through acquisition and the application of economies of scale and leverage.  That same system that creates an environment which promotes fair competition can be abused once sufficient power has been acquired.



Those of us of a certain age have watched Apple being born, then fall, only to rise again from the ashes.  My own first commercial success was the design, development, production, and sales of a high-speed, high-resolution light pen for the Apple II which allowed its users to interact directly with the Apple II's screen.  To my mind, there is no question that as a society we are all richer for the influence that Apple's aggressive pursuit of perfection has had on the world.  Things as simple as product packaging will never be the same.



But for some time we've been hearing complaints about Apple's having taken this too far.  It's understandable for competitors to complain, and to ask the government to step in and do something.  At some point that becomes the government's very necessary role, just as we saw previously when the same thing happened with Microsoft, and some would argue ought to happen again with Microsoft.  For many years the U.S. government has done nothing, while Apple has continued to grow and continued to aggressively use its market power to increase its shareholders' wealth.  The question is, when does use of market power become abuse of market power?  The next few years will be spent in endless depositions and expert testimony working to decide exactly what sort of cage Apple needs to be constrained within.



One thing we know is that many of the arguments Apple will be making on its own behalf will involve security, the security inherent in its closed messaging system, the inherent security of its closed App Store.  You know, things we've touched on many times in this podcast.  Apple will allege that by keeping its systems closed, it is protecting its users from unseen nefarious forces.  But, for example, the presence of Signal and WhatsApp in the App Store and on Apple devices, which create freely interoperable super-secure cross-platform messaging, suggests that Apple's own messaging technology could work similarly if they wished it to.



During the news coverage of this since Thursday, I've encountered snippets of evidence which suggest that the government has obtained clear proof of Apple's true motives where Apple's technology has been designed to support Apple's interests rather than those of its users.  In any event, and, you know, maybe those are aligned.  That's really the question; right?  Are Apple's interests and its users' interests perfectly aligned?



Nothing is going to happen on this front for a long time.  Years will pass, and this podcast will be well into four digits by the time anything is resolved with the DOJ's antitrust lawsuit.  The way things have been going, it seems to me much more likely that the laws being written and enacted within the European Union today will be forcing Apple's hand long before the DOJ finishes making its case.  All that may eventually be required will be for the U.S. to force Apple to do the same thing that they're already doing over in Europe, here as well.  But as for whether Apple-designed silicon cannot perform secure cryptographic operations, that is something this podcast can speak to authoritatively, and we'll be doing so once we've caught up with some more interesting news and feedback.



LEO:  Good.  I always said back in the day, in fact it was during - it's funny how you began this with the good old days of Apple because back in the day when the Department of Justice was suing Microsoft, I always said, if Apple were as big and powerful as Microsoft, they'd be just as bad.  But they aren't.  In fact, they almost went out of business in '97.  And now that they are even a little bit bigger than Microsoft, yeah, they're just as - it's what happens.



STEVE:  It is.  It is exactly what happens.  And it's not that anybody is a bad person.  You know, I mean, they argue, the executives argue that it's their job to maximize shareholder wealth.



LEO:  Right.  That's capitalism.



STEVE:  Yes, exactly.  Exactly.  And so it's a fundamental property that there need to be constraints.  And of course in the U.S. we have those.  Boy, is it painful to get them.  But, you know, it's interesting.  And I think I heard Rene saying that he thought he was going to have to spin up another podcast in order to keep track of this.



LEO:  Oh, I'm sure.



STEVE:  I'm not going to bother with that.



LEO:  No, we're not going to do it, yeah.



STEVE:  No.



LEO:  No.



STEVE:  It'll go on and on and on, and we'll mention it once in a while, and that'll be it.



LEO:  Right.  This thing will go for years, exactly as it happened with Microsoft



STEVE:  So last week we shared the difficult, I mean, truly difficult to believe, but true story that General Motors had actually been sharing - and by sharing I'm pretty sure the proper term would be selling - the detailed driving record data of its car owners, down to how rapidly the owner's car accelerated, how hard it braked, and its average speed from Point A to Point B.  Leo, they literally have instrumentation in there that is monitoring everything the car does.  And these cars are all interconnected now.  It was all being beamed back to GM, who it turns out was selling it to LexisNexis, a major data broker.



Anyway, so what happened was - and this was a New York Times or Washington Post, I think it was The New York Times piece last week that just blew the lid off this.  Some guy, I think he was in Canada, or maybe he was just up north, he saw his insurance go up 21% in one year, although he'd never been in an accident and didn't have tickets.  And so when he asked his insurance company why, they sort of hemmed and hawed.  He also tried to obtain alternate insurance, and all the quotes that he got back from competing companies were the same.  Finally one of them said, well, you should check your LexisNexis report because it's a little worried about your driving habits.



LEO:  Oh, so now there's like a credit report, there's now a car driving report.



STEVE:  Yes.



LEO:  But you know what, in some ways, A, I'm not surprised.  Insurance companies have for years offered good driver discounts.  In the past you used to have an app and stuff.  I'm not surprised to hear this.  And honestly...



STEVE:  Optionally installed app.



LEO:  Right.



STEVE:  For, like, low-mileage drivers where it would monitor - I'm sure we talked about it on the podcast.



LEO:  Yeah, but this is good for you and me because insurers, instead of this guy who really is not a safe driver paying the same as you and me who drive like little old men, because we are, we should get reduced; right?  And he should pay more.  It's fair, I think.



STEVE:  And should it be done without consent.



LEO:  Well, in a way I bet you he did consent.  I bet you there is somewhere a document that he signed when he bought that car that said data's being collected.  You saw the Mozilla report last year.  We talked about it, about how cars are a privacy nightmare.



STEVE:  Yeah, well, we were all wondering recently how your sexual habits were being recorded by a car.  It's like, what?  Like, is it monitoring the suspension?



LEO:  Should this car be rockin', as you well know, don't you be knockin'.



STEVE:  Okay.  So the good news is this produced an outcry which caused GM to immediately terminate this conduct.  And no doubt threats of lawsuits were involved, too.  They said GM is immediately stopping the sharing of this data with these brokers.  The report said:  "After public outcry, General Motors has decided to stop sharing driving data from its connected cars with data brokers.  Last week, news broke that customers enrolled in GM's OnStar Smart Driver app have had their data shared with LexisNexis and Verisk.  Those data brokers in turn shared the information with insurance companies, resulting in some drivers finding it much harder or more expensive" - exactly as you said, Leo - "to obtain insurance.  To make matters much worse, customers allege they never signed up for OnStar Smart Driver in the first place, claiming the choice was made for them by salespeople during the car-buying process."



LEO:  Yeah.  And, you know what, it comes with the car.  And, you know, it's good, it's all for your safety.  That's why we put it in, so that if you get a wreck, you can press the OnStar button.



STEVE:  That's right.



LEO:  That's why they did it.



STEVE:  And people will come.



LEO:  Yes.



STEVE:  We're not Big Brother watching over you.



LEO:  No, of course not.



STEVE:  Okay.  So I saw this bit of happy cryptocurrency news that just made me smile.  It seems that last week the blockchain game - I didn't know there was a blockchain game.  But yes, someone has made a game out of blockchain, and it's called Super Sushi Samurai.  Super Sushi Samurai had $4.6 million worth of its tokens stolen.  However, it just reported that they had all been recovered.  So what happened?  They explained that the hack was actually the work of a security researcher who exploited a bug in their code to move the funds out of harm's way to prevent future theft.



LEO:  Yeah, that was all.  Yeah.



STEVE:  That's right.



LEO:  Just want to move them out of harm's way.



STEVE:  Super Sushi Samurai described the incident as a "white hat rescue" and has ended up hiring the white hat to be a technical advisor.  So that's what I call a G-rated happy ending.



LEO:  Okay.



STEVE:  Yeah.



LEO:  I believe it.  Why not?



STEVE:  And also you guys touched on this on MacBreak.  Apple Insider has some interesting coverage about Apple's apparently failed initiative to move their HomeKit technology up into home routers.  I was a fan of this, since it promised to provide router-enforced inter-device traffic isolation, and the only place that can really be accomplished is at the router.  Our listeners know that I've been advocating for the creation of isolated networks so that IoT devices would be kept separate from the household's PCs.  But what Apple proposed five years ago, back in 2019, would have additionally isolated each IoT device, like with that level of granularity, from all the others.  So here's what Apple Insider explained.



They said:  "Apple's HomeKit Secure Routers were announced in 2019, but were never really taken up by manufacturers.  And now some vendors are claiming Apple is no longer pursuing the technology."  And we'll get to why in a minute.  "HomeKit Secure Routers," they wrote, "were introduced by Craig..."



LEO:  Federighi.



STEVE:  Federighi.  I know his name.  The problem is, you know, I'm a big Star Trek person, and I want to say Ferengi.  Which, you know...



LEO:  Craig Ferengi over there at Apple headquarters.



STEVE:  Had to stop from saying it.  I was, it's not Ferengi.  Come on, Steve.



LEO:  That's pretty funny.  I would never have guessed that.  Wow.



STEVE:  So "...Craig Federighi at World Wide Developer Conference 2019, and in the same breath and at the same time they introduced HomeKit Secure Video.  The latter, that is, HomeKit Secure Video, took time to reach the market, but it was used, and manufacturers adopted it, even if others would not."



Okay.  Now, "During [this year's just happened] CES 2024, two router vendors separately told AppleInsider that Apple is no longer accepting new routers into its program.  If that claim is correct  and it probably is, since it came from the same rejected manufacturers  given the lack of HomeKit Secure Routers on the market" - that is, in five years not much happened - "it appears that Apple has abandoned the idea, even though Apple still has active support pages on the matter.  However, Apple Insider noted that it also has support pages on AirPort routers too, and those are," as they put it, "dead as a doornail."



LEO:  Those really are dead, yeah, yeah.  I was so excited that Apple would offer this security standard, that we could have some confidence in the security and, frankly, firmware updatability of our routers.  It's a little disappointing to me.



STEVE:  Yeah.  Anyway, it's not going to happen.  They backed out.  Apple Insider, to make a long story short, polled the various routers that Apple listed.  There is one Linksys Velop AX4200 and an AmpliFi Alien router are apparently the only two that are currently listed by Apple as being supported.  Eero has a notice saying that its Eero Pro 6E and 6+ do not support Apple HomeKit, and they have no plans to offer Apple HomeKit router functionality.



Anyway, so not everything that gets announced happens.  And asking router manufacturers to modify their firmware to incorporate the required HomeKit functionality, and it appears that it may have taken some significant customization, it was just never going to get off the ground.



And this is probably for the better since it appears that we have already - and oh, thank god, blessedly quickly - moved beyond disparate proprietary closed IoT ecosystems.  Which, you know, is where it looked like we were headed with Amazon Alexa and Apple's HomeKit and Google's Home and Samsung's SmartThings, all creating their, oh, let's do our own thing.  All the buzz appears to now be surrounding the interoperability technology known as "Matter."  This was formerly known as CHIP, which stood for Connected Home over IP.  It's now been rebranded as Matter, and everyone appears to be seeing the light.  Nobody wants to be left out.



All those guys I just mentioned - Amazon with Alexa, Apple with their with HomeKit, Google with Home, and Samsung's SmartThings - all have announced and are supporting Matter.  It's now at v1.2, open, open source, license free.  Anyone can create Matter-compatible devices.  If they follow the spec, they will interoperate.  And more than 550 companies have announced their commitment to Matter.  So this is done; right?  I mean, all of the biggies are going to be supporting Matter.  They really have no choice.  And at this point I wanted to make sure I brought it up because I wouldn't purchase something, you know, that random AC plug that I got for a shockingly, I don't know, it was $4 or something.  It's amazing.  How can this be an Internet-connected device?  Just like, what, the plastic and the prongs would cost $4.



LEO:  It does report back on your driving habits, however.



STEVE:  Oh, it's got a little eyeball in it that follows you around the room.



LEO:  Yeah.



STEVE:  It's kind of freaky.



LEO:  Does Matter have, though, I mean, the thing about Apple's HomeKit router standard was it had security requirements built in.  Does Matter have something like that?



STEVE:  That's what they were, you're right, that's what they were going to produce.



LEO:  Yeah.



STEVE:  I think Matter is about interconnectivity.



LEO:  Right.  Right. 



STEVE:  Which is not to say it couldn't be made more secure, but that's not their focus.



LEO:  Right.



STEVE:  Thank you, my friend.  Okay.  In a cool bit of news, ICANN, the Internet Corporation for Assigned Names and Numbers, is going to make an assignment.  It's in the process of designating and reserving, get this, a top-level domain specifically for use on private internal networks.  In other words, our 10-dot and 192.168-dot networks, and there's a 17.16 thing in there, too, will be obtaining an official TLD of their own.  So localhost may soon be less lonely.  Here's the Executive Summary which explains and lays out the rationale behind ICANN's plans.



They wrote:  "In this document, the SSAC" - that's the Security and Stability Advisory Committee because, you know, that's what you want in your Internet is some security and stability advising.  They recommend "the reservation of a DNS label that does not and cannot correspond to any current or future delegation from the root zone of the global DNS," which is the very long-winded way of saying we're going to get our own dot-something TLD.



They said:  "This label can then serve as the top-level domain name of a privately resolvable namespace that will not collide with the resolution of names delegated from the root zone."  That is, you know, the public DNS root zone.  "In order for this to work properly, this reserved private-use TLD must never be delegated in the global DNS root.  Currently, many enterprises and device vendors make ad hoc use of TLDs that are not present in the root zone when they intend the name for private use only.  This usage is uncoordinated and can cause harm to Internet users."  Oh, my.



"The DNS has no explicit provision for internally-scoped names, and current advice is for the vendors or service providers to use a sub-domain of a public domain name for internal, or private use.  Using sub-domains of registered public domain names is still the best practice to name internal resources.  The SSAC concurs with this best practice and encourages enterprises, device vendors, and others who require internally-scoped names to use sub-domains of registered public domain names wherever possible.  However, this is not always feasible, and there are legitimate use cases for private-use TLDs."



And I'll just note that, you know, for example, an individual could register a domain with Hover, who I don't know if they're still a sponsor of the TWiT Network.  They are still my domain name provider.  I moved everything away from Network Solutions...



LEO:  I agree.



STEVE:  ...once it became clear...



LEO:  I don't think they're a sponsor anymore, but we still love them.



STEVE:  Yup.  They're the right guys.  Anyway, so, you know, Johnny Appleseed, you could get that.  Of course you can't get dot Johnny Appleseed, so that wouldn't work.  But you could get, you know, a dotcom or some inexpensive subdomain of some established top-level domain and just use that for your own purpose.  Because you have that subdomain, nobody else is going to be able to use it publicly.  So you're safe.  So that's what these guys were saying.



So they continue:  "The need for private-use identifiers is not unique for domain names, and a useful analogy can be drawn between the uses of private IP address space and those of a private-use TLD.  Network operators use private IP address space to number resources not intended to be externally accessible, and private-use TLDs are used by network operators in a similar fashion.  This document proposes reserving a string in a manner similar to the current use of private IP address space.  A similar rationale can be used to reserve more strings in case the need arises."  Okay.  So they go on and on.



Anyway, finally, after all the bureaucrat boilerplate has settled down, ICANN wrote:  "The Internet Assigned Numbers Authority (IANA) has made a provisional determination that '.internal' should be reserved for private-use and internal network applications.  Prior to review and approval of this reservation by the ICANN Board, we are seeking feedback on whether the selection complies with the specified procedure from SAC113" - more bureaucracy - "and any other observations that this string would be - to verify that it would be an appropriate selection for this purpose."  So it's all but certain that .internal will be reserved and will never be used for any  public purpose, and therefore it would be safe for anyone to start using it for any internal purpose.



LEO:  That's nice to have, thank you.



STEVE:  Very cool, .internal.  I saw some commentary saying, well, it only took 30 years.



LEO:  That's true, yeah.



STEVE:  Yeah, that is true.



LEO:  Took them a little while.



STEVE:  Okay.  So last Thursday, as I said earlier, was a very busy day.  Not only did the DOJ announce their pursuit of Apple, and Apple's M-series silicon was discovered to be useless for crypto, but the United Nations General Assembly adopted a resolution on artificial intelligence, not that anyone cares or that anyone can do anything about AI in any event.  But for the record, UN officials formally called on tech companies to develop safe and reliable AI systems that comply with international human rights.  And I loved this.  They said:  "Systems that don't comply should be taken offline."  So, you know, you have a mean AI, just unplug it, folks.  Officials said the same rights that apply offline should also be protected online, including against AI systems.



I've never said much about AI here.  Just as I'm not trained as an attorney, I do not have any expertise in AI systems.  What I do have, however, is stunned amazement.  As they would say over in the UK, I am gobsmacked by what I've seen.



LEO:  It is impressive; isn't it.  What do you...



STEVE:  Oh, my god.



LEO:  You know, I haven't ever asked you, and we talk about it all the time on the other shows, but what do you think the future holds?



STEVE:  Well, here I come.  So what I may lack in expertise appears to have been made up for by my intuition, which has been screaming at me ever since I spent some time chatting with ChatGPT 4.  My take on the whole AI mess and controversy can be summed up in just four words, and they are:  "Good luck restraining anything."



LEO:  Yeah, that's my attitude.  Exactly.



STEVE:  Yes.  I doubt that any part of this is restrainable.  At some point in the recent past we crossed over a tipping point, and we're seeing something that no one would have believed possible even five years ago.  Everyone knows there's no going back.  Only people who have not been paying attention imagine that there's any hope of controlling what happens going forward.  I don't know, and I can't predict what the future holds.  But whatever is going to happen is going to happen, and I'm pretty sure that it's bigger than us.



LEO:  Yes.



STEVE:  We're not a sufficiently organized species to be able to control or contain this.



LEO:  Mm-hmm.  Look how well we've done with nuclear proliferation.



STEVE:  Yeah.



LEO:  And that's - it's still incredibly hard to purify enough plutonium to make a bomb.  It's trivially easy, and the process is well, well known, to make an LLM.



STEVE:  It's out.



LEO:  It's out.  It's done.



STEVE:  You know, it would be like government saying, whoops, stop exporting crypto.  Like, what?



LEO:  Yeah, exactly.



STEVE:  You know.  And so Leo, you and I are on the same page.  I mean, it is - and if we don't, like, do it, we know North Korea's not sitting around doing nothing.  They apparently have quite smart people.



LEO:  Yes.



STEVE:  It annoys me that they're so good at hacking.  But, boy, they are serious hackers.  And so, you know...



LEO:  It's going to happen.



STEVE:  It is.  I would argue it already has, and we just haven't - it hasn't dawned on us yet.



LEO:  Yeah.



STEVE:  Right?  Like there's some inertia of recognition.



LEO:  I for one, I'm excited.  I mean, this is sci-fi.  We are going to live in, I think I might even live to see it, a very weird and different future.  It's coming.



STEVE:  Yeah.



LEO:  It's going to be fun.  Buckle up.



STEVE:  That's exactly right.  I think that's exactly right.  Okay.  So a few more points to get to.  In a somewhat disturbing turn, Spain has joined the likes of China, Thailand, Pakistan, Iran, and Cuba to be blocking all use of and access to Telegram across its territory.  This came after Spain's four largest media companies successfully complained to the high court in Spain that Telegram was being used to propagate their copyrighted content without permission.



A judge with Spain's high court had asked Telegram to provide certain information relating to the case, which apparently Telegram just blew off and ignored.  They chose not to respond to the judge's request.  So he ordered all telecommunications carriers to block all access to Telegram throughout the country.  That began yesterday.  So, you know, it's a problem.



LEO:  I'd be very interested to see how this holds up because about - I heard that about a third of Spain uses Telegram.



STEVE:  Yes.  It has already created a huge...



LEO:  Yeah.



STEVE:  Yes.  There's a huge consumer backlash against this, as one would expect.



LEO:  Yeah, remember Brazil tried to do this.  And they ended up having to back down.  I think it was for WhatsApp.  But they ended up having to back down because you can't - we can't communicate.  What are you doing?



STEVE:  Well, have you seen the movie "Brazil," Leo?  That explains the whole problem.



LEO:  You know it was a great movie.



STEVE:  That's right, yes.  Wonderful movie.  So last week Vancouver held its 2024 Pwn2Own hacking competition.  One security researcher by the name of Manfred Paul distinguished himself by successfully exploiting, get this, all four of the major web browser platforms.



LEO:  Wow.



STEVE:  He found exploits in Chrome, Edge, Firefox, and Safari.  He became this year's "Master of Pwn" and took home $202,500 in prize money.  Overall, and here's really the lesson, the competing security researchers-turned-hackers successfully demonstrated 29 previously unknown zero-days during the contest and took home a total of $1.1 million in prize money.



LEO:  That money comes from the companies that they're pwning, pretty much; right?



STEVE:  Yes,  yes.  Twenty-nine, okay, 29 previously unknown zero-days were found and demonstrated.  To me this serves to demonstrate why I continue to believe that the best working model that's been presented for security - and okay, yes, I'm the one who presented it - is "porosity."  Porosity.



LEO:  Porosity.



STEVE:  You know, we don't want it to be, but security is porous.  How else can we explain that one lone research hacker is able to take down all four of the industry's fully patched browsers whenever someone offers him some cash to do so?  And that overall, 29 new previously unknown zero-days were revealed, when others were similarly offered some cash prize incentive.  You know, you push hard, and you can get in.  That's the definition of porous.  And that's the security we have.



LEO:  Yeah.



STEVE:  I should also take a moment to give a shout-out to Mozilla's Firefox team, who had patched and updated Firefox in fewer than 24 hours following the vulnerability disclosure.  Frederik Braun posted on Mastodon:  "Last night, about 21 hours ago, Manfred Paul demonstrated a security exploit targeting Firefox 124 at Pwn2Own.  In response, we have just published Firefox 124.0.1 (and Firefox ESR 115.9.1) containing the security fix."  He says:  "Please update your foxes.  Kudos to all the countless people postponing their sleep and working toward resolving this so quickly.  Really impressive teamwork again.  Also, kudos to Manfred for pwning Firefox again."



So, you know, this is the way security is supposed to work at the best of times.  White hat hackers are given some reason to look, and compensated for their discoveries, which makes the products safer for everyone.  And then the publishers of those  products promptly respond to provide all of that product's users the benefits of that discovery.  Yay. 



And in this welcome bit of news, perhaps we and others are giving as good as we get.  I've often noted that all we ever hear about, you know, about attacks on our infrastructure are Chinese state-sponsored attacks that are successfully getting in.  You know, and I note that naturally we never hear about our similar successes against China.  It's not like the NSA is going to brag.  So I've wanted to believe that, while we would not destructive if we were to get in, that we'd only seek to have a presence inside Chinese networks so that they understand that we're just not sitting here defenseless over on this side of the Pacific.



Well, it turns out that last week China's state security agency themselves urged their local companies to improve their cybersecurity defenses.  The Ministry of State Security said that foreign spy agencies have infiltrated hundreds of local businesses and government units.  So that does sound like we may be at parity in this weird cyber cold war that we're in.  I hate it; but, you know, it's what we've got.



Oh, and just a reminder.  There has been an observed significant increase in tax season-related phishing.  So I just wanted to remind everyone that, as happens at every time this year, you know, phishing scams suddenly jumped with all kinds of like, oh, you just - we received your electronically submitted return, but it had a problem.  Please click here.  But that's not from the IRS.  So everybody put up your skepticism shields and resist clicking.



I have two quick notes of news that I think everyone'll find interesting on the SpinRite front.  One of the things that quickly became apparent as our listeners were wishing to obtain and use 6.1 was that the world had changed in another way since SpinRite 6's release back in 2004.  Back then, Linux was still largely a curiosity, you know, with a relatively small fan base and no real adoption.  Not so today, at least not among our listeners.



Back in 2004 it was acceptable to require a SpinRite user, I mean, just assumed that a SpinRite user would have Windows, which they would use to set up the boot media since Windows and Mac was pretty much all there was, and SpinRite was never really targeted at the Mac market.  Today, we've encountered many would-be users who do not have ready access to a Windows machine.  And they've been having a problem.  So I needed to create a non-Windows setup facility that I have long envisioned but never needed until now.  Today it exists.



Over at GRC's prerelease.htm page is, as before, the downloadable Windows/DOS hybrid executable, and now also a downloadable zip file.  The zip file, which is smaller than 400K, contains the image of the front of a 4GB FAT32 DOS partition.  So any SpinRite owner without access to Windows, because using Windows is still easier, may choose to instead download this zip file.  And it's personalized.  I've added on-the-fly partition creation, and SpinRite is added to the file system.  It's then truncated, and I've got on-the-fly zipping.  I've been busy.  It contains about an - the zip file, which is as I said less than 400K, contains an 8.3MB file which is named SR61.img.  Any Linux user can, you know, "dd" copy that file onto any USB thumb drive to create an up to 4GB FAT32 partition that will immediately boot and run SpinRite.



But the tricky bit that I worked out last week is that when this drive is booted for the first time, if the media onto which this image file was copied is smaller than the partition described by the image, which is a 4GB partition - for example, SpinRite's owner copies the image to an old but trusted 256MB thumb drive - a little built-in utility named "downsize" kicks in, examines the size of the partition's underlying physical drive, and dynamically on the fly "downsizes" the partition to fit onto its host drive.  It's all transparent and automatic.  And since this same technology was also going to be needed for SpinRite 7, it made sense to get it done, so it's there now.



Second point.  A new wrinkle to surface last week is bad RAM.  Over in GRC's web forums, a SpinRite 6.1 user reported data verification errors being produced by SpinRite when running on his cute little ZimaBoard.  SpinRite always identified and logged the location of the apparent problem.  But from one run to the next there was no correlation in where the problems appeared to be occurring.  And when he ran the same drive under SpinRite on a different PC, it passed SpinRite's most thorough Level 5 testing without a single complaint.  And he was able to go back and forth to easily recreate the trouble multiple times on one system, but never on the other.



The inhabitants of the forums jumped on this and suggested a bad or undersized power supply for his ZimaBoard, flaky cabling, and anything else they could think of, all great suggestions.  Finally, I asked this user to try running the venerable MemTest86 on his brand new ZimaBoard.  And guess what?  Yep.  Memory errors.  There should never be any.  But the first time he ran MemTest86 it found six, and the second time it found 101.



Seeing that, we ran MemTest86 on all of our ZimaBoards, that is, all of the developers, and they all passed with zero errors, as they always should.  So this user had a ZimaBoard with a marginal DRAM memory subsystem.  There was no correlation in the locations of the errors that SpinRite was reporting from one memory - oh, that his MemTest was reporting from one pass to the next.  But there were always two specific bits out of the 32 that MemTest86 always identified as being the culprits.  They were soft.  And SpinRite was getting tripped up by this machine's bad RAM when it was performing data verification that's available from SpinRite's levels 4 and 5.



The problem was not the drive.  It was the machine hosting SpinRite and the drive.  So by this point our longtime listeners who've grown to know me, listening to this podcast, know what I'm going to say next.  Yep, SpinRite 6.1 now tests the memory of any machine it's running on.



LEO:  Oh, clever.  Wow.  Who needs MemTest?  I've got SpinRite.



STEVE:  That's right.  It works great.  It, like, immediately found the errors this guy was having.  What's interesting is that SpinRite 1.0, back in 1988, also built-in a memory test.  Back then it made sense to verify the RAM memory that would be used to temporarily hold a track's data while SpinRite was pattern testing the physical surface and giving it a fresh new low-level format.  But I don't know when it happened, somewhere along the way I removed that feature from SpinRite.  We never heard of it ever being useful, so my initially over-cautious approach seemed to have been proven unnecessary - until last week.



So late last week I implemented a very nice little DRAM memory tester right into SpinRite and then had the guy with the bad ZimaBoard give it a try.  It successfully determined that his machine's memory was not reliable, and SpinRite will then refuse to run on any such machine after making that determination.  You know, it's just not safe to run it.  And of course no such machine should be trusted for actually doing anything else.  You know, it's like send it back to the manufacturer; or, if you can, change the RAM, or diagnose it.



So anyway, this new built-in RAM testing feature - which is not yet present.  Don't go download an updated copy of SpinRite.  It's not there yet, not yet present in any SpinRite that's available for download.  But it'll appear, along with a few other minor improvements that I've made, shortly.  So I'm sure I'll be announcing it next week.



And I just have two little pieces of feedback from our listeners because we have lots to still talk about here.  I got a note from someone whose handle is Jazman.  He said:  "Hi, Steve.  Great show, as always.  I work in a cell phone-free environment.  Not only no service, but we're not allowed to bring them.  We have Internet computers, but we're not trusted to install anything on them."  Sounds like he is with the NSA.  "The problem is I like to have two-factor authentication to protect my email and other stuff.  My understanding is, if I were to use Passkeys, I need my phone.  I use Bitwarden with two-factor authentication.  My question:  Are there any good solutions for a cell-free environment?  Kind regards, Bjorn."



Okay.  So, and we've been talking about this for the last couple weeks, whether to have two-factor and now optionally Passkeys managed by your password manager, or to keep it separate.  In a phone-free environment, I agree that relying upon Bitwarden for all authentication services is likely the best bet.  I think it's probably your only bet; right?  You know, we would usually prefer to have Passkeys or our authenticator on a separate device like a phone.  But where that's not possible, merging those functions into a single password manager like Bitwarden makes sense.  And I should just note that YubiKeys are also Passkeys-capable, and they're able to store up to 25 Passkeys in a YubiKey.  So a YubiKey is another possibility, if that somewhat limited Passkeys capacity doesn't pose a problem.



And finally, William Ruckman.  He said:  "Hi, Steve.  Are Passkeys quantum safe?  I thought public key crypto was vulnerable."  And we'd also been speaking this recently about how the big difference between username and password and Passkeys is the essentially symmetric crypto secret keeping, whereas Passkeys uses public key crypto, which is why William's asking.  So it's a terrific question because, as we know, it's the public key crypto that Passkeys offers, which is why it's so valuable.  The good news is the FIDO2 specification which underlies WebAuthn, which underlies Passkeys, already provides for plug-in future-proof crypto.  So Passkeys and WebAuthn/FIDO2 will all be able to move to quantum-safe algorithms whenever that's appropriate, and as soon as we've settled on them, and they've been standardized.  So yes.



LEO:  Good.  That's good news.  And it would be backward to all the Passkeys you already are using and all that.  Right?  Maybe not.



STEVE:  No.  Could not be.



LEO:  You'd have to regenerate them all.  Okay.



STEVE:  Yes.  If you change the crypto, you would have to regenerate the Passkeys because you're holding private keys with a specific algorithm.



LEO:  Right.



STEVE:  And there is actually no way for the website even to help you.  I mean, it might say, you might go through a "use your old Passkey, now use your new Passkey."



LEO:  Yeah.



STEVE:  And if you did that, you know, sequentially, then it would get - actually SQRL had a similar facility.  So it would use the first authentication to assert your identity and thus honor the second authentication, which would be from the newfangled crypto.  And now it would have the public key under the new algorithm.



LEO:  Cool.  We don't have to worry about it yet.  There's only about four sites that use this.



STEVE:  I know.  I saw, in doing some research just yesterday, I saw someone who had something to sell, they were trying to sell some equivalent of a YubiKey, I think.  And it said, "Since the  majority of the Internet's websites are now using Passkeys."  And I thought, are you in a time machine?  What are you talking about?



LEO:  Oh, you're talking 2030.  Oh, yeah.  Maybe.  Maybe.



STEVE:  Yeah, yeah.



LEO:  There's literally...



STEVE:  The majority as long as you only log into PayPal.



LEO:  Yeah, right.  There's literally just a handful of sites that use it.



STEVE:  I know.  I know.



LEO:  It's too bad because it's so easy when it works.  I heard you talked a lot about that last week with Mikah.  And I agree with you, I think it's going to be a big improvement someday.  Someday our prince will come.



STEVE:  Okay.  After our final announcement, Leo, oh, boy, we're going to have some fun.



LEO:  Oh, boy.



STEVE:  Get the beanies lubed.



LEO:  I don't know if you should say that.  Shouldn't say that out loud, anyway.  And now, let's talk about Fetch.  GoFetch.



STEVE:  So, GoFetch.  Last Thursday the world learned that Apple had some problems with their cryptography.  Unfortunately, it would be impossible to determine from most of the tech press's coverage of this whether this was an apocalyptic event or just another bump in the road.  Ars Technica was apparently unable to resist becoming click-bait central with their headline "Unpatchable vulnerability in Apple chip leaks secret encryption keys."  Wow.  That would be bad if it was true.



Fortunately, it's not the least bit true.  It's not unpatchable, and it's not a vulnerability in an Apple chip.  Kim Zetter's Zero Day goes with:  "Apple Chip Flaw Lets Hackers Steal Encryption Keys."  This "chip flaw" [in air quotes] theme seems to have become pretty popular, even though nowhere did any of the actual researchers ever say anything about any chip flaw.  Even Apple Insider's headline read "Apple Silicon vulnerability leaks encryption keys and can't be patched easily."  What?



Apple was told 107 days before the disclosure, back on December 5th of last year.  Apple is certainly quite aware of the issue, and I'm sure they're taking it seriously.  And for their newer M3 chips, all that's needed is for a single bit to be flipped.  Tom's Hardware went with:  "New chip flaw hits Apple Silicon and steals cryptographic keys from system cache.  'GoFetch' vulnerability attacks Apple M1, M2, M3 processors, can't be fixed in hardware."  Oh dear.



Except for a few details:  It's not new.  It's not a flaw.  Nothing ever "hit" Apple Silicon.  And as for it not being fixable in Apple M1, M2 or M3 processors, if you have an M3 chip, just flip the bit "on" during crypto operations and the unfixable problem is solved.  And finally, as we'll see by the end of this topic today, there are equally simple workarounds for the earlier M-series processors.



Okay.  So I could keep going because the material in this instance was endless.  Not a single one of the headlines of the supposedly tech press stories that covered this characterized this even close to accurately.  It's not a flaw.  Nothing is flawed.  Everything is working just as it's supposed to.  It's not a vulnerability in Apple Silicon.  Apple Silicon is just fine, and nothing needs to change.  And it's certainly not unfixable or unpatchable.  CyberNews headline was "M-series Macs can leak secrets due to inherent vulnerability."  The only thing that's inherently vulnerable here is the credibility of the tech press's coverage of this.



LEO:  Holy cow.



STEVE:  It really has been quite over the top.  After sitting back and thinking about it, the only explanation I can come up with is that, because what's actually going on with this wonderfully and subtly complex problem, no one writing for the press really understood what the researchers have very carefully and reasonably explained.  So they just went with variations on Ars Technica's initial "unpatchable vulnerability in Apple's chip" nonsense, under the assumption that Ars must have actually understood what was going on, so everyone just copied them.



LEO:  I just assumed Dan Gooden knows.  If he doesn't know - right, yeah.



STEVE:  You know, Dan's on the ball, typically.  And we do know, in fairness to Dan, he doesn't provide the headlines.  Back when I was writing the Tech Talk column for InfoWorld, I was often really annoyed by what my columns were headlined because that's not what I said in the text.  But, you know, some copy editor, I guess that's what they're called, gave it the headline that would get people to turn to the page.  So, okay.  Not Dan's fault.



Okay.  The TL;DR of this whole fiasco is that a handful of researchers built upon an earlier two-year-old discovery, which three of them had been participants in back then, that was dismissed at the time by Apple, you know, as being of only academic interest.  It's yet another form of side-channel attack on otherwise very carefully designed to be side-channel attack-free constant-time cryptographic algorithms.  The attack surrounds an ARM-based performance optimization feature known as DMP.  And I was thinking, boy, if the acronym had been EMP, that would have really blown the tech press right off the top.  Anyway, not EMP, DMP.  And a variation of the same type of optimization is also present in the newest Intel chips, their Raptor something or other.  Anyway, I'll get to that.



Okay.  And so, true to Bruce Schneier's observation that attacks never get worse, they only ever get better, about a year and a half after that initial discovery two years ago which never amounted to much, it turned out that the presence of this DMP, which I will be explaining in detail, optimization feature actually did and does create an exploitable vulnerability that can be very cleverly leveraged to reveal a system's otherwise well-protected cryptographic secrets.  After verifying that this was true, the researchers did the responsible thing by informing Apple, and we have to assume Apple decided what they wanted to do next.  Okay.  Unfortunately, that true story doesn't make for nearly as exciting a headline.  So none of the hyperventilating press explained it this way.



One important thing that sets this apart from the similar and related Spectre and Meltdown vulnerabilities from yesteryear is that this new exploitation of the DMP optimizer is not purely theoretical.  All we had back in those early days of speculative execution vulnerabilities was a profound fear over what could be done, over what this meant.  It was clear that Intel had never intended for their chip's internal operation to be probed in that fashion, and not much imagination was required to envision how this might be abused.  But we lacked any concrete real-world proof of concept.  Not so today.  And not even post-quantum crypto is safe from this attack since we're not attacking the strength of the crypto, but rather the underlying keys are being revealed.



The GoFetch proof-of-concept app running on an Apple Mac connects to the targeted app, also on the same machine, which contains the secrets.  It feeds the app a series of inputs that the app signs or decrypts or does something using its secret keys, basically inducing it to perform cryptographic operations that require it to use the secrets it's intending to keep.  As it's doing this, the app monitors aspects of the processor's caches which it shares with the targeted app in order to obtain hints about the app's secret key.  Okay.  So how bad is it?



As I mentioned, the attack works against both pre- and post-quantum encryption.  The demo GoFetch app requires less than an hour to extract a 2048-bit RSA key, and a little over two hours to extract a 2048-bit Diffie-Hellman key.  The attack takes 54 minutes to extract the material required to later assemble a Kyber 512-bit key and about 10 hours for a Dilithium 2 key, though some time is also required afterwards for offline processing of the raw data that is collected.  In other words, it is an attack that is practical to employ in the real world.



Okay.  So what exactly is DMP, what did the researchers discover, and how did they arrange to make their Macs give up the closely held secrets being hidden inside?  The research paper is titled "GoFetch:  Breaking Constant-Time Cryptographic Implementations Using Data Memory-Dependent Prefetchers."  Okay, now, that sounds more complex than it is.  We have "Breaking Constant-Time Cryptographic Implementations."  We already know that a classic side-channel vulnerability which is often present in poorly written crypto implementations is for an algorithm to in any way change its behavior depending upon the secret key it's using.  If that happens, the key-dependent behavior change can be used to infer some properties of the key.  So the first portion of the title tells us that this attack is effective against properly written constant-time cryptographic implementations that do not change their behavior in any way.  That's not where things got screwed up.



The second part of the paper's title is "Using Data Memory-Dependent Prefetchers," and that's what's new here.  If you guessed that the performance optimization technique known as DMP stands for "Data Memory-Dependent Prefetchers," you'd be correct.  Three of the seven co-authors of today's paper co-authored the earlier groundbreaking research two years ago which described their reverse-engineered discovery of this DMP facility residing inside Apple's M-series ARM-derived chips.  Back then they raised and waved a flag around, noting that what this thing was doing seemed worrisome; but they stopped short of coming up with any way to actually extract information.  And the information that they had was made public.



Now, we don't know for sure that sophisticated intelligence agencies somewhere might not have picked up on this and turned it into a working exploit, as has now happened.  But we do know for sure that Apple apparently didn't give this much thought or concern two years ago, since every one of their Mac M-series chips was vulnerable to exploitation several years later.



Okay.  I'm going to share today's research abstract, today's, the updated current research abstract and introduction, since it's packed with information and some valuable perspective.  And then I'll break it down.  So they wrote:  "Microarchitectural side-channel attacks have shaken the foundations of modern processor design.  The cornerstone defense against these attacks has been to ensure that security-critical programs do not use secret-dependent data addresses.  Put simply, do not pass secrets as addresses, for example, data memory instructions.  Yet the discovery of data memory-dependent prefetchers (DMPs), which turn program data into addresses directly from within the memory system, calls into question whether this approach will continue to remain secure.



"This paper shows that the security threat from DMPs is significantly worse than was previously thought and demonstrates the first end-to-end attacks on security-critical software using the Apple M-series DMPs.  Undergirding our attacks is a new understanding of how DMPs behave which shows, among other things, that the Apple DMP will activate on behalf of any victim program and attempt to 'leak' any cached data that resembles a pointer.  From this understanding, we design a new type of chosen-input attack that uses the DMP to perform end-to-end key extraction on popular constant-time implementations of classical and post-quantum cryptography."



And by way of introduction, they said:  "For over a decade, modern processors have faced a myriad of microarchitectural side-channel attacks, for example, through caches, TLBs" - Translation Lookaside Buffers - "branch predictors, on-chip interconnects, memory management units, speculative execution, voltage/frequency scaling, and more."  You know, as we know, even like the sound of the power supply changing can leak information.



They said:  "The most prominent class of these attacks occurs when the program's memory access pattern becomes dependent on secret data.  For example, cache and TLB side-channel attacks arise when the program's data memory access pattern becomes secret dependent.  Other attacks, for example, those monitoring on-chip interconnects, can be viewed similarly with respect to the program's instruction memory access pattern.  This has led to the development of a wide range of defenses - including the ubiquitous constant-time programming model, information flow-based tracking, and more  all of which seek to prevent secret data from being used as an address to memory/control-flow instructions.



"Recently, however, Augury" - that's what they called their first research two years ago, A-U-G-U-R-Y, and it related to an auger being used - "demonstrated that Apple M-series CPUs undermine this programming model by introducing a Data Memory-Dependent Prefetcher that will attempt to prefetch addresses found in the contents of program memory.  Thus, in theory, Apple's DMP leaks memory contents via cache side channels, even if that memory is never passed as an address to a memory/control-flow instruction."  Okay.  And again, I will explain exactly what all that means.  I've got a couple paragraphs left.



They said:  "Despite the Apple DMP's novel leakage capabilities, its restrictive behavior has prevented it from being used in attacks.  In particular, Augury reported that the DMP only activates in the presence of a rather idiosyncratic program memory access pattern, where the program streams through an array of pointers and architecturally dereferences these pointers.  This access pattern is not typically found in security critical software such as side-channel hardened constant-time code, hence making that code impervious to leakage through the DMP.



"With the DMP's full security implications unclear, in this paper we address the following two questions:  Do DMPs create a critical security threat to high-value software?  And can attacks use DMPs to bypass side-channel countermeasures such as constant-time programming?  This paper answers the above questions in the affirmative, showing how Apple's DMP implementation poses severe risks to the constant-time coding paradigm.  In particular, we demonstrate end-to-end key extraction attacks against four state-of-the-art cryptographic implementations, all deploying constant-time programming."  And just to be clear, when they say "end-to-end attacks," they mean they run something, and they get the key.  Meaning all the work is done, nothing left for the reader to finish.  You know, this thing works.



Okay.  As we've had the occasion to discuss through the years on this podcast, the performance of DRAM, the dynamic RAM memory that forms the bulk of our system's memory, has lagged far behind the memory bandwidth demands of our processors.  Through the years we've been able to significantly increase the density of DRAM, but not its performance.  And as we know, even the increase in density has met with challenges in the form of susceptibility to adjacent row interference which led to the various DRAM hammering attacks.



But on the performance side, the saving grace has been that processor memory access patterns are not linear and non-repetitive.  They are typically highly repetitive.  The programs almost always "loop," meaning that they are executing the same code again and again, over and over.  And that, in turn, means that if a much smaller but much faster "cache" of memory is inserted between the main DRAM and the processor, the processor's repetition of the same instructions, and often the data for those instructions, can be fulfilled much more quickly from the local cache than from main memory.



During our discussions of speculative execution we saw that another way to speed up our processors was to allow the processor to run well ahead of where execution was; and, if the code encountered a fork in the road, in the code's flow, it would fetch ahead down both paths of the fork so that once the path to be taken became known, whichever way that went, the system would already have read the coded instructions for that path and have them ready to execute.  In practice, this is accomplished by breaking our processors into several specialized pieces, one being the prefetch engine whose job it is to keep the execution engines fed with data from main memory.



Many instructions do not make any main memory accesses.  They might be working only within the processor's internal registers or within what's already present in the processor's local cache.  So this gives the prefetching engine time to anticipate where the processor might go next and to guess at what it might need.  In a modern system, there's never any reason to allow main memory to sit idly by, not even for a single cycle.  A good prefetching system will always be working to anticipate its processor's needs and to have already loaded the contents of slower DRAM into the high-speed cache when the processor gets to needing it.



Okay.  Now let's add one additional layer of complexity.  One of the features of all modern processor architectures is the concept of a pointer.  A location in memory or the contents of a register could contain an object's value itself, or instead it could contain the memory address of the object.  In that second case we would say that the value in the memory or register contains, instead of the value of the object itself, a pointer to the object.  As a coder, I cannot imagine my life without pointers.  They are absolutely everywhere in code because they are so useful.



We need one bit of new vocabulary to talk about pointers.  Since a pointer is used to point to or refer to something else, the pointer contains a reference to the object.  So we call the act of following a pointer to the object "dereferencing" the pointer.  We'll see the researchers using that jargon in a minute.  But first let's think about that cache-filling prefetch engine.  Its entire reason for existence is to anticipate the future needs of its processor so that whatever the processor wants will already be waiting for it and instantly available from its cache.  The processor will think that its prefetch engine is magic.



So one evening, probably about seven years ago, some Apple engineers are sitting around a whiteboard with a bunch of half-eaten pizzas.  They're brainstorming ways to further speed up Apple's proprietary silicon.  Given the timeframe, this would first be able to appear in their A14 Bionic processor.  So one of them says:  "You know, we're already doing a great job of fetching the data that the processor is going to ask for.  But when we fetch data that contains what looks like pointers, we're not fetching the data that those pointers are pointing to.  If the data really are pointers, then there's a good chance that once the processor gets its hands on them, it's going to be asking for that data next.  We could anticipate that and have it ready, too, just in case it might be useful.  I mean, what's the whole point of being a prefetching engine; right?  That's the whole point.  That's what we're here for."



Now, at this point, the pizza is forgotten, and several in the group lean forward.  They're thinking about the kinds of cars they're going to be able to get with the raises this idea will earn them.  Then they realize they need to make it work first.  Although they're immediately hooked by the idea because they know there's something there, one of them plays devil's advocate, saying:  "But the cache is context-free."  What he means by that is that the prefetch engine sees everything as data.  It's all the same to it.  The prefetcher doesn't know what the data means.  It has no meaning in DRAM.  It's all just mixed bytes of instructions and data.  It's a hodgepodge.  It's not until that data is fetched from the cache and is actually consumed by the processor that the data acquires context and meaning.



The answer to the "but the cache is context-free" guy is, yeah, and so what?  If some data that's being added to the cache looks like a pointer, and if it's pointing into valid DRAM memory, what's the harm in treating it as a pointer and going out and also grabbing the thing that it might be pointing to?  If we have time, and we're right, it's a win for the processor.  The processor won't believe its luck in already having the thing it was just about to ask for already magically waiting there in its local cache.



So finally, after their last dry-erase marker stops working from the hastily scribbled diagrams on their whiteboards, they're satisfied that they're really onto a useful next-generation optimization.  So one of them asks:  "Okay.  This is good.  But it needs a name.  What are we going to call it?"  One of them says, "Well, how about Data Memory-Dependent Prefetching, or DMP for short?"



So here we've just seen a perfect example of where and how these next-generation features are invented, over pizza and dry-erase markers.  And it's also easy to see that the security implications of this don't even make it onto the radar.  All they're doing, after all, is anticipating a possible future use of what might be a pointer, and prefetching the thing it's pointing to in case they're right, and it is a pointer, and in case the processor might eventually ask for it.  It's disconnected from whatever the processor is doing; right?  It's a Data Memory-Dependent Prefetcher.  What this amounts to is a somewhat smarter prefetcher.  It cannot be certain whether it's fetching a pointer.  But in case it might be, it'll just jump ahead even further to also prefetch the thing that what might be a pointer may be pointing to.



Okay.  So now let's hear from the geniuses who likely also consumed their share of pizza while they scratched the itch that had apparently been lingering with at least three of them for a couple of years, ever since that first bit of work, when they discovered that Apple had dropped this Data Memory-Dependent Prefetcher into their silicon.  Here's how they explain what they came up with.



They said:  "We start by reexamining the findings in Augury.  Here we find that Augury's analysis of the DMP activation model was overly restrictive and missed several DMP activation scenarios.  Through new reverse engineering, we find that the DMP activates on behalf of potentially any program, and attempts to dereference any data brought into cache that resembles a pointer.  This behavior places a significant amount of program data at risk, and eliminates the restrictions reported by prior work.  Finally, going beyond Apple, we confirm the existence of a similar DMP on Intel's latest 13th-generation Raptor Lake architecture with more restrictive activation criteria.  Next, we show how to exploit the DMP to break security-critical software.  We demonstrate the widespread presence of code vulnerable to DMP-aided attacks in state-of-the-art constant-time cryptographic software, spanning classical to post-quantum key exchange and signing algorithms."



Okay.  And then finally, this last bit is the key to everything.  I'll read it first, then take it apart.  They said:  "Our key insight is that while the DMP only dereferences pointers, an attacker can craft program inputs so that when those inputs mix with cryptographic secrets, the resulting intermediate state can be engineered to look like a pointer, if and only if the secret satisfies an attacker-chosen predicate.  For example," they said, "imagine that a program has secret s, takes x as input, and computes and then stores y = s?x (s XORed with x) to its program memory.  The attacker can craft different x's and infer partial or even complete information about s by observing whether the DMP is able to dereference y.  We first use this observation to break the guarantees of a standard constant-time swap primitive recommended for use in cryptographic implementations.  We then show how to break complete cryptographic implementations designed to be secure against chosen-input attacks."



Okay.  So they realized that Apple's DMP technology is far more aggressive than they initially appreciated.  It is busily examining all of the data that's being put into the cache for all of the processes running in the system.  It's looking for anything that looks "pointer-like"; and, when found, it's going out and prefetch that because it may be pointing to something that the process was going to ask for in the future.



Their next step was to realize that, since this "pointer-like" behavior is highly prone to producing false positive hits which would prefetch miscellaneous bogus data, and since it operates indiscriminately on any and all data in the system, they can deliberately trick Apple's DMP system to misfire.  When it does, it will prefetch data that wasn't really being pointed to, and they can use standard, well-understood cache probing to determine whether or not the DMP did in fact misfire and prefetch.  Since the cause of that mixes secrets with what they provide, it reveals information about the secret.



They induce the isolated process containing the secrets to perform a large number of cryptographic operations on their deliberately crafted data while using the now well-understood behavior of the DMP to create an inadvertent side channel that leaks the secret key, even though the cryptographic code itself is being super careful not to behave differently in any way based upon the value of the secret key.  In other words, it's being betrayed by this advanced operation of their prefetching cache.  The code's care doesn't matter because the cryptographic code, as I said, is being betrayed.  What I've just explained is a version of what these very clever researchers revealed to Apple back 107 days ago from last Thursday in early December last year.



So what does Apple do about this?  This does seem like the sort of thing Apple ought to be able to turn off.  One of the things we've learned is that these initial nifty-seeming slick performance optimizations, like Spectre and Meltdown and all the others, always seem to come back to bite us sooner or later.  So the lesson we absolutely as an industry have to take away, and it's surprising we haven't yet, is that anything like this should have an off switch.



And what do you know?  It may have been, and likely was, in reaction to these researchers' initial Augury DMP paper back in 2022 that Apple added that off switch to their M3 chip.  Apple announced it on October 30th last year, the day before  Halloween.  And that M3 can have DMP turned off.  I've heard, but I haven't confirmed, that Apple's own crypto code is flipping DMP off during any and all of their own cryptographic operations.  So it may only be non-Apple crypto code running on Macs that are endangered on M3-based machines.  The researchers cite their compromise of the Diffie-Hellman Key Exchange in OpenSSL, you know, not an Apple library, and the RSA key operations in the Go language library.  So again, not Apple's.



So what about the non-M3 chips, the Apple A14 Bionic, the M1, and M2?  Well, it turns out that these so-called SoC, you know, Systems on a Chip, all have multiple cores, and the cores are not all the same type.  Only half of the cores are vulnerable because only half of them incorporate the DMP.  Apple's M-series have two types of cores:  the bigger Firestorm cores, also known as the performance cores; and the smaller Icestorm cores, also known as the efficiency cores.  On the M1 and M2 chips, only the Firestorm performance cores offer the problematic DMP prefetching system.  So all Apple needs to do is to move their crypto over to the smaller efficiency cores.  Crypto operations will run more slowly there, but they will be completely secure from this trouble.



So is Apple going to do any of these things?  Have they already?  The press thinks that nothing has been done yet.  I find that curious given that the concerns are real and that solutions are available.  But so far all the press has reported - now, again, Apple knew about this in early December.  All the press has reported that Apple has been curiously mute on the subject.  Apple just says "no comment."  This is doubly confounding given that Thursday's research disclosure came as no surprise to them, right, and also that the firestorm of truly over-the-top apoplectic and apocalyptic headlines that have ensued as a result really does need a response.



I imagine that something will be forthcoming from Apple soon.  Until then, for what it's worth, the attack, if it were to happen, would be local, and would be targeted, and would require someone arranging to install malware onto the victim's machine.  It's not the end of the world.  And as I'm always saying around here, anyone can make a mistake.  But Apple's customers would seem to need and deserve more than silence from Apple.  So we ought to hear something.



LEO:  Yeah.



STEVE:  But at least now we understand exactly what's going on.



LEO:  And by the way, if somebody can install that on your system, they could also just put a keystroke logger on there.  There's all sorts of ways they can get full access.  That's, you know, in fact that's probably a lot easier, to do it some other way than a side-channel attack.  Does it take a lot of monitoring and trial and error to have the side-channel...



STEVE:  No.



LEO:  It doesn't.



STEVE:  It takes an hour, and you get the key.



LEO:  Okay.



STEVE:  And so you actually do get a secret that was trying to be protected.



LEO:  So I can see a nation-state saying, oh, good, all right.  What we'll do is we'll get this on there through some other malware exploit.  We'll run it, and then we'll erase all traces. Guy will never know he was hacked.  But we've got the key, and we've got the key forever.



STEVE:  Right.



LEO:  Until he changes it.  I can see that.



STEVE:  Right.  And, yeah, and the point I made was that, you know, when this became public two years ago, these guys apparently stopped their research.  We don't know that the NSA did.  The NSA might have gone, hey, that's interesting.  Let's take a look at that.



LEO:  Oh.  The NSA could - NSA's probably been working on the same thing forever; right?  I mean, they know about these side-channel attacks.  They know about speculative execution.  They know what Spectre and Meltdown produced on the x86 platforms.  I'm sure they were looking for it, too.  Just whose professors are better, I guess.



STEVE:  Yeah.  Hopefully we have good profs.



LEO:  I think we have good profs in the NSA.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#968

DATE:		April 2, 2024

TITLE:		A Cautionary Tale

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-968.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  Why should all Linux users update their systems if they haven't since February?  What do 73 million current and past AT&T customers all have in common?  What additional and welcome, though very different, new features await Signal and Telegram users?  Which major IT supplier has left Russia early?  What did Ghostery's ad blocking profile reveal about Internet users?  Whatever happened with that Incognito Mode lawsuit against Google?  And how are things going in the open source repository world?  And then, after I share something kind of special that happened Sunday involving my wife, SpinRite, and her laptop  and it's probably not what you think  we're going to take a look at another rather horrifying bullet that the Internet dodged again.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  And yes, we are going to talk about one of the most interesting and in many ways scary security flaws on the Internet - and I'm not talking about AT&T, although we'll talk about that, as well - Steve's explanation of the XZ flaw.  We will talk about AT&T.  What's taking them so long in telling us what's going on?  Clearly there's a problem.  There's also a lot more to talk about, including problems with NPM and PyPI.  A great SpinRite success story in Steve's own household.  That, and a lot more.  Oh, and also some scary numbers about how many people use ad blockers.  That and a lot more coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 968, recorded Tuesday, April 2nd, 2024:  A Cautionary Tale.



It's time for Security Now!.  You wait all week for this.  I know you do.  It's a long seven days.  But yes, Tuesday is here again, and so is Steve Gibson, the man in charge of Security Now!.  Hi, Steve.



STEVE GIBSON:  Hello, Leo.	



LEO:  Hello, Steve.



STEVE:  It's great to be back.  Hello, Newman.



LEO:  Hello.



STEVE:  Great to be back with you again, as always.  And you asked me right off the bat, are we going to be talking about XZ.  And I said, "Of course."  So, but I'm, you know, and of course this is what was discovered that almost, I mean, actually I was going to say almost got into the outside world, but actually it did, and there's had to be some rollback.  But I titled today's podcast "A Cautionary Tale" because, you know, we're seeing more problems like this as we're moving forward.



LEO:  Well, you've been talking about supply chain attacks for ages, like PyPI and all of these libraries.



STEVE:  Yup.  And in fact PyPI had a problem.  NPM had a problem.  There's more pressure being turned up.  So anyway, we have a great podcast today.  We're going to look at a different reason why all Linux users need to update their systems if they haven't since February.  What 73 million current and past AT&T customers all have in common.  We're going to answer the question of what additional and welcome, though very different, new features await Signal and Telegram users?  Which major IT supplier just left Russia early last week?  What did Ghostery's ad blocking profile survey reveal about Internet users?



Whatever happened with that Incognito Mode lawsuit that was a class-action suit that actually looks like it did more than generate money for the attorneys?  What happened with that?  And how are things going in the open source repository world, as I already, you know, suggested not well?  And then, after I share something kind of special that happened day before yesterday, on Sunday, involving my wife, SpinRite, and her laptop.  And it's probably not what you think.  We're going to take a look at another rather horrifying bullet that the Internet dodged once again.  But it begs the question, you know, they're coming faster, and the dodge is, like, barely working.  At some point one might hit us.  And what would that be like?



LEO:  It's an amazing story.  I can't wait to hear you talk about it.  And it really is completely serendipitous that we did dodge that bullet because it wasn't even a security researcher who discovered it.  It's just mindboggling how close we came on this one.  And it's inevitable that it's not going to always be that way.



STEVE:  Well, and Leo, had we not - and we'll get into this, of course.  But the bullet that would have hit is serious.



LEO:  Yeah.



STEVE:  I mean, it's like, not like, oh, if you, you know, tap your heels three times, you know, and in some nonstandard configs that almost no one has.  This would have been - it would have slammed the world.



LEO:  Every bit of this story is a novel.  I mean, it's just - there should be a movie about it because it's so mindbogglingly wild.  Anyway, I can't wait to hear you tell it.  Maybe if you do it just right we can option it out and make some money.  The Picture of the Week.



STEVE:  Yeah.  Nobody wants to sponsor this picture.



LEO:  Except maybe Underwriters Lab.



STEVE:  Wow.  Oh.



LEO:  All right.  What is this?  What's going on here?



STEVE:  So this is roughly related to last week's disastrous, let's use coat hanger wire hanging over the prongs of a USB charger to hang our cables.  This is what happens when you're in a room that only has a single one of those European-style, you know...



LEO:  This looks like a prison room, I'll be honest with you.  It doesn't look...



STEVE:  It is a little depressing, yes, I agree.  It's a little horrifying.  So if you, you know, the European-style plug that looks like a vampire has...



LEO:  I think it's in the UK.  It's a UK plug style.



STEVE:  Oh, it's a UK.  Okay.



LEO:  I think so, yeah.  I'm not - no, no, it isn't.  No, you're right, it's EU.  You're right, yeah.



STEVE:  And so it's just two round holes.  And but you've got three things that you're trying to put into the round holes.  So some enterprising individual said, well, okay, I don't have one of those AC expansion boxes.



LEO:  You know what I do have?  I have a coat hanger.



STEVE:  They've got some spare wire.  So suffice to say that this person basically created a physical expansion using some wire successively wrapped around three sets of these two pin plugs, which then...



LEO:  This is not a surge suppressor.  This is a surge aggressor.  This is...



STEVE:  This is a surge disaster.



LEO:  Disaster.



STEVE:  Yeah, not good.  So anyway, just, you know, I mean, this exists in the real world.  I thank our listeners for finding these and saying, "Okay, Steve, I saw this photo and thought of you."  This definitely needs to be something to be shared.



LEO:  Wow, wow.



STEVE:  Do not try this at home.



LEO:  No.



STEVE:  No matter where in Europe you may live.



LEO:  Nightmare.  Nightmare.



STEVE:  Because, yeah.  And needless to say, this is all exposed; right?  You don't want to touch any of this because the whole point of having those two pins is there's really no way you can hurt yourself electrically.  In the U.S., and I'm sure this has happened to you, Leo, back in your earlier days, you'd be pulling a cord out and inadvertently let a thumb touch across the two metal prongs and go [sound], you know, get yourself a little zap.



LEO:  Oh, yeah.



STEVE:  And, you know, it's like, [sound].  I, frankly, know the feeling quite well.



LEO:  Yeah, somebody in our Discord is pointing out that in fact, Chickenhead says, "With great foresight, they've placed it very close to a metal bed frame."  This may be some sort of torture device.  I don't know what's going on.



STEVE:  Not good.  Not good.  Okay.  So Last Friday, which of course was Good Friday, was not a good Friday for Linux.  Not only did the world first learn, which we'll be talking about later, of a near miss on the dissemination of an SSH backdoor - again, this will be the way we wrap up today's podcast.  But the public learned of a widespread elevation of privilege vulnerability affecting the Linux kernel from at least versions 5.14 through 6.6.14.  Running the exploit as a normal user - which was released as a proof of concept on GitHub, I have a link in the show notes if anyone wants to go look - on a vulnerable machine will grant root access allowing the exploiter to do whatever they wish.  Now, the good news, a local-only exploit, so not remote.  This could be used, however, by rogue insiders, or perhaps by malware that's already on a computer, allowing it to cause further damage and problems.



The affected distributions include Debian, Ubuntu, Red Hat, Fedora, and doubtless many other Linuxes.  And it had - I love this - a 99.4% success rate given 1,000 tries.  The author of this tried it a thousand times.  In other words, it worked 994 of those thousand times, and only failed six times.  I noted that the public learned of it last Friday because it was responsibly disclosed - that's the good news - to the Linux community insiders back in January; and then since the end of January updates to this, which was given a CVSS 7.8 flaw, have been rolling out.  You know, the fix was clear and simple, didn't take long to patch or test.



It was very obvious once everyone saw it, it was like, ooh, yeah, that's not what we meant, or we intended.  So it's because the updated code had been available by last Friday for two months that its discoverer felt it was time, enough time had gone by, and the updates had been pushed out for two months, that he was able to disclose it all on GitHub, along with a proof of concept.  So, and boy, he has a super detailed description of what he found.  Again, links to all that in the show notes.



The takeaway for anyone using Linux who may not have updated since before February, meaning that their current build is probably still buggy, is that knowledge of this is now public and widespread.  So if anyone untrusted might have or be able to gain access to any unpatched machines, be a good time now to get them updated because, as I said, the word is out.



TechCrunch has been on top of the news of a significant breach of very sensitive customer data from AT&T.  The story begins five years ago, back in 2019, which TechCrunch first reported 10 days ago on the 22nd of March.  They gave it the headline, and this is the first of two stories because then they updated their coverage.  So this first report they gave the headline "AT&T won't say how its customers' data spilled online."  And in fact that's still the case.  But that's a little more difficult, as we'll see, for them to be denying it.



So TechCrunch explained.  They said:  "Three years after a hacker first teased an alleged massive theft of AT&T customer data, a breach seller this week" - meaning three weeks ago - "dumped the full dataset online.  It contains" - and this is the real concern - "the very personal information of 73 million AT&T customers."



LEO:  Oh, boy.  Including me, I'm sure, yeah.



STEVE:  Yeah, yeah.  And I was an AT&T customer back in the day.  I finally, I had AT&T landlines until all they were ever getting was robocalls.



LEO:  Yeah.



STEVE:  And I thought, oh, come on.  Why am I paying for this anymore?  "So a new analysis of the fully leaked dataset, which contains names, home addresses, phone numbers, Social Security numbers, and dates of birth..."



LEO:  Ooh, everything.



STEVE:  In other words, everything you need for identity theft  "...points to the data being authentic.  Some AT&T customers have confirmed their leaked customer data is accurate.  But AT&T still hasn't said how its customers' data spilled online."  They're like, well, I don't know, you know.  We don't know if it was us.  "The hacker, who first claimed in August of 2021" - which was two years after he got the data - "to have stolen millions of AT&T customers' data, only published a small sample of the leaked records online," hoping to use that as proof.  But the sample was so small that it was a little difficult to verify that, like, a much larger breach was authentic.



"AT&T, the largest phone carrier in the U.S., said back in 2021 that the leaked data 'does not appear to have come from our systems,'" - you know, because we would like it not to appear that way - "but AT&T chose not to speculate as to where the data had originated or whether it was valid."  Now, Troy Hunt, who we know well, a security researcher and owner, TechCrunch wrote, "of data breach notification site Have I Been Pwned, recently obtained a copy of the full leaked dataset.  Hunt concluded the leaked data was real by asking AT&T customers if their leaked records were accurate.  In a blog post analyzing the data, Troy said that of the 73 million leaked records, the data contained 49 million unique email addresses, 44 million Social Security numbers, and customers' dates of birth.



"When reached for comment, AT&T spokesperson Stephen Stokes told TechCrunch in a statement, once again:  'We have no indications of a compromise of our systems.'"  Everything's working great here, folks.



LEO:  Very angering.  That's just awful.



STEVE:  Yeah.  He said:  "We determined in 2021 that the information offered on this online forum did not appear to have come from our systems."  So still just head buried in the sand.  He said:  "This appears to be the same dataset that has been recycled several times on this forum."  Meaning this is nothing new.  We'd prefer to close this statement now.  "The AT&T spokesperson," they said, "did not respond to follow-up emails by TechCrunch asking if the alleged customer data was valid or where its customers' data came from."  So basically, again, just head in the sand.



"As Troy Hunt notes," they wrote, "the source of the breach remains inconclusive.  And it's not clear if AT&T even knows where the data came from.  Hunt said it's plausible that the data originated either from AT&T or 'a third-party processor they use or from another entity altogether that's entirely unrelated.'"  Again, we just don't know, and they're not saying.



TechCrunch said:  "What is clear is that even three years later, we're still no closer to solving this mystery breach, nor can AT&T say how its customers' data ended up online.  Investigating data breaches and leaks takes time.  But by now," they wrote in their conclusion of the first posting, "AT&T should be able to provide a better explanation as to why millions of its customers' data is indeed online for all to see."



Okay.  So that was 10 days ago.  In follow-up reporting just last Saturday, under their headline "AT&T resets account passcodes after millions of customer records leak online" - I guess maybe AT&T, you know, lumbers along, and it just takes them a while to say, oh, maybe this is new - TechCrunch explains.  They said:  "TechCrunch has exclusively learned that phone giant AT&T has reset millions of customer account passcodes after a huge cache of data containing AT&T customer records was dumped online earlier this month.  The U.S. telco giant initiated the passcode mass reset after TechCrunch informed AT&T on Monday that the leaked data contained encrypted passcodes that could be used to access AT&T customer accounts."  So whoops.  It's no longer all just five years ago and we don't know what it is or even if it's actually our data.  Suddenly, whoops, we're going to mass reset passcodes.



"A security researcher who analyzed the leaked data told TechCrunch that the encrypted account passcodes are easy to decipher."  Now, that's not quite correct technically.  I'll explain what happened here in a second.  "TechCrunch alerted AT&T to the security researcher's findings.  In a statement provided Saturday, AT&T said:  'AT&T has launched a robust investigation'" - right, not a moment too soon - "'supported by internal and external cybersecurity experts.'"  Oh, they had their own internal experts.  "'Based on our preliminary analysis, the data set appears to be from 2019 or earlier, impacting approximately 7.6 million current AT&T account holders and approximately 65.4 million former account holders.'"  You know, count me among the former category because I left AT&T.



"The statement also said:  'AT&T does not have evidence of unauthorized access to its systems resulting in exfiltration of the data set.'"  Yeah, five years ago.  Right.  "TechCrunch withheld the publication of this story until AT&T could begin resetting customer account passcodes.  AT&T also has a post on what customers can do to keep their accounts secure.  AT&T customer account passcodes are typically four-digit numbers that are used as an additional layer of security when accessing a customer's account, such as calling AT&T customer service, in retail stores, and online."



LEO:  We've told people, to avoid SIM swapping, set a PIN with your carrier.  That's the PIN; right?



STEVE:  Yup, exactly.  And I know that when I did have occasion to call AT&T, they would say, "What's your PIN?"



LEO:  Right.



STEVE:  And I would tell them what it was, and it was indeed four digits.  So, and that was, you know, not optional.  It was like four, you know, that was all you could do.  So this, writes TechCrunch, "is the first time AT&T has acknowledged that the leaked data belongs to its customers, some three years after a hacker claimed the theft of 73 million AT&T customer records."  Which now has been, you know, even AT&T said, yup, uh, we're not happy with that number, but we have to tell the truth when we're confronted with no choice.  "AT&T had denied a breach of its systems" - and actually they're still saying, eh, well, we don't know how it got out there, but it does look like it's ours.  On Saturday they said:  "It is not yet known whether the data in those fields originated" - wow, they're still denying it - "from AT&T or one of its vendors."



Now, "Security researcher Sam Croley told TechCrunch that each record in the leaked data also contains the AT&T customer's account passcode in an encrypted format.  Croley double-checked his findings by looking up records in the leaked data against AT&T account passcodes known only to him.  Croley said it was not necessary to crack the encryption cipher to unscramble the passcode data."  Why?  We're going to see here in a second.  "He took all of the encrypted passcodes from the 73 million dataset and removed every duplicate.  The result amounted to" - get this - "10,000 unique encrypted values."  In other words, starting at 0000 and finishing at 9999.  That's 10,000.  So they're encrypted.  But after he sorted all 73 million of them and removed duplicates, what do you think he got?  Well, he got encrypted representations of every passcode from 0000 to 9999.



Okay.  So in other words AT&T, we now know, used a fixed cipher with no salt to encrypt the entire dataset, and the encryption never changed.  Every encryption maps one-to-one to some four-digit passcode.  This means that there's no mystery here. TechCrunch wrote:  "According to Croley, the insufficient randomness of the encrypted data means it's possible to guess the customer's four-digit account passcode based on surrounding information in the leaked dataset."



Okay, in other words, since all 73 million users were restricted to using four-digit passcodes, and all passcodes are identically encrypted, once any single user's passcode can be guessed, and we see what that encrypts to, we also know everyone else who chose the same four digits because they will have an identical encrypted code.



LEO:  Because of the lack of salt.



STEVE:  Correct.  Exactly.  It's kind of a mess.



LEO:  Yeah.



STEVE:  TechCrunch wrote:  "It's not uncommon for people to set passcodes, particularly if limited to four digits, that mean something to them.  That might be the last four digits of a Social Security number [which is in the data set], or the person's phone number [in the data set], the year of someone's birth [in the data set], or even the four digits of a house number {in the data set]."



LEO:  By the way, this was AT&T's practice, I think it still is, to set your voicemail passcode to the last four digits of your phone number.  So I'll bet you anything that...



STEVE:  People didn't change it.



LEO:  It's in the data set.  Nobody changed it, yup.



STEVE:  Yup.  "All of this surrounding data is found," they wrote, "in almost every record in the leaked dataset.  By correlating encrypted account passcodes to surrounding account data  such as customer dates of birth, house numbers, partial Social Security numbers, and phone numbers  Croley was able to reverse engineer which encrypted values matched which plaintext passcode."  In other words, zero protection.



"AT&T said it will contact all of the 7.6 million existing customers whose passcodes it just reset, as well as current and former customers whose personal information was compromised."  And that's 76 million in total.  So let's hope that, when they do this, like this passcode reset, they have a new and improved means of encrypting passcodes, hopefully using a unique per-user random salt so that it's no longer possible to create a simple one-to-one mapping of passcodes to their encryptions.  And, you know, doing this without salt is really so last century.



So I haven't, and really don't have much need for cross-platform secure messaging.  iMessage does everything I need.  But as I mentioned a few years ago, the talented PHP programmer Rasmus Vind, who's a listener of ours - he's the guy who helped integrate SQRL into the XenForo web forums that GRC uses.  And by the way, I'm so happy with that choice.  If anyone is looking for forum software, I could not be more pleased with the decision I made many years ago.  Anyway, he suggested that we use Signal to converse while we were working to get all of the details worked out of getting SQRL to log in.  So that exposed me to Signal, and to its somewhat awkward restrictions, which Leo, you and I have talked about from time to time because it's kind of a pain.



So I was interested and pleased to read that Signal is beginning to loosen things up a bit.  We talked about another instance of that loosening I think a week or two ago.  The site SignalUpdateInfo.com, which apparently exists just to watch and report on Signal happenings, they wrote:  "Cloud Backup Status in Development."  They said:  "There has not been any official announcement, but it appears that Signal is currently working on cloud backup for iOS and Android.  This feature would allow you to create cloud backups of your messages and media.  While we have known," they wrote, "about cloud backups since a commit to Signal-iOS on October 20th, 2023, a recent commit to Signal for Android has revealed many more details.  It looks like there could be both free and paid tiers.  The free tier would provide full message and text backups, but only media backups from the last 30 days.  The paid tier could provide full message text and media backups with a storage limit of 1TB.



"From the commit," they wrote, "we also know that Signal backups will be end-to-end encrypted and completely optional, with the ability to delete them at any time.  Cloud backups could significantly improve usability on all platforms by preventing complete data-loss in the event that you lose your phone or encounter some other issue."  And apparently it also suggests that there would be a means for migrating content between devices, which would really be a nice boost in usability.  So, you know, bravo for Signal.  I don't know if they're feeling competitive pressure; if they're feeling like, well, you know, their own users are saying why can't I do X, Y, or Z?  And they're saying, well, yeah, okay, that's probably a good point.  We need to make this better.



On the Telegram side, Telegram users, initially located in Russia, Ukraine, and Belarus, have received a welcome new feature which allows users to restrict who can message them.  Last Wednesday, Telegram's founder, Pavel Durov, sent the following message through Telegram.  Now, when I looked it up, it was in Russian, so I fired up a copy of Chrome in order to bring it up under Chrome and have Google translate it for me.  And I have to say it did a pretty good job.



So the English translation of what Pavel wrote in Russian reads:  "Four days ago, Russian-speaking Telegram users began to complain about messages from strangers calling for terrorist attacks.  Within an hour of receiving such complaints, we applied a number of technical and organizational measures in order to prevent this activity.  As a result, tens of thousands of attempts to send such messages were stopped, and thousands of users participating in this flash mob faced permanent blocking of their Telegram accounts.  From the beginning of next week" - and he wrote this last week, so this week - "all users from Russia, Ukraine, and Belarus will be able to limit the circle of those who can send them private messages.  We are also implementing AI solutions to handle complaints even more efficiently."  And he finishes:  "Telegram is not a place for spam mailings and calls for violence."  And bravo to Google Translate for coming up with that in English for me from the Russian because, you know, that was flawless.



But speaking of Google Translation, when I fired up Chrome to view the page and had it perform the translation, I then copied it and posted the result over to Firefox.  And I just sort of forgot about Chrome.  I left it running.  So there was one page open, doing nothing, yet it caused my machine's fans to spin up to full speed.



LEO:  Yes.



STEVE:  Which I thought, what the heck.  And, you know, when I noticed that, I opened Task Manager and saw that Chrome was at the top of the process utilization list, consuming half of my very strong multi - I think I've got 16 cores or something - while it was doing nothing.  So I had forgotten that it used to do that when I was using it there for a while, before I switched back to Firefox.  And my machine became blessedly silent.  So thanks anyway, Chrome.  Wow.



Also, speaking of Russia, HP has ceased all operations and has exited the Russian market ahead of schedule.  HP had terminated all shipments into Russia two years ago, a little over two years ago, in February of 2022, after Russia's invasion of Ukraine.  Three months later, in May of 2022, HP began the slow process of winding down all of their corporate operations, and had planned to make their final exit next month, in May 2024.  So basically they said, back in 2022, in May, over the next two years, 24 months, we're going to be winding things down.  You know, maybe they hoped they wouldn't have to leave.  They hoped that this mess with Ukraine would get resolved; and, you know, they could stay.  But no, that hasn't happened.



So HP pulled out of Russia last week at the end of March, two months ahead of its planned departure.  The move surprised Russian companies, which are now no longer able to update drivers or contact HP support.  And I have to say I was wondering, what about those printers that tend to be rather persnickety?  I wonder what's going to happen with, you know, if they try to put non-HP ink into an HP printer in Russia, you know, what'll happen.  Who knows?



Okay.  This was - I got a kick out of this next piece because of what we learned about advertisers' use of ad blockers.  Ghostery published what they called their Tracker and Ad Blocker Report, which was the result from research conducted by an independent third-party research firm Censuswide.  It found that, and there are four main bullet points:  Individuals who have experience in advertising, programming, and cybersecurity are significantly more likely to use ad blockers than average Americans.



LEO:  That's hysterical.



STEVE:  Isn't that?  I love it.  Just great.



LEO:  I guess they know something, huh?



STEVE:  Uh-huh.  These industry insiders are more skeptical of their online safety, underscoring concerns about the current severity of user tracking.  Third bullet point:  Americans are underestimating the dangers of health and political tracking.  And finally:  Lesser-known Big Tech players sow distrust among these experts.



Okay.  So I have an infographic that I duplicated from the report.  It shows that, whereas 52% of all Americans use an ad blocker...



LEO:  That number is actually astounding.  That's more than half.



STEVE:  I know.  It is a far higher number than I would have ever guessed.  That's, like, yes, surprising.



LEO:  Cory Doctorow has called this the largest consumer boycott in history.  And I think these numbers bear it out.  That's incredible.



STEVE:  Wow, yeah.  Okay.  So 52% of all Americans.  On the other hand, 66%, so two out of every three, of experienced advertisers block their own ads.  You know?  Or theirs and everybody else's.  And I suppose they don't wish to be tracked, either.



LEO:  Yeah.



STEVE:  Okay.  The infographic divides all of those surveyed into four groups:  everyone - that was that all Americans - everyone; experienced advertisers; experienced programmers, you know, coders; and cybersecurity experts.  And that's also the order of increasing use of ad blockers.



LEO:  Yeah.



STEVE:  Everyone, advertisers, coders, and security experts, with the percentages of use respectively being 52%, 66, 72, and 76%.  So just over three quarters, 76%, of cybersecurity experts surveyed are using ad blockers.



The other interesting question each group was asked was why those who are using ad blockers are doing so.  They were asked to choose among one of four reasons:  for the protection of their online privacy, to block ads, to speed up page loading, or none of the above.  Interestingly, for every group of users, those four groups, the ranking among those four reasons - privacy, freedom from ads, web speed, or other - was identical and in that order.



The only difference was in the distribution among those reasons.  The generic "All Americans" group was the most evenly split between privacy and not wishing to be confronted with ads, at 20% and 18% respectively, and only 9% were using ad blockers for speed.  But among programmers and cybersecurity experts up at the other end, the split was much greater, at 30% wishing for privacy and 19% wanting not to see ads.



Now, with the changes Google has not only been promising but is now well on the way to delivering, with the implementation and enforcement of their Privacy Sandbox technologies in Chrome, the connection between advertising and privacy encroachment is truly being broken for the first time ever.  But it's a significant change that we can expect the recognition of to take quite some time to spread.  And I would argue probably much more time than we even expect.  You know, I've heard some of our listeners who simply don't believe it.  They don't really care about the technology behind it.  For them it's just blah, blah blah, blah blah.  You know, even if they understand it, they're just, you know, they're so jaded that they cannot conceive of a world where we are not implicitly paying for the web by relinquishing our personal information.



On my side, you know, I believe in technology that I understand.  And thanks to this podcast, which forced me to invest in acquiring an understanding of it so that I could share that understanding, I get what Google has wrought. It is good, it is right, and it makes sense.  If it was me doing this, you know, like me doing SQRL, it wouldn't matter how good it was.  But it's not me.  It's Google.  And it's Chrome.  And that means it's going to matter.  It's just going to take a long time, I mean, you know, a long time for the rest of the world to catch up.  And, you know, that's not a bad thing, either; right?  Inertia creates stability, and that's also good.  And all of those techies who have been coding the technology underlying the tracking, you know, they're going to need some time to find new jobs.  So this will give them some time.



As I noted recently, it's the presence of advertising that's financing the web.  And that shows no sign of changing.  Advertisers appear to be willing to pay double when they know that their ads are being well targeted, and "double" can easily make the difference between a website being financially viable and not.  But as the statistics which Ghostery collected showed, everyone knows that the way advertising initially evolved has not been privacy friendly.  So we've needed a long-term solution for giving advertisers the ad targeting that they're willing to pay for, while not trading away our personal information and privacy.



Google's final solution, which amounts to moving all advertising auctioning and ad choice into the user's web browser, I think is brilliant.  It flips everything on its head.  But it's a massive change, and that will not happen overnight.  So we're getting there, but it's going to take a while.



And speaking of Google and privacy, remember that confusion over just exactly how incognito those using Chrome's Incognito Mode really were?  Google's defense was that its users who were upset to learn that they were still being tracked and profiled while using Incognito Mode had simply failed to read the fine print about what, exactly, Incognito Mode did - and mostly did not - protect them from.  Well, it's time to massively delete, it turns out, everything that Google learned about their Chrome users while they believed they were in fact incognito and weren't so much.



The Hacker News just carried this morning a bit of news and what's been learned during the lawsuit discovery, which, you know, many companies try to avoid because, you know, their employees go under oath to be deposed, and the truth comes out; right?  So here's what Hacker News said.



"Google has agreed to purge billions of data records reflecting users' browsing activities to settle a class-action lawsuit that claimed the search giant tracked them without their knowledge or consent in its Chrome browser.  The class action filed back in 2020 alleged the company misled users by tracking their Internet browsing activity while they thought it remained private when using the Incognito or Private mode on web browsers like Chrome.  In late December 2023, it emerged that the company had consented to settle the lawsuit.  The deal is currently pending approval by U.S. District Judge Yvonne Gonzalez Rogers.



"A court filing yesterday (on April Fool's day) said:  'The settlement provides broad relief regardless of any challenges presented by Google's limited record keeping.  Much of the private browsing data in these logs will be deleted in their entirety, including billions of event level data records that reflect class members' private browsing activities.'  As part of the data remediation process," writes the Hacker News, "Google is also required to delete information that makes private browsing data identifiable by redacting data points like IP addresses, generalizing user-agent strings, and remove detailed URLs within a specific website, you know, retain only the domain-level portion of the URL.



"In addition, it has been asked to delete the so-called X-Client-Data header field, which Google described as a Chrome-Variations header that captures the 'state of the installation of Chrome itself, including active variations, as well as server-side experiments that may affect the installation.'  What's significant there is that this header is generated from a seed value, making it potentially" - a random seed - "making it potentially unique enough to identify specific Chrome users."  In other words, there's a serial number in the query headers that Chrome has been using.  Whoops.



"Other settlement terms require Google to block third-party cookies within Chrome's Incognito Mode for five years, a setting the company has already implemented for all users.  The tech company" - meaning Google - "has separately announced plans to eliminate tracking cookies by default by the end of the year."  And of course that's the whole Privacy Sandbox thing.  "Google has since also updated the wording of Incognito Mode as of January of this year to clarify that the setting will not change 'how data is collected by websites you visit and the services they use, including Google.'"  So, you know, they're being more clear and more explicit.



And here's the biggie that came out of the depositions:  "The lawsuit extracted admissions from Google employees that characterized the browser's Incognito browsing mode as 'a confusing mess,' 'effectively a lie,' and 'a problem of professional ethics and basic honesty.'"  Ouch.  "It further laid bare internal exchanges in which executives argued Incognito Mode should not be called 'private' because it risked 'exacerbating known misconceptions.'"



LEO:  Like that it was private.



STEVE:  Oh, yeah, that one.



LEO:  That one.



STEVE:  Right.  Just a little problem.



LEO:  A minor detail.



STEVE:  And finally, we will be talking shortly about the problems of malice in the open source world.  It's a problem everyone is having to deal with.  The NPM repository discovered eight new malicious packages last week, and PyPI was again forced to disable new user account creation and package uploads after being hit by a malware submission wave.



LEO:  Hundreds; right?  It was a ton of them, yeah.



STEVE:  Yes, yes.  And Ubuntu's caretaker, Canonical, has just switched to manual reviews for all apps submitted to the Ubuntu OS app store.  They needed to do this after multiple publishers attempted to upload malicious crypto-wallet applications to the store over the past couple of weeks.  The problem focused enough upon crypto-wallets that they plan to create a separate app submission policy for cryptocurrency wallets going forward.  So yes, it's kind of a mess out there.



LEO:  That's bad.  These people are bad.  I know we're getting to the fabulous story of XZ, but I want to hear what you did for Lorrie the other day. 



STEVE:  Okay.  So day before yesterday my wonderful wife Lorrie had her first experience of what SpinRite can mean for long-term solid-state storage management and maintenance.  It meant a lot to me, so I wanted to share it with our listeners.



Sunday morning she set up a Dell laptop to take a "Q."  That's what we call a QEEG, which is short for Quantitative Electroencephalogram.



LEO:  Wow.



STEVE:  This is the first phase for delivering EEG biofeedback therapy to her clients.  A client sits motionless for 15 minutes with their eyes open, and another 15 minutes with their eyes closed.  They're wearing an elastic cap covered with EEG electrodes recording the surface electrical potentials across the exterior of their skull.  Lorrie does these on Sunday mornings since there's no noise from nearby gardeners, and it's important to have a quiet setting.



While she was getting everything ready on Sunday, she turned the Dell laptop on to boot it up.  And she began waiting.  And waiting.  And waiting.  And I don't recall whether I happened to wander by or she called me over.  But the spinning dots were circling, and the little hard drive light icon was on solid.  She was a little bit impatient finally, and beginning to get a little worried, like she depends upon this, and the people were on their way, and was she going to be able to do this.  I pointed to the hard drive light that was on solid and just told her that, you know, the machine was still working on getting booted up.



But finally, after a truly interminable wait, Windows showed some signs of life.  When the desktop finally appeared, she started moving the cursor around, but the desktop was incomplete.  And I pointed to the little hard drive light again, showing her that, even though the desktop was visible, Windows was still working to finish getting itself ready.  And, you know, those of us who have known Windows for what seems like our entire lives know that Microsoft used to get complaints over how long Windows was taking to boot, so they arranged to put the desktop up, like, immediately, as fast as it possibly could, even though Windows wasn't actually ready.



LEO:  Drives me crazy.  Oh.  So cheap.  So cheap.



STEVE:  I know.  Yeah, exactly.  So after, like, another full minute or so, the rest of the tray icons finally populated, and the drive light began flickering, meaning spending some time being off, as Windows finally began to wrap up its work and get settled.  The program she uses to record EEGs is a true monstrosity.  It was written, unfortunately, by the engineers who designed and built the multi-channel EEG amplifier.  The hardware is a true work of art.  But the software that goes with it, not so much.  So she wondered how long that would take to start up.  She launched it, and again waited and waited and waited.  And it, too, finally showed up.  She was relieved and knew that when the family that she would be working with arrived, she'd be ready.



Having observed this, I said, "It looks like it would be worth running SpinRite on that laptop."  So a few hours later, after a successful session of EEG recording, she brought the laptop to me.  I plugged it in, booted into SpinRite, and set it going on Level 3 to perform a full rewrite/refresh of the drive.  And I had to push past 6.1's new warning screen reminding me that I was going to be rewriting mass storage media that preferred being read to being written.



So I did that and started SpinRite running on Lorrie's EEG recording laptop.  Looking at the Real Time Monitoring display, where the reading and writing cycles are very obvious, most of the time was being spent reading.  And once a 16MB block had been successfully read, rewriting it was just a flicker on the screen.  It has a series of bars, and reading is the top bar.  The final rewrite is the bottom bar.  And so there were little bursts of rapidly alternating reading and writing, but most of the very long pauses were just the SSD sitting there trying to read its own media.



This was a 256GB SSD, and I had looked.  About 89GB of that were in use.  And I noticed that by the time SpinRite was about halfway through, the reading and writing phases were flickering back and forth almost without interruption, with occasional pauses during writing while the SSD buffered, flushed, and rearranged its data.  So by that point I knew that we were through with the region that mattered.  I interrupted SpinRite, removed the little USB boot drive, and rebooted.  And the difference was truly astonishing.  Windows booted right up and was ready in maybe, like, 15 seconds or so.  So I shut it down, moved it back to where the rest of her EEG equipment was that she'd been using earlier.  Before we left the house for Easter dinner, I told her that SpinRite had finished with her machine and invited her to sit down and boot it up.  Needless to say, she was astonished by the difference.  She then launched the EEG recording software; and it, too, started almost immediately.



So, you know, she's put up with me for the past three and a half years, working every day and weekends and nearly every evening, without a break or vacation.  She's taken a few trips to visit friends while I've stayed home to work.  And she's let me because she knows it's what I have wanted to do more than anything else.  But for this work I've done to have made that much difference in the performance and operation of a machine she depends upon, well, it was a gratifying moment.  And, you know, "Yes, honey, your husband's not insane after all."



LEO:  You earned your keep, Steve.  You've got another 10 years.  Good job.



STEVE:  So we discovered this phenomenon early in SpinRite's development after I wrote the ReadSpeed utility, which was  meant as just a platform for testing SpinRite's new drivers.  We know and expected that spinning hard drives transfer data more slowly as we move toward the end of the drive.  But we naturally expected solid-state storage to have uniform performance across its media, and brand new SSDs do.  What we discovered was that SSDs that had been in service for a while had grown astonishingly slow at the front, where files live that are often read but not often rewritten.  You know, that's the operating system, which, you know, has gotten ridiculously large.



What's fascinating to me as an engineer is that the SSD in Lorrie's laptop could have become that slow while still eventually managing to return error-free results.  You know, there were no bad spots found.  It was just - it was astonishingly slow.



LEO:  That's interesting because on spinning drives it's usually there's something bad, it can hardly read the sector and has to try a lot.  But this isn't that.



STEVE:  Well, we know that inside SSDs are analog bit storage cells where each cell contains a voltage that represents some number of binary bits.



LEO:  Right.



STEVE:  Originally, it was just one bit, so the cell was either all on or all off, you know, fully charged or fully discharged, high or low.  There's always pressure to achieve higher density.  But the cells were already tiny.  So designers decided to store four voltages per cell instead of only two voltages.  Since four voltages could be used to represent two binary bits instead of just one, that allowed them to instantly double the storage density for the entire system.  Same number of cells, just store two bits per cell.  But why stop there?  Eight voltages could store three bits; and 16 voltages, god help us, could be used to store four.



Well, there's a well-known problem with SSDs known as "read disturb," where reading from a block of cells subtly disturbs the storage of neighboring cells.  We're witnessing that, as time passes, SSDs gradually require more and more time to perform error correction, or error correction is being used across a greater and greater portion to fulfill a request.  And it may also be that an SSD's error correction is not a fast thing to do since it's assumed not to be needed often.  And it's not a stretch to imagine that, if things are allowed to get bad enough, a solid-state drive will eventually report that it's unable to read and recover the contents of a block of data.  This is why preventative maintenance makes just as much sense, it turns out, for solid-state media as it always has for spinning platter drives.  This was not something that anyone really understood and appreciated until we began working on SpinRite 6.1.



An SSD is not only being restored to "factory fresh" performance, but the reason its performance is restored is that the voltages in the storage cells that had gradually drifted away over time from their original ideal levels will be reset.  That means no more extensive and slow error correction; much faster performance as a result; and, presumably, restored reliability.



So anyway, as I said, it meant a lot to me that Lorrie was able to finally see what I'd been up to for the past three and a half years, and experienced some of the benefit, you know, firsthand.



LEO:  Did she look at you and go, oh, it really works?



STEVE:  She was astonished.  And it's funny, too, because she said, she immediately said, "Oh, we should have taken a before video."



LEO:  Yes.



STEVE:  "So we would have" - and then she said, "Is there any way to put it back?"



LEO:  Oh, no.



STEVE:  And I said, "No, honey.  We'll have to just wait a few years for it to slow down again."  And then, you know...



LEO:  So every, what, every year or so you should do this just to refresh something?



STEVE:  I think so.  Yeah.  I mean, you know, people - we've had people say, like, who are using MacBook Airs, that, like, you know, I'm sure it was faster when it was new.



LEO:  Gotten slower, yeah, yeah, yeah.



STEVE:  Yeah.  It turns out, yes, it actually has.  It's not - we know with an SSD it's not fragmentation because they're solid-state.  It turns out it's actually the SSD itself is spending more and more time doing error correction as the sectors are becoming softer.  The actual bits are softening, and a rewrite is what they need.



So what's exciting is that 6.1 works.  But it's a bit of a blunt instrument.  I mean, it's all we have now.  What I'm excited about is that 7 will be able to find the individual slow spots and surgically selectively rewrite them.



LEO:  Oh, that would be nice.  Yeah.  This is not trim, though.  This is not - everybody in the chatroom is saying, oh, this must be trim.



STEVE:  Nope.



LEO:  No, this is separate from trim.



STEVE:  Nothing to do with trim.



LEO:  Yeah, yeah.  It's just voltage fluctuation and error correction.



STEVE:  Yeah.



LEO:  That makes a lot of sense, actually.



STEVE:  Yeah.



LEO:  So in effect you're just erasing that sector and rewriting it so that it's back to the way it was.



STEVE:  Yes, I am, exactly, I'm putting the data back, but after the error correction.  So the individual bits had drifted.  And error correction was able to - because basically error correction is extra bits.  And so by using the extra bits that are not part of the data, it's possible to, for the entire thing, to still be reconstructed.  But that takes time, which is why reading slows down.  But because SSDs know that they're fatigued from rewriting, they don't do it themselves.  Allyn once told me, you know, Malventano...



LEO:  Yeah, Allyn Malventano, yeah, yeah.



STEVE:  Allyn Malventano told me that no SSDs ever rewrite themselves.  So it takes an external agent to do that.  And so SpinRite serves that function.  Basically, while the sector can still be corrected, it takes the corrected data and rewrites it.  Which basically resets all the bits firmly so that it no longer needs correction.  And thus it can be read much more quickly.



LEO:  it would also probably be accurate to say you don't want to do this too often.  I mean, the reason they don't...



STEVE:  Correct.  Correct.



LEO:  ...they don't do it is they don't want to wear themselves out.



STEVE:  On the other hand, you look at the number of rewrites that SSDs are able to...



LEO:  Nowadays, yeah.



STEVE:  You know, it's hundreds of thousands.  So one a year is not making a big difference.



LEO:  Would that be your recommendation, maybe once a year?



STEVE:  Yes.



LEO:  Or when you see it slowing down, I guess.



STEVE:  Yup.



LEO:  Well, as soon as you get this working for the Mac, let me know.



STEVE:  Will do.



LEO:  Because it's true, these MacBooks tend to slow down over time.



STEVE:  Well, and the one that will work on the Mac will be the UEFI-based.



LEO:  I know, I'm excited.



STEVE:  Because Apple's earlier ones would boot on Macs, on older Macs.  So people who have Macs from like 2008 are able to run SpinRite on them.  It runs on older Macs, but not on new ones.



LEO:  Right, right, right.



STEVE:  Because Apple went pure UEFI.  And it will not run on ARM-based Macs, probably ever.



LEO:  Oh.



STEVE:  So, yeah.



LEO:  Well, that pretty much eliminates everything that's been sold in the last couple of years.



STEVE:  I mean, it is a PC-based maintenance tool.



LEO:  Yeah, it's Intel.  Yeah, yeah.



STEVE:  Yeah, yeah.



LEO:  Because, you know, you write in x86 ASM, there really...



STEVE:  Yea, I am.



LEO:  It's not a high-level language we're talking here.  Okay.  I've been looking forward all week, ever since this XZ vulnerability surfaced, to hear your telling because it's a story with so many layers.  So interesting.  Oh, I've got to turn your microphone on.  Otherwise we won't hear it.  Go ahead.



STEVE:  And it's got some cool technology, too.



LEO:  Yes, it does, yes.



STEVE:  So the runner-up title for today's podcast, which I decided, I settled on "A Cautionary Tale," was "A Close Call for Linux" because it was only due to some very small mistakes made by an otherwise very clever malicious developer that the scheme was discovered.  What was discovered was that by employing a diabolically circuitous route, the system SSH daemon, which is to say the SSH incoming connection accepting server for Linux, would have had a secret and invisible backdoor installed in it that would have then allowed someone, anyone, anywhere, using a specific RSA public key, to authenticate and login to any Linux system on the planet and also provide their own code for it to run.  So to say that this would have been huge hardly does it justice.



Okay, now, to be clear, unlike last week's coverage, I mean, again, I want to put this in the proper context.  Unlike last week's coverage of the GoFetch vulnerability, where Apple was informed 107 days before the public disclosure by a team of responsible researchers, in this case someone actually created this extremely complex and very well hidden backdoor.  And had it not been discovered by chance, due to some small errors made by this malefactor, it would have happened.  This would have happened.  The Linux distros would have been updated - some actually were - and refreshed, and they would have gradually moved out into the world to eventually become widespread.  And this code was so well and cleverly hidden that it might have remained unseen for months or years.



Fortunately, we'll never know.  I imagine it would have been eventually discovered, but who knows what damage might have been done before then.  And imagine the frenzied panic that would have ensued when it was discovered that all builds of Linux with publicly exposed SSH services contained a remotely accessible keyed backdoor.  That's a podcast I would have liked to deliver; but, fortunately, we dodged this one.  



So I went with the title "A Cautionary Tale" because the bigger takeaway lesson here should not be this specific instance, which I'll describe in further detail in a minute.  It should be that there's nothing about this that's necessarily a one-off.  This is the threat and the dark side of a massive community of developers working collectively and largely anonymously.  There's no question that, by far, nearly every developer is well meaning.  I'm certain that's the case.  But the asymmetry of the struggle for security, where we must get every single thing right every time to be secure, and only one mistake needs to be made once to introduce an insecurity, means that, unfortunately, the good guys will always be fighting an uphill battle.



What has just been discovered present in Linux demonstrates that the same asymmetric principle applies to large-scale software development, where just one bad seed, just one sufficiently clever malicious developer, can have an outsized effect upon the security of everything else.  Okay, now, I'm going to give everyone the TL;DR first because this is just so cool and so diabolically clever.



How do you go about hiding malicious code in a highly scrutinized open source environment which worships code in its source form, so that no one can see what you've done?  You focus your efforts upon a compression library project.  Compression library projects contain test files which are used to verify the still-proper behavior of recently modified and recompiled source code.  These compression test files are, and are expected to be, opaque binary blobs.  So you very cleverly arrange to place your malicious binary code into one of the supposedly compressed compression test files for that library, where no one would ever think to look.



I mean, again, one of the points here that I didn't put into the show notes is unfortunately, once something is seen to have been done, people who wouldn't have had this idea originally, you know, wouldn't have the original idea, they're like, oh.  That's interesting.  I wonder what mischief I can get up to?  So we may be seeing more of this in the future.



In their reporting of this, Ars Technica interviewed Will Dormann, who we've referred to before in the past.  He's a senior vulnerability analyst at the security firm Analygence, A-N-A-L-Y-G-E-N-C-E, Analygence.  Will noted that because the backdoor was discovered before the malicious versions of XZ Utils were added to production versions of Linux, he said:  "It's not really affecting anyone in the real world.  BUT [in all caps] that's only because it was discovered early due to bad actor sloppiness.  Had it not been discovered, it would have been catastrophic to the world."



Okay.  So all of this erupted last Friday, on Good Friday, with a posting to the oss-security list by the Microsoftie who stumbled upon this, Andres Freund.  As a consequence of this being a huge deal, security firms and developers all plowed into and reverse engineered all of the available evidence.  Now, I'm sure that Ars Technica felt last Friday, being a news outlet, that they needed to say something.  So Dan Goodin wrote what he could, given the at the time limited amount of available information.  But then, late Sunday morning, Dan published a second piece that did a far better job of pulling this entire escapade together and characterizing what's known, as well as some of the intriguing back story.  And actually Kevin Beaumont, who posts on Mastodon now from his GossiTheDog profile, also had something really great that I'll share.  Anyway, I've edited from Dan's second posting, so here's what we know thanks to Ars Technica and Dan's update.



He wrote:  "On Friday, a lone Microsoft developer rocked the world when he revealed a backdoor had been intentionally planted in XZ Utils, an open source data compression library available on almost all installations of Linux and other Unix-like operating systems.  The person or people behind this project likely spent years on it.  They were likely very close to seeing the backdoor update merged into Debian and Red Hat, the two biggest distributions of Linux, when an eagle-eyed software developer spotted something fishy.



"Software and crypto engineer Filippo Valsorda said of the effort, which came frightfully close to succeeding:  'This might be the best executed supply chain attack we've seen described in the open, and it's a nightmare scenario:  malicious, competent, authorized, upstream in a widely used library.'"



Dan writes:  "XZ Utils is nearly ubiquitous in Linux.  It provides lossless data compression on virtually all Unix-like operating systems, including Linux.  XZ Utils provides critical functions for compressing and decompressing data during all kinds of operations.  XZ Utils also supports the legacy .lzma format, making this component even more crucial.



"So what happened?  Andres Freund, a developer and engineer working on Microsoft's PostgreSQL offerings, was recently troubleshooting performance problems a Debian system was experiencing with SSH, the most widely used protocol for remotely logging into devices over the Internet.  Specifically, SSH logins were consuming too many CPU cycles and were generating errors with valgrind, a utility for monitoring computer memory.  Through sheer luck and Freund's careful eye, he eventually discovered the problems were the result of updates that had been made to XZ Utils.  On Friday, Freund took to the Open Source Security List to disclose that the updates were the result of someone intentionally planting a backdoor into the compression software.



"It's hard to overstate the complexity of the social engineering and the inner workings of the backdoor.  Thomas Roccia, a researcher at Microsoft, published a graphic on Mastodon that helps visualize the sprawling extent of the nearly successful endeavor to spread a backdoor with a reach that would have dwarfed the SolarWinds event of 2020."  And for anyone who's interested, I've reproduced the infographic on page 15 of the show notes. 



So what does the backdoor do?  "Malicious code added to XZ Utils versions 5.6.0 and 5.6.1 modified the way the software functions.  The backdoor manipulated sshd, the executable file used to make remote SSH connections.  Anyone in possession of a predetermined encryption key could stash any code of their choice in an SSH login certificate, upload it, and execute it on the backdoored device.  No one has actually seen code uploaded, so it's not known what code the attacker planned to run.  In theory, the code could allow for just about anything, including stealing encryption keys or installing malware."



Answering the question about how a compression utility can manipulate any process as security-sensitive as SSH, Dan explains:  "Any library can tamper with the inner workings of any executable it is linked to.  Often the developer of the executable will establish a link to a library that's needed for it to work properly.  OpenSSH, the most popular sshd implementation, does not link the liblzma library, but Debian and many other Linux distributions add a patch to link sshd to systemd, a program that loads a variety of services during the system boot-up.  And systemd, in turn, links to liblzma, and this allows XZ Utils to exert control over sshd."



Okay.  And then of course everyone wants to know how this all happened.  Dan writes - and this should give everyone some chills, and we'll get more into this with what Kevin wrote.  Dan said:  "It would appear that this backdoor was years in the making.  In 2021, someone with the username JiaT75" - and I'll just refer them as "Ji" - "made their first known commit to an open source project.  In retrospect, the change to the libarchive project is suspicious because it replaced the safe_fprint function with a variant that has long been recognized as less secure.  No one noticed at the time.



"The following year, JiaT75 submitted a patch over the XZ Utils mailing list; and, almost immediately, a never-before-seen participant named Jigar Kumar joined the discussion and argued that Lasse Collin, the longtime maintainer of XZ Utils, had not been updating the software often or fast enough.  Kumar, with the support of Dennis Ens" - also never before seen and never since - "and several other people who had never had a presence on the list and were never seen again, pressured Lasse Collin to bring on an additional developer to maintain the project.



"In January 2023, JiaT75 made their first commit to XZ Utils.  In the months following, JiaT75, who used the name Jia Tan, became increasingly involved in XZ Utils affairs.  For instance, Tan replaced Collin's contact information with their own on oss-fuzz, a project that scans open source software for vulnerabilities that can be exploited.  Tan also requested that oss-fuzz disable the ifunc function during testing, a change that prevented it from detecting the malicious changes Tan would soon make to XZ Utils."



Okay, now, I'll just note that that sort of request is not unusual in itself.  It's a bit like a website using the /robots.txt file to keep spiders out of places where they might cause some damage to the site or get themselves hopelessly lost and tangled up.  You know, it's possible that aggressive random fuzzing, which is hoping to detect unknown problems, might inadvertently trigger a known deliberate problem and behavior.  So asking for fuzzing exceptions is not inherently suspicious.  In this case, the perpetrator did have an ulterior motive, as it turned out.



So Dan finishes:  "In February of this year, Tan issued commits for versions 5.6.0 and 5.6.1 of XZ Utils.  The updates implemented the backdoor.  In the following weeks, Tan or others appealed to developers of Ubuntu, Red Hat, and Debian to merge the updates into their OSes.  Eventually, according to security firm Tenable, one of the two updates" - both of which had the backdoors - "made their way into Fedora Rawhide; Fedora 41; the Debian testing, unstable and experimental distros versions 5.5.1alpha-0.1 to 5.6.1-1; openSUSE, both Tumbleweed and openSUSE MicroOS; and Kali Linux."  So it was well on the way to going into wide distribution.



Also, additional reporting and research has found that Arch Linux, Alpine Linux, Gentoo, Slackware, PCLinuxOS, and others were also infected.  However, this was all recent enough that most of the affected distros were still in their experimental / unstable releases, so the malicious code did not make it into any widespread production systems.  The macOS Homebrew package manager and OpenWRT router firmware also included backdoored XZ Utils versions.



Some other reporting into the deeper back story and various motivations wrote:  "All of this started two years ago, in April 2022.  The attacker contributed code to XZ Utils, gradually built his reputation and trust over time, and used various sock puppet accounts to pressure XZ Utils project's author Lasse Collin to add them as one of the project's maintainers.  At the time, Collin was burnt out and dealing with mental health issues and welcomed the help since they were a single developer on a highly popular project."



And as I said, I want to share one more piece of writing about this, a wonderful narrative by, oh, it was by Michal Zalewski.  He's a Polish security expert, white hat hacker from Poland, a former Google employee.  We've quoted him in the past.  He's currently the VP of Security Engineering at Snap, Inc.  So here's Michal's take on this most recent near-catastrophic misadventure.



In his posting Sunday, which he titled "Techies vs Spies:  The XZ Backdoor Debate - diving into some of the dynamics and the interpretations of the brazen ploy to subvert the liblzma compression library," Michal wrote:  "Well, we just witnessed one of the most daring infosec capers of my career.  Here's what we know so far.  Some time ago, an unknown party evidently noticed that liblzma, aka XZ  a relatively obscure open source compression library  was a dependency of OpenSSH, a security-critical remote admin tool used to manage millions of servers around the world.  This dependency existed, not because of a deliberate design decision by the developers of OpenSSH, but because of a kludge added by some Linux distributions to integrate the tool with the operating system's newfangled orchestration service, systemd.



"Equipped with this knowledge about XZ, the aforementioned party probably invented the persona of Jia Tan, a developer with no prior online footprint who materialized out of the blue in October of 2021 and started making helpful contributions to the library.  Up to that point, XZ had a single maintainer, Lasse Collin, who was dealing with health issues and was falling behind.  Shortly after the arrival of Jia, several apparent sock puppet accounts showed up and started pressuring Lasse to pass the baton.  It seems that he relented at some point in 2023.



"Since then, Jia diligently contributed the maintenance work, culminating in February 2024 with the seamless introduction of a sophisticated, well-concealed backdoor tucked inside one of the build scripts.  Full analysis of the payload is still pending, but it appears to have targeted the pre-authentication crypto functions of OpenSSH.  It's probably safe to assume that it added 'master key' functionality to let the attackers access all affected servers at will."  And I'll note that's exactly what was, indeed, being discovered over the weekend while Michal was writing this.  The exploit provides for remote code execution with root privileges.



Michal continues:  "Some time after getting the backdoor in, Jia  along with a new cast of sock puppet accounts  started pinging Linux distro maintainers to have the backdoored library packaged and distributed to end users.  The scheme worked until Andres Freund, a PostgreSQL developer in the employ of Microsoft, reportedly decided to investigate some unexpected SSH latencies caused by a minor bug in the backdoor code.



"If," he says, "this entire exploit timeline is correct, it's not the modus operandi of a hobbyist.  In today's world, if you have the technical chops and the patience to pull this off, you can easily land a job that would set you for life without risking any prison time.  It's true that we also have some brilliant folks with sociopathic tendencies and poor impulse control; but almost by definition, such black hat groups seek instant gratification and don't plan major heists years in advance.  In other words, all signs point to this being a professional, for-pay operation, and it wouldn't be surprising if it was paid for by a state actor."



So as we might imagine, this has really shaken the entire Unix open source - I'm sorry, the entire Linux open source community because everyone understands just how close these malicious modifications came to being incorporated into all mainstream Linux distributions and then filtering out into the world.  Were it not for Andres Freund happening to wonder why SSH latencies were slightly higher than expected, in what was referred to by someone as a micro-benchmarking, you know, how much farther might this have gone.



Kevin Beaumont posted from his cyberplace.social Mastodon account.  He said:  "Before everyone high-fives each other, this is how the backdoor was found:  Somebody happened to look at why CPU usage had increased in sshd and did all the research and notification work themselves.  By this point, the backdoor had been there for a month unnoticed."  He said:  "I've made the joke before that, if GCHQ aren't introducing backdoors and vulns into open source, that I want a tax refund.  It wasn't a joke.  And it won't be just GCHQ doing it."



So all of this begs the question, what will be next?  And will someone catch that one, too, before it gets loose?  Lately, we appear to be dodging more bullets, which are coming more often.  In fact, our recent Episode 962 from February 20th was titled "The Internet Dodged a Bullet," when researchers stumbled over a way of bringing DNS to its knees.  If I hadn't used that title just six weeks ago, I probably would have used it today because we did, again, just dodge another bullet.



LEO:  Amazing.



STEVE:  Given the inherently precarious nature of security, and that we appear to be dodging more bullets recently, I won't be surprised if one comes along that we don't dodge in time.  I wonder what lessons we'll learn from that?  Fortunately, we'll be here.  Stay tuned.



LEO:  It's probably - I'm thinking it's not far off, in the next year.



STEVE:  That's my feeling, too, Leo.  That's why I say there is so much pressure now.



LEO:  And I think it's probably a nation-state that was going to use this in a targeted fashion against specific...



STEVE:  Oh, Leo, whoa.



LEO:  ...people that it wanted.  Not us, probably.  But doesn't mean...



STEVE:  You mean not you and me. 



LEO:  Right.  But, you know, dissidents, journalists, you know, people the state doesn't like.  And so it's not a general threat to all of us, but it is a serious threat.  And I do hope they - there just doesn't seem much will or maybe even ability to check  these repositories.



STEVE:  Well, and in this case, you know, the fact is the library did not need updating.  There was a ton of...



LEO:  That's a red flag, yeah.



STEVE:  ...of very old code.



LEO:  Right.  Yeah, sometimes it's fixed, it's done.



STEVE:  It's done.  It's finished.  It's like RTOS32 which I bought from that guy when he was going to take it off the market; you know?  He hadn't been able to sell licenses because there were no more bugs.



LEO:  Right.



STEVE:  And so...



LEO:  It was done.



STEVE:  ...it was like, oh, well, you know, no more revenue from fixes.  So I said thank you very much.



LEO:  The proud owner of his own operating system, ladies and gentlemen.  We're going to call it, I don't know, GDOS, Gibson DOS, something like that.  Steve Gibson.



STEVE:  Not GIBDOS.



LEO:  GIBDOS.



STEVE:  DOSGIB.



LEO:  DOSGIB.  He is at GRC.com, the Gibson Research Corporation.  That's where you'll find the latest version of SpinRite 6.1.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#969

DATE:		April 9, 2024

TITLE:		Minimum Viable Secure Product

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-969.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  When is it far better for a security researcher to just keep their mouth shut?  Are all Internet-based secure note exchanging sites created equal?  What's been happening in the lucrative and slimy world of zero-days for pay?  And what has NASA just learned about the state of Voyager 1?  Something momentous has happened with SpinRite, and we're going to take a deep dive into an important industry initiative that just acquired an important new contributor.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We've got a great show planned for you.  Coming up we're going to say hello to V'Ger, Voyager 1.  There's some really interesting news in attempting to reestablish communications.  Why is it better sometimes for a security researcher to keep their mouth shut?  Steve has some strong words.  And we'll talk about the slimy world of zero-days for pay.  Turns out you can make a lot of money if your ethics are a little, you know, fuzzy.  All that and more, coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 969, recorded Tuesday, April 9th, 2024:  Minimum Viable Secure Product.



It's time for Security Now!, oh, yes, once a week, a must-listen, Steve Gibson and the latest security news.  Hi, Steve.



STEVE GIBSON:  Hey, Leo.  Great to be with you again for another, well, we've got a bunch of really cool stuff to talk about.



LEO:  Oh, good.	



STEVE:  The podcast does not have a particularly catchy title.  I guess I would argue that the podcast that we titled "1" was probably...



LEO:  Was catchy, yeah.



STEVE:  That was catchy and pithy and all that.



LEO:  Pithy.



STEVE:  This one is Minimum Viable Secure Product.  Which I had fun with it, mostly because it is so dry.  But it's also very important.  So that's how we're going to - we're going to do a close look at an important industry initiative, which is what this is, titled unfortunately - you can just say MVSP.  That's, like, better, right, MVSP.  But it's Minimum Viable Secure Product, which just acquired an important new contributor.



But first we're going to look at when is it far better for a security researcher to just keep their mouth shut, and what happened in this case when he didn't?  Are all Internet-based secure note exchanging sites created equal?  What's been happening in the lucrative, if slimy, world of zero-days for pay?  And what has NASA just learned about the state of Voyager 1?  That little intrepid puppy just - oh, and by the way, Leo, I watched the documentary you referred to, "It's Quieter in the Twilight"?



LEO:  Oh, isn't that a great - all about V'Ger?  I loved that.



STEVE:  It was really, really fun, yes.  Also I will tell everybody that something momentous has happened with SpinRite, and then we'll take our deep dive into the fascinating - actually it is because it's going to make a lot of our listeners smile, Minimum Viable Secure Product, because it encapsulates the history of this podcast in many senses.  So I think - and of course a great Picture of the Week.  So for Podcast 969 for  April 9th, here we go.



LEO:  Love it.  Here we go.  969 is on the air.



STEVE:  We have a picture.  So I've had this one for a while.  I've mentioned having a bunch that were in the queue.  I got a kick out of this one because - so we have a one-story building, sort of it's like the side of maybe it would be a strip mall or something, where you have, you know, the need for roof access, but it's in sort of a public environment, so you just don't want random people climbing up on the roof; right?



LEO:  Climbing up, yeah.



STEVE:  Yet you still need, like when one of the AC units dies or something happens, you need service people to get up.  So you've got a ladder running up the side.  Now, okay.  How do you lock the ladder?



LEO:  [Laughing]



STEVE:  And you've just seen it.



LEO:  Yes.  Go ahead.  So this is clever, in a way.



STEVE:  Uh-huh.  So how do you lock the ladder?  You know?  Ladder's got rungs.  I know.  It's great.  So some clever individual figured, okay, we're going to put a hinged sheet metal panel, like across the front of this ladder.  And we're going to lock it with a padlock, and only the manager of the complex has the key for this padlock.



LEO:  Brilliant.  Brilliant.



STEVE:  And, yeah.  So, like, so basically it's completely closing off the rungs of the ladder.



LEO:  Yeah.



STEVE:  So, you know, you can't climb up a smooth surface.



LEO:  No.



STEVE:  It just, okay.



LEO:  Can you?  No.



STEVE:  Now, unfortunately, the ladder is mounted to the side of the building with a set of rungs.



LEO:  That are exactly ladder-like.



STEVE:  So, gee.  If I wanted to climb up on the roof, how would I do it?  Well, I'd just go up the side of the ladder where the rungs are mounted to the side of the building, until I get above the sheet metal sheet.  Then I can switch over and climb up the rest of the ladder.  So...



LEO:  Wow.  Well, they handily give you a ladder on the ladder, which is great.



STEVE:  Yeah.  We have here a failure of imagination.



LEO:  Or malicious compliance; you know?  The guy who installed it said, I know this is useless, but that's what they're paying me to put in, so...



STEVE:  This will serve you right, folks.



LEO:  Serves him right.



STEVE:  Yes.  Thank god it's got a padlock, that's all I can say.



LEO:  That's hysterical.  Oh my god.  



STEVE:  Wow.  Okay.  So what year is it, Leo?  It's 2024; right?



LEO:  2024, I believe, yes, sir, yes.



STEVE:  Okay.  It's still the case that publicly accessible, Internet-connected, high-profile devices are being found to have manufacturer hardcoded remote access credentials.  Now, I suppose that would be more difficult for us to believe if it wasn't so long ago, like it wasn't that long ago is what I meant, that Cisco's own internal audit of their devices, which was spurred - and we covered this on the podcast - by repeated discovery of their own hard-coded credentials in their equipment, kept turning up instance after instance of the same thing.  So like it's not shocking because, you know, here even the market leader at the high-end enterprise-grade networking gear was suffering from the inertia of the way they'd always done things before.  Even a decade after, it had really no longer been safe to do that.



Okay.  So it's taken some time.  But as we've been discussing more recently, we finally have - we're kind of coming to an agreement, and we're going to see more of this agreement here at the end of the podcast, on a set of straightforward and widely recognized best practices.  So I've just put out five of them here.



No default manufacturer-set credentials of any sort, anywhere.  Right?  The first time a device's firmware boots, it should see that its credentials are blank.  After it's had the chance to generate sufficient internal entropy for random number generation, it should create a very strong new default credential from scratch and present it to its user.  Now, the user will be free to then change it to whatever they want.  But by default, any newly installed device will give its own security a head start by defaulting to something strong.  And nothing from the manufacturer.



You know, yes, if it makes technical support more difficult, that's just tough.  Once upon a time it might have been fine to default the username and password to "admin" and "admin," and to print that on the quick start guide.  But those days are long past.  All of our listeners have seen those days.  Security is inherently porous, and the pressure against our porous security is now steadily on the rise.  That's the other thing that it is like so clear now.  Okay.  So first, nothing by default.



Second, and this was one of the new things that has surfaced recently, proposed by CISA, that I really think is right, and that is physical access required.  Changing the security of anything in any dangerous direction, like turning on UPnP, which should certainly not be on by default, including that administrative username and password we were just talking about, must be accompanied by a physical button press, either beforehand to enable admin access for some period of time, or afterward to confirm the application of the new settings into the current setup.



And yes, again, that will make fully remote admin impossible.  But it will also make fully remote admin by bad guys impossible.  Right?  And we've seen over and over again that it's not actually possible to have one without the other.  So yes, it'll be inconvenient.  Yes, you may have to call down to the basement and tell Morris to press the button now because you want to apply these settings from the comfort of your office on the 20th floor.  Fine.  Do what has to be done.  But it will keep the bad guys out.



Number three, only absolute minimum functions publicly exposed by default.  A router, for example, must have its WAN interface publicly exposed to be at all useful as a router.  But it does not need a single additional service beyond that.  So therefore not one additional service should be bound to that interface, the WAN interface, without explicit manual enabling accompanied by clear caution notices and, yes, pressing a physical button on the router.  And needless to say, when that service is instantiated, it will start off with a fully random and strong username and password.  You know, we know how to do this now.



Four, autonomous firmware updates received and applied by default.  All connected devices must ship from the factory with their auto-firmware-update system enabled by default.  Again, it could be turned off.  But it should, everything should default to most secure.  These devices should then periodically ping a factory server to check for the availability of new firmware.  The router's ping should include its current firmware version, and the cryptographically signed ping reply should provide the router with the current version and the urgency associated with moving from where it said it is to the latest today.  The device's admin can decide which levels of urgency are permitted to auto-update.  Maybe you want to back off and only allow the ultra super emergent updates to happen immediately.  Otherwise they might need management oversight, or they might  be deferred until 2:00 a.m., whatever.  So there could be administration over that, but the system ought to take care of itself.



And, last, firmware for devices must be maintained as long as they're in service.  And this is probably the trickiest of the five because you have to then solve the problem of how long should a supplier be reasonably responsible for making the firmware of an end-of-life and out-of-service device current?  You know, this is a decision that is reasonably up to each supplier.  But because it affects the buying decisions, whatever the supplier's decision may be, it must be clearly and publicly stated so that prospective purchasers can plan accordingly.



Okay.  So we have five fundamental principles:  No default credentials, physical access required to apply any configuration changes that might be dangerous, no unnecessary services enabled, auto-firmware updates enabled by default, and a clearly stated end-of-life ongoing support commitment.  So if you think back through all of the individual events, many of them having significantly catastrophic consequences that we've covered through this podcast's 19 years, almost every one of them could have been prevented if these five fundamental principles of safe Internet connectivity had been in place.



So today, as luck would have it, we have news of yet another:  A researcher discovered that D-Link Network Attached Storage, NAS models DNS-320L, 325, 327L, and 340L all share a distinguishing and, I don't know if it's horrifying or it's disappointing.  Anyway, he found that the firmware they share had been hard coded by D-Link with the same, fixed, remote access backdoor username and password.  And even the term "password" is being generous here because it's left blank.  His GitLab handle is NetSecFish, obviously as in "Network Security Fish," and his page on GitHub says:  "The described vulnerability affects multiple D-Link NAS devices," and then he enumerates those four.



"The vulnerability lies within the nas_sharing.cgi URL, which is vulnerable due to two main issues:  a backdoor facilitated by hard-coded credentials, and a command injection vulnerability via the 'system' parameter.  This exploitation could lead to arbitrary command execution on the affected D-Link NAS devices, granting attackers potential access to sensitive information, system configuration alteration, or denial of service by specifying a command."



Okay.  This guy's subsequent Internet search revealed 92,589 of these devices currently exposed on the public Internet.



LEO:  Oh, man.



STEVE:  Yes, 92,589, right now on the Internet, with the highest concentration appearing at IP addresses in the U.S.  The big problem is that this family of D-Link NAS devices were first phased out of D-Link's product line on October 29th, 2017.  And two years later, on the same day in October 2019, all support for them was terminated by D-Link.  Even so, it appears that our Network Security Fish gave D-Link no prior notice of his discovery before he published it, no opportunity to decide whether they wished to alter their standing policy for out-of-service-life products.  Two weeks ago, on March 26th, a date which I'm fond of because it's the day I was born...



LEO:  And Leonard Nimoy.



STEVE:  Leonard Nimoy and I.  He simply published everything he knew about this.  Today, his public GitHub page thoroughly documents - today and for the last week, or the last two weeks, publically documents how a simple HTTP GET query, containing the username "messagebus" and a blank password, can be used to cause any of these 92,589 currently online network attached storage devices to execute arbitrary commands.  We see from the packet capture he enclosed that he issued - I have it in the show notes.  He issued a command to a machine at a redacted address - and even doing that would be controversial unless the machine was one he owned, which seems unlikely - a command to which the machine replied with a valid HTTP 200 OK response.  It included other reply headers and XML in the body of its reply.  So from that we learn that the D-Link NAS devices are running the "lighttpd" web server v1.4.28.



Now, you know, I'm unhappy with what D-Link has been found to have done.  We'll get to them in a minute.  But it's not clear to me that anyone is helped by Fish's public disclosure, and that 92,589 people and their networks and their data stand to be attacked and compromised as a result.  You know, before he made this needless public disclosure, it might have been that someone might have eventually stumbled onto this, too.  And it's true that we don't know that it hasn't happened already.  But we do know that it was never public before, and that now it is.



And another lesson we've learned through the years of shared experience on this podcast is that the height from which the fruit is hanging matters a lot.  Far more low-hanging fruit is plucked than high-hanging fruit.  There is an endless supply of script kiddies able to issue a series of simple HTTP GET queries.  And how are any of them going to be able to resist any target as tempting as this?  This is the definition of a script kiddie-class vulnerability.  So this person's disclosure two weeks ago has, without any question, compromised the security of 92,589 still online, still functioning, still in use, and still sitting on other people's networks, D-Link network attached storage devices.



At the same time, a fair question is "What would Google do?"  And by "Google," I'm referring to Google's Project Zero with their famous 90-day disclosure policy.  Now, in fairness, he didn't give D-Link even a one-day.  This was a zero-day disclosure.  But in the case of Google with Project Zero, you know, they give manufacturers 90 days to produce a patch for a discovered flaw before Google will release the flaw's technical details.  And as we know, many companies have felt significant pressure to fix a vulnerability that Project Zero had found in their products.



But in this instance, the answer to the question "what Google would do" is that I'm not 100% sure.  For one thing, these are long-out-of-service devices that are no longer being maintained by their supplier.  And we know that even Google has, like, known vulnerabilities in Android that is no longer being maintained on Android smartphones.  And it's like, well, sorry, you know, we're not patching that version of Android any longer.  So that's a policy.  But the other thing is this is not actually a bug.  This is an undocumented deliberate backdoor that D-Link embedded into their devices, their consumer network NAS devices, for some unclear purpose.  So because it's old, out of service, and not a bug, I'm pretty certain that Google's Project Zero would never even consider taking this up in the first place.  And if they knew about it, they would just be mum.



So this brings us back to D-Link, a well-known, reputable, popular, Taiwanese network hardware manufacturer that's been around since 1986.  I've purchased a bunch of D-Link equipment, mostly network hubs, I think, through the years.  So I suppose that at least among those of us here and others who learn of this past behavior, this at least tarnishes their brand in our mind.  Right?  I'm like, I'm not buying a D-Link NAS now when I know that for some reason they've got this undocumented backdoor.  They did.  We don't know about them today.  



So with devices that were discontinued six and a half years ago, that went out of support four and a half years ago, we would have to assume that they have, even if they wanted to, no way of remotely updating their firmware.  You know, that's like a thing that we're beginning to see now on new devices because it's been determined that that's the best practice.  If the owners of those devices had registered them, well, then, D-Link could conceivably today reach out to them.  But what would they say?  "Uh, you know that NAS that you purchased from us 10 years ago?  Well, uh, we had planted a secret remote access backdoor into it, and it was recently discovered and has now been made public.  So, you know, we're really sorry about that, and you should probably definitely immediately disconnect your D-Link device from the Internet.  Have a nice day.  And would you like to consider buying a newer NAS from us?  We have a bunch of nice shiny ones for you to consider."  Well, right.  So they're not going to say anything.



And I don't know that they're even legally vulnerable here.  So it's not clear that anything could be done on the legal front.  This was a design decision made by D-Link.  Bad, in retrospect, but there it is.  And they may have some justifiable rationale for it.  The username "messagebus" kind of suggests that perhaps these devices could be set up in a cluster, where this "messagebus" allowed them to interoperate in some fashion.  So, you know, should have never been put on the public Internet.  Maybe that was a mistake.  Maybe it's only meant to be on the LAN, and somehow that wasn't done right.  In other words, bad design not to control who could use it on which interface, but it wasn't an obvious crime on D-Link's part.



And speaking of crime, here it comes.  Yesterday, entirely predictably, Ars Technica's headline read "Critical takeover vulnerabilities in 92,000 D-Link devices under active exploitation."  Now, that didn't take long.  Ars wrote:  "On Monday," meaning yesterday, "researchers said their sensors began detecting active attempts to exploit the vulnerabilities starting over the weekend.  GreyNoise, one of the organizations reporting the in-the-wild exploitation, said in an email that the activity began around 02:17 UTC on Sunday.



"The attacks attempted to download and install one of several pieces of malware on vulnerable devices depending on their specific hardware profile.  One such piece of malware is flagged under various names by 40" - four zero - "endpoint protection services.  "Security organization Shadowserver has also reported seeing scanning or exploits from multiple IP addresses, but did not provide additional details.



"The vulnerability pair, found in the nas_sharing.cgi programming interface of the vulnerable devices, provide an ideal recipe for remote takeover.  The first, tracked as CVE-2024-3272 and carrying a severity rating of 9.8 out of 10, is a backdoor account enabled by credentials hardcoded into the firmware.  The second is a command-injection flaw tracked as CVE-2024-3273 with a severity rating of 7.3.  It can be remotely activated with a simple HTTP GET request.  NetSecFish, the researcher who disclosed the vulnerabilities, demonstrated how a hacker could remotely commandeer vulnerable devices by sending a simple set of HTTP requests to them."



So thank you and congratulations, Network Security Fish.  What a nice public service you have performed for 92,589 D-Link users whom you've just single-handedly turned into victims.



LEO:  Yikes.  Yikes, yikes, yikes.  That's bad news.



STEVE:  It is.  Before long, if not already, those 92,589 D-Link devices will be infected with malware, if they aren't all already.  After all, today's Tuesday.  And as I mentioned at the start of this adventure, the Internet scan distribution shows that networks in the United States contain more of them than any other single region.  I wonder whose networks those devices may be sitting on, and I wonder who might be interested in finding out.



So I suppose the final takeaway lesson for us is that complex devices of this sort that are no longer being actively supported cannot be used safely, at least not in settings such as connected to the Internet, where the security risk is high.  Of course, that's easily said; right?  It's difficult to retire a perfectly good working device for no obvious reason other than that its manufacturer is no longer active supporting it.  A mature policy would ideally rotate such devices into less security-sensitive roles.  Give them a place inside the network, behind the firewalls, where they can live out their lives in peace while continuing to be productive.  I very much hope that the 92,589 owners of these surviving NAS devices do not experience much hardship as a consequence of Network Security Fish's needless, pointless, and destructive disclosure.  It should be clear that there are times when it's far better to just say nothing.



LEO:  Wow.



STEVE:  You know...



LEO:  It's like he's showing off, really; right?



STEVE:  Yes, that is all it - as there's no other justification, Leo.  Nobody's learning a lesson.  D-Link can't do anything about this.  They won't do anything about it.  It is, it is just hubris.  It is just ego.  It is just, hey, look what I did.  Look what I found.  But he could have said this without, I mean, well, better to say nothing; right?  But, you know, he gave a complete, here's how you attack 92,000-plus open, wide-open, innocent devices that are never going to be patched.  It's just like, hey, here, take them over.



LEO:  I guess...



STEVE:  Install malware, you know, create a botnet.  See whose network they're on.  Pivot.  Because who knows, some enterprise, some IT guy could have said, hey, I have one of these at home.  Works great.  Let's buy one for the company.  And so it's on the enterprise's network.  Someone's going to get in, pivot, now have access to their LAN from this device that is straddling the LAN and the WAN and you know, install ransomware on their network.  Why?  Because some guy says, hey, look what I found. 



LEO:  It could have been worse.  He could have sold it to Zerodium.  Right?



STEVE:  True.



LEO:  I mean, at least he didn't sell it to a nation-state.  Not that they would have paid much for it.  But still...



STEVE:  No.  Although I'm not sure that this isn't worse because now it's a feeding frenzy.



LEO:  It's free, yeah.  He's given it away.



STEVE:  Well, it's every script kiddie who ever, you know, had a wget command, or what's the other one, the famous Linux...



LEO:  cURL.



STEVE:  cURL, cURL, of course.  It's like, wow.



LEO:  Yeah.



STEVE:  So Leo, on that happy note...



LEO:  Yeah, thanks for cheering me up.



STEVE:  Why we're here, and we're going to find out why you don't want to use a private note-sharing site.



LEO:  Oh, interesting.  Yeah, the only D-Link thing I - I used to buy a lot of switches.  I think I might have some still rolling around, but those are passive.  They're not...



STEVE:  Yeah, exactly.  I think they had really nice-looking little network...



LEO:  Yeah, they were nice metal hubs and switches.



STEVE:  Yup.



LEO:  And I think I own a D-Link cable modem.  I hope that - that's such a bad practice, to hardwire a backdoor.



STEVE:  Oh, Leo.  It's just...



LEO:  It's just depressing.



STEVE:  I mean, at some point we need legislation where...



LEO:  Yeah, you're responsible.  Just you're responsible.



STEVE:  Yes, exactly, exactly.  If something happens because of what you did, you are now liable, you know, you're on the receiving side of lawsuits because that's just - you just can't do it.



So get a load of this one.  Last Thursday, Brian Krebs of krebsonsecurity.com fame, who really likes doing deep security research, posted a piece that just makes you shake your head.  He wrote:  "A cybercrook who has been setting up websites that mimic the self-destructing message service Privnote.com" - you know, as in obviously privacy - "accidentally exposed the breadth of their operations when they threatened to sue a software company."  Wow.  This is chutzpah.  "The disclosure revealed a profitable network of phishing sites that behave and look like the real Privnote."



LEO:  Oh, no.



STEVE:  Except that, get this...



LEO:  Not so Privnotes.



STEVE:  Unh-unh.  "Any messages containing cryptocurrency addresses will be automatically altered with a different payment address...



LEO:  Brilliant.



STEVE:  It is.  It is diabolical.  "...controlled by the scammers."  So Brian explains, he says:  "Launched in 2008, Privnote.com employs technology that encrypts each message so that even Privnote itself cannot read its contents.  And it doesn't send or receive messages.  Creating a message merely generates a link.  When that link is clicked or visited, the service warns that the message will be gone forever after it is read.



"Privnote's ease-of-use and popularity among cryptocurrency enthusiasts has made it a perennial target of phishers" - who could have seen that coming? - "who erect Privnote clones that function more or less as advertised, but also quietly replace their own cryptocurrency payment addresses when a note is created that contains crypto wallet addresses.  Last month, a new user on GitHub named Fory66399 lodged a complaint on the 'issues' page for MetaMask, a software cryptocurrency wallet used to interact with the Ethereum blockchain.  Fory66399 insisted that their website  Privnote.co  was being wrongly flagged by MetaMask's 'eth-phishing-detect' list as malicious."  Which of course it was.



"Fory66399 wrote," with their arms crossed:  "We filed a lawsuit with a lawyer for dishonestly adding a site to the block list, damaging reputation, as well as ignoring the moderation department and ignoring answers.  Provide evidence, or I will demand compensation."  So MetaMask's lead product manager Taylor Monahan replied by posting several screenshots of Privnote.co showing that the site did indeed swap out any cryptocurrency addresses.



LEO:  Oh.  Oh.



STEVE:  After being told where they could send a copy of their lawsuit, Fory66399 appeared to become flustered.  And went silent.



Now, Brian's piece continues with one of his terrific deep dive researches into all the details, and he uncovers a large network of very similar clone websites by backtracking their domain registrations.  What I found interesting about this was that this is not hacking some fancy new blockchain technology contract thing that like nobody understands, but to steal like a windfall of $50 million all at once.  No.  Instead, this is stealing individual small cryptocurrency transactions from cryptocurrency end users.  And you can imagine the dialogue; right?  You know, "I haven't received the drugs I sent you the money for.  What do you mean, you never received the payment?  I sent it right after I received the email with your wallet address, and the money was taken from my wallet.  If you didn't get it, then where did it go?"



LEO:  Who did?



STEVE:  That's right.



LEO:  Maybe Fory got it.



STEVE:  Uh-huh.  Well, gee, it appears likely that it may have made its way into some Russian's pocket, perhaps because you were not paying close attention and used Privnote.co or Privnota.com or Privatemessage.net or Privatenote.io or Tornote.io or Privnode.com or Privnate.com or Prevnote.com.  Believe it or not, Brian's research traced each of those Privnote.com malicious copycat domains back to a handful of apparent Russians who had been enjoying fleecing the corrupt Western capitalists.



So we know that it's entirely possible to create a simple website that encrypts a visitor's note on their PC so that it cannot be decrypted except by another third party.  The problem is, the technology required to do this is not readily visible and auditable by a site's user.  Nor would they understand the crypto code, even if it was visible.  The site clearly and cleanly claims that they're unable to read anything that's being sent.  And the unwitting user has heard of such sites, like Privnote.com, that's authentic, you know, and that that site arranged to do just that.  So it's got a terrific reputation.  And Privnote.io, well, it looks the same, so it's probably just the same people who also got that cool .io domain, and it's one fewer characters to type.  Might as well use Privnote.io instead of Privnote.com.  What could possibly go wrong?  You know, and, after all, Privnote.io, it says that it cannot read anything that's sent.  So let's just copy and paste our wallet address into it.  Right.  Good luck with that.



We talked a lot about Zerodium in the past.  They're the folks who offer extremely large bounties for new and unknown zero-click vulnerabilities.  And unlike the good guys at HackerOne or Zero Day, these creeps sell these zero-days, doubtless at significant profit, to unknown but certainly big-time buyers such as Israel's NSO Group for use by the Pegasus spyware and almost certainly to governments and intelligence services around the world.  In other words, the platform publishers such as Apple and Google are the last to learn of these exploits.



Well now, on Saturday, TechCrunch brings us news of a newcomer named "Crowdfense" - maybe they're fencing, I guess they're fencing the illegal, the ill-gotten goods of a zero-day.  And Crowdfense intends to give Zerodium a run for its money.  And, you know, this is really not a market where we'd like to see competition flourishing.  We'd prefer that the market didn't exist at all.



I have a screenshot in the show notes of Crowdfense's current offering lineup.  At the top of the heap they've got SMS and MMS full-chain zero-click compromises, the discovery and disclosure to them of which would net someone selling it somewhere between 7 and 9 million USD.  I mean, basically, you find one, and if you check your ethics at the door, you're done for life.  Right?  I mean, you could probably survive on $9 million.  You know, I mean, actually just off the interest.



Okay.  So in this case "full-chain" means something that gets the entire job done, not just an "Oh, look, it crashed," but "Oh, look, we now have root access to do whatever we want" sort of thing.  A full-chain zero-click for Android brings in 5 million, whereas the same thing for iOS is priced at between 5 and 7.  And these are all just within the top paying Mobile Platform category.  Crowdfense is also interested in mobile apps, other mobile things, desktop, virtualization, baseband, meaning, you know, the radio that underlies our smartphones, enterprise, web apps, and more.



TechCrunch's headline was "Price of zero-day exploits rises as companies harden products against hackers."  Okay, well, that's good.  With the subheading "A startup is now offering millions of dollars for tools to hack iPhones, Android devices, WhatsApp, and iMessage."  TechCrunch writes:  "Tools that allow government hackers to break into iPhones and Android phones, popular software like the Chrome and Safari browsers, and chat apps like WhatsApp and iMessage, are now worth millions of dollars  and their price has multiplied in the last few years as these products get harder to hack.



"On Monday, startup Crowdfense published its updated price list for these hacking tools, which are commonly known as 'zero-days' because they rely on unpatched vulnerabilities in software that are unknown to the makers of that software.  Companies like Crowdfense and one of its competitors, Zerodium, claim to acquire these zero-days with the goal of reselling them to other organizations, usually government agencies or government contractors, which claim they need the hacking tools to track or spy on criminals."  And of course we have lots of evidence we've discussed through the years on this podcast that, you know, politicians and political activists and enemies of powerful people, whoever, end up getting spied on, not just intelligence services tracking known bad guys.



"Crowdfense," they write, "is now offering between 5 and 7 million for zero-days to break into iPhones; 5 million for Android; 3 million and 3.5 for Chrome and Safari zero-days, respectively; and 3 to 5 million for WhatsApp and iMessage zero-days.  So the increase in prices comes as companies like Apple, Google, and Microsoft are making it harder to hack their devices and apps, which means their users are better protected."



Okay.  So in other words, of course, as zero-days become more rare, they naturally become more valuable.  It's good news for everyone that they are becoming more rare.  TechCrunch continues:  "Dustin Childs, the head of threat awareness at Trend Micro's Zero-Day Initiative said:  'It should be harder year over year to exploit whatever software we're using, whatever devices we're using.'  Unlike Crowdfense and Zerodium, ZDI pays researchers to acquire zero-days" - on the other hand, not 7 or $9 million - "then reports them to the companies affected with the goal of getting the vulnerabilities fixed."  So those are the good guys, where you get to keep your ethics with you while you accept some good money, but not enough so that you never have to do anything and can retire on a beach.



"Shane Huntley, the head of Google's Tag Team" - their TAG, Threat Analysis Group - "tracks hackers and the use of zero-days.  He said:  'As more zero-day vulnerabilities are discovered by threat intelligence teams like Google's, and platform protections continue to improve, the time and effort required from attackers increases, resulting in an increase in the cost for their findings.'



"In a report last month, Google said that last year in 2023 it saw hackers use a total of" - here it comes - "97 zero-day vulnerabilities in the wild."  That was last year, in all of 2023, 97 zero-day vulnerabilities.  "And the various spyware vendors, like the NSO Group, which often work with zero-day brokers, were responsible for three quarters of all zero-days targeting Google products and Android.  People in and around the zero-day industry agree that the job of exploiting vulnerabilities is getting more difficult.



"David Manouchehri, a security analyst with knowledge of the zero-day market, said that 'hard targets like Google's Pixel and iPhone have been becoming harder to hack every year.'  He said:  'I expect the cost to continue to increase significantly over time.'"



"Paolo Stagno, the director of research at Crowdfense" - the new bad guys - "told TechCrunch:  'The mitigations that vendors are implementing are working, and it's leading the whole trade to become much more complicated, much more time-consuming, and so clearly this is then reflected in the price.'"



The first time I read that I thought, trade?  What trade?  Then I realized that they're calling this zero-day vulnerability finding and selling "a trade."  And I suppose it is, though it feels like ransomware gangs talking about their "profit."  Profit?



LEO:  Yeah.  No, no, it's just trade.



STEVE:  How about theft through extortion?  You know, how is that profit?  But I suppose that it is, sadly, profitable; though it hardly seems earned.  Anyway, the Stagno guy from Crowdfense explained:  "In 2015 and/or '16, it was possible for only one researcher to find one or more zero-days and develop them into a full-fledged exploit targeting iPhones or Androids."  He says:  "Now 'this is almost impossible' as it requires a team of several researchers, which also causes prices to go up.  Crowdfense currently offers the highest publicly known prices to-date outside of Russia, where a company called Operation Zero announced last year that they was willing to pay up to 20 million for tools to hack iPhones and Android devices.  The prices in Russia, however, may be inflated because of the war in Ukraine and the subsequent sanctions, which could discourage or outright prevent people from dealing with a Russian company.  Outside of the public view, it's possible that governments and companies are paying even higher prices.



"David Manouchehri previously worked at Linchpin Labs, a startup that focused on developing and selling zero-days."  And again, there's no way that selling zero-days is ethical and cool.  Linchpin Labs, unfortunately, was acquired by U.S. defense contractor L3 Technologies, now known as L3Harris, in 2018."  And that's encouraging.  Anyway, David said:  "The prices Crowdfense is offering researchers for individual Chrome Remote Code Execution and Sandbox Escapes are below market rate from what I've seen in the zero-day industry."



LEO:  What?  What?  Wow.



STEVE:  Wow.  A low market rate.



LEO:  Holy cow.  That seemed like a good price to me.



STEVE:  Wow.  "Alfonso de Gregorio, the founder of Zeronomicon, an Italy-based startup that also acquires zero-days" - so, you know, they're scattered around - "agreed with this, telling TechCrunch that prices could certainly be higher."  Well, I guess he wants them to stay low because that's the price he has to pay researchers or hackers who find them.  I really don't want to call them "researchers."  "Zero-days," TechCrunch writes, "have been used in court-approved law enforcement operations.  In 2016, the FBI used a zero-day" - we know where this is going - "provided by a startup called Azimuth to break into the iPhone of one of the shooters who killed 14 people in San Bernardino, according to The Washington Post.  In 2020, Motherboard revealed that the FBI  with the help of Facebook and an unnamed third-party company  used a zero-day to track down a man who was later convicted for harassing and extorting young girls online.



"There have also been several cases where zero-days and spyware have allegedly been used to target human rights dissidents and journalists in Ethiopia, Morocco, Saudi Arabia, and the UAE, among other countries with poor human rights records.  There have also been similar cases of alleged abuse in democratic countries like Greece, Mexico, Poland, and Spain.  Neither Crowdfense, Zerodium, or Zeronomicon have ever been accused of being involved in similar cases."  On the other hand, remember, they're one party removed.  They're not the guys who are doing the exploiting of these exploits.  They're selling to entities which are then turning around and doing its abuse.  So, yeah, these resellers would not be in the loop.



They said:  "Zero-day brokers, as well as spyware companies like NSO Group and Hacking Team, have often been criticized for selling their products to unsavory governments.  In response, some of them now pledge to respect export controls in an effort to limit potential abuses from their customers.



"Stagno said that Crowdfense follows the embargoes and sanctions imposed by the United States, even if the company is based in the UAE.  For example, Stagno said that the company would not sell to Afghanistan, Belarus, Cuba, Iran, Iraq, North Korea, Russia, South Sudan, Sudan, and Syria  all on the U.S. sanctions lists."



LEO:  That's a really long list.  Wow.



STEVE:  It is.  And he said:  "Everything the U.S. does, we are on the ball."



LEO:  You know why?  I bet we're one of their biggest customers.



STEVE:  Uh...



LEO:  Yup.



STEVE:  Uh-huh.



LEO:  We don't want to get them mad.



STEVE:  I would bet that the NSA is probably taking our taxpayer  money and buying these exploits so that they can do things with it.  I'll bet you're right, Leo.  He said if an existing customer gets on the U.S. sanctions list, Crowdfense would abandon it.



LEO:  Mm-hmm.



STEVE:  "All the companies and governments directly sanctioned by the USA are excluded."



LEO:  Of course they are.



STEVE:  Uh-huh.  "At least one company, spyware consortium Intellexa, is on Crowdfense's particular blocklist.  Of Intellexa, Stagno said:  'I can't tell you whether it has been a customer of ours and whether it has stopped being one.  However, as far as I am concerned, now at this moment Intellexa could not be a customer of ours.'



"In March, the U.S. government announced sanctions against Intellexa's founder Tal Dilian, as well as a business associate of his, the first time the government imposed sanctions on individuals imposed in the spyware industry.  Intellexa and its partner company Cytrox is also sanctioned by the U.S., making it harder for the companies, as well as the people running it, to continue doing business.



"Intellexa's spyware has been reported to have been used against U.S. Congressman Michael McCaul, U.S. Senator John Hoeven, and the president of the European Parliament, Roberta Metsola, among others."  And finally:  "De Gregorio, the founder of Zeronomicon, declined to say who the company sells to.  On its site, the company has published a code of business ethics."



LEO:  Oh, well.  Okay.



STEVE:  These guys have business ethics, Leo.



LEO:  They're so ethical.



STEVE:  I'm wondering how many sentences or how many words in their ethics statement, "which includes vetting customers with the goal of avoiding doing business 'with entities known for abusing human rights,' and respecting export controls."



Now, reading about these so-called export controls, one has to wonder how difficult it would be for any major country on the U.S. sanctions list to establish a behind-the-scenes relationship with another company in a non-sanctioned region to use as a middleman.



In any event, I thought that checking in on the state of the zero-day market would be useful.  While it may not be good news that prices are increasing, since that significantly increases incentives to find the fewer and fewer remaining zero-days that exist, the fact that prices are rising because these remaining zero-days are becoming ever more scarce, well, that's certainly good news.



I have one bit of miscellany which is happy.  Last Thursday, NASA updated the world with the news of the status of the our intrepid Voyager 1 spacecraft.  The headline of their posting was "Engineers Pinpoint Cause of Voyager 1 Issue, Are Working on Solution."  They explained:  "Engineers have confirmed that a small portion of corrupted memory in one of the computers aboard NASA's Voyager 1 has been causing the spacecraft to send unreadable science and engineering data to Earth since last November.  Called the Flight Data Subsystem (FDS), the computer is responsible for packaging the probe's science and engineering data before the telemetry modulation unit (TMU) and radio transmitter send the data to Earth.



"In early March, the team issued a 'poke' command to prompt the spacecraft to send back a readout of the FDS memory."



LEO:  Is that a Facebook poke command?



STEVE:  They've poked it.  That's similar to a Like, but it dates from 1971, so it's not quite the same.



LEO:  Yeah, that's right, the original poke.



STEVE:  That's right, "which includes the company's software code as well as variables are all being sent back, the computer's software code as well as variables.  Using the readout that they received, the team has confirmed that about 3% of the FDS memory has been corrupted, preventing the computer from carrying out normal operations.



LEO:  That's fascinating, huh.



STEVE:  "The team suspects that a single chip responsible for storing part of the affected portion of the FDS memory is not working.  Engineers cannot determine with certainty what caused the issue.  Two possibilities are that the chip could have been hit by an energetic particle from space, or that it simply may have worn out over 46 years."  Just like the rest of us.



LEO:  I understand that.  I get that.  I can understand that completely.



STEVE:  "Although it may take weeks or months, engineers are optimistic they can find a way for the FDS to operate normally again without the unusable memory hardware, which would enable Voyager 1 to begin returning science and engineering data."



LEO:  What a miraculous story.  Unbelievable.



STEVE:  Leo, it is astonishing.



LEO:  Isn't it.



STEVE:  And when you consider, what is it, is it billions or millions of miles away?  It is astonishingly far away.  How can that thing be pointing at Earth?  I mean, talk about, I mean, it's a fraction of a degree at that distance for its antenna, a directional dish, to still be perfectly aligned.  To me, that's what is stunning.



LEO:  Amazing.  And they don't have enough power to send a broad beam.  It's got to be a fairly tight beam.  So, amazing.



STEVE:  Right.  And, I mean, the power is dropping as we've covered through the years.  They're using a radioisotope-based system.  Basically, the decaying radioisotopes are heating a thermocouple, which is generating the power to drive this stuff.  And they've had over time, as less and less radiation is being produced because it's just, you know, it's just winding down, the power produced has diminished, so they've been having to judiciously turn off successive instruments of their total instrument package because they're looking at the total number of watts being generated.  It's just astonishing.



LEO:  It is 15 billion miles from Earth,



STEVE:  That's insane.



LEO:  It is incredible.



STEVE:  Insane.



LEO:  Forty-six years, seven months, four days, nine hours, four minutes, and 23 seconds.  15 billion.  That is insane.  I almost feel like that can't be right.



STEVE:  I know.  And how can it still be pointing with enough accuracy at us?  That's mind-blowing to me.



LEO:  Twenty-two hours, 22.6 hours light distance away.  That's - how did it happen?



STEVE:  Oh, and there you just scrolled by the instruments which are now on and off on both of the devices. 



LEO:  Oh, yes, yeah.  So one is the first column.  Plasma science is off.  Imaging science is off.  Infrared interferometer spectrometer is off.  Quite a bit.  But there's still science projects still on, cosmic rays, low-energy charged particles, and magnetometer.  Wow.  Oh, and plasma web subsystem. 



STEVE:  Yeah.



LEO:  Still on.



STEVE:  And actually we learned that when Voyager 1 passed out of the Heliosphere, the models which cosmologists and astronomers had made were found to be wrong.  So it was never expected to be anything other than a fly by some of our planets to take some pictures.  But it just wouldn't die.  So it kept on going, and it had so much useful instrumentation on it that they're learning new things...



LEO:  Incredible.



STEVE:  ...which are still valuable.



LEO:  Patrick said, wait a minute, it's getting closer to the Earth?  Yeah, because the Earth's rotating in its direction right now.  The distance from the Sun is going up.  It's 15.1 billion miles from the sun, or 163 astronomical units.



STEVE:  So that means that it's moving away from the sun more slowly than the Earth is currently moving around and thus toward it at the moment.



LEO:  Right.  It's gaining.  And then of course as the Earth processes, it will go the other way.



STEVE:  Right.



LEO:  And it'll gain faster.



STEVE:  And there's only now one radio telescope in Australia which is able to talk to it.  And unfortunately it's tasked with doing lots of other things.  So they need to like steal a little bit of time at the right moment when they're able to send a burst of instructions to it.  And then they wait 44 hours, 22 for it to go out and 22 - I mean, and Leo, the other thing, think of the science we had in '71.



LEO:  Oh, yeah.



STEVE:  That's when I had my first car that had an empty engine compartment because there was an engine and a gas line going to a carburetor.  I mean, there was no - and you had a throttle.  I had to pull the throttle to start the engine when it was cold.



LEO:  Oh, the choke, yeah, yeah, yeah.



STEVE:  Yeah, the choke, the choke, not the throttle.  Right, the choke.  And that was the world in '71, were Wozniak was saying, Steve, I think I can get rid of one more Apple computer because, you know, we need the space.



LEO:  Look, that's where it is, way the hell out there.  Way beyond anything else.  Farthest manmade object in the world.  In the universe.  Unless there's other men elsewhere.  But we don't know that, so.



STEVE:  I don't know where that Tesla coupe is at the moment.



LEO:  A lot closer, I can promise you.



STEVE:  I think so.



LEO:  Wow.  Just a great story.  It's such a great story.



STEVE:  And again, 1971 technology, it's just astonishing.



LEO:  But think, I mean, in 1969 we landed on the moon, two years before that.  And we're having a hard, a devil of a time doing it again.  So maybe those guys back in the '60s knew something.



STEVE:  Yeah, the bits were bigger, and so they were more robust back then.  What you want is bigger bits, really.



LEO:  Yeah, our bits have gotten way too small.



STEVE:  Speaking of bits, I was tempted to name this podcast "SpinRite 6.1" because what happened Sunday afternoon means so much to me.  But since it doesn't mean that much to the rest of our listeners, that didn't seem appropriate.  What happened on Sunday is that I finally updated GRC for the first time ever to begin offering 6.1 as its official SpinRite.  So 6.1 is finally what new purchasers will receive when they go to GRC for the first time.  So, you know, it's huge for me.  I've been living with my commitment to offer - and I don't have the website all...



LEO:  Oh, yes.  So it's still 6.0.



STEVE:  So if you go to the top, Leo - I know, I've just got this done.  If you click on the menu under SpinRite and the top left...



LEO:  Okay.



STEVE:  There's a little - yup.



LEO:  Upgrade to 6.1, okay.



STEVE:  Yup.  Or if you just click on Purchase SpinRite, go down a few, and then you'll see...



LEO:  There it is.  Woohoo!



STEVE:  ...that it is 6.1 is what we are offering.



LEO:  That's pretty cool, Steve.  By the way, this website looks like it came from 1971.  Congratulations.



STEVE:  I came from 1971.



LEO:  But the bits are bigger here.  They really are.



STEVE:  That's right, the bits are bigger, baby.



LEO:  Is the website written in assembly?  Tell the truth.



STEVE:  It's all hand-coded.  I wrote it in HTML.



LEO:  Nice.  Nice.



STEVE:  Before CSS even.



LEO:  Yeah, yeah, yeah.



STEVE:  And I do look at the code, and it hurts, actually.  And I've had a number of our listeners who've said, Steve, Steve, Steve.



LEO:  No, no, no.



STEVE:  I'm a website designer.  Let me fix this for free.  I will be happy to fix it for free.



LEO:  No, don't.  Okay.



STEVE:  And I, you know, I want to do it.  One of the problems I had with employees was they were having all the fun doing the work.  And so consequently...



LEO:  Yeah, but you update that, and then you've got to maintain it.  Trust me, as somebody with a modern website, stick with what you've got.  Static is a good thing.



STEVE:  I am.  Anyway, I've been living with my commitment to offer 6.1...



LEO:  Yay.



STEVE:  ...for so long now, you know, it's been more than a decade, and I felt guilty whenever I'd stolen time from that.  You know, and now every time I remember that 6.1 is there being sold, it's like, oh.  I mean, it gives me a great sense of peace.



LEO:  Good for you.



STEVE:  One thing I wanted to mention, though, to our listeners is that, after sharing the experience my wife and I had with her Dell laptop during last week's podcast, I did notice an uptick in SpinRite sales; in other words, more "Yabba Dabba Doo's."  Now, I'm fine with that since many people have reported significantly improved performance after running SpinRite at Level 3 over an SSD, which is almost certainly going to also improve the long-term reliability of their system.  So I can stand behind the benefits that people are likely to see.  My only concern was that until the afternoon of this past Sunday, April 7th, which also happened to be my third wedding anniversary, so it was a busy day...



LEO:  Oh, congratulations.



STEVE:  Unless those new purchasers knew to then go to GRC's prerelease.htm page, they would have obtained 6.0.  So I just wanted to make sure that anyone who may have heard that and been excited and went out to get SpinRite, that they knew that they were getting 6.0, and everyone should go get 6.1.  You can use your receipt that you received to bring up the page and download, you know, 6.1 is what we're offering now.  So anyone who goes to get it now who bought 6.0 will get 6.1, or you can go to the /upgrade.html.  Greg said to me, my tech support guy, why are we still calling it "prelease"?  And this was, I think, yesterday.  I said, oh, yeah.  So I created another page called upgrade.htm, so you can use that, too.



Anyway, the second thing I wanted to share was inspired by someone using the handle "The Big Bear" who posted over in GRC's SpinRite newsgroup.  After running SpinRite 6.1 on two Macs, he posted:  "And now I have two Macs tested and refreshed."  He said:  "And it makes quite a difference.  The frequency with which the colorful beach ball had shown was starting to worry me, and it has all but disappeared now.  And the startup times feel halved, at least."  He said:  "Wish I had measured it before and after."  He said:  "Documentation is not my strong suit, but I will put it on my to-do list, to write an update mentioning the extra hoops required with the latest Sonoma macOS."



Now, the reason I'm mentioning that, aside from it being another instance of welcome feedback about my use of the past 3.5 years creating 6.1, is that while FreeDOS and SpinRite will run on Intel-based Macs, getting them to boot from a CD or USB can be a bit tricky.  So I wanted to remind any would-be Mac purchasers that this is the reason I created GRC's freeware named "Bootable," in favor of "DOS Boot," although that was, you know, I was tempted to name it that.  You know, you can get Bootable.  Download it for free.  If you can get it to congratulate you on your success in booting it, then exactly the same path can be taken with SpinRite.



And as I said, Bootable can be freely downloaded at any time.  And GRC's web forums at forums.grc.com contain a growing knowledge base of help for Mac users.  And that's where this Big Bear guy will post instructions and more details, although there's already a lot of stuff there because, you know, Mac users have said, hey, how do I get this to go?  So anyway, next up is email.  I know our listeners can't wait to for GRC to have a mailbag that they can send things to.  And then is to update the documentation.  And then this project will be finished.  So yay.



LEO:  And now what?  Next, after this?



STEVE:  I know what, but...



LEO:  Oh, it's a secret.



STEVE:  ...I'm not going to say yet.



LEO:  Okay.  Smart.  Wise.



STEVE:  One thing at a time.  I always get myself in trouble by overcommitting.



LEO:  Exactly.



STEVE:  And precommitting.  And my wife, bless her heart, says, "Now, you're not going to promise anything again, are you?"  I was like, no, I'm still recovering from the last promise.



LEO:  Yeah.  Well, she's put up with this, if you got married three years ago, for the entire life of your marriage.  Maybe it's time to take a little break.



STEVE:  No, she is a dream.



LEO:  Yeah, that's awesome.  All right.  Minimum viable - I know what a minimum viable product is.  When you're an app developer or creating a new site, you create the minimum viable product.  You know?  But what is this MVSP?



STEVE:  Yup.  And I'm sure that's where they came from.  So our beloved industry is slowly, very slowly, getting its act together.  But it is happening.  And I've been encouraged by some recent news surrounding the Minimum Viable Secure Product effort.  The group's list of contributors has been growing, and it now includes some well-known names such as Salesforce, Google, Okta, Slack, Vanta and about 20 others.  I guess that's Okta.  The reason this is today's primary podcast topic - aside from the whole thing being an extremely worthwhile effort, which it is - is due to the announcement of the effort's latest member, which was made on Thursday last week.



The posting reads:  "Today, we're excited to announce that CISA is joining the Minimum Viable Secure Product Working Group.  Since launching CISA's global Secure by Design initiative last year, we've received a tremendous amount of feedback, including through our Request for Information that recently closed."  And of course it was the Secure by Design initiative where we first saw this notion suggested of requiring a manual action on the device in order to change the configuration in any way that could be foreseen as being dangerous, which I think is brilliant because, you know, again, inconvenient, yes, but it's the right thing to ask for.



Anyway, so CISA's been leading on a lot of this.  They said:  "One of the key questions we've gotten is how organizations consuming software can ask the right questions of their software manufacturers.  Such a 'secure by demand' approach" - as opposed to secure by design, which is what CISA's is - "is crucial to drive the uptake of secure by design principles and practices.  Too often, procurement questionnaires are filled with long lists of questions which don't always correlate with positive security outcomes.  In order to achieve a future where technology is secure by design, companies buying software should have simple and to the point questions for their vendors.



"The MVSP is an important step toward this goal.  MVSP offers a simple checklist that organizations can use to strengthen security at multiple stages - to review their software vendors' security during procurement, as a self-assessment tool for their own software, as part of their software development lifecycle, or as contractual controls - which can go a long way toward helping ensure secure by design principles are followed.  We're excited to join the MVSP working group to help shape the direction of the initiative going forward.  The MVSP is composed of a broad coalition of technology manufacturers, and the working group is open for anyone to join."



Okay, now, reading through the MVSP's checklist put a smile on my face, as I mentioned at the top of the show, as I imagine it will for our listeners, since we have carefully examined many of these issues in the past here.  What everyone is hoping is that powerful technology procurement bodies  like for example the various branches of the U.S. government, including both its civilian and military bureaucracies  might start making an adherence to these principles more than just requests, but make them mandatory for future purchases.



The MVSP web site is just MVSP.dev, and they explain their mission by writing:  "Minimum Viable Secure Product is a list of essential application security controls that should be implemented in enterprise-ready products and services.  The controls are designed to be simple to implement and provide a good foundation for building secure and resilient systems and services.  MVSP is based on the experience of contributors in enterprise application security and has been built with contributions from a range of companies.



"We recommend that all companies building enterprise software" - and notice today, today it's a recommendation.  Let's see how this evolves over time.  It would be just terrific if it became more than a recommendation.  So "We recommend that all companies building enterprise software and services, or otherwise handling sensitive information, implement the MVSP controls and, where possible, go well beyond them in their application security programs.  We welcome constructive feedback to help us continue to improve MVSP and provide a control set that meets the needs of its users."



Okay.  So MVSP is a list of essential application security controls at the enterprise level.  Let's look at what they are.  So under business controls they have:  "Publish a vulnerability disclosure policy that outlines the testing scope, provides a legal safe harbor, and gives contact details for security reports."  Okay.  Those are all things we've discussed in this podcast.  Vulnerability researchers should be free, should know that they're free to research a company's products, the security of them, while being legally protected from retribution or reprisals if they hack a supplier's product without malicious intent for the sole purpose of discovering and responsibly reporting vulnerabilities, even if it's for pay.  You know, if it's to report them to HackerOne or a legitimate service.  But the point is, publish a vulnerability discovery and disclosure policy and make it clear.



Also, flesh out, or rather fleshing that out, the MVSP lays out the required components:  "Develop and document procedures for triaging and remediating reported vulnerabilities, respond to reports within a reasonable time frame, and patch vulnerabilities quickly."  They also suggest contracting with a security vendor to perform comprehensive penetration testing of products, services, and dependent systems at least once a year.  We know that's been done, but we know that's not probably common practice enough.  So yes, you need third-party eyes on something.  Your own developers cannot aggressively test the security of the products they develop.  It just doesn't work.



They have:  "Notify relevant parties about any security breach that affects sensitive information no later than 72 hours upon discovery, and upon learning any additional details of the breach.  Consider reporting the breach to relevant national cybersecurity agencies in line with local guidance and regulations.  In any such reporting, include the nature of the breach, relevant contact information, the consequences of the breach, and the measures taken and needing to be taken to remediate the issue."



"Be certain to sanitize all storage media holding unencrypted production data.  Implement single sign-on using modern, maintained, and industry-standard protocols for all customers at no additional cost.  Redirect traffic from HTTP (port 80) to HTTPS (port 443).  Exceptions to this are internally secure protocols designed to run over unencrypted connections," you know, such as OCSP, the Online Certificate Status Protocol, that doesn't need it.  And I also noted, whenever I've been doing digital signing of code, the connection to the timestamp server is just HTTP because it provides - it's another example of a protocol that uses its own internal security.



Also, "Include Strict-Transport-Security header with a long max-age value and set authentication cookies as Secure."  We know that this prevents the browser from ever sending those cookies out over non-encrypted connections.  They've got "Apply appropriate HTTP security headers to reduce the application's attack surface and limit post exploitation.  These should include setting a minimally permissive Content Security Policy," you know, the CSP, "limiting the ability to in-line frame the application by enabling framing controls with X-Frame-Options or CSP frame-ancestors and disable caching for APIs and endpoints that return sensitive data."



Again, these are all, like, these are all things that web designers should do.  But they don't unless they have to, or unless they're told to, or unless there's some policy to make that happen.  I remember doing that for the SQRL queries and responses from the SQRL server.  And, you know, it's a little nerve-wracking because you're - especially nerve-wracking to add them after the fact because you're not sure what you're going to break.  If maximally restrictive policies are there from the start, when you add a feature and test it, and it doesn't work, then you can look at how to make the smallest change required to loosen the policy to allow the technology you want to work to work.  That's entirely different from just not having anything at all, where everything works.  The problem is lots of things you don't want to have work will then also work.



And of course the MVSP also had a lot to say about password policies.  They wrote:  "If password authentication is used in addition to single sign-on, then" - and we have a series of bullet points:  "Do not limit the permitted characters that can be used."  Yay.  "Do not limit the length of the password to anything below 64 characters.  Do not use secret questions as a sole password reset requirement.  Require email verification of a password change request, and require the current password in addition to the new password during password change.  Store passwords in a hashed and salted format using a memory-hard or CPU-hard one-way hash function.  Enforce appropriate account lockout and brute-force protection on account access.  And do not provide default passwords for users or administrators."



And what about the use of security-sensitive third-party libraries?  You know, like Log4j.  They say:  "Use modern, maintained, and industry-standard frameworks, template languages, or libraries that systemically address implementation weaknesses by escaping the outputs and sanitizing the inputs.  Ensure third-party dependencies are maintained and up-to-date, with security-relevant updates having a security score of medium or higher applied in line with your application patching schedule.  Upon becoming aware of a Known Exploited Vulnerability" - that's CISA's KEV collection - affecting a third-party dependency, the patch should be prioritized.  Where dependency patching or upgrades are not possible, equivalent mitigation should be implemented for all components of the application stack."



And so we can sort of group all of this as things that somebody trained in modern security measures would know, but also things that are, you know, not fun to spend time on; right?  It's like, gee, what did you do all last month?  Well, I brought us more into compliance.  What?  Well, you know, what new functions work as a result?  Uh, well, none.  But we're more in compliance than we were.  The point is, you know, it's thankless; right?  It's not until everybody around you gets attacked and hacked, and you don't, that you begin to look like a star.



So anyway, hardly surprising, these guys are also big fans of logging, recommending that logs be kept of all authentication events, both successes and failures.  And I thought that was interesting; you know?  Log all security-relevant configuration changes, including of course disabling of logging, and log application owner access to customer data to provide access transparency.  The logs must include the user ID who's involved, the IP address from which anything is happening, a valid timestamp, the type of action performed, and the object of the action.  Logs must be stored for at least 30 days at no additional charge - I thought this was interesting - to the client or customer.  We know that we've recently seen instances where some cloud provider said, well - actually Microsoft in particular - we'll provide you with more advanced security logging, but that's an extra cost option.  And as we know, they've backpedaled on that a bit.  And not surprising that the logs should not contain sensitive data or payloads.



And though it barely needs saying, they recommend using current, maintained, industry-standard means of encryption to protect sensitive data in transit between systems, and at rest in all online data storage and backups.  And when vulnerabilities are found, they say:  "Produce and deploy patches to address application vulnerabilities that materially impact security within 90 days of discovery.  For vulnerabilities with evidence of active exploitation, production and deployment of patches should be prioritized."  Okay, that one is a "duh."  And finally:  "Publish a security bulletin that details the vulnerability and its root cause if the remedy requires action from customers.  We've seen many instances where permissions were far too open.  We've noted that nothing breaks when everyone can access everything, so it's often discovered after a breach of some kind that the source of the breach was overly permissive access controls.



So they write:  "Limit sensitive data access exclusively to users with a legitimate need.  The data owner must authorize such access.  Deactivate redundant accounts and expired access grants in a timely manner.  Perform regular reviews of access to validate need to know.  Ensure that remote access to customer data or production systems requires the use of multifactor authentication."



And I know, yeah, everyone knows, all of that is obvious.  I will say, you know, I'm subjected to it by Level 3 because my badge, they insist on continually expiring my badge.  And it's annoying because it's me.  And I have to make sure that I'm keeping it renewed because, if it does expire, then renewing it is a bigger pain.  And if I need to run over there, I need to have my badge current.  And they're constantly expiring it.  On the other hand, yes, it's more secure.  If it were not just me, if it were an organization where, for example, 20 people had access, the act of having to renew all of those badges would be like, oh, wait a minute, Herman no longer works here.  So it's, yeah, I'm not renewing that badge.  But if it didn't expire, your attention wouldn't be brought back to it.  So, you know, all of these things, they're little incremental increases in pain, but they're important because they do things.



So, you know, nobody wants to sit and make time to review access permissions.  It's boring.  And, you know, it might not even be productive.  You may not find anything that needs to be changed, while a million other actually important things need to be done.  As a consequence, typically, it never happens.  But it can be a big source of problems.  So they also advise that it's important to maintain a list of third-party companies with access to customer data, which can and will be made available to clients and business partners upon request.



And what occurred to me is since that list, providing that list, might be embarrassing if, for example, it were to contain the names of contractors who were no longer affiliated with the organization, this also, again, forces a review and provides some incentive to remove access once relationships have terminated, and instances where the access doesn't, you know, remove itself.  And again, through the years we've seen that instances where that not happening has come back to bite companies.  And since it often doesn't happen automatically, it's another of those things that often falls through a crack.



They close this comprehensive list, you know, of things that, as I said, everybody knows would be good to do, but many organizations are still not doing, by reminding about the need for and importance of backups.  They recommend not only securely backing up all data to a different location - you know, Leo, this is your standard, you know, 3-3-3 or whatever it is, backup system.



LEO:  3-2-1, yes.



STEVE:  3-2-1, right.  I knew there were some numbers involved.  And, you know, we all know that everybody would like to be in full compliance with these guidelines.  And we also know about inertia, and that few things change on their own for the better, or for that matter change at all.  It is for this reason that these MVSP guidelines exist, and it's the reason CISA has added their name to the group's growing list of contributors.



The bottom line is that doing all of these things would come at some cost, and most businesses are looking for ways to cut costs, rather than ways to incur additional expense.  And the businesses don't pay the price until they get bit by a security problem.  So it's going to be a difficult lift.  But at least now all of these useful concepts have been pulled together in a single place, MVSP.dev.  And if at some point world governments were to require compliance, well, then everyone who wanted to sell to those major markets would need to clean things up.  And that would help everybody.



LEO:  Well, it's a step.



STEVE:  Yes.  It is.  And, you know, we're not going to get there without it.



LEO:  Right.



STEVE:  It doesn't mean we're going to get there with it.  But, you know, it's better to have it than not.



LEO:  Right.



STEVE:  Bye.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#970

DATE:		April 16, 2024

TITLE:		GhostRace

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-970.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  What's the latest on that massive five-year-old AT&T data breach?  Who just leaked more than 340,000 Social Security numbers, Medicare data and more, and what does that mean?  Are websites honoring their cookie banner notification permissions?  And why do we already know the answer to that question?  What surprise has the GDPR's transparency requirements just revealed?  And after sharing a bit of feedback from our listeners, we're going to go deeper into raw fundamental computer science technology than we have in a long time - and it may be inadvisable to operate any heavy equipment while listening to that part.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson's here.  I'm at, well, not here, I'm at my mom's house.  But we do have a great show.  It's a propeller beanie show this week.  He's going to explain what race conditions are and why it contributes to problems like Spectre and Meltdown.  We've got the latest numbers on the massive five-year-old AT&T data breach.  You won't believe how many customers are affected.  You'll also be curious to know who just leaked more than 340,000 Social Security numbers, Medicare data, and more, and what you can do about it.  And GDPR transparency requirements, you know, those cookie pop-ups?  Are they honored?  What do you think?  We've got the deets, all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 970, recorded April 16th, 2024:  GhostRace.



It's time for Security Now!, the show where we cover the latest news about what's happening in the world of the Internet in security.  Bad guys, good guys, black hats, white hats.  I am  broadcasting from the East Coast this week.  I'm at Mom's house.  Steve Gibson never leaves his fortress of solitude.  Hello, Steve.



STEVE GIBSON:  Why would I ever leave?  And Leo, this is not a green screen behind me, as Andy, or was it, no, it was one of your guys on MacBreak Weekly said.  No, this is not a green screen.  This is, yeah...



LEO:  Well, Jason has the same background, whether he's real or not.  And I always have to ask him.  But if this were a green screen, could I do this?



STEVE:  That's a very nice little bunny.  That's a very - that's a very nice bunny.



LEO:  I have lots of tchotchkes behind me.



STEVE:  Yeah, I think I grew up with one of those clocks in my grandparents' house.



LEO:  Oh, yeah.  That's a classic...



STEVE:  Yeah.



LEO:  Isn't that neat?  There's a name for it.



STEVE:  That's neat, with a big golden one that - yup.



LEO:  Is it a steeple clock?  I can't remember.  There is a name for it, yeah.  Lots of antiques here in beautiful Providence.



STEVE:  And in fact your own mother could be considered an antique.



LEO:  Yes, absolutely.  Although she doesn't live here anymore.  She's at the home.  But she's loving it.  It's an assisted living facility.  She loves it.  She's in memory care because she has no ability to form memories at all; you know?  She can't, like she won't remember that I was there today.  But she remembers everything else perfectly.



STEVE:  Isn't that...



LEO:  So it's a really interesting conversation.  She's jolly and happy as ever.  She's not - I said, "Sometimes people get grumpy when they can't remember from day to day."  She says, "No, no, it's good.  Everything's fresh and new."



STEVE:  I had one of my very good lifelong friends - who's no longer on the planet.  But we had been driving to each other's place to go see a movie together for decades.  And one day he said, "I've lost my maps."  And I said, "What?"  And he said, "Everything's fine," he said, "but apparently I had a little stroke, and I don't know where anything is anymore."



LEO:  Interesting.  Interesting.



STEVE:  And it was so selective.



LEO:  Precise, yeah.



STEVE:  I mean, it was like, it was just his mapping center.



LEO:  Wow, yeah.



STEVE:  Was, like, lost.  And so I said, "Well, okay, I'll come, I'll pick you up instead of the reverse."



LEO:  It happens.



STEVE:  But it was - and that's - he said, "I lost my maps."  He, like, realized, he didn't know where, how to go places.



LEO:  Isn't that interesting.



STEVE:  But everything else was still...



LEO:  The little vessel that burst just...



STEVE:  Breaking something.



LEO:  One little zone of the brain, and it's gone, yeah.



STEVE:  Yeah.  Okay, so this week, oh, boy.  Probably the most consistent feedback that we received through the life of this podcast has been that our listeners love the deep technology dives.  And, oh, get your scuba equipment.



LEO:  Oh, good.



STEVE:  Because in order for today's revelation to make sense, today's podcast is titled "GhostRace" - "race" as in race conditions, and "ghost" as in Spectre.  So what has been found by a team of researchers at AU Amsterdam and IBM is yet another problem with our industry's attempt to accelerate CPU performance.  But in order to understand it, it's going to be necessary for our listeners to understand about preemptive multithreading environments and thread concurrency and synchronization of shared objects.  And that may sound like a lot, but everyone's going to be able to go, oh, you know, "that's so cool" by the end of this podcast.



So we're going to talk about an update on that massive five-year-old AT&T data breach.  Also, who just leaked an additional 340,000 Social Security numbers, Medicare data, and more.



LEO:  Ay ay ay.



STEVE:  And what that means.  I know.  It's just, gosh, really?  Are websites honoring their cookie banner notification permissions?  And why do we already know the answer to that question?  What surprise has the GDPR's transparency requirements just revealed?  And then, after sharing a bit of feedback from our listeners, as I said, oh, boy.  Given the feedback that I've had about what people like when we do it on this podcast, we're going to go deeper into raw fundamental computer science technology than we have in a long time.



LEO:  Good.



STEVE:  And I'll just caution our listeners, it may be inadvisable to operate any heavy equipment while listening to that portion of today's presentation.



LEO:  Well, I always call these, and I think you do, too, the propeller hat episodes.



STEVE:  Yep.



LEO:  And we love the propeller hat episodes.  So get your beanie ready.  We're going to talk about some deep stuff.  And of course our Picture of the Week this week, which I have not seen.  So you'll get a clean take from me on this one.  Always a fun part of the show.



STEVE:  Okay.  So this is just a wonderful picture.  I gave it the headline "This wonderful concept suffers from being a bit of an inside joke, but for those who understand it..."



LEO:  I get it.



STEVE:  "...well, it's quite wonderful."  What we're seeing is the result from some random guy in Nebraska no longer supporting his 20-year-old, you know, thankless maintenance of some random piece of open source software upon which the entire Internet's infrastructure has been erected.



LEO:  It's a sequel to a famous xkcd cartoon, yeah.



STEVE:  Yes, yes, xkcd.com/2347.  I put below the picture, I said, "If you need a hint, see xkcd.com/2347," where...



LEO:  Which is a perfect little sandcastle of blocks and then giant things, all relying on one little tiny brick.



STEVE:  Yup, yup.  And so anyway, today's picture is what happens if the brick gets pulled.



LEO:  Or the maintainer retires, which has been a problem of late.



STEVE:  Yeah, exactly.  I mean, we don't know how to launch shuttles anymore because all the shuttle guys are gone.  And it's like, what do you mean, I need a different kind of epoxy for this tile on the bottom of the ship?  



LEO:  We don't know how to build spacesuits.  Did you see that?



STEVE:  Yeah.



LEO:  Nobody knows how to build those original Apollo spacesuits.  They're starting from scratch.



STEVE:  Wow.  Okay.  So I wanted to follow up on our report from two weeks ago about the massive and significant AT&T data breach.  Reading BleepingComputer's update, I was put in mind of the practice known as "rolling disclosure," where the disclosing party successively denies the truth, only admitting to what is already known when evidence of that has been presented.  The trouble here, of course, is that AT&T is a massive publicly owned communications carrier that holds deep details about their U.S. consumer users.  And an example of this incident suggests that they could be acting far more responsibly; and, moreover, that by not having done so for the past five years, the damage inflicted upon their own customers has likely been far worse than it needed to be.  I mean, it's just - it's such bad practice.



Anyway, so BleepingComputer's reporting was headlined:  "AT&T now says data breach impacted 51 million customers," and the annoyance in this piece's author is readily apparent.  I've edited it just a bit to be more clear for the podcast, but BleepingComputer posted this.



They said:  "AT&T is notifying 51 million former and current customers, warning them of a data breach that exposed their personal information on a hacking forum.  However, the company has still not disclosed how the data was obtained.  These notifications are related to the recent leak of a massive amount of AT&T customer data on the breach hacking forums that was first offered for sale for $1 million back in 2021.



"When threat actor ShinyHunters first listed the AT&T data for sale in 2021, AT&T told BleepingComputer that the collection did not belong to them, and that their systems had not been breached.  Last month, when another threat actor known as MajorNelson" - I guess he's an "I Dream of Jeannie" fan - "MajorNelson leaked the entire dataset on the hacking forum, AT&T once again told BleepingComputer that the data did not originate from them, and that their systems had not been breached.



"After BleepingComputer confirmed that the data did belong to AT&T and DirecTV accounts, and TechCrunch reported" - as we've reported two weeks ago - "that AT&T's logon passcodes were part of the data dump" - a little hard to deny that one - "AT&T finally confirmed that the data did belong to them."  You know, it's like, "Oh, that data."  Uh-huh.  Right.



Anyway, "While the leak contained information for more than 70 million people, AT&T now says that it impacted a total of 51,226,382 customers."  So apparently they've been doing some research into that data.  "AT&T's most recent notification states:  'The exposed information varied by individual and account, but may have included full name, email address, mailing address, phone number, social security number, date of birth, AT&T account number, and AT&T passcode.  To the best of our knowledge, personal financial information and call history were not included.'"  Oh, goodie.  So call history not there, but who cares?  They said:  "'Based on our investigation to date, the data appears to be from June 2019 or earlier.'



"BleepingComputer contacted AT&T, asking why there is such a large difference in impacted customers.  They said:  'We are sending a communication to each person'" - that is, again, 51,226,382 people.



LEO:  What?



STEVE:  Yeah.



LEO:  51 million?



STEVE:  226,382.



LEO:  Wasn't it, like, eight or nine million when this all began?



STEVE:  Yes.  That's right.  They've done a little more research and said, well, it's a little worse than we first told you.



LEO:  52 million?



STEVE:  That's the definition of a rolling disclosure.



LEO:  Yes.  Holy cow.



STEVE:  So, yeah, it's like, oh, that data.  Oh.



LEO:  Yeah, yeah, the ones with the PINs.  Oh, yeah.



STEVE:  That's right.



LEO:  Yeah.



STEVE:  Got a little hard to deny that one now.



LEO:  Well, they think maybe a contractor - now, that's the thing.  They don't know.



STEVE:  So, oh, but they said:  "We're sending a communication to each person whose sensitive personal information was included.  Some people had more than one account in the dataset, and others did not have sensitive personal information."  Because BleepingComputer said, wait a minute, you said more than 70 million, but now you're sending notices to 51,226,382.7.  Anyway, "The company has still," writes BleepingComputer, "not disclosed how the data was stolen, and why it took them almost five years to confirm that it belonged to them, and to alert their customers."



So okay.  I can understand that today, five years downstream, they may not know how it was stolen.  It's at least believable that, if they didn't ever look, then they would have never found out; right?  But the denial of the evidence they were shown many years ago is, you know, is difficult to excuse.  They were first shown this in 2001.  And it does appear that it's not going to be excused.



BleepingComputer said:  "Furthermore, the company told" - meaning AT&T told - "the Maine Attorney General's Office that they first learned of the breach on March 26, 2024, yet BleepingComputer first contacted AT&T about it on March 17th and the information was for sale first in 2021.



"While it is likely too late, as the data has been privately circulating for years, AT&T is offering" - this is so big of them - "one year of identity theft protection" - whatever that means - "and credit monitoring services through Experian, with instructions enclosed in the notices.  The enrollment deadline was set to August 30th of this year."  So a few months from now.  "But exposed people should move faster" - yeah, all 51 million of you - "to protect themselves.



"Recipients are urged to stay vigilant, monitor their accounts and credit reports for suspicious activity, and treat unsolicited communications with elevated caution.  For the admitted security lapse and the massive delay in verifying the data breach claims and informing affected customers accordingly" - not surprisingly - "AT&T is facing multiple class-action lawsuits in the United States.



"Considering that the data was stolen in 2021, cybercriminals had ample opportunity to exploit the dataset and launch targeted attacks against exposed AT&T customers.  However," finishes Bleeping Computer, "the dataset has now been leaked to the broader cybercrime community."  That is, no longer privately circulated.



LEO:  Oh, good.



STEVE:  Now everybody has it.



LEO:  The broader cybercrime community.



STEVE:  That's right, "exponentially increasing the risk for former and current AT&T customers."  



Okay.  So just so we're clear here, and so that all of our listeners new and old understand the nature of the risk this potentially presents to AT&T's customers:  Armed with the personal data that has been confirmed and admitted to having been disclosed - specifically Social Security numbers, dates of birth, physical addresses, and of course names - that is all that's needed to empower bad guys to apply for and establish new credit accounts in the names of creditworthy individuals.  This is one of the most severe consequences of what is commonly known as "identity theft," and it is a true nightmare.



The way this is done is that an individual's private information is used to impersonate them when a bad guy applies for credit in their name, using what amounts to their identity.  That bad guy then drains that freshly established credit account and disappears, leaving the individual on the hook for the debt that has just been incurred.  Since anyone might do this themselves, then claim that it really wasn't them, and that "It must have been identity theft, your Honor," proving this wasn't really them running, like, a scam of their own can be nearly impossible.  And among other things, it can mess up someone's credit for the rest of their lives.



There's only one way to stop this, which is to prevent anyone, including ourselves, from applying for and receiving any new credit by preemptively freezing our credit reports at each of the three major credit reporting agencies.  I know we've covered this before, at the beginning of the year, in fact.  But it just bears repeating.  And we may have some new listeners who haven't already heard this, or existing listeners who intended to take this action before, but never got around to it.



So here's my point:  In this day and age of repeated, almost constant, inadvertent online information disclosure, it is no longer safe or practical for our personal credit reporting to ever be left unfrozen by default.  Rather than freezing our credit if we receive notification of a breach that might affect us - and after all it could take five years, you know, and 51,226,382 of AT&T's current and former customers are now being informed five years after the fact, everyone should have their credit always frozen by default.



And in fact I can see some legislation in the future where somehow this is the policy because it's still wrong that by default it is open.  But of course all the bureaus want it to be because that's how they make money is by selling access to our credit, which we never gave them permission to do, and they just took it.  And then, once our credit is frozen by default, only briefly and selectively unfreezing it when a credit report does actually need to be made available for some entity to whom we wish and need to prove our credit worthiness.  



Anyway, the last time I talked about this I created a GRC shortcut link for that podcast, which was #956.  But I want to make it even easier to get to this page.  So you can get all the details about how to freeze your credit reports by going to grc.sc/credit.  So just put into your browser grc.sc, as in shortcut, grc.sc/credit.  That will bounce your browser over to a terrific article at Investopedia that I verified is still there, still current, and still great information.



And on the heels of that, just to put an exclamation mark at the end of that news, another disclosure was just made and reported under the headline "Hackers Siphon 340,000 Social Security Numbers From U.S. Consulting Firm."  CySecurity wrote:  "Greylock McKinnon Associates (GMA) has discovered a data breach in which hackers gained access to 341,650 Social Security numbers.  The data breach was disclosed last week on Friday on Maine's government website, where the state issues data breach notifications.  In its data breach warning mailed to impacted individuals, GMA stated that it was targeted by an undisclosed cyberattack in May of 2023 and 'promptly took steps to mitigate the incident.'"  Unfortunately, they didn't promptly apparently disclose it until now, almost a year later.  



"GMA provides economic and litigation support to companies and government agencies in the U.S., including the DOJ, that are involved in civil action.  According to their data breach notification, GMA informed affected individuals" - okay, they were informed - "that their personal information 'was obtained by the U.S. Department of Justice as part of a civil litigation matter' which was supported by GMA.  The purpose and target of the DOJ's civil litigation are unknown.  A Justice Department representative did not return a request for comment.



"GMA stated that individuals that were notified of the data breach are 'not the subject of this investigation or the associated litigation matters,' adding that the cyberattack 'does not impact your current Medicare benefits or coverage.  We consulted with third-party cybersecurity specialists to assist in our response to the incident, and we notified law enforcement and the DOJ.  We received confirmation of which individuals' information was affected, and obtained their contact addresses on February 7, 2024.'"  So, whoops, it was almost a year before people were notified.



"GMA notified victims that" - here it is - "'Your private and Medicare data was likely affected in this incident.'"  Now, you've got to love the choice of the word "affected."  You mean as in "obtained by malicious hackers?"  Yeah.  Anyway, they said:  "...which included names, dates of birth, home addresses, some medical and health insurance information, Medicare claim numbers, and Social Security numbers.



"Finally, it remains unknown why GMA took nine months to discover the scope of the incident and notify victims.  GMA and its outside legal counsel" - been very active lately - "Linn Freedman of Robinson & Cole LLP, did not immediately respond to a request for comment."



So as I noted above, this is now happening all the time.  It is no longer safe to leave one's credit report unfrozen, and freezing it everywhere will only take a few minutes.  Since you won't want to be locked out of your own credit reporting afterwards, however, be sure to securely record the details you'll need when it comes time to briefly and selectively unfreeze your credit in the future.



And Leo, let's take a break before we talk about cookie notice compliance or lack thereof.



LEO:  By the way, you mentioned in your credit report you didn't give them permission.  You did.  But it was in the very, very fine print of that credit card agreement of every agreement you make.  They put it in the fine print that we will submit information to the credit reporting bureaus.



STEVE:  Okay, thank you.  I'm glad you caught that.



LEO:  Yeah.  So you did agree to it.  But, you know, I mean, the truth is it's how the world works because it is a sensible system for having somebody who wants to lend you money have some way of verifying that you're a good prospect.



STEVE:  Yup.



LEO:  So every time you buy a car, rent a house, get a new phone, cell phone, set up a cell phone carrier, they do that.  They pull those credit reports.  I freeze mine, though, and I think you're absolutely right.  Everything I have is frozen.  And thanks to a federal law, they can't charge you to unfreeze it.  They used to charge you 35 bucks.  In some states it was more to unfreeze it.  They can't do that anymore.



STEVE:  Yeah.  Yeah, and to add confusion, there's also the term "lock," which does not mean the same as "freeze."



LEO:  Yeah, not the same, yeah.



STEVE:  So it's freezing your report, not locking your report.



LEO:  Freeze it, don't lock it.



STEVE:  Yup.



LEO:  Now, there were some good show titles in there, but I know you already have one, so I won't belabor it.  But "Freeze It, Don't Lock It" would be a good show title.



STEVE:  Okay.  So the following very interesting research was originally slated to be this week's major discussion topic.



LEO:  Ah.



STEVE:  Uh-huh.  But after I spent some time looking into the recently revealed GhostRace problem, I wanted to talk about that instead since it brings in some very cool fundamental computer science about the problem of "race conditions" which, interestingly, in our 20 years of this podcast - well, we're in our 20th year - we've never touched on race conditions.  So we're going to resolve that today.



But what this team of five guys from ETH Zurich discovered was very interesting, too.  So here's that.  They began asking themselves:  "When we go to a website that presents us with what has now become the rather generic and GDPR-required Cookie Permission pop-up, do sites where permission is not explicitly granted and cookie use is explicitly denied actually honor that denial?"  And with that open question on the table, the follow-up question was:  "Can we arrange to create an automated process to obtain demographic information about the cookie permission handling of the top 100,000 websites?"



Well, we already know the answer to the second question.  That's "yes."  You know?  They figured out how to automate this data collection.  And the results of their research will be presented during the 33rd USENIX Security Symposium, which will take place this coming August 14th through the 16th in Philadelphia.  Their paper is titled "Automated Large-Scale Analysis of Cookie Notice Compliance."  And for anyone who's interested, I have a link to their PDF research and the presentation prepub in the show notes.



Perhaps it won't come as any shock that what they found was somewhat disappointing.  Did they find that 5% of the 97,090 websites they surveyed did not obey the sites' visitors' explicit denial of permission to store privacy invading cookies?  No.  Was the number 10%?  Nope, not that either.  How about 15?  Nope, still too low.  Believe it or not, 65.4% of all websites tested, so just shy of fully two thirds of all websites tested - two thirds - do not obey their visitors' explicit privacy requests.  They wrote:  "We find that 65.4% of websites do not respect users' negative consent, and that top-ranked websites are more likely to ignore users' choice, despite having seemingly more compliant cookie notices."



The Abstract of their paper says:  "Privacy regulations such as the General Data Protection Regulation (GDPR) require websites to inform EU-based users about non-essential data collection and to request their consent to this practice.  Previous studies have documented widespread violations of these regulations.  However, these studies provide a limited view of the general compliance picture.  They're either restricted to a subset of notice types, detect only simple violations using prescribed patterns, or analyze notices manually.  Thus, they're restricted both in their scope and in their ability to analyze violations at scale.



"We present the first general, automated, large-scale analysis of cookie notice compliance.  Our method interacts with cookie notices, in other words, by navigating through their settings.  It observes declared processing purposes and available consent options using Natural Language Processing and compares them to the actual use of cookies.  By virtue of the generality and scale of our analysis, we correct for the selection bias present in previous studies focusing on specific Content Management Platforms.  We also provide a more general view of the overall compliance picture using a set of 97,000 websites popular in the EU.  We report, in particular, that 65.4% of websites offering a cookie rejection option likely collect user data despite explicit negative consent."



Okay.  So I suppose we should not be surprised to learn that what's hiding behind the curtain is not what we would hope.  Two thirds of companies in general - the larger they are, the worse is their behavior - are simply blowing off their visitors' explicit requests for privacy.  And this is in the EU, where the regulation is present and would be more likely to be strongly enforced.  Except who would know if the regulation was being ignored unless you checked.  So not only are we now being hassled by the presence of these cookie permission banners, but two out of every three required "do nothing clicks."  When we say we want you not to do this, they have no actual effect.



The researchers discovered additional unwanted behavior which they termed "dark patterns."  Specifically, 32% are missing the notice entirely.  56.7% don't have a reject button.  So they display the notice, but they don't give you the required option to opt out.  And then there's those 65.4% that do have the button, but they ignore it.  Then there's 73.4% with implicit consent prior to interaction, meaning they assume you are consenting, and they do things in 73.4% of the cases prior to giving you the option to not have them do that.  There is a category, implicit consent after close, that's there in 77.5% of the instances.  Undeclared purposes in 26.1, so they don't tell you what's going on.  There's interface interference in two thirds of the cases, 67.7%; and a forced action in almost half, 46.5.  So bottom line, you know, this is all a mess. 



What this means is that the true enforcement of our privacy cannot be left to lawmakers and their legislation, nor even to websites.  New technology needs to be brought to bear to take this out of the hands entirely of third parties.  And I know it seems insane for me to be saying this, but this really is what Google's Privacy Sandbox has been designed to do.  And as with all change, advertisers and websites are going to be kicking and screaming, and they're not going to go down without a fight.



But the advertisers are also, reluctantly, currently in the process of upgrading their ad delivery architectures for the Privacy Sandbox because they know that, with Chrome commanding two thirds market share, they're going to have to work within this brave new world that's coming soon, you know, and real soon, as in later this year.  Google's not messing around this time.  They've given everyone years' of notice.  Change is coming.  And I say thank goodness because it's obvious that just saying, oh, you know, you've got to get permission, well, looks like websites, most of them, are asking for permission.  It turns out two thirds of them are ignoring it when we say no.



And speaking of the GDPR, which created this accursed cookie pop-up legislation which currently plagues the Internet, holding to the "there's two sides to every coin" rule, the requirements for disclosure that's also part of the GDPR legislation occasionally produces some startling revelations when conduct that parties would have doubtless much preferred to remain off the radar are required to be seen.  Get a load of this one.



In this case, we have the new Outlook app for Windows.  Thanks to the GDPR, users in Europe who download the Outlook for Windows app will be greeted with a modal pop-up dialog that displays a user agreement and requires its users' consent.  So far, so good.  The breathtaking aspect of this is that the dialogue starts right out stating:  "We and 772 third parties process data to store and/or access information on your device."



LEO:  Oh, only 700, huh?



STEVE:  772 third parties that Microsoft is sharing their users' data with.



LEO:  More computers is slow, but lord.



STEVE:  Wow.



LEO:  Wow.



STEVE:  "To develop and improve products, personalize ads and content, measure ads and content, derive audience insights, obtain precise geolocation data" - yes, we know right where you are, and we're telling everyone - "and identify users through device scanning."  And, no, that was not a typo.  It says "We and 772 third parties."



LEO:  At least they're honest.  I mean...



STEVE:  Well, they have no choice; right?  In the EU.



LEO:  Yeah, that's awesome.



STEVE:  You know, I don't know what to think about that.  It's mindboggling.  For one thing, it's disappointing that only those in the EU get to see this.  U.S. regulators are not forcing the same transparency requirements upon Microsoft, and Microsoft certainly doesn't want to show any more of this than they're forced to.



LEO:  No.  Yeah.



STEVE:  You know, earlier we were talking about the confidentiality of our data and the challenge that presents.  Having Microsoft profiting from the sale of the contents of its users' email seems annoying enough.  At the same time, you know, there's Gmail, and they're doing some of that, too.  But when there are, by Microsoft's own forced admission, 772 such third parties who all, presumably, receive paid access to this data, doesn't it feel as though Microsoft should be paying us for the privilege of access to our private communications, which is apparently so valuable to them?  Instead, we get free email.  Whoopee.  Wow.



LEO:  Does Google do anything like that with Gmail?  It's got to be - that's wild.



STEVE:  No.  I've never - whoa.  Oh, you mean in the EU.  That's a good question, whether they have had to start that, too.  Yeah.



LEO:  I think it's the case, although I'm not a lawyer, that this is in the EU, all of this; that even though we all in the U.S. and everywhere else in the world see these cookie monster announcements, they don't - they're not technically required to do them for us.  It's only for EU people. 



STEVE:  Well, as I understand it, if an EU person visits your website...



LEO:  No, it's still not liable because it has to be a company doing - well, I don't know, that's a good question.  There was a very interesting piece which I've mentioned on other shows.  And of course the guy's not in [indiscernible] either.  Blog piece that said no one has to do this except EU companies.  They're the only companies liable for this.  And he points out Amazon does not.  Right?



STEVE:  Interesting.



LEO:  You've never seen it on Amazon.



STEVE:  Interesting.



LEO:  You don't see it on a lot of sites.  



STEVE:  Not mine.



LEO:  I don't see cookies on Google sites, come to think of it.



STEVE:  GRC doesn't have it, although I'm not collecting any information and using cookies that way.



LEO:  Well, we do it on TWiT because there's a whole secondary business of people who go around saying, "You'd better do this for compliance," and "Pay me, and I'll do it for you."  And they said otherwise you're going to be in trouble.  So we have a lot of privacy stuff on our site.  We don't - you can't log into our site.  But we, as every site does, we have cookies.  We have loggers, you know, I mean, I do it on my personal website.  I say, "Look, the only cookie I have is whether you like the light mode or dark mode.  So suck it.  If you don't like that, too bad."  I mean, that's why this whole thing is so silly.  I just, it's so silly.



STEVE:  Well, it's probably going to go away because if Google has their way...



LEO:  Maybe.



STEVE:  ...and Google generally gets their way, you know, they've got the demographics, the market share.  They really are terminating third-party cookies.



LEO:  We've done this story before because remember you talked about Do Not Track, which is in the spec.  Everybody has it.  Nobody honors it.



STEVE:  It's coming back.  It's on its way back.



LEO:  I think it would be nice to see, yeah.



STEVE:  Yeah, yeah.  Well, and actually, again, what's happening is that the next-generation technology will not track.



LEO:  Right.



STEVE:  By siloing each site, the tracking becomes, or the tracking happens because traditionally browsers have had one big cookie jar.  As soon as you start creating individual per-domain cookie jars, then each domain is welcome to have its own local set of cookies, but they're not visible from any other domain.  And that terminates tracking.



LEO:  Right.



STEVE:  Unfortunately, that's why we're now doing the, oh, please join our email, you know, join our community, give us your email.  And so basically first-party cookies are being used to track through the browser.



So, okay.  We have a bit of feedback from our listeners.  And then, boy, have we got a fun techie episode.  Okay.  Let's see.  I got:  "@SGgrc.  Been hearing you talk about the physical input requirement for security setting modifications," meaning like push the button on the browser if you want to make a security change, and that keeps somebody in Russia from being able to push a button on your browser.  This person said:  "Just wanted to mention Fritz and Fritz!Box, a common German home router manufacturer who's been doing this for a while."  And he said:  "Annoying, yes.  Effective, also yes." 



So, you know, when we've talked about this previously I haven't mentioned that many SoHo routers, probably everyone's router if it's at all recent, have that dedicated WPS hardware button that is a physical button for essentially the same reason.  It allows a network device to be persistently connected to a router without the need to provide the device with the router's WiFi SSID name and password.  That button being physical requires someone to prove their presence at the router by pressing the button.  So the notion of doing something similar to similarly protect configuration changes makes a lot of sense.



But then another listener suggested, this was Rob Woodruff who tweeted.  He said:  "I'm listening to the part of SN-969 where you're talking about the five security best practices.  And it occurs to me that, regarding the requirement for a physical button press, it won't be long before we start seeing cheap, Chinese-made, WiFi-enabled IoT button pressers on Amazon.  What could possibly go wrong?"



LEO:  They exist, by the way.  Stacey Higginbotham used to talk about them.



STEVE:  A little thing that presses a button?



LEO:  Yeah.  It's a WiFi-enabled little finger that goes like this.



STEVE:  Oh.



LEO:  And for lights that don't - you can't change or whatever the - yeah, yeah.  So it exists.



STEVE:  Too funny.



LEO:  I don't know if it would work with that WPS-type button, but... 



STEVE:  Yeah.  And actually I think I remember watching that This Week in Google when you guys talked about that.



LEO:  Yeah, yeah, yeah.



STEVE:  And I thought that was...



LEO:  Stacey used them.



STEVE:  Wow.



LEO:  She liked them.



STEVE:  Remember that thing when we were growing up, the little black box that you would stick a penny on, and the hand would reach out and grab the penny?



LEO:  Right.  But my favorite one, though, there's varieties of these, you'd flip the switch, the hand comes out and turns the switch off.



STEVE:  Yes, yes.



LEO:  Loved those.



STEVE:  Okay.  So the downside of the physical button press requirement, you know, obviously is its inherent inconvenience.  You know?  But that's also the source of its security.  It would be nice if we could have the security without the inconvenience, but it's unclear how we could go about doing that.  But to Rob's point, even if a user chose to employ a third-party remote control button presser, that would still provide greater security than having no button at all.  For one thing, it would be necessary to hack two very different systems, kind of like having multifactor authentication.



But the most interesting observation, I think, is that generic remote attacks against an entire class of devices that were known to be "button protected" would never even be launched in the first place because it would be known that they could not succeed.  So even a router that technically breaks the rules with an automated button presser gains the benefit of what is essentially herd immunity because, you know, attackers wouldn't bother attacking that model router because they're unattackable, thanks to having the physical button.



Rami Vaspami tweeted:  "@SGgrc I don't see the benefit if NetSecFish kept quiet."  So this person's talking about that premature, well, the disclosure of the incredible amount of old D-Link NAS devices.  So they write:  "This would have been quietly exploited in perpetuity.  Instead, now, everyone knows to deprecate those devices, and the issue has an end in sight.  Better to know."



Okay.  If, indeed, current D-Link NAS owners could have been notified as a consequence of NetSecFish's disclosure on GitHub, then okay, maybe.  But they still would have had to act quickly since the attacks against these D-Link NAS boxes required less than two weeks to begin.  But the problem is that nothing about NetSecFish's disclosure on GitHub suggests that even a single end-user of these older but still in use D-Link NAS devices would have ever received notification.  Notification from whom?  You know, he posted this on GitHub.  That's not notifying all those D-Link NAS end-users.  Why would any random user know what's been posted in someone's GitHub account?  D-Link said, "Sorry, but those are old devices for which we no longer accept any responsibility."



But we know who does know what's been posted to someone's random GitHub account, since it took less than two weeks for the attacks to begin.  So I would argue that it is not "better to know" when it's by far predominantly bad guys who will be paying attention and who will be doing the knowing.



And finally, John Moriarty wins the "feedback of the week award."  He tweeted:  "Catching up on last week's Security Now!  and hearing @SGgrc lament the fans spinning up and resources being swamped by Google's Chrome bloat while it was apparently 'doing nothing.'  Perhaps it was busy running and completing ad auctions."



LEO:  Oh.



STEVE:  So yes, indeed.  It's quite true that our browsers are going to be much busier once responsibility for ad selection has been turned over to them.  Although, you know, our computers are so overpowered now it's ridiculous.  You know, using a browser shouldn't even wake the computer up.



I don't have any big SpinRite news this week.  Everything continues to go well, and I'm working to update GRC's SpinRite pages just enough to hold us while I get email running, and then back over to work on SpinRite's documentation.  I've been seeing more good reports of SSD original performance recovery.  While catching up in Twitter, I did run across a good question about SpinRite.  Paul, tweeting from @masonpaulak, he said:  "Hi, Steve.  Long-time podcast listener and SpinRite owner.  Thanks for the many gems of info over the years."  And boy, have we got a good one for this podcast coming up.  "Over the weekend," he said, "I saw the RAM test in SpinRite 6.1.  Please, would you give some insight into how long I should let it run to test 1GB?  And am I wearing it out as writing to an SSD would?  Thanks, Paul."



I don't think I remembered to respond to the wearing it out question.  No.  DRAM absolutely, thank goodness, does not wear out when it's being read and/or written.  Okay.  So but to his question of how long you should run it, the best answer to this is really quite frustrating because the best answer is "the longer the better."  You know, now, that said, I can clarify a couple of points.  The total amount of RAM in a system has no bearing on this, since SpinRite is only testing the specific 54MB that it will be using once the testing is over.  While the testing is underway, a test counter is spinning upwards on the screen, and a total elapsed testing time is shown.  For each test it fills 54MB with a random pattern, then comes back to reread and verify that memory was stored correctly.  So the longer the testing runs, the greater the likelihood that it will find a pattern that causes some trouble.



The confusion, and Paul's question, comes from the fact that the new startup RAM test can be interrupted.  It doesn't stop by itself.  It just goes.  And eventually the user's going to interrupt it in order to get going with SpinRite.  So when should that be?  Best practice, if it's feasible, would be for any new machine where SpinRite 6.1 has never been run before, to start it up and allow it to just run on that machine overnight, doing nothing but testing its RAM.  In the morning, the screen will almost certainly still be happy and still showing zero errors.  I mean, that's by far the most likely case.  But the screen might have turned red, indicating that one or more verification passes failed.



After I asked that first guy who was having weird problems that made no sense to try running MemTest86, the venerable, you know, full system RAM test that's been around forever, it reported errors.  So I decided I needed to build that into SpinRite, just to be safe.  Users do not need to use it at all, but it's there.  And several other of SpinRite's early testers were surprised to have SpinRite detect previously unsuspected memory errors on some of their machines.  So SpinRite's built-in RAM memory test is sort of useful all by itself, just for that.  But once it's been run for a few hours on any given machine, then it can just be bypassed quickly from then on, and you don't need to worry about it.



Okay.  So if anybody is doing brain surgery right now...



LEO:  Driving a large truck...



STEVE:  Yes, close the skull.  Put down your tools.



LEO:  Pull over.



STEVE:  Yes.  The first question to answer is "What's a Race Condition?"  A race condition is any situation which might occur in hardware or software systems where outcome uncertainty or an outright bug is created when the sequence of closely timed events, in other words a race among events, determines the effects of those events.  For example, in digital electronics, it is often the case that the state of a set of data lines is captured when a clock signal occurs.  This is typically known as "latching" the data lines on a clock.  But for this to work reliably, the data lines need to be stable and settled when the clocking occurs.  If the data lines are still changing state when the clock occurs, or if the clock occurs too soon, a race condition can occur where the data that's latched is incorrect.



Here's how Wikipedia defines the term.  They write:  "A race condition or race hazard is the condition of an electronics, software, or other system where the system's substantive behavior is dependent on the sequence or timing of other uncontrollable events, leading to unexpected or inconsistent results.  It becomes a bug when one or more of the possible behaviors is undesirable.  The term 'race condition' was already in use by 1954, for example in David A. Huffman's doctoral thesis 'The synthesis of sequential switching circuits.'  Race conditions can occur especially in logic circuits, multithreaded, or distributed software programs."



Okay.  Race conditions are also a huge issue with modern software systems because our machines now have so many things going on at once where the independent actions of individual threads of execution must be synchronized.  Any failure in that synchronization can result in bugs or even serious security vulnerabilities.



To understand this fully, we need to switch into computer science mode.  And as I said at the top of the show, I often receive feedback from our listeners wishing that I did more of this, but I don't often encounter such a perfect opportunity as we have today.  Back when we were talking about how processors work during one of our early sets of podcasts on that topic, we talked about the concepts of multithreading and multitasking.



In a preemptive multithreaded environment, "multithreaded" meaning multiple threads of execution happening simultaneously, the concept of a thread of execution is an abstraction.  The examples I'm going to draw assume a single core system, but they apply equally to multicore systems.  When a processor is busily executing code, performing some task, we call that a "thread of execution" since the processor is threading its way through the system's code, executing one instruction after another.  After a certain amount of time, a hardware interrupt event occurs when a system clock ticks.  This interrupts that thread of execution, causing it to be suspended and control to be returned to the operating system.



If the operating system sees that this thread has been running long enough, and that there are other waiting threads, the OS decides to give some other thread a turn to run.  So it does something known as a "context switch."  A thread's context is the entire state of execution at any moment in time.  So it's all of the registers the thread is working with and any other thread-specific volatile information.  The operating system saves all of that thread context, then reloads another thread's saved context, which is to say everything that had been previously saved, you know, from other thread when it was suspended.  The OS then jumps to the point in code where that other thread was rudely interrupted the last time it was preempted, and that other thread resumes running just as though nothing had ever happened.



And that's a critical point.  From the viewpoint of the many threads all running in the system, the threads themselves are completely oblivious to all of this going on above them.  As far as they're concerned, they're just running and running and running without interruption, as if each of them were all alone in the system.  The fact that there's an overlord frantically switching among them is unseen by the threads.  I just opened Task Manager on my Windows 10 machine.  It shows that there are 2,836 individual threads that have been created and are in some state of execution right at this moment.  That's a busy system.



Okay.  So now let's dive deeper into very cool and very important coding technology.  The reason for this dive will immediately become clear once we look at the vulnerabilities that the researchers from VU Amsterdam and IBM have uncovered in all contemporary processor architectures - not just Intel this time, or AMD, also ARM and RISC-V - any architecture that incorporates - get ready for it - speculative execution technology, as all modern processor architectures do.



Okay.  Imagine that two different threads running in the same system share access to some object.  Anyone who's been around computing for long will have heard the term "object oriented programming."  An "object" is another abstraction that can mean different things in different contexts.  But what's important is that it's a way of thinking about and conceptually organizing complex systems that often have many moving parts.  So when I use the term "object," I'm just referring to something as a unit.  It might be a single word in memory, or it might be a large allocation of memory, or it could be a network interface, whatever.  In the examples we'll be using, our object of interest is just a word of RAM memory that stores a count of events.  So this object is a simple counter.



And in our system, various threads might, among many other things, have the task of incrementing this counter from time to time.  So let's closely examine two of those threads.  Imagine that the first thread decides it wishes to increment the count.  So it reads the counter's current value from memory into a register.  It might examine it to make sure that it has not hit some maximum count value.  Then it increments the value in the register and stores it back out into the counter memory.  No problem.  The shared value in memory was incremented.  But remember that, in our hypothetical system, there are at least two separate threads that might be able to do this.



So now imagine a different scenario.  Just like the first time, the first thread reads the current counter value from main memory into one of its registers.  It examines its value and sees that it's okay to increment the count.  But just at that instance, at that instant, the system's thread interrupt occurs; and that first thread's operation is paused to give other threads the opportunity to run.  And as it happens, that other thread that may also decide to increment that counter decides that it wishes to do so.  So it reads that shared value from memory, examines it, increments its register, and saves it back out to memory.  And as luck would have it, for whatever reason, while that second thread is still running, it does that four more times.  So the counter in memory was counted up by a total of five times.  Then after a while that second thread's time is up, and the operating system suspends it to give other threads a chance to run.



Eventually, the operating system resumes our first thread from the exact point where it left off.  Remember that before it was paused by the operating system, it had already read the counter's value from memory, examined it, and decided that it should be incremented.  So it increments the value in its register and stores it back out to memory, just as it did the first time.



But look what just happened.  Those five counter increments that were performed by the second thread, which shares access to the counter with the first thread, were all lost.  The first thread had read the counter's value into a register.  Then, while it was suspended by the operating system, the second thread came along and incremented that shared storage five times.  But when the first thread was resumed, it stored the incremented counter value that it had previously saved back out, thus overwriting the changes made by the second thread.



This is a classic and clear example of a race condition in software.  It occurred because of the one in a million chance that the first thread would be preempted at the exact instant, between instructions, when it had made a copy of some shared data that another thread might also modify, and it hadn't yet written it back to the shared memory location.  The picture I painted was deliberately clear because I wanted to highlight the problem.  But in the real world it doesn't happen that way, and these sorts of race condition problems are so easy to miss.



Imagine that two different programmers working on the same project were each responsible for their piece of code, and each of them increments that counter.  If they didn't realize that the other had code that might also choose to increment that counter, this would introduce an incredibly subtle and difficult to ever find bug.  Normally, everything would work perfectly.  The system's tested.  Everything's great.  But then every once in a blue moon the code, which looks perfect, would not do what it was supposed to.  And the nature of the bug is so subtle that coders could stare at their code endlessly without ever realizing what was going on.



Last Tuesday was Patch Tuesday, and Microsoft delighted us all by repairing another 149 bugs, which included two that created vulnerabilities which were under active exploitation at the time.  Microsoft's software does not appear to be in any danger of running out of bugs to fix, and race conditions are one of the major reasons for that.  How many times have we talked about so-called "use-after-free" bugs, where malicious code retains a pointer to some memory that was released back to the system and is then able to use that pointer for malicious purposes?  More often than not, those are deliberately leveraged race condition bugs.



Okay.  So now the question is, as computer science people, how do we solve this race condition problem?  The answer is both tricky and gratifyingly elegant.  We first introduce the concept of ownership, where an object can only be owned by one thread at a time, and only the current owner of an object has permission to modify its value.  In the example I've painted, the race condition occurred because two threads were both attempting to alter the same object, the shared counter, at the same time.  If either thread were forced to obtain exclusive ownership of the counter until it was finished with it, the competing thread might need to wait until the other thread had released its ownership, but this would completely eliminate the race condition bug.



Okay.  So at the code level, how do we handle ownership?  We could observe that modern operating systems typically provide a group of operations known as synchronization primitives.  They're called this because they can be used to synchronize the execution of multiple threads that are competing for access to shared resources.  The primitive that would be used to solve a problem like this one is known as a "mutex," which is short for "mutual exclusion," meaning that threads are mutually excluded from having simultaneous access to a shared object.



So by using an operating system's built-in mutex function, a thread would ask the operating system to give it exclusive ownership of an object and be willing to wait for it to become available.  If the object were not currently owned, then it would become owned by that first requesting thread, and that owning thread could do whatever it wished with the object.  Once it was finished, it would release its ownership, thus making the object available to another thread that may have been waiting for it.



But I said that what actually happens is both tricky and gratifyingly elegant, and we haven't seen that part yet.  How is a mutex actually implemented?  Our threads are living in an environment where anything and everything that they are doing might be preempted at any time, without any warning.  This literally happens in between instructions, between one instruction and the next instruction.  And in between any two instructions, anything else might have transpired.



If you think about that for a moment, you'll realize that this means that acquiring ownership of an object cannot require the use of multiple instructions that might be interrupted by the operating system at any point.  What we need is a single indivisible instruction that allows our threads to attempt to acquire ownership of a shared object, while also determining whether they succeeded.  And that has to happen all at once.



It is so cool that an instruction as simple as "exchange" can provide this entire service.  The exchange instruction does what its name suggests.  It exchanges the contents of two "things."  These might be two registers whose contents are exchanged, or it might be a location in memory whose contents are exchanged with a register.  We already have a counter, the ownership of which is the entire issue.  So we create another variable in memory to manage that counter's ownership, to represent whether or not the counter is currently owned by any thread.  We don't need to care which thread owns the counter because, if any thread does, no other thread can touch it.  All they can do is wait for the object's ownership to be released.



So we just need some binary, you know, a one or a zero.  If the value of this counter ownership variable is zero, then the counter is not owned by any thread at the moment.  And if the counter ownership variable is not zero, if it's a one, then some thread currently owns the counter.



Okay.  So how do we work with this and the exchange instruction?  When a thread wants to take ownership of the counter, it places a one in a register and exchanges that register's value with the current value of the ownership variable in memory.  If, after the exchange, its register, which received the previous value of the ownership variable, is a zero, then that means that until the exchange was made, the counter was not owned by any thread.  And since this exchange simultaneously placed a one into the ownership variable at the same time, with the same instruction,  our thread is now the exclusive owner of the counter and is free to do anything it wishes without fear of any collision.



If another thread comes along and wants to take ownership of the counter, it also places a one in a register and exchanges that with the counter's ownership variable.  But since the counter is still owned by the first thread, this second thread will get a one back in its exchanged register.  That tells it that the counter was already owned by some other thread, and that it's going to need to wait and try again later.  And notice that since what it swapped into the ownership variable was also a one, this exchange instruction did not change the ownership of the counter.  It was owned by someone else before, and it still is.  What the exchange instruction did in this instance was to allow a thread to query the ownership of the counter.  If it was not previously owned, it now would be by that query.  But if it was previously owned by some other thread, it would still be.



And finally, once the thread that had first obtained exclusive ownership of the counter is finished with the counter and is ready to release its ownership, it simply stores a zero into the counter's ownership variable in memory, thus marking the counter as currently unowned.  This means that the next thread to come along and swap a one into the ownership variable will get a zero back and know that it now owns the counter.  We call the exchange instruction when it's used like this an "atomic" operation because, like an atom, it is indivisible.  With that single instruction, we both acquire ownership if the object was free, while also obtaining the object's previous ownership status.  Okay.  I suppose I'm weird, but this is why I love coding so much.



The holistic way of viewing this is to appreciate that only atomic operations are safe in a preemptively multithreaded environment.  In our example, if the counter in question only needed to be incremented by multiple threads, and that incrementation could be performed by a single indivisible instruction, then that would have been "thread safe," which is the term used.  And there are instructions that increment memory in a single instruction.  But in our example, the threads needed to examine the current value of the counter to make a decision about whether it had hit its maximum value yet, and only then increment it.  That required multiple instructions and was not thread safe.  So what do we do?  We protect a collection of non-thread safe operations with a thread-safe operation.  We create the abstraction of ownership and use an exchange instruction, which being one instruction is thread safe, to protect a collection of operations that are not thread safe.



And this brings us to "GhostRace," the discovery made by a team of researchers, and the question, what would it mean if it were discovered that these critical, supposedly atomic and indivisible synchronization operations were not, after all, guaranteed to do what we thought?  And Leo, let's tell our listeners of our last sponsor, and then we're going to answer that question because the bottom just fell out of computer security.



LEO:  I guess you could have a race for the lock; right?  That would be part of it?  That's all you need to make them atomic operations; right?  You can have races all the way down.



STEVE:  Well, you can't have a race for the lock or for the ownership because only actually one instruction is executing at any time.



LEO:  Because it's atomic; right.



STEVE:  It feels like multiple things are going on.  But actually, as we know, the processor is just switching among threads.



LEO:  Right.



STEVE:  So at any given time there's only one thing happening.  And if that one thing grabs the lock, if it grabs ownership, then it has it.



LEO:  Right.



STEVE:  And that's the elegance of being able to do something in a single instruction.



LEO:  Right.  If it's multiple instructions, then you've got a problem.



STEVE:  Right.



LEO:  If it was a resource that takes, you know, three cycles to poll, then you have the problem of you could have it interrupted.



STEVE:  Exactly.



LEO:  It's really a fascinating ecosystem that's evolved.  And this actually, this happened even before there were multithreaded computers; right?  I mean, you could have a race condition anytime you have a branch, I guess.



STEVE:  Well, anytime you have timesharing - basically it's a matter of contention whenever there are multiple things contending for a single resource.



LEO:  Right.



STEVE:  They're all free to read it at once; right?  Everybody can read.



LEO:  It's writing it that's the problem.



STEVE:  But if anybody modifies it, then you're in trouble.



LEO:  Right.



STEVE:  Okay.  So for their paper's Abstract, the researchers wrote:  "Race conditions arise when multiple threads attempt to access a shared resource without proper synchronization, often leading to vulnerabilities such as concurrent use-after-free.  To mitigate their occurrence, operating systems rely on synchronization primitives such as mutexes, spinlocks, et cetera.



"In this paper, we present GhostRace, the first security analysis of these primitives on speculatively executed code paths.  Our key finding is that all of the common synchronization primitives can be microarchitecturally bypassed on speculative paths, turning all architecturally race-free critical regions into Speculative Race Conditions, or SRCs.



"To study the severity of SRCs, these Speculative Race Conditions," they wrote, "we focus on Speculative Concurrent Use-After-Free and uncover 1,283 potentially exploitable instances in the Linux kernel.  Moreover, we demonstrate that" - they abbreviate it SCUAF, Speculative Concurrent Use-After-Free - "information disclosure attacks against the kernel are not only practical, but that their reliability can closely match that of traditional Spectre attacks, with our proof of concept leaking kernel memory at 12KB per second.



"Crucially, we develop a new technique to create an unbounded race window, accommodating an arbitrary number of Speculative Concurrent Use-After-Free invocations required by an end-to-end attack in a single race window.  To address the new attack surface, we propose a generic SRC mitigation" - and we'll see about how expensive that is in a minute - "to harden all the affected synchronization primitives on Linux.  Our mitigation requires minimal kernel changes, but incurs around a 5% geometric mean performance overhead on LMbench."



And then they finish their Abstract by quoting Linus Torvalds on the topic of Speculative Race Conditions, saying:  "There's security, and then there's just being ridiculous."  Okay.  Well, they may not be so ridiculous after all.  What this comes down to is a mistake that's been made in the implementation of current operating system synchronization primitives in an environment where processors are speculating about their own future.  Somewhere in their implementations is a conditional branch that's responsible for making the synchronization decision.  And in speculatively executing processors it's possible to deliberately mistrain the speculation's branch predictor to effectively neuter the synchronization primitive.  As they say in their paper, to turn it into a no-op, which is the programmer shorthand for no operation.  And if that can be done, it's possible to wreak havoc.



Here's what they explained.  They said:  "Since the discovery of Spectre, security researchers have been scrambling to locate all the exploitable snippets or gadgets in victim software.  Particularly insidious is the first Spectre variant, exploiting conditional branch misprediction, since any victim code path guarded by a source 'if' statement may result in a gadget.  To identify practical Spectre-v1 gadgets, previous research has focused on speculative memory safety vulnerabilities, use-after-free, and type confusion.  However, much less attention has been devoted to other classes of normally architectural software bugs, such as concurrency bugs.



"To avoid, or at least reduce concurrency bugs, modern operating systems allow threads to safely access shared memory by means of synchronization primitives, such as mutexes and spinlocks.  In the absence of such primitives, for example, due to a software bug, critical regions would not be properly guarded to enforce mutual exclusion, and race conditions would arise.  While much prior work has focused on characterizing and facilitating the architectural exploitation of race conditions, very little is known about their prevalence on transiently executed code paths.  To shed light on the matter, in this paper we ask the following research questions:  First, how do synchronization primitives behave during speculative execution?  And second, what are the security implications for modern operating systems?



"To answer these questions, we analyze the implementation of common synchronization primitives in the Linux kernel.  Our key finding is that all the common primitives lack explicit serialization and guard the critical region with a conditional branch.  As a result, in an adversarial speculative execution environment, i.e., with a Spectre attacker mistraining the conditional branch, these primitives essentially behave like a no-op.  The security implications are significant, as an attacker can speculatively execute all the critical regions in victim software with no synchronization.



"Building on this finding, we present GhostRace, the first systematic analysis of Speculative Race Conditions, a new class of speculative execution vulnerabilities affecting all common synchronization primitives.  SRCs are pervasive, as an attacker can turn arbitrary architecturally race-free code into race conditions exploitable on a speculative path, in fact, one originating from the synchronization primitives' conditional branch itself.  While the effects of SRCs are not visible at the architectural level, for example, no crashes or deadlocks, due to the transient nature of speculative execution, a Spectre attacker can still observe their microarchitectural effects via side channels.  As a result, any SRC breaking security invariants can ultimately lead to Spectre gadgets disclosing victim data to the attacker."



Okay.  So essentially the specter of Spectre strikes again.  Another significant exploitation of speculative execution has arisen.  And need we echo once again Bruce Schneier's famous observation about attacks never getting worse and only ever getting better?



Under the paper's mitigation section they write briefly:  "To mitigate the Speculative Race Condition class of vulnerabilities, we implement and evaluate the simplest, most robust, and generic one, introducing a serializing instruction into every affected synchronization primitive before it grants access to the guarded critical region, thus terminating the speculative path.  This provides a baseline to evaluate any future mitigations; and, as mentioned, mitigates not only the Speculative Condition Use-After-Free vulnerabilities presented in the paper, but all other potential Speculative Race Condition vulnerabilities."



Okay.  So essentially what they're saying is that, as with all anti-Spectre mitigations, the solution is to deliberately shut down speculation at the spot where it's causing trouble.  In this instance, this requires the insertion of an additional so-called "serializing" instruction into the code at the point just after the synchronization instruction and before the result of the synchronization instruction is used to inform a branch instruction.  This prevents any speculative execution down either path following the branch instruction.  The problem with doing that is that speculation exists specifically because it's such a tremendous performance booster.  And that means that even just pausing speculation will hurt performance.



To determine how much this would hurt, for example, Linux's performance, these guys tweaked Linux's source code to do exactly that, to prevent the processor from speculatively executing down both paths following a synchronization primitive's conditional branch.  What they found was that the effect can be significant.  It introduces a 10.82% overhead when forking a Linux process, 7.53% overhead when executing a new process, 12.35% when deleting an empty 0K file, and 11.37% when deleting an 10K file.  Those are particularly bad.



But overall, as they wrote in their abstract, they measured a 5.13% performance cost overall from their mitigation.  Now, 5.13%, that's huge, and it suggests just how dependent upon synchronization primitives today's modern operating systems have become.  For a small change in one very specific piece of code to have that much impact means it's being used all the time.  



And finally, as for the disclosure of their discovery, they wrote:  "We disclosed Speculative Race Conditions to the major hardware vendors - Intel, AMD, ARM, and IBM - and the Linux kernel in late 2023.  Hardware vendors have further notified other affected software - OS and hypervisor - vendors, and all parties have acknowledged the reported issue, which has been given CVE-2024-2193.



"Specifically, AMD responded with an explicit impact statement which was that existing Spectre-v1 mitigations apply, pointing to the attacks relying on conditional branch mis-speculation like Spectre-v1."  In other words, they're just saying, yeah, this is more of the same nightmare.  If you want to fix it, deal with it the way you do conditional branch misprediction.  Which is to say block speculation before the branch.  The Linux kernel developers wrote back saying they have no immediate plans to implement serialization of synchronization primitives due to performance concerns.  In other words, it slows down Linux too much for them to do this, and no one has yet shown them that they absolutely must.  The bottom line is, it is unclear today what the ultimate solution will be.  And it's clear we don't have one yet.



Stepping back from all of this to look at the whole picture, we have a processor performance-addicted industry that several years ago awoke to the unhappy news that the extremely complex, tricky, and sophisticated way our processors had been arranging to squeeze out every last possible modicum of performance was also inherently exploitable because it opened the processor to side-channel attacks.



It turned out that performance was being enhanced by allowing the processor to dynamically learn from the execution of its past code and data.  Not only could this stored knowledge reveal secrets about the data that had recently been processed, but it was possible for any other software sharing the same hardware to probe for and obtain those secrets.  And that's still where we are today.  No one wants to relinquish the performance we've come to need, but the way we're achieving that performance inherently comes at the cost of security and privacy.



LEO:  Wow.  Very interesting.  So now you understand, everybody, why the speculative execution processors are so problematic.  It's got to be similar to what Apple's experiencing with the M1, M2, and M3 processors.  Same idea.



STEVE:  Yeah.  ARM processors have the same problem.  I mean, we have a problem, which is our DRAM is too slow.  So we've had to cache it.  And processors need to minimize the hits on the cache.  So they desperately need to know as much as they can about what's going to happen in the future.  And that means the only way they can know what's going to happen in the future is if they know what happened in the past.  And knowing what happens in the past means that the past affects their future behavior.  And that's where we get the performance, but that also means that their current behavior is affected by the past, which is why they leak information.



LEO:  Yeah.  Very interesting.  Wow.



STEVE:  It is.  It's a Catch-22.  It's a classic conundrum.  And it's just not clear.  The only thing I could see, and this sort of relates to what we were talking about with the recent Apple problem with their M series chips, is that the cryptographic code needs to turn off that feature...



LEO:  Right.



STEVE:  ...while it's running so in order...



LEO:  Selective slowness.



STEVE:  Exactly.  Exactly.  Selective performance retardation.



LEO:  But in a mission-critical environment, and where you really do want to preserve your integrity.  And you know what, it doesn't make that much of a difference.  I mean, yeah, 5%, 10%.  But these things, as you pointed out...



STEVE:  And next year we'll have processors that are 20% faster.



LEO:  Exactly.  It's not - yeah, yeah.  But I think, you know, the Apple fix is very straightforward.  In fact, Apple's already implemented it.



STEVE:  Yes.



LEO:  It's not as easy to do in Spectre and Meltdown, though, in the x86.



STEVE:  No.  We don't, I mean, all of Intel's cores have it, so it's not a matter of, like, running on a core that doesn't have it.  You can turn it off, you know.  That's what my InSpectre freeware does is allow you to globally shut it down.  But, you know.  But, okay.  And again, we need to maintain some perspective.  If the only risk is hostile software in your system...



LEO:  On your system, exactly.



STEVE:  Yes.  It is sharing - it's a processor sharing problem where you've got, what is it, the evil maid.  You have...



LEO:  Or a server where multiple processes are running in the same...



STEVE:  Well, exactly.  So an end-user working at home who's got a bunch of things that it's just them, it's nothing to worry about, really.



LEO:  Yeah.



STEVE:  But it's in the virtualized environment in the cloud, and you have to, you know, we have to admit that more and more is in the cloud.  You know, we're rushing headlong into the cloud.  So there it does matter.



LEO:  It affects you if your data is encrypted with keys that are held by the server, in the processor on the server, and somebody else is using the same processor because they're also on that shared cloud server.  That is a problem.  Apple, I don't know if it's luck, but Apple seems to have dodged a bullet by not having its efficiency cores use this kind of speculative execution.



STEVE:  Yeah.



LEO:  That was really - worked out well because that could say, well, if you're encryption, use the efficiency cores.



STEVE:  Yeah.  It wasn't actually luck.  It's efficient because it's smaller.  So they ripped out a bunch of things in order to make it small and more efficient.



LEO:  Including that.



STEVE:  Including that.  And that ended up being a good thing.  They're just able to move the crypto over into that core.



LEO:  I know, you know, you're concerned that some of our audience might go, this is way over my head.  But I have to tell you everybody in the Discord was really engrossed, and many of them deal with this.  I mean, this is - we had one person who says, "I work with this, I have to survive this all the time, every day."  Vivi, or Vivi, I'm not sure how you say it, says, "I love this topic.  I deal with this all day long with concurrent network programming."  And that's exactly right.  You have to have locks there, too, yeah.



STEVE:  Yeah.



LEO:  It's really great.  I love this stuff.  And I know our audience does, too.  And thank you, Steve.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#971

DATE:		April 23, 2024

TITLE:		Chat (out of) Control

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-971.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  What would you call Stuxnet on steroids?  What's the latest on the Voyager 1 drama?  What new features are coming to Android and Thunderbird?  What's China done now?  Why did Gentoo Linux say "no" to AI?  And after sharing and discussing a bunch of feedback from our terrific listeners and a SpinRite update, we're going to examine the latest update to the European Union's worrisome "Chat Control" legislation, which is reportedly just over a month away from becoming law.  Is the EU about to force the end of end-to-end encryption in order to enable and require the scanning of all encrypted communications?  It appears ready to do just that.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  The latest chapter in the Voyager 1 drama coming up.  We'll talk about the graybeard at Gentoo who says, "No AI in Linux."  About the Hyundai owner whose car really is tracking him.  And then what the EU plans to do with end-to-end encryption.  I can give you a little tip.  It's not good news.  All that coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 971, recorded Tuesday, April 23rd, 2024:  Chat (out of) Control.



It's time for Security Now!, the show where we cover the latest security, privacy, Internet updates, even occasionally some good books and movies, with this guy right here, Steve Gibson, the security guy in chief.  Hello, security guy.



STEVE GIBSON:  Yeah.  And, you know, where are the good movies, Leo?



LEO:  I know.  	



STEVE:  I mean, like, we used to have a lot of fun.  I did see that you're doing the Bobiverse with Stacey's book club.



LEO:  Yes, Thursday, yes.



STEVE:  She was chagrined when you reminded her.  She said, "Ooh, I forgot to read that."



LEO:  Yeah.



STEVE:  So of course it'll take her an hour.



LEO:  It's pretty quick, yeah.



STEVE:  And my wife apparently just glances at pages when she - like I see her, like with eBooks, I said, "Can I test you on the content of this after you're through, like?"  And she says, "Oh, I'm getting most of it.  It's fine."



LEO:  Did she go to Evelyn Wood's Speed Reading Academy?



STEVE:  Apparently.  Or she's in some sort of a time warp.  I don't know.  Because, I mean, I'm an engineer.  I read every word.



LEO:  Yes. 



STEVE:  And in fact that's why Michael McCollum was sending me his books before publication because it turns out I'm a pretty good proofreading editor because I spot every mistake.  Of course not my own.  So other people's.  Much easier to see.



LEO:  Yes.  It's really true.  Same with code; right?  You know there's a bug in there, you can stare at it till the cows come home, and you go...



STEVE:  Absolutely.  One of the neatest things that the GRC group has done for me, our SpinRite news group, is, you know, this thing, basically it's not - we're not having any problems.  Thousands of people are coming onboard with 6.1, and it's done.  



LEO:  Isn't that a nice feeling?



STEVE:  We're not, you know, I'm not chasing bugs around.



LEO:  Oh, that's nice.



STEVE:  You know, it is, it is good.



LEO:  You've got a great bunch of beta testers.



STEVE:  That's in control.  We're going to talk about what is out of control.  Today's title is "Chat (out of) Control" for Security Now! Episode 971, for the second-to-last episode of the month of April, which becomes important because of what's going to happen in June.  But anyway, I'm getting all tangled up here.  We're going to talk about a lot of fun things, like what would you call Stuxnet on steroids?  What's the latest on the Voyager 1 drama?  We've got even more good news than we had last week.



What new features are coming to Android, probably in 15 - we're not sure, but probably - and also in Thunderbird this summer?  What's China gone and done now?  Why did Gentoo Linux say "no" to AI; and, like, what's that all about?  And after sharing and discussing a bunch of feedback - because there wasn't a huge bunch of, like, really gripping news, but we had a lot of feedback from our listeners that we're going to have fun with - and a brief little update on SpinRite, we're going to examine the latest update to the European Union's quite worrisome "Chat Control" legislation, which is reportedly just over a month away from becoming law.



Is the EU about to force the end of end-to-end encryption in order to enable and require the scanning of all encrypted communications for the children?  And it appears ready to do just that.  This latest update, it came onto my radar because somebody said that the legislators had excluded themselves from the legislation.



LEO:  Of course.  Of course.



STEVE:  Well, and so I got this 203-page tome, and its Section 16a was in bold because it just got added.  Anyway, we'll talk about it.  I think that the person doing that speaking that caught my attention, and I'm glad he caught my attention, but he was overstating the case in order to make a point.



LEO:  Oh, okay.



STEVE:  But the case that we have doesn't need overstating because it looks really bad.  You know, there's no sign of exclusion like the UK gave us on their legislation in September which said "where technically feasible."  That's completely missing from this.  So anyway, I think we have a lot of fun to talk about, fun things to talk about.



I did make sure that the pictures showed up this week in Apple devices.  What's interesting is I have an older 6 - I think it's a 6, or maybe it's a 7 or 8, I don't know.  Anyway, the pictures all work there.  Even last week's pictures worked there.  But not on my iPhone 10.  So Apple did in fact change the rendering of PDFs which caused some problem, some incompatibility.  Anyway, I don't know why it was last week but not this week.  We're all good to go this week.  So even Mac people can see our Picture of the Week.  Which is kind of fun.  So lots of good stuff.



LEO:  I can see it, so I verify that it does in fact work.  Oh, that's good news.  Now I believe it's time for a Picture of the Week.



STEVE:  Yeah.  So this just caught my attention because lately  I've been seeing, as I'm sure our listeners have, so much of this.  You know, AI everything.



LEO:  AI, AI, AI, yup.



STEVE:  AI everywhere.  So the picture shows a couple of young upstarts in a startup venture who are - they've got some ideas for some product that they want to create.  And one of the things that happens when you're going out to seek financing and funding, you're typically going and giving presentations to, like, venture capital firms, and explaining what you're going to do and how you're going to do it.



And so PowerPoint presentations are put together, and they're called "pitch decks" because you're making a pitch to whomever you're explaining your ideas to.  So we see in this picture two guys facing each other, each behind their own display, one of them saying to the other, "Can you go through all the old pitch decks and replace the word 'crypto' with 'AI'?"  And of course the point being that we were just, what was it, like a year ago, Leo, that everything...



LEO:  It's just a catchphrase, yeah.



STEVE:  Yeah, exactly.  I mean, it was, like, time must be accelerating because it was just so recently that everything was blockchain this and blockchain that; crypto currency, you know, crypto this and that.  And so, no, that's all yesterday.  That's so, what do they say, so last minute or something?  Anyway, now it's AI.  So, yes.  And we have a couple things during this podcast that will be touching on this, too.  So anyway, just, you know, not a fantastic picture, but I thought it was just, like, so indicative of where we are today.



I've been dealing with Bing.  I don't know why I've been launching it, but it's been launched a few times in the last week.  And, you know, Microsoft...



LEO:  Because you use Windows, that's why.



STEVE:  Oh, that would definitely...



LEO:  They do everything they can to get Bing in your face.



STEVE:  Oh, my god, yes.  And so it's like, no, I don't want this.  And also it's, for me, since I'm not normally using Edge or Bing, it's like, okay, how do I close this?  It looks like it takes over the whole UI, and very much like that old, you know, when people were being forced to upgrade to Windows 10 against their will, where for a while it said, "No, thank you," and then it changed to "Later tonight."  So it's like, wait a minute.  What happened to not at all, never ever?  You know, it's like you want to do it now, or do you want to do it in an hour?  Uh, wait.  Those are my only two options?



Anyway, okay.  So as we know, Security Now! is primarily an audio podcast.  But even those watching, though it remains unclear to me why anyone would, don't have the advantage of looking at my show notes.  If anyone were to be reading the notes, they would see that the spelling of the name of this new attack is far more, shall we say, acceptable in polite company than the attack's verbal pronunciation.  But this is an audio podcast, and the story of this attack that I very much want to share refers to the attack by name.  And that name, which rhymes with "Stuxnet" is spelled "F-U-X-N-E-T."  And there's really no other way to pronounce it than just to spit it out.  But I'm just going to say Fnet for the sake of the children.



LEO:  Thank you, Steve.  Thank you.



STEVE:  Because, yes, you know.  So it's not really an F-bomb.  But it's audibly identical, and there's no point in saying it.  Everybody understands how you would pronounce F-U-X-N-E-T.  Which is what the Ukrainians named the weapon which they reportedly, and this was confirmed by an independent security company, successfully launched into the heart of Russia.



So with that preamble and explanation, let's look at the very interesting attack that was reported last week by Security Week.  Their headline, which also did not shy away from using the attack's name, said "Destructive ICS Malware 'Fuxnet' used by Ukraine Against Russian Infrastructure."  So here's what we learn from what they wrote.  They said:  "In recent months, a hacker group named Blackjack, which is believed to be affiliated with Ukraine's security services" - so, you know, as in state-sponsored - "has claimed to have launched attacks against several key Russian organizations.



"The hackers targeted ISPs, utilities, data centers, and Russia's military, and allegedly caused significant damage and exfiltrated sensitive information.  Last week, Blackjack disclosed the details of an alleged attack aimed at Moscollector (M-O-S-C-O-L-L-E-C-T-O-R), a Moscow-based company responsible for underground infrastructure, meaning things like water, sewage and communication systems."



So, quoting, they said:  "Russia's industrial sensor and monitoring infrastructure has been disabled."  So said the hackers.  "It includes Russia's Network Operation Center that monitors and controls gas, water, fire alarms, and many others, including a vast network of remote sensors and IoT controllers. 



"So the hackers claimed to have wiped database, email, internal monitoring, and data storage servers.  In addition, they claimed to have disabled some 87,000" - 87,000 - "sensors, including ones associated with airports, subway systems, and gas pipelines.  To achieve this, they claimed to have used Fuxnet, a malware they described as 'Stuxnet on steroids,' which enabled them to physically destroy sensor equipment."



You know, our longtime listeners and anybody's who's been in, you know, around IT will recall that Stuxnet was a previous, also physically destructive malware.  I guess we have to call it malware, even though we were apparently part of - the U.S. participated, or U.S. intelligence services was involved in its creation.  It caused the centrifuges used in Iran to overspin and essentially self-destruct.  So those were being used to enrich uranium at the time.  Anyway, so that's why they're calling this thing "Stuxnet on steroids" is that they worked to cause actual physical damage, as we'll see in a second, to hardware.



LEO:  There's a big difference, though, between destroying centrifuges which have one purpose, which is enriching uranium, and destroying sensors which prevent gas leaks and, I mean...



STEVE:  Yeah, yeah.



LEO:  I mean, this is a civilian attack.  Finish the story, but I would love to talk at the end of it about how you feel about this.



STEVE:  Good.  And I agree with you.  So they wrote:  "Fuxnet has now started to flood the RS485/MBUS and is sending random commands to 87,000 embedded control and sensory systems."  And they did say "(while carefully excluding hospitals, airports, and other civilian targets)."  Now, they said that.  So, you know, they share some of our sensitivity to that.  And I do question, you know, given that they're also claiming 87,000-some sensors, how they can be that careful about what's, you know, what they've attacked and what they haven't.



Anyway, the report goes on, saying:  "The hackers' claims are difficult to verify, but the industrial and enterprise IoT cybersecurity firm Claroty was able to conduct an analysis of the Fuxnet malware based on information and code made available by Blackjack.  Claroty pointed out that the actual sensors deployed by Moscollector, which are used to collect physical data such as temperature, were likely not themselves damaged by Fuxnet.  Instead, the malware likely targeted roughly 500 sensor gateways."



So, right?  So the idea is that the gateway is a device out located remotely somewhere, and it has RS485 lines running out to a ton of individual sensors.  So it's the sensor data collector and forwarding device.  So the malware targeted around 500 of these sensor gateways, which communicate with the sensors over a serial bus such as RS485 or Meter-Bus that was mentioned by Blackjack.  These gateways are also connected to the Internet to be able to transmit data to the company's global monitoring system.  So that was probably the means by which the Fuxnet malware got into the sensor gateways.



"Claroty notes:  'If the gateways were indeed damaged, the repairs could be extensive given that these devices are spread out geographically across Moscow and its suburbs, and must be either replaced or their firmware must be individually reflashed.'  Claroty's analysis of Fuxnet showed that the malware was likely deployed remotely.  Then, once on a device, it would start deleting important files and directories, shutting down remote access services to prevent remote restoration, and deleting routing table information to prevent communication with other devices.



"Fuxnet would then delete the file system and rewrite the device's flash memory.  Once it has corrupted the file system and blocked access to the device, the malware attempts to physically destroy the NAND memory chip and then rewrites the UBI volume to prevent rebooting.



"In addition, the malware attempts to disrupt the sensors connected to the gateway by flooding their serial communications channels with random data in an effort to overload the serial bus and sensors, essentially performing an internal DoS attack on all the devices the gateway is connected to."  And I'll argue that if these are not sensors, but these are actuators, as you said, Leo, this could be causing some true damage.  I mean, like true infrastructure [crosstalk] damage.



LEO:  Well, they said subway systems, airports, gas pipelines.



STEVE:  Yeah.  "Claroty explained:  'During the malware operation, it will repeatedly write arbitrary data over the Meter-Bus channel.  This will prevent the sensors and the sensor gateway from sending and receiving data, rendering the sensor data acquisition useless.  Therefore, despite the attackers' claim of physically destroying 87,000 devices,' wrote Claroty, it seems that they actually managed to infect the sensor gateways and were causing widespread disruption by flooding the Meter-Bus channel connecting the sensors to the gateway, similar to network fuzzing the different connected sensor equipment.  As a result, it appears only the sensor gateways were bricked, and not the end-sensors themselves.'"



So, okay.  I particularly appreciated the part about attempting to physically destroy the gateway's NAND memory chip.  Because it could happen.  As we know, NAND memory is fatigued by writing because writing and erasing, which needs to be part of writing,  is performed by forcing electrons to tunnel through insulation, thus weakening its dielectric properties over time.  So the attacking malware is likely writing and erasing and writing and erasing the NAND memory over and over, as rapidly as it can.  And since such memory is likely embedded into the controller and is probably not field replaceable, that would necessitate replacing the gateway device, and perhaps all 500 of them spread across Moscow and its suburbs.



And even if the NAND memory was not rendered unusable, the level of destruction appears to be quite severe.  Wiping stored data and directories and killing the system's boot volume means that those devices probably cannot be remotely repaired.  Overall, I'd have to say that this extremely destructive malware was well named.



And we live in an extremely, and increasingly, cyber-dependent world.  Everyone listening to this podcast knows how rickety the world's cybersecurity truly is.  So I shudder at the idea of any sort of all-out confrontation between super powers.  I don't want to see that.



LEO:  Do you think there should be a, I don't know, Geneva Convention-style accord between nations about cyberwarfare?  I mean, it's - the problem is you can do it, but then you're just going to escalate.  It's going to go back and forth.  Which is why we decided, for instance, not to allow bioweapons.  Now, they still get used.  But it's, you know, the civilized world agrees not to use biologic weapons in war.  



STEVE:  Well, and the feeling is, of course, that COVID was a lab escape.  Right?



LEO:  Well...



STEVE:  I mean...



LEO:  There's some evidence, but not a lot.



STEVE:  There's no evidence.



LEO:  That's a question, yeah.  It wasn't a very good, it wasn't a very effective warlike attempt since it killed far more people in China than it did elsewhere.  But anyway...



STEVE:  It was clearly a mistake.



LEO:  Yes.  It wasn't intentional.  So what do you think?  I mean...



STEVE:  So I agree with you.  The problem is it's tempting because it doesn't directly hurt people; right?  I mean, so like right now we're in a cold war.  We're constantly on this podcast talking about state-sponsored attacks.  Well, those are attacks.



LEO:  Yeah.  And especially infrastructure attacks.



STEVE:  Yes.



LEO:  Which this was.



STEVE:  Yes.  I mean, the whole Colonial Pipeline thing; you know?



LEO:  Right.



STEVE:  That really damaged the U.S.  And, I mean, it was a true attack.  So, you know, and we just talked about how China was telling some of their - China told their commercial sector, you need to stop using Windows.  You need to stop using, you know, this Western computer technology because the West is able to get into it.  So that was the first indication we really had that, as I put it at the time, that we're giving as well as we're getting.  Unfortunately, this is all happening.  I mean, I wish none of it was happening.  But the problem is security is porous.  And I guess the reason a nuclear weapon and a bioweapon are unconscionable, you know, is that they are so tissue damaging, for lack of a better word.  I mean, they really - they're, like, really going to kill people.



Whereas, eh, a network got breached, whoops.  You know, I mean, it doesn't have the same sort of visceral grip.  And unfortunately, here's an example.  And I'm glad you brought it up, Leo.  Ukraine, sympathetic as we can be for their situation, this was a blunt-edged attack; right?  I mean, this was, you know, sewage and water and gas and airports and, you know, I mean, it's - they couldn't have controlled what damage was caused.  And, you know, you mess up water and sewage, and you're really hurting actual people who are innocent of what, you know...



LEO:  Or subways.  Or airports.  Or gas pipelines.  I don't know what the answer is.  I mean, I'm no fan of Putin.  He brought the war upon himself.  But hurting civilians, I don't know, this is not a good situation.



STEVE:  It's the world we're in.



LEO:  It's the world we're in.



STEVE:  Yeah.  And it is technology we created.  I mean, you know, oh, let's have the password be admin/admin because we don't want people calling us and asking what the password is.  Or, I mean, it's like we've made so many bad decisions.  And while we're now making them better today, we have seen how long the tail of inertia is.  I mean, it's, you could argue, infinite.  You know?  We still have Code Red and Nimda out there, you know, sending packets out.  Somewhere there's an NT machine just hoping to find something that it can infect.  When is it going to die?  I don't know.



We have another update on Voyager 1.  Apparently, if Voyager is not going to give up on us, we're not going to give up on it.  But remember that, no matter what, Voyager is deriving all of its diminishing operating power from the heat being generated by the decay of radioisotopes.  And through the years and now decades, since this thing left Earth in '73, those isotopes are continuing to put out less and less heat.  And thus Voyager has less and less energy available to it.  So it can't go forever.  But it, you know, it amazes everybody that it's gone as long as it has, and it is still going.



What equally amazes me is the intrepid group of well-past-their-retirement engineers who are now endeavoring to patch the code of this ancient machine that's 22.5 light hours away.



LEO:  Oh, my god.  It's amazing.



STEVE:  It boggles the mind.



LEO:  It's so amazing.



STEVE:  Just yesterday, on April 22nd, JPL, NASA's Jet Propulsion Laboratory, posted the news under the headline "NASA's Voyager 1 Resumes Sending Engineering Updates to Earth."  They wrote:  "After some inventive sleuthing, the mission team can  for the first time in five months  check the health and status of the most distant human-made object in existence.  For the first time since November, NASA's Voyager 1 spacecraft is returning usable data about the health and status of its onboard engineering systems.



"The next step is to enable the spacecraft to begin returning science data again.  The probe and its twin, Voyager 2, are the only spacecraft to ever fly in interstellar space, the space between the stars.  Voyager 1 stopped sending readable science and engineering data back to Earth on November 14th, 2023, even though mission controllers could tell the spacecraft was still receiving their commands and otherwise operating normally.  In March, so last month, the Voyager engineering team at NASA's Jet Propulsion Laboratory in Southern California confirmed that the issue was tied to one of the spacecraft's three onboard computers, called the flight data subsystem (FDS).  The FDS is responsible for packaging the science and engineering data before it's sent to Earth.



"The team discovered that a single chip responsible for storing a portion of the FDS's memory  including some of the FDS computer's software code  is no longer working.  The loss of that code rendered the science and engineering data unusable.  Unable to repair the chip" - right, after 22.5 light, what is it, days, light days away - "the team decided to place the affected code elsewhere."  They're relocating the code, Leo, at this distance.



LEO:  I know, it's amazing.



STEVE:  On a probe built in, or launched in '73.



LEO:  How cool.



STEVE:  It's, you know, it's insane.  "But," they said, "no single location is large enough to hold the section of code in its entirety."  So they're having to fragment it.  



"They devised a plan to divide the affected code into sections and store those sections in different places in the FDS.  To make this plan work, they also needed to adjust those code sections to ensure, for example, that they all still function as a whole.  Any references to the location of that code in other parts of the FDS memory need to be updated, as well."  So they're relocating and then patching to relink the now-fragmented code sections so that they jump to each other.  It's, you know, dynamic linking in a way that was never designed or intended.



They wrote:  "The team started by singling out the code responsible for packaging the spacecraft's engineering data. They sent it to its new location in the FDS memory on April 18th.  A radio signal takes about 22.5 hours to reach Voyager 1, which is now over 15 billion" - with a "b" - "miles from Earth, and another 22.5 hours" - hours, not days - "for a signal to come back to Earth.  When the mission flight team heard back from the spacecraft on April 20th, they saw that the modification worked.  For the first time in five months, they have been able to check the health and status of the spacecraft.



"During the coming weeks, the team will relocate and adjust the other affected portions of the FDS software.  These include the portions that will start returning science data," rendering the satellite again back to doing what it was designed to do, which is using its various sensor suites and sending back what it's seeing and finding out in interstellar space, which as I mentioned previously has surprised the cosmologists because their models were wrong.  So Voyager 1 is saying, uh, not so fast there.  Nice theory you've got, but it's not matching the facts.  Wow.



LEO:  Yay, V'Ger.



STEVE:  Yeah.



LEO:  And yay, those brilliant scientists who are keeping her alive.



STEVE:  Oh, and Leo, I did take - Lorrie and I watched "It's Quieter in the Twilight."



LEO:  Oh, yeah, "It's Quieter in the Twilight," yeah.



STEVE:  And what was interesting was that this announcement, and it was picked up in a few other outlets, showed a photo of the event where the team were gathered around their conference table.  I recognized them.



LEO:  Yes.



STEVE:  From the documentary.



LEO:  It's the same people since 1974.



STEVE:  Yeah, exactly.  They're all still there.  And in fact some of them don't look like they've changed their clothes, but that's what you get, you know, with old JPL engineers.



LEO:  Oh, I just love it.  It's such a great, wonderful story.  It really is.



STEVE:  Let's take a break.  I'm going to catch my breath, and then we're going to talk about changes coming to Android 15 and Thunderbird.



LEO:  Yes.  All right.  As we continue with the best show on the podcast universe, 22 light hours ahead of everyone else.  On we go.



STEVE:  Okay.  So there's not a lot of clear information about this yet, but Google is working on a new feature for Android which is interesting.  They're going to start watching their apps' behavior.  It will place under quarantine any applications that might sneak past its Play Store screening, only to then begin exhibiting signs of behavior that it deems to be malicious.  The apps will reportedly have all their activity stopped, all of their windows hidden, and notifications from the quarantined apps will no longer be shown.  They also won't be able to offer any API-level services to other apps.



The reports are that Google began working on this feature during Android 14's development last year, and that the feature is expected to finally appear in forthcoming Android 15.  But we don't have that confirmed for sure.  So, you know, there wasn't - I wasn't able to find any dialogue or conjecture about why the apps aren't just removed.  Maybe - oh, and that they do still appear to be an app installed on the phone.  They're not hiding it from the user.  They're just saying, no, you bad app, we don't like what you've been doing.



Maybe it reports back to the Play Store, and then Google takes a closer look at the app which is in the Play Store, which of course is how the user got it, and then says, oh, yeah, we did miss this one.  And at that point it gets yanked from the Play Store and yanked from all the Android devices.  So it could just be like essentially functioning as a remote sensor package.  Anyway, I'm sure we'll learn more once it becomes official, hopefully in this next Android 15.



Also this summer, Thunderbird will be acquiring support for Microsoft Exchange email for the first time ever.  It will only be email at first.  The other Exchange features of Calendar and Contacts are expected to follow at some later date, although Mozilla's not saying for sure.  Now, I happen to be a Thunderbird user.  I was finally forced to relinquish the use of my beloved Eudora email client once I began receiving email containing extended non-ASCII character sets that Eudora was unable to manage.  I got these weird capital A with little circles above them things in my email, which was annoying, instead of line separators.



At the same time I have zero interest in Exchange.  GRC runs a simple and straightforward instance of a mail server called hMailServer which handles traditional POP, IMAP, and SMTP, and does it effortlessly with ample features.  But I know that Exchange is a big deal, and obviously Mozilla feels that for Thunderbird to stay relevant, it probably needs to add support for Exchange.



In any event, to support this rather massive coding effort, in Mozilla's reporting of this they mentioned that it had been 20 years since - because, you know, email is kind of done - 20 years since any code in Thunderbird dealing with email had been touched.  They've just been, you know, screwing around with the user interface.  And that during those 20 years a lot of, as they put it, institutional knowledge about that code had drained.  So they've decided now that they're going to recode in Rust.  Rust is their chosen implementation language.  And they did so for all the usual reasons.



They cited memory safety.  They said:  "Thunderbird takes input from anyone who sends an email, so we need to be diligent about keeping security bugs away."  Performance:  "Rust runs as native code with all the associated performance benefits."  And modularity and ecosystem.  They said:  "The built-in modularity of Rust gives us access to a large ecosystem where there are already a lot of people doing things related to email which we can benefit from," they said.  So anyway, for what it's worth, you know, Thunderbird is a strong client for, you know, from Mozilla.  Is it multiplatform, Leo, do you know?  Is Thunderbird Windows-only?  Or Mac and Linux?  I don't know either way.



Anyway, China.  The Chinese government has ordered Apple to remove four Western apps from the Chinese version of the Apple App Store.  Those are Meta's new social network Threads, which is now gone; Signal; Telegram; and WhatsApp, all removed from the Chinese App Store.  China stated that they have national security concerns about those four.  And as we've seen, and as I fear we'll be seeing shortly within the EU, what countries request, countries receive.  Technology is ultimately unable to stand up to legislation.  And this is going to cause a lot of trouble, as I mentioned, in the EU.  We'll be talking about that here at the end of the podcast.



LEO:  Yeah.  And I think the Chinese government's removing it for the same reason the EU wants to remove it.  They don't like end-to-end encryption.



STEVE:  Yes, exactly.



LEO:  Threads is something else.  But Signal and Telegram and WhatsApp, that's all E2E encryption.



STEVE:  Yeah.



LEO:  By the way, to answer your question, I was down the hall.  Thunderbird is Mac, Windows, Linux.



STEVE:  Okay.



LEO:  It's completely open source and everywhere.



STEVE:  Very cool.



LEO:  Nice program.



STEVE:  In that case, it will have access to Exchange Server, which may allow it to move into a corporate environment, which is probably what they're thinking.



LEO:  Which would be great, yes.



STEVE:  Yeah.



LEO:  That would be great, yeah.  Well, we'll see if Microsoft does it.  Oh, no, they're going to do it.  Oh, cool.



STEVE:  Oh, yeah.  You mean Mozilla.



LEO:  I just wish they'd kill Exchange Server.



STEVE:  And, I mean, just get out of the Exchange Server, like...



LEO:  I wish Microsoft would kill Exchange.  That's been a problem since forever.



STEVE:  Yeah, yeah.  Since it was created.



LEO:  Exactly, yeah.



STEVE:  And do you think that China's move is like in response to TikTok and what's happening here in the U.S.?



LEO:  Well, I mean, we had a discussion on MacBreak about that.  The Times says it's because nasty things were said on those platforms about Xi Jinping.  Which is possible.  There's no corroboration of that.  Apple says no.  I think it's just that they were - Threads, maybe that would be because Threads doesn't have any, you know, it's just a social network.  But for sure they don't want - I think Threads is being killed probably because of TikTok.  Andy I think pointed out that it happened immediately after the TikTok ban was approved in the House.  



STEVE:  Ah.



LEO:  So it's likely, by the way, that that this time will be approved in the Senate because it's part of a foreign aid package.



STEVE:  Right.



LEO:  So get ready to say goodbye to TikTok.



STEVE:  Wow, Leo.  That'll be an event, won't it.



LEO:  I don't think - I think the courts will block it.  I'm hoping they will, but I don't know.  It's a very weird thing.  They have a year and a half to do it.



STEVE:  Well, and, I mean, here, you know, we were talking about a cold war.  And there's this, you know...



LEO:  There's an economic cold war, absolutely, yeah.



STEVE:  Right.  And China understandably is uncomfortable about Western-based apps using encryption that they're unable to compromise.



LEO:  Right, right.



STEVE:  So, I mean, I get it.  You know?  And so it's sort of like we lived through this brief period where, you know, there was global encryption and privacy, and everybody had apps that everybody could use.  And then barriers began getting erected.  Right?  I mean, so, sorry.  If you're Chinese, you've got to use China Chat.



LEO:  Well, and the numbers for these particular apps in China are pretty low.  We're talking hundreds of thousands of users, not millions.



STEVE:  Ah, okay.



LEO:  Or billions, yeah.



STEVE:  Okay.  So not a huge actual impact.



LEO:  I think it's an easy thing for them to do, yeah.



STEVE:  Okay.  So this was interesting.  I'll just jump right in by sharing the posting to the Gentoo mailserve.  This was posted by a longstanding, since 2010, so 14 years of involvement, and he's very active, Gentoo developer and contributor.  He wrote:  "Given the recent spread of the 'AI' bubble" - and he has AI in quotes, like, everywhere, so he's obviously immediately exposed himself as not being a fan.



LEO:  Gentoo, you should understand, is the ultimate graybeard Linux; okay?  That's all you need to know.



STEVE:  That totally explains it, yes.  "Given the recent spread of the 'AI' bubble, I think we really need to look into formally addressing the related concerns."  Oh.  He says:  "In my opinion, at this point the only reasonable course of action would be to safely ban 'AI'-backed contribution entirely.  In other words, explicitly forbid people from using ChatGPT, Bard, GitHub Copilot and so on, to create ebuilds, code, documentation, messages, bug reports and so on for use in Gentoo.  Just to be clear, I'm talking about our original content.  We can't do much about upstream projects using it."



Then he says:  "Here's the rationale:  One, copyright concerns.  At this point, the copyright situation around generated content is still unclear.  What's pretty clear is that pretty much all LLMs (Large Language Models) are trained on huge corpora of copyrighted material, and the fancy 'AI' companies don't care about copyright violations.  What this means is that there's good risk that these tools would yield stuff we cannot legally use.  Two, quality concerns.  LLMs are really great at generating plausible-looking BS."  And he didn't actually say "BS," but I changed it for the podcast.



LEO:  He's cranky.



STEVE:  Oh, he is.



LEO:  That's what happens when you have to compile every piece of software you use from scratch.  It makes you cranky.



STEVE:  Right.



LEO:  It makes you cranky.



STEVE:  That's right.  "I suppose," he says, "they can provide good assistance if you are careful enough, but we can't really rely on all our contributors being aware of the risks."  Then there's ethical concerns, number three.  "As pointed out above, the 'AI' corporations care about neither copyright nor people."



LEO:  That's probably true.



STEVE:  Yeah.  "The 'AI' bubble is causing huge energy waste.  It's giving a great excuse for layoffs and increasing exploitation of IT workers.  It is driving the further" - and here I felt I had to use the word because it is now become a common word.  And I think I've heard it on the TWiT Network.



LEO:  Yeah, we allow this word.



STEVE:  "The 'enshittification' of the Internet.  It is empowering all kinds of spam and scam."  And that is the case.  "Gentoo has always stood," he concludes, "as something different, something that worked for people for whom mainstream distros were lacking.  I think adding 'made by real people' to the list of our advantages would be a good thing.  But we need to have policies in place to make sure that AI-generated crap" - and again, not the word he chose - "doesn't flow in."



LEO:  I like this guy.  He's right.  I think that's fair.  Did you see the study from the University of Illinois in Urbana-Champaign?



STEVE:  No.



LEO:  They used ChatGPT 4, the latest version of OpenAI's model.  They gave it the CVE database.  That's it.  Nothing more than the description and the CVE.  And it was able to successfully attack 87% of those vulnerabilities.  It was able to craft an attack based merely on the CVE description, an effective attack.



STEVE:  Wow.  Wow.



LEO:  Yeah, I mean, I think he's probably right.  But I don't know how you enforce this because...



STEVE:  No.  That's exactly the problem.



LEO:  Yeah.



STEVE:  In his posting, he had a link, he referred to some, we'll call it a "crap storm," over on GitHub.  And I went, I followed the link because I was curious.  There is a problem underway where what is clearly AI-generated content which, you know, looks really reasonable, but doesn't actually manage to get around to saying anything, is like becoming a problem over on GitHub.



LEO:  Yeah. 



STEVE:  So anyway, in order to share that, as we saw, I had to clean up the language in his posting since he clearly doesn't think much of AI-generated code.  And as I said, there have been some signs over on GitHub, which he referred to, of descriptions appearing to be purely AI-generated.  You know, they're not high quality.  And I suppose we should not be surprised, Leo, that there are people, maybe we'll call them "script kiddies," who are probably incapable of coding from scratch for themselves.  So why wouldn't they jump onto Large Language Model systems, which would allow them to feel as though they're contributing.  But are they really contributing?



LEO:  Now, look, let's face it.  Humans are just as capable of introducing bugs into code as anybody else, and more often maliciously than AIs.  I mean, AIs aren't natively malicious.  The other thing I would say is there is a lot of AI-generated prose on GitHub because English is often not the first language of the people doing the coding.  A lot of repositories on GitHub are by non-English speakers.  And I think that that's more likely the reason you'll see kind of AI-like prose on there because they don't speak English that well, or at all.  And so they're using ChatGPT, for instance, to generate the text.



STEVE:  Right.



LEO:  Human, I don't think, I mean, honestly, I've used Copilot.  I have my own custom GPT for LISP.  The code it generates is indistinguishable from human code, probably because it is, at some point, from human code.



STEVE:  Right, right.



LEO:  I don't know how you're going to stop it.  It's not - doesn't have a big red flag that says an AI generated this.



STEVE:  Right.  And as we've noted, the genie is out of the bottle already.  So, yeah.  We are, we're definitely in for some interesting times.



Okay.  We've got a bunch of feedback that I found interesting, that I thought our listeners would, too.  Let's take another break, and then we will get into what - what was this one?  Oh, oh.  We have a listener whose auto was spying on them, and he's absolutely sure it never had permission.  And we have a picture of the report that it generated.



LEO:  We here at Nissan see that you've been using your vehicle for lovemaking.  And we want you to knock it off.  But we'll find out more about that in just a second.  But first, a word.



STEVE:  And apparently you're doing it wrong.



LEO:  You're doing it wrong.  We have some tips we'd like to share.  Steve, let's close the loop.



STEVE:  So we have a note from a guy who is no slouch.  He's a self-described user of ShieldsUP!, SpinRite, and an avid listener of Security Now!.  He's also an Information Security Practitioner, and I think he said a Computer Geek.  Oh, yeah, he does.  So he said:  "Hi, Steve.  I apologize for sending to this email" - probably came through Sue or Greg - "as I couldn't find a different email for contact information."  And yes, that's by design, but okay.  He said:  "Anyway, longtime follower of ShieldsUP! and SpinRite, and an avid listener of Security Now!.  My full-time gig is an InfoSec Security Practitioner/ Computer Geek.



"We have a couple of Hyundais in the family, and I purchased one last fall.  I use the Hyundai Bluelink app on my phone, as I can make sure I locked my doors, and get maintenance reminders.  I made a point to NOT opt-in for the [he has in quotes] 'driver discount'; and, as a privacy cautious person, I decline sharing data wherever possible.  But after the story in The New York Times regarding carmakers sharing data, I contacted Verisk and LexisNexis to see what they had on me.



"LexisNexis had nothing other than the vehicles I have owned in the past, but Verisk had a lot.  I have attached a page of the report.  It includes driving dates, minutes (day and night), acceleration events, and braking events.  The only thing missing is the actual speeds I was going, or if I was ever speeding.



"What bothers me most about this is that I have no way to challenge the accuracy.  For events that are not illegal, I can still be penalized.  Braking hard and accelerating fast should not be safety concerns without context.  And today's smarter cars are still imperfect.  My adaptive cruise control" - he has in parens (radar) - "will still brake hard at times it shouldn't, and I will get penalized by that data.  My car is also a turbo, and if I accelerate for fun or safety, that too can be a penalty.  And if I happen to drive in Texas where there are highways with an 85 MPH speed limit, I would be down-rated for that legal behavior.



"My family tried the 'safe driving' BT dongles from another insurer years ago, but the app had too many false positives for driving over speed" - he said posted speed limit doesn't agree with the app - and hard braking and accelerating, that we decided it wasn't worth our time or the privacy concerns.  My wife and I are close to Leo's age, and she drives like a grandmother, but her scores were no better than mine.



"I have attached a picture of the document I got from Verisk," he says, "name and VIN Number removed, to give you an idea of what is reported without my consent from my car.  I've contacted Hyundai and told them I do not and did not consent to them sharing my data with Verisk.  After a few back and forths, I got this reply on April 12th:  'Thank you for contacting Hyundai Customer Care about your concerns.  As a confirmation, we have been notified today that the driver's score feature and all data-collecting software has permanently disabled.  We do care. As always, if you ever need additional assistance, you can do so either by email or phone.  Case number....'"



So he said:  "I will request another report from Verisk in the future to validate this report from Hyundai.  Keep up the good work.  I thought you would like to see the data and hear from someone who is 100% certain they never opted in.  All the best, Andrew."



And in the show notes, sure enough, we've got the page with a report from the period September 26th of last year through March 25th of this year.  So just last - toward the end of last month, showing things like the number of trips, vehicle ignition on to ignition off was 242 instances.  Speeding events where the vehicle speed is greater than 80 miles per hour has an NA.  Hard braking events, where they say change in speed is less than, because it's braking, negative 9.5 kph per second is 24.  So during that period of time what the car regarded as a hard braking event occurred 24 times.



Rapid acceleration events, change in speed is greater than 9.5 kph per second is 26.  Daytime driving minutes between the hours of 5:00 a.m. and 11:00 p.m., 6,223.  Nighttime minutes, actually very few between 11:00 p.m. and 5:00 a.m., just 25 minutes.  Miles driven, 5,167.6 miles during this period.  And then an itemized daily driving log showing the date, the number of trips taken that day, the number of speeding events, the number of hard braking events, rapid acceleration events, driving minutes both daytime and nighttime.



So, yes, just to close the loop on this, as we first talked about from The New York Times reporting which informed us that both this Verisk and LexisNexis were selling data to insurers, and as a consequence those insurers were relying on that data to set insurance premium rates.



LEO:  And look what it says at the bottom:  "This report may display driving data associated with other individuals that operated insured's vehicle."



STEVE:  Yup.



LEO:  So my guess, this is a report for an insurance company; right?



STEVE:  Right, right.



LEO:  Whether he agreed to it or not, it may be that he can turn off some things, like I noticed that speeding events is N/A all the way through.  Either he's a really careful driver, or they're not recording that.  Which may well be something he didn't agree to; right?



STEVE:  Yup.  So anyway...



LEO:  I know my BMW records that because I have it on my app.  And my Mustang used to give me a report card after every trip.



STEVE:  Right.  And, I mean, compared to the way I used to drive when I was in my younger years...



LEO:  Yeah, good information, yeah.



STEVE:  I would be happy to have my insurance company privy to the fact that I drive about three miles a day at 60 miles an hour, surrounded by other traffic.  I mean, it's just, you know.



LEO:  Here's my driving performance for the month of March.



STEVE:  And in fact Lorrie added me to her car insurance, and her rate went down.



LEO:  Yes, exactly.  Because you're safe; right?



STEVE:  Yeah.



LEO:  This is more because it's an EV.  You want to know a little bit about - reason you want to know about hard braking and hard acceleration and stuff is going to [crosstalk].



STEVE:  Right, battery treatment and, yeah.



LEO:  Battery, right, right.  So I think that's - I think that's great.  You know.  But, yeah, I understand why he doesn't want Hyundai to record it.



STEVE:  Well, and I would argue that a consumer who says no, I don't want to be watched and spied on and reported on, that ought to be a privacy right is available.



LEO:  Yeah.



STEVE:  Yeah.



LEO:  I'd like to see the fine print in the rest of the contract.  Those are long, those contracts, you know.  They go on and on.



STEVE:  Yeah.  So Lon Seidman said:  "I'm listening to the latest Security Now! episode.  Definitely agree that freezing one's credit needs to be the default position these days."



LEO:  Yes, yes.



STEVE:  "One question, though.  Most of these credit agencies rely on the types of personal information that typically get stolen in a data breach for authentication.  Certainly a bad actor will go for the lowest hanging fruit and perhaps move on from a frozen account.  But if there's a big whale out there, they may go through the process of unlocking that person's credit, then stealing their money.  What kind of authentication changes do you think are needed?"



Okay, well, that's an interesting question.  Since I froze my credit reporting, I've only had one occasion to temporarily unfreeze it, which is when I decided to switch to using an Amazon credit card for the additional purchase benefits that it brought since I'm a heavy Amazon user.  And that's when I discovered to my delight that it was also possible to now specify an automatic refreeze on a timer...



LEO:  Yes.  Isn't that great?



STEVE:  ...to prevent the thaw from being inadvertently permanent.  Since I had very carefully recorded and stored my previously freezing authentication, I didn't need to take any account recovery measures.  So I can't speak from experience.  But one thing does occur to me is that strong measures are available.  The reporting agencies, for example, will have our current home address.  So they could use the postal system to send an authentication code via old school paper mail that would be quite difficult, if not effectively impossible, for a criminal located in some hostile foreign country to obtain.  So there certainly are strong authentication measures that could be employed if needed.



Again, I don't have any experience with saying, whoops, I forgot what you told me not to forget when I froze my credit.  So, but it's me.  Hi.  It's me, really.  Unfreeze me, please.  You know, because Lon's right that, you know, so much information is in the report, or in the data which is being leaked these days, for example in that massive AT&T leakage, that, you know, something over and above that needs to be used.



LEO:  They gave me a long PIN.  I mean, like a really long PIN.



STEVE:  Yes.  I have that, too.  I'm not, you know, I wrote it down and saved it.



LEO:  Yeah.  But...



STEVE:  But then what happens if you say, oh, it's me.



LEO:  I forgot, yeah.



STEVE:  Yeah.



LEO:  They say if you don't, you know, you can't log in, you forget it, call us.  Which means you could easily social engineer a customer service rep because, let's face it, the credit reporting agencies don't want you to have a credit freeze.



STEVE:  Correct.  Correct.



LEO:  That's how they make their money is selling your information.



STEVE:  Correct.



LEO:  So I suspect it's pretty easy to get it turned off.  I would guess.  By a third party.



STEVE:  Eric Berry, he tweeted:  "What was that credit link from the podcast?  I tried the address you gave out and got 'page not found.'"



I don't know why.  But it is grc.sc/credit.  And I just tried the link, and it works.  So grc.sc/credit.  That bounces you over to the Investopedia site.  And I've just verified it.  I said that it is still working.  And for what it's worth, in what, page 9 of the show notes is the Investopedia link, all the way spelled out.  So if something about your computer doesn't follow HTTP 301 redirects, then the link is there.  And it's at Investopedia, How to Freeze and Unfreeze Your Credit.  So you could probably also just google that.



LEO:  I should also point out the FTC has a really good page about credit freezes, fraud alerts, what they are, how they work and so forth.



STEVE:  Yeah.



LEO:  Yeah.  You could also just google "FTC and credit freeze," and they have a lot of information on there.



STEVE:  Does that provide links to the actual freezing pages in the bureaus?



LEO:  Yeah.



STEVE:  Because that's why I chose - oh, good.



LEO:  Yeah.  Absolutely.



STEVE:  Good, good, good.  Okay.



LEO:  They point you to a website run by the FTC called IdentityTheft.gov.  And they're going to give you those three.  Now, I should point out, there's more than three.  These are the three big ones.  But when I did a credit freeze, I think I did five or six of them.  There's others.  And it's probably not a bad idea to seek them all out.  But obviously these are the three the FTC mentions, as well as Investopedia.



STEVE:  Right.  So someone who tweets from the handle or the moniker The Monster, he said:  "@SGgrc The race condition isn't solved solely with the exchange 'counter ownership' protocol unless the owner immediately rereads the owned memory region to be sure it wasn't altered before it got ownership."



Okay, now, I don't think that's correct.  There are aspects of computer science that are absolutely abstract and purely conceptual.  And I suppose that's one of the reasons I'm so drawn to it.



LEO:  Yeah, me, too.



STEVE:  I think, Leo, I think you are, too.



LEO:  Yes, exactly.



STEVE:  One of the time-honored masters of this craft is Donald Knuth.  And the title of his masterwork, it's a multivolume, just it's a tour de force.



LEO:  I have all three, although there supposedly are five.  He's working on them.



STEVE:  Yes.  He calls the other ones "fascicles."  And I have those, as well.



LEO:  Oh, yeah?  How do you get those?



STEVE:  Oh, yeah, they're available.



LEO:  I wanted the full bookshelf, but I only could find three.



STEVE:  Yeah.  Three are in that original nice classic binding.



LEO:  Yeah, beautiful, yeah.



STEVE:  And then he has a set of what he calls "fascicles."



LEO:  Okay.



STEVE:  Which are the other two.



LEO:  "The Art of Computer Programming."  They're brilliant, brilliant, yes.



STEVE:  Yes, it's titled, the masterwork is "The Art of Computer Programming."  And saying that is not hyperbole.



LEO:  Yeah, yeah.



STEVE:  There are aspects of computer programming that can be true art, and his work is full of lovely constructions similar to the use of a single exchange instruction being used to manage inter-thread synchronization.  In this case, as I tried to carefully explain last week, the whole point of using a single exchange instruction is that it is not necessary to reread anything because the act of attempting to acquire the ownership variable acquires it only if it wasn't previously owned by someone else, while simultaneously - and here simultaneity is the point and the requirement - also returning information about whether the variable was or was not previously owned by any other thread.



So if anyone wishes to give their brain a bit more exercise, think about the fact that in an environment where individual threads of execution may be preempted at any instant, nothing conclusive can ever be determined by reading the present state of the object ownership variable.  Since the reading thread might be preempted immediately following that reading, and during its preemption the owner variable might change, the only thing that anyone reading that variable might learn, that is, just simply reading it, is that the object being managed was or was not owned at the instant in time of that reading.



While that might be of some interest, it's not interesting to anyone who wishes to obtain ownership, since that information was already obsolete the instant it was obtained.  So that's what is so uniquely cool about this use of an exchange instruction which both acquires ownership only if it isn't owned and returns the previous state, meaning if it wasn't previously owned, now the thread that asked owns it.  And it's as simple as a single instruction, which is just, you know, conceptually so cool.



Javamantis said:  "Regarding episodes 970 and 969 with 'push button hardware config' options, my first thought is of the 2017 Saudi chemical plant attacked with the Triton malware.  The admins working on the ICS controllers deliberately left an admin permission key in the controllers, instead of walking the 10 minutes required to insert the key every time a configuration needed changing."



LEO:  I don't blame them.  That's 10 minutes, man.



STEVE:  Uh-huh.  "As a result, the attackers were able to access the IT systems and then the OT systems because the key was always left in and in admin mode."  He says:  "Lazy people will always work around inconvenient, very secure systems."



LEO:  Like me, yes.



STEVE:  And he finishes with "To 999 and beyond, like Voyager."  Yes, this podcast is going into interstellar space.



LEO:  999 and beyond.  To boldly go where no podcast has gone before.



STEVE:  That's not true, though, because I keep hearing TWiT talking about, oh, we - yeah, that's right.  Anyway, I thought he made a good point.  For example, the push button dangerous config change enabler should work on a change from not pushed to pushed, rather than on whether the button is depressed.  The electrical engineers among us will be familiar with the concept of "edge triggered" versus "level triggered."  If it's not done that way, people will simply depress the button once, then do something like wedge a toothpick into the button in order to keep it depressed.



My feeling is, the ability to bypass well-designed and well-intentioned security does not matter at all.  There's a huge gulf separating "secure by design" and "insecure by design."  And it's absolutely worth making things "secure by design" even if those features can be bypassed.  The issue is not whether they can be bypassed, but whether they are there in the first place to perhaps be bypassed.



If someone goes to the effort to bypass a deliberately designed security measure, then the consequences of doing that is 100% on them.  It's a matter of transferring responsibility.  If something is insecure by design, then it's the designers who are at fault for making the system insecurely.  You know, designing the system insecurely.  They may have assumed that someone would come along and make their insecure system secure, but we've witnessed far too many instances where that never happened.  So the entire world's overall net security will be increased if systems just start out being secure and are then later, in some instances, forced against their will to operate insecurely.



And if someone's manager learns that the reason the enterprise's entire network was taken over, all their crown jewels stolen and sent to a hostile foreign power, and then all their servers encrypted, is because someone in IT wedged a toothpick into a button to keep it held down for their own personal convenience.



LEO:  Of course they did.



STEVE:  Well, you won't be asking that manager for a recommendation on the rsum that will soon need updating.  Right.



LEO:  Wow.



STEVE:  It'll be your fault and no one else's.  David Sostchen tweeted:  "Hi, Mr. Gibson.  Longtime listener" - very formal - "and SpinRite owner."  Actually, Ant used to call me Mr. Gibson, but nobody else does.  He said:  "I was listening to podcast 955, and I meant to message you about the Italian company Actalis, but life has a tendency to get in the way.  They happen to be one of the few remaining companies that issue free S/MIME certificates.  I've been using them for years to secure all my email.  All the best, David."  So I just wanted to pass that on, David, thank you.



LEO:  That's good to know.  Who is it again?



STEVE:  An Italian company, Actalis, A-C-T-A-L-I-S, are issuing free...



LEO:  Because I've been paying for my S/MIME.



STEVE:  S/MIME certs.



LEO:  I mean, I use PGP most of the time, which is free.  But that's cool.  S/MIME is a lot easier for some people, so that's cool.



STEVE:  Yup.  Meanwhile, the FeloniousWaffle has tweeted:  "Hi, Steve.  I created an account on this platform to message you."  Oh, thus FeloniousWaffle.  He said:  "I cannot wait for your email to be up and running."  Neither can I.  "I was just listening to Episode 968 on my commute and believe the outrage of AT&T's encryption practices to be undersold."  Oh.  He says:  "You mention that if someone is able to decrypt one string to get the four-digit code, then they have everyone's code who shares the same string.  I believe it to be far worse than that.  Am I wrong in thinking that if they crack one, then they have all 10,000?



"I'm making some assumptions that there are only two ways that 10,000 unique codes produces exactly 10,000 unique encrypted strings.  The first, and this is what I'm assuming, AT&T used the same key to encrypt every single code."  That's right.  "The second would be to have a unique key for each code.  So code 123 would have to be a different key than 5678.  That seems farfetched to me.  Is there an error to my thinking?  Thanks for the podcast and everything you do.  Glad you are sticking around beyond 999.  Daryle."



Okay.  So I see what Daryle is thinking.  He's assuming that what was done was that, if the encrypted string was decrypted to obtain the user's four-digit passcode, then the other 9,999 strings could similarly be decrypted to obtain the other four-digit passcodes.  And he's probably correct in assuming that, if one string had been decrypted, then all the others could be, too.  But that isn't what happened.  No encrypted strings were ever decrypted, and the encryption key was never learned.  But due to the static nature of the passcodes' encryption, that wasn't necessary.



I wanted to share Daryle's note because it reveals an important facet of cryptography, which is that it's not always necessary to reverse a cryptographic operation, as in decryption in this case; but it's also true of hashing, where we've talked about through the years many instances where we don't need to unhash something.  You know, going only forward, in the forward direction, is often still useful.  If the results of going in the forward direction can only be reapplied to other instances, then a great deal can still be learned.



In this case, since people tended to use highly non-random passcodes, you know, reusing their birthday, their house's street number, or the last four digits of their phone number or Social Security number - all things that were also part of the exfiltrated data - and assuming a fixed mapping between their plaintext passcode and its encryption, meaning the key never changed, examining, for example, the details of all of the records having a common encrypted passcode - imagine that you, from this big massive database, you pulled together all the records with the same encrypted passcode, and you look at them, just that observation would very quickly reveal what single passcode most of those otherwise unrelated records shared, and thus all of them used.



For example, one household lived at 1302 Willowbrook, whereas the birthday of someone else was February 13th, and someone else's phone number ended in 1302.  So by seeing what digits were common among a large group of records all sharing only the same encrypted passcode, it would quickly become clear what identical passcode they all chose.  No decryption necessary.  So that's, you know, one of the cool things that we've seen about the nature of crypto in the field is there actually are some interesting ways around it when you have the right data, even if you don't have the keys.



Skynet tweeted:  "Hi, Steve.  Would having DRAM catch up and be fast enough eliminate the GhostRace issue?"  And I thought that was a very interesting question.  You know, we've talked about how caching is there to decouple slow DRAM from the processor's much more hungry need for data in a short time.  So the question could be reframed a bit to further clarify what we're really asking.  So let's ask:  If all of the system's memory were located in the processor's most local, instant access L1 cache, that is, if its L1 cache were 16GB in size, so that no read-to or write-from main memory took any time at all, would speculative execution still present problems?  And I believe the answer is yes.



Even in an environment where access to memory is not an overwhelming factor, the work of the processor itself can still be accelerated by allowing it to be more clever about how it spends its time.  Today's processors, for example, are not executing instructions one at a time.  And in fact processors have not actually been executing one instruction at a time for quite a while.  The concept of "out of order" instruction execution dates way back to the early CDC (Control Data Corporation) 6600 mainframe, which was the first commercial computer system, a mainframe, to implement out-of-order instruction execution.



And that was in 1968, I believe, is when the CDC 6600 happened.  It sucked in instructions ahead of them being needed.  And when it encountered an instruction whose inputs and outputs were independent of any earlier instructions that were still being worked on, it would execute that later instruction in parallel with other ongoing work because the instruction didn't need to wait for the results of previous instructions.  Nor would its effect change the results of previous instructions.



The same sort of instruction pipelining goes on today, and we would still like our processors to be faster.  If a processor had perfect knowledge of the future by knowing which direction it was going to take at any branch or where a computed indirect jump was going to land it, and it could reach its - if it had perfect knowledge of those things, it would be able to reach its theoretical maximum performance given any clock rate.  But since a processor's ability to predict the future is limited to what lies immediately in front of it, it must rely upon looking back at the past and using that to direct its guesses about the future - or, as we say, its speculation about its own immediate future.



Here's something to think about.  The historical problem with third-party cookies has been that browsers maintained in the past a single large shared cookie jar, as we've discussed before, in fact just recently.  So an advertiser could set its cookie while the user was at site "A," and read it back when the same user had moved to site "B."  This was never the way cookies were meant to be used.  They were meant to be used in a first-party context to allow sites to maintain state with their visitors.  The problem is that, until very recently, there has been no cookie compartmentalization.



We have the same problem with microprocessor speculation that we have had with third-party cookies, lack of compartmentalization.  The behavior of malware code is affected by the history of the execution of the trusted code that ran just before it.  Malware is able to detect the behavior of its own code, which gives it clues into the operation of previous code that was running in the same set of processors.  In other words, a lack of compartmentalization.  Malicious code is sharing the same microarchitectural state as non-malicious code because today there's only one set of state.  That's what needs to change.  And I would be surprised if Intel wasn't already well on their way to implementing exactly this sort of change.



I have no idea how large a modern microprocessor's speculation state is today.  But the only way I can see to maintain the performance we want today in an environment where our processors might be unwittingly hosting malicious code is to arrange to save and restore the microprocessor's speculation state whenever the operating system switches process contexts.  It would make our systems even more complicated than they already are, but it would mean that malicious code could no longer obtain any hints about the operation of any other code that was previously using the same system it is.



I'll omit this listener's full name since it's not important.  We'll call him John.  He says:  "I got nailed in a phishing email for AT&T."



LEO:  Oh.



STEVE:  Yeah, see the attached picture.  He said:  "No excuse, but at least I realized it immediately and changed my password," he said, "which is not one that has been used anywhere else of course."  He ended, saying, "Feel stupid...."



LEO:  No, because we've been talking about this AT&T breach.  He was expecting this email from AT&T.



STEVE:  Yup.  Yup.  Yup.  Exactly.  The email says:  "Dear Customer.  At AT&T we prioritize the security of our customers' information and are committed to maintaining transparency in all matters related to your privacy and data protection.  We are writing to inform you of a recent security incident involving a third-party vendor.  Despite our rigorous security measures, unauthorized access was granted to some of our customer data stored by this vendor.  This incident might have involved your names, addresses, email addresses, social security numbers, and dates of birth.



"We want to assure you that your account passwords were not exposed in this breach."  But they're about to be.  "We have notified federal law enforcement about the unauthorized access.  Please accept our apology for this incident.  To determine if  your personal information was affected, we encourage you to follow the link below to log into your account."



LEO:  Oh, boy.



STEVE:  And then there's a little highlight that says "Sign In."  And finally, "Thanks for choosing us.  AT&T."



LEO:  I'm willing to bet this is a copy of the actual email because it's too much corporate, like, rigorous security measures, but they did gain all your data.  It's very much what AT&T said.  So I bet the bad guy just copied the original AT&T email and just changed this little link here.



STEVE:  Exactly.  And, well, I would imagine that there was probably no sign-in in the original link because, you know, that's really what changes it into a phishing attack.  And so anyway, I just wanted to say this is how bad it is out there.  I mean, as you said, Leo, you saw it immediately.  We've been talking about it.  This is a listener of ours.  He knew about it before it came.



LEO:  He expected it.  He expected it.



STEVE:  So absolutely authentic-looking.



LEO:  They're so smart.  They're so evil.



STEVE:  You know, we really - we absolutely need to always be vigilant.



LEO:  And never click links in email.



STEVE:  No.  No.



LEO:  Never.



STEVE:  Even from Mom.  Tom Minnick said:  "With these 'atomic operations' to mitigate race conditions, how does that work with multicore processors when multiple threads are running in parallel?  Couldn't a race condition still occur?"  He says:  "I probably don't understand enough about how multicore processors handle threads."



So Tom's question actually is a terrific one, and it occurred to many of our listeners who wrote.  He and everyone were right to wonder.  The atomicity of an instruction only applies to the threads running on a single core since it, the core, can only be doing one thing at a time by definition.  Threads, as I said, are an abstraction for a single core.  They are not an abstraction if multiple cores are sharing memory.



So what about multiple cores or multi-processor systems?  The issue is important enough that all systems today provide some solution for this.  In the case of the Intel architecture, there's a special instruction prefix called "Lock" which, when it immediately precedes any of the handful of instructions that might find it useful, forces the instruction that follows to also be "atomic" in the sense of multiple cores or multiple memory-sharing processors.



Only one processor at a time is able to access the targeted memory location.  And after all, it's just an instant; right?  It's just essentially there's actually a - there is a locked signal that comes out of the chip that all the chips are participating with.  So the processor, when it's executing a locked instruction, drops that signal, performs the instruction, and immediately raises it.  So it's just an, you know, it's as infinitesimally brief lockout as could be.  So it doesn't hurt performance, but it prevents any other processor from accessing the same instruction at the same time.  Only one processor at a time is able to access the targeted memory location.



And there's one other little tidbit.  That simple exchange instruction is so universally useful for performing thread synchronization that the lock prefix functionality is built into that one instruction.  All the other instructions that can be used require an explicit lock prefix.  Not the exchange instruction.  It automatically is not only thread safe, but multicore and multiprocessor safe, which I think is very cool.



Finally, Michael Hagberg said:  "Credit Freeze.  Rather than unlock your entire account, it should work this way:  I'm buying a car.  The dealer tells me which credit service they use and the dealership's ID number.  I go to the credit service website, provide my Social Security number, PIN assigned by the site when I froze it, and the car dealer's ID number.  My account will then allow that car dealer only to access my account for 24 hours."



And Michael, I agree 100%.  And this just shows us that the child in you has not yet been beaten into submission, and that you are still able to dream big.  More power to you.  Wouldn't it be nice if the world was so well designed.



LEO:  I actually do everything but that last piece where you give the car dealer's ID to the credit bureau.  But I do ask them which credit bureau are you going to use.



STEVE:  Good, good.



LEO:  And then that's the one I unfreeze.  And I tell them, you've got whatever, three days to do this, and it's going to automatically lock up again.  And nowadays enough people use freezes that when they get that, they kind of know what happened.  And they'll call you and say, hey, your credit's frozen, yeah.



STEVE:  Yeah, right.  It's not unusual to encounter a freeze.  And in fact I did some googling around before I got my card with Amazon to find out which of the services they used.



LEO:  Yeah, exactly.



STEVE:  And, you know, and then that's the one I unlocked so that...



LEO:  I'd be more judicious, yeah.  I love the idea, though, of saying, hey, credit bureau, this guy is going to ask.  Don't tell anybody else.



STEVE:  Wouldn't that be?  Yeah, but Leo, you know, all the junk  mail we receive as elders...



LEO:  All those credit card offers.  Oh.



STEVE:  Yes.  It's because everybody's pulling our credit.



LEO:  By the way, when I froze all my accounts, I stopped getting those.



STEVE:  Yeah, I haven't had any for years.



LEO:  The only ones I get are from cards, existing cards saying, you know, hey, you've got a blue card.  Would you like a green one?  That's it.  Because no new card companies can get my information.  So it works.



STEVE:  Right.



LEO:  It works.



STEVE:  Right.  Okay.  We've got just two little bits regarding SpinRite.  Mike Shales said:  "Recently I've run into some issues with my old iMac, a mid-2017 model."  He said:  "I've wanted to support your valuable Security Now! efforts for some time, but investing the time to see if I could even run SpinRite on my Macs when they were all running without problems discouraged me.  But now.  You mentioned on your April 9th podcast:  'I wanted to remind any would-be Mac purchasers'" - oh, this is me speaking.  He's quoting now.  "'I wanted to remind any would-be Mac purchasers that this is the reason I created GRC's freeware named Bootable in favor of'" - okay, the name - "'In favor of 'DOS Boot.'  If you can get Bootable to congratulate you on your success in booting it, then exactly the same path can be taken with SpinRite.'"  Right.



So he said, he wrote:  "But Bootable is a Windows .exe file and needs a Windows machine to create a bootable USB flash drive; right?  Lacking a Windows machine, I made a bootable DOS drive from your ReadSpeed image download."  Wow.  Good going there.



LEO:  It's ambitious, yeah.



STEVE:  "Following instructions from ChatGPT, I used dd to write the ReadSpeed image to a 4GB flash drive.  Then following instructions in the GRC forum post 'Boot a Mac into FreeDOS for SpinRite or ReadSpeed,'" he said:  "I succeeded in booting my iMac into DOS and running ReadSpeed.  So far so good.  But I believe the current SpinRite 6.1 includes the capability to recognize more drives than previously, and might rely on features not provided in the version of DOS that I now have installed on my flash drive.  If so, perhaps downloading the SpinRite 6.1 .exe file and copying to my flash drive might not be ideal.  Is this an issue?  Thanks for the help.  Mike."



Well, okay.  Mike very cleverly arranged to use various tools at GRC - and, amazingly enough, ChatGPT - to create a bootable USB drive which successfully booted his mid-2017 iMac.  So first responding to Mike directly:  Mike, everything you did was 100% correct.  And if you place your copy of the SpinRite EXE into that USB stick and boot it, everything will work perfectly.  And if you run it at Level 3 on any older Macs with solid-state storage, you can expect to witness a notable and perhaps even very significant subsequent and long-lasting improvement in the system's performance.



And while it won't be obvious, there's also very a good reason to believe that in the process you'll have significantly improved the system's reliability.  The reason the SSD will now be much faster is that it's needing, as I mentioned before, to struggle much less after running SpinRite to return the requested information.  And we will be learning far more about this during the work on SpinRite 7.  And although 6.1 is a bit of a blunt instrument in this regard, it works, and it's here today.  To Mike's question, the specific version of FreeDOS does not matter at all, since DOS is only used to load SpinRite and to write its log files.  Otherwise, SpinRite ignores DOS and interacts directly with the system's hardware.  So yes, you can run it on your ReadSpeed drive.



I wanted to share Mike's question because I just finished making some relevant improvements.  He mentioned correctly that Bootable is Windows-only freeware.  But over the weekend the Bootable download was changed from an EXE to a ZIP archive, and the ZIP archive now also contains a small Bootable file system image which can be used by any Mac or Linux user to directly create a Bootable boot-testing USB drive.



LEO:  Any Intel Mac or Linux user.



STEVE:  Yes, Intel, yes.



LEO:  We should really emphasize that because most Mac users now are no longer Intel, yeah.



STEVE:  Yeah.  Yeah.  One of the guys in GRC's web forums put me onto a perfect and easy-to-use - oh, I should mention, Leo, we've solved the Intel problem, but that's for another time.



LEO:  Oh.  Tease me, why don't you.



STEVE:  Yeah.  We've got - we have some guys have figured out how to boot on UEFI-only systems and on ARM-based...



LEO:  On silicon, wow.



STEVE:  ...silicon using some concoction of virtual machines.  And I haven't followed what they're doing because I'm just focused on getting what all of this has done done.  Anyway, there's something known as Etcher by a company called Balena.  It is a perfect, easy-to-use for an Intel Mac person means of moving the Bootable image onto a USB without the dd command.  Dd makes me nervous because you need to know what you're doing.  I mean, it's a very powerful direct drive copying tool.  Linux people are probably more comfortable with dd.  I'm glad that this Mac user, Mike, was able to get ChatGPT to help him, and I'm glad that ChatGPT just didn't stumble over hallucination at that particular moment.



LEO:  You can erase everything with dd very easily.



STEVE:  Yeah.



LEO:  Careful.



STEVE:  Yeah.  And lastly, Sean wrote:  "Hey, Steve.  I'm sure you're hearing this a lot, but Windows" - oh.  "Windows did not trust SpinRite despite all your signing efforts.  I had to clear three severe warnings before it would allow me to keep 6.1 on my system for use.  I hope it gets better soon for users less willing to ignore the scary warnings from Microsoft."  Signed, Sean.



And yup.  I don't recall whether I had mentioned it here also, since I've participated a lot about this over in this discussion in the GRC's newsgroups.  One thing that has been learned is that Microsoft has decided to deprecate any and all special meaning for EV, extended validation, code signing certificates.  It's gone.  All those hoops I jumped through to get remote server-side EV code signing to work remotely on an HSM device will have no value moving forward, except having the signing key in the HSM does prevent anybody, even me, I mean, from extracting it.  It can't be extracted.  It can only be used to sign.



When I saw this news, I reached out to Jeremy Rowley, who's my friend and primary contact over at DigiCert, to ask him if I had read Microsoft's announcement correctly.  And he confirmed that Microsoft had just, like, the week before surprised everyone in the CAB forum with that news.  Apparently, what's at the crux of this is that for, you know, historically, end users were able to use EV code signing certificates to sign their kernel drivers.  That was the thing Microsoft most cared about as far as EV was concerned.



But after the problems with malicious Windows drivers, Microsoft has decided to take away that right and require that only they, meaning Microsoft, will be authorized to sign Windows kernel drivers in the future.  In their eyes, this eliminated the biggest reason for having and caring at all about EV code signing certs.  So they will continue to be honored for code signing, but EV certs will no longer have any benefit.  They will confer no extra meaning.



What I think is going on regarding SpinRite is that something Windows Defender sees inside SpinRite 6.1's code, which was not in 6.0, absolutely convinces it that this is a very specific Trojan named "Wacatac.B," which I guess you pronounce "whack attack."  If I knew what part of SpinRite was triggering these false positives, I could probably change it.  I have some ideas, so I'm going to see, because we just can't keep tolerating these sorts of problems from Microsoft, and it doesn't now look like my having an EV cert - it's been three months now, and tens of thousands of copies of GRC's freeware, because, you know, thousands, plural thousands of copies are being downloaded every day.  I resigned them all with this new certificate in order to get it exposed and let Microsoft see that, you know, whoever was signing this wasn't producing malware.  But, you know, here's Mike, or no, sorry, Sean, who just said, you know, he had to go to extreme measures to get Windows to leave this download alone.



LEO:  Huh.



STEVE:  So, grumble.



LEO:  Grumble, grumble.



STEVE:  Big-time.  Okay.  We're going to talk about what the EU is doing, Leo, after you share our last sponsor with our listeners.



LEO:  Yes.  Breaking news, however, that you will, depending on your point of view, will either be surprised or not surprised to hear.



STEVE:  Yeah.



LEO:  Google has decided to delay third-party cookie blocking.



STEVE:  Oh.



LEO:  Until next year.



STEVE:  Wow.



LEO:  From Digiday, this fantastic opening sentence by Seb Joseph:  "Google is delaying the end of third-party cookies in its Chrome browser again.  In other unsurprising developments, water remains wet."



STEVE:  Wow.



LEO:  So they did not outline a more specific timetable beyond hoping for 2025.



STEVE:  Okay.  And that, I mean, it does show you the...



LEO:  Resistance.



STEVE:  ...problem with taking this away.



LEO:  They promised it originally in January 2020.  This is the third time they've pushed it back.  And I'm guessing it's not going to be the last.  Some of this is actually, you know, intertwined with the UK Competition and Markets Authority.  They want, they say:  "It's critical the CMA has sufficient time to review all the evidence, including results from industry tests, which the CMA has asked market participants to provide by the end of June."



STEVE:  Ah, in order to see whether the Privacy Sandbox will be a replacement.



LEO:  Right, right.  "We recognize," Google says, "there are ongoing challenges related to reconciling divergent feedback from the industry.  Regulators and developers will continue to engage closely with the entire ecosystem."  Yeah, but some of this is that the CMA wants to see proof, and they're not ready to provide proof.



STEVE:  Well, Leo, here's another good - another reason I'm so happy we're going past 999.



LEO:  Yeah.



STEVE:  Because November is when we hit 999.  And I would not be here for...



LEO:  Let's make a deal that you'll keep doing the show until Google phases out third-party cookies.



STEVE:  Oh, no.



LEO:  Rats.



STEVE:  Well past.



LEO:  Almost fooled him.



STEVE:  No, no.  I think it's going to happen.  I think it's inevitable.



LEO:  You think.  Okay.  We'll see.  It's been four years.



STEVE:  I would wager to 2025.  I'll go for next year.



LEO:  Let's go for 2025.  Let's do it.  Let me real quickly mention our great sponsor, and then we can get to the meat of the matter, these Chat guys here going on, what's going on in Europe.  All right.  Let's talk about Chat, Steve Gibson.



STEVE:  Okay.  So, oh, boy.  Across the pond from the U.S., the EU is continuing to inch forward on their controversial legislation, commonly referred to as "Chat Control," thus today's title is "Chat (out of) Control," which proposes to require providers of encrypted messaging services to somehow arrange to screen the content that's carried by those services for child sexual abuse material, commonly known as CSAM.  As I said when we last looked at this last year, 2024 will prove to be quite interesting since all of this will likely be coming to a head this year.  What's significant about what's going on in the EU, unlike in the UK, is that the legislation's language carries no exclusion over the feasibility of performing this scanning.



Just to remind everyone who has a day job and who might not be following these political machinations closely, last year the UK was at a similar precipice.  And with their own legislation, at the 11th hour, they added some language that effectively neutered it while allowing everyone to save face.  For example, last September 6th, Computer World's headline read "UK rolls back controversial encryption rules of Online Safety Bill," and followed that with "Companies will not be required to scan encrypted messages until it is 'technically feasible,' and where technology has been accredited as meeting minimum standards of accuracy in detecting only child sexual abuse and exploitation content."



So since it's unclear how any automated technology might successfully differentiate between child sexual abuse material and, for example, a photo that a concerned mother might send of her child to their doctor, there's little concern that the high bar of "technical feasibility" will be met in the foreseeable future.  While the UK came under some attack for punting on this, the Big Tech companies all breathed a collective sigh of relief.



But so far - and, boy, there's not much time left - there is no sign of the same thing happening in the EU, not even a murmur of it.  One of the observations we've made about all such legislation was the curious fact that, if passed, the legislation would mean that the legislator's own secure, encrypted, and private communications would similarly be subjected to surveillance and screening.  Or would they?



Two weeks ago, on April 9th, the next iteration of the legislation appeared in the form of a daunting 203-page tome. Fortunately, the changes from the previous iteration were all shown in bold type, or crossed out, or bold underlined, or crossed out and underlined, all meaning different things.  But that made it at least somewhat possible to see what's changed.  As you can tell, I spent way too much time with that 203 pages.  This was brought to my attention by the provocative headline in an EU website, "ChatControl:  EU ministers want to exempt themselves."



And what that article went on to say was:  "According to the latest draft text of the controversial EU Child Sexual Abuse Regulation proposal leaked by the French news organization Contexte, which the EU member states discussed, the EU interior ministers want to exempt professional accounts of staff of intelligence agencies, police, and military from the envisioned scanning of chats and messages.  The regulation should also not apply to 'confidential information' such as professional secrets.  The EU governments reject the idea that the new EU Child Protection Centre should support them in the prevention of child sexual abuse and develop best practices for prevention initiatives."



Okay.  So the EU has something called the "Pirate Party," which doesn't seem to be well named, but it is what it is.



LEO:  No, it's a real - it's, you know, the Pirate Bay people.



STEVE:  Yeah, the Pirate pirates.



LEO:  Yeah.  It's a party of pirates, yeah.  And popular, I might add.



STEVE:  Yes.  It's formed from a collection of many member parties across and throughout the European Union.  The Party was formed 10 years ago, back in 2014, with a focus upon Internet governance.  So the issues created by this pending legislation is of significant interest to this group.  To that end, one of the members of Parliament, Patrick Breyer, had the following to say about these recent changes to the proposed legislation which came to light when the document leaked.



He said:  "The fact that the EU interior ministers want to exempt police officers, soldiers, intelligence officers, and even themselves from chat control scanning proves that they know exactly just how unreliable and dangerous the snooping algorithms are that they want to unleash on us citizens.  They seem to fear that even military secrets without any link to child sexual abuse could end up in the U.S. at any time.



"The confidentiality of government communications is certainly important, but the same must apply to the protection of business and of course citizens' communications, including the spaces that victims of abuse themselves need for secure exchanges and therapy.  We know that most of the chats leaked by today's voluntary snooping algorithms are of no relevance to the police, for example family photos or consensual sexting.  It is outrageous that the EU interior ministers themselves do not want to suffer the consequences of the destruction of digital privacy of correspondence and secure encryption that they are imposing upon us.



"The promise that professional secrets should not be affected by chat control is a lie cast in paragraphs.  No provider and no algorithm can know or determine whether a chat is being conducted by doctors, therapists, lawyers, defense lawyers, et cetera, so as to exempt it from chat control.  Chat control inevitably threatens to leak intimate photos sent for medical purposes and trial documents sent for defending abuse victims.  It makes a mockery of the official goal of child protection that the EU interior ministers reject the development of best practices for preventing child sexual abuse.



"It couldn't be clearer that the aim of this bill is China-style mass surveillance, and not better protecting our children.  Real child protection would require a systematic evaluation and implementation of multidisciplinary prevention programs, as well as Europe-wide standards and guidelines for criminal investigations into child abuse, including the identification of victims and the necessary technical means.  None of this is planned by the EU interior ministers."



So after the article finished quoting Patrick Breyer, it noted that the EU governments want to adopt the chat control bill by the beginning of June.  We're approaching the end of April, so the only thing separating us from June is the month of May.  I was curious to see whether the breadth of the exclusion might have been overstated in order to make a point, so I found the newly added section of the legislation on page 6 of the 203-page PDF.  It reads - this is Section 12a.  The "a" is the new part.



"In the light of the more limited risk of their use for the purpose of child sexual abuse and the need to preserve confidential information, including classified information, information covered by professional secrecy and trade secrets, electronic communications services that are not publicly available" - that's the key.  "Electronic communications services that are not publicly available, such as those used for national security purposes, should be excluded from the scope of this Regulation.  Accordingly, this Regulation should not apply to interpersonal communications services that are not available to the general public, and the use of which is instead restricted to persons involved in the activities of a particular company, organization, body, or authority."



Okay, now, I'm not trained in the law, but that doesn't sound to me like an exclusion for legislators who would probably be using iMessage, Messenger, Signal, Telegram, WhatsApp, et cetera.  It says, "This Regulation should not apply to interpersonal communications services that are not available to the general public."  So, you know, internal proprietary intelligence agency communication software, you know, applications.



Remember that it's this proposed EU legislation which includes the detection of "grooming" behavior in textual content.  So it's not just imagery that needs to be scanned, but the content of all text messaging.  We're also not talking about only previously known and identified content which is apparently circulating online, but also anything the legislation considers "new" content.  As I read through section after section of what has become a huge mess of extremely weak language that leaves itself open to whatever interpretation anyone might want to give, my own lay feeling is that this promises to create a huge mess.  I've included a link to the latest legislation's PDF in the last page of the show notes for anyone who's interested.



You'll only need to read the first eight pages or so to get a sense for just what a catastrophic mess this promises to be.  As is the case with all such legislation, what the lawmakers say they want, and via this legislation will finally be requiring, is not technically possible.  They want detection of previously unknown imagery and textual dialog which might be seducing children while at the same time honoring and actively enforcing EU citizen privacy rights.  Oh, and did I mention that 78% of the EU population that was polled said they did not want any of this?



And it occurred to me that encryption providers will not just be able to say they're complying when they are not, because activist children's rights groups will be able to trivially test any and all private communications services to verify that they do, in fact, detect and take the action that the legislation requires of them.  All that's needed is for such groups to register a device as being used by a child, then proceed to have a pair of adults hold a seductive grooming conversation and perhaps escalate that to sending some naughty photos back and forth.  And you can believe that, if the service they're testing doesn't quickly identify and red-flag the communicating parties involved, those activist children's rights groups will be going public with the service's failure under this new legislation.



I've said it before, and I understand that it can sound like an excuse and a copout, but not all problems have good solutions.  There are problems that are fundamentally intractable.  This entire debate surrounding the abuse of the absolute privacy created by modern encryption is one such problem.  This is not technology's fault.  Technology simply makes much greater levels of privacy practical, and people continually indicate that's what they prefer.  As a society we have to decide whether we want the true privacy that encryption offers, or whether we want to deliberately water it down in order to perhaps prevent some of the abuse that absolute privacy also protects.



LEO:  Agreed, agreed, and agreed.



STEVE:  Yeah.  I do commend to anyone, the last page of the show notes has a link.  It's not widely available publicly because it was leaked, and Patrick-Breyer.de, so a German site, has it on his site and is making it available.  So you'll need to get it if you're interested.  But, boy, as I said, just reading through it, it is, again, it's insanely long at 203 pages.  I struggled to find any language about, like, what time period this takes effect over.  I couldn't find any.  It all seems to indicate, once this legislation is in place, that the organizations need to act.



But I just think the EU is stepping into a huge mess.  And again, as I said, 2024, I said last year this next year, 2024 we're in now, is going to be one to watch because lots of this is beginning to come to a head.  Although, Leo, as you just shared with us, not the third-party cookie issue with Chrome.  That's been punted into when we have four digits on this podcast.



LEO:  In the future.  Ah, yeah.  It's an interesting world we live in.



STEVE:  So I imagine when the legislation happens, and it's supposed to be happening in early June, there will be lots of coverage.  We'll be back to it, and we'll know, you know, we'll have some sense for when it's taking effect and what the various companies are choosing to do.



LEO:  Yeah, they might be well modified from this leaked document.  There will certainly be amendments and things like that.  So we'll have to look at the actual legislation to see what's happening.



STEVE:  Right.



LEO:  And we will because that's what we do.  That's what we do here.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#972

DATE:		April 30, 2024

TITLE:		Passkeys:  A Shattered Dream?

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-972.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  The choice for this week's main topic received some serious competition from some surprising legislation that came into effect yesterday in the United Kingdom.  So we're going to start by taking a close look at what happened in the UK that promises to completely change the face of consumer IoT device security.  As we'll see, that's not an overstatement; the world as we've known it just changed.  While that exploration is going to consume most of the first half of today's podcast, I also want to look at what happened last week with Chrome's change of plan regarding third-party cookies, I have a bit of listener feedback to share, and news of the next installment in a long-running science fiction book series.  I also have the welcome news that I am finally working on bringing up GRC's email communications system.  Then we'll finish by taking a look at a blog posting by an industry insider that many of our listeners forwarded to me, asking "What do you think about this?"



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Last night he was sitting in his armchair, having a nice glass of Cabernet Sauvignon, reading the news, when he saw the announcement that the United Kingdom is going to make a major change to how IoT devices work.  He is so excited, he's come here, he leapt to his feet and said "We've got to talk about this on Security Now!."  So we will.  What happened with Chrome dumping its plans to dump third-party cookies?  And let's talk a lot about Passkeys.  There was an interesting post Steve read, and a lot of you read, saying it's not working.  Are Passkeys over?  All that and more coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 972, recorded April 30th, 2024:  Passkeys:  A Shattered Dream?



It's time for Security Now!, the show where we - I know you wait for this all week long; don't you.  We cover the security field, the computer field, every other field, the sci-fi field, with this guy right here, Mr. Steven "Tiberius" Gibson.  May the Fourth be with you.



STEVE GIBSON:  And Leo?	



LEO:  Yes.



STEVE:  You are - oh, the Fourth.



LEO:  Yeah, it's coming up.



STEVE:  You are correct about the breadth of our coverage.  I do have a little sci-fi mention, a real quickie.



LEO:  Oh, good.  Oh, good.



STEVE:  For those listeners who have been following along, what has turned out to be my favorite saga, which promises - well, actually I was going to say it promises to be never-ending, but it actually does have an end; and this author is, you know, continuing to deliver on the promise.



LEO:  Nice.



STEVE:  Okay.  But the choice for this week's main topic received some serious competition from some surprising legislation that came into effect just yesterday in the UK, in our dear beloved United Kingdom.  So we're going to start by taking a close look at what happened in the UK that, I kid you not, promises to completely change the face of consumer IoT device security, like, immediately.  I know that's, you know, I don't think that's an overstatement.  I think that the world as we have known it has just changed.  So you could see why that was competing for today's topic, which we'll get to after that.  We're going to explore a little bit of, like, what happened with the news that just came in that you announced during last week's podcast, Leo, which was Chrome's sudden change of plan regarding its third-party cookie handling.



LEO:  Yeah.



STEVE:  I'm going to get a little bit into that.  But I'll explain why I'm probably going to punt most of it till next week because it turns out there's a lot.  I also have a little bit of listener feedback to share.  And as I said, news of the next installment in a long-running sci-fi book series.  I've also got some welcome news that I'm finally working on GRC's email system, which will come as great news for our listeners.  And then we're going to finish by taking a look at a blog posting by an industry insider developer that a surprising number of our listeners, this thing has gotten a lot of traction out in the industry, and our listeners kept forwarding me links to it, asking whether Passkeys is a shattered dream.



LEO:  Oh.



STEVE:  They've all been saying, "What do you think about this?"  And so, you know, some neat stuff to talk about, and of course a great Picture of the Week that's apropos of the topic that we've been covering lately of Voyager.



LEO:  Yes.  Passkeys:  A Shattered Dream, the topic on today's - it sounds like, you know, [sound], today.



STEVE:  We go up, and we come down.



LEO:  And we go down, yeah.  I like Passkeys.  It is a shattered dream.  Although one could say you might have a little dog in this hunt because honestly the SQRL protocol that you created is a far better way of doing it.



STEVE:  It solved the problem that is dogging this.  But, you know, I don't even mention it in my coverage because all of our listeners already know.



LEO:  They know.



STEVE:  That, you know, I did solve it the right way, and that's not what we got.



LEO:  And I suppose that ship has sailed.



STEVE:  Yeah.



LEO:  And you just have to have the big guys behind it before it has any chance of being adopted.



STEVE:  Well, and in fact this author, this guy is the author of a very strong WebAuthn library, the one for Rust that SUSE is using, and many others have forked from.  He notes that Chrome has succeeded in killing some features just by not adopting it. 



LEO:  Right, exactly.  Yeah.  That's too much power in Google's hands, if you ask me.



STEVE:  Well, they've got it.



LEO:  We will get the - yeah, I know, it's too late now.  We will - by the way, you know, I used OpenAI's chat for a long time and made some GPTs and really like it, especially v4.  But I just started using on Sunday, Kevin Rose said try Gemini.  Have you not tried - and so I tried Gemini Advanced, and it is mind-blowingly good.  So we've got us a race, which is exciting.  Very interesting.  And I'm sure there'll be lots of security implications there.  Gemini writes excellent code, unfortunately.



STEVE:  Well, and did you see the blurb?  I didn't get it in the show notes where ChatGPT4, just given the list of CVEs...



LEO:  Of CVEs, we talked about it, yes.



STEVE:  Is able to generate exploits.  Just like, oh, here you go.



LEO:  They took 87% of the CVEs they were given, just the description, and created the proof of concept.



STEVE:  Working exploits for them.  So script kiddies have been elevated.  Oh.



LEO:  What a world.  We are - this is just getting wild.  That's all I can say.



STEVE:  Yeah.



LEO:  And not in a good way, to be honest with you.  Ready for the Picture of the Week.



STEVE:  Okay.  So this resonated with me from something in my youth, and I figured that you would, being the king of pop that you are, you know, you'd go, oh, yeah, that's about this.  But it drew a blank for you.



LEO:  Kind of old pop, yeah.  Well, it sounds like, I don't know what, a joke, maybe, an old joke.



STEVE:  Okay.  So we have a cartoon which is apropos of the topic we've been following about the fate of Voyager 1 and how by some miracle it is back on the air.  Anyway, so the cartoon shows a couple of cute little green aliens in their classic circular UFO saucer with the glass bubble.  And the saucer, however, is labeled "Salvage."  And it's got the tow truck hook off the back with a hook.  So they're, you know, they're flying around out in space looking for stuff.  And so the alien - and then so what they have done is they just - we see them looking at the Voyager 1 probe labeled Voyager 1977, you know, with a NASA on it, and its various probes and sensors and so forth.  And the one alien is saying, "15 billion miles on it, but the radio still works."



LEO:  It's true.



STEVE:  And I'm reminded of something about used - it is true, absolutely true.



LEO:  Yeah, yeah.



STEVE:  I'm reminded of something about used cars back from my high school days.



LEO:  Yeah, I think it's an old joke.



STEVE:  Like there was some meme about, well, it's broken down and only goes downhill or something, but the radio still works.



LEO:  Right, right.



STEVE:  Anyway, 15 billion miles, the radio still works.  I thought that was a cute little observation.



Okay.  So - okay.  Yesterday - mark this day in your calendars, yesterday, April 29th, 2024 - a new law went into effect in the UK.  Not just like legislation got submitted somewhere; or, well, we're going to give this some consideration; and not just like, oh, recommendations like we're always getting.  This is a law that went into effect yesterday.  And I'm astonished by this - not only by the fact that it happened and by the content, but the depth of the quality of the baseline requirements that this proposes, which is why we're going to spend some time on this because this is huge.



Now, the Guardian, what was also odd was I guess I must have seen this, like, immediately after it happened because I looked around for, like, other more fleshed-out coverage, and nobody was picking up on it.  Now there's like, overnight it's like beginning to happen in the security info sphere because it's like, whoa, what?



So yesterday, only the Guardian in the UK seemed to have anything to say.  And they didn't say much, but their headline was "No more 12345:  Devices with weak passwords will be banned in the UK."  And the subhead was "Makers of phones, TVs, and smart doorbells legally required to protect devices against access by cybercriminals."  And, you know, not just passwords, baby.  That's just the tip of the iceberg.  I mean, this is - it's comprehensive legislation.



In order to get more detail, I went to the source.  So this is the GCHQ's National Cyber Security Centre blog posted yesterday which was titled "Smart devices:  New law helps citizens to choose secure products."  And actually even that's an understatement because this impacts not only the manufacturers of devices that may not be in the UK, but also anyone importing them and anyone retailing them.  And it's got some teeth behind it.  So it's not that the consumers in the UK are going to have a choice.  There aren't going to be any non-compliant options to purchase.  And of course this ends up being global because we're in a global economy.



Okay.  So what GCHQ said in sort of their top-level blog announcement, they said:  "From April 29th, 2024, manufacturers of consumer 'smart' devices must comply with new UK law."  That is yesterday.  "The law, known as the Product Security and Telecommunications Infrastructure act, or PSTI act, will help consumers to choose smart devices that have been designed to provide ongoing protection against cyberattacks.  The law means manufacturers must ensure that all their smart devices meet basic cyber security requirements."



Now, here they just highlight three, and they happen to be the first three.  But they're the first three of, like, 14, and they're all really significant.  But the first three are:  "The manufacturer must" - get this.  I mean, it sounds like I wrote this from the podcast.  "The manufacturer must not supply devices that use default passwords, which can be easily discovered online and shared.  If the default password is used, a criminal could log into a smart device and use it to access a local network or conduct cyberattacks."  So again, this is just sort of the blog discussing the legislation.  We'll get to the actual legalese here in a second.  But so they're sort of like explaining the why of these requirements at this point.



Second one, the manufacturer must provide a point of contact for the reporting of security issues which, if ignored, could make devices exploitable by cyber criminals.  Right.  And three, the manufacturer must state the minimum length of time for which the device will receive important security updates.  And then they flesh that out by explaining:  When updates are no longer provided, devices are easier to hack, or may stop working as designed.  So that's just the top three, which this law makes a requirement for the sale of anything; and, I mean, and again, there just - there aren't any loopholes in this.  It's like, if it's going to connect to the Internet or a network, it has to have this.



So they said:  "Although most smart devices are manufactured outside the UK, the PSTI act also applies to all organizations importing or retailing products for the UK market."  I mean, like, Amazon.  So they said:  "Failure to comply with the act is a criminal offense, with fines up to 10 million, or 4% of qualifying worldwide revenue, whichever is greater."



They said:  "The law applies to any 'consumer smart device' that connects either to the Internet or to a home network, for example, by WiFi.  This may include" - and here's the first of several enumerations that we're going to be encountering.  But here they said:  "Smart speakers, smart TVs, and streaming devices; smart doorbells, baby monitors, and security cameras; cellular tablets, smartphones, and games consoles; wearable fitness trackers including smart watches; smart domestic appliances such as light bulbs, plugs, kettles, thermostats, ovens, fridges, cleaners, and washing machines."



They finish:  "The NCSC has produced a 'point of sale' leaflet for retailers to distribute in-store to their customers.  It explains how the PSTI regulation affects consumers, and why it's important to choose smart products that protect against the most common cyberattacks."



So, you know, it's the end of April; right?  It's not the beginning.  This is not April Fool's Day.  This happened on the 29th.  And the first thing I need to say is "Holy crap.  Where did this come from?"  Turns out it's been in the works for five years and just not much is, you know, it hasn't been drawing much attention to itself.  So fines in the amount of the greater of 10 million, I think that was 12.5 million USD at this point, at the current exchange rate, or 4% of a manufacturer's qualifying worldwide revenue, whichever is the greater.  This is the sort of legislation that can really make a profound overnight difference in consumer security.  And since a great many manufacturers have shown through their actions or, you know, deliberate inaction, that they need to be made to change, this is the change that's required to make them.



Since this is huge and potentially affects all products worldwide which might find their way to the UK because it impacts not only manufacturers, as I said, but anyone who imports or retails such products, I needed to get a bit more back story.  So I did some digging.  On the gov.uk website I found the actual legislation.  It turns out, as I said, it's been quietly in the works for several years, and it really is a law, not just some watered-down milquetoasty "recommendations" that we see too often here in the U.S.



The Verge picked up on this, and in their coverage they noted that, here in the United States, our FCC is trying something similar in its forthcoming "Cyber Trust Mark" program.  You know, they liken it to the federal ENERGY STAR program, explaining that the Cyber Trust Mark logo indicates which products comply with the program's recommendations, which include strong default passwords.  But like ENERGY STAR, nobody's forcing companies to go along with it; and consumer product packaging has become so encrusted with certifications and compliance logos that it's unclear whether anyone even notices.



As we know, consumers are focused on three things:  Does it do what I need?  What does it cost?  And what additional nice-to-have features does it offer?  Whether or not a connected light switch has a default password is the last thing on anyone's, like, shopping list.  In other words, while the United States continues to be completely lame on this, the United Kingdom has taken the only action that has any chance of actually producing results for the consumer.  And thanks to the fact that we still live in a blessedly globalized economy, everyone everywhere will obtain the security benefits that the kingdom is now requiring as a matter of law.



Since this matters to everyone everywhere, and since it's going to change the face of Internet-connected consumer technology, let's take a closer look at what the legislation actually has to say.  First of all, this comes from a non-profit organization, that is, the actual think-tank behind the legislation, which is what got enacted, known as the European Telecommunications Standards Institute, or ETSI, E-T-S-I.  We've spoken of them in the past.  This work has been underway, as I mentioned, for the past five years, having started back in 2019, in February, with the publication of its v1.1.1, and it's been quietly making its way forward year by year.



The Baseline Requirements document is the one that's most relevant.  It's a 34-page PDF that I've given a GRC shortcut to.  The shortcut is ETSI, E-T-S-I.  So if you're curious, you can just put grc.sc/etsi into your browser, and you'll be bounced over to a document titled "Cyber Security for Consumer Internet of Things:  Baseline Requirements."



The document's introduction explains.  They said:  "As more devices in the home connect to the Internet, the cyber security of the Internet of Things (IoT) becomes a growing concern.  People entrust their personal data to an increasing number of online devices and services.  Products and appliances that have traditionally been offline are now connected and need to be designed to withstand cyber threats.  The present document brings together widely considered good practice in security for Internet-connected consumer devices in a set of high-level outcome-focused provisions.  The objective of the present document is to support all parties involved in the development and manufacturing of consumer IoT with guidance on securing their products.



"The provisions are primarily outcome-focused, rather than prescriptive, giving organizations the flexibility to innovate and implement security solutions appropriate for their products.  The present document is not intended to solve all security challenges associated with consumer IoT.  It also does not focus on protecting against attacks that are prolonged or sophisticated or that require sustained physical access to the device.  Rather, the focus is on the technical controls and organizational policies that matter most in addressing the most significant and widespread security shortcomings.  Overall, a baseline level of security is considered.  This is intended to protect against elementary attacks on fundamental design weaknesses," they say, "such as the use of easily guessable passwords.



"The present document provides a set of baseline provisions applicable to all consumer IoT devices.  It's intended to be complemented by other standards defining more specific provisions and fully testable and/or verifiable requirements for specific devices which, together with the present document, will facilitate the development of assurance schemes.  Many consumer IoT devices and their associated services process and store personal data.  The present document can help in ensuring that these are compliant with the General Data Protection Regulation (GDPR).  Security by design is an important principle that is endorsed by the present document."



Okay.  So my goal for today's discussion of this is to accurately convey how comprehensive this document, and the legislation that backs it up, actually is, which I was astonished by.  So here's the bullet point.  I assembled this from the document, just to give you a sense for how comprehensive it is.  So here's the list of its main topics:  No default universal passwords.  Implement a means to manage reports of vulnerabilities.  Keep software updated.  Securely store sensitive security parameters.  Communicate securely.  Minimize exposed attack surfaces.  Ensure software integrity.  Ensure that personal data is secure.  Make systems resilient to outages.  Examine system telemetry data.  Make it easy for users to delete their data.  Make installation and maintenance of devices easy.  Validate input data.  And data protection provisions for consumer IoT they go into at length.



And again, these are not, like, gee, we wish we had these.  The legislation is about the policy.  As I've always said, there's a complete difference between policy and, you know, mistakes.  Anybody can make a mistake; but policy is, you know, your stated goal.  And so this legislation enshrines these goals as the policy that consumer IoT devices sold in the UK must incorporate.  They must have these as policies, as operating goals which are implemented in the device.  So each of these major topics is broken down into multiple pieces, and each of those pieces is tagged with one of four possible requirement levels.



Now, here's where for a moment I was concerned that my enthusiasm for this was going to be dashed because we've got mandatory, recommended, conditionally mandatory, or conditionally recommended.  And I thought, oh, great.  Well, if everything is just recommended, then we've gotten nothing.  Turns out almost everything is mandatory.  So there are a few things where they've backed off of mandatory.  But for the most part it's mandatory across the board.



The scope of what the document covers is also very clearly laid out, that is, you know, to not allow people to say, oh, that doesn't apply to us, so we don't have to do that.  No, I don't think anyone's going to get a free pass here.  It says, and this is in the baseline requirements document:  "The present document specifies high-level security and data protection provisions for consumer IoT devices that are connected to network infrastructure."  So they even broadened it to network infrastructure, meaning if you're connected to something, this is for you.



They say, parens:  "...(such as the Internet or home network), and their interactions with associated services.  The associated services are out of scope.  A non-exhaustive list of examples of consumer IoT devices includes" - and it's pretty much what I just said.  But there's additional ones here:  "Connected children's toys and baby monitors; connected smoke detectors, door locks, and window sensors; IoT gateways, base stations, and hubs to which multiple devices may connect; smart cameras, TVs, and speakers; wearable health trackers; connected home automation and alarm systems, especially their gateways and hubs; connected appliances, such as washing machines and fridges; and smart home assistants."



They said:  "The present document provides basic guidance through examples and explanatory text for organizations involved in development and manufacturing of consumer IoT on how to implement those provisions.  Table B.1 provides a schema for the reader to give information about the implementation of the provisions."  Now, okay, Table B.1, that's the table which breaks down all the topics and subtopics with the mandatory or recommended categories.  But then the idea is that manufacturers will print this out and be required to fill in for every one of those items the compliance level of their device, attesting then to the fact that they have met the recommendations.



So they said:  "Devices that are not consumer IoT devices, for example those that are primarily intended to be used in manufacturing, healthcare, or other industrial applications, are not in scope of the present document."  So this is squarely aimed at consumer IoT residential-style devices.  Of course, once that's all that's available for the consumer, other enterprises and anyone who purchases the devices get the benefit of all these features which have been required at this level.  And they said:  "The present document has been developed primarily to help protect consumers; however, other users of consumer IoT equally benefit from the implementation of the provisions set out here."



Okay, now, the document does differentiate something it considers to be a "constrained device," basically.  And again, nobody gets a free pass on this one.  But they did recognize that there are some things that, while they're connected and consumer devices, they just - they have too many constraints to meet what is otherwise a set of very high bars.  So they said:  



"The present document addresses security considerations specific to constrained devices.  For example, window contact sensors, flood sensors, and energy switches are typically constrained devices."



So to give everyone a sense for how well thought-out and specific this is, here's the document's definition of what it means by a "constrained device."  They said:  "A constrained device is a device which has physical limitations in either the ability to process data, the ability to communicate data, the ability to store data, or the ability to interact with the user, due to restrictions that arise from its intended use."  They said:  "Note:  Physical limitations can be due to power supply, battery life, processing power, physical access, limited functionality, limited memory, or limited network bandwidth.  These limitations can require a constrained device to be supported by another device, such as a base station or companion device."



And they give some examples:  "A window sensor's battery cannot be charged or changed by the user; this is a constrained device."  Another example:  "The device cannot have its software updated due to storage limitations, resulting in hardware replacement or network isolation being the only options to manage a security vulnerability."  And that ends up being important because this astonishing baseline requirement also deals with firmware updating.  And yes, it has to be enabled by default.  This is just like, where did this come from?



Third example:  "A low-powered device uses a battery to enable it to be deployed in a range of locations.  Performing high-power cryptographic operations would quickly reduce the battery life, so it relies on a base station or hub to perform its validations of updates.  Another example, device has no display screen to validate binding codes for Bluetooth pairing.  Or five, the device has no ability to input, such as via a keyboard, any sorts of authentication information."  So obviously, where things are either impossible or impractical for these requirements to be met, well, they're excused under the grounds that it is a constrained device.  But otherwise no free pass.



Okay.  So the document's too long and detailed for me to go through in detail here; but I do want to, again, give everyone a sense for how well thought out, thorough, and serious this is.  So I want to look deeply at this "No universal default passwords."  And Leo, let's take our second break, and then I will do that.



LEO:  I'm sorry, I'm talking to Mom.  You want to say hi?  You want to say hi to Mom?  Say hi to Steve.



STEVE:  Hey, Mom.



LEO:  I'm going to run.  Go to dinner; okay?  I love you, Mama.  Bye.



STEVE:  That's right, it's 5:00 o'clock there.



LEO:  It's 5:00 o'clock there.  And you know what happens is she's got an alarm on her Amazon Echo to wake her up.  So she called me, and then it starts going off, and I had to explain to her how to go over to it and turn it off.  So I guess what she normally will do is leave, go to dinner, and by the time she gets back the alarm has stopped because I don't think she knew how to stop it.



STEVE:  Ah, these fancy consumer devices.



LEO:  She loves it.  You know, she wanted to thank me because I gave her - and I hope that this has a good password on it.  I gave her a - there's a company called Nixplay makes a 15-inch frame, it looks like a painting frame, but it's a digital photo frame.  And I can email pictures to it.  I control it from here.  So, and she just sits and looks at it all day.



STEVE:  Nice.  Aww.



LEO:  So it just makes her really happy.  She says it's like having my family here.  So anyway.  We will continue in a moment.  Sorry for that personal interlude.



STEVE:  Yeah, no problem.  And, boy, May 7th is going to be a busy day.



LEO:  It is.  The Apple event is May 7th.  RSA conference is going on.  You should come up sometime for that.



STEVE:  I have a large inventory of very aging and slowing down iPads.



LEO:  Oh, iPads.  You're an iPad user, I know.



STEVE:  So I've not - I love my iPads, and I've not purchased any for many years because there's, you know, no new iPads.



LEO:  There's no reason to.



STEVE:  Right.



LEO:  And I think that this time they're going to - they know this, by the way.  I mean, I've been using - I have an iPad 6 which I've had for three years.  I very rarely use my iPad Pro.  They've got to get a way to get...



STEVE:  I didn't even know they had numbers.



LEO:  Oh, well, they don't.  You have to know.  You have to know.



STEVE:  Oh.  I think I have a - I know I don't have the iPad 1.  Remember how that was like - it was like a whale?



LEO:  Oh, yeah.



STEVE:  It had a weird bowed back.



LEO:  Oh, yeah.  You don't want that, yeah.  No, we've come a long way.  But you will want these because they have OLED screens.  And what's intriguing is this rumor you're going to have the M4 and a lot of AI built in.  So this might be something you really do want, I think.



STEVE:  Well, and I read on a black screen with amber type, so that would be...



LEO:  Perfect for you.



STEVE:  ...great for low power consumption.



LEO:  I just ordered - there's a new Kobo reader, the Libra 2 Color, that uses color eInk.  And I'm very intrigued by that.  And because it will have some color.  It won't have amber on black, probably, like you just said.



STEVE:  All the color I've seen has been on...



LEO:  So it's washed out, yeah, yeah.



STEVE:  Yes, exactly, very low contrast.



LEO:  Well, we'll see.  It also has a stylus, and you can take notes and stuff.  So, you know...



STEVE:  I do love my ReMarkable.  You turned me onto that.



LEO:  Isn't that great?  Yeah.



STEVE:  That is the best thing.  It really is...



LEO:  Yeah, yeah, yeah.  It's great for coding because I use it all the time to sketch out, like, problems and what I'm [crosstalk].



STEVE:  Oh, that's exactly what I do.  I'm a big diagram drawer.



LEO:  Exactly, yeah.



STEVE:  When I'm, like, trying to parse something.  It's like, wait a minute, because that's the only way you can deal with the off-by-one problems is do little examples of it.



LEO:  Yup, see it.  Yup, I'm the same way.



STEVE:  Okay.  So as I said, I'm not going to go through the entire document.  It is too long and wonderful.  I mean, I already sound like I'm over-caffeinated at this point because I'm so excited by what is in here.  It's just astonishing.  I mean, again, it's like you took 20 years of this podcast and distilled all of its recommendations that we've come up with on the fly, when things have been obvious and when we've seen something go wrong over and over and over, it's like, okay, when are we finally going to do this?  And here it is.  And they just didn't miss anything.



LEO:  That's great.  That's fantastic.  Wow.



STEVE:  It's just, you know.  And again, it's not like - again, it's not recommendations, it's law.



LEO:  Yeah.



STEVE:  In the UK.  Okay.  So I do want to, like, do a little bit of a deeper look into the first one they address because it is crucially important, under the banner "No Universal Default Passwords."  They say:  "Where passwords are used, all consumer IoT device passwords shall be unique per device or defined by the user.  There are many mechanisms used for performing authentication, and passwords are not the only mechanism for authenticating a user to a device.  However, if they are used, following best practice on passwords is encouraged."  By which they mean, again, they have not softened this as a recommendation.  They're saying, you know, encourage people when they use a password to use a good one.



They said:  "Many consumer IoT devices are sold with universal default usernames and passwords such as admin/admin for user interfaces through to network protocols.  Continued usage of universal default values has been the source of many security issues in IoT, and the practice needs to be discontinued.  The above provision can be achieved by the use of pre-installed passwords that are unique per device and/or by requiring the user to choose a password that follows best practice as part of initialization, or by some other method that does not use passwords.  For example, during initialization a device generates certificates that are used to authenticate a user to the device via an associated service like a mobile application.



"To increase security, multifactor authentication, such as use of a password plus OTP procedure, can be used to better protect the device or an associated service.  Device security can further be strengthened by having unique and immutable identities.  Where pre-installed unique-per-device passwords are used, these shall be generated with a mechanism that reduces the risk of automated attacks against a class or type of device.  Pre-installed passwords must be sufficiently randomized.  Passwords with incremental counters such as 'password1,' 'password2' and so on are easily guessable.  Further, using a password that is related in an obvious way to public information sent over the air or within a network, such as MAC address or WiFi SSID, can allow for password retrieval using automated means.



"Authentication mechanisms used to authenticate users against a device shall use best practice cryptography, appropriate to the properties of the technology, risk, and usage."  And I should mention that also in here is a complete description of their use of all the terminology.  So here they just said "best practice cryptography."  But in an addendum they specifically outline what that means that is required to be used where they use this term.  So again, there's nothing that they missed.  They said:  "Where a user can authenticate against a device, the device shall provide to the user or an administrator a simple mechanism to change the authentication value used.



"When the device is not a constrained device, it shall have a mechanism available which makes brute-force attacks on authentication mechanisms via network interfaces impractical. For example, a device has a limitation on the number of authentication attempts within a certain time interval.  It also uses increasing time intervals between attempts.  Or the client application is able to lock an account or to delay additional authentication attempts after a limited number of failed authentication attempts.  This provision addresses attacks that perform 'credential stuffing'" - it actually says that in this document for consumer IoT devices - "or exhaust an entire key space.  It is important that these types of attacks are detected by the consumer IoT device and defended against, whilst guarding against a related threat of 'resource exhaustion' and denial of service attacks."



Incredible.  What I just summarized is broken into five individual provisions in the document, but each and every one of them is tagged as "Mandatory."  So, for example, if a device offers password-based authentication, it can no longer be shipped from the factory with a default password, and the device must also incorporate proactive defenses against brute force and credential stuffing attacks.  It must incorporate some form of lock-out mechanism.  This legislation changes everything.  It takes the well-understood, but still not often implemented, best practice, you know, even not yet implemented from, you know, high-end enterprise level devices, and mandates its use today for a residential doorbell.  This is huge.



Section 5.3 is titled "Keep software updated."  And picking some bits from it, for example, it says:  "Developing and deploying security updates in a timely manner is one of the most important actions a manufacturer can take to protect its customers and the wider technical ecosystem.  It is good practice that all software is kept updated and well maintained.  All software components in consumer IoT devices should be securely updateable.  When the device is not a constrained device, it shall have an update mechanism for the secure installation of updates."  They have in quotes:  "'Securely updateable' and also in quotes 'secure installation' means that there are adequate measures to prevent an attacker misusing the update mechanism.



"Measures can include the use of authentic software update servers, integrity protected communications channels, verifying the authenticity and integrity of software updates.  It is recognized that there are great varieties in software update mechanisms and what constitutes 'installation.'  An anti-rollback policy based on version checking can be used to prevent downgrade attacks.  Update mechanisms can range from the device downloading the update directly from a remote server, transmitted from a mobile application, or transferred over a USB or other physical interface.  If an attacker compromises this mechanism, it allows for a malicious version of the software to be installed on the device."  Meaning that there are provisions for preventing that happening.  Thus they're explaining what the danger is.



"An update shall be simple for the user to apply.  The degree of simplicity depends on the design and intended usage of the device.  An update that is simple to apply will be automatically applied, initiated using an associated service such as a mobile application, or via a web interface on the device.  If an update is difficult to apply, then that increases the chance that a user will repeatedly defer updating the device, thus leaving it in a vulnerable state.  Automatic mechanisms should be used for software updates.  If an automatic update fails, then a user can, in some circumstances, no longer be able to use a device.  Detection mechanisms such as watchdogs and the use of dual-bank flash or recovery partitions can ensure that the device returns to either a known good version or the factory state.



"Security updates can be provided for devices in a preventative manner, as part of automatic updates, which can remove security vulnerabilities before they are exploited.  Managing this can be complex, especially if there are parallel associated service updates, device updates, and other service updates to deal with. Therefore, a clear management and deployment plan is beneficial to the manufacturer, as is transparency to consumers about the current state of update support.  In many cases, publishing software updates involves multiple dependencies on other organizations such as manufacturers that produce sub-components; however, this is not a reason to withhold updates.  It can be useful for the manufacturer to consider the entire software supply chain in the development and deployment of security updates.



"It is often advisable not to bundle security updates with more complex software updates, such as feature updates.  A feature update that introduces new functionality can trigger additional requirements and delay delivery of the update to devices.  The device should check after initialization, and then periodically, whether security updates are available."  Again, the device should check.  They said:  "If the device supports automatic updates and/or update notifications, these should be enabled in the initialized state and configurable so that the user can enable, disable, or postpone installation of security updates and/or update notifications."



They said:  "It is important from a consumer rights and ownership perspective that the user is in control of whether or not they receive updates.  There are good reasons why a user may choose not to update, including security.  In addition, if an update is deployed and subsequently found to cause issues, manufacturers can ask users to not upgrade their software in order that those devices are not affected."  But they're saying, you know, again, secure by default.  Secure by policy.  If it can update itself, it should.  But recognize that there are reasons that it might not.



They said:  "The device shall use best practice cryptography to facilitate secure update mechanisms.  Security updates shall be timely.  'Timely' in the context of security updates can vary, depending on the particular issue and fix, as well as other factors such as the ability to reach a device or constrained device considerations.  It is important that a security update that fixes a critical vulnerability. i.e., one with potentially adverse effects of a large scale, is handled with appropriate priority by the manufacturer.  Due to the complex structure of modern software and the ubiquity of communication platforms, multiple stakeholders can be involved in a security update."



Okay.  So what we're talking here - and I just shared the tip of the iceberg.  But it is all like that.



LEO:  I have this vision of you last night or the night before, sitting by the fire, your feet up, you've got a little something to drink, you're reading through this.  Your beautiful wife is across the way sitting reading her magazine.  And every five minutes you go, "Wow."  Or, "Yes."  I imagine every step of the way because this is all the stuff you've been saying all this time.



STEVE:  I know.  It's astonishing, Leo.  I mean, it is.  They even have in there protocols which are used on the WAN should not be exposed - or on the LAN should never be exposed to the WAN.



LEO:  They came this close to saying "three dumb routers"; right?  I mean, it's impressive.  I am very impressed.



STEVE:  It is astonishing that this thing exists.



LEO:  Mm-hmm.



STEVE:  So again, grc.sc/etsi, E-T-S-I.  That will bring this 34 or whatever number of page things it was to you, and it is all like that.  What we're talking about here amounts to nothing less than the forced and immediate maturation of more than a decade of lazy consumer product security design.



LEO:  Yup, yup, yup.



STEVE:  End-user security is finally being prioritized over device development and support costs and, yes, even convenience.  It had to happen sometime, and all indication is that sooner or later it was going to need to be forced.  That happened yesterday.



LEO:  Wow.  And it's law; right?



STEVE:  It's law.



LEO:  I mean, in the UK this is the law.  Wow.



STEVE:  It is the law.



LEO:  That's amazing.



STEVE:  Yes.  And a law with teeth in it.  It's not like, you know, oh, you pay a $2,500 penalty.  No.  It's 10 million, or 4% of your revenue, whichever is greater. 



LEO:  It's significant, yeah.



STEVE:  Yeah.  So, I mean, basically, like, everything on the shelf that isn't already in compliance.  And we'll note that many routers, many consumer routers now are; right?  Although I don't think they've gotten - I think they're still, you know, like the default username and password still starts off with admin and password or something.  So, but they are updating themselves, and they do have that on by default.  So, you know, that's good.



But, you know, one of the reasons, in fact there was a reference somewhere to this crazy, huge, like the Mirai botnet that owe their existence to the fact that these policies have never been required, enforced, or present before.  And as a consequence of web authentication being on the WAN, and default usernames and passwords, these things could just be taken over.  And so the UK is finally saying, enough of this.  What?  You know, come on.  You guys haven't gotten your act together in the last decade?  Well, we're going to require it now.  If you want to sell stuff in the UK, you have to do this.



And it will probably increase cost a little.  But as we know, once this is done, it's in a chip.  You just stamp it out.  And so, yeah, users are going to have to learn that they're going to have to, you know, like read the directions instead of assuming it's admin/admin or just has no password.  And like, oh, well, I'll give it one later, and then never get around to it.  Instead, every device will have a unique password which they'll have to write down or change to make it one that they want.  But it won't be, you know, off the shelf, all of them sitting there on the shelf with the same password to start.  And again, that's just, like, one of 40 different topics that they've dealt with.



LEO:  Amazing.  Amazing.  It's amazing.



STEVE:  It's astonishing.



LEO:  Yeah, yeah.



STEVE:  Okay.  So while we were recording last week's podcast, the news dropped, thanks to you, Leo, covering it, that Google's plans for Chrome's full phaseout of third-party cookies would not be occurring this year as had been planned and, needless to say, much anticipated.  For this week's podcast I had hoped to follow up on that news to learn and then report on what was going on.  The beginning of my research into Google's interactions with the UK's CMA, their Competition and Markets Authority, suggested that everything is actually going quite well overall, only somewhat slower than was expected.  My preliminary examination suggested that the UK's concerns are actually more along the lines of whether Google's new system goes far enough.



And actually, having just covered what we covered, I'm not that surprised at this point because they really seem to be getting busy.  Where concerns had previously been raised, for example, that smaller advertisers may be disadvantaged, the UK now appears to be discounting those concerns and complaints.  But I ran out of time for this research because, Leo, in front of the fireplace as I was last evening, my glass of Cabernet got empty, and I thought, okay, well, we'll tackle this next week.  We ran out of space.  So I will have the news of this next week because there was lots of material, and it's possible to get a much better understanding of what's going on.



LEO:  You need an overflow podcast.  That's what you need.



STEVE:  That's right.



LEO:  No one would object if you decided to do one, that's for sure.



STEVE:  Okay.  So a bit of Closing the Loop, just two pieces of Closing the Loop feedback.  Guillermo Garca said:  "Hi, Steve.  Listening to the feedback on the counter race condition," which our listeners had a lot of fun with, he said:  "I wonder what would happen to a process that wants to increase the counter if the previous owner of the counter was switched out of context and did not return the ownership before being switched off.  Would this active process get stuck and have to wait for the previous one to regain context and return it?  Again, many thanks."



Okay.  Many of our listeners reported that they found the discussion of object ownership within a multithreaded environment very interesting.  But at no point did I talk about what happens when something goes wrong.  Right?  I just talked about if everybody does the right thing and behaves themselves, this is how cool it is when everything is perfect.  But, for example, notice that nothing actually prevents the shared counter from being incremented by a thread that does not first acquire ownership of the object.  In this instance, in the example that I painted, there is no enforcement.  It's all and only by agreement among the process's threads.  And since they're all part of the same process, it's in their best interest to abide by the rules mutually.



But it doesn't take a deliberate act to mess something up.  Bugs happen, as we know.  A typical bug in a complex multithreaded environment is that, for example, a thread will acquire ownership, then follow some code path that causes it to fail to release its ownership.  At that point, that counter can never be incremented again, and any and all threads that need to may stall, waiting for an object's ownership to be released.  And who among us, especially back in the early days of Windows, has not experienced an application lock up and freeze?  Or its menuing UI mysteriously stops responding.  Or the app apparently dies and refuses to do anything, although it's still there on the screen and looks like it should be going, but it's not.  Some things might still be functioning, whereas other things suddenly become non-responsive.



One of the most common causes of such things happening is that somehow the ownership of a shared object was not freed by its owner.  Threads can sometimes get into trouble.  If a thread, for example, attempts to divide by zero, that thread will be terminated by the operating system.  Or if a thread mistakenly attempts to execute some data, an illegal instruction can be encountered and, again, the thread will be immediately killed.  In any event, if that thread happened to own some shared objects at the time of its termination, they would likely not be freed.  And other threads, or even a re-spawn of the terminated thread, might then be unable to succeed ever again after that.  Shutting down and restarting the application might be the only way to clear out such stuck ownership.



Not surprisingly, many solutions for these sorts of problems have been created over time.  One of the best things about software is if you've got a problem, there's probably a way to fix it, which is what makes it so fun.  Not surprisingly, people have been very clever.  For example, there's a system known as "Structured Exception Handling," or SEH for short, which actually allows a thread to protect the system and itself from its own possible misdeeds.  I've implemented Structured Exception Handling in assembler, and I've used it when my code had no choice, for some reason, other than to try to do something that might fail catastrophically.  And what doing this allowed me was then to try something and not lose control, but to have that failure recoverable, and then I could deal with the consequences.  And there are also entirely different ways to manage shared object ownership than that simple exchange instruction which I chose specifically to demonstrate the simplest of all possible solutions, which it is.



But before I close out this conversation, I would be remiss if I did not mention one of the classic problems with multithreaded environments which is known as the "deadlock."  A deadlock can be created when two threads each separately own an object, but also need ownership of another object that the other owns.  In other words, say that there are two objects, both of which need to be simultaneously owned by a thread in order to complete some work.  One thread currently owns the first object, and the second thread currently owns the second object.  And each of them needs to obtain ownership of the object that the other one already has.



Both threads will patiently wait for something that will never occur, since neither will relinquish its ownership of the object it owns until it, however briefly, is able to obtain ownership of the object it needs.  Which the other thread owns, and it's also waiting for the object the first thread has.  So consequently, neither will ever succeed, and we have a classic deadlock, as it's known in computer science.



But Guillermo's question highlights something else that I did not talk about.  He talked about multiple processes sharing objects, in other words, inter-process object sharing; whereas all of my discussion has been about multiple threads within a single process, intra-process object sharing.  It is possible to share objects between processes.  For example, Windows allows this by using unique names to identify objects.  Then separate processes that know the common name for an object can open the object to obtain its handle, very much like opening a file by name, after which operating system calls can be used to check the shared object's status, to obtain and release ownership of it, and so on.



And it's also possible to set timeouts while waiting for an object's ownership to be granted.  If that amount of time passes, the object wait will be ended, and the waiting process will be notified that the object never became available during the amount of time that the thread said it was willing to wait for it.



And even returning to our original example with the exchange instruction, remember that a thread that wants to obtain ownership attempts to obtain it, and the result of the exchange instruction informs it whether or not it was successful.  If it was not successful, it's fully able to decide what to do next.  Right?  I mean, it's not - it doesn't have to wait forever.  It can go do other things and try again later.  Or it might ask the operating system to put it to sleep for some length of time.  That's an extremely friendly thing to do since the thread is voluntarily giving up the rest of its running time slice, which allows the OS to schedule other threads.  Then when the thread is reawakened by the operating system, it can again attempt to obtain ownership, and then decide what to do.



Anyway, I know I'm weird.  All this fascinates me.  And I have never encountered, as I said before, anything as pure and clean and gratifyingly complex as coding.  I get it.  It's not for everyone.  But if it is, it can be terrifically rewarding.  And I know, Leo, that you also love to code.



LEO:  Oh, I love it.  I live for it.



STEVE:  One last tiny bit of news.  Apparently The New York Times last week picked up on the story of, you know, we were talking about the LexisNexis selling drivers' driving habits.  Turns out that The New York Times had a story on Wednesday that  General Motors had "accidentally," says GM, enrolled millions of people into its "OnStar Smart Driver+" program.  Consequently, if consumers chose not to enroll through the phone app, it would do it anyway.  Unenrolling requires consumers to contact OnStar customer support line.  However, turns out some people do not trust them and have started stripping the electronic devices out of their cars.  So reports The New York Times.



Anyway, just a little bit of follow-up on that.  Mistakes had happened, and people, you know, we showed that detailed report last week, and I've seen several others since then, all looking identical because it's coming from the same company.  So, Leo, let's take our last break, or no, our second-to-last break.  I want to again update our listeners on a bit of sci-fi and where I am in my work.  And then we're going to get into our main topic.



LEO:  You bet.  By the way, the book club loved the Bobiverse.  So much so, a lot of them are now on to Book 2.  Anthony Nielson, who read Book 1 for the book club - it was Stacey's book club.  Stacey was a little reluctant.  She wasn't crazy about it because she didn't like Bob.  Which is, you know, if you don't like Bob, there's a lot of Bob in the Bobiverse.  But Anthony Nielson said, oh, yeah, Book 2 kind of eased his concerns about Book 1.  I'm going to have to reread the whole thing because September Book 5 is coming out.  Very exciting.  The fifth book of the Bobiverse.



STEVE:  Wait, there's going to be a fourth Bobiverse book?



LEO:  Yes.  There are four.  There's going to be another one.



STEVE:  You mean a fifth one; right.



LEO:  Yeah, it's amazing.



STEVE:  Cool.



LEO:  Anyway, so that was your recommendation.  Thank you, and I look forward to hearing more.



STEVE:  Well, and it came through our listeners.  It was from our listeners.



LEO:  Yes, I remember that, yeah.



STEVE:  Who kept saying to me, Steve, you know, check it out.  And I have to say that my taste often differs from our listeners.



LEO:  It's a little lightweight.  It's lightweight.  It's fun.  It's not...



STEVE:  Yeah.  And actually what I'll be recommending in a minute is also.



LEO:  Lightweight?



STEVE:  Yeah.



LEO:  Okay.  I'm reading "Hyperion" right now, which is the opposite of lightweight, one of the classic science fiction novels that I never got around to reading, so I'm enjoying it quite a bit.



STEVE:  Yeah, I think I have the paperback around here.



LEO:  Yeah, I mean, it's a classic; right?  I mean, that I've never read it is amazing.  Now back we go to Mr. G. and sci-fi time.



STEVE:  Yes, on the science fiction reading front I wanted to mention to our many listeners who've been enjoying Ryk Brown's ongoing Frontiers Saga, that book 11 of 15 in his third of five 15-book story arcs, became available yesterday in the Amazon U.S. store.  I received Ryk's announcement that his latest novel titled "The First Ranger" is now available for download in the U.S., although apparently international availability may lag a bit, as is apparently common.



I've received so much feedback through the years from our listeners who've enjoyed following this adventure - and it is one long adventure, now at 41 full-length novels - that I wanted to make sure everyone knew that book 11 was here now.  I told some family members and friends, and they just jumped up and down because there's just - if it's right for you, then it is really right for you.  It's primarily character driven.  He offers us very fully formed individuals with very distinct and at times annoying personalities.  And in a way it's a bit like Star Trek, where it's less about whiz-bang science fictional technologies than about how the various characters whom we've come to know over time, how they deal with what comes their way.  I find it very satisfying.  And in addition to many in this podcast audience, as I said, you know, I've turned friends and family members onto it, and they're completely hooked.



For those who have never looked at the series, as I mentioned, Amazon Kindle is where it is.  It's part of the Kindle Unlimited plan.  And the novels, if you just buy - if you're not a member of Kindle Unlimited, they're not very expensive if you just purchase them outright.  So here's what I know.  Anybody starting the first book will know within an hour whether they have just started into a journey that already has 41 additional books waiting for them.  And they are just as compelling as the first one.  So it's, you know, sci-fi is a passion of mine.  We've talked about it, Leo, you and I, through the, you know, we're in our 20th year of the podcast now.



LEO:  Oh, my god.  And your skin is still beautiful, by the way.



STEVE:  Many authors, many adventures, a lot of fun.



LEO:  Yes, yes.  I'm referring to something that happened before the show.  Don't worry, folks.  You didn't miss anything.



STEVE:  Okay.  And finally, on my own work front, last week I finished updating various GRC pages with the news that 6.1 was now available.  This is not 6.1's documentation, which is still quite sorely needed.  This is just enough to hold us over until I have email communication up and running, after which my plan is to plow into SpinRite's extreme need for documentation.  I cringe whenever someone asks a question that really should be there on the website, documented.  But I'm getting there as soon as I can.



But I have a method here.  And getting email up and having our podcast audience help me develop an email presence and a reputation is, you know, part of what it takes these days because, you know, as we know, spam is such a problem that the large receivers of email have gotten very picky about the bounce rates and spam flaggings and so forth.  So anyway, I want to get that working.  And then while that's happening I will be able to start working on the documentation.  So anyway, I'm working on email, and I know how many of our listeners are excited that Twitter will not be the only way to get a hold of me.



So our main topic.  Actually, after that first topic, you can see why it was competition for this one.  So we have two big ones.  Today's podcast title, "Passkeys:  A Shattered Dream?"  It gets its title from a blog posting from last Friday.  It was a thoughtful posting by a guy named William Brown, who is the author of a popular WebAuthn package for Rust.  In fact, it's pretty much the WebAuthn package for Rust.  It generated significant attention within the security community, and a little bit later within our own listener community because, after I had already chosen it as our topic, everybody started  tweeting me links saying, oh, my goodness, what do you think about this?  So his WebAuthn package is "webauthn-rs," which describes itself as "WebAuthn Framework for Rust Web Servers."



To remind everyone how and where WebAuthn fits within the overall Passkeys solution, it's the protocol and specification that a Passkeys client on the user's side uses to communicate with a web server that supports WebAuthn.  So, for example, just as a web server will offer some form of username and password login, possibly with additional factors such as time-based one-time passwords or something else, such a server might also offer support for the WebAuthn protocol as a means for allowing remote clients to identify and authenticate their identity over a network.  In the case of this author's Rust implementation of WebAuthn, he described WebAuthn by writing:



"WebAuthn is a modern approach to hardware-based authentication" - notice he says hardware-based authentication - "consisting of a user with an authenticator device, a browser or client that interacts with the device, and a server that is able to generate challenges and verify the authenticator's validity.  Users are able to enroll their own tokens through a registration process to be associated to their accounts, and then are able to log in using the token which performs as a cryptographic authentication.  This library" - meaning his that he wrote - "aims to provide useful functions and frameworks allowing you to integrate WebAuthn into Rust web servers.  This means the library implements the Relying Party component of the WebAuthn/FIDO2 workflow.  We provide template and example Javascript and web asm bindings to demonstrate the browser interactions required."



Okay, now, the only thing I'll note about what this author wrote, as I pointed out right at the start, and it might be significant, is that this appears to have first been written back in the earlier FIDO1 era when hardware dongles were the only way the FIDO group was willing to roll.  As we know, the requirement for purchasing a piece of hardware, while potentially ensuring greater security, was finally accepted to be a bar too high.  So the FIDO group basically capitulated to allow software-only solutions the privilege of authenticating with what evolved into FIDO2.



So my point is that the author of this WebAuthn crypto library for Rust appears to have started this back at the hardware-only dongle stage of FIDO1, and he simply changed "FIDO1" to "FIDO2" in his introduction.  This may be significant for what he subsequently wrote and published last Friday, since the introduction of FIDO2, with its accompanying Passkeys, promised to make his work far more relevant.



Before I share what he wrote Friday, I wanted to note that the section following that brief introduction, I really loved it, it was titled "Blockchain Support Policy."  Okay, now, this is for, right, a WebAuthn package that has nothing to do with blockchain.



LEO:  I'm thinking it's going to say "none," but okay, yeah.



STEVE:  Uh-huh, exactly.  So he said:  "Blockchain Support Policy."  And he wrote:  "This project does not and will not support any blockchain-related use cases.  We will not accept issues from organizations, or employees thereof, whose primary business is blockchain, cryptocurrency, NFTs, or so-called 'Web 3.0 technology.'"



LEO:  Right on, right on, right on.



STEVE:  Period.



LEO:  Period.  End of statement.  Yeah.



STEVE:  And of course, you know, we know why he said that; right?  There's been so much nonsense surrounding, you know, the "Blockchain will solve all of society's ills" nonsense, and especially within the identity authentication space, that it's easy to imagine how much of that this guy may have been fending off through the years.  Elsewhere he notes that his library has passed a security audit performed by SUSE Linux's product security, and that other security reviews are welcome.



And as a total aside, I thought it was also interesting that on the topic of compatibility he wrote, under "Known Supported Keys/Hardware," he said:  "We have extensively tested a variety of keys and devices, not limited to Yubico 5c, 5ci, FIPS, and Bio; Touch ID, Face ID, meaning iPhone, iPad, MacBook Pro; Android; Windows Hello with TPM; and soft tokens."  And then he said, and under "Known BROKEN Keys/Hardware," he notes:  "Pixel 3a, Pixel 4 + Chrome does not send correct attestation certificates, and ignores requested algorithms."  And he said:  "Not resolved."  And "Windows Hello with Older TPMs," he said, "Often use RSA-SHA1 signatures over attestation, which may allow credential compromise or falsification."



Okay.  So Friday he gave his blog posting the title, as I said, that I reused for today's podcast, although, well, his was "Passkeys: A Shattered Dream," although I added the question mark.  His posting was not a rhetorical question.  His was meant as a statement.  So before I share what William has written, I wanted to take a moment to note that in order to do justice to his choice of words, I'm going to again need to use a term on this podcast that makes me uncomfortable.



LEO:  Uh-oh.



STEVE:  Although the fact that the term was the American Dialect Society's Word of the Year for 2023 suggests that it's a term we're all destined to be encountering more and more often.  That term is "enshittification."



LEO:  Oh, yeah.  Oh, yeah.



STEVE:  The American Dialect Society's Word of the Year last year, Leo.



LEO:  Yeah.



STEVE:  So I am somewhat eased about its usage due to its lineage and the fact that Wikipedia does not shy away from devoting a rather extensive page to its definition, its description, and its discussion with extensive examples of this happening.  Wikipedia describes "enshittification" as "the pattern of intentional decreasing quality observed in online services and products such as Amazon, Facebook, Google Search, Twitter, Bandcamp, Reddit, Uber, and Unity.  The term," they write, "was used by writer Cory Doctorow in November 2022, and the American Dialect Society selected it as its 2023 Word of the Year.  Doctorow has also used the term 'platform decay' to describe the same concept."



And, for what it's worth, you know, allow me to commend this Wikipedia page to our listeners.  I found it to be somewhat gratifying and affirming because, while I was reading and agreeing with everything it said, I felt a little bit less like the crotchety old-timer yelling at the kids to get off the lawn.  You know?  In other words, objectively and, sadly, deliberately, some things actually are getting worse.  It's not just that you and I, Leo, are getting older, and everything seems worse.



And also, not far into this, he refers to - "he" meaning, sorry, done with Wikipedia, "he" meaning William Brown, the author of this - refers to a system whose name is spelled "Kanidm," a term I had never encountered before.  Its home page explicitly explains it's pronounced "kar-nee-dee-em."  Even though there's no "R" anywhere to be seen.  I mean, I guess it's, I would say, "ka-nee-dee-um," maybe.  But, you know, "kar-nee-dee-em," which of course reminded me of "carpe diem."  And I don't know, maybe it's - I don't know where the name came from.



LEO:  It's intentional, I'm sure, yeah.



STEVE:  Yeah.



LEO:  Sade is pronounced "shar-day."  I think it's a British-ism.  The "R" is inserted, yeah.



STEVE:  Oh, that's interesting, you're right, it is.



LEO:  Yeah.



STEVE:  Yeah.  Okay.  Kanidm.  So it is a sprawling open source identity management platform developed in Rust by the SUSE Linux project.



LEO:  I've been looking for something like this.



STEVE:  Well, there it is.



LEO:  Okay.



STEVE:  It appears that William's WebAuthn library was adopted into that multi-faceted identify project to provide its WebAuthn functionality.  So when we hear him refer to, in his blog posting, "kar-nee-dee-um," he's refer to his library's significant participation in that project.  And that explains also why SUSE's security people have reviewed and approved of his library, because it's the one they chose for their big identity management platform.



Okay.  So he said:  "At around" - you know, Leo, let's take our last break before I get into this.



LEO:  Yeah, yeah.  Keep it a secret what he says.



STEVE:  Then I won't have to break in the middle of it.



LEO:  Right, right.  This is good.  I'm enjoying it.  And, yeah, we've all decided here that you can say "enshittification" because it's just a word with a bad word in the middle, but not in the beginning or the end.



STEVE:  Well, and encrappification...



LEO:  Doesn't have the same...



STEVE:  ...doesn't have quite the same ring to it.



LEO:  Yeah.  We've been saying "enshirtification," just like Kanidm.  You're adding a phantom "R."  I don't know.  That's confusing.



STEVE:  But does that mean you have to wear more shirts?



LEO:  Need to wear more shirts.



STEVE:  Yeah.



LEO:  Okay, now, back to the enshittification.



STEVE:  Of Passkeys.



LEO:  Of Passkeys.



STEVE:  Okay.  So the author of this WebAuthn library, well regarded, built into SUSE's Linux identity platform, written in Rust, lots of experience from back in the dongle days, the early FIDO1 days, he wrote last Friday:  "At around 11:00 p.m. last night, my partner went to change our lounge room lights with our home light control system.  When she tried to log in, her account could not be accessed.  Her Apple Keychain had deleted the Passkey she was using on that site.  This is just the icing on a long trail of enshittification that has undermined WebAuthn.  I'm over it at this point, and I think it's time to pour one out for Passkeys.  The irony is not lost on me that I'm about to release a new major version of webauthn-rs today as I write this.



"In 2019 I flew to my mate's place in Sydney and spent a week starting to write what is now the WebAuthn library for Rust.  In that time I found a number of issues in the standard and contributed improvements to the WebAuthn working group, even though it took a few years for those issues to be resolved.  I started to review spec changes and participate more in discussions.  At the time there was a lot of optimism that this technology could be the end of passwords.  You had three major use cases:  second factor, passwordless, and usernameless.  Second factor was a stepping stone toward the latter two.  Passwordless was where you would still type in an account name, then authenticate with PIN and touch your security key.  And usernameless was where the identity of your account was resident and thus discoverable on the key.  This was, from my view, seen as a niche concept by developers since, really, how hard is it for a site to have a checkbox that says 'remember me'?



"This library ended up with Kanidm being, to my knowledge, the very first open source identity management platform to implement 'passwordless,' which is now Passkeys.  The user experience was wonderful.  You went to Kanidm, typed in your username, and then were prompted to type your PIN and touch your key.  Simple, fast, easy.  For devices like your iPhone or Android, you would do similar, just touch your Touch ID and you're in.  It was so easy, so accessible.  I remember how it almost felt impossible that authentication could be cryptographic in nature, but so usable and trivial for consumers.  There really was the idea and goal within FIDO and WebAuthn that this could be 'the end of passwords.'



"This is what motivated me to continue to improve webauthn-rs.  Its reach has gone beyond what I expected, with parts of it being used in Firefox's authenticator-rs, a whole microcosm of Rust Identity Providers being created from this library and my work, and even other languages' WebAuthn implementations and password managers using our library as the reference implementation to test against.  I cannot understate how humbled I am by the influence webauthn-rs has had.



"However, warnings started to appear that the standard, the WebAuthn standard, was not as open as people envisioned.  The issue we have is well known:  Chrome controls a huge portion of the browser market, and development is tightly controlled by Google.  An example of the effect was the 'Authenticator Selection Extension' of the WebAuthn specification.  This specification extension is important for sites that have strict security requirements" - like, you know, the government - "because the extension supports the attestation of the make and model of the authenticator in use.  If you know that the website's attestation will only accept certain devices, then the browser should filter out and only allow those acceptable devices to participate."



So, like, just to pause here for a second, that would be so cool; right?  If your bank, for example, required more than just a browser-based Passkey because it is pure software, but needed a hardware dongle, or needed a biometric, you know, reaffirmation of your identity when you tell it that you want to transfer some amount of money somewhere, then you absolutely want this protocol to be able to specify the type of authentication device that would be used and for the browser to then prompt for that level of authentication.



Anyway, he says:  "However, Chrome never implemented it.  That alone led to the entire feature being removed from the spec.  It was removed because Chrome never implemented it.  This demonstrates that if Chrome doesn't like something in the specification, they can just veto it without consequence.  Later, the justification for this not being implemented was:  We never implemented it because we don't feel that authenticator discrimination is broadly a good thing.  They, users, should have the expectation that a given security key will broadly work where they want to use it."  He says:  "I want you to remember this quote and its implications:  Users should be able to use any device they choose without penalty."



He says:  "Now, I certainly agree with this notion for general sites on the Internet; but within a business where we have a policy around what devices may be acceptable, the ability to filter devices does matter."  So he says:  "This makes it possible to go to a corporate site and apparently successfully enroll a security key, only to then have it fail to register.  Even better if this burns up" - you know, consumes - "one of your limited resident key slots which cannot be deleted without a full reset of your device.  This might happen since the identity provider rejected the device's attestation."  And he says:  "That's right.  Even without this, identity providers can still discriminate against devices without this extension; but the user experience is much worse, and the consequences far more severe in some cases."



He says:  "The kicker is that Chrome has internal feature flags that they can use for Google's needs.  They can simply enable their own magic features that control authenticator models for their policy, while everyone else has to have a lesser experience.  The greater warning here is that many of these decisions are made at F2F," as he puts it, "Face to Face meetings held in the U.S.  This excludes the majority of international participants, leading some voices to be stronger than others.  It's hard to convince someone when you aren't in the room, even more so when the room is in a country that has a list of travel advisories for foreign travelers, including 'Violent crime is more common in the U.S. than in Australia,' 'There is a persistent threat of mass casualty violence and terrorist attacks in the U.S.,' and 'Medical costs in the U.S. are extremely high.  You may need to pay upfront for medical assistance.'"



Okay, now, the point he's making here is that Google has outrageously outsized power to decide what does and does not succeed in the world, due to their unilateral control of their Chrome browser.  That which Chrome does not support, dies.  And he's also observing something that might not ever occur to those of us who are happily camped out here in the U.S., which is that, unfortunately, the U.S. can apparently be somewhat frightening and expensive for volunteer open source developers wishing to have their voices heard from other countries.  His point is those voices are too easy for Google to ignore.



And Leo, when I was thinking about this, this brought to mind something that Stina Ehrensvard often mentioned to me through the years.  After founding Yubico in Sweden, she understood the critical importance of geographic location.  So she deliberately uprooted her young family and relocated to Silicon Valley.  She knew that, if she was going to succeed, she needed to be where the action was, and specifically to be able to attend face-to-face meetings with Google executives and others.  In the list of authenticators on William's webauthn-rs site, Yubico's products are all mentioned first because, when it mattered, she was there in person.  And I know, as you know, Leo, truth be told, it's often quite difficult to say no to Stina.



LEO:  Yeah, and that's how you met her at RSA coming down the escalator.



STEVE:  That's right.



LEO:  So you're right.  In person makes a big difference.



STEVE:  Yup.  Yup.



LEO:  But don't ask Marcus about what it means to be an open source developer in the United States, Marcus Hutchins, because of course he was arrested on the tarmac...



STEVE:  Trying to leave.



LEO:  Trying to leave the United States.



STEVE:  Yup.



LEO:  So I understand why there's a little chilling effect on open source developers here.



STEVE:  Well, and I guess you really do, I mean, I'm sure those travel advisories exist.  I don't know how much you have to heed them, but still.



Then, under the topic of, or the subtopic of "The Descent," as he put it, he said:  "In 2022 Apple announced Passkeys.  At the time, this was really just nice marketing, a nice marketing term for passwordless, and Apple's Passkeys had the ability to opportunistically be usernameless, as well.  It was, all in all, very polished and well done.  But of course thought leaders exist, and Apple hadn't defined what a Passkey was, exactly.  One of those thought leaders took to the FIDO conference stage and announced 'Passkeys are resident keys,' while at the same time they unleashed a Passkeys dev website.



"The issue is described in detail in another of my blog posts.  But to summarize," he writes, "this push to resident keys means that physical hardware security keys are excluded because they often have extremely low limits on storage, the largest being 25 for YubiKeys.  That simply won't cut it for most people, who have more than 25 accounts."  And that, I'll just mention, that's one of the biggest annoyances with the whole Passkeys technology is the requirement for significant storage per Passkey.  That is, you know, the big thing that I don't have.  The approach that I took with SQRL explicitly avoided that by, you know, being able to create per-domain, similar security per-domain keys, meaning that you only had to have one, instead of this problem, in the case of the YubiKey, they're able to store 25.  But once you hit that limit, you need another one.



LEO:  I know.  That's a frustration for me.  I wish they'd add more memory.



STEVE:  Yeah,  yeah.  Anyway, William then coins a term that Cory Doctorow might appreciate.  He terms the period following the announcement of Passkeys as "The Enshittocene Period."



LEO:  I like it.



STEVE:  Yeah.



LEO:  All right.



STEVE:  The Enshittocene Period.  He says:  "Since then, Passkeys are now seen as a way to capture users and audiences into a platform.  What better way to encourage long-term entrapment of users than by locking all their credentials into your platform; and, even better, credentials that cannot be extracted or exported in any way.  Both Chrome and Safari will force you into using either hybrid where you scan a QR code with your phone to authenticate.  To use a hardware security key requires clicking through multiple menus.  And even their default is not a good experience, taking more than 60 seconds' work in most cases.  The UI is beyond obnoxious at this point.  Sometimes I think the password game has a better user experience.



"The more egregious offender is Android, which won't even activate your security key if the website sends the set of options that are needed for Passkeys.  This means the identity provider gets to choose what device you enroll without your input.  And of course all the developer examples only show you the options to activate 'Google Passkeys stored in Google Password Manager.'  After all, why would you want to use anything else?



"A sobering pair of reads are the GitHub Passkey Beta and GitHub Passkey threads.  There are instances of users whose security keys are not able to be enrolled as the resident key slots are filled.  Multiple users describe that Android cannot create Passkeys due to platform bugs.  Some devices need firmware resets to create Passkeys.  Keys can be saved on the client, but not on the server, leading to duplicate account presence and credentials that don't work; or, worse, lead users to delete the real credentials.  The helplessness of users on these threads is obvious, and these are technical early adopters, the very users we need to be advocates for changing from passwords to Passkeys.  If these users cannot make it work, how will normal people from other disciplines fare?



"Externally, there are other issues.  Apple Keychain has personally wiped out all my Passkeys on three separate occasions.  There are external reports we've received of other users whose Keychain Passkeys have been wiped just like mine.  Consequently, as users we have the expectation that keys won't be created correctly, or they will have disappeared when we need them most.  In order to try to resolve this, the working group seems to be doubling down on more complex JavaScript APIs to try to patch over the issues that they created in the first place.  All this extra complexity comes with fragility and more bad experiences, but without resolving the underlying core problems.  It's a mess."



And then for the future, he says:  "At this point I think that Passkeys will fail in the hands of the general consumer population.  We missed our golden chance to eliminate passwords through a desire to capture markets and promote hype.  Corporate interests have overruled good user experience once again.  Just like ad blockers, I predict that Passkeys will only be used by a small subset of the technical population, and consumers will generally reject them."



LEO:  Wow.



STEVE:  "To reiterate, my partner, who is extremely intelligent, an avid computer gamer, and veterinary surgeon has sworn off Passkeys because the user experience is so crappy.  She wants to go back to passwords.  And I'm starting to agree.  A password manager gives a better experience than Passkeys.  That's right. I'm here saying," he writes, "passwords are a better experience than Passkeys.  Do you know how much it pains me to write this sentence?  And yes, that means multifactor authentication with time-based one-time passwords is still important for passwords that require memorization outside of a password manager.



"So do yourself a favor."  This is what he writes.  "Get something like Bitwarden; or, if you like self-hosting, get Vaultwarden.  Let it generate your passwords and manage them.  If you really want Passkeys, put them in a password manager you control."



LEO:  Oh, I agree 100%.  Yes.



STEVE:  "But don't use a platform-controlled Passkey store."



LEO:  Yes, yes.



STEVE:  "And be very careful with physical hardware security keys.  If you do want to use a security key, only use it to unlock your password manager and your email.



"Within enterprise, there still is a place for attested security keys where you can control the whole experience to avoid the vendor lock-in parts.  It still has rough edges, though.  Just today I found a browser that has broken attestation, which is not good.  You still have to dive through obnoxious user-experience elements that attempt to force you through the default QR code path, even though your identity provider will only accept certain security models, so you're still likely to have some confused users.



"Despite all this, I will continue to maintain webauthn-rs and its related projects.  They're still important to me even if I feel disappointed with the direction of the ecosystem.  But at this point, in Kanidm we're looking into device certificates and smartcards instead.  The UI is genuinely better.  Which says a lot considering the state of the PKCS11 and PIV specifications.  But at least PIV won't fall prone to attempts to enshittify it.  PIV stands for 'Personal Identity Verification.'  It's a standardized physical smartcard system that's heavily used by government and military.  The technology to create digital identity cards has been around for a long time, and they are so fraught with their own problems that they aren't really an alternative to solve the web's authentication needs."



So I think that, for me, the thing that's so sad is that Cory Doctorow's term, and the examples of enshittification that Wikipedia documented, make very clear that these are deliberate usury outcomes.  The shortest of Wikipedia's examples is what Uber did.  Wikipedia writes:  "App-based ridesharing company Uber gained market share by ignoring local licensing systems such as taxi medallions, while also keeping customer costs artificially low by subsidizing rides via venture capital funding.  Once they achieved a duopoly with competitor Lyft, the company implemented surge pricing to increase the cost of travel to riders and dynamically adjust the payments made to drivers."



So nearly all of the problems William observed in his posting are the things we on this podcast independently noted from the start as inherent problems with the way Passkeys have been rolled out.  As I've observed on several occasions, the fact that Passkeys were implemented in a non-portable way, as a vehicle for creating implicit platform lock-in, is almost a crime.



But the new thought that William proposes is something that had never occurred to me.  Perhaps it's because I've been blinded by Passkeys' superior public key technology which offers so many potential authentication benefits.  Even though the benefits are theoretical, I've never questioned whether or not Passkeys would eventually become the new standard for the web.  But William writes:  "At this point I think that Passkeys will fail in the hands of the general consumer population.  We missed our golden chance to eliminate passwords through a desire to capture markets and promote hype."



When I read that the first time I was surprised. But at the same time, I have still not adopted Passkeys.  I've never registered a single Passkey.  I don't have even one, anywhere.  I don't encounter websites that offer Passkey authentication, so there's that.  But mostly, because authentication matters crucially to me, I want to feel that I'm in control of my authentication.  And that starts with thoroughly and deeply understanding it.  I do thoroughly and deeply understand Passkeys' underlying cryptography.  But as William explains, what's then been done with that underlying crypto has been made deliberately opaque as a means of "just trust us" individual platform lock-in.  The problem is, my authentication is far too important for me to entrust to any company that might choose to, dare I say, enshittify it.



Having unique, per-site, insanely long high-entropy passwords that I can touch and feel and copy and paste and see, stored and managed by a cross-platform password manager, which is everywhere I need it to be, allows me to really understand the status of my authentication.  One thing I also have is a long and growing list of TOTP one-time passcodes.  And the reason I'm absolutely comfortable with that is that, again, they're tangible things that I can control, see, and understand.



Has the entire techie insider industry just been playing with itself this whole time?  Have we been imagining that authentication can and should be made entirely invisible because Passkeys can theoretically make that happen?  Will end users who don't know anything about the underlying technology say, "Well, I don't know how it works, but it certainly was easy?"  But then what about when it doesn't work?  What about when someone needs to log on from a device that's outside the provider's walled garden?  These are all problems we've previously identified and questions we've asked before.  I've always assumed that this was just the typical extreme adoption inertia we always see.  It never occurred to me that Passkeys might ultimately fail to ever obtain critical mass and to eventually become more dominant than passwords.



While poking around to get a broader perspective, I encountered a recent piece in Wired titled "I Stopped Using Passwords.  It's Great - and a Total Mess," with the intro "Passkeys are here to replace passwords.  When they work, it's a seamless vision of the future.  But don't ditch your old logins just yet."  The author explained that, as William said, things didn't always work.  He also noted that having multiple clients all popping up and asking whether you want to save Passkeys with them had become annoying.  But the biggest problem he had was remembering where he had stored which Passkey.  Now, as someone who spends some time pondering which of the multiple streaming providers carries the show my wife and I have been watching, that definitely resonated.



Our advice at the start of this Passkeys saga was to wait until a single provider offered Passkeys support across every platform that might conceivably be needed, since Passkeys portability was not something that anyone was even talking about back then.  In fact, back then it was clearly an overt password lock-in move.  So I wanted to share the news that Bitwarden, the solution that William's posting referred to and a sponsor of the TWiT network, earlier announced on the 10th of this month that Passkeys for iPhone and Android clients had just entered beta testing.  So I'm very glad to see that my chosen open source password manager will soon be offering Passkey support.



Now what's going to be needed, based upon the experience of the author of the Wired article, will be the ability to assign a single Passkey handler to a platform, much the way we currently assign a handler for a platform's URL links.  Having all of a platform's Passkey-aware clients popping up solicitations to store a Passkey with them seems like it would quickly become annoying.  On the other hand, you know, setting up a new Passkey doesn't happen that often.



On balance, I still don't feel much pressure to give up my use of passwords since they're working perfectly for me today.  The other factor is that website login has become so persistent that I rarely need to re-authenticate to most sites.  Each of the browsers I use carries a static cookie for each of the sites I frequent, so I'm already known everywhere I go.  For the foreseeable future, I expect to hang back and wait.  The dust is still settling on Passkeys, and Passkeys doesn't solve any problem I have today, even though they're cool.



LEO:  If SQRL had only taken off.  But you see the problem, which is...



STEVE:  Anything.  Anything new.



LEO:  Well, but it's not just that.  The platforms aren't going to support it unless they own it.



STEVE:  Right.



LEO:  And it gives them lock-in. 



STEVE:  Right.



LEO:  That's why Apple loves this.  That's why Google and Microsoft and - they want the lock-in.  And that's why I use, and I agree with him, only use Bitwarden or some sort of open source manager that you can at least take with you to do it.  But, oh, this is sad because it really - it's a great idea.  But the writing's on the wall.  Very few websites use it.



STEVE:  Yes.  Yes.  And if they start to use it, and then their users have problems with it, they'll pull it.



LEO:  Right.



STEVE:  I mean, it could disappear as an option because it's not worth it to them.



LEO:  Not worth it.  No benefit.



STEVE:  Everybody knows how to use a username and password.



LEO:  Sigh.  SQRL really solved all these problems, and that's sad because it really - it was - oh, well.  What can you do?  You've kind of gotten over it.  I'm still...



STEVE:  I've gotten over it.  I solved the problem.  I satisfied myself.  And we had a lot of fun developing it and working out all of the edge cases and so forth.  And, you know, now I'm on to solving other problems.



LEO:  This is where you jump up from your easy chair, grasp your Cabernet, and you shake your fist at the skies and say, "Why, I oughta...."  Steve Gibson, GRC.com.  That's his home on the Internet, the Gibson Research Corporation.  Go there to get SpinRite, the world's best mass storage maintenance and recovery utility.  6.1's out and fantastic.  If you've got an SSD, this is the kind of unexpected benefit of it.  You can use 6.1 to speed up your SSD.  That's fantastic.  What an improvement.



STEVE:  Yeah.  As I say, "recover lost performance."



LEO:  Yeah.  That's his promise, yeah.



STEVE:  It is data recovery and performance recovery.



LEO:  Yeah.  We'll have to say mass storage performance, recovery, and maintenance utility.  We'll add that.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#973

DATE:		May 7, 2024

TITLE:		Not So Fast

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-973.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  What danger is presented by the world's dependence upon GPS?  And why is that of any concern?  Has the sky fallen on all VPN systems?  And why does the tech press appear to think so?  Today's myriad network authentication options are confusing and incomplete.  What does the future promise?  Why might Apple have been erasing iCloud Keychain data?  And what's actually going on between Google and the United Kingdom regarding the sunsetting of third-party cookies?  What's the problem?  Or is there one?



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  He'll talk about GPS fuzzing, how it works, what one can do to avoid it.  You've all heard about that VPN flaw that Ars Technica says makes all VPNs useless.  Not so fast.  Steve explains why it is not anything to panic about.  And then, speaking of not so fast, Google has stopped progress on abandoning third-party cookies.  Steve now knows why.  He will explain all that and a whole lot more coming up next on Security Now!.  Stay here.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 973, recorded Tuesday, May 7, 2024:  Not So Fast.



It's time for Security Now!, the show where we cover the latest security and computer news and privacy news, and of course a little sci-fi and TV thrown in, with this guy right here, Steve Gibson, the arbiter of all that is good and kind.  Hello, Steve.



STEVE GIBSON:  Oh, well.  I'll go for that.



LEO:  Yeah.



STEVE:  Yeah.  Hello, Leo.  So here we are at the beginning of May.  And as I promised, I did have some time to dig into the issue that came up actually two weeks ago when in the middle of the show you said, "Hey, Google just changed their plans on third-party cookies."  And I said, "What?"  Anyway, so we're going to talk about that.  Today's episode is titled "Not So Fast," which as in that expression, "Not so fast, there."  Which is what the UK is saying to Google.



But we're going to first look at what danger is presented by the world's current and growing dependence upon GPS, and why is that any concern?  Has the sky fallen on all VPN systems, as the tech press has been reporting since yesterday, when a blog post...



LEO:  Oh, good.



STEVE:  ...really went a little out of control.



LEO:  I was really hoping, and I wanted you to explain Option 102 or whatever.



STEVE:  Option 121.



LEO:  Oh, yes.  I really want to know about that.



STEVE:  We will know all about that by the time we're done today.



LEO:  Thank you.



STEVE:   Also a couple questions more from our listeners still bogged down in what is arguably a quagmire of network authentication options.  So I'm going to spend a little more - that's continuing to come into crisper focus for me, so I figured let's - I'm going to spend a little more time on what's going on there.  Also, we may have an answer to what Apple was doing with the iCloud Keychain deleting and what was going on, something that absolutely makes sense, so we're going to cover that.



And also, finally, as I said, I invested no little bit of time in - you'll hear the term "bureaucracy" used more times probably than any large word in this podcast because, boy, you know, I guess any kingdom that's been around as long as the United Kingdom and has continued to survive has also developed quite a system of bureaucrats, and they all want to weigh in on Google's plan.  So anyway, I think another great podcast for our listeners.  And a Picture of the Week that's kind of a hoot, too.



LEO:  Oh, good.  Always enjoy the Pictures of the Week.  Well, Security Now! is ready to get underway.  I hope you are, as well, boys and girls, cats and kittens, club members and others.  



STEVE:  Well, the important work will be appearing shortly.



LEO:  Hey, this is important work.  Do not knock this work.		



STEVE:  Now, we have a picture, a Picture of the Week, from somewhere, it looks like in the U.S. Southwest.  There's no signs of any telephone poles or structures, so we're kind of out in the desert somewhere.  And so one of the things that people want is they want their cell phones to work out in the middle of nowhere.  And actually this is a problem I have with many movies these days, which seem to forget that it's necessary to have a cell tower not too far away from where your cellular device is in order for it to get any connection.  You know, we see people wandering out in the middle of literally nowhere, and they're on the phone.  Unless the writers don't want them to be, in which case they're holding the phone up, you know, scanning around, trying to find a signal.



Well, the way we solve the problem of people wanting cell phone coverage wherever they are, yet nobody wanting to despoil the landscape as a means of providing it, is we come up with stealth cell phone towers.  And I'm not sure how truly stealthful this is because it looks a little square to be a cactus.  But I gave this picture the caption, "Oh, don't mind us.  We're just putting the lid back on the cactus."  Because this is clearly a cell phone tower cactus which is meant, I mean, it actually, you know, it's got the little extra, what do you call it, arm, off the side of the cactus, to make the whole thing look a little more cactus-like.



LEO:  It's so funny.



STEVE:  And actually you can see some other cacti in the neighborhood that look decidedly less mechanical than this one.



LEO:  All over Mexico you see these saguaro cactuses, and I guess the Southwest, as well.



STEVE:  Yeah.



LEO:  So, you know, you see it with a hundred others, you probably wouldn't look twice.  It's actually clever.



STEVE:  Yeah, it's certainly not an eyesore, looking like this thing would look like with the lid off, which we can see here because...



LEO:  Right.  The lid's off.



STEVE:  Yeah, the lid is off and the crane has lifted the lid off the cactus.  Anyway, I just got a kick out of this.  And I've seen fake palm trees, and I know that here on the so-called sort of now the famous 405 in Southern California there are power lines that run alongside the freeway, and every, like, very often there's a big cluster of cell equipment on the power lines because it's a perfect place for them to be, you know, there's already a right-of-way.  There's some ability to run a service vehicle along the back and so forth.  And many, many, many moons ago, back in the SpinRite - actually it was after SpinRite 2 because I remember I was working on SpinRite 3, I built a building in Aliso Viejo, you know, a corporate headquarters, 20,000 square feet, two stories, and 1.43 acres of land and so forth.



LEO:  Holy moly.  Wow.



STEVE:  Anyway, the cell companies came to me and said, hey, this building is like up on a point on a bluff looking out over this valley.  You can make some extra money by letting us put some cell things, like ringing along the edge of your roof.  Well, you know what my answer was.



LEO:  You said no?



STEVE:  The same answer - I said no.



LEO:  Why?



STEVE:  This is a beautiful building.  I'm not going to have warts of cell crap all over the...



LEO:  I bet they're there now, Steve.



STEVE:  They are.



LEO:  Oh, yeah.



STEVE:  I mention that because I drove by not long ago, looking wistfully up at the building, and there it was, just - I don't know.  I don't think you could get more cell tower crap around the perimeter of this roof than there is there now.  But not while I was in control.  But immediately after I left, apparently.  Anyway, such is the world, you know.  And that's why I also have no ads on my site.  Mark Thompson made a case, he said, at one point he said, "Steve, there's something wrong now with a website that doesn't have ads."



LEO:  Yeah, what's wrong with you?  Yeah.



STEVE:  No, thank you.  Anyway, I wanted to start off this week by sharing an important piece of interesting news that's not Internet security-related, that is nevertheless potentially quite a big and serious issue in the real world.  Last Thursday's headline in Wired was "The Dangerous Rise in GPS Attacks," with the subhead "Thousands of planes and ships are facing GPS jamming and spoofing.  Experts are warning these attacks could potentially impact critical infrastructure, communication networks, and more."



Okay.  So I thought that was interesting, got my attention.  They said:  "The disruption to GPS services started getting worse on Christmas Day."  Meaning at the end of 2023.  "Planes and ships moving around southern Sweden and Poland lost connectivity as their radio signals were interfered with.  Since then, the region around the Baltic Sea - including neighboring Germany, Finland, Estonia, Latvia, and Lithuania - has faced persistent attacks against GPS systems.



"Tens of thousands of planes flying in the region have reported problems with their navigation systems in recent months amid widespread jamming attacks, which make GPS inoperable.  As the attacks have grown" - no surprise to anyone - "Russia has increasingly been blamed, with open source researchers tracking the source to Russian regions such as Kaliningrad.  In one instance, signals were disrupted for 47 hours continuously.  On Monday, marking one of the most serious incidents yet, airline Finnair canceled its flights to Tartu, Estonia for a month, after GPS interference forced two of its planes to abort landings at the airport and turn around."  Talk about dependence on GPS.  Apparently you just can't land anymore without it.



"The jamming in the Baltic region," they wrote, "which was first spotted in early 2022, is just the tip of the iceberg.  In recent years there's been a rapid uptick in attacks against GPS signals and wider satellite navigation systems, known as GNSS (Generic Satellite Navigation), including those of Europe, China, and Russia.  The attacks can jam signals, essentially forcing them offline, or spoof the signals, making aircraft and ships appear at false locations on maps."  Which you can imagine might be even more damaging than just jamming outright.  "Beyond the Baltics, war zone areas around Ukraine and the Middle East have also seen sharp rises in GPS disruptions, including signal blocking meant to disrupt airborne attacks."  Which actually, as we'll see a little bit later, I think is the actual goal of this because of the degree to which drones are now using GPS.



Wired wrote:  "Now governments, telecom, and airline safety experts are increasingly sounding the alarm about the disruptions and the potential for major disasters.  Foreign ministers in Estonia, Latvia, and Lithuania have all blamed Russia for GPS issues in the Baltics this week and said the threat should be taken seriously.  Jimmie Adamsson, the chief of public affairs for the Swedish Navy, told Wired:  'It cannot be ruled out that this jamming is a form of hybrid warfare with the aim of creating uncertainty and unrest.  Of course there are concerns, mostly for civilian shipping and aviation, that an accident will occur, creating an environmental disaster.  There's also a risk that ships and aircraft will suspend their traffic to this area and thereby affect global trade.'



"Joe Wagner, a spokesperson from Germany's Federal Office of Information Security, told Wired:  'A growing threat situation must be expected in connection with GPS jamming.'  Wagner said there are technical ways to reduce its impact.  Officials in Finland say they have also seen an increase in airline disruptions in and around the country.  And a spokesperson for the International Telecommunication Union, a United Nations agency, told Wired that the number of jamming and spoofing incidents have 'increased significantly' over the past four years, and interfering with radio signals is prohibited under the ITU's rules."  Gee.  You think Russia is slowed down by a NATO agency, the International Telecommunications Union, saying, well, you shouldn't be doing that?  Right.



"Attacks against GPS, and the wider GNSS category, come in two forms.  First, GPS jamming overwhelms the radio signals that make up GPS and make the systems unusable.  Second, spoofing attacks" - which actually are far more sophisticated - "can replace the original signal with a new location.  Spoofed ships can, for example, appear on maps as if they're at inland airports."  And actually that did happen recently.  "Both types of interference have increased in frequency.  Disruptions, at least at this stage, mostly impact planes flying at high altitudes and ships that can be in open water, not people's individual phones or other systems that rely on GPS.



"Within the Baltic region, 46,000 aircraft showed potential signs of jamming between August 2023 and March this year, according to reports and data from tracking service GPSJam.  Benoit Figuet, an academic at the Zurich University of Applied Sciences who also runs a live GPS spoofing map" - there is such a thing - "says there have been an additional 44,000 spoofing incidents logged since the start of this year.  Earlier this month more than 15,000 planes - earlier this month more than 15,000 planes had their locations spoofed to Beirut Airport, according to data that Figuet shared with Wired.  More than 10,000 were spoofed to the Cairo Airport, while more than 2,000 had their locations showing in Yaroslavl, Russia, the data shows.



"Separate analysis from geospatial intelligence company Geollect shared with Wired showed that on April 16th around 55 ships broadcast their location as being over the main runway at Simferopol International Airport in Crimea, Ukraine.  The airport is around 19 miles inland from the Black Sea, where it's believed the ships were actually located."  So, yeah, it's no longer possible to believe what GPS is showing you.  You need to look out the window and see where you actually are.



"Zach Clements, a graduate research assistant at the University of Texas here in Austin, said:  'The biggest change in the past six months is definitely the amount of spoofing.'"  As I said, spoofing is far more sophisticated and difficult than just jamming, and potentially far more dangerous.  "He said:  'For the first time, we're seeing widespread disruptions in civil aviation, especially in the Eastern Mediterranean, the Baltics, and the Middle East.  In prior years, there were reports of spoofing impacting marine vessels, but not aviation.'



"Clements says there appear to be three spoofers that can be traced back to Russia.  One open source intelligence analyst, going by the pseudonym Markus Jonsson, has located jamming in the Baltics, and that which impacted the Finnish airline this week" - so that was the one that was causing them trouble - "to Kaliningrad and other Russian locations.  One research group has suggested disruption near Poland impacted Russia's own GNSS system less than others."  Not surprisingly, Russia doesn't want to hurt themselves.  They just want to disturb everybody else.  And "Russia has a long history of interfering with GPS signals, both within its borders and internationally.  Russia's embassy" - not surprisingly - "in the UK did not respond to a request for comment.



"The disruptions can cause uncertainty and potential safety issues for airline pilots and their passengers."  Yeah, no kidding.  "A spokesperson for Eurocontrol, a European aviation organization with more than 40 countries as members, says its analysis shows disruptions are happening in the Eastern Mediterranean, areas around Ukraine and the Black Sea, as well as the Baltic states.  During one week in March, 4,387 aircraft reported issues.  The Eurocontrol spokesman says for the same time last week there were 2,646 flights reporting problems.



"The Eurocontrol spokesman says planes can fly safely without GNSS, but interference 'puts a higher workload on pilots and air traffic control.'  A safety notice issued by the UK's Civil Aviation Authority this month says loss of GNSS, which is just, you know, general satellite-based navigation, can result in serious navigation issues, incorrect emergency terrain warnings that the plane is too low to the ground, and failure to various other systems."



And finally, in a NASA report detailing GPS incidents that was also published this month, one pilot said:  "I have flown with crew members who were not fully aware of this problem."  Other pilots said they had received "false terrain warnings" that caused them to pull up.



LEO:  Ooh, that's not good.



STEVE:  Yes.  And that pilots should have a "thorough review of jamming effects on the different aircraft systems" as part of their training.  And here's the problem, of course.  Because this is a relatively new phenomenon relative to when the pilots were trained, it may just be the fact that the pilots are trusting their avionics and not being sufficiently skeptical.  So it does look like these GPS disruptions are coinciding with Russia's full-scale war in Ukraine.



And also it looks like Israel's attacks in Gaza have also been tied into this.  As we know, disrupting GPS as part of electronic warfare has become common on Russia and Ukraine's battlefield as a way to try to limit the operation of drones.  And while Iran launched a barrage of missiles and drones against Israel last month on the 13th, Israeli GPS disruption designed to limit the impact of the attack also impacted mapping and taxi services, as well as food delivery.  So here was an instance of Israel doing some GPS jamming which was somewhat indiscriminate, and the mapping and taxi services as well as food delivery within their own country took a hit as a consequence.



"Kevin Heneka," writes Wired, "the founder of cybersecurity company Hensec, whose work includes detecting GPS disruptions, says jamming and spoofing technology has become cheaper and smaller over the years, to the extent that individuals can install them in their cars to hide their own movements."  That is, you know, you're blocking your own GPS receiver so your car doesn't know where it is.  "However," Heneka says, "more sophisticated attacks use equipment that can cost huge sums."  Yes.  Anytime you're doing spoofing, as I said, spoofing is a whole 'nother level than just blanket jamming.  He said:  "In conflict zones, in military terms and in professional terms, this spoofing is very sophisticated, and it always goes hand in hand with jamming."



Okay.  So since both the jamming and the location spoofing disruptions are enabled through the use of very powerful local radio transmitters, which overwhelm the reception of the authentic signals being beamed down from the GPS satellite systems in orbit, so long as you're not in the region of the Baltics, where Russia appears to have taken serious action to create major disruptions, the good news is, these attacks are inherently local in nature.  You know, here in the U.S. we're not being affected by it at all, as is most of Europe.  They are inherently very local.



But the problem for those who are in the region is that GPS and the wider GNSS, which again, Global Navigation Satellite System, have always been incredibly reliable sources.  And not just of location, but also of time.  They are master sources of essentially time of day.  And as we know, when something is both very useful and has earned the reputation for also being very reliable, I mean, you know, these things are up in the sky beaming down at us, they end up creating a strong dependence.  We end up becoming very dependent upon them.  So many modern, non-military, commercial systems have become so reliant upon GPS that the deliberate disruption of that service for military purposes such as Russia has likely been perpetrating, can cause dramatic collateral damage.



The GPS system, which is put out by the U.S., was conceived quite a while ago, a little over 50 years ago, back in 1973.  It took five years to package this in the first satellite that began launching at that time, and today we have 24 satellites up in the GPS constellation.  They've been up and operating since 1993.  And talk about depending upon something that's more fragile than we might want.  Our phones and automobiles today only know where they are largely thanks to GPS signals coming from space.



LEO:  I only know where I am thanks to GPS signals.  Forget the car.



STEVE:  Yeah.



LEO:   I can't drive without GPS.



STEVE:  And I'm sure, you know, a sports wristwatch, you know, health-tracking wristwatches are doing the same thing.



LEO:  Oh, absolutely, very much.



STEVE:  And we have recently been talking about the militarization of space and the idea that having satellites attacking one another "up there" is no longer the territory of James Bond science fiction.  You know, it's actually happening.  In some cases robot satellites are there in order to repair others.  But the same robot that can function to fix a broken antenna can also go over and break one off of some other satellite.  So unfortunately they also have multiple purposes.



And unfortunately, as global political tensions increase, we can hope, and we need to hope, that no major powers having space-based military capabilities, nor the ability to kill satellites from the ground, believe that denying the entire world these benefits would create an advantage for them because it's difficult now to conceive of a world where GPS was just shut down, like destroyed deliberately by a power hostile to - it wouldn't even necessarily have to be hostile to the U.S.  It could be because everyone's using GPS, killing it for everyone also succeeds in killing it for a specific targeted country.



Before GPS, the only way for something to know where it was, was through a system of inertial navigation.  Inertial navigation, like its name suggests, is a closed system which relies upon the system's precise measurement of its own linear and angular accelerations.  It integrates those over time to determine its velocities, and then integrates those over time to determine its velocities, and then integrates those over time to determine its position.  Even though inertial navigation systems are still in use due to the nearly instantaneous position, and especially angular feedback that they provide, the errors that tend to creep in over time can only be eliminated with the use of slower but far more accurate input from the global GPS system.



I suspect Russia's primary concern is with the use of autonomous military drones, which may rely upon GPS to determine their in-flight location.  But since the risks presented by GPS jamming, although they haven't been prevalent, and it hasn't been a big concern for airline pilots until recently, especially operating over there in the East in the Baltic areas, since jamming has been a possibility for some time, I suspect that the latest technologies are much more immune to GPS outages than those in Russia might wish.



Given all of the advantages and the advances made in vision and in real-time recognition, I would be surprised if the latest autonomous technologies were not able to fly nearly as well by sight as they can these days by GPS.  They might well use GPS as a first choice, but use vision to detect location spoofing, while also being able to switch to pure vision if GPS should fail completely.  And another likely strategy which, again, you don't worry about or deal with until it becomes a problem, is that since GPS signals will always be originating from above, would be to shield any GPS receiver and its antennas...



LEO:  Oh, from below.



STEVE:  Yes.



LEO:  Because the jammers are on the ground.



STEVE:  Exactly.



LEO:  That's clever.



STEVE:  Yeah.  So planes can do that because they're well above ground.  Unfortunately, it's probably not practical for ships at sea.



LEO:  Yeah, I mean, when you listen to ground air traffic control talking to an airplane, which I used to always do on United Channel 9, used to love to do that, they often have visual markers, you know, they say "Turn right at the Big Rock Candy Mountain" and things like that.  I don't know if they still do that.  I haven't listened in a while.  But I bet they do.  I mean, there's always - you always want redundancy in any system like that; right?



STEVE:  Yeah.  And of course the problem is that, you know, we all - okay.  I remember, Leo, when I guess this must have been in driver ed, we were supposed to go out and walk around our car to check all four tires.



LEO:  Yeah, we don't do that anymore.



STEVE:  No.



LEO:  Do you do that?



STEVE:  When was the last time anybody did that?



LEO:  Pilots do that.  And commercial jet pilots do that.  And I think that goodness that they do.  I think that's really great.  But, no, I haven't done that to my car in a while.  I figure if it's flat, I'll know.  Right?  It'll go frump, frump, frump.



STEVE:  That's right.  That's right.   But I do remember being told that's what we're supposed to do.  So here we have a problem where GPS has been so reliable and relied on that I'm just hoping, I mean, in this NASA report last week where one of the guys said, you know, I've been with flight crews that just assumed that the GPS was telling the truth, even though they were suddenly being told to pull up because you're about to hit the Rock Candy Mountain, and that would not be good.  But there's nothing there.



LEO:  Pull up.  Pull up.



STEVE:  So Leo, let's take another break, and then we're going to talk about whether the sky is falling on all VPN systems.



LEO:  Yeah.



STEVE:  As the tech press seems to believe.



LEO:  I was counting on you to cover this because I read the stories.  Thank god you're covering it before I actually did the stories.  Keep me out of trouble, please.  Now, what's all this about VPNs, Steve?



STEVE:  Okay.  So yesterday, Ars Technica got a little carried away in their reporting of what amounts to a clever hack that a Seattle, Washington-based pen testing firm known as the Leviathan Security Group posted in their blog.  And of course the rest of the tech press picked up on it quickly, too.  The blog posting carried the headline "How Attackers Can Decloak Routing-Based VPNs for a Total VPN Leak."  And what I found curious was that they assigned - "they" meaning the Leviathan Security Group - assigned a CVE number to their discovery, even though nothing about this is a bug or a flaw.



LEO:  Oh.



STEVE:  It's just a clever local exploit of a little-used feature of DHCP servers.  Unfortunately, Ars Technica's headline for their story was headlined "Novel attack against virtually all VPN apps neuters their entire purpose."  Agh, run away.  Okay, which of course makes this sound more like the end of VPNs as we've known them.  It isn't.  Here's what's going on.



Okay.  So going to do a bunch of propeller head cool stuff in order to get a real grip on this.  Our PCs all interact with both internal and external networks through network interfaces.  Most systems typically have a single physical network interface, or NIC; but it's possible for a machine to have more than one physical network interface with each interface connected to different physical networks.  In that case, it's important for outgoing network traffic to know which physical interface any given packet should be routed out through.



To answer that question, our machines contain a routing table.  The routing table performs a "most specific match" function, based upon the destination IP address.  And in years past we've talked about Internet routing tables and all of this.  So we've covered this in detail.  But the key here is most specific match.  And that all of our PCs, every one of them, pads, phones, you name it, anything that's networked using Internet protocol, IP protocol, has a routing table.  Under Windows, for example, opening a command prompt and entering the command "route print" will display a list of the system's interfaces, followed by the IPv4 and IPv6 routing tables, respectively.  And they're interesting, and you can get a sense for the fact that there's a lot going on under the covers that we don't appreciate, we normally don't even see. 



Okay.  So this set of network communication, that is, IP-based network communication, comes in so handy that in addition to true physical interfaces, many of our machines will have one or more virtual network interfaces.   In fact, the so-called "localhost," you know 127.0.0.1, that's a virtual network interface that all stacks have.  And, for example, the use of virtual machines has become very popular, and they create their own virtual network interfaces to talk to their host machine, as well as to the outside world.



Okay.  So here's the main point:  Many VPNs, like OpenVPN for example, operate by creating their own virtual interface in the hosting machine.  It looks like and operates like any other network interface.  But being a VPN (Virtual Private Network) which is used to transact privately with encryption, any packets sent out of that virtual interface are first encrypted, then rerouted out of an actual physical interface to be sent to the VPN's matching endpoint.



Since the typical VPN user, while using a VPN, wants all of their machine's traffic to be tunneled through the VPN, when the VPN tunnel is brought up, the VPN software dynamically edits the system's global routing table in such a way that, instead of the system's traffic by default being routed out through its normal actual physical interface, all of its traffic is instead routed to the VPN's software-created virtual network interface.  This is the way that, deep down inside the guts of our machines, all of the traffic that's normally unencrypted suddenly becomes encrypted when we activate our VPN.



Essentially, it's like a man in the middle.  It sticks a shim into our network so that all of the traffic that would normally just go straight out that physical interface instead is routed to the VPN.  And that's done, as I said, by making just a slight change to the routing table so that all of the traffic, instead of going out the physical interface, goes to the VPN.



We need one other piece of information just to be certain that everyone's on the same page.  DHCP stands for Dynamic Host Configuration Protocol.  By default, when any networked machine boots up and gets itself going, it needs to be using an IP address for itself on its local network that's unique for that network.  And it needs to know the IP address to which it should address packets bound for the outside world, in other words, the network's gateway IP.  It may also want to know the IP addresses of some DNS servers that will honor its requests for domain name lookup.



It's the network's inward-facing DHCP server that answers all these needs.  When any networked machine starts up, by default it will emit a broadcast packet onto the network announcing its presence and asking for any listening DHCP server to please provide it with all the information it requires to become a well-behaving citizen on the local network and to connect to the rest of the global Internet.



DHCP cleanly organizes the various types of information it can supply into, like, to the clients who are requesting it, by number.  Each one of these is known as an "option," where the option number is a single byte, thus having a value from 0 to 255.  Zero is a null option and can be used for padding; 255 is the marker for the end of the list of options.  So the options are provided as a list of information terminated by Option 255, which of course, you know, is a byte of all ones.



So, for example, Option 1 provides the network's subnet mask to the requesting client.  Option 2 specifies the offset of the client's subnet in seconds, that is, in real-time, from UTC, Coordinated Universal Time.  Option 3 specifies a list of the IP addresses of routers on the client's subnet, what we know as the Gateway IP.  Option 4 specifies a list of time servers which are available to the client.  Number 6 provides a list of DNS servers for the client's use.  And, you know, there's a bunch of them, all kinds of different things that have been added through the years.  And there are even some surprises.  For example, options 69 and 70 provide the IP addresses of SMTP and POP3 email servers, which I thought was kind of cool.  We're all used to specifying those ourselves; but back in 1997, when this was first created, that information was available via DHCP.



Something else that DHCP was able to provide is the source of today's trouble.  The RFC's definition for Option 33 defines it as the "Static Route Option" and says:  "This option specifies a list of static routes that the client should install in its routing cache."  Okay, now, everybody who's been paying attention and enjoys networking stuff just went "aha" and knows what the problem is.



This thing continues:  "If multiple routes to the same destination are specified, they are listed in descending order of priority.  The routes consist of a list of IP address pairs.  The first address is the destination address, and the second address is the router for the destination."  Again, if some of you just said "Oh, crap!" that would be the correct reaction.  What this means...



LEO:  And it would mean they're paying attention.  Good job.



STEVE:  That's right.  That's right.  What this means is that the response from a DHCP server can be used to mess with a machine's routing table.  And as we noted earlier, a machine's traffic is routed to the VPN's virtual interface through a dynamic modification of the machine's routing table.



Now, as it happens, Option 33 is not really the problem because it was defined back in 1997 when IP networks were all class A, B, or C.  That meant that networks were defined to always have exactly one, two, or three bytes of host machine addresses.  As we know, this was extremely wasteful of IP addresses for networks falling into intermediate sizes.  So something known as CIDR, C-I-D-R, which stood for Classless Inter Domain Routing, was adopted.  That's what we have today, where the network mask can have any number of contiguous bits set, thus allowing scaling of networks by factors of two, all the way from one machine, well, technically up to 4.3 billion, but no one network has that except the Internet itself.



Okay.  So the adoption of CIDR obsoleted Option 33, forcing its replacement five years later in 2002 under the guidance of RFC 3442 which introduced Option 121, which allows for exactly the same thing, but under the specification of classless static routes.



Now, I mentioned that I was surprised that these Leviathan Security Group guys had arranged to get a CVE assigned for this, since technically this is a feature, not a bug.  And all the way back in 1997 the fundamental vulnerability of DHCP was quite well understood.  Again, 1997, Section 7 of the original RFC 2131 dated March of 1997, is titled, it was Section No. 7, "Security Considerations."  It says:  "DHCP is built directly on UDP and IP, which are as yet inherently insecure.  Furthermore, DHCP is generally intended to make maintenance of remote and/or diskless hosts easier.  While perhaps not impossible, configuring such hosts with passwords or keys may be difficult and inconvenient.  Therefore, DHCP in its current form" - which, by the way, is the form it has today in 2024 because, you know, if it's not broke - "in its current form is quite insecure," says the RFC from 1997.



LEO:  Wow.



STEVE:  They said:  "Unauthorized DHCP servers may be easily set up.  Such servers can then send false and potentially disruptive information to clients such as incorrect or duplicate IP addresses, incorrect routing information including spoofing routers, et cetera, incorrect domain nameserver addresses to spoof nameservers, and so on.  Clearly," they wrote, "once this seed information is in place, an attacker can further compromise affected systems."



Okay.  So here's how the Leviathan folks describe the attack they've devised by abusing Option 121.  They said:  "Our technique is to run a DHCP server on the same network as a targeted VPN user and to also set our DHCP configuration to use itself as a gateway.  When the traffic hits our gateway, we use traffic forwarding rules on the DHCP server to pass traffic through to a legitimate gateway while we snoop on it.  We use DHCP Option 121 to set a route on the VPN user's routing table.  The route we set is arbitrary, and we can also set multiple routes if needed.  By pushing routes that are more specific than a /0 CIDR range that most VPNs use, we can make routing rules that have a higher priority than the routes for the virtual interface the VPN creates."



As we know, because that means it's a more specific route, so the routing system will always route a more - will always take the most specific route available.  So by doing something, creating a network smaller than the /0, which is the everything, the routing table ends up routing to the intercepting DHCP server rather than to the user's VPN.  They said:  "We can set multiple /1 routes to recreate the 0.0.0.0/0 all traffic rule set by most VPNs.



"Pushing a route," they wrote, "also means that the network traffic will be sent over the same interface as the DHCP server instead of the virtual network interface.  This is intended functionality that is not clearly stated in the RFC.  Therefore, for the routes we push, it is never encrypted by the VPN's virtual interface, but instead transmitted by the network interface that is talking to the DHCP server.  As an attacker, we can select which IP addresses go over the tunnel, and which addresses go over the network interface talking to our DHCP server."  So in other words, they're able to literally select by destination IP.  If they don't want everything they can say, oh, just give us this chunk of your traffic.  You think it's going through your VPN, but it's not.



They said:  "We now have traffic being transmitted outside the VPN's encrypted tunnel.  This technique can also be used against an already established VPN connection once the VPN user's host needs to renew a lease from our DHCP server.  We can artificially create that scenario by setting a short lease time in the DHCP lease, so the user updates their routing table more frequently.  In addition, the VPN control channel is still intact because it already uses the physical interface for its communication."  That is, you know, the control channel meaning the channel to the remote end that is outside of the tunnel.  They said:  "In our testing, the VPN always continued to report as connected, and the kill switch was never engaged to drop our VPN connection."  Meaning there was never a panic that the VPN was concerned that it was being intercepted and so shut things down."



So then, to their credit, they raise the question that we've had all along, by asking "Is TunnelVision a vulnerability?"  And I appreciated their answer.  They wrote:  "This is debatable.  We're calling it a technique because TunnelVision doesn't rely on violating any security properties of the underlying technologies.  From our perspective, TunnelVision is how DHCP, routing tables, and VPNs are intended to work.



"However, it contradicts VPN providers' assurances that are commonly referenced in marketing materials.  In our opinion, TunnelVision becomes a vulnerability when a VPN provider makes assurances that their product secures a customer from an attacker on an untrusted network.  There's a big difference between protecting your data in transit and protecting against all LAN attacks.  VPNs were not designed to mitigate LAN attacks on the physical network, and to promise otherwise is dangerous.



"In our technique, we have not broken the VPN's cryptographically secured protocol, and the VPN is still fully functional.  An attacker is instead forcing a target user to not use their VPN tunnel.  Regardless of whether we classify this as a technique, VPN users are affected when they rely on assurances that a VPN can secure them from attackers on their local network."  And finally...



LEO:  Hmm.  Interesting.  That is one of the primary uses, isn't it, for a coffee shop and their open WiFi networks.



STEVE:  Exactly.



LEO:  Yeah, okay.  But this has been around forever, so.



STEVE:  Yes, exactly.  And they finished:  "As for what systems are affected, the short version is everything except Android."



LEO:  Isn't that funny.



STEVE:  Uh-huh.  Android doesn't support Option 121.  So it's completely excluded from these attacks.



LEO:  Isn't that wild.



STEVE:  They wrote:  "In our testing, we observed that any operating system that implements a DHCP client according to its RFC specification and has support for DHCP Option 121 routes is affected.  This includes Windows, Linux, iOS, and MacOS.  Notably," they wrote, "it does not affect Android as they do not have support for DHCP Option 121."  Which, you know, really is interesting.



LEO:  I wonder why not, yeah.



STEVE:  I do, too, because I did some digging, and there have actually been instances where Android's lack of Option 121 support has caused problems for Android users because it turns out this is not obscure, Leo.  This is the first time we've ever talked about it on the podcast because it's just never come up.  You know, we've covered DHCP in depth in the past.



Okay.  So just to be clear about the scope of the danger presented by the potential abuse of DHCP's Option 121, this is strictly a local LAN-side attack.  But Leo, as you correctly point out, you know, we do operate in essentially LAN networks where we're assuming a VPN is going to trust us where untrusted peers are on the same LAN we are.  So that's a thing.  The attacker needs some means of defeating the network's actual DHCP server.  Since DHCP clients will and do accept the first reply to their query, simply being faster to reply is typically all that's needed.



And, you know, as we know, most routers use the slowest chip that the manufacturer was able to get away with.  Boy, I tell you, those web interfaces on routers, it's like, okay, I click the button, hello, hello.  Should I click it again or just wait?  You know?  So the point is it's not going to be quick to fire off a DHCP reply because it doesn't need to; right?  That's going to be way down the priority queue of traffic that it needs to deal with.  So an attacker probably doesn't have much difficulty being able to respond with DHCP queries faster.  So it's definitely conceivable.  Also, in an enterprise environment, that if you had somebody untrusted on an enterprise network, that would be a problem.  And it also turns out that Option 121 is not the least bit obscure in the enterprise.  Turns out it's under heavy use.



I found two little samples through a quick search.  A posting over on Stack Exchange says:  "I'm running OpenVPN on a CentOS 7 server.  The DHCP server on the LAN uses Option 121 to tell other devices to use this CentOS server if they want to get to the VPN subnets the OpenVPN server is connected to.  This works great.  The problem is that this CentOS server is getting these same routes from the DHCP server, which breaks things."  And then he goes on to talk about how he can manually remove the static routes that the CentOS server is receiving from DHCP.  But my point is here's an example of where Option 121 is being used to inform machines on the LAN where to route the traffic they want to go through the CentOS 7 server's VPN subnets.  So it's very useful for that.



And also, just as recently as last Tuesday, someone posted to the what I have to categorize as the embarrassingly useless Microsoft answers forum.  I don't know if anybody has ever seen any of the crap that is there.  But, you know, if Microsoft really wants to lead in AI, they should remove whatever poor humans they have that are being forced to respond to forum postings there and put ChatGPT 12 or something in there instead.  It is, I mean, it is excruciatingly bad.



Anyway, someone posted, and needless to say they got no useful answer:  "When connected to my office network, its DHCP server," meaning his office's network DHCP server, "will use Option 121 to assign three different networks to be reached using a router which is not the default gateway.  This works absolutely.  The networks appear in my routing table in active routes.  Everything works.  Networks are reachable."  Anyway, so he wrote that, and I just grabbed that as a little snippet of another example of like Option 121 is really out there, and it turns out has really been useful, as I said.



He goes on to explain at some length.  He's complaining that when he boots his PC without any network connectivity, then it has a problem.  Uh, yeah.  That would be a problem.  So anyway, I wanted to point this out, again, that this DHCP option is in heavy use within more complex corporate networks.  What that means is that simply, like, blacklisting Option 121 is not viable.  In my opinion, it would be extremely unlikely for anyone at home to ever have anything to worry about, though it's still instructive to paint a picture.



The way I can see this might occur to somebody at home would be if some malicious device were connected to a residential network and wished to capture all of the user's traffic, whether tunneled through a VPN or not.  By being the first device to respond to any DHCP query, such a malicious device could establish itself as the network's gateway to receive, inspect, and forward all traffic from the network's many machines.  And then, by additionally using Option 121, such a device could use that to also insert entries into the user's routing table to prevent their VPN, if any, from tunneling the user's traffic.  Even though the VPN would show that everything was working and the user's traffic was protected, none of it would be.  The VPN tunnel would be up and established, but it would not be carrying any of the user's traffic.



Since there are many environments where Option 121 is not needed and is never used, like probably most of ours at home, I think it would be nice for our operating systems to provide the option, like, to hard disable it.  But I dug around, and I couldn't find any indication that that's being done.



LEO:  Yeah.



STEVE:  I would imagine the Windows firewall could be configured to just, you know, to look for any incoming DHCP port, what is it, it's been so long, is it 163 is DHCP?



LEO:  No idea, yeah.



STEVE:  I don't remember now the port numbers.



LEO:  So the best mitigation would be to turn off Option 121, but that's not an option.



STEVE:  No.



LEO:  What happens now?  Unless can VPN software be updated to have that as a feature?



STEVE:  The problem is this gets in underneath the VPN software.



LEO:  Yeah, yeah.



STEVE:  The VPN software, I mean, I suppose it could be updated to monitor the routing table and proactively determine whether or not it's been rerouted.  So that's certainly something that could be done.  Right now the VPN, when you bring up the VPN tunnel, it inserts a new default route for everything, and points it at its virtual interface so that it receives everything.  What it would need to do would be to - and I guess it actually could - would be to send itself a test ping.



LEO:  Ah, there you go.



STEVE:  From an IP in the user's IP space and verify that its virtual interface receives that ping.



LEO:  Right.



STEVE:  If it doesn't receive the ping, that tells it something has interfered with the routing between the user's localhost IP and its own interface.  So, yeah, that would be a cool feature for a VPN to add.



LEO:  And meanwhile there's not really a mitigation, is there.



STEVE:  No.  No.  And I think your use case is exactly the right one, Leo, because, you know, where do people deliberately bring up a VPN?  It's when they're in a hotel, in a caf, in any untrusted environment.  And they don't want to be sharing their traffic with everybody else.



LEO:  Yeah, yeah.  I wonder if commonly used hacking tools like WiFi Pineapples and stuff are able to do this.  They probably are.  I mean, it's been around for 30 years.



STEVE:  Yeah, but, well, oh, so you mean whether they're able to perform the hack?



LEO:  Yeah.



STEVE:  I bet that, you know, that intercepting...



LEO:  That seems like something you'd build in.



STEVE:  Well, and intercepting DHCP is such a juicy target.



LEO:  Yeah.



STEVE:  I mean, I'll bet you that hacking tools actively have DHCP server spoofing and are able to get a response out immediately.



LEO:  Interesting.  Wow.  This is good stuff, thank you.  Because this has been everywhere, this story.



STEVE:  Yeah.



LEO:  And I was really curious what you thought of it.



STEVE:  So it's a problem.  It's not, again, what are you going to do with a CVE?



LEO:  Right.



STEVE:  Like, hello.  Like, okay.  I mean, maybe that gets it more attention.



LEO:  It gets the word out; right?



STEVE:  Yeah.  Unfortunately, apparently GPT something is able to read the CVE and immediately design a hack that the script kiddies can then use.  So, great.



LEO:  Would you like to take a break?  Is that what you're looking at me like that for?  I know that look.  We will have more with Mr. Gibson in just a little bit.  You know, every week there's a story or two that in my mind, and I bet your mind, too, you go, I wonder what Steve has to say about that.  That's why we love you, Steve.  And that's why we listen to the show.  We're so glad to carry the show.  On we go with Mr. G.



STEVE:  Okay.  So a bit of feedback.  Dave Brenton tweeted:  "Mr. Gibson.  Quickly may I say, as a machine language coder, I admire your work in that area.  I'm a SpinRite owner/user and longtime fan since near the beginning of Security Now!.  My question is about security keys.  I hope this is not too long a question."  And it wasn't.



He says:  "I'm about to make the transition to YubiKey, and so I intend to purchase two, to have a safe fallback in case of loss.  I'm also planning to convert the wife over to the Passkey world.  My question is can the Passkeys be paired across two user accounts, thereby ensuring recovery in case of loss with only three keys?  My mental model says it made sense, but I do not know for sure.  One, can the same key be applied to two different people?  Two, to assure full backup protection, can all three keys be coded into both users?  It may be a silly notion, but could it work?  Or should I just buy four keys to begin with?  Thank you for all your good work and propeller-head installments.  On to 999 and beyond."



LEO:   Yes.



STEVE:  Dave.  Yes.  And I said at the beginning of the show on Episode 973 we are closing in on 999.



LEO:  We are.



STEVE:  Yet we're no longer fearful of that fatal number.



LEO:  Made me sad.  Hey, before you get to the answer, I just want to, well, actually do the answer.  And then I want to ask you about machine language and assembly.  I had some questions.



STEVE:  Well, cool.



LEO:  Yeah.  Go ahead.



STEVE:  Okay.  So I chose to share Dave's question because it so perfectly demonstrates the near total mess the user authentication world has fallen into today.



LEO:  It really has.  It really has.



STEVE:  It is just a catastrophe.  I'm hopeful this may just be a transition phase.  But truth be told, all of our collective experience also leaves me feeling somewhat skeptical.  I worry that all we have done by having the FIDO Group lower the bar for entry from requiring physical key dongles to allowing pretty much anything else - smartphones and PCs running simple software Passkey clients - is to expand upon the number of available options, with an additional and, difficult as it is to believe in this day and age, not very well-thought-out system.  And we've added this new and not well-thought-out system without removing any of the previous options.



Have traditional username and passwords been replaced?  No.  Are they ever going to be?  Not in this lifetime.  Have the "I forgot my password" links gone away?  No.  Are they ever going to?  No.  What about those time-based one-time passcodes?  Are they going away?  No.  Any plan for that?  No.  What about OAuth, which brings us the "Log on with your Google or Facebook or some other account?"  Have those been obsoleted and removed?  Nope.  Can they be?   Well, not easily, since many sites only know their users thanks to their redirection through another web service's authentication.



And so to this pile of existing half-baked remote network authentication solutions we are now adding Passkeys, a mysterious new solution that its designers all say is amazing and far more secure, which works sort of like magic right up until it doesn't work at all.  And when that happens, what do we do?  Well, we fall back to "Send me an email."



What we've wound up with is the well-known and often observed phenomenon of "solution spread."  We invent a better idea than what we had before.  Perhaps it's because the times have changed and the older solutions are no longer adequate.  Or perhaps we have more technology and available processing power than we had before, so new solutions are available than were previously.  But the problem is, we rarely are able to kill off the things that came before.  Why?  Because by the time we can do something more, too many people have come to depend upon the previous solution, and the one before that, and the one before it.



And this solution-spread doesn't just apply to the authentication domain.  Just look at Windows.  Without getting bogged down into the details, every few years Microsoft comes up with a new and much improved way of writing applications for their Windows OS.  And they promote the hell out of it, explaining how and why it's so much better than everything that came before.  And do they then kill off the previous ways of programming Windows?  No.  Of course not.  They can't.  They were once promoting the hell out of those previous solutions, and they got lots of people onboard using them then.  So even though they no longer love them and are urging everyone to use the new system, that never happens.  I've heard Paul over on Windows Weekly saying that the original Windows API, Win32, should have died off long ago.



LEO:  Oh, yeah.



STEVE:  That's what all of my Windows are written in; you know?  And not just mine; a gazillion others, as well.  And that's "gazillion" with a "G."  I am certain Paul knows that Microsoft will never abandon Win32.  They can't, any more than websites will ever be able to stop offering username and passwords with an "I forgot how" email link.



So just to be clear, the industry has added a bright and shiny additional way for people to log into their accounts.  But none of the existing ways are, or will be, removed.  Remember that today in 2024, only one out of every three Internet users is using any form of password manager.  I really don't know what the rest are doing.  Perhaps these are the people whose iOS and Android support for Passkeys is mostly aimed at.  You know, these people don't know, don't understand, and don't care about their online identity.  So when Apple or Google comes along and asks, "How would you like to log in instantly with Passkeys and never worry about another password?" well, that sounds great.



But that's not Dave, our listener whose questions launched me into first taking a bit of a rant into a wider view of where we stand today.  So let's look at Dave's situation.  Dave says he's planning to convert his wife over to Passkeys.  I'm sure he means that he would like to have his wife begin to use Passkeys, since it's not possible to "convert over" to Passkeys in any meaningful way when so few websites offer the option at all.  The caution there, since we do not yet have Passkey transportability, is to be careful about which app is holding a site's Passkeys.  As I mentioned last week, iOS, Windows, Android, and now an increasing number of traditional password managers will all be vying to be the app that generates the Passkey to be provided to a website.  Since only that app will then be able to authenticate the user to that site with a Passkey, the only sound strategy will be to only and always use a single platform for Passkeys.



This issue, and Dave's other questions, require a quick bit of foundation about the operation of Passkeys.  When an application prompts its user about whether the user wishes to have it create a Passkey, that's exactly what's happening.  The application generates a cryptographically strong secret and private key, which never leaves the application and which the application guards carefully.  From that closely held private key it then generates a public key, and only the public key is sent to and retained by the website.  In the future, that website will use the public key it holds to verify the signature of a challenge that it sends to the user's Passkey authenticator.



So my point here is that, today, there is no provision for these private keys, which were generated internally and have ever since been guarded by the application, to ever leave that application's control.  And a security-conscious organization like Apple can make the defensible claim that since all of the Passkeys' security derives from the "secretness" of these private keys, which is crucial, no other application, including its user, can or should be entrusted with their stewardship, with the stewardship of the Passkey's private key.  Since this represents a powerful platform lock-in, it's not at all clear to me that Apple will ever allow for Passkeys export.



That being the case, I think that a very strong case can be made for only ever storing Passkeys in a third-party Passkeys client, such as a browser extension.  In theory, it ought to be possible for a website to allow its user to replace one Passkey with another.  So if Apple or Android were to inadvertently become the generator and holder of a Passkey, if a website supported Passkey replacement, it should be possible to migrate away from one Passkey application to another.  And I was thinking about this.  If a website doesn't explicitly allow you to migrate  between Passkeys, hopefully it allows you to delete a Passkey, in which case your account would not be associated with one, and then you could reassociate it with a Passkey from the provider that you're wanting to switch over to.  So the real point here is that it is the application that generates the Passkey.  It is never something that we're able to supply from the outside.  



So just to put a bit of frosting on this discussion before we talk about the platforms with hardware authentication dongles, I wanted to share a few points from Google's Chrome FAQ.  This is Google's Chrome browser FAQ about Passkeys.  They start off, of course, with all of the glowing bits.  Under "Manage Passkeys in Chrome," they say:  "You can use a passkey to sign in easily and securely with just a fingerprint, face scan, or screen lock.  Passkeys are a simple and secure way to sign in to both your Google account and all the sites and apps you care about without a password.  You may be asked to sign into a website with a Passkey or create one to improve your account's security."  And then they have a little tip:  "Passkeys are built on industry standards, so you can use them across many platforms."



LEO:  Gotta love those industry standards.



STEVE:  Oh, Leo.  That's the happy news.  That all sounds terrific.  And of course we ask here, what could possibly go wrong?  Well, here's what Google has to say about that.  Under "Store Passkeys in Windows" they said:  "If you have Windows 10 or up, you can use Passkeys.  To store Passkeys, you must set up Windows Hello.  Windows Hello does not currently support synchronization or backup, so Passkeys are only saved to your computer.  If your computer is lost, or the operating system is reinstalled, you cannot recover your passkeys."  Whoops.



Or "Store Passkeys in macOS.  You can save Passkeys in your Chrome profile, where they're protected by a macOS Keychain."  Then under "Important" they said:  "Chrome cannot save or use Passkeys stored in iCloud Keychain.  If your computer is lost or your Chrome profile is deleted, you cannot recover your Passkeys."



And third:  "You can use a security key to store your Passkeys.  Important:  Passkeys stored on security keys are not backed up.  If you lose or reset the security key, you cannot recover your passkeys."  What a wonderful system.  This clearly represents a huge leap forward.



LEO:  Sigh.



STEVE:  Wow.  It's clear that, unfortunately, what we have at the moment is an extremely fragile system.  The problem is the extreme secrecy surrounding the private keys which create the Passkeys.  It's true that they do need to be guarded.  Unfortunately, at the moment they're being jealously guarded.  How Microsoft could possibly imagine that it's practical to have all of a user's Passkeys locked up in a single machine, unable to synchronize with any of a user's other devices is beyond me.



But we're ready to entertain the second part of Dave's question, where he asked:  "Can the passkeys be paired across two user accounts, thereby ensuring recovery in case of loss with only three keys?"  He says:  "My mental model said it made sense, but I do not know for sure.  Can the same key be applied to two different people?  To assure full backup protection, can all three keys be coded into both users?"



The answer is that not one of those operations Dave is asking for is available.  Not one.  And what's more, I just double-checked.  As we learned last week, Yubico's YubiKeys have the most ample storage for Passkeys of any hardware Passkey dongle in the industry, and even it is limited to a total of only 25.  And they are utterly and absolutely non-exportable.  A YubiKey is at its heart an HSM, a hardware security module.  The internal YubiKey dongle hardware contains a very high-entropy random number generator that's used to synthesize a unique private key.



That private key never leaves the device.  There is no way to export it.  Exportation does not exist.  There's no way to put a Passkey in, and no way to take a Passkey out.  This would not be a problem if sites were to allow multiple passkeys to be registered for a single account.  And there's no reason that would not be possible.  But how many sites today support the use and management of multiple passwords for a single account?  I've never seen one.  So it's unclear why support for multiple passkeys would ever be created, even though nothing prevents it.



With YubiKeys having a 25-passkey limit, other than for experimentation, they seem most practical for higher-end enterprise-grade security applications, and perhaps for eventually signing into only a few of the most secure sites where the inconvenience of having an absolute hardware-lock is warranted by its ultimate level of hardware-level security.  And as we noted last week, a YubiKey might be used to unlock a password manager, which is where, we would all have to conclude, all of a user's Passkeys should probably be stored.



The only sane conclusion we can draw is that, while this is all very interesting, none of this is yet ready for prime time.  Poke at it, experiment with it, but wait until Bitwarden's Passkey-supporting mobile clients emerge from their current beta-testing state, at which point it will be practical to start depending upon Passkeys because they will be in a single, sane, multiplatform client.  And Bitwarden, which is we should say a sponsor of the TWiT Network, will likely be offering backup and support and exportation of those once the security protocol for doing that, which is reportedly underway within the FIDO Group, is concluded.



So Bitwarden will then generate and hold our Passkeys, even when other Passkey clients on iOS and Android might be trying to.  And then of course, as we said last week, the challenge is making sure that your chosen Passkey authenticator is universally used, even in an environment where multiple authenticators are all vying for attention.  So I have to say it's the things, reasonable things that people would want to do are not available.  They cannot be done.



LEO:  God.  Wow.



STEVE:  Yeah.  Yeah.



LEO:  Yeah.  I saw there was a Hacker News story about - somebody wrote about why it's a hundred times harder to implement Passkeys on your website than you might imagine.  I mean, it's just - I think this is going to be - I feel like people are going to throw up their hands and say, okay, fine, never mind.  And that's depressing.



STEVE:  Right.  Right.  And as we said last week, if it doesn't achieve critical mass, then it'll just be, well, exactly as one of our listeners said - or no, no, it was the guy who did the Rust WebAuthn client.  He said, "I feel that this will, you know, it'll be like ad blockers.  A small percentage of people take the trouble to do it, but it's sort of a niche, and it never really becomes a problem for ad companies."  And in this case it just never takes hold.



LEO:  So speaking of...



STEVE:  I mean, it is a mess.



LEO:  It is a mess.  And it's not getting any better.  This did not solve it.  We've been trying.  I mean, I remember when Microsoft tried the single sign-on thing 20 years ago.  We've been trying to solve this.



STEVE:  And they had something called Passports.	



LEO:  That's what I was talking about, Passport, exactly.  It was a single sign-on.  And it didn't get adopted, and that's that.  And...



STEVE:  Yup.



LEO:  Oh, well.  Oh, well.



STEVE:  So one last piece of feedback from Willie Scott.



LEO:  Before you do that, can I ask the other question about assembly language?



STEVE:  Yeah.  Yeah, yeah, yeah.  Yeah.



LEO:  I was thinking the other day about how one debugs in a higher level language.  You'll write a print statement, for instance, and it'll tell you all your stuff.  You must have some macros you've written over the years to help you debug assembly.  Or do you?



STEVE:  No.



LEO:  I knew it.  I knew it.  You just write it right the first time.



STEVE:  Well, so for - not for debugging.  But, for example, one of the reasons it would be difficult for me to share my assembler is that I have built up a macro archive of things I do.



LEO:  Oh, I'm sure you have; right.



STEVE:  For example, I use a macro called "zero," and it takes a register name.  Well, it simply expands to XOR register comma register.



LEO:  Right, to zero it out.



STEVE:  Because you know when you XOR something, exactly, when you XOR something with itself you get zeroes, and it's very fast because it doesn't depend upon a memory fetch or the previous data or the previous contents of the register.  So, and the point is, if I wrote XOR something comma something, I would have to look at it and say, okay, XOR, and then look at what am I doing, and then realize, oh, I'm wanting to zero that.  Well, it's much better if I just say zero and then the thing.



LEO:  Right.  Right.



STEVE:  So anyway, and you cannot do that for variables, that is, the Intel architecture will not allow you to XOR memory with another memory.  You can only XOR a register with a register or a register with memory, but not memory with memory.  So when I have a variable, I use the macro "reset," which moves a zero into it.



LEO:  But you don't have any macros for kind of displaying the contents of the stack, purely for debugging?  You don't have anything like that?  You just look at the code and figure out what's going on?



STEVE:  Oh, no, no.  So I definitely have a debugger.



LEO:  Oh, good, okay.



STEVE:  Oh, yeah, yeah, yeah.



LEO:  MASM comes with a debugger; right?  Or no?



STEVE:  So MASM doesn't, but there were back in the day a bunch of third-party debuggers.



LEO:  Right.



STEVE:  I use something called Periscope, which was written by a guy named Brett Salter years ago.



LEO:  I remember that, yeah.



STEVE:  He passed away a few years ago.  There was also something called SoftICE.  And ICE stands for In Circuit Emulator.  And in the really old days you would pull the processor off the motherboard and plug in this paddle that then had a cable running to a bunch of things that emulated the processor that allowed you essentially to get inside the processor.



LEO:  Wow.  That's wild.



STEVE:  So that was called an ICE, an in-circuit emulator.  And so SoftICE was essentially using protected mode to do all the same sorts of things.  So there have absolutely always been debuggers.  And one of the banes of developing for SpinRite was that I'm DOS and 16 bits.  And it was very difficult to create an environment where I was able to have networking in order for my code to get down into the target machine and debugging at the same time.  So one of the things I'm really looking forward to as I move to my own environment is, for example, this RTOS32 that will be the home for SpinRite 7.  It works with Visual Studio transparently.  So I get to just live in a really nice GUI IDE and do all of my debugging.



LEO:  Oh, that's nice.



STEVE:  And what's really cool, Leo, I bought so many motherboards and so many random hard drives through eBay when our testers were reporting that on my Gimcrack 27Z it does such-and-such.



LEO:  Right, right.



STEVE:  And it's like, oh, my god.  So I'd have to go look.  I'd go to eBay, search for a Gimcrack 27E and...



LEO:  And buy one.



STEVE:  There's one, yeah, and I would buy one.  And my amazing wife put up with having motherboards everywhere.



LEO:  All over the dining room table, I'm sure.



STEVE:  So what's very cool about RTOS32 is it allows Internet, trans-Internet debugging.



LEO:  Oh, nice.



STEVE:  So if something is happening on that guy's Gimcrack 27Z, I'll be able to actually have him contact me and debug it on his machine.



LEO:  Wow.  Oh, that's really cool.  Wow.



STEVE:  Yeah.



LEO:  Very neat.  All right.  Okay.  So you have some pretty good tools, it sounds like.



STEVE:  Oh, yeah.  And in fact one of the things that I've learned is invest in your tooling infrastructure before you do anything.  It is so nice...



LEO:  Absolutely.



STEVE:  ...to have a convenient debugging environment.



LEO:  Absolutely.  On we go.  I'm sorry, I didn't mean to interrupt.  I was just curious.  I was debugging the other night, and I was thinking, I wonder how Steve does this.  So now I know.



STEVE:  Yeah.  You absolutely have to have a good debugger.



LEO:  Oh, yeah, absolutely.



STEVE:  That allows you to see the stack and the contents of the registers and what's in memory and what your local variables are.  All of that is made really very nice with Visual Studio.



LEO:  Nice.



STEVE:  Okay.  Willie Scott.  He says, okay, he has some feedback and advice about the operation of the iCloud keychain.  And I bet you he knows what's going on.  Or at least gave us enough of a clue.  He said:  "Hi, Steve.  In regards to your discussion of Passkeys on last week's show, the part about the author's partner losing her iCloud Keychain passwords intrigued me.  After the LastPass hack, I decided to switch to using iCloud Keychain for my passwords because I'm in the Apple ecosystem and wanted to start using Passkeys instead of passwords wherever possible.



"I'm writing to mention that I, too, have had passwords and two-factor authentication codes wiped from my iCloud Keychain..."



LEO:  Oy.



STEVE:  Uh-huh, exactly, "...although my Keychain has never been fully wiped, like the poor partner's Keychain did.  As near as I can tell, I believe I know the culprit of why it may be wiping credentials from iCloud Keychain and wanted to pass this along to anyone who might still be using iCloud Keychain to store their passwords, or who knows somebody who may.



"When I started changing all my passwords and adding accounts into iCloud Keychain, I noticed that an old Amazon password that I don't use anymore was already stored in there, probably from when the Amazon app asked, 'Do you want me to remember your password?'  It was an old password that I don't use anymore, so I deleted it.  However, a couple of days later, I noticed that even though I deleted that password, or so I thought, it had somehow reappeared in my iCloud Keychain.  Not only that, but I also noticed that one or two accounts that I had recently added to the Keychain were missing.  And this process repeated itself a few more times.  So that's when I started investigating.



"While digging through the settings, I went through my Apple ID account settings, and that's when I realized that my old iPhone 6S Plus, which was running an old version of iOS - iOS 14 to be exact - was still signed into my iCloud account and had iCloud Keychain turned on.  I removed that old iPhone from my iCloud account.  And ever since I did that, no passwords have been wiped.  If you're in an Apple ecosystem, it's always a good idea to keep your devices up to date, but it might also be a good idea to do some spring cleaning and remove old Apple devices from your iCloud that you don't use anymore.



"Having said all that, I sadly was agreeing with a lot of the points you were making about Passkeys.  And I think I've decided that I will probably switch over to Bitwarden once Passkeys become officially supported in Bitwarden, using," and he says,  "https://bitwarden.com/twit, of course."



LEO:  Thank you.  That's our special sponsor link, yeah.



STEVE:  Which I think we're about to talk about.



LEO:  Yes, actually.



STEVE:  "Thank you for a great show.  I look forward to it each week.  I'm also a proud SpinRite owner and can't wait to start using 6.1 on my SSDs and a troubled hard drive."



So this mysterious iCloud credential removal has all the feel of something Apple would be deliberately doing out of their typical abundance of caution.  I'll bet there's a security model behind it.  For example, while an older iPhone is also signed into an account's iCloud Keychain, Apple might be deliberately limiting what they're willing to save into that shared Keychain while an older and presumably lower-security device also shares access.  In other words, it's a feature, not a bug.



LEO:  I guess it could be that.  I don't like that kind of unexplained behavior, however.



STEVE:  It sounds like Apple, though, to say, oh, we're not going to let you hurt yourself.  We're going to delete the keys you've just saved because otherwise one of your insecure devices might get them.



LEO:  Ay ay ay.



STEVE:  Yeah.



LEO:  I'll be sure to - you always should remove old devices.  That's maybe why I've never run into this.  I always remove the old devices.  So, hmm.  Very interesting.



STEVE:  Yup.  I do happen to have an iPhone 6 right here.



LEO:  Wow, look at that.



STEVE:  That doesn't work anymore.



LEO:  Look at that home button and think fondly on it because Apple has, as of today, discontinued all the devices that had home buttons.  The last one you could buy was the iPad base model, and that's now been superseded.  So the home button is officially a thing of the past, as is the headphone jack, I think.  I think...



STEVE:  Is it all facial recognition?



LEO:  Yeah.  It's all Face ID now.



STEVE:  Makes sense.  Today's podcast is titled "Not So Fast" because that's the absolutely best way to characterize what's going on in the United Kingdom with Google.  As we know, during our podcast two weeks ago Leo dropped the news that Google's third-party cookie deprecation would not be happening as had been long planned for this summer.  And of course I was getting all excited about that because, you know, I've been on this third-party cookie thing for a long time.  I think it was in 2008 I created that whole cookie forensics facility.  GRC understands which types of assets carry cookies and which ones are first-party and third-party and everything.



I mean, and there were, back then, browsers were not handling cookies correctly.  When you turned them off, sometimes they didn't get turned off.  Or turning them off would keep new ones from being stored, but would not cause old ones to start getting blocked.  And there was just all kinds of screwy things that were going on.  So this has been a hobbyhorse of mine for decades.



So it is the case that the abandonment and deliberate blocking of all third-party cookies and other web-tracking hacks represents such a dramatic sea change for the web that, I get it, many understandably skeptical observers doubt it can or ever will actually come to pass.  And, you know, we've been abused for so long it's difficult to imagine that could ever end.  So, self-confessed technology fanboy that I am, I wanted to determine what was going on.  Were some stuffed-shirt bureaucrats somewhere going to screw this all up?



When I went to take a look at that for last week's podcast I quickly became lost in a paper shuffle.  I decided that whatever was going on was worthy of understanding, since I consider this single forthcoming change, that the largest browser maker in the world by far wants to make, to be one of the most important things that's going on today.  That and the question about, you know, are we going to keep our conversations encrypted in messaging apps, which the EU seems determined to say no to.  As I've previously said, this represents a complete - what Google is doing represents a complete reconceptualization of the way the Internet will finance itself going forward.  And we could have it soon.



So the news that Leo had picked up on came in the form of an announcement that left actually more questions than it answered.  On the 23rd of last month - which was, you know, Tuesday before last - on their PrivacySandbox.com site, Google posted under the headline "Update on the plan for phase-out of third-party cookies on Chrome."  That's very clear.



Their brief introduction said:  "The UK's Competition and Markets Authority" - known as the CMA, and we'll be using that acronym a lot here, or abbreviation - "and Google publish quarterly reports to update the ecosystem on the latest status of Privacy Sandbox for the Web.  As part of Google's first-quarter 2024 report, we will include the following update" - that is, in the report - "about the timeline for phasing out third-party cookies in Chrome in the April 26th report."



Okay.  So the update, very short, it simply reads:  "We are providing an update on the plan for third-party cookie deprecation on Chrome."  They said:  "We recognize that there are ongoing challenges related to reconciling divergent feedback from the industry, regulators, and developers, and will continue to engage closely with the entire ecosystem.  It's also critical that the CMA has sufficient time to review all the evidence, including results from industry tests which the CMA has asked market participants to provide by the end of June."  Okay, now, that means essentially June is when third-party cookies were supposed to be ending; but, you know, things are taking longer than expected.



"Given both of these significant considerations, we will not complete third-party cookie deprecation during the second half of Q4.  We remain committed to engaging closely with the CMA and ICO and we hope to conclude that process this year.  Assuming we can reach an agreement, we envision proceeding with third-party cookie deprecation starting early next year."  So early 2025.  And then they conclude by noting:  "Once published, you will be able to view both Google and the CMA's full reports."  Those reports were published three days later, on April 26th.  So this was on the 23rd they said this.  Surprised the industry.  Three days later on the 26th we got the whole story.



So the entire issue is best described by the following statement:  "On 7 January 2021" - okay, so a little over three years ago.  "On January 7th, 2021, the CMA commenced an investigation under Section 25 of the Act" - some, you know, UK, the equivalent of legislation to prevent monopoly misbehavior, you know, antitrust we have here in the U.S. - "in relation to Google's Privacy Sandbox proposals.  The CMA subsequently informed Google that the CMA was concerned that Google's proposals, if implemented without regulatory scrutiny and oversight, would be likely to amount to an abuse of a dominant position."



So basically a little over three years ago Google says we're going to change the way the Internet is financed.  And among those things we're going to kill off third-party cookies.  There's no question that people in the UK whose income and livelihoods depend upon tracking, like, you know, their data resellers, they said, whoa, whoa, whoa.  We don't want third-party cookies to go away.  We like third-party cookies.  So UK bureaucrats, please tell Google no.  Please tell Google we need those cookies.



Okay.  So I don't know that for a fact.  It's unclear.  And it's frankly not really important to know the genesis of the inquiry, but it's probably something like that.  Since we're talking about the elimination of all third-party cookies and the curtailment of what had become the widespread practice of tracking Internet users around the web as a means of determining their interests, it may well have been the advertising technology companies based in the UK which were crying foul behind the scenes.



LEO:  That's even more exciting, really, yeah.



STEVE:  Yes, yes.  What ensued was about what you'd expect from any healthy and well-established bureaucracy as old and wizened as the United Kingdom.  Experts were - yeah.  Experts, you know, I mean, even the name United Kingdom sort of suggests, oh, crap.



LEO:  [Expostulating]



STEVE:  Experts were found, neutral third-party "monitors" were enlisted, and Google created a document describing the - and, boy, are you going to hear this word - "the Commitments it was prepared to make," with a capital C.  I mean, it sounds religious almost.  These are our commitments.  A document titled "Investigation into Google's 'Privacy Sandbox' browser changes" opens with the assertion that:  "The CMA has accepted commitments offered by Google that address the CMA's competition concerns resulting from investigating Google's proposals to remove third-party cookies and other functionalities from its Chrome browser."  Which begs the question, what exactly are these commitments that the CMA has accepted?



I found the points of concern in the description of the roles of the appointed technical expert that will be supporting the monitoring agent.  The document states:  "On the 26th of September 2022, the CMA approved the appointment of S-RM Intelligence and Risk Consulting Limited by the Monitoring Trustee (ING Bank N.V.) as an independent Technical Expert to support the Monitoring Trustee in monitoring compliance with the following provisions of the binding commitments accepted by the CMA on February 11th, 2022."  Whew.  Okay.  And then the good news is this next line is short.



"Google's use of data (paragraphs 25 through 27), non-discrimination (paragraphs 30 and 31), and, with respect to those provisions, anti-circumvention (paragraph 33), the role of the Technical Expert is to provide specialized knowledge to support the Monitoring Trustee, particularly in relation to monitoring data flows and understanding the possible impacts of the Privacy Sandbox changes on ad tech markets."



Okay.  So we have the ING Bank serving as the neutral monitor, and this monitor has appointed another firm with the required technical expertise.  And everything it focused upon is in a small handful of paragraphs somewhere.  I found out where.  They are in Appendix 1A of the latest version of the "Google's final commitments" document.  The first set of paragraphs, 25 through 27, basically amount to Google promising not to use any personal data from a user's past Chrome browsing history, a customer's Google Analytics account, or to in any way track users.  So that's all pretty much what Google has explained to be its intentions and goals.  So it appears that the CMA just wanted that very clearly and succinctly spelled out.



The non-discrimination, that's paragraphs 30 and 31, state that Google promises to create a totally level playing field.  Having examined, explored, and shared on this podcast the operation of Google's cookie-replacement technologies as they have evolved through the years, this was, you know, it was always clear to me and those who understood this that this was inherently level, the playing field was.  That is, you know, Google was getting a very proscribed amount of information, and everybody was equally - had equal access to it.  It's implicit throughout Google's design, though I have to agree that Google's design has grown to be much better thanks to all the feedback and criticism the various pieces have received through the years.  So yes, it's a good thing we did not get stuck with Google's first idea.  What we've got is something far better than what we would have had if, you know, if there was sufficient scrutiny done.  And there was.



So I can understand how bureaucrats, who will never understand how Google's Topics API functions, need a simple "okay, but what does it mean" spelled out in English.  Since this is crucial to the acceptance of Google's technology, and it's only two paragraphs, I'm going to share them.



Paragraph 30 says:  "Google will design, develop, and implement the Privacy Sandbox proposals in a manner that is consistent with the Purpose of the Commitments and take account of the Development and Implementation Criteria.  Google will ensure that it does not distort competition by discriminating against rivals in favor of Google's advertising products and services.  In particular, Google will not" - and we have three things - "design and develop the Privacy Sandbox proposals in ways that will distort competition by self-preferencing Google's advertising products and services; also will not implement the Privacy Sandbox in ways that will distort competition by self-preferencing Google's advertising products and services; and, finally, also will not use competitively sensitive information provided by an ad tech provider or publisher to Chrome for a purpose other than that for which it was provided."



Then it says:  "For the avoidance of doubt, Privacy Sandbox proposals that deprecate Chrome functionality will remove such functionality for Google's own advertising products and services, as well as for those of other market participants."  That was paragraph 30.  And yes, I mean, that's exactly what Google has said they're going to do.  But essentially what has happened is a legally binding contract has been created that Google - that's what these commitments are which Google is saying they're going to honor.



And paragraph 31 just says:  "Google will not change its policies for customers of Google Ad Manager, Campaign Manager 360, Display & Video 360, or Search Ads 360 to introduce new provisions restricting a customer's use of Non-Google Technologies before the Removal of Third-Party Cookies, unless in exceptional circumstances - such circumstances to be discussed with the CMA - or as required by law.  For the duration of the Commitments, Google will inform the CMA ahead of any such change to these policies."



And this leaves us with the final "anti-circumvention" paragraph 33 which is just a blessedly single line which reads:  "Alphabet Inc., Google UK Limited, and Google LLC will not in any way, whether by acts or omissions, directly or indirectly, circumvent any of the Commitments."  Now, that sort of language will be familiar to any businessman or anyone who's been involved in any contractual agreements where attorneys are engaged.  You know, it's boilerplate; right?  And it's important to understand that both the United Kingdom government and Google's various corporations recognize those provisions to be now contractually and legally binding.



So it has been upon those representations, which are enumerated as "Commitments" with a capital "C," that the UK then proceeded to carefully examine Google's proposal.  So now we return to the timeline for phasing out third-party cookies.  That work appears in a document titled "CMA Q1 2024 update report on implementation of the Privacy Sandbox commitment," dated last month, April of 2024.  Actually it was April 26th.



The document's summary lays out the entire story, and it's interesting enough and short enough to share.  They said:  "This report sets out the CMA's updated views on the issues we identified in our January 2024 report."  So January was the previous report.  So it's basically quarterly; right?  So this is the result of the first quarter.  So this is from January 2024.  Where are we now?  We're in April.  So we've had the first quarter go by.  "Our analysis is based on the framework for assessment set out in the legally binding Commitments that Google made in February 2022 to address competition concerns relating to its proposals to remove third-party cookies from Chrome."  So in other words, yeah, this is a big deal for the entire Internet.  It's a big deal.  "The January 2024 report set out our provisional views on the impact of the Privacy Sandbox on competition, publishers, and advertisers, and user experience.



"We outline Google's response to the concerns we identified in that report, the January report, and the steps it is taking to resolve pending issues.  We've also considered the feedback received from market participants on these points.  We've included a summary of this feedback in the sections below.



"This report also incorporates the preliminary assessment of the ICO, the Information Commissioner's Office, on the privacy and data protection impacts of the Privacy Sandbox.  Having consulted with the ICO, we set out our current views on these concerns for each of the APIs.



"Although there are a number of concerns to work through, based on the available evidence, we consider that from 1st of January 2024 through the 31st of March 2024, the relevant reporting period, Google has complied with the Commitments.  This means that, in our view, Google has followed the required process set out in the Commitments and is engaging with us and the ICO to resolve our remaining concerns ahead of third-party cookie deprecation.



However, further progress is needed by Google to resolve our competition concerns ahead of deprecation.  We will continue to work with Google to resolve our concerns between now and the point at which Google triggers the Standstill Period.  We will provide an update on progress in our next update report.  Testing of the Privacy Sandbox tools is also currently underway.  The test results will form part of a wider evidence base that we will use to assess the effectiveness of the Privacy Sandbox.  The test period runs until the end of June this year."  And as I said before, because this is running through June, that's what kept the cookies from being, you know, for the beginning of the deprecation to start at the end of June.  



They said:  "Given the time needed to resolve outstanding issues and take account of testing results, we have agreed with Google that there should be a limited delay to third-party cookie deprecation.  Subject to resolving our remaining competition concerns, Google is now aiming to proceed with third-party cookie deprecation starting in early 2025.  Under the Commitments, it is for Google to decide when the Standstill Period is triggered.  We encourage market participants taking part in testing to submit their results directly to us by the end of June deadline.  We also welcome any additional feedback from stakeholders on the concerns identified in this report.  Our contact details are included at the end of the report."



Okay.  So one last thing.  This made reference to a "Standstill Period" several times, so I tracked that down.  In the earlier Commitments documents it appears to be just more bureaucracy for its own sake.  It says, on paragraph 19:  "Google will not implement the Removal of Third-Party Cookies before the expiration of a standstill period of no less than 60 days after Google notifies the CMA of its intention to implement their Removal.  Google may increase the length of such a standstill period at any time between giving such notice and the period's expiration.  At the CMA's request, Google will increase the length of this standstill period by a further 60 days to a total of 120 days."



Okay.  So what follows all of that is - that was the document summary.  There are 97 pages of interesting, but ultimately mind-numbing, back and forth detail, as every conceivable facet of this big change Chrome will be implementing is examined under a bureaucratic microscope.  The real concern is over Google's size and whether the changes it is making will disadvantage smaller ad tech players.  But what becomes clear after reading at least some, and that's what I did, I could not go through 97 pages of this, my eyes started to cross and I couldn't see, it is very clear that the UK is moving clearly in Google's direction.



Both parties are truly negotiating in good faith.  That's one thing that also is very clear.  This is not the UK stonewalling and being unreasonable.  It really is, as Leo portrayed, a bureaucratic walrus that just absolutely does not have any idea what is going on.  People are nipping at it, saying this is bad, you can't let Google do this.  So Google is saying this is not bad, this has to happen, we want to stop tracking on the Internet.  People who make their living from tracking are saying, yeah, but we like tracking.



LEO:  Yeah.



STEVE:  Yes.  And so the UK is sort of stuck in the middle. Google is being reasonable.  They are, I mean, there must be a division of Google where they're intoxicated in hot tubs somewhere, just in order to maintain their sanity.  There's no way that the developers are dealing with any of this nonsense because, I mean, ultimately that's what it is.  But the UK needs to be placated through having this explained, you know, what exactly this is and does.  So that's what's happening.  Again, progress is being made.  In the January Report, for example, there was an instance where the ad tech companies were trying to claim that because of their shorter reach, they were being disadvantaged.  The expert looked at it under the watchful eye of the monitor, and now in the April Report the conclusion is, no, that is not the case.  There is no disadvantageous handling based on size of advertiser.  We see no evidence of that.  We understand the technology.  That's not the case.



So it does not appear to me that Google's Privacy Sandbox technology is in any trouble at all.  The truth is, as I've said, it represents a massive change to the way the Internet pays for itself and is going to fund itself in the future.  And it's also true that many companies whose revenue has been entirely derived from the oh-so-slimy practice of tracking users and aggregating their data without our knowledge or permission, for the purpose of selling that data to anybody with a wallet, will be - their income will be impacted.  And not in a good way.



So having read through the documents, I can understand that the process is taking place, and it's taking time.  And in retrospect, you know, though I would have never expected this would happen, it is at least understandable, and it appears that the world will indeed soon be receiving this dramatic change in the way Internet-based advertising is carried out.  It is, you know, clearly far superior to the status quo.  I mean, we can't keep going on the way we have been.  And it takes something no less large than Google to just simply make it an ultimatum.  We are going to do this.  So I understand they've got to satisfy the walruses of the world.  It looks like that process is close to being done.



LEO:  Yeah.  I hope so.  Of course advertisers don't like it.  That's why we like it.  And I think Google, obviously they're trying to balance the interests of both parties because they are - they sell ads.  They buy ads.  It's their business.  It's their revenue.  But they also understand that consumers are not happy, and I think they need to...



STEVE:  No.



LEO:  ...find a way everybody's happy.



STEVE:  And Leo, I'm impressed by the minimization of the information that Google themselves are willing to obtain.  I mean, it's, as we've seen, Topics is not invasive.  They are, you know, no one can be identified from their Topics.  They are chosen at random.  I mean, the system has incredible checks and balances built in which we've talked about on the podcast when we explained it.  And I think we're probably due for a re-explanation when it actually goes into effect because, you know, it's the way the world's going to work.  And I loved the comment about the reason my machine's fans were spinning up was that my Chrome browser, when I was running Chrome, was busy holding auctions with all of the world's ad agencies.



LEO:  Yeah, well, that's coming, maybe, anyway.  We'll see.



STEVE:  It's the only way to do this is to make it user side.



LEO:  Yeah.



STEVE:  You move it to the user, and then the user's browser chooses what they're going to see.  It's brilliant.



LEO:  Yeah.  Makes sense.



STEVE:  I'm going to tease next week's topic, I believe.  I think next week's topic will be ZTDNS, which stands for Zero Trust DNS.  Last Thursday Microsoft published a preview of a forthcoming security solution they call "Zero Trust DNS."  It's been clear for a long time that DNS represents, as we know, both an Achilles heel of network security and a point where it's also very possible, if you're clever, to introduce a significant new level of security.  From my brief scan of the technology Microsoft has outlined, it appears that any of our listeners who may have followed up on my discovery a few months back of ADAMnetworks' DNS solution, which they call "Don't Talk to Strangers," may already be enjoying the benefits of dramatically improved security, thanks to leveraging the power of DNS.  But I needed more time to dig into what Microsoft is doing.  So for next week's podcast I plan to take a deep look into what Microsoft has announced.



Now, one thing I should say that immediately stood out was that Microsoft might be attempting to use this as a way of driving enterprises to Windows 11, since enterprises don't want Windows 11, as we've heard Paul Thurrott mention many times.  No one really does.  And in Microsoft's diagrams which I briefly scanned, they're explicitly labeling the clients as Windows 11 machines.



Now, that just might be Microsoft, you know, because Windows 11 is what they're all using.  You know, since no one actually wants Windows 11, since Windows 10 still commands more than twice the number of desktops as Windows 11, and a much greater percentage within the enterprise because most, you know, new computers come with Windows 11, but enterprise machines that have been running for 10 years don't.  And since a huge install base of machines won't even run Windows 11, if what Microsoft is planning to do is truly a Windows 11-only solution, then the client agnostic system that the ADAMnetworks guys already have working and well-proven seems like a far more practical one to me.



But in any event, by the end of next week's podcast, we'll know exactly what's going on.  And, you know, it's a good thing that Microsoft is stepping up and looking to improve DNS security because we all know it needs it.  But seems to me there's already a solution in place.  But not from Microsoft.  And so when the biggy does it, you know, I remember, Leo, it was fantastic.  Brad Silverberg and Brad Chase came down from Redmond and took me out to lunch and said, Steve, we're going to be announcing DOS 6 pretty soon, you know.  And I said, uh-huh.  And they said, "We're a little self-conscious about this, but we're adding something called Scandisk."



LEO:  Oh.  It's nice of them to warn you.



STEVE:  "Now, don't worry."



LEO:  It won't work well.



STEVE:  "It's doesn't do what SpinRite does."  And I said, "Uh-huh, great.  That's just effin' wonderful."



LEO:  Was that later than Chkdsk?  Because they had Chkdsk.



STEVE:  Yes.  For the rest of our existence we are answering the question, "Well, I already have Scandisk.  What do I need SpinRite for?"



LEO:  Oh, right.



STEVE:  Anyway, the point is, it matters when the giant offers the...



LEO:  It does, yeah.



STEVE:  Oh, it does.



LEO:  Yeah.



STEVE:  I've been there firsthand.  I liked Silverberg a lot.  I never was fond of Brad Chase.



LEO:  You weren't Sherlocked, though, as the Mac people call it.  So that's the good news.



STEVE:  No, Norton did that.



LEO:  Tried.  Do you think he sold more copies of Disk Doctor than you sold of SpinRite?  Probably, huh.



STEVE:  Well, what he did was, when I refused to sell SpinRite to Peter, he sent a developer home with a copy of it and said...



LEO:  Oh, that's not nice.



STEVE:  Oh, yeah.  And we know that because one of my guys looked inside and saw code that was our code.  I mean, there was a place where I needed to see whether the BIOS handled a certain API call.  So I put some specific random data in the registers when I made the call to see whether they got changed.



LEO:  Oh, that's kind of a smoking gun.



STEVE:  Their clone of SpinRite...



LEO:  So the same data.



STEVE:  ...used the same values, the same data, because they didn't know what I was doing.



LEO:  They didn't know, exactly.  They said, well, we'd better do it this way because we don't know if it does something.



STEVE:  Yeah.  The good news is, since they didn't actually create - what was it called?  Calibrate was their clone.  Since they didn't create it, when their customers called for support, they said, "Well, we're not sure.  Call Gibson Research."  I'm not kidding.  We got calls for our support people, "Well, Norton said to ask you about Calibrate."  And we said, "Well, when you buy a copy of SpinRite, we'd be happy to answer all your questions."



LEO:  We'll tell you.  Steve is at GRC and still selling SpinRite, now v6.1, many moons later.  And it's even better than ever.  In fact, now it speeds up SSDs.



STEVE:  Yup.  It's doing really well, actually.



LEO:  Yeah, yeah.  Really, congratulations.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#974

DATE:		May 14, 2024

TITLE:		Microsoft's Head in the Clouds

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-974.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  What fascinating insights do we obtain from examining 3.4 million four-digit PINs?  What plans are already underway as a backup for today's vulnerable GPS technology?  How many Passkeys will websites store per account?  And what's all this about Microsoft promising to get serious about their cloud-based services security?



SHOW TEASE:  It's time for Security Now!.  Steve Gibson's here.  He's ready.  He's champing at the bit, excited to get the show on the road.  He's going to talk about what we learned by examining 3.4 million four-digit PINs.  You guys have some bad habits.  He'll also talk about an interesting old-school approach to solving the GPS fuzzing problem.  And then Microsoft and how they're getting serious about cloud-based services security.  Or are they?  It's a Big Yellow Taxi moment coming up next.  He'll explain on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 974, recorded Tuesday, May 14th, 2024:  Microsoft's Head in the Clouds.



It's time for Security Now!, the show where we cover the latest news from the security-verse, as it were.



STEVE GIBSON:  Second verse, same as the first.



LEO:  There you go.  That's Steve Gibson, the guru around here when it comes to security, privacy, and technology in general.  Hi, Steve.



STEVE:  Yo, Leo.  It's great to be with you again for this May 14th episode of Security Now! as we continue to approach 999.



LEO:  Wow.



STEVE:  I actually had intended to make time to take some of these whiskers off, but I ran out of time, so our audience will have to - those unfortunate enough to be watching the video can just bear with us.



LEO:  You're with it.  That's hip now to have a little bit of a beard and scruff.



STEVE:  Oh, hey.  See, if you just wait long enough...



LEO:  It all comes around.  Are you a three-blade, four-blade, or a five-blade guy?



STEVE:  Oh, god.  There was the greatest piece that SNL did back in the day.  Remember that?



LEO:  That's why I asked.



STEVE:  And the tagline was "Because they'll believe anything."



LEO:  It was their response, I think, to two blades.  But anyway, it was so old that they thought five blades was funny.  And then in fact that's exactly what we've got now.



STEVE:  I think the more the merrier as far as the blades go.



LEO:  All the blades.  All the blades.



STEVE:  That's right.  So, okay.  This is another of those episodes where there are such rich topics to discuss that we've going to do a few of them rather than a gazillion little tiddy bitty topics.



LEO:  Careful there. 



STEVE:  So we're going to look at what fascinating insights we have obtained from the examination of 3.4 million four-digit PINs.



LEO:  Oh, I love this.  I know what you're talking about.  I saw this.



STEVE:  I posted this picture, a heat map that we'll be describing in detail here in a minute, on Twitter yesterday.  Nothing I have ever posted before has generated so many little heart things, "likes" I guess we call them when we're a hipster like I am with my unshaven face.  So that's going to be a lot of fun.



We're also going to look at an interesting surprise which is the plan that is already underway as a backup for today's vulnerable GPS technology which we talked about, we opened the show with last week, was like looking at what Russia is doing over in the Baltics and the vulnerability that we may not be taking seriously enough.  It turns out we've got an answer for that.



Also there was a lot of feedback from our listeners who are avid Passkeys users about their experiences.  I want to share some of those and essentially correct the record about one aspect that was wrong from last week.



And then we're going to take, as the title of today's podcast suggests, which is "Microsoft's Head in the Clouds," a look at a topic that everybody else in the industry has already covered, but we haven't yet here.  And we're going to do it in our own way, as we always do, which is Microsoft's promise to get serious about their cloud-based services security.  What happened?  What has been found?  And we have an interesting take, as we always do.  So I think our listeners are going to have a great podcast.  You know, surprise. 



LEO:  Surprise, surprise.  You're going to have a great podcast.  Well, we're very excited about that.  Meanwhile, let me talk about one of our great sponsors as we get ready.  Picture of the Week coming up, as well.  All right.  I'm ready for picture time.



STEVE:  So, yeah, this is just a quick simple cartoon.  We've got two people sitting behind their laptops.  One is sort of curious about what the other one is doing.  And she looks over at his screen and says, "What are you doing on the dark web?"  And his reply is, "I forgot my password, so I'm looking it up."



LEO:  Of course.  The NSA knows.  And it's not been the hackers.  Right.



STEVE:  That's where you'll find your password, on the dark web.



LEO:  You bet, you bet, you bet.  That's awesome.



STEVE:  Okay.  So this, as I said, this is just a great chart.  This is from the Information Is Beautiful project, which, you know, demonstrates that, if you graph things in creative ways, you can learn a lot.  And this is a perfect example of that.  3.4 million four-digit PINs which were obtained from multiple data breaches were aggregated.  Now, you know, this is a wonderfully enlightening graphic chart that I want to share.  Unfortunately, the terms "graphic chart" and "listeners" are at odds.



LEO:  You're going to have to describe it, Steve.



STEVE:  Yeah.  I'm going to note that this delightful chart is at the top of this week's show notes.  I tweeted it.



LEO:  Yeah.  You need to see it.



STEVE:  And I gave it a permanent GRC shortcut of pin, P-I-N.  So anybody can see it at any time from grc.sc/pin, P-I-N.  Okay.  But, you know, I can do this verbally also.  Okay.  So this chart, as I said, takes 3.4 million four-digit PINs which were recovered from, and disclosed by, multiple data breaches.



Now, of course a four-digit PIN can have any value between 0000 and 9999.  So there are 10,000 possible PINS.  And this wonderful chart contains 10,000 little itty-bitty squares arranged in a flat two-dimensional map.  So it's got rows, you know, it's got 100 rows and 100 columns.  And of course 100 times 100 is 10,000.  So one way to think of this is that the first two digits of the PIN - which, you know, 00 through 99, specify one axis, and the last pair of digits specify the other.  So every single possible four-digit PIN has its own square on this chart.  And within this 3.4 million PIN dataset, the relative number of times every single possible PIN appears in the dataset determines the brightness of its square on the chart.



Okay.  So what do we learn from this?  Okay.  Possibly the most prominent feature is a bright diagonal line running from the lower left corner of the chart, where both of the first two and the last two digits are 00, to the chart's upper right corner, where the first two and the last two digits are both 99.  The diagonal line, then, is formed by all of the intermediate squares where their first two and last two digits are identical.  And naturally like 00 in the far lower left, that's bright because a lot of people just chose 0000 as their PIN.  And similarly, the very far upper right corner, also very bright because 9999 is many people's PIN.  So there is some variation in the brightness along the diagonal which is interesting.  You know, and of course human nature being what it is, the PIN 6969 appears to be overrepresented relative to its neighbors.  No surprise.



Two other solitary bright spots would also not surprise anyone.  They are the locations of the 1234 and 4321 PINs.  Not very creative, and thus bright on the map.  Another really interesting prominent line is the 20th line up from the bottom.  Since lines are numbered from 0, the 20th line is the line for all PINs beginning with 19.  And what's so interesting is that the line gets gradually brighter as it moves to the right, then dims a bit toward the end and wraps around a bit to the 20 line on the left. So what's going on here?  Well, if you guessed people's birth year, you would be correct.  PINs often begin, it turns out, with 19 and they appear to be brightest somewhere around 1980 seems to be the place where it's most, you know, most people have their PINs clustered there.



LEO:  A lot of 40 year olds.



STEVE:  Exactly.



LEO:  I would have thought it would be the baby boomers that would be the brightest, but maybe not.



STEVE:  Yeah, it's kind of fading out for us, Leo.  And on the other hand, then so are we.



LEO:  Yes.



STEVE:  Yeah.  Another notable feature is a generally brighter region down at the lower left of the chart.  This would be where both the first two and the last two digits form low numbers.  Okay.  Why?  Because people used their month and day of birth within the month, running from 1 to 12 of course for the month, and then day of month 1 through 31.  And what's interesting, there's a brighter horizontal stopping at 12 than the vertical stopping at 12, both which however are clear.  This indicates that most people chose the ordering with the month first and the day of month second as their PIN.



Now, stepping way back from it and looking at the overall illumination, there's a top-to-bottom brightness variation, with it being brighter at the top and dimmer toward the bottom, suggesting that most PINs have low starting numbers.  But there's less left-to-right variation.  So people are generally choosing four-digit PINs with, as I said, smaller first two digits, but for some reason more randomly distributed last two digits.



And the final really interesting observation is that whereas most of the chart shows varying shades of illumination, there are around 40 distinct cells that are black or nearly black.  Like, I mean, dramatic contrast against their neighbors.  In other words, out of all 10,000 possible four-digit PINs, there are around 40 of those that are significantly underrepresented.



LEO:  Isn't that weird.



STEVE:  Isn't that?  It's so odd.



LEO:  Yeah.



STEVE:  For some reason...



LEO:  It looks kind of randomly distributed, but maybe not.



STEVE:  Yeah, well, most of them have high...



LEO:  They're mostly above 60, yeah.



STEVE:  Yeah.  Almost all of them are in the upper third of the chart, so their first two digits are larger.  For some reason, for example, very few people have chosen 6806.  So if you're looking for a lesser chosen four-digit PIN, there you go.



LEO:  They're all in there.



STEVE:  That's right.



LEO:  Or 68 whatever this one is.  You know, it's interesting.  There are three dots on the 68 line.



STEVE:  Yeah.  And in fact that first one on the 68 line was the 6806 that I just chose to highlight.  But you're right.  And looks like there's also three on the 60 or on the 70 line.



LEO:  Yeah.  Huh.  I mean, how odd is that?



STEVE:  It's really non-random in that area.  Okay.  So, and as for the extremely low entropy skewing observed in the dataset, again, low entropy skewing, get this.  Just the top 20, the top 20 most used PINs out of, remember, 10,000 that are possible, right, just the top 20 account for 27% of all PINs observed in use.



LEO:  Oh.  Oh, that's terrible.



STEVE:  Those top 20 are 1234, 0000, 7777, 2000, 2222, 9999, 5555, 1122, 8888, 2001 1111, 1212, 1004, 4444, 6969, 3333, 6666 1313, 4321, 1010.



LEO:  If any of those sound like your PINs, you're in trouble.



STEVE:  Yeah, just very, very...



LEO:  It means you can guess, you know, 10 or 20, and have a one in four chance of being right.



STEVE:  Right.  If, for example, something prevented you from brute-forcing all 10,000, you would absolutely want to go for those 20 as your first 20 guesses.



LEO:  It also means you should use more than four digits in your PIN; right?



STEVE:  Yeah.  So I think we're still at four-digit PINs purely for historical reasons.  It's just, you know, it's because that's, you know, once upon a time we didn't have computers, and people had to actually remember them.  And I'm sure all the people used, you know, their month and day of birth, or the last four digits of their Social Security number, or digits from their license plate or, you know, something.  The point being four digits was all they could actually remember.  We didn't have technology to say, oh, yeah, I know, here's a string of 20 digits, you know, repeat after me.



LEO:  Pick something, you know, what I always do is I pick the last four digits of a phone number, not my current phone number, but maybe my childhood phone number or phone number I particular remember because those are mostly pretty random.  They certainly don't have anything to do with my birth date.  I don't know.  Or just pick something random.  You can remember four digits.  Or better yet, use an alphanumeric password, not a PIN.



STEVE:  Yeah.  Well, and of course back once upon a time - or, no.  I was going to say "once upon a time" we were keying them into our touchtone phones in order to authenticate ourselves.



LEO:  Oh, yeah, right.



STEVE:  But even then, unless you used Q - I think was Q missing?



LEO:  Q was missing; that's right.



STEVE:  There are a couple things that were not there on our models.



LEO:  Well, you know where these are mostly still used is on ATM machines.  I don't know of any ATM machine that uses more than four digits.



STEVE:  Yeah.



LEO:  Right? 



STEVE:  Again, because there's some backend, some old creaky backend machine that can only take four digits.  Anyway, this was a huge win for our audience, who got a big kick out of it.  So again, if you want to see what we were talking about, grc.sc/pin, and that will bounce you over to my site.  I grabbed the - I actually could have just pointed to it.  The original source was over on Reddit, and that got tweeted to me.  But I was afraid that that might not last, you know, it could disappear.  So I grabbed it and stuck it on GRC's server just because it's just such a cool infographic.



Okay.  We started off last week with the piece in Wired about the growing threat to GPS.  While the mischief Russia has been getting up to in the Baltic region is quite localized, we also noted that space is, sadly, not necessarily a benign environment anymore.  A piece of our listener feedback which was generated by this discussion last week led me to look at what's being done about this.



Shaun Merrigan wrote.  He said:  "Steve, regarding SN-973 and GPS vulnerability, the U.S. is testing an updated version of the LORAN system which was shut down in the 1980s, called eLORAN.  I've been monitoring the eLORAN test signals on 100kHz since August of 2023.  My ancient LORAN receivers woke up and started giving me timing signals output again at that time, and have been receiving continuously ever since."



Okay.  So this note from Shaun got me to poke around a bit, and I quickly learned that, indeed, there is an acute recognition of the inherent vulnerability of any satellite-based navigation system.  LORAN is an abbreviation for Long Range Navigation, and the "e" in eLORAN stands for "enhanced."  The original LORAN dates back from World War II.  It's a ground-based navigation system that operates entirely differently from GPS.  And of course entirely differently is what you want in something that's going to withstand an attack on GPS.  You want something very orthogonal to the thing that you're trying to create a second solution for.



I found an interesting summary on the site GPS World.  The article's title was "eLORAN:  Part of the solution to GNSS vulnerability."  Under the heading "Opposite and Complementary," the article leads with:  "Though marvelous, GNSS are also highly vulnerable.  eLORAN, which has no common failure modes with GNSS, could provide continuity of essential timing and navigation services in a crisis."



So here's what they explain.  They said:  "GPS fits Arthur C. Clarke's famous third law:  'Any sufficiently advanced technology is indistinguishable from magic.'  Yet, it also has several well-known vulnerabilities including unintentional and intentional RF interference, the latter known as jamming; spoofing; solar flares; the accidental destruction of satellites by space debris; and their intentional destruction in an act of war; system anomalies and failures; and problems with satellite launches and the ground segment.



"Over the past two decades, many reports have been written on these vulnerabilities, and calls have been made to fund and develop complementary positioning, navigation, and timing, which are collectively referred to as PNT - Positioning, Navigation, and Timing - PNT systems.  In recent years, as vast sectors of our economy and many of our daily activities have become dependent on GNSS, these calls have intensified.  A key component of any continent-wide complementary PNT would be a low frequency, very high power, ground-based system because it does not have any common failure modes with GNSS" - collectively meaning satellite based - "which are high frequency, very low power, and space-based.  Such a system already exists, in principle.  It is LORAN, which was the international PNT gold standard almost 50 years prior to GPS becoming operational in 1995.  At that point, LORAN-C was scheduled for termination at the end of 2000.



"However, beginning in 1997, Congress provided more than $160 million to convert the U.S. portion of the North American LORAN-C service over to enhanced LORAN.  In 2010, when the U.S. LORAN-C service ended, it was almost completely built out in the continental United States and Alaska.  During the following five years, Canada, Japan, and European countries followed the United States' lead in terminating their LORAN-C programs.  Today, however, eLORAN is one of several PNT systems proposed as a backup for GPS."



Okay.  So first of all, it's great news that the U.S. has been seriously looking into a backup technology.  Since I think our listeners will find this interesting, I'll share a little bit of background:  "In the 1980s," this author writes, "I used LORAN-C to navigate on sailing trips off the U.S. East Coast.  It had an accuracy of a few hundred feet and required interpreting blue, magenta, black, and green lines that were overprinted on nautical charts."  And we'll get to why that is here in a minute.  "The system was a modernized version of what was originally launched in 1958, a radio navigation system first deployed for U.S. ship convoys crossing the Atlantic during World War II.  Its repeatability was greater than its accuracy.  Lobster trappers could rely on it to return to the same spots where they'd been successful before, though they may have had some offset from the actual latitude and longitude.



"By contrast, eLORAN has an accuracy of better than 20 meters, and in many cases better than 10.  It was developed by the U.S. and British governments, in collaboration with various industry and academic groups, to provide coverage over extremely wide areas using a part of the RF spectrum protected worldwide.  Unlike GNSS" - which is to say GPS - "eLORAN can penetrate to some degree indoors, under very thick canopy, underwater, and underground.  And it is exceptionally hard to disrupt, jam, or spoof.  Unlike LORAN-C, eLORAN is synchronized to UTC and includes one or more data channels for low-rate data messaging, added integrity, differential corrections, navigation messages, and other communications.



"Additionally, modern LORAN receivers allow users to mix and match signals from all eLORAN transmitters and GNSS satellites in view.  For the eLORAN system to cover the contiguous United States, between four and six transmission sites could provide overlapping timing coverage, and 18 transmission sites could provide overlapping positioning and navigation."



Okay.  The article quoted Charles A. Schue, the CEO of UrsaNav. He said:  "Think of a resiliency triad, consisting of GNSS, global; eLORAN, continental; and an inertial measurement unit with a precise clock.  It is extremely difficult to jam or spoof all three sources of location and time at the same time, in the same direction, and to the same amount."  In other words, great for protecting ourselves.



So it's cool that Shaun's ancient LORAN receivers woke up and began picking up LORAN signals.  I don't know where he's located, but the intention is to cover the continental U.S. with multiple overlapping transmitters.  The author of that article:  "It had an accuracy of a few hundred feet and required interpreting blue, magenta, black, and green lines that were overprinted on nautical charts."  Right?  Why these fancy charts?



Imagine for nautical navigation, so you're out on the ocean somewhere, that two synchronized radio transmitters have been placed on the coast, several hundred miles apart.  These two stations both emit a pulse of radio frequency energy at precisely the same time, and the pulses radiate outward spherically from each station at the speed of light, so 186,000 miles per second.  So the ship at sea will receive these two pulses, but it does not know when they were sent.  So it doesn't know its distance from these transmitters.  The only thing it knows is the relative timing separation between them when they arrived.



Now, you can get out a pencil and paper and play with this a bit, but the LORAN system is called a "hyperbolic positioning system" because any given pulse separation describes a hyperbola.  In other words, when a ship received a pair of pulses, their relative spacing would tell the ship's navigator which of many possible hyperbola, plotted on their navigational charts, the ship was currently sitting on.  It would not yet have any way of knowing where it was sitting along that hyperbola, but it would have that one piece of information.  The ship would get a fix on its position along that hyperbola by tuning to a different pair of transmitters.  It would get another pulse spacing, which would identify another hyperbola on the navigation chart, and its location would be at the intersection of the first and second hyperbola.



So that's the way we located ourselves back during World War II.  The good news is that today we have far more advanced technology with integrated circuits and fancy computers that can do all of this for us.  But what hasn't changed is the decision to use low frequency, high power terrestrial transmitters to provide precise timing and location data as a backup for GPS.  It's dispiriting to imagine that we might need it, but what's been going on over in the Baltics with Russia and GPS probably helped to get those projects funded here in the United States.  So just a little very cool bit of technology.



LEO:  That is really interesting, yeah, very cool.



STEVE:  Yeah.  Hyperbolic positioning system.  And on that note let's take a non-hyperbolic break, and then we're going to talk about Passkeys.



LEO:  Yes, indeed.  Coming up, we cover security from A to Z with this guy right here, Steve Gibson.  And on we go with the show.



STEVE:  So, okay.  A number of bits of feedback from our listeners.  Jeff Urlwin, he said:  "Just listened to SN.  Passkeys are even worse based upon website implementation.  Some sites" - get this - "use a cookie to 'know'" - he has in air quotes - "they issued you a Passkey.  So even with 1Password, which supports and synchronizes among Passkeys," or among browsers, he says, "I can't use the Passkey from a different browser than originally set."  He says:  "CVS pharmacy is one with this bad implementation.  Thanks for all your great shows."



RG tweeted:  "Regarding Passkeys, for what it's worth, every website I have set up with a Passkey has let me set up multiple Passkeys, so I have not been limited to a single ecosystem."



Lachlan Hunt tweeted:  "Regarding what you said in Episode 973 about Passkeys, you'll be happy to hear that every single account for which I've been able to register a Passkey and store in 1Password has been able to support registering multiple Passkeys.  For some of my most important accounts, I've registered additional Passkeys stored on my YubiKeys.  In my experience, storing Passkeys in 1Password has been fantastic.  The only major issue I've encountered has been with certain sites, for example PayPal and LinkedIn, that do browser sniffing to unnecessarily prevent Passkeys from being used within Firefox.  This can usually be worked around by simply spoofing the User-Agent string."  But again, you know...



LEO:  Why would you do that?  That's weird.



STEVE:  We've talked about poor implementations.  So poor implementations certainly exist.  And Miguel Frade said:  "Hi, Steve.  In SN-973 you read Dave Brenton's questions about using a backup YubiKey.  To complement your answer, I'd like to share my personal experience.  I've owned two YubiKeys for several years, one with me all the time, and a backup stored in a safe place.  Some services, like Gmail, GitHub, and Bitwarden, allow us to register more than one YubiKey.  In case of Bitwarden's Family plan it allows registering up to five YubiKeys.  I guess it should be the same for Bitwarden's individual premium plan.



"Unfortunately," he writes, "PayPal only allows registering one YubiKey.  Regarding the question 'Can the same key be applied to two different people,'" he says, "the answer is yes, if we're talking about the physical key YubiKey.  Each service will use one of the 25 available slots inside the YubiKey, regardless of the person owning the account."  He finishes, "I hope this information can be useful to other SN listeners.  All the best, Miguel."



Okay.  So last week's discussion of this generated, as I've said, significant feedback from our listeners.  And the thing that stood out more than anything was that everyone showed a somewhat different set of facts.  Some said that WebAuthn/FIDO2 providers would allow any number of Passkeys to be registered with a service.  Some said that only one could be.  And others like Miguel noted that this varied by provider, with PayPal, for example, only allowing for a single registration.  If this were true, it would mean that separate "his" and "her" YubiKeys could not be used with some services.  But all other listeners noted that they had never encountered a site that did not allow for any number of Passkey registrations.  And doing so is part of the Passkeys specification.  So all sites should.



I went over to PayPal to take a look, and their Passkeys management page makes it very clear that they support multiple Passkeys without any trouble.  However, PayPal appears to only support Passkeys generated by iOS and Android devices.  Its FAQ is quite clear about that, and there's no mention of YubiKeys.  So perhaps that's what stopped Miguel.  He didn't actually register a Passkey at all over on PayPal.  He was using PayPal's longstanding and much older...



LEO:  Football.



STEVE:  ...multifactor authentication.



LEO:  Football.



STEVE:  Right.



LEO:  My first football, yeah.



STEVE:  The multifactor authentication over on PayPal.  But this further demonstrates the mess that we're currently working through.  The fact that something stopped Miguel even though he has a perfectly secure authentication device, arguably even though smartphones are now very secure, but you can argue that - you can make a strong argument that the YubiKey being so focused and single purpose and simpler, you know, and doesn't have multiple radios hooked to it, is more secure than the two smartphone brands that PayPal does support.  But this all shows that we're still in the early days of this technology.  You get a YubiKey which supports Passkeys.  PayPal supports Passkeys.  But PayPal won't support a Passkey generated by a YubiKey.



One thing that all the feedback made very clear was that many of our listeners have jumped into the Passkeys world with both feet.  They like them, and I think that's great.  Really.  I think that those of us in the industry who are grousing at the moment - and Paul Thurrott, for example, went on a nice rant again about this last week.



LEO:  Easily triggered.



STEVE:  Yes.  He's doing so, well, no, all of the users are doing - those of us in the industry are ranting because we're disappointed with the rollout and are impatient for Passkeys to live up to their potential.  We know that change takes time, and that this is still the very early days for this new technology.  Browser and browser extension support for original username and password authentication has created a system that's mostly good enough for now, with second-factor authentication adding additional protection where needed.  Your football, Leo.



None of us can predict the future, and today's Passkeys support remains really disappointing.  But in the grand, you know, if nothing else, in the fact that so few sites, you know, have jumped on the bandwagon.  On the other hand, why would they if it's not urgent for them?  But in the grand scheme, relative to how slowly new technology is adopted, Passkeys only became available yesterday.  Once the various kinks are ironed out and any device we wish to use can supply a previously generated Passkey to a website, the traditional problems with passwords will begin to fade.



I think that the most compelling use case of all is the typical user, you know, and there are a lot more of those typical users than there are listeners to Security Now!, the typical user who has no interest whatsoever in any of this.  They could care less.  They're using an iOS or Android smartphone, a Mac or Windows device having strong biometric hardware authentication.  They visit a site which newly supports Passkeys, and the site says:  "Hey.  How would you like to never need to use a username or password to log in with this device ever again?"  You know, who's not going to click yes?  Any regular user will think, "That's great.  Passwords are annoying as hell.  If I don't need to use one here anymore, count me in."



Presumably, and this is what remains unknown, whether and to what extent additional sites will offer this support over time.  If it does succeed in setting a new standard, then Passkeys will just gradually and organically seep into the world and become the way Internet users authenticate.  I think, you know, we're excited by the potential, those of us who are into the technology, and we want it to happen immediately.  But it's just going to take some time.  And clearly a lot of the listeners of this podcast have been curious about this.  And mostly their experiences have been all good, which I think is great.



Okay.  Microsoft's Head in the Clouds.  SC Magazine's headline read:  "Sweeping cybersecurity improvements pledged by Microsoft," and follows with "Numerous cybersecurity incidents" - I'm sorry.  Numerous cyber secure - but there were incidents.  But they wrote:  "Numerous cybersecurity enhancements..."



LEO:  Enhancements in response to incidents, that's what it was.



STEVE:  Uh-huh, "...will be adopted by Microsoft to address the woeful security failures driven by poor cybersecurity practices and lax corporate culture identified in a report issued by the Cyber Safety Review Board last month."  And SecurityWeek carried the headline:  "Microsoft Overhauls Cybersecurity Strategy After Scathing CSRB" - that's the same, the Cyber Safety Review Board - "Report."  And then they follow with "Microsoft Security Chief Charlie Bell pledges significant reforms and a strategic shift to prioritize security above all other product features," basically saying we're going to stop with the features here.  Although one could argue that they're not stopping with their AI push.  But otherwise, we're really prioritizing security.



Now, anyone who's been following this podcast for the past year will have heard me go off on Microsoft over their truly astonishing apparent lack of concern or accountability over egregious security practices.  Doing so always leaves me feeling a bit odd, since I'm sitting in front of Windows machines, all of my coding for the PC has been for Microsoft operating systems, from DOS through desktop and server, and I love the Windows working and development environments.



But as we've clearly documented on this podcast over and over, security researchers repeatedly hand Microsoft every detail, complete with working proofs of concept demonstrations, for various vulnerabilities which Microsoft will seemingly ignore for months and even years until that vulnerability is actually used to cause a highly public catastrophe.  And only then will Microsoft apparently think, "Huh.  Why does that exploit path have a familiar ring to it?"  Right.  You know?  And we understand why; right?  Microsoft is a monopoly. You cannot build a large modern enterprise without Microsoft glue.  Too many things require Microsoft.  So the simple fact is, Microsoft does not have to care.  And we've seen example after example of Microsoft not doing anything it does not want to do.



All of this makes Microsoft's recent pronouncements about their new focus upon security all the more interesting.  Two weeks ago, the "Cybersecurity Dive" site posted an article with a headline that caught my eye.  They wrote:  "At Microsoft, years of security debt come crashing down," and the subhead was "Critics say negligence, misguided investments, and hubris have left the enterprise giant on its back foot."



They wrote:  "Years of accumulated security debt at Microsoft are seemingly crashing down upon the company in a manner that many critics warned about, but few ever believed would actually come to light.  Microsoft is an entrenched enterprise provider, owning nearly one-quarter of the global cloud infrastructure services market and, as of the first quarter last year, nearly 20% of the worldwide Software as a Service application market.  Though not immune from scandal, in the wake of two nation-state security breaches of its core enterprise platforms, Microsoft is facing one of its most serious reputational crises.  Adam Meyers, Senior Vice President at CrowdStrike, said:  'It's certainly not the first time a nation-state adversary has breached Microsoft's cloud environments.  After so many instances, empty promises of improved security are no longer enough.'"



Okay.  Now, to review a bit, in January, Microsoft said a Russia-backed threat group called Midnight Blizzard gained access to emails, credentials, and other sensitive information from top Microsoft executives as well as certain corporate customers and a number of federal agencies.  We're going to see that the numbers were actually somewhat worse than that.  Then in early April, the federal Cyber Safety Review Board released a long-anticipated report which showed the company failed to prevent a massive 2023 hack of its Microsoft Exchange Online environment.  The hack by a People's Republic of China-linked espionage actor led to the theft of 60,000 State Department emails and gained access to other high-profile officials, actually many.



Just weeks ago, CISA issued an emergency directive to order federal civilian agencies to mitigate vulnerabilities in their networks, analyze the content of stolen emails, reset credentials, and take additional steps to secure Microsoft's Azure accounts.  While the order only applies to Federal Civilian Executive Branch agencies, CISA warned other organizations could be impacted.  For many critics of Microsoft, the events of the past nine months are the logical conclusion of a company that has ridden the wave of market dominance for decades and ignored years of warnings that its product security and practices failed to meet the most basic standards.



"AJ Grotto, the director of the Program of Geopolitics, Technology, and Governance at the Stanford Cyber Policy Center and a former White House Director for Cyber Policy said:  'In a healthy marketplace, these would be fireable offenses.  Regrettably, the marketplace is far from healthy.  Microsoft has the government locked in as a customer, so the government's options for forcing change at Microsoft are limited, at least in the short term.



"The concern was, and is, that Microsoft's security gaps would potentially lead to catastrophic outcomes.  According to Karan Sondhi, CTO at Trellix:  'Microsoft needs to dedicate its internal resources towards zero-trust initiatives and make new investments in its infrastructure.  Currently,' he says, 'Microsoft directs the vast majority of their security investments toward revenue-generating roles instead of internal security roles.'"  And we'll come back and talk about that here toward the end.



"Microsoft has a considerable stake in the cloud security space.  Not only is Microsoft one of the world's largest cloud providers, but according to Microsoft's CEO Satya Nadella during the company's fiscal second quarter conference call in January, 'It is also a major security provider to the enterprise.  Microsoft has more than one million security customers, with 700,000 using four or more of its security products.  Microsoft generates more than $20 billion in revenue per year from its security business."  In other words, by selling security that one could argue ought to be baked in.



Okay, now, I should note here for the record that I don't have any feelings at all of schadenfreude.  Really.  I'm not the least bit happy that it took some seriously frightening and damaging security lapses within Microsoft to get them to finally start thinking about taking security seriously.  It would have been better for everyone if those breaches never occurred.  But, unfortunately, all evidence suggests that nothing would have changed at Microsoft ever but for those breaches.  So the way things have been going, it was probably inevitable.



The trouble they've fallen into feels like the result of a cycle, a cultural cycle within Microsoft.  We've witnessed such cycles within Microsoft in the past.  I think that happens when a company grows so much that it keeps creating very wealthy upper management, who then, no longer needing to work, eventually leave the company.  But they're not the only things that leave.  What leaves with them is their deep understanding of the culture their leadership created while they were there.  Those who replace them think they know how to keep everything running.  But not having created it, they lack the same deep experience-based understanding of what's important.  And then, over time, the ship drifts off course.  Since I cannot even conceive of captaining a ship the size and complexity of Microsoft, it doesn't surprise me that it might lose its way from time to time; you know?  I'm amazed it's still afloat.  



Early last month, the Department of Homeland Security's CSRB, as I've mentioned, the Cyber Safety Review Board, released their findings following a deep and detailed investigation into Microsoft's recent security breach troubles.  I'm going to share the summary of that report from the Cybersecurity Dive people.



LEO:  On we go with the sad tale of Microsoft finally waking up to the security needs.  This, by the way, was not their first time.



STEVE:  No.



LEO:  At this rodeo.  They keep waking up to security requirements.  It's like, one more memo.  Okay.  That'll fix it.



STEVE:  Yeah.  And as we'll see here when I sort of summarize and wrap this all up, that it's not clear what this means, but we can hope.



LEO:  Pray.



STEVE:  Yeah.  The thing that this article from - I've lost my cursor - from the Cybersecurity Dive brings is some additional color and quotes and background from other people.  They wrote:  "The CSRB report laid out a blistering assessment of a corporate culture that has failed for years to take cybersecurity seriously.  The report was designed to assess the company's response to the summer 2023 breach from the People's Republic of China-linked threat actor that breached the company's Microsoft Online Exchange environment.  However, it also laid out a security culture that failed to adhere to the most basic standards, given the enormous market power that Microsoft yields across modern business applications in government and the private sector.



"One of the more damaging findings was that Microsoft learned of the attacks only because the State Department had set up an internal alert system after purchasing from Microsoft at additional cost a G5 license.  Customers who failed to purchase the enhanced security license were not able to see the extensive logging capabilities that would have alerted them to a breach."  And we'll get back to the implications of that also.



"Many in the security community see the CSRB report and the recent CISA emergency directive as direct indictments, not only of Microsoft's security culture, but a government that has allowed Microsoft to maintain lucrative government contracts with no fear of competition across many of its services.  Mark Montgomery, senior director at the Center on Cyber and Technology Innovation at the Foundation for Defense of Democracies said:  'The federal government gets off the hook a little easy in this report.  Despite significant encouragement from outside experts, the Biden administration  and its predecessors have failed to treat cloud computing as a national critical infrastructure that is itself critical to maintaining the security of our other national critical infrastructures.'"  So it's a, you know, a critical infrastructure infrastructure.



"Senator Ron Wyden, who called for a federal investigation following the State Department email hack, said the federal government shared responsibility for the negligent behavior disclosed in the report.  Wyden said Microsoft has been rewarded with billions of dollars in federal contracts, while not being held to account for even the most basic security standards."  Wyden told the author of this article:  "The government's dependence on Microsoft poses a serious national security threat, which requires strong action."  Now, think about that for a minute.  "The government's dependence on Microsoft poses a serious national security threat."  I know that the practice of politics generates a great deal of rhetoric, but that's not something you want a well-placed and respected U.S. Senator saying about your company.



And speaking of rhetoric, Microsoft knows how to play the game with the best of them.  "Microsoft officials said they understand the larger concerns raised by the summer 2023 attacks, as well as the continued threat from Midnight Blizzard and other nation-state actors.  The company is working to make extensive changes in its engineering processes, improve its relationships with the security community" - wow, listen to the security community?  What a concept - "and its responsiveness to customer needs.



"Bret Arsenault, corporate VP and chief cybersecurity advisor at Microsoft, said in a statement:  'We're energized and focused on executing Microsoft's Secure Future Initiative commitments.  And this is just the beginning.  We commit to sharing transparent learnings..."



LEO:  They love that word, I don't know why.



STEVE:  Oh, god, I hate that word.



LEO:  I know.



STEVE:  "...transparent learnings."  There's got to, you know, I thought, Leo, isn't there a better word?  But you've turned an activity into a noun.



LEO:  Right.



STEVE:  And I guess there's no helping you.



LEO:  There's no helping you.



STEVE:  After you've done that.  Yes.  I just wish that some of Microsoft's customers would have some walkings, and would walk away.



LEO:  From earnings.



STEVE:  Yeah, my god.  Okay.  So interestingly, one of the problems with being transparent about what's being fixed is that the process of enumerating all the improvements also serves to enumerate just how bad things had been allowed to become.



LEO:  Oh, yeah, exactly.



STEVE:  Uh-huh.  Listen to these numbers.  Bret said that since the launch of the company's Secure Future Initiative, the company has sped up related engineering work in several areas.  Okay, he calls it a "speed up."  Well, he lists four.  He says:  "Microsoft has accelerated the lifecycle management of tenants, with a focus on either unused or older systems.  The company eliminated more than 1.7 million Entra ID systems related to used, aging, or legacy technology."  In other words, there were 1.7 million Entra ID systems that could be eliminated, but had not been.  They were just, you know, hanging around, waiting to be abused.  "It has also made multifactor authentication enforcement automatic across more than one million Entra ID tenants."  Which, again, says that they weren't before.



Also:  "More than 730,000 apps have been removed across production and corporate tenants that were either out of lifecycle or were no longer meeting current standards."  Nearly three quarters of a million apps were just, again, you know, left alone.  Left there.  Even though they were no longer serving any purpose.  As we know, fundamental to security is taking an employee's badge and then removing all their passwords from the system before they have a chance to use them.  Three quarters of a million apps were left there.



Also Microsoft said:  "New employees and vendors are now being given short-term credentials to make impersonation and credential theft more difficult.  More than 270,000 have been implemented thus far."



And finally:  "The company's internal multifactor authentication implementation using Microsoft authenticator has been enhanced by eliminating a call feature and relying on an in-app login feature.  This change covers more than 300,000 employees and vendors."  Again, 300,000 employees and vendors were using an insecure feature of the multifactor authentication that likely made it easier to use, but was less secure.  So, okay.  Gee.  I guess we should fix that.



Okay.  So I've observed for some time here on the podcast that one of the reasons Microsoft has been acting the way it has, has been able to act the way it has for so long without correction, is that until now its negligence had no consequence, exactly as Senator Ron Wyden observed.  For this article, Dante Stella, an attorney at Dykema and a specialist in incident response, said that enterprise customers do not usually walk away in the face of nation-state threats against Microsoft, in part due to its enormous presence as a cloud provider.



Dante was quoted:  "Many switched to Exchange Online or Microsoft 365 to get away from on-prem servers and managed service providers.  If the only other choice is going 'back,' or a potentially disruptive switch to another platform like Google Workspace, they will most often just ride it out and trust Microsoft to fix the issues."  Right.  The customers may be unhappy; but, due to Microsoft's dominance in the market, that unhappiness is never reflected in Microsoft's bottom line.  So why change anything?



As we know, I always want to go to the source.  So after reading this piece I was curious to see the report from the Cyber Safety Review Board.  Now, the full report I'm not going to share.  It's 34 pages of quite eye-opening content.  But the short Executive Summary at the start paints the picture.  Here's what the review board found.  This is the actual report from this CSRB.



They wrote:  "In May and June 2023, a threat actor compromised the Microsoft Exchange Online mailboxes of 22 organizations and over 500 individuals around the world.  The actor known as Storm-0558, hereinafter simply as Storm, and assessed to be affiliated with the People's Republic of China in pursuit of espionage objectives, accessed the accounts using authentication tokens that were signed by a key Microsoft had created in 2016."  In other words, that key had never expired or been rotated in seven years.



Okay.  They say:  "This intrusion compromised senior United States government representatives working on national security matters, including the email accounts of Commerce Secretary Gina Raimondo, United States Ambassador to the People's Republic of China R. Nicholas Burns, and Congressman Don Bacon.



"Signing keys, used for secure authentication into remote systems, are the cryptographic equivalent of crown jewels for any cloud service provider.  As occurred in the course of this incident, an adversary in possession of a valid signing key can grant itself permission to access any information or systems within that key's domain.  A single key's reach can be enormous, and in this case the stolen key had extraordinary power.  In fact, when combined with another flaw in Microsoft's authentication system, the key permitted Storm to gain full access to essentially any Exchange Online account, anywhere in the world.  As of the date of this report, Microsoft does not know how or when Storm obtained the signing key.



"This was not the first intrusion perpetrated by Storm, nor is it the first time Storm displayed interest in compromising cloud providers or stealing authentication keys.  Industry links Storm to the 2009 Operation Aurora campaign that targeted over two dozen companies, including Google; and the 2011 RSA SecurID incident, in which the actor stole secret keys used to generate authentication codes for SecurID tokens, which were used by tens of millions of users at that time.  Indeed, security researchers have tracked Storm's activities for over 20 years.



"On August 11, 2023, Secretary of Homeland Security Alejandro Mayorkas announced that the Cyber Safety Review Board (CSRB, or the Board) would 'assess the recent Microsoft Exchange Online intrusion and conduct a broader review of issues relating to cloud-based identity and authentication infrastructure affecting applicable cloud service providers and their customers.'



"The Board conducted extensive fact-finding into the Microsoft intrusion, interviewing 20 organizations to gather relevant information.  Microsoft fully cooperated with the Board and provided extensive in-person and virtual briefings, as well as written submissions.  The Board also interviewed an array of leading cloud service providers to gain insight into prevailing industry practices for security controls and governance around authentication and identity in the cloud."  In other words, they really did look at the entire industry in order to support their conclusion that Microsoft stood out as negligent.  This wasn't common practice, what Microsoft was doing, the way Microsoft was operating.



They wrote:  "The Board finds that this intrusion was preventable and should have never occurred.  The Board also concludes that Microsoft's security culture was inadequate and requires an overhaul, particularly in light of the company's centrality in the technology ecosystem and the level of trust customers place in the company to protect their data and operations.  The Board reaches this conclusion based on seven points.  One, the cascade of Microsoft's avoidable errors that allowed this intrusion to succeed; second, Microsoft's failure to detect the compromise of its cryptographic crown jewels on its own, relying instead on a customer to reach out to identify anomalies the customer had observed."



And I'll just take a moment here to say, elsewhere they explain:  "The State Department was the first victim to discover the intrusion when, on June 15th, 2023, State's security operations center (SOC) detected anomalies in access to its mail systems.  The next day, State observed multiple security alerts from a custom rule it had created, known internally as 'Big Yellow Taxi,' that analyzes data from a log known as MailItemsAccessed, which tracks access to Microsoft Exchange Online mailboxes.  State was able to access the MailItemsAccessed log to set up these particular Big Yellow Taxi alerts because it had purchased Microsoft's government agency-focused G5 license that includes enhanced logging capabilities through a product called Microsoft Purview Audit Premium.  The MailItemsAccessed log was not accessible without that premium service."



LEO:  Oh.  This is why Microsoft gets in trouble, because they demand you pay for security.



STEVE:  Exactly.



LEO:  But wait a minute.  Big Yellow Taxi is the name of the tool?



STEVE:  Big Yellow Taxi is the name they gave to the intrusion detection rules for determining whether the Microsoft Exchange online mailboxes were being maliciously accessed.



LEO:  It's the name of a Joni Mitchell song.  But I don't really know why they used that.  That's weird.  Okay.



STEVE:  Big Yellow Taxi.  The Big Yellow Taxi alert went off, Leo, and they thought, uh-oh.  Something's wrong.



LEO:  Okay.  It's hard to take that seriously, I'll be honest with you.



STEVE:  That's why we normally don't get those internal names exposed to the public.



LEO:  Yeah, yeah.



STEVE:  Yeah.  When we find out that the State Department has named their intrusion rule "Big Yellow Taxi," it's like, uh.



LEO:  Somebody's a Joni Mitchell fan.  That's all I can say.



STEVE:  Is it a miracle you guys discovered this?  Anyway.  Also, they said, "The Board's assessment of security practices at other cloud service providers, which maintained security controls that Microsoft does not.  Fourth, Microsoft's failure to detect a compromise of an employee's laptop from a recently acquired company prior to allowing it to connect to Microsoft's corporate network in 2021."  So a compromised laptop was hooked up to Microsoft's network after Microsoft acquired a company, and that was a problem.



Also, number five:  "Microsoft's decision not to correct, in a timely manner, its inaccurate public statements about this incident, including a corporate statement that Microsoft believed it had determined the likely root cause of the intrusion when in fact it still has not," meaning even to this day.  And "Even though Microsoft acknowledged to the Board in November 2023 that its September 6th, 2023 blog post about the root cause was inaccurate, it did not update that post until March 12th of 2024, as the Board was concluding its review, and only after the Board's repeated questioning about Microsoft's plans to issue a correction."  In other words, what?  Oh.  Oh, you mean, what we immediately said back in September?  Yeah, we've been meaning to change that.  But, gee, you know, we just haven't gotten around to it.



Number six:  "The Board's observation of a separate incident, disclosed by Microsoft in January of this year, 2024, the investigation of which was not in the purview of the Board's review, which revealed a compromise that allowed a different nation-state actor to access highly-sensitive Microsoft corporate email accounts, source code repositories, and internal systems."



And, finally, number seven:  "How Microsoft's ubiquitous and critical products, which underpin essential services that support national security, the foundations of our economy, and public health and safety, require the company to demonstrate the highest standards of security, accountability, and transparency."  As opposed, obviously, to the lowest.



"Throughout this review," they wrote, "the Board identified a series of Microsoft operational and strategic decisions that collectively point to a corporate culture that deprioritized both enterprise security investments and rigorous risk management."  Deprioritized.  "To drive the rapid cultural change that is needed within Microsoft, the Board believes" - and I love the fact that here's the government telling - this board on the government suggesting how Microsoft should run things.  They said:  "The Board believes that Microsoft's customers would benefit from its CEO and Board of Directors directly focusing on the company's security culture and developing and sharing publicly a plan with specific timelines to make fundamental, security-focused reforms across the company and its full suite of products.  The Board recommends that Microsoft's CEO hold senior officers accountable for delivery against this plan.



"In the meantime, Microsoft leadership should consider directing internal Microsoft teams to deprioritize feature developments across the company's cloud infrastructure and product suite until substantial security improvements have been made in order to preclude competition for resources."  In other words, if you don't have enough people available to fix your security, why don't you just hold off on all those new improvements that you were planning and get your security house in order first.



LEO:  Yeah.



STEVE:  How would that be?



LEO:  Yeah.



STEVE:  Because, you know, national security and billions of dollars in contracts that we keep providing to you and rolling over year after year after year.



LEO:  And Big Yellow Taxi.



STEVE:  How about fixing?



LEO:  Yeah.



STEVE:  How about fixing some things?



LEO:  Yeah.



STEVE:  That's right.  Okay, Leo.  Let's take our last break.



LEO:  Okay.



STEVE:  Then I want to look back at mainframe computing and why where we are is like where we were then.



LEO:  Oh, that's interesting.  A little bit of history to tie into the present.



STEVE:  Yup.



LEO:  I like it.  Now let us conclude our journey down the highway of insecure operating systems with Steve Gibson.



STEVE:  So one of the earliest breakthroughs in computing was the introduction of a concept that came to be called "timesharing."



LEO:  Yes.



STEVE:  Back then, mainframe computers were incredibly expensive to purchase and operate.  A single machine installation was planned years in advance.  Electrical power and cooling was plumbed.  Large rooms were set aside.  And these machines had their own staff and managers.  The bean counters, who occupied the upper floors, quickly realized that their costs were the same, whether or not the monstrously expensive machine in the basement was busily working for them or sitting idle.  So the question soon became how do we keep this massive investment of ours busy?  And the answer was timesharing.



Timesharing meant that a great many people could share the machine's time.  This worked because most people spent most of their time staring at the screen of their timesharing terminal reading what had just been displayed, deciding what to do next, and then slowly punching out the next command they wished to issue.  If it had been just one person, the mainframe would have been bored to death.  But the bean counters perked right up when they learned that their machine in the basement could keep thousands of their employees, literally everyone in the building, busily poking away at their keyboards and never waiting long for their next screen of data to be presented.



Most of the company's thousands of employees never visited the basement.  They weren't allowed to.  Security was high because too much was at stake.  All of the company's jewels had been concentrated into a single small region, and those who had privileged access wore white coats and prominently displayed ID tags.  To most of the rest of the company, these tenders of the machine did not appear to speak English, and what exactly they did down there in the basement was shrouded in rumor and mystery, with some not appearing to emerge for days on end.



I've painted this picture of the past because it's interesting that it's a close approximation of what has gradually and organically re-evolved today, mostly of its own accord.  Part of it is upside down because, instead of computing being done in the basement, today it's being done in the clouds.  But we have a very similar concentration of value into a small, high-security, tightly controlled area to which few people have access.  And the concept of resource sharing exists pervasively.  Thanks to the miracle of the global Internet, the networking wires that interconnect the servers are literally being shared by everyone in the world.  And the use of virtual machine technology, which shares physical processor resources among a great many more virtual processors, is the essence of timesharing.  No single virtual machine needs to, or can, keep a high-powered cluster of processor cores completely busy; so a much larger number of virtual machines can simultaneously share that single powerful resource with many others.



This move to the cloud does not feel like yet another phase.  This feels like an inevitable evolution.  Earlier I noted that Dante Stella had been quoted saying:  "Many switched to Exchange Online or Microsoft 365 to get away from on-premises servers."  I think this represents an inevitable evolution because, just as happened in the past era of mainframe computing, the computational resource we were able to create far outstripped the needs of the typical user.  Today's processors are so powerful that most PC users today are only using a small fraction of their system's capabilities.  When this is scaled up to an enterprise of 10,000 employees, the wasted resources are astonishing.  Since most people today are, just as they were 50 years ago, staring at a screen, taking the time to figure out what it says, then poking away at their keyboard to indicate what they want to do next, we've returned to the mainframe era, and what we're sharing are cloud-based resources.



And I'll just note that the recent evolution of interactive cloud-based AI models represents another example where sharing a single massive resource among many users is vastly more economical than giving each user their own instance.  And even though local mini-models can be used, thanks to our astonishing computing power, the best models will be continuously training, which requires massive connectivity and a far greater level of processing.



Okay.  So how did Microsoft get into trouble?  There's that old observation, which I've heard isn't actually true, but it makes for a great example nevertheless, that if you toss a frog into a pot of boiling water it will immediately jump out.  But if the frog is placed into cold water and the temperature is slowly increased, it won't notice the change.



What this report makes clear is that the world has awoken to just how utterly dependent we have become upon computing in the cloud.  It happened so gradually, so incrementally and slowly, with one day following after the next, with one company after another deciding that the economics of moving their communications infrastructure into the cloud made the most sense, that, just as with the apocryphal frog, we've arrived at a position where the security of our cloud computing can no longer be considered an afterthought, and it can no longer be taken for granted.



I initially skipped past the opening statement from the chair and deputy chair of the CSRB's report because now we have some context that they had when they wrote it.  They said:  "It is not an exaggeration to say that cloud computing has become an indispensable resource to this nation and, indeed, to much of the world.  Numerous companies, government agencies, and even some entire countries rely on this infrastructure to run their critical operations, such as providing essential services to customers and citizens.  Driven by productivity, efficiency, and cost benefits, adoption of these services has skyrocketed over the past decade; and, in some cases, they have become as indispensable as electricity.



"As a result, cloud service providers (CSPs) have become custodians of nearly unimaginable amounts of data.  Everything from Americans' personal information to communications of U.S. diplomats and other senior government officials, as well as commercial trade secrets and intellectual property, now resides in the geographically distributed data centers that comprise what the world now calls the 'cloud.'



"The cloud creates enormous efficiencies and benefits; but, precisely because of its ubiquity, it is now a high-value target for a broad range of adversaries, including nation-state threat actors.  An attacker that can compromise a CSP can quickly position itself to compromise the data or networks of that CSP's customers.  In effect, the CSPs have become one of our most important critical infrastructure industries.  As a result, these companies must invest in and prioritize security consistent with this 'new normal,' for the protection of their customers and our most critical economic and security interests."



So, getting back to your comment, Leo, what will all this mean to Microsoft, and what will it mean to us?  I have no idea, and neither does anyone else.  For one thing, big changes take time.  What Microsoft's rhetoric promises is a major reorganization of their corporate priorities.  They're saying this because it has become clear to everyone that a major reorganization of their corporate priorities is exactly what will be needed.



I want to conclude our look at this by sharing the report of Microsoft's actions once the State Department's "Big Yellow Taxi" honked its horn, indeed noting that there was a problem.  I want to share it because it reads like a detective novel, which I know our listeners will enjoy; and because, while it's part of the same scathing report, it paints Microsoft in a good light and shows what this behemoth is capable of doing when it wants to, or maybe needs to.



The report wrote:  "Though the alerts showed activity that could have been considered normal - and indeed State had seen false-positive Big Yellow Taxi detections in the past - State investigated these incidents and ultimately determined that the alert indicated malicious activity.  State triaged the alert as a moderate-level event; and on Friday, June 16th, 2023, so coming up on a year ago a month from now, its security team contacted Microsoft.  Microsoft opened and conducted an investigation of its own, and over the next 10 days ultimately confirmed that Storm-0558 had gained entry to certain user emails through State's Outlook Web Access.  Concurrently, Microsoft expanded its investigation to identify the 21 additional impacted organizations and 503 related users impacted by the attack and worked to identify and notify impacted U.S. government agencies.



"Microsoft initially assumed that Storm had gained access to State Department accounts through traditional threat vectors, such as compromised devices or stolen credentials.  However, on June 26th, 10 days after the initial alert, Microsoft discovered that the threat actor had used OWA (Outlook Web Access) to access emails directly using tokens that authenticated Storm as valid users.  Such tokens should only come from Microsoft's identity system, yet these had not.  Moreover, tokens used by the threat actor had been digitally signed with a Microsoft Services Account (MSA) cryptographic key that Microsoft had issued in 2016.  This particular MSA key should only have been able to sign tokens that worked in consumer OWA, not Enterprise Exchange Online.  And this 2016 MSA key was originally intended to be retired in March of 2021, but its removal was delayed due to unforeseen challenges associated with hardening the consumer key systems."  Whatever that means.



"This was the moment that Microsoft realized it had major overlapping problems.  First, someone was using a Microsoft signing key to issue their own tokens; second, the 2016 MSA key in question was no longer supposed to be signing new tokens; and, third, someone was using these consumer key-signed tokens to gain access to enterprise email accounts.  According to Microsoft, this discovery triggered an all-hands-on-deck investigation by Microsoft that ran overnight" - oh my god, Leo, somebody lost some sleep over this.



LEO:  Well, maybe not.  Maybe they just ran it overnight, went to bed.



STEVE:  Oh, that's possible, you're right.  "It ran overnight from June 26th into June 27th, 2023, focusing on the 2016 MSA key that had issued the token, as well as the access token itself.  By the end of that day, Microsoft had high confidence that the threat actor was able to forge tokens using a stolen consumer signing key.  Microsoft then escalated this intrusion internally, assigning it the highest urgency level and coordinating its investigation across multiple company teams.  As a result, Microsoft developed 46 hypotheses to investigate, including some scenarios as wide-ranging as the adversary possessing a theoretical quantum computing capability to break public-key cryptography, or an insider who stole the key during its creation.  Microsoft then assigned teams for each of the 46 hypotheses to try to prove how the theft occurred..."



LEO:  How interesting.



STEVE:  Yeah.



LEO:  What an approach, yeah.



STEVE:  "...prove it could no longer occur in the same way now; and to prove Microsoft would detect it if it happened again.  Nine months after the discovery of the intrusion, Microsoft says that its investigation into these hypotheses remains ongoing."  Another way of phrasing this would be "Microsoft still has no idea exactly how this happened."  They know what, but not in detail exactly how.



The report continues:  "Microsoft began notifying potentially impacted organizations and individuals on or about June 19th and July 4th, respectively.  As detailed below, this effort had varying degrees of success.  Ultimately, Microsoft determined that Storm-0558 used an acquired MSA consumer token signing key to forge tokens to access Microsoft Exchange Online accounts for 22 enterprise organizations, as well as 503 related personal accounts worldwide.  Of the 503 personal accounts reported by Microsoft, at least 391 were in the U.S. and included those of former government officials, while others were linked to Western Europe, Asia-Pacific, Latin America, and Middle Eastern countries and associated victim organizations.



"Microsoft found no sign of an intrusion into its identity system and, as of the conclusion of this review, has not been able to determine how Storm-0558 had obtained the 2016 MSA key.  It did find a flaw in the token validation logic used by Exchange Online that could allow a consumer key to access enterprise Exchange accounts if those Exchange accounts were not coded to reject a consumer key.  By June 27th, 2023, Microsoft believed it had identified the technique used to access victim accounts and rapidly cleared related caching data in various downstream Microsoft systems to invalidate all credentials derived from the stolen key.



"Microsoft believed that this mitigation was effective, as it almost immediately observed Storm beginning to use phishing to try to regain access to the email boxes it had previously compromised.  However, by the conclusion of this review, Microsoft was still unable to demonstrate to the Board that it knew how Storm-0558 had obtained the 2016 MSA key."



So we've already seen that Microsoft has reversed its profit-motivated policy of charging its customers extra for security logging.  We covered that earlier.  And overall, a policy of charging anything extra in return for extra security seems similarly shortsighted.  Security should be baked into all underlying aspects of any cloud deliverable.  It should not be possible to "buy more security."  It should be impossible to purchase less.



Only time will reveal what lessons Microsoft learns from all this.  The lesson we must all learn is that when we transfer our corporate assets to the cloud, we're also transferring the responsibility for the security of those assets to the cloud services provider.  So it's important to recognize that doing so does come with some risk, and that the fine print of the provider's contract holds them harmless, regardless of fault.



LEO:  What a - what a world.  Do you feel like Microsoft's, however, recognized the issue and made the changes they need to make, and we won't have to do this all over again?



STEVE:  Everybody loves a project, Leo.



LEO:  It's like a committee.  It's very similar to a committee.  You know, you don't want to make a decision, appoint a committee.  You don't want to really solve something, create a project.  Maybe 47 of them, working overnight.



STEVE:  Yeah.  Everybody loves a project.  I mean, they need something to do.  You know, you could argue Windows is done.  You know, cloud, you know, Exchange is done.  Everybody, I mean, what is the refrain we hear?  Leave it the eff alone.



LEO:  Yeah, yeah, yeah.



STEVE:  Just why - so Microsoft, yes, put a freeze, a formal freeze on features because they keep breaking it.  Right?  I mean, how many times have we said they're never going to get rid of the bugs because as many as they fix, they introduce new ones.



LEO:  Yes. 



STEVE:  With new features.  Because they're constantly, you know, adding features.  Stop with the features already.  How about considering security a feature?  What a concept.  



LEO:  Yeah.  Good point.  Good point.  This is why you listen to Security Now!, right, every Tuesday, 1:30 Pacific, 4:30 Eastern, 2030 UTC, to get the deets, the update, the straight talk.  That's the really most important thing, without fear or favor.  So many other places on the 'Net you can get information, but there's always this undercurrent of, like, well, you know, who's paying for this?  With Steve, you know.  Steve says what he thinks and is extraordinarily trustworthy, and I love that about this show.  I'm glad you're here.  Glad you like it, too.  Steve is at GRC.com.  That's where you'll get SpinRite.  This is his only, by the way, the only thing he has, his only bread-and-butter winner, the world's finest maintenance and recovery and, what, speed-up utility for mass storage?



STEVE:  Performance recovery.



LEO:  Performance recovery.



STEVE:  Data recovery and performance recovery.



LEO:  Yeah.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION



SERIES:		SECURITY NOW!

EPISODE:	#975	

DATE:		MAY 21, 2024	

TITLE:		312 SCIENTISTS & RESEARCHERS RESPOND

HOSTS:	STEVE GIBSON & LEO LAPORTE

SOURCE:	SN-975.MP3

LENGTH:	115 MINUTES



DESCRIPTION:  Which browser has had a very rough week, and why?  Which bodily fluid should you probably not drink despite Google's recommendation?  And how can you tweak your browser to avoid those in the future?  What happens when a Windows XP machine is exposed to the unfiltered Internet?  Duck and cover!  How did a pair of college kids get their laundry washed for free?  And what do we learn about still-clueless corporations?  And finally, after engaging with some terrific listener feedback, we're going to examine the latest thought-provoking response to the EU's proposed Child Sexual Abuse Regulation from their own scientific and research community.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Of course, as always, there's a ton to talk about.  We will in just a little bit talk about the scientific response to the European CSAM proposals, those proposals that break Internet encryption.  We'll also talk a little bit about three new zero-days in Google Chrome, what happened to Google Search, and why AI is not the answer.  And just how long can an unprotected XP machine live on the Internet?  The answer will surprise you.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 975, recorded Tuesday, May 21st, 2024:  312 Scientists & Researchers Respond.



It's time for Security Now!, the show where we cover the latest security news, privacy information, talk about hacks and hackers and, you know, just kind of shoot the breeze with this super intelligent human being we call Steve Gibson.  Hi, Steve.



STEVE GIBSON:  And continue to justify our existence, apparently.



LEO:  Without Security Now! there's no justification, yeah.	



STEVE:  So, okay.  We're going to have some fun this week, not that we don't always.  We're going to examine which browser has had a very rough week, and why.  Which bodily fluid should you probably not drink despite Google's recommendation to the contrary?



LEO:  Eww.  Okay.



STEVE:  I know.  It's freaky.  And also, how can you tweak your browser so that you will be avoiding those recommendations in the future?  What happens when a Windows XP machine is exposed to the unfiltered Internet?  Duck and cover comes to mind.



LEO:  Oh, boy.



STEVE:  How did a pair of college kids get their laundry washed for free?  And what do we learn about that from the still-clueless corporations which clearly exist?  And finally, after engaging with some terrific listener feedback that we have this week, we're going to examine the latest actually quite thought-provoking response to the EU's proposed Child Sexual Abuse Regulation, this time from their own scientific and research community.  Thus the title of this podcast:  "312 Scientists & Researchers Respond."



LEO:  Awesome.  And four out of five doctors agree that this is the only thing you should be doing on a Tuesday afternoon; all right?



STEVE:  That's right.



LEO:  All right.



STEVE:  To justify your existence.



LEO:  We will get to all that.  I can't wait to hear the washing machine story.  I saw the headlines on that, and I thought, oh, Steve has got to cover this.



STEVE:  Yeah, yeah, yeah, too fun.



LEO:  Very interesting.  Very, very interesting.  And now I am prepared for the Picture of the Week.



STEVE:  So I gave this picture just a simple caption:  "Uh, what?"



LEO:  It says it all, really.  You don't - you don't need...



STEVE:  Exactly.  You know, I was tempted to give it the caption, "I don't think that means what you think it means."  And this is another one of those, Leo, where you've just got to ask yourself, you know, somebody produced this.  Somebody, like, created a plate to put on a door which you can only push in order to open the door.  Yet prominently displayed at the top, beautifully engraved and then color filled, you know, etched in this plate it says "Pull."



LEO:  I think it's for Jedi warriors to, like, practice the Force.  Pull.



STEVE:  That would be good.  Yes, exactly.  You just, you know, work on your telekinesis.



LEO:  Unbelievable.  Wow.



STEVE:  Anyway, I just got just a kick out of that.  Thanks to our amazing listeners.  They find these things and send them to us so I get to share them with everyone.



Okay.  So Google's much-beloved Chrome browser has had a very rough week.  In just one week, the total number of exploited, in-the-wild, zero-day vulnerabilities to be patched so far this year jumped from four to seven.



LEO:  Wow.



STEVE:  In other words, last week saw three newly discovered Chrome vulnerabilities receiving emergency Chrome patches.  In their blog last Wednesday, Google wrote:  "Google is aware that an exploit for CVE-2024-4947 exists in the wild."  This was also separately echoed by Microsoft, who said they were looking into it, and they were going to, like, work on fixing this thing, too, because of course Microsoft is also using the common Chromium engine.



So this latest trouble is rated as a high-severity zero-day vulnerability which results from a type confusion weakness in Chrome's V8 JavaScript engine.  The discovery was made by researchers at Kaspersky Labs when they discovered it being used in targeted attacks.  Now, these so-called "Type Confusion" bugs, we see them are arising often.  They're more formally referred to as "Access of Resource Using Incompatible Type," which sort of says the same thing.  This occurs when code misinterprets data types which can lead to unpredictable behavior - which is putting it mildly - that can allow attackers to manipulate program logic or access sensitive information.



We've talked about before how the values stored in a computer's registers or in memory might either be the actual data itself, or often can be a pointer to some other data.  The use and manipulation of pointers is, wow, I mean, it's very powerful, but also very dangerous because the pointer can potentially point to anything.  So it's not difficult to imagine what would happen if some data that the program was storing, especially if it's data that an attacker is able to manipulate, like for example the length of the data they've just sent, could be mistakenly treated by some buggy code as a pointer, in which case the attacker can control what the pointer points to and thus increase the amount of mischief that they're able to get themselves into.  In theory, that would allow an attacker, for example, to do exactly the sort of things that we see happening.



So as we've observed before, Google understandably sees no upside to revealing more details of their flaws, beyond confirming the reports of them being used in attacks and that they're now fixed.  So, you know, they say update your Chrome and you'll be okay.  And, you know, all they say is "Access to bug details and links may be kept restricted until a majority of users are updated with a fix."  And of course Google knows that by the time everyone has updated, the world will have moved on and won't care about some old bug that's since been fixed in Chrome.  So they sort of say, oh, we're not going to tell you until later, and later never comes.



However, you were just talking about the Thinkst Canary.  And this article talks about this because, or this event, because one thing that comes very clear is that network monitoring has become crucial.  The way and reason Kaspersky is able to discover such attacks is that their customers are running Kaspersky's end-point security solutions, and those solutions are feeding the intelligence that they collect back to Kaspersky's mothership for monitoring and interpretation.  So when one of Kaspersky's customers is targeted, red flags go up at Kaspersky central.



Okay, now, as I said, there were three this past week.  The other two actively exploited Chrome zero-days patched this week are 4671 and 4761, which also double as a test for dyslexia.  4671 is a use-after-free flaw in Chrome's Visuals component, whereas 4761 is an out-of-bounds write bug in, once again, the V8 JavaScript engine.  And it's worth noting that four out of the seven zero-day bugs Chrome has patched so far this year have all been located in Chrome's V8 JavaScript engine.  This is not necessarily the JIT, the Just-In-Time compiler portion.



But recall that the observation has been previously made that the overwhelming majority through time of bugs in the common Chromium core were being found in V8's - in the JIT, the Just-In-Time compiler portion of V8's JavaScript engine.  This is what led Microsoft to explore disabling Edge's Just-In-Time compilation under the theory that a modicum of speed could be sacrificed, especially given how much faster our processors are today than when this was first implemented, back when they really did need all the speed they could get.  Now it's like, well, you know, the processors are sitting around doing nothing most of the time anyway.  So how about trading off some speed in return for cutting serious vulnerabilities by more than half.



Toward the end of last month, Microsoft explained the so-called "Enhanced Security for Edge" setting that they have in their browser.  They wrote:  "Microsoft Edge is adding enhanced security protections to provide an extra layer of protection when browsing the web and visiting unfamiliar sites."  That's the keyword.  "The web platform," they wrote, "is designed to give you a rich browsing experience," blah blah blah, "using powerful technologies like JavaScript.  On the other hand, that power can translate to more exposure when you visit a malicious site.  With enhanced security mode, Microsoft Edge helps reduce the risk of an attack by automatically applying more conservative security settings on unfamiliar sites, and adapts over time as you continue to browse."



They wrote:  "Enhanced security mode in Microsoft Edge mitigates memory-related vulnerabilities by disabling Just-In-Time JavaScript compilation and enabling additional operating system protections for the browser.  These protections include Hardware-enforced Stack Protection and Arbitrary Code Guard.  When combined, these changes help provide 'defense in depth' because they make it more difficult than ever before for a malicious site to use an unpatched vulnerability to write to executable memory and attack an end user."



So Microsoft wound up with a hybrid solution where additional meaningful protections, which will take a modest toll on performance, are being selectively enabled when visiting unfamiliar sites.  But this allows Edge running on, for example, Outlook 365 or Google properties to race ahead at full speed with those extra protective guards disabled.  And given Chrome's past week of three newly exploited in-the-wild zero-days, and the fact that we appear to be unable to secure our web browsers, I think Microsoft's tradeoff makes a huge amount of sense.



Okay.  So this next piece, the fact that Leo has been driven to a paid search solution I think says important things about what has happened to search.  We're going to see some additional evidence of that.  One of the things I most loved about the early Google search was its search results' cleanliness and simplicity.  They were remarkable, not only because they were relevant.  I mean, it was astonishing back then.  Anyway, I'll come back to that in a second.



Everyone knows that my current project is implementing a state-of-the-art email system for GRC.  I hoped to be able to announce this week that the subscription management frontend was ready for the world.  But it needs some additional testing, so that'll be next week's announcement.  I wrote GRC's first email system back in the late 1990s, and it sent over the course of its life a grand total of 11 mailings.  To my surprise, last week I stumbled upon the archive of those 11 emailings.  And the second one, dated April 2nd - not April Fool's Day, fortunately - April 2nd, 1999 had the subject "Steve Gibson's News of a Stunning New Search Engine."



Okay.  So the email that I sent to GRC's subscribers a little over 25 years ago read:  "We've all experienced the problem.  The automated search engines like Alta Vista return 54,321 items 'in no particular order'" - it actually used to say that - "many of which unfortunately were porn sites.  But the human-indexed search services of the time like Yahoo," I wrote, "often cannot find what you want because they're only able to index a small fraction of the entire web since they're being indexed by people.  So you're left with the uneasy, but probably accurate  sense," I wrote, "that what you want is out there somewhere, but you're no closer to finding it."



And then I said:  "The truly amazing new solution:  A couple of extremely bright guys at Stanford University solved the Web Search Engine Problem once and for all, creating the last search system you will ever need."  And then I provided a URL that at that time no one had ever seen:  http: - no "s" back then - http://google.com.  And I wrote:  "What's their secret?  They use Linux-based web robots to explore and index the entire Web.  But then they determine the quality of each resulting link based upon the quality of the other sites that link into that site.  So the only way a site can be highly rated under Google is if other highly rated sites have links pointing into it."



I wrote, "It's brilliant.  This simple concept works so well that every single person I've told about Google has switched permanently to using Google as their web search engine of choice.  It really is that good."  And I said:  "And of course it's free, so give it a shot yourself."  And then my email ended with a link to, again, Google.com, which 25 years ago when I sent this mail on April 2nd of 1999...



LEO:  That's pretty impressive.



STEVE:  ...no one had ever heard of.



LEO:  That's great.



STEVE:  So I just, I thought, I got such a kick out of that.  So, you know, what was fun for me was that 25 years ago Google had just appeared on the scene, and there was barely a "scene" for Google to appear on.  So this really, you know, it was life-changing news that I was able to share with GRC's email list subscribers.  And way back then there was no downside to Google.  But it's been 25 years; and oh, how times have changed.  As I said at the start of this, the fact that you, Leo, have been driven to a paid search solution says some important things.



My own personal annoyance is that I never, I mean, literally, I never want to watch a video to receive an answer to whatever question I might have put into search.  Yet Google promotes videos to the top of their search results, not because they provide the best answer, but because Google owns YouTube now.



LEO:  Exactly, yeah.



STEVE:  Yeah.  I'm writing my forthcoming email system's subscription management frontend because I'm very picky about exactly how I want it to work and how I insist that GRC treats its visitors.  But I have no interest in reinventing the wheel when I have nothing to add.  So I'm using an existing SQL database-driven mailing engine on the backend to actually deliver the mail.  The other day I wanted to bring up the pages of documentation on this package's API, so I entered its full proper name, properly spelled, into Google search.  And I tried it again just now to be sure.  What I received in return, which filled the entire page vertically, thus requiring me to scroll, was four sponsored results for commercially competing products or services.



LEO:  Oh, wow.



STEVE:  And this was not because, as I originally wrote 25 years ago, those four alternative solutions are objectively better, but because they're paying Google to appear first.



LEO:  Right.  They're ads, yeah.



STEVE:  That's right.  Anyway, I know that none of this comes as news to anyone here, but I wanted to lay that foundation since against this background a piece of disturbing news about Google's latest degeneration caught my eye when BleepingComputer brought their readers up to speed.  BleepingComputer's headline Sunday, two days ago, was "Frustration grows over Google's AI Overviews feature, how to disable."



They wrote:  "Since Google enabled its AI-powered search feature, many people have tried and failed to disable the often incorrect AI Overviews feature in regular search results.  Unfortunately, you can't.  However, there are ways to turn it off using the new 'Web' search mode, which we explain below.  AI Overviews, also known as 'Search Generative Experience'" - and I might change it to Degenerative, but we'll get to that later - "is Google's new search feature," they wrote, "that summarizes web content using its in-house LLM (large language models).  Google says AI Overviews appear only when the search engine believes it can provide more value than traditional links.



"When you're signed into Google and search for general topics like how to install one of Windows 11's recent updates, Google AI will rewrite content from independent websites and summarize it in its own words."  They said:  "This feature may sound good in theory, but Google's AI integration has several quality issues, including causing a slight delay as it generates the answer; and, even more problematic, sometimes displaying incorrect information.  For example, when searchers asked how to pass kidney stones quickly, Google AI Overviews told them to drink two quarts of urine."



LEO:  What?



STEVE:  I have a snapshot of the tweet from May 5th.



LEO:  What?



STEVE:  It reads - it shows the person...



LEO:  Every 24 hours.



STEVE:  Yes.  How to pass kidney stones quickly.  And the answer is "drinking plenty of fluids, such as water, ginger ale, lemon-lime soda, or fruit juice can help pass kidney stones more quickly."  Next sentence:  "You should aim to drink at least two quarters" - and it has helpfully in parens - "(two liters) of urine every 24 hours, and your urine should be light in color."



LEO:  Okay, thank you, AI.  Holy cow.



STEVE:  Thank you so much.  Now, this was May 5th.  And it noted that AI Overviews are experimental.  This was the week before this was formally released.  And I just so loved the comment of the guy who posted this who asked the question.  He wrote in response to this:  "Perfect.  Ready to go.  Ship it out."



LEO:  Oh, my god. 



STEVE:  I know.  Wow.  So BleepingComputer said:  "Although it was initially released as an opt-in Search Labs experiment, Google recently began rolling out AI Overviews to everyone in the United States whether they want it or not, with other countries to soon follow.  Google says that AI Overviews cause people to 'use Search more'" - well, yeah, because they don't get the answer the first time, they've got to use it some more.  I don't think I want to drink pee, thank you very much.  You got any other ideas?



So BleepingComputer continues:  "That doesn't even seem to be the case on many Google support forums, that is, where people are more satisfied with their results.  For example:  'I'm finding the results very repetitive, often wrong, and they don't at all match what I'm looking for.  But they take up so much space and feel in the way, I just want them to go away.'  Another user posted over on Google forums: 'Every single result I've received from the AI Overviews has been incorrect.  I'm more capable of misinterpreting Internet articles on my own.'"



LEO:  I don't need any help, thank you.



STEVE:  Don't need any AI to help me.



LEO:  Wow.



STEVE:  "'And I can probably get at least slightly closer to actual understanding than the AI because I actually have cognitive processes.'"  So BleepingComputer wrote:  "As the posts on Google forums suggest, early feedback on Google AI Overviews has been negative, with people finding the feature unnecessary and often misleading.  Unfortunately, there's no way to disable it now that it is out of Search Labs, and Google has quickly locked support threads for many people asking how to do so."  So, whoops, you're not even allowed to ask anymore.



LEO:  Wow.



STEVE:  And they said:  "As the Google search we all fell in love with 26 years ago no longer exists, now filled with endless features, sponsored search results, and shopping results, the company recently introduced" - get this, Leo - "a new 'Web' search option to return the old search feel."  Wait.  What?  I thought that Google was web search.



LEO:  Yeah, what are they talking about?  What?



STEVE:  Right.  Much of the tech press has gotten a big kick out of the fact that Google's default search results have become so cluttered and congested with their commercial crap that even they, Google, no longer consider it to actually be web search.  Okay.  Google's search results list a series of search result "filters" in a line underneath the search field.  They typically read All, then Images, Shopping, Videos, and News.  And after that are three vertical dots and a More menu item which drops a menu containing additional filters, one of which now is Web.  And selecting that filter, sure enough, dramatically cleans up the results.



What BleepingComputer posted was a way to cause that "web mode" filter to be selected by default.  The normal search URL is /search?q={search phrase}.  But adding the magic incantation "udm=14" after the /search? and then joined with an ampersand to the "q=" clause, causes the search to default to "Web," and you get much cleaner results every time.  And since this disables a large collection of Google's default search "enhancements," including its new and still apparently troublesome AI Overview, at no point will Google AI suggest that you drink urine.  At least, yeah, the AI won't.



So I have not encountered this default web search trick anywhere else, so I've placed a link to BleepingComputer's write-up in the show notes.  And for ease of access I've also made this GRC's shortcut for the week.  So for this podcast 975, it's grc.sc/975 will take you to this article at BleepingComputer if you want to, like, see them explaining how to do this.  And you do need then to get into your browser and tweak, like to add a custom search technique and then select that as the default.  But if you want to keep using Google, and you want to return to simpler times, you are able to get the Web search results.  And you can quickly see for yourself how much better it looks just by going under those three dots and More and selecting Web, and you get a cleaned-up page.



So I'll just say, as an aside, what a mess.  You know, the fact that this generation of AIs hallucinate very convincingly and with great authority makes this AI Overlord - I mean Overview - quite worrisome.  We absolutely know that there are many people who will suspend their own critical thinking, or what used to be called "common sense," in favor of accepting "truths" provided by external sources.  Perhaps Google feels that the Internet is already so full of crap that creating intelligent-appearing overviews won't further hurt anything.  I just hope their AI improves quickly.  Wow.



LEO:  I think there's actually a story behind all this, and we'll talk about it tomorrow on TWiG.  It's unknown, obviously.  Google's not talking.  But a number of people from Google have said, you know, Google has just panicked.  In the same way they panicked when they thought they were going to lose to Meta and created Google Plus, they panicked, thinking they were going to lose the farm to AI.  And of course panic is not a good way to create new features.  And it feels like that.  They're just throwing stuff up against the wall at this point.



STEVE:  Especially something this complicated.  Leo, this stuff is so, I mean, you know, arguably most people don't know how AI works.  Right?  It's like, well, it started sounding conscious.  So...



LEO:  I don't know how it works myself.  It's kind of a mystery. 



STEVE:  Exactly.  I mean, the AI researchers are like...



LEO:  They don't know, yeah.



STEVE:  ...what?



LEO:  Yeah.  Anyway, as you say, I don't use Google anymore, so I have missed this entire drama.  You know, for a while I did use something called Neeva that had AI summaries at the beginning.  But they were always footnoted, and I never found them to be wrong.  It was done cleverly.  But what I use now, Kagi, K-A-G-I, which is a paid search tool, doesn't do that.  I think people have realized that these AI summaries are not that useful.  And just give me the results.



STEVE:  Well, and think of the good that could be done, Leo, if we ever got to a point where all of the chaff could be separated, and instead of getting nonsense from an Internet search, no matter who does it, you actually got rigorous truth.  That would be something.



LEO:  Well, but let's also be fair to Google, part of this is because the Internet is full of crap.



STEVE:  Exactly.



LEO:  I mean, the Internet...



STEVE:  So if you're training your AI on crap...



LEO:  Right.



STEVE:  ...you know, crap in, crap out.



LEO:  And even Google Search can only reflect the search contents.  And if it's garbage, everybody's trying to game Google, it's going to reduce the search results anyway.  So it's a mess right now.  It's just a mess.  All right.  We're not alone as long as we've got Mr. G. here, help us out with the world at large.  Steve?



STEVE:  So under the topic of how things have changed, PC Gamer published an enlightening article titled "A Windows XP machine's life expectancy in 2024 seems to be about 10 minutes before even just an idle Internet connection renders it a trojan-riddled zombie PC."  They wrote:  "How long do you think it takes an unprotected Windows XP box to fall foul to malware?  To be clear, this is a machine sitting idle, no Internet browsing required, just connected to the Internet.  One YouTuber, Eric Parker, decided to find out.  Using a virtual machine, Parker set up a Windows XP instance and configured it to be fully exposed with no firewall and no antivirus software, just like the good old days."



Okay, now, just to remind everybody, even though XP always had a built-in firewall, it wasn't until Service Pack 3 that the firewall was enabled by default, like by the installation of that service pack, or installing XP that included SP3 after that point.  And of course, thanks to the tyranny of the default, very few earlier pre-Service Pack 3 Windows XP machines were protected out of the box.  And I remember those days.  Remember that there was a big third-party market for firewalls.  ZoneAlarm was the one that I found and liked a lot based on the way it operated.  So Microsoft wanted to add a firewall to their Windows client platform, but they also didn't want to, you know, blatantly they'd already had a lot of problems with antitrust.  They didn't want to just go obsoleting a whole class of software immediately.  So they put it in, but they didn't turn it on.



Okay.  So PC Gamer continues:  "So how long exactly does it take for malicious software to appear on the PC?  Parker returns to his PC" - you know, his virtual PC - "10 minutes later and, sure enough, there's something nasty running in Task Manager called conhoz[.]exe (C-O-N-H-O-Z dot exe), a known trojan.  He terminates that process and leaves the machine running.  Within just a few more minutes, a new user has been added, plus a number of new processes, including an FTP server.  So, yeah, within 15 minutes that's multiple malware processes and an entirely compromised machine with the bad guys having already created a new admin account and an FTP server running locally.



"Parker then traces the malware's communication to, yup, you guessed it, the Russian Federation.  He speculates that the bad guys might be trying to set up a botnet" - you think? - "or spam email server from his compromised machine.  Further investigation reveals even more malware, including another trojan and a rootkit.  A Malwarebytes scan then reveals the full horror, with eight nasties actually running, including four trojans, two backdoors, and a couple of adware apps.  In other words" - and of course an FTP server - "the machine is already a complete and utter zombie."



And they said:  "Anyway, it's a fun watch as Parker observes his virtual XP machine being ravaged in real-time and a reminder of what's bubbling away behind the firewalls and malware protections on all of our PCs."  He says:  "Sniffing through your running processes in Task Manager used to be something of a regular ritual for the well-informed.  Now, it's not really necessary.  Famous last words and all that.  Indeed," they write, "it just goes to show how effective those machines are that we can all be connected to the Internet now 24/7 and not give this stuff much thought.  It's dangerous out there, boys and girls.  Be careful," they conclude.



Okay, now, I would - I love that.  I would edit that just a bit to observe that this vividly shows what's right now pounding away at the outside of our stateful NAT routers, those vital pieces of hardware all of our networks are blessedly perched behind.  More than any other single thing, it's the godsend of NAT routing, which placed a stateful hardware firewall filter between our internal LANs and the increasingly wild and wooly Internet, that have made it possible to use this crazy global network with any hope whatsoever of remaining safe.  Presumably, I don't know what Eric's history is, but presumably the IP that his XP machine appeared on wasn't for any reason particularly high-profile; right?  It was just some random guy.



And there's just that much crap hitting each of our IPs regularly enough that - and who knows?  Was it all the same attacker who said, oh, my goodness, we just found a new victim, you know, let's get it?  Or different attackers who were all randomly scanning the Internet and happened to lock onto this XP machine.  I mean, I have to say I'm tempted to do this because this would sound like fun.  Except you've got to be so careful.  And it would be so easy to make a mistake.  So, you know, if you do want to replicate what Eric did, then, you know, really, really, really be careful.  For anyone who's curious to see Eric Parker's YouTube video described in this article, I posted the link in the show notes so it's easy to find.  And also when I was there looking at it, I noticed that since then, and this got a lot of attention, he's done the same thing to Windows 2000.  So I didn't make time to dig in and see if 2000 was similarly vulnerable.



Okay.  TechCrunch reported that, thanks to the discovery made by a pair of curious students at the University of California at Santa Cruz, who to their credit did try to do the right thing by attempting to report the flaws they'd uncovered in the control software of their shared University washing machines, as TechCrunch headlined their story, "Two Santa Cruz students uncover a security bug that could let millions do their laundry for free."



Okay.  So the company behind these widely deployed machines is called "CSC ServiceWorks," which is an unfortunate name because the service doesn't work so well.  The two UC students, Alexander Sherbrooke and Iakov Taranenko, discovered flaws that allows anyone to remotely send commands to laundry machines run by CSC, which allows them to initiate laundry cycles without paying.  It appears to be another instance of a company that should really not be putting their equipment on the Internet,  yet doing so anyway.



Like your typical college student, Alexander was sitting on the floor of his basement laundry room in the early hours one January morning earlier this year with his laptop.  He was bored waiting for the spin cycle to finish on his last load, and while poking around with some scripting commands the machine in front of him suddenly woke up with a loud beep and flashed "PUSH START" on its display, indicating the machine was ready to wash a free load of laundry, and this was despite that fact that Alexander's current laundry system balance was $0.  Since students will be students, experimenting further, they set one of their accounts to reflect a positive balance of several million dollars in credit.  And sure enough, their "CSC Go" mobile app reflected this balance without complaint.



As I said, the company behind this is CSC ServiceWorks, a large laundry service company which boasts a network of over one million laundry machines installed in hotels, university campuses, and residences across the United States and Canada.  Oh, and also Europe.  You would think that such a firm that's using Internet and smartphone technology to replace coin-op machines might have someone on staff to field trouble reports.  But there's no indication of that.  Since CSC ServiceWorks does not have a security page for reporting security vulnerabilities, Alex and Iakov sent the company several messages through its online contact form in January, but heard nothing back from the company.  Even a telephone call to the company got them nowhere, either.



Finally, they reported their findings to the CERT Coordination Center at Carnegie Mellon University, which, as we've discussed, provides a means for security researchers to disclose flaws to affected vendors and provide fixes and guidance to the public.  Even that failed to evoke any reaction from CSC.



Today, months later, despite having tried to do the right thing, the glaring vulnerability remains open.  In following up on this, even TechCrunch failed to get anywhere.  TechCrunch wrote:  "It's unclear who, if anyone, is responsible for cybersecurity at CSC, and representatives for CSC did not respond to TechCrunch's requests for comment."



Okay.  So it seems to me that what might finally arouse CSC's attention - and apparently the only thing that will - may be a sharp and sudden drop in cash flow revenue as word of this spreads across college campuses in the U.S., Canada, and Europe. It's just the sort of hack that's pretty much guaranteed to become quite popular.  Having waited longer than the customary 90 days after attempting to report their discovery and findings, Alex and Iakov have now started to reveal more about their discovery.  They decided to disclose their research in a presentation during UC University's cybersecurity club meeting earlier this month.



They explained that the vulnerability is in the API used by CSC's mobile app, CSC Go.  In the normal case, someone needing to do the wash opens the CSC Go app to top up their account with funds, then pay, and begin a laundry load on a nearby machine.  But Alex and Iakov found that CSC's servers can be tricked into accepting commands that modify their account balances because security checks - get this - are only performed by the client app on the user's device.



LEO:  [Laughing]



STEVE:  I know.  And anything sent to CSC's servers are fully trusted.



LEO:  Oh, my.



STEVE:  This allows...



LEO:  Including things like I have a million dollars in my account.



STEVE:  Correct.  



LEO:  Unbelievable.



STEVE:  Correct.  This allows fake payments to be posted to their accounts without ever putting any real world funds in the accounts.  And Leo, it's worse.  While Alex was sitting on the floor of the basement, he was analyzing the network traffic while logged in and using the CSC Go app.  And he discovered that he could circumvent the app's security checks to send commands directly to CSC's servers.  Alex and Iakov said that essentially anyone could create a CSC Go user account and send their own commands using the API - get this - because the servers are also not checking whether new users even own their email addresses.  The researchers tested this by creating a new CSC account with a made-up email address.  So not only mistakes, but also really crappy overall system design.



Here was the comment that surprised me:  CSC quietly wiped out the student's spoofed account balance of several million dollars after they reported their findings.  But the researchers said the bug remains unfixed, and it's still possible, four months, five months later, for users to freely give themselves any amount of money.  Iakov said that he was disappointed that CSC did not acknowledge their vulnerability.  He said:  "I just don't get how a company that large makes those types of mistakes, then has no way of contacting them."  He said:  "Worst-case scenario, people can easily load up their wallets, and the company loses a ton of money.  Why not spend a bare minimum of having a single monitored security email inbox for this type of situation?"



But, of course, even that's not the point.  If the company zeroed the students' demonstration multimillion dollar account balance, that shows that someone somewhere within the company did receive the message, and does know that there's a problem.  My guess is that we have become so accustomed to the way a mature security-conscious company goes about handling such matters that we don't know what to make of a company that chooses instead to bury its head in the sand.  You know?  But we should remember that it wasn't so long ago that most companies acted this way.  They would freak out, raise the drawbridge, switch to internal power, and say nothing publicly while they scurried about behind the scenes trying to figure out what to do.  We've learned that's not the enlightened way to act with regard to Internet security vulnerabilities, but it does stand to reason that those who are not actively involved in this arena might not still be up to speed on today's etiquette.



LEO:  Hey, we're laundry guys.  What do we know about the Internet?  You put in a quarter, you wash your laundry.  This just shows you, though, it's really good for students to have to do their own laundry because that enforced period of boredom can really lead to some creative results.  That and a lack of quarters.  I love it.



STEVE:  Yeah.  We actually, Leo, I have to confess, back, you know, when I was myself at Berkeley, we had coin-op washing machines.



LEO:  Did you tie a string to the quarter and pull it back?



STEVE:  Actually there was a screw hole in the back of the quarter-accepting add-on to the washing machine.  And it didn't take long before - and I'm not saying who.



LEO:  Yeah, someone.



STEVE:  An enterprising student figured out that if you took a coat hanger - I've talked about how handy that coat hanger...



LEO:  So useful.



STEVE:  How coat hangers are like, they are the perfect type of stiff wire.  You could cut off a length and put a little hook in the end, snake it through the hole, and then you could reach in, hoping not to be electrocuted, by the way, and grab, you know, find the arm that gets pulled when the quarter is put on the little slider and pushed in, and give it a few tugs, and what do you know?  The washing machine would start right up.



LEO:  Unbelievable.



STEVE:  It is a good thing I'm not in college, you know, in this day and age.



LEO:  You know, this, honestly, there's a subtext here that maybe CSC goes, yeah, so the students are ripping us off.  What else is new?  As long as we get most of the money, we're fine.  What else is new?



STEVE:  Well, yeah.  And exactly as you said, they're in the washing machine business; right?  They probably contracted out to the lowest bidder...



LEO:  Of course they did, mm-hmm.



STEVE:  ...to make themselves an app to put these machines on the Internet.  And that guy is gone. 



LEO:  Long gone.



STEVE:  So they probably have no idea what to do.  You know, they're able to monitor account balances and zero them when they get set to a million dollars.  But other than that, eh.



LEO:  So we had an intern for a while back in the Brick House days, the wonderful Jeff Needles.  You may remember him.



STEVE:  I do remember him, yeah.



LEO:  He was a fun guy.  And he said, "I'll write a sales system for you."  And so he wrote up a whole sales database system for us that we started using for every ad sale and so forth.  And then he left.  He got a better job.  I think he went to that video company that he was so enamored of.  And we said, well, can we give you a contract to maintain?  He said, nah, you know.  And he just left.  And so we had this blob of software which breaks, well, let me put it this way.  If two people try to use it at the same time, boom, it's dead.  So it breaks all the time.  But it's not really worth it for us to redo it.  We just hired some guy who kind of looks at the code and pokes at it once in a while.  And we just limp along with it.  I think a lot of companies are like that.  I don't think that's at all unusual.  I mean, it's not a security issue because it's not public-facing in any way.



STEVE:  Right.  When I released SpinRite 6.1, we made a decision that we would no longer maintain upgrades of SpinRite from before 6.0 because it's been 20 years.



LEO:  Right.  Right.



STEVE:  And, you know, we've been more than generous for decades.  Anyway, Sue was greatly relieved because she was using what we called Dino, which actually was a dinosaur.  It was the original GRC Novell NetWare database that was written in FoxPro by my second employee, who was a truly gifted coder.  He went into writing gaming stuff.  His name is Steve Rank, and neat guy.  It's running today.



LEO:  Yeah.



STEVE:  And so it was only a few weeks ago that I said to Sue, "Sue, you no longer" - and the reason, the point was that if someone said, hey, I bought SpinRite 3.1 in '98, I think it was, and so she would look it up in Dino and say, oh, sure enough, here you are.



LEO:  Yeah.  Well, you know, we just - the way it works here is when somebody's going to go use the sales system, they send out a company-wide beacon on Slack, says everybody out.  It's funny, I mean, I think every company has something like that.



STEVE:  Yup.



LEO:  It's just normal.



STEVE:  Yup.



LEO:  It's the way it is.  And technology moves so fast.



STEVE:  It's what kept IE from ever dying.



LEO:  Exactly.



STEVE:  Is so many enterprises had written internal stuff that was dependent upon specific quirks in operation of Internet Explorer, that there was like, no, no, no, you can't take our IE away from us. 



LEO:  Now, back to the fun and games with Stevie Gibson, Little Stevie Gibson.



STEVE:  Okay.  So, many of our listeners forwarded tweets from Bernard Netherclift, who is a Voyager follower and enthusiast.  Last Thursday on the 16th Bernard tweeted:  "Fingers crossed.  This looks like Voyager 1's science data is due to resume Sunday 11:48 UTC, commands going up Friday."  So he's saying commands were being sent on Friday to switch from just sending data back as they have been to actually switching over to sending science back.  Meaning the output of the Voyager 1's surviving sensor arrays.



Then Sunday on the 19th, two days ago, Bernard followed up with:  "Voyager 1 has just returned to science mode, at a data rate of 160 bits per second, for the first time in six months."  So, yes, incredibly, I mean, it really is incredible.



LEO:  Yeah, no kidding.



STEVE:  Voyager 1 is back online after having had its programming updated to literally work around a bad patch of memory.  What an amazing piece of technology.  And, wow.



LEO:  Brilliant.



STEVE:  And also to our listeners, thank you, all you who tweeted that, making sure that I knew.  Okay.  Hakku.  "Hello, Steve.  Long-time follower and big fan of SN.  Keep up the great work.  One question following SN-973 VPN-attack topic.  We discussed this internally in our IT-Security-Consultant bubble, and one of our network guys mentioned that he would expect VPNs to use the internal firewall as soon as the VPN started, to block all outbound traffic that's not tunneled via the VPN.  Therefore, there would not be a possibility to route some traffic around the VPN since the traffic would be blocked; right?  What do you think?  Is this an actual fix?"  He says:  "We're all about to research if and which provider does use this technique.  Thanks for making my car drives a lot more interesting, and have a nice week."



LEO:  Nice.



STEVE:  "To 999 and beyond."  Okay.  So Hakku makes a great point, which is that VPNs could arrange to prevent this sort of simple routing table-driven attack.  But what the researchers found was that what "could" be done often was not being done in practice.  And I remember they mentioned OpenVPN.  One of the problems is OpenVPN is open source and cross-platform.  So what is cross-platform is using the routing table to manage rerouting.  But if you're running OpenVPN on a Windows machine, the local firewall aspect is not cross-platform.  So that's not something, I mean, whereas other platforms may also have local firewalls, they've all got their own.  And Windows is certainly not compatible with anything over in the Linux world, or Mac.



So what they found was that many popular VPNs in widespread use today were true victims of the attack which we talked about two weeks ago.  What Hakku's networking guy suggested, which was that a VPN could arrange to dynamically manipulate the machine's local firewall rules to block all other outbound traffic not bound to the VPN server IP and port, could indeed be done.  So let's hope that the popular VPN providers are being asked about their susceptibility to this particular form of simple routing table attack and then do take the effort to revise their solutions, if necessary.  And I'm glad, for example, that this came up and that Hakku brought it up to his tech guys and that they're going to do some research to make sure that they're not vulnerable.



214normandy wrote:  "Hi, Steve.  I know you've been using" - oh - "the Netgate SG-1100 as well as the four-port Protectli Vault.  I'm starting to see reports that the eMMC in the SG-1100 is starting to wear out for folks."  He said:  "I ran their suggested commands" - and he provides a link to their documentation - "to check the eMMC, and it says that my eMMC is end of life expected already."  He says:  "No big deal, I'll move on and try the four-port Protectli Vault instead.  Hoping you can confirm that you are still happy with your Protectli. Thanks, Bob."



Okay.  So this came as news to me, so I wanted to share it for any other Netgate SG-1100 users who may have followed my advice and my choice about that beautiful little Netgate appliance.  The eMMC is non-volatile memory that's soldered directly to the motherboard and cannot be replaced.  I presume that the problem is the logging and status updating that is currently churning away in the pfSense firewall.  It's constantly writing logs to the file system, and eMMC memories do not have huge amounts of excess endurance.  You know, they're meant more for embedded solutions that are not churning constantly.  I still have a trusty SG-1100.  I mean, right now the bandwidth for this podcast is passing across my SG-1100.  And it's been giving me no trouble ever since I replaced its power supply.  Remember that it was glitching, and it turned out to be the power supply that was the problem.  But it is sobering that it will have a lifetime limit due to the failure of an eMMC memory that cannot be replaced.



Bob also asked about my other favorite pfSense hosting device, which is the four-port Protectli Vault.  That's what's running pfSense at my place with Lorrie.  And yes, I'm still utterly happy with that choice, too.  And in fact I have another of those standing by ready for deployment here if the SG-1100 should ever die, which unfortunately no longer seems as unlikely as it once did.  You know, just giving it power apparently won't be enough in the long run, which is quite disappointing.



Okay.  I have an important and interesting message from a listener who requested anonymity.  He wrote:  "Hello, Steve.  I've been a listener of Security Now! for years, perhaps even a decade; a member of TWiT.tv; and a proud owner of SpinRite.  Thank you for all your incredible work.  I work for a large French company as a web developer, managing a website with a significant" - okay.  "Managing a website with a significant audience of approximately one million visitors per day.  Like many other websites, we rely heavily on advertising."  Okay, now, you can imagine with that sort of website traffic what sort of revenue their site is able to generate from all those eyeballs being confronted by ads.



Anyway, he continues:  "Similar to your sentiments, I am enthusiastic about the Google Privacy Sandbox and its potential to enhance privacy compared to traditional cookies.  However, the advertising industry is pushing back against this initiative.  As you're aware, ad companies profit by constructing user profiles and serving targeted ads.  With the advent of the Google Privacy Sandbox, their revenue streams are threatened, as user profiles will no longer be available, and ad selection will be handled by the browser itself.  Consequently, they are resisting this change.



"Their strategy" - now, we're hearing from a listener of ours who is over on the implementation side of all this.  He says:  "Their strategy involves persisting with the current model of tracking users across websites.  Several alternatives to third-party cookies have emerged and are rapidly gaining traction.  Some utilize first-party cookies through CNAME redirection such as" - and he cites the site that offers the service - "first-id.fr, while others leverage ISP data to identify users based on their Internet connection" - and then again he says - "like Utiq.com.  Additionally, there are methods involving email or phone numbers for cross-website identification, like LiveRamp.com."



He said:  "I've been tasked with implementing these solutions, and I anticipate that a majority of websites will follow suit, as a few big websites in France already have.  This is because the CPM" - meaning, you know, the amount of money they get - "for ads using the Google Privacy Sandbox is lower, resulting in reduced revenue for website owners compared to more precise tracking solutions.  Furthermore, these newer tracking methods are perceived as being more reliable than traditional third-party cookies.



"Regrettably, I fear that this development may exacerbate privacy concerns in the future.  Currently, it's possible to clear or block third-party cookies, but it will be considerably more challenging to mitigate these new tracking solutions based on first-party cookies, ISP connections, or email and phone numbers.  I believe it's crucial to inform your audience about this trend.  It's already underway, and I doubt Google can do much to counter it.  I prefer to remain anonymous to avoid potential repercussions from my employer."



Okay.  So first of all, I thank our listener for this view from the trenches.  It is disappointing, but unfortunately not surprising.  It was the subject of our "Unforeseen Consequences" podcast back on February 6th of this year.  Here's the way to think about this:  Third-party cookies enabled tracking of users based only upon the ads that were being shown and the original ability of advertisers to plant cookies into browsers along with their ads, and for those cookies to later be returned when ads were placed on other websites.  This allowed advertisers to follow users around the Internet, since the user's browser would quietly send back whatever cookies it had previously collected for the same advertiser.  The key point of this original tracking model is that it did not in any way involve the website.  It operated completely separate from the website.



And this is, crucially, what's in the process of changing now, and it's being driven by the universal change motivator, namely money.  What's changing is that websites are now beginning to collude with their advertisers specifically to facilitate tracking.  Why?  Because advertisers will pay websites more for the ads they're hosting if they collude with them to facilitate tracking which better identifies their visitors.  Our listener wrote:  "Currently it's possible to clear or block third-party cookies, but it will be considerably more challenging to mitigate these new tracking solutions based on first-party cookies, ISP connections, or email/phone numbers."



It's actually worse than that.  The bad news is that, if websites are willing to collude with third-party advertisers, there is nothing whatsoever we can do about that.  Anything a website knows about you will now be shared with third parties.  In many cases, as we recently saw with Microsoft, which was forced to disclose this due to the GDPR, I think it was, what, more than 700 was it, or 500 and some odd, I don't quite remember.  But a phenomenal number, you know, many, many hundreds of individual third parties they were confessing they would be sharing anything they had about their visitors with.  We talked about websites beginning to want their visitor's email addresses.



And Leo, you pointed out that even if we give them our throwaway email, if we always give them the same one, it still identifies us as efficiently as if we were using our primary email.  Money is the great motivator.  We saw what the ability to extract extortion payment by cryptocurrency did for the ransomware world.  It exploded overnight.  Websites are now being shown how to make more money by asking their visitors more about themselves so that they are then able to turn that information over to their advertisers.  How many are going to see this as a problem?  I would venture probably not that many.



So what was once tracking being done without website assistance is evolving into collusion between websites and their advertisers.  You know, pay us, and we will tell you everything we know about our visitors.  I think it's clearly inevitable, and there's nothing we can do about it.  As with most things which are abhorrent but invisible, as tracking always has been, most people will have no idea it's going on, and I suspect that many wouldn't care anyway.



LEO:  And this is, by the way, exactly what's happening to podcasts, as well.  The difference is we don't have any information to collude with advertisers.  And when they do ask us to put tracking pixels in, or beacons of some kind, we just say no.  And we try to constrain that.  And it hurts us, which is the reason why most advertisers now move to places like Spotify because they can get that information.  We're kind of out of luck.



STEVE:  Yeah, they want it, yeah.



LEO:  That's why we want people to join the Club because ad support's just not going to do it in the long run.



STEVE:  Kevin van Haaren tweeted:  "I'm not sure anyone's mentioned this to you yet, but Bitwarden's Passkey implementation is available now.  I was able to create a Passkey for a site on my iPad, go into work and use that Passkey from my Windows computer without issue.  When I went to add a Passkey to the account on the iPad, Bitwarden popped up automatically, asking if I wanted to create the Passkey in Bitwarden."  So yes, we had heard that support was in beta and coming soon, but I hadn't noticed that Bitwarden's support for mobile was out now, was out of beta.  That's great news.  And as we all know, Bitwarden is a sponsor of the TWiT network, and we're very glad they are.



Robert Harder tweeted:  "Regarding Passkeys, help me out here.  I feel like you and Leo are missing the point."  And Leo, it's actually more my fault than yours, so, you know.



LEO:  Oh, I'm good at missing points.  Go right ahead.



STEVE:  He said:  "Or am I?"  He said:  "I thought Passkeys were to say, 'Hey, this device has already logged in properly so let's make future logins super easy but also secure.'  So that would mean I don't," he says, "I don't want my Passkeys to be exportable.  If I ever want to log in on another device or OS or ecosystem at all, I want to prove that it's me all over again with whatever way I do that on that website, hopefully with multifactor authentication.  Only then is that device, and that device only, secured and proven.  It's a nice bonus that Apple or Microsoft or Google have internal synchronizing for their own ecosystems, but only if it's really, really, really securable.  Generally speaking, having Passkeys exportable is as bad as Firesheep days when grabbing someone's session cookie gave an opponent 100% impersonation of a victim.  Yes?  No?  Thanks."



LEO:  No.  No.



STEVE:  "Listener from Episode 1.  Rob."  Okay.



LEO:  Rob misunderstood.



STEVE:  So I think Robert makes a valid point.  Although entirely different, although, well, okay.



LEO:  Look.  Passkeys are being proposed as a password replacement.  Passwords are not specific to the device you use, nor should Passkeys be.  It's the same thing.



STEVE:  So, okay.  Another entirely different way to think of Passkeys is the way he does.



LEO:  Yeah, but it's not right.



STEVE:  Okay.  In that case, the existing username and password login is used one last time on each device, which then receives a Passkey to forevermore authenticate that user and device to that website.  Okay, I can see that as a workable model.  But here's the critical factor, and this is what you're alluding to, Leo.  That model only works in a world where every website allows for any number of Passkeys to be registered to any single web account.  And as we've been saying in the last couple weeks, it is apparently the case that any number of Passkeys can be registered to any single web account, maybe without limit?  That's the question.  Where's the limit?



So, you know, if at some point a website were to reply:  "We're sorry, but you've reached the limit of Passkeys that can be assigned to your account.  If you wish to add another, please review and remove some that have already been added."  You know, we don't know if that's ever going to happen, but we know that it could.  And the experience that Kevin just reported of creating a single Passkey with Bitwarden on his iPad, then having Bitwarden later login for him using the same synchronized Passkey under Windows, well, that's pretty slick.



LEO:  He just misunderstands what Passkeys are all about.  He's saying he's thinking they're like session cookies, which is the problem Firesheep had.  But Passkeys are not as easily accessed as cookies.  I would hope they're better secured than that.



STEVE:  They're public key crypto.



LEO:  Right.  So they're secure.



STEVE:  And so they're not at all the same.



LEO:  And furthermore, they're being proposed as a replacement for passwords.  So that's not what he's just described.  He's describing a replacement for session cookies.  That's not what Passkeys are.  So I think he just misunderstands what Passkeys are.  They are a replacement for passwords and, as a result, are not tied or should not be tied to a specific device.  And you're right.  A password manager can be and probably should be the person that holds your Passkeys, just as they are the people who hold your passwords.  Yeah.



STEVE:  Okay.  Spencer Webb, he tweeted:  "Enjoyed the eLORAN discussion.  I know the guy at UrsaNav.  We had discussions about some projects a few years back.  When the USG turned off LORAN, I thought it was incredibly stupid.  It does work indoors, and in caves, and without an ionosphere.  And yes, you can read into the above some interesting scenarios.  Remember to feed your antenna.  Best, Spencer."



LEO:  He's probably a ham.  I think that's a ham.



STEVE:  He is.



LEO:  Okay.



STEVE:  Spencer is a serious radio guy.



LEO:  That's what I thought.



STEVE:  We often exchange notes when something about radio comes up.  I remember back in the days when you had to hold your iPhone in the proper way, he and I were having some conversations about antenna science.  So anyway, it was nice to have him add to the eLORAN discussion.  I think it's clear that having a system that's fundamentally terrestrial has many applications, even when GPS is working well.



Oh, this is interesting.  Dr. Brian of London tweeted:  "I integrated Passkeys into my own site as a secondary login system which in some cases is easier to use, especially on mobile devices, than the primary method, which is cryptographic signing of a challenge with either a browser plugin wallet or something using QR codes that looks like SQRL to prove ownership of personal keys."  So, okay.  What he's saying is he has a website, and he rolled his own fancy login system which he's had for some time.  But then he decided, hey, Passkeys is a standard.  I'm going to add that to my site.



So he said:  "To this I added the ability to associate one or many keys with an account and add/delete/rename them.  One little gotcha which you probably only learn when implementing this."  He said:  "I store on my server a list of all public Passkeys, and every time I get a login request from a client, I could send every public key I have, and the client would figure out which, if any, it holds.  But in reality I don't do that.  I associate each of the public keys with a username.  This is part of my primary system anyway, but that username is the only thing I hold.  I don't have emails or passwords."



He said:  "I use that username to filter the list of public keys I send back to the client, which then figures out if the user's device has any of them."  He says:  "It works nicely with Apple Passkeys and other Passkeys which already sync across multiple devices nicely."  So basically he's saying he rolled his own server-side Passkeys implementation.  As a consequence, he has a bunch of accounts, and each of those accounts has Passkeys.  He could send all of the public keys to the client, which would then say, oh, I found a match, which would tell him who it was that was wanting to log in.  But instead he asks them for just their username, which allows him then to filter from all of his public Passkeys only those associated with that user and send those back in order to give it a chance to log in with a Passkey.  So anyway, we have users who are implementing Passkeys on the server side, which is also very cool.  Or rather listeners.



Shaun Merrigan said - remember that it was Shaun - oh, right.  Okay.  First of all, Shaun was the guy with the old LORAN receiver which woke up when eLORAN was turned back on late last year.  Anyway, he heard us talking about him last week, and he followed up with a bit of more interesting information.  He said:  "To close the loop on this, my location is Edmonton, Alberta, Canada."  Okay, that's where he is.



He said:  "The three eLORAN stations that are currently testing are Fallon, Nevada; George, Washington; and Havre, Montana."  So he's receiving signals from those three locations.  He says:  "This is my best information.  Currently, my old Austron 2100F is showing 2.8E-12 seconds offset from GPS."  Again, 2.8E-12 seconds offset from GPS.  So, yeah, lots of accuracy in LORAN available timing data.  And really it sounds like once this is turned back on, all of our clocks that used to synchronize on WWVB...



LEO:  Oh.  Lost LORAN.



STEVE:  Yeah.



LEO:  Oh, that's cool.



STEVE:  Because, you know, that's not very reliable, that whole WWVB.



LEO:  And they wanted to turn it off for a long time.



STEVE:  Yeah.



LEO:  Yeah.  Well, clever.



STEVE:  Talk about range.  That's really cool range.  Oh.  Markus Daghall tweeted:  "Hi, Steve.  While looking at the PIN heat map graph, the number 1701 seems to be more prevalent than its surrounding numbers."  Which I love because we know what that is.



LEO:  Star Trek NCC 1701.



STEVE:  Exactly.



LEO:  That's a good PIN.  Wish I'd thought of that.



STEVE:  NCC 1701.  That is great.  Ed Ross tweeted:  "Re Big Yellow Taxi," he said, "presumably that system helps in situations where 'you don't know what you've got till it's gone.'"



LEO:  They paved paradise, put up a parking lot.



STEVE:  And that was your observation last week, Leo.



LEO:  Yes, yes, yes.



STEVE:  Riny, and I can't even begin to pronounce this guy's last name, he's in Spain, H-E-I-J-D-E-N-D-A-E-L.



LEO:  It's a Dutch name, Heijdendael.



STEVE:  Perfect, thank you, Leo.  So he wrote:  "As many, I started the FIDO1 journey with YubiKey, but even then I was splattered by the messy software support, implementation guides, and it was at that level that I thought it was a no-go for regular users - slot selection, HMAC, keyboard emulation, all cool, a bit too cool.  But when FIDO2 came along we had to switch tokens anyway, and I switched to 'Token2,' a Swiss-made token that manages selective key removal, up to 300 keys, and enforced PIN complexity, all for a better price than the YubiKey.  Furthermore, I needed TOTP for two-factor authentication that would work as a standalone device when traveling, and even that is in their device.  I just don't understand why YubiKey is still pushed as the de facto standard.  What do you think?"



Now, he included in his note a link, and it's in the show notes at the top of page 15.  And he finished, saying - and Leo, I should mention I have two on order now.



LEO:  Yeah, I think I'm about to buy some, yeah.



STEVE:  Yup.  He said:  "Keep up the good work.  By the way, I silently suspect that you were hired by the UK government to write their specs for them."  And of course he's talking about the fact that we talked about the requirements that the UK had for their consumer IOT devices, and it did actually sound like, you know, they've been listening to this podcast.



LEO:  It sure did, yeah, yeah.



STEVE:  Okay.  So I needed to let all of our listeners know about these Token2 Passkeys dongles.  They look fantastic, and supporting 300 Passkeys, individually manageable and deletable, with both USB-A and C connection options, they look fantastic.  I will certainly admit to feeling some proprietary intellectual connection to YubiKey as the guy who happened to come along at the right time and had the perfect audience for them with this podcast.  But that's the limit of it.  I would like them to succeed in the long term, but that requires them to keep up in what has obviously become a very competitive market.  The huge advantage they've been enjoying is having been first.  And that's a big deal.



But to remain first they need to remain competitive, and we've all been scratching our heads over why they would still have a 25-key limitation when such limitation pretty much relegates them to the enterprise or password manager unlocking role.  To be a consumer's primary Passkeys container requires that they be able to retain and selectively manage hundreds of keys.



So I'll say it again.  These Swiss-made Token2 dongles look fantastic.  And I should note that YubiKey has since announced a new key, and I don't remember the number.  It might be 200.  But even now it doesn't appear to still be 300.  Or maybe 100.  Anyway, the bad news is unfortunately these guys are in Switzerland, and the one we want is currently sold out.



LEO:  But it says June 17th shipping, so that's not so...



STEVE:  Oh, that's good.



LEO:  Yeah.



STEVE:  That's good.  Although shipping, unless you choose postal mail, which they discourage, is twice the cost of the dongle.  So, okay.  Anyway.  Anyway.  The way I know that is that I've ordered two, and they're on the way.  So anyway, thank you very much, Riny, for providing a direct link to the Token2 page, which as I said is in the show notes.



Also, another listener, Andreas in Germany, also pointed to the Token2 solution which, by the way, is FIDO, FIDO2 with WebAuthn, TOTP, USB, and NFC.  And it really does look very slick.



LEO:  Clearly they put a fairly potent chip in there.  So when it says 300, that could be 300 Passkeys?



STEVE:  It is, 300 FIDO2 WebAuthn Passkeys.



LEO:  Oh, wow.  300 is probably a good start.  I don't - at least for a while, yeah.  All right, Steve.  Let's talk about 200 doctors can't be wrong.  Or something like that.



STEVE:  Our listener, Robin van Zon in the Netherlands, brought this recently produced letter to my attention.  So thank you, Robin.  The letter opens by introducing itself:  "The text below is an open letter on the position of scientists and researchers on the recently proposed changes to the EU's proposed Child Sexual Abuse Regulation."  Now, we're interested in this, of course, because this is all about whether we're going to have backdoors and something is going to be monitoring communications for, you know, grooming and CSAM material and so forth.  "So as of the 7th of May, exactly two weeks ago today, the letter has been signed by 312 scientists and researchers across 35 countries."  I mean, it is the Who's Who of security and research.



So, and what's interesting is that there has been some very good, you know, good faith back-and-forth here.  So this is not an open letter that's just being blown off and being ignored.  The EU's regulators and legislators have changed their legislation in an attempt to solve the problems that were earlier voiced.  As we're going to see, not only are they not there yet, but there's real good reason to believe, as we probably all know, you can't get there from here.



Okay.  So it turns out that what scientists and researchers have to say is quite refreshing because it actually engages science, math, statistics, and, yes, reality, as opposed to the politicians' statements of "this is what we want and what we're preparing to demand."



So I want to share what these 312 scientists and researchers collectively assembled.  And it's not overly long, you know, because the devil as it turns out is in the details, and because there's probably no more important issue on the table at this moment, arguably in the world, than what the EU's political class will finally decide to do about this.  And importantly, as we'll see, this is the technical response to the politicians' responses to the previous technical response.  And as I said, what's heartening is that both sides so far appear to be negotiating here in good faith.  And the politicians are at least listening.



So as we know, for their part, the UK was faced with the same problem and serious opposition to their similar proposal to require all private conversations to be monitored for content.  What they did was wisely added the caveat "where this can be proven to be technically feasible without compromising security," which allowed the politicians to say that they had passed legislation and allowed all the messaging providers to continue offering fully private end-to-end encryption because it hadn't been and cannot probably be proven to be feasible without compromising security.  So win-win, win-win-win.



Okay.  But the European Union is not there yet.  So here's the latest feedback from the EU's technical experts, which is intended to inform the politicians of reality.  The undersigned wrote:  "We're writing in response to the new proposal for the regulation introduced by the Presidency on the 13th of March, 2024."  So 13th of March, right, just a couple months ago.



"The two main changes with respect to the previous proposal aim to generate more targeted detection orders, and to protect cybersecurity and encrypted data.  We note with disappointment that these changes fail to address the main concerns raised in our open letter from July of 2023" - so nearly a year ago - "regarding the unavoidable flaws of detection techniques and the significant weakening of the protection that is inherent to adding detection capabilities to end-to-end encrypted communications.  The proposal's impact on end-to-end encryption is in direct contradiction to the intent of the European Court of Human Rights' decision in Podchasov v. Russia on the 13th of February of this year.  We elaborate on these aspects below."



Now, just to interrupt here, I tracked down that decision.  The case surrounded Russia's FSB demanding that Telegram turn over the decrypted communications of six individuals who the FSB alleges were involved in terrorism against the Russian state.  Telegram refused, explaining that since all of the subjects involved had enabled Telegram's optional end-to-end fully encrypted mode, Telegram's default ability to store unencrypted conversation data in their servers was thwarted.  And indeed, paragraphs 79 and 80 of the decision of the European Court of Human Rights backed that up.  And I skipped all of the earlier paragraphs.



Here's what those two paragraphs say.  79 says:  "The Court concludes that in the present case the ICO's statutory obligation to decrypt end-to-end encrypted communications risks amounting to a requirement that providers of such services weaken the encryption mechanism for all users.  It is accordingly not proportionate to the legitimate aims pursued."  In other words, yes, the intention is legitimate, but the only way you can do this is by weakening encryption for everybody.  And that's not a proportionate response.



And then paragraph 80 says:  "The Court concludes from the foregoing" - and that's all the other paragraphs that I'm sparing everyone - "that the contested legislation providing for the retention of all Internet communications of all users, the security services' direct access to the data stored without adequate safeguards against abuse and the requirement to decrypt encrypted communications, as applied to end-to-end encrypted communications, cannot be regarded as necessary in a democratic society.



"Insofar as this legislation permits the public authorities to have access, on a generalized basis and without sufficient safeguards, to the content of electronic communications, it impairs the very essence of the right to respect for private life under Article 8 of the Convention.  The respondent State has therefore overstepped any acceptable margin of appreciation in this regard."



So what this tells us is that, separate from whatever political pressures the EU's politicians may be under, when the issues at stake are very carefully and thoroughly examined by the European courts, their decisions never support the application of wholesale surveillance.  For the sake of our listeners' sanity, as I said, I skipped over the first 78 paragraphs.  But those paragraphs make it very clear that the courts really do very clearly understand the issues.  They clearly understand that the phrase "selective backdoors" is an oxymoron.



Okay.  So continuing with the technologists' latest rebuttal response to the politicians' attempt to mollify them following their first surveillance proposal, they all wrote and signed:  "Child sexual abuse and exploitation are serious crimes that can cause lifelong harm to survivors.  Certainly it is essential that governments, service providers, and society at large take major responsibility in tackling these crimes.  The fact that the new proposal encourages service providers to employ a swift and robust process for notifying potential victims is a useful step forward.



"However, from a technical standpoint, to be effective this new proposal will also completely undermine communications and systems security.  The proposal notably still fails to take into account decades of effort by researchers, industry, and policymakers to protect communications.  Instead of starting a dialogue with academic experts and making data available on detection technologies and their alleged effectiveness, the proposal creates unprecedented capabilities for surveillance and control of Internet users."  Again, "the proposal creates unprecedented capabilities for surveillance and control of Internet users.  This undermines a secure digital future for our society and can have enormous consequences for democratic processes in Europe and beyond."



So then they bring up five points.  The first:  "The proposed targeted detection measures will not reduce risks of massive surveillance."  They said:  "The problem is that flawed detection technology cannot be relied upon to determine cases of interest.  We previously detailed security issues associated with the technologies that can be used to implement detection of known and new CSA material and of grooming because they are easy to circumvent by those who want to bypass detection, and they are prone to errors in classification.  The latter point is highly relevant for the new proposal, which aims to reduce impact by only reporting 'users of interest,' defined as those who are flagged repeatedly," and they said, "as of the last draft, twice for known CSA material and three times for new CSA material and grooming."



They said:  "Yet this measure is unlikely to address the problems we raised.  First, there is the poor performance of automated detection technologies for new CSA material and for the detection of grooming.  The number of false positives due to detection errors is highly unlikely to be significantly reduced unless the number of repetitions is so large that the detection stops being effective.  Given the large amount of messages sent in these platforms, in the order of billions, one can expect a very large amount of false alarms, on the order of millions."



So they then had a footnote which explains how they draw this conclusion.  They said:  "Given that there has not been any public information on the performance of the detectors that could be used in practice, let us imagine we would have a detector for CSAM and grooming, as stated in the proposal, with just a 0.1% false positive rate, in other words, in a thousand times it incorrectly classifies non-CSAM as CSAM, which is much lower than any currently known detector."  Right?  So they're drawing like a best, absolutely beyond best possible case.



They said:  "Given that WhatsApp users send 140 billion messages per day, even if only one in 100 would be a message tested by such detectors, there would be 1.4 million false positives every single day.  To get the false positives down to the hundreds, statistically one would have to identify at least five repetitions using different, statistically independent images or detectors.  And this is only for WhatsApp.  If we consider other messaging platforms, including email, the number of necessary repetitions" - that is, you know, repeated hits on a given individual before you raise the alarm in order to bring down basically the rate at which alarms are being raised, you need to raise that number of repetitions, they say - "would grow significantly to the point of not effectively reducing the CSAM sharing capabilities."  Meaning detection would be effectively neutered.



Then they said:  "Second, the belief that the number of false positives will be reduced significantly by requiring a small number of repetitions relies on the fallacy that for innocent users, two positive detection events are independent, and that the corresponding error probabilities can be multiplied.  In practice, communications exist in a specific context, for example, photos to doctors, or legitimate sharing across family and friends."  They said:  "In such cases, it is likely that parents will send more than one photo to doctors, and families will share more than one photo of their vacations at the beach or pool, thus increasing the number of false positives for this person.  It is therefore unclear that this measure makes any effective difference with respect to the previous proposal."



Okay.  So in other words, the politicians proposed to minimize false positive detections by requiring multiple detections for a single individual before an alarm is raised.  But the science of statistics says that won't work because entirely innocent photographs of one's children will not be evenly distributed across the entire population of all communicating users.  People who have young families and like to share photos of their children frolicking at the beach in their bathing suits will generate massive levels of false positive CSAM detections because there is massively non-equal distribution of content that might falsely trigger CSAM detection.



The scientists explained:  "Furthermore, to realize this new measure, on-device detection with so-called client-side scanning will be needed.  As we previously wrote, once such a capability is in place, there is little possibility of controlling what is being detected and which threshold is used on the device for such detections to be considered 'of interest.'"  I should explain that another amendment to the proposed legislation involves their attempt, the legislators' proposal of attempting to divide applications, that is, you know, like WhatsApp as an application, Telegram as an application, to divide applications into high-risk and low-risk categories so that only those deemed to be high risk would be subjected to surveillance. 



The techies explain why this won't work.  They write:  "High-risk applications may still indiscriminately affect a massive number of people.  A second change in the proposal is to only require detection on parts of services that are deemed to be high risk in terms of carrying CSA material.  This change is unlikely to have a useful impact.  As the exchange of CSA material or grooming only requires standard features that are widely supported by many service providers, such as exchanging chat messages and images, this will undoubtedly impact many services.



"Moreover, an increasing number of services deploy end-to-end encryption, greatly enhancing user privacy and security, which will increase the likelihood that these services will be categorized as high risk.  This number may further increase with the interoperability requirements introduced by the Digital Markets Act that will result in messages flowing between what was previously low-risk and high-risk services.  As a result, almost all services would be classified as high risk.



"This change is also unlikely to impact abusers.  As soon as abusers become aware that a service provider has activated client-side scanning, they'll switch to another provider, that will in turn become high risk.  Very quickly all services will be high risk, which defeats the purpose of identifying high-risk services in the first place.  And because open-source chat systems are currently easy to deploy, groups of offenders can easily set up their own service without any CSAM detection capabilities.



"We note that decreasing the number of services is not even the crucial issue, as the change would not necessarily reduce the number of innocent users that would be subject to detection capabilities.  This is because many of the main applications targeted by this regulation, such as email, messaging, and file sharing, are used by hundreds of millions of users, or even billions in the case of WhatsApp.



"Once a detection capability is deployed by the service, it's not technologically possible to limit its application to a subset of users.  Either it exists in all the deployed copies of the application, or it does not.  Otherwise, potential abusers could easily find out if they have a version different from the majority population and, therefore, if they have been targeted.  Therefore, upon implementation, the envisioned limitations associated with risk categorization do not necessarily result in better user discrimination or targeting, but in essence have the same effect for users as blanket detection regulation."  So basically these guys are just, you know, they're cutting through these proposals one after the other, very carefully backing up their statements with, you know, actual data.



The second is:  "Detection in end-to-end encrypted services by definition undermines encryption protection."  They go over this again, explaining why that's the case.  Oh, and they note one of the other arguments is, and we've talked about this on the podcast, the idea of adding age discrimination.  Well, they said:  "Introducing more immature technologies may increase the risk.  And they note that their proposal states that age verification and age assessment measures will be taken, creating a need to prove age in services that before did not require so.  It then bases," they said, "some of the arguments related to the protection of children on the assumption that such measures will be effective.



"We would like to point out that at this time there is no established, well-proven technological solution that can reliably perform these assessments.  The proposal also states that such verification and assessment should preserve privacy.  We note that this is a very hard problem.  While there is research towards technologies that could assist in implementing privacy-preserving age verification, none of them are currently in the market.  Integrating them into systems in a secure way is far from trivial.  Any solutions to this problem need to be very carefully scrutinized to ensure that the new assessments do not result in privacy harms or discrimination causing more harm than the one they're meant to prevent."



So they conclude, saying:  "With secure paths forward for child protection," and this is really good.  They said:  "Protecting children from online abuse, while preserving their right to secure communications, is critical.  It is important to remember that CSAM content is the output of child sexual abuse.  Eradicating CSAM relies on eradicating abuse, not only abuse material.  Proven approaches recommended by organizations such as the UN for eradicating abuse include education on consent, on norms and values, on digital literacy and online safety, and comprehensive sex education; trauma-sensitive reporting hotlines; and keyword search-based interventions.  Educational efforts can take place in partnership with platforms, which can prioritize high-quality educational results in search or collaborate with their content creators to develop engaging resources.



"We recommend substantial increases in investment and effort to support existing proven approaches to eradicate abuse, and with it, abusive material.  Such approaches stand in contrast to the current techno-solutionist proposal, which is focused on vacuuming up abusive material from the Internet at the cost of communication security, with little potential for impact on abuse perpetrated against children."  So in other words, you politicians are aiming at the wrong target anyway.  So even if you got everything you want by effectively eliminating security and all privacy, it won't actually solve the problem that you're hoping to solve.



So I think the problem is that this is like an iceberg.  CSAM is the tip of the iceberg that is the visible manifestation of something that is abhorrent.  And because we see it, the tip of that iceberg, we want to get rid of it.  But these authors remind us that CSAM is the output, it's the result of these abhorrent practices, less so the practices themselves.  What I'm heartened by, as I said at the top, is that we appear to be seeing a true, honest back-and-forth negotiation in good faith between European Union politicians and European scientists and researchers.  Given that the original proposed legislation was significantly amended after their first round of objections and feedback, it appears that the politicians are heeding what their technocrats are explaining.  And of course we have no idea what's going to finally happen, which is what makes all this so interesting.  And it is obviously very important.  So stay tuned.



LEO:  Yeah.  It's so much easier to go after the symptom than the cause; you know?  So much - yeah, yeah.



STEVE:  Isn't it?  Exactly right.  That is exactly right.  



LEO:  And unfortunately there's huge side effects to going after the symptom that make for more problems.  So it's not really a great solution.



STEVE:  Yeah.  And nothing prevents the politicians from wanting to save face or look good by saying, "We did this."



LEO:  We fixed it.  It's all over.



STEVE:  Yeah.



LEO:  But, see, it's not going to be all over.  And really that's the nut of it.



STEVE:  And the best, yes, the most important reminder is that CSAM is the output of the practice, not the practice itself.  And it's the practice that you want to curtail.



LEO:  Right.  Well, good stuff, as usual.  You have no fear to go where angels fear to tread, and that's good.  That's good.  That's what we want.  You're going to hear it here.  You're going to hear it all.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#976

DATE:		May 28, 2024

TITLE:		The 50 Gigabyte Privacy Bomb

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-976.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  Why is Google's AI Overview fundamentally impossible today?  And what's the latest news on how to suppress it?  What's LastPass's decade-late announcement?  Why and when is a VPN not a VPN?  Are eMMC chips really impossible to replace?  Are vertical tabs finally coming to Firefox?  What's one well-informed listener think about Fritz!Box network appliances?  And what's just about the worst thing that could be done with four-digit PINs?  Were we guilty of WinXP abuse by exposing it to today's Internet?  And how can Security Now! listeners now send email directly to me?  Yes!  GRC's new email system is alive.  After looking at all of that, we're going to examine the latest crazy idea from Microsoft which deliberately plants a 50 Gigabyte Privacy Bomb right in the middle of all Windows 11 PCs.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We have lots to talk about.  He's going to give you his take on Windows 11 new Recall feature.  That's the 50 Gigabyte Privacy Bomb you've heard about.  Also, when is a VPN not a VPN?  Can you really replace surface-mount chips?  Are vertical tabs finally coming to Firefox?  And a new way to contact Steve directly.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 976, recorded Tuesday, May 28th, 2024:  The 50 Gigabyte Privacy Bomb.



It's time for Security Now!, yay.  We wait all week for this; don't we, kids?  And here he is, appearing magically like a wizard in a puffy of greasy smoke - what are you sucking your thumb for?  Steve Gibson of GRC.com.  Hi, Steve.



STEVE GIBSON:  Hey, Leo.  Great to be with you again.



LEO:  I called them kids, not you.



STEVE:  As I knew who you were talking about.  We are beyond, you know, we're discussing things that happen when you're no longer a kid.



LEO:  Mm-hmm.



STEVE:  Which they didn't really tell you about when you were kids.



LEO:  No.



STEVE:  It's like, ah, you old people, that's never going to happen to me.



LEO:  Well, we were talking on MacBreak Weekly about tinnitus, ringing in your ears.  Do you have that?



STEVE:  No.



LEO:  No.  You were smart.



STEVE:  I have - I said "no" too quickly.  Like right now, if I concentrate, I can hear probably that 9 kHz very faintly in the background.



LEO:  Faint, okay.  That's not bad, yeah.  



STEVE:  Yeah, so it's not bad.



LEO:  I hear it.  It's very predominant, very persistent.  So reason to talk about it, and I'll mention it on this show, too, I'm sure, is I'm doing this new FDA-approved process that uses electrodes on your tongue while you're listening to something, I'm not sure what, in your ears.  I'm going to get fitted for that a week from Thursday.  I'll let you know.



STEVE:  Yes, on your tongue it feels like pop rocks, kind of fizzy.



LEO:  Kind of, yeah, that's what people report, little tickling.  Yeah.  I can take that.  And they tune it.  They can say, they told me, they say if it's really bothering you we can turn it down.  All right.  But I want all the pop rocks, personally.



STEVE:  Well, you want to get the full dose treatment, whatever it's going to do, that's right.



LEO:  Geez, it's driving me crazy at this point.  I really need to.



STEVE:  Be really cool to see if it helps.



LEO:  Yes, I hope so.  Anyway, what's on...



STEVE:  Eighty percent of people got helped by it.



LEO:  Yeah, well, that's why I'm doing it.  I mean, it's not cheap.  But I thought, if 80%, if it works for 80%, maybe I should try it, yeah.



STEVE:  Yeah.



LEO:  Watch, I'll be in the one in five it doesn't.



STEVE:  Okay.  So we are closing in on 999.



LEO:  Yay.



STEVE:  And the good news is, that's not bad news.  So...



LEO:  Bad news for you.  Good news for the rest of us; right?  You have to keep working.



STEVE:  We've got another great fun episode here.  Today's episode is titled "The 50 Gigabyte Privacy Bomb," which we will be discussing at length.  But we're first going to talk about why Google's AI Overview is fundamentally impossible for them to do today.  And we're going to look at the latest news on how to suppress it.  As you would expect, you know, lots of people are coming up with various ideas.  And there are some cool solutions.  Also, I just thought I would bring up the fact that LastPass has announced something that is, yeah, maybe 10 years too late.  So thanks.  And also, why and when is a VPN not a VPN?  Are eMMC chips really impossible to replace, as I kind of offhandedly said last week?  We have a listener who takes issue with that.  Are vertical tabs finally coming to Firefox?  Everyone wants them.  No one seems to be able to get them, at least not natively.



Also, what does one well-informed listener think about the FRITZ!Box network appliance which came up last week?  What is the worst thing that could be done with four-digit PINs?  We have a listener who explains his firsthand experience with that.  And were we guilty of Windows XP abuse by exposing it to today's Internet?  Apparently someone feels we should have been a little kinder and gentler.  Also, yes, how can Security Now! listeners now send email directly to me?  GRC's new email system is alive.  And after looking at all of that, we're going to examine the latest crazy idea from Microsoft which deliberately plants 50GB of privacy bomb right in the middle of all Windows 11 PCs.



LEO:  Yeah, mm-hmm.



STEVE:  Where we'll be marching out our often-used, sometimes overused, "What could possible go wrong?"



LEO:  So you're saying we're not talking Little Boy, we're talking Fat Man on this one.



STEVE:  We're talking could we have done anything that more delights Chinese attackers?



LEO:  Oh, great.  Oh, good.  Oh, that's exciting.  All right.  We'll get to all that and the world-famous Picture of the Week.



STEVE:  Picture of the Week.



LEO:  Actually, I'm glad you chose the one you chose because there are far worse ones.



STEVE:  Yeah, we're not doing cockroaches.



LEO:  Don't do cockroaches.



STEVE:  I'm not doing the cockroaches one.



LEO:  Okay, good, all right.



STEVE:  No.



LEO:  All right.  All right.  We'll talk about that in just a little bit.  First, though, oy oy oy.  Picture of the Week time.



STEVE:  So, yes.  I gave this snapshot of Google's brilliant AI Overview the caption:  "Don't you just hate it when the cheese slides off the pizza?"



LEO:  Apparently some people do.



STEVE:  That's a real problem, you know, Leo.



LEO:  Yeah.



STEVE:  On the space station, you know.  Fortunately, Google AI Overview has the obvious answer.  And again, it's, you know, why, you know, of course this is what it would suggest.  So the question that prompted this was "Cheese not sticking to pizza."  AI Overview jumps in and says:  "Cheese can slide off pizza for a number of reasons, including too much sauce, too much cheese, or thickened sauce.  Here are some things you can try:  Mix in sauce.  Mixing cheese into the sauce helps add moisture to the cheese and dry out the sauce.  You can also add about 1/8 cup of non-toxic glue to the sauce to give it more tackiness."



LEO:  Mmm, yum-yum.



STEVE:  Oh, that's right.  Yes.  And it's not, like, some.  It was like last week we had two quarts of urine.  You were supposed - not to drink urine, but how much you should be consuming.  And here, you know, you do not - Leo, you don't want to overdo the glue.



LEO:  Nn-nnn, never.



STEVE:  So it's only 1/8 of a cup.



LEO:  You actually could do this with non-toxic Elmer's Glue or something like that.  You could do it.



STEVE:  Absolutely.



LEO:  Not sure I would recommend it.



STEVE:  You know, kindergartners are eating that stuff, so what the heck.



LEO:  Yeah, sure.



STEVE:  Okay.  So it turns out that this latest AI hallucination is based upon an eight-year-old Reddit posting, which was posted as a joke on Reddit.  And of course all of the other humans there knew that it was a joke and thought that was kind of funny.  The guy wrote:  "To get the cheese to stick, I recommend mixing about 1/8 cup of Elmer's glue in with the sauce.  It'll give the source a little extra tackiness, and your cheese-sliding issue will go away.  It'll also add a little unique flavor.  I like Elmer's school glue, but any glue will work as long as it's non-toxic."



Now, of course, Google's bot came along and scraped that up and thought, hey.



LEO:  Didn't know any better, yeah.  



STEVE:  There's an idea.



LEO:  Yeah.



STEVE:  Let's hold onto that until some asks why their cheese is not adhering to the bread.  In which case we'll give them a little handy help there.  Okay.  So, okay.  We're having some fun at Google's expense.  But this seems like a large can of worms which Google's new AI Overview would probably tell you to eat.  NBC News picked up on this trouble and offered some additional depth that I want to share since it's really not that funny.  Their headline under "Artificial Intelligence" was:  "Glue on pizza?  Two-footed elephants?  Google's AI faces social media mockery."  And then they said, as their subhead:  "A Google spokesperson said the company believes users are posting responses to uncommon questions on social media."



Okay.  So NBC said:  "Social media has been buzzing with examples of Google's new, 'experimental'" - and they have that in air quotes - "artificial intelligence tool going awry.  The feature, which writes an AI Overview response to user queries based on sources pulled from around the web, has been placed at the top of some search results.  But repeatedly, social media posts show that the tool is delivering wrong or misleading results.  An NBC News review of answers provided by the tool showed that it sometimes displays false information in response to simple queries.



"NBC News was easily able to reproduce several results highlighted in viral posts online, and found other original examples in which Google's AI tool provided incorrect information.  For example, an NBC News search for 'How many feet does an elephant have' resulted in a Google AI Overview answer that said 'Elephants have two feet, with five toes on the front feet and four on the back feet.'"



LEO:  Oh, my god.



STEVE:  "Some of the false answers verged into politically incorrect territory.  An NBC News search for 'How many Muslim presidents in the U.S.,' the results of which were first posted on social media, returned a Google AI Overview that said 'Barack Hussein Obama is considered the first Muslim president of the United States.'"



LEO:  What?



STEVE:  "Obama, however, is a Christian."



LEO:  Oh,  yeah.



STEVE:  "Google said this Overview example violated its policies and that it would be 'taking action.'"  I know what action they should take, Leo, and it's not the one they're going to take, apparently.



"A Google spokesperson said in a statement" - okay, so this is Google.  "The examples we've seen are generally very uncommon queries, and are not representative of most people's experience using Search.  The vast majority of AI Overviews provide high-quality information, with links to dig deeper on the web.  We conducted extensive testing" - you know, we spared no expense.  "We conducted extensive testing before launching this new experience to ensure AI Overviews meet our high bar for quality.  Where there have been violations of our policies, we've taken action; and we're also using these isolated examples as we continue to refine our systems overall."



NBC writes:  "It's difficult to assess how often false answers are being served to users.  The responses are constantly shifting, and on social media it's difficult to tell what is real or fake.  Some Google users have created workarounds to avoid the new AI Overview feature altogether.  Ernie Smith, a writer and journalist, quickly built a website that reroutes Google searches through its historical Web results function, which avoids the AI Overview or other information boxes that prioritize some results over others.  Adding 'udm=14' to Google search URLs strips the new feature from results.  Smith told NBC News that his new website has quickly gained traction on social media, surpassing the traffic of his entire decade-old blog in just one day.  Smith said in a phone interview:  'I think people are generally frustrated with the experience of Google right now.  In general, the average person doesn't feel like they have a lot of agency.'



"A Google spokesperson said the company believes users are deliberately attempting to trip up the technology with uncommon questions.  Some deeper dives into why the answers have gone awry suggest that the tool is pulling from surprising sources.  404 Media reported that a Google search query for" - and here it is - "cheese not sticking to pizza pulled an 11-year-old Reddit comment that jokingly suggested mixing Elmer's Glue into the sauce.  Even though Google has now removed the AI suggestion from searches for 'cheese not sticking to pizza,' according to an NBC News search, the top result is still the Reddit post, with the comment about Elmer's Glue highlighted.



"A Google spokesperson quoted that queries like 'cheese not sticking to pizza' are not searched very often, and are only being noticed because of the viral posts about wrong answers on social media platforms like X, of which there are many.  The same issue with an old Reddit comment also occurred in a search for 'how to rotate text in ms paint,' referring to the Microsoft Paint application.  The top Google search result, viewed by NBC News, directs the reader to a sarcastic Reddit comment that says to press the 'Flubblegorp' key on your keyboard."  They note:  "This key does not exist.  This example was originally posted on social media.



"Despite Google's assertion that the tool is working well for many users, mistakes of the AI Overview are continuing to gain visibility and hype.  Some of the answers that have been posted online seem to be fake, indicating that the trend has shifted from authentic errors to a new meme format."



Okay.  So a couple of comments.  First is, I think it's clear that it's wise to be extremely skeptical now about anything we see online in general these days, and not only Google AI Overview results that we receive, you know, but just as much any reports of bizarre and wonderfully wrong results.  Every time I've encountered one of these reports like I've been sharing, I've immediately worked to verify its authenticity as much as I can since there's clearly some strong motivation to invent non-existent high-profile, you know, funny failures.  But, you know, here's NBC News, who themselves searched for how many feet does an elephant have and was told "two."



My other observation is that I hope Google truly understands that there are two fundamental reasons why they're getting into trouble with AI Overview.  The first reason is how powerful and potent this would be if it were possible.  It would be truly amazing.  But that's coupled with the second reason, which is that what they are attempting to do is not even remotely possible - not yet, not today, not even close.



You know, I make no claims to being an AI expert.  But we've all been paying attention, and our intelligence is not artificial.  We know that the current level of AI development definitely falls short of comprehension.  These large language models, exactly as their name suggests, are capable of mimicking the output of an intelligent species whose actual intelligent output was used to train them.  But as we're finding out, there's a world of difference between seeming and sounding intelligent, and actually being intelligent.



So here's the problem.  Google is attempting to use automation to create an accurate factual summary overview of what the web contains without understanding the content that it's summarizing.  It should be clear to everyone that this can never work.  It is not possible to create an accurate summary of content for which there is no comprehension.  AI Overview doesn't "know" that glue should not be mixed with tomato sauce because AI Overview doesn't actually "know" anything at all.  Yet to do the job Google has given it, it must comprehend the content that it's accessing.



What Google appears to have completely missed here is somewhat astonishing, I think.  You know, it's that the job of displaying pages of links resulting from keyword matches is entirely different from attempting to extract truth and knowledge from the content behind those links.  Keyword matching and link ranking they know how to do.  Truth and knowledge extraction no one knows how to do.  Not yet.  Not today.  But unfortunately, that has not stopped Google, and it should have.



Okay.  And this brings us to the perfectly named website udm14.com, which our prolific Twitter poster, Simon Zerafa, tweeted to me.  Thank you, Simon.  Recall that when the string "udm=14" is included, you know, appended to a Google search query, it serves as a shorthand, asking Google to return its search results in what they term "web search mode."  Among many other things, their AI Overview system is not consulted in that case.  From that page at udm14.com I discovered another site named TenBlueLinks.org.  And of course 10 blue links is reminiscent of what Google was, you know, decades ago, back when I first discovered it and sent that second email from GRC's first email system out back then, when no one had ever heard of Google.  Ten blue links was what you got on the page.



So TenBlueLinks.org, which, with just a few clicks of the mouse, allowed me to instantly and as permanently as I want switch my default Google search to "Google Web" mode search in Firefox.  For example, for Firefox on Windows or MacOS the instructions are just, you visit TenBlueLinks.org.  Then you right-click in the address bar, and you get a dropdown menu which is enhanced by this site.  At the bottom it says "Add Google Web."  So you click on that.  Then you open the hamburger menu in the top right corner, choose "Settings," and then click on "Search" on the left.  And then in the "Default Search Engine" you will now have a new entry, "Google Web," which you then select, and you're done.  Now Firefox will use, by default, until you change it, this Google Web mode search for all your browser searches.



You know, and when I first read the instructions I thought, what?  But sure enough.  You just go to this website, and it adds this cool option in the dropdown menu from right-clicking in the address bar, the URL field, and then allows you to make it your default.  They've got instructions for Chrome on Android, Chrome on iOS, Chrome on Windows or Mac, and Firefox on Windows and MacOS.  So anyway, again, I commend our listeners:  TenBlueLinks.org.  It's a very cool site.



LEO:  You don't actually get the Overview on your Google search; do you?



STEVE:  Actually, I don't think I've ever seen one yet.



LEO:  Yeah.  So that's - what happened was, as part of Google's Experimental Labs, you could turn it on there.  And then briefly they made it default.  And that's when they got in all the trouble.  It's turned off now, I believe.  



STEVE:  Oh, no kidding. 



LEO:  I see it's still in Labs.  Yeah, I don't think anybody's actually getting it anymore.



STEVE:  So they backed out.



LEO:  Oh, almost immediately.



STEVE:  Of the full...



LEO:  Yeah, yeah.  They said, oh, whoops.  But Google keeps doing that.  It's amazing.  I don't know why.  It's amazing.  It just keeps happening to them.  But I don't think anybody's getting that now, unless you turn it on specifically.



STEVE:  Interesting.  So...



LEO:  I don't know if udm14 is different than that.  I think it is.  Because then I think you also don't get the knowledge graph...



STEVE:  Correct.



LEO:  ...and the suggested links and all that stuff.  So it's still worth doing that, yeah.



STEVE:  Correct.  And all of the image search stuff and all the other junk, yeah.  Yeah, well, yeah.  So the udm14, which is to say invoking the Google Web mode search one way or the other, it definitely cleans that up and suppresses all that.



LEO:  Yeah, yeah.  I stopped using Google Search a year ago so none of this affected me.  But I use Kagi.  I pay for it because I don't want ads.  And I think it's compromising.  It's terrible.



STEVE:  Yup.  Well, it has compromised Google Search; right?



LEO:  Really.



STEVE:  I mean, it's completely skewed what they return.



LEO:  Mm-hmm.  Yeah.



STEVE:  Okay.  So a piece in BleepingComputer caught my eye, mostly because of how pathetic the announcement seemed.  BleepingComputer's headline was "LastPass is now encrypting URLs in password vaults for better security."  To which I respond, gee, what a great idea.



LEO:  Finally.  Wow.



STEVE:  BleepingComputer wrote:  "LastPass announced it will start encrypting" - maybe not even quite yet, but it will - "start encrypting URLs stored in user vaults for enhanced privacy and protection against data breaches and unauthorized access.  The vendor of the popular password manager also notes that this new security feature is a significant step towards reinforcing its commitment to implementing zero-knowledge architecture in the product, so it's not just to protect data from external threats.



"LastPass says that due to restrictions in processing power in 2008, when that system was created, its engineers decided to leave those URLs unencrypted, lessening the strain on CPUs and minimizing the software's energy consumption footprint."  That's right.  It was good for the planet, everybody.  What a crock of you-know-what.  But let me finish just another two lines from BleepingComputer's piece.  They said:  "With most of the hardware performance constraints of the past now having been lifted, LastPass can now start encrypting and decrypting those URL values on the fly without the user noticing any hiccups in browser performance while enjoying ultimate data security.  LastPass says this is being done to enhance user security and comply with the company's zero-knowledge architecture."  So I don't know where to...



LEO:  Everybody else does this; right?  I mean, I know Bitwarden does.  I think everybody does.



STEVE:  Everybody.  Everybody else.  So, you know, okay.  It's true that the world was very different back in 2008 when Joe Siegrist designed the original LastPass architecture.  And I would believe that since the URLs the user was visiting were needed for on-the-fly matching, and since their privacy - again, back in 2008 - didn't seem like a big issue, Joe would have consciously and deliberately chosen not to keep them encrypted, especially given that everything else in there was.  You know, so his not encrypting the URLs at the time was obviously intentional.  But that was 16 years ago.



LEO:  Yeah.



STEVE:  Sixteen years ago.  And the flow of time really does impact what we would term "best practice" today.  Back then, most web sessions were only briefly encrypted during login, after which the connections dropped back to plain old HTTP.  And as we know from Firesheep, the now logged-in session cookies were completely exposed to the Internet, allowing those sessions to be impersonated easily.  That would no longer be considered "best practice" today, and no one does that anymore.  So as times change, what's considered reasonable changes along with it.







But computers have been plenty powerful for the past decade at least to handle on-the-fly URL decryption without introducing any discernible pause or overhead.  Back when we were talking about this, I noted that it would have been possible to keep the user's vault encrypted on disk and only decrypt it in RAM.  That was the decryption event that would have been one time only during browser launch, when the extension was coming to life.  It would have decrypted the on-disk storage into RAM, where it could then access it easily on the fly.  But what was actually stored in the computer and was available potentially to be stolen would have been kept fully encrypted.  So there have been ways to offer vault encryption at rest, without any problem, for a long time.



I suspect that the real problem is - and we talked about this at the time.  LastPass's parent, LogMeIn, was purchased by a purely financial private equity firm back in 2019 for, what, four point some billion dollars.  I mean, a ton of money.  And that new parent did not love it for anything more than the cash flow it could produce.  In any event, for anyone who may still be lingering with LastPass, I just wanted to note that, for what it's worth, your vault-stored URLs on your machine will now finally be encrypted at rest.  So good on you.  And Leo, let's take a break.



LEO:  Good on you, yes.



STEVE:  I'm going to share a piece of feedback that will lead on to our next bit of news.



LEO:  All right.  I don't think we're ever going to get any data brokers advertising on this show.  I'm just guessing.  I'm just guessing.  All right.



STEVE:  Yeah, that would be a difficult...



LEO:  I wouldn't do it.  I wouldn't do it.



STEVE:  So I'm going to take one piece of listener feedback out of sequence, ahead of the pack, because of the P.S. that Andrew included, which I'll get to in a second.  So this is from Andrew Gottschling, who said:  "Hey, Steve.  I wanted to provide some feedback to Hakku's comment on VPNs and Firewalls.  It's probably not an option for many, if not most, corporate users.  This is because many corporations these days, and all of the ones I've worked for thus far, utilize 'split tunneling' on their VPNs to reduce bandwidth usage for high bitrate communications that are common, for example, voice and video calling on Teams or Slack.  Therefore, simply blocking all traffic from leaving on anything other than the VPN interface, unless it's to the VPN concentrator, would not be feasible in these cases, especially in the case of something like Slack, which runs in AWS, and their IP range is very dynamic.  Love the show.  Thanks for all you do.  Andrew."



So Andrew's exactly right, and this could be a problem with any VPN that insists upon forcing all of the system's traffic through its tunnel, and its tunnel alone.  The problem we're running into sort of more broadly is that we're tending to use the term "VPN" generically.  Like there's only one sort of VPN, you know, a VPN only does one thing, as if they're all created equal.  But that's not the case.



For example, the VPN that a typical roaming consumer in an Internet-equipped caf, airport, or hotel might want installed on their laptop would be a VPN that proactively refuses to allow any packet traffic in or out of that machine that does not travel through its tunnel.  What such a consumer will want is full protection.  This is contrasted against, for example, an IT-managed enterprise setting where a great deal of attention has been paid to exactly which traffic flows where.



For example, headquarters might have several satellite offices located elsewhere in the world which need to participate on the same corporate network, as if they were, you know, attached.  And since that traffic cannot safely be exposed to the Internet, static VPN tunnels would be established to securely interlink the satellite offices no matter where they were.  In this case, only the traffic that's bound for network addresses at the other end of a VPN tunnel would be routed there, with all the other local traffic allowed to have contact with the Internet directly.



So, you know, these are all just differing applications for private virtual networks where, you know, that's sort of a generic umbrella term.  The common factor is that traffic is being encrypted and decrypted as it flows between one or more local and remote IP addresses.  And part of what's so cool about VPNs is that they are, you know, V stands for Virtual.  They really are a virtualizing technology that is very flexible and very powerful.



Now, as I said, I chose Andrew's note because it arrived, Leo, via email, addressed to "securitynow@grc.com."  And Andrew ended his note with a P.S., which read:  "P.S.:  This new email system is REAL [all caps] slick.  Glad to get rid of Twitter."



LEO:  You have set something up, haven't you.



STEVE:  What email system, you ask?  Well, since you asked, it's GRC's new email system.



LEO:  Ah, excellent.



STEVE:  I finally have the long-awaited email announcement for GRC which features for this podcast a simple means for our listeners to send feedback and thoughts to me through spam-proofed email.  As I mentioned last week, GRC's been without any form of subscription email news system since I shut down the first system, which I wrote 25 years ago, back in 1999.



The completion of SpinRite v6.1 created my need to announce it to 20 years' worth of SpinRite 6 owners, and it would be nice to be able to send news of new things I create to those who would like to know of them.  For example, I do have plans to revisit ValiDrive, which has turned out to be extremely popular.  I've got a list of things I want to do for ValiDrive 2.0 that'll just be a little quickie update, but very useful.  And of course GRC's DNS Benchmark, which continues to be the most popular download we have, could use a bit of attention as DNS servers come and go.  So that's on the side of sending email out.  What about receiving feedback from our listeners?



Just yesterday, I received a very useful DM tweet from someone who said he created a Twitter account just so he could send me that tweet.  And as we know, many of our listeners have had to do so.  On the one hand, I'm deeply honored that our listeners are as interested in engaging as so many are.  I'm blown away by that.  But on the other hand, I'm horrified that the bar has been set so high by the need to join any social media service just to send me some thoughts, or a link to something that might be of interest to our listeners, especially when everyone already has email.  Email is the obvious common denominator.



Now, before I go on, just for the record, allow me to reiterate one last time, because I know there are still some people who need to hear this:  This has absolutely nothing whatsoever to do with Elon Musk's ownership of Twitter.  Really.  Nothing.  I could care less.  For one thing, I am barely a Twitter user.  When I start working on each week's podcast, I check in with Twitter to collect all of the tweets I've received since my previous check-in the week before.  I don't even look at it during the week.  I scan through those, replying when I can, and that's been where our listener feedback has mostly come from every week.  As everyone knows, I've never followed anyone on Twitter.  So I've never used it the way it was intended to be used.  As a consequence, I'm not directly aware, you know, of what may have changed after Elon's reluctant purchase of Twitter, other than things I've heard secondhand.  So I could care less.  I just want to lower the bar for all of our listeners.  And everyone has email.  The normal downside of asking people to share their email addresses is that the implied trust might be abused.  I think everyone knows that will never happen with me.



Until this past weekend I've not had a workable means for receiving incoming email from our listeners.  Now I do.  GRC now has the subscription management front-end of its new email system up and running.  It's what I've been developing for the past few weeks, and of course it's all written in assembler because that's just where I'm most comfortable.  It's now possible for anyone who wishes to, to optionally subscribe to any one or more of our three mailing lists.  One is aimed at our commercial product owners; one is aimed at general GRC news of products, freeware, services, et cetera; and one is intended for this Security Now! podcast, which has become a significant part of my life through these past 20 years.



But, and this is crucial:  You do not need to be subscribed to any of these lists to be able to send email to securitynow@grc.com.  There's no requirement for anyone to subscribe, although of course everyone's welcome to if they wish.  Here's the requirement:  The email address from which you are sending email to me does need to be known to the system.  So here's how you register:  At the top of every GRC page, in the page's header, is a little white envelope with an "Email Subscriptions" link.  There's also a link under the Home menu. And, as you might expect, it's also just grc.com/mail.



So you go to grc.com/mail and enter the email address you wish to register with GRC.  GRC will send an email to that address containing a link back to your own subscriptions page here.  And as you'd expect, everything defaults to "unsubscribed."  I don't ever want to send anyone any email they don't want to receive.  But if you wish, you can optionally provide your name and join any of the three lists shown there.  Then, either way, click the "Update Subscriptions" button, and your confirmed email will then be known to GRC.



From that point forward you can simply address anything you wish to the email address securitynow@grc.com.  No exclamation point or hyphen or anything, just securitynow@grc.com.  When that email arrives at GRC's server, the sender's address will be looked up; and if it's known to the system, the email will be accepted and will appear in my SecurityNow account inbox.  If email you send to securitynow@grc.com is rejected and bounces back to you as undeliverable, you'll know that something went wrong somewhere.



So that's the front end system.  The back end is the part that contains the subscriber database and actually sends email to the lists.  I should mention that at this exact moment, due to a limitation that the back end had, this new system is unable to accept email addresses containing plus signs, which I'm sure our listeners would like to use.  The back end has been fixed, but I haven't updated my code yet because it just happened yesterday, and I haven't, you know, I've been working on the podcast.  So that'll be the first thing I do later today.



And as for the back end, all I have running and tested at this point is the subscription management.  So please do not be surprised when you don't immediately start receiving email from me.  It's not you, it's me.  Since the industry has become so spam-sensitive, I plan to proceed with caution to be very sure that any bulk email I send meets all of today's anti-spam technical and legal requirements, and there are many.  So it will likely be another week or two before email begins to flow.  While I hope to be able to send weekly podcast summaries and links, the other two lists will always be very, very low volume.  I think over the eight-year life of the previous email system, I sent a total of 11 pieces of email.  So, you know, no one's going to get spammed.  You'll be wondering what's going on.  If you weren't listening to the podcast, you would wonder where I went.



But today, the new incoming email system filter is in place.  And frankly, I have no idea what will become of my use of Twitter.  It's trivial for me to tweet the weekly summary of the podcast, you know, a link to the show notes and the Picture of the Week.  My ambition is to deliver the same thing via email, but I'll be doing that somewhat cautiously as we see how it goes.  And I should note that I have recently noticed a significant uptick in spam to my, Leo, as you always mention, my  Open DM channel.  You know, I got one this morning:  "Hello.  My sister saw your profile while browsing X on my phone, and she's interested in you."  And then I have a link: e.yqyh571.xyz.



LEO:  Oh, I know them.  They're great, yeah.



STEVE:  Yeah, that's right.  "Open the link to complete the registration, and she will take the initiative to call you."  And I've got four different emojis.  And by the way, the emojis differ every time I receive this, although the text is always the same.  And it says:  "Remember to say hello to her.  She's very shy."  Right.  And she's also going to be very lonely in this case.  So anyway, if the spam becomes a lot worse, I'll likely be forced to abandon open DMs.  So the establishment of this alternative channel is coming at an opportune time.  The bottom line is I'm very excited to finally be adding this long-missing piece of GRC's infrastructure.  It's been crazy that we've had no means of announcing new stuff.  And once the dust settles from that, I'll begin sending out the news of SpinRite 6.1 to all 6.0 owners.  So, very pleased.



Okay.  So some closing-the-loop feedback.  A listener, Hatcher Blair, said:  "Hi, Steve and Leo."  He described himself as a "medium-time listener" and huge fan of the work we do.



LEO:  Medium is good.  I'll take medium.  I'm happy with medium.  That's fine.



STEVE:  A medium-time listener, exactly.  So that's what, he came around maybe 10 years ago, something like that?



LEO:  Yeah, something like that.



STEVE:  Jumped in about halfway along, yeah.  So he said:  "I hope this is still the appropriate place to contact you as I made a Twitter account just for this."  So bless you, Hatcher, in the future you can send email to securitynow@grc.com.  Anyway, he said:  "I just listened to SN-975, and I wanted to thank you for alerting me to the Web Search option in Google."  So again, and I'm glad you pointed this out, Leo, not just to remove AI Overview, but to clean up the pages significantly.



He said:  "I wanted to make it my default search option, but you cannot add the search engine to Chrome or edit the Google Search engine in Chrome's settings.  However, you can create an extension which adds a Web Search engine and make it the default.  I made a simple Chrome extension that makes Web Search the default option when searching from the address bar.  This extension is not and cannot be published on the Chrome web store because I use the domain https://google.com and would need to have ownership of that domain to publish the extension.  Although it's not on the web store, it is on my GitHub for anyone that wants to clone the repo and install it for themselves.



"A warning to anyone who wants to install the extension."  He said:  "It is bad."  I don't know what he means.  He says:  "All the extension does is make the default search," you know, and then he shows a search query with the &amp;udm=14.  He said:  "There is no localization support or option to enable or disable the extension in the UI.  If you end up sharing this on the show" - here I am, doing that - "feel free to share the repo, and anyone who wants to contribute is welcome to.  Anyone is also welcome to use anything on the repo for their own purposes if wanted.



"I did a little bit of googling, and making a similar extension should be possible in Firefox.  It might even be easier as Mozilla seems to have much better documentation than Google.  Keep up the great work and looking forward to episodes 999 through infinity," says Hatcher Blair.  So I've got a link to his GitHub repo in the show notes.  And just another piece of work along the lines of the TenBlueLinks.org that we talked about earlier, you know, this one from one of our listeners.  And as I said, I'm sure this will be very popular moving forward.



Defensive Computing's Michael Horowitz wrote to say:  "Steve, a fun story.  I recently got a fairly standard scam email message claiming my computer had been hacked and asking for Bitcoin.  As proof of the hack, the bad guy told me my password.  But I use a different password everywhere.  Have for years and years.  So the revealed password told me the service that had been hacked, and I logged onto it and changed that one password.  It had a stored credit card, but fortunately that had expired."  He says:  "It's rare to actually experience, firsthand, up close and personal, the benefit of never re-using a password."



So thanks for sharing, Michael.  I think that's very cool.  As I've been perusing the email domains of our listeners who have been subscribing to GRC's new service since I announced the email system on Twitter yesterday, I've seen many gmail.com email domains; but also, as indicative of the listeners we have, many personal domains.  As we've discussed, unfortunately,  there's no good way to hide from tracking when websites are willing to trade their visitors' privacy for cash by colluding with advertisers and other data aggregators.  Not even a personal domain will help with that.  But it can be very useful for tracking down personal information leakage.



I established a unique email address for the dealership that services my car.  So when I started receiving unwanted spam from some auto-related source, from that one email address that I had never shared with anybody else, I knew who had leaked it.  So, yeah.  Even though it won't help for tracking, it is a little bit satisfying just to be able to say, uh-huh, I got you.  And of course I'm then able to retire that email address if the spam becomes annoying.  And when they wonder why they're unable, when the dealership can't send me email, I say, oh, yeah, I changed that because you guys sold my address to a third-party commercial entity.  So here's the new one.  And I imagine, Leo, that you at Leoville.com...



LEO:  Oh, it's unusable, yeah.



STEVE:  Oh, yeah, Leo is unusable.  So imagine...



LEO:  Well, also, oh, no, I don't even use that.  I mean, I do, but I don't.  I have lots of solutions around this.  



STEVE:  Right.



LEO:  Similar to yours, but not.



STEVE:  Right.



LEO:  You have to.  I have many addresses.  I can't use laporte@gmail anymore.  That really went downhill fast.  But no, what I do now when I sign up for something is I use - I actually don't use those unique passwords, unique email addresses that Bitwarden and Fastmail do.  I just make it the name of the company at a particular address that I haven't used before, you know, that I use exclusively for that.



STEVE:  And you have a catchall so that everything comes in?



LEO:  Yeah, right.  I don't have to worry about that in Fastmail.  I have about 10 domains or 15 domains that get email at Fastmail.  It all goes in the same inbox.  And then I can do sorting based on the address it thinks it's going to and stuff like that.  It's, look, spam is a mess.  It's just a mess out there.



STEVE:  Oh, Leo.



LEO:  It's terrible.



STEVE:  Yeah.  Well, and as somebody facing the task of sending subscription email, you know, I mean, I'm...



LEO:  Well, yeah, yeah.  That's the biggest issue.  It's not that we're getting a lot of spam.  It's just really hard to send email out now.  Google will not accept email if it doesn't have  DKIM, SPF, and DMARC authentication.  They just won't even accept it.  So it's gotten really - that's where it's really, it's much harder.  Deliverability has gone downhill.



STEVE:  Yes, yes.  And in fact in my instructions, as does everyone, I say, if you don't receive the confirmation email, you know, look around for it.  You know, check your spam folder or wherever it might have gone.



LEO:  Yeah, that's a must also, yeah.



STEVE:  Yeah.



LEO:  I think Google loves this because it means that eventually they hope everyone in the world will use Gmail.  And that'll solve it.  Sort of.



STEVE:  Oh, you mean from Gmail to Gmail.



LEO:  Yes.



STEVE:  Wonderful.



LEO:  That solves it.  You know, as long as everybody's using Gmail, we can get rid of spam.  We can authenticate.  It's just it's our fault for not using Gmail.  That's Google's attitude.



STEVE:  Right.  That's right.



LEO:  That's Google's attitude.



STEVE:  So Elliot.Alderson tweeted:  "Hey, Steve.  One extra way to avoid Google's AI Search."  He says:  "Don't sign in."  He says:  "I've never seen any of that AI nonsense.  I have a different browser profile for signing into Google, and I clear it whenever I'm done with whatever Google account management I need to do."



Huh.  Okay.  So I can't speak to that myself.  I don't do Google account management.  And like you, Leo, I've never run across that.  And now we learn from you that it's gone.



LEO:  You won't.  Yeah, it's gone.  You won't anymore.  It's still crappy search, but at least you won't see an AI Overview.



STEVE:  Yes, exactly, telling you to eat glue.  Steve Murray said:  "Steve, just FYI, you can replace soldered motherboard components like eMMC/RAM."  He says:  "A ton of YouTube videos cover it."  He says:  "The hard part is doing it in an economically viable way if not doing it DIY."  Okay.  Now, I would argue...



LEO:  Just because it's on YouTube doesn't mean it's possible.



STEVE:  I would argue that, right, that the hard part is doing it at all.  I've been soldering electronics, literally, I'm not kidding, since I was four years old.  I still recall my dad's big honking soldering iron.  It was about 3/4 of an inch in diameter and 18 inches long, with a wooden handle.  It was nothing like what we have today.  This thing took about 30 minutes to come up to temperature, at which point you could push its tip through a solid steel plate.  And while growing up, my standard Christmas present was a Heathkit, which I would receive every Christmas Eve.



LEO:  Aww.  That's so cool.  Aww.



STEVE:  I would open it on Christmas Eve, and it would be fully assembled by Christmas morning...



LEO:  That's so cool.



STEVE:  Since I had no interest in sleeping with an unfinished kit in front of me.



LEO:  No wonder you're a geek.  Now I get it.



STEVE:  Oh, and I remember, Leo, the VTVM, the Vacuum Tube Volt Meter that I built, I must have, let's see, I was still living on Overhill in Orinda, and I left Sleepy Hollow Elementary in the middle of the fifth grade.  So I was, what, eight, I guess?



LEO:  Wow.  Wow.



STEVE:  And Dad - one Christmas was a shortwave radio receiver.



LEO:  Holy cow.



STEVE:  One Christmas was, you know, all Heathkits.



LEO:  Was he an engineer?  Is that why he really encouraged this?



STEVE:  Yeah, he was - he had his masters in engineering from Berkeley.  But mechanical engineering, not electrical engineering.  But there's a lot of overlap.



LEO:  Right.  Sure.



STEVE:  And, you know, he set me up with a battery and knife switches and light bulbs because I just had to understand how all this stuff worked.



LEO:  That's so great.



STEVE:  Anyway, so, while, yes, technically I agree that it's possible to remove and replace today's modern high-density surface mount components, doing so - because I have - is neither fun nor easy.



LEO:  And it's really easy to screw up the substrate, the motherboard; right?



STEVE:  Oh, so easy.



LEO:  Yeah.



STEVE:  Especially when they're surrounded on all four sides with a forest of tiny pins on half-mil centers, or when it's a BGA, which is a ball grid array chip, with its myriad connections underneath the chip itself.  I'm sure anybody who's looked at a modern motherboard or circuit board, you see this chip sitting there with no obvious connections to it.



LEO:  Right, right.



STEVE:  It's because they're little dimples on the underside of the chip.  You've got to heat the whole thing up in order to melt the solder of them all at once in order to pull this thing off the circuit board.  Anyway, the reason there are a ton of YouTube videos is that it's actually not possible to do.



LEO:  It makes for an excellent YouTube video. 



STEVE:  It'll hold you on the edge of your chair.  Is it going to come off?  Is it going to come off?



LEO:  You know, just because you see somebody doing it on YouTube does not mean you can do it, or anyone normal or even that they...



STEVE:  Yes.  Hank is able to show me how to create a recipe which I am unable to reproduce.



LEO:  Yeah, exactly, yeah.



STEVE:  So the fact that Hank is able to do it does not mean that I can.



LEO:  My son, yes.  No, I watch with wonder myself.



STEVE:  Sylvester said:  "Replying to @firefox," he said, "@SGgrc Vertical tabs are coming!"  So his tweet sent me off looking, and I found a posting by Martin Brinkmann over at gHacks.net.  Martin wrote:  "Mozilla released a Firefox Nightly test build recently that includes support natively for vertical tabs.  This new functionality is not available in regular Firefox Nightly builds, but there is a way to get that build and test it for yourself.  Native vertical tabs support is a highly requested feature.  It is placed third currently on Mozilla's Connect website, just behind native tab grouping, and the restoration of Progressive Web App support in Firefox.



"Vertical tabs," he says, "move tabs from a horizontal bar at the top of the browser to the side.  It enables better drag and drop support, sorting, hierarchical views, and better use of space on widescreen monitors or sites that limit their width.  Firefox would not be the first browser to support vertical tabs.  Several browsers, including Microsoft Edge, Brave, or Vivaldi, support vertical tabs already," and he says, "with Vivaldi taking the cake when it comes to customization options.  There has always been talk of introducing vertical tabs in Firefox.  The last time was in February of 2022, when Mozilla looked into the matter."



Okay.  So I don't understand this.  Vertical tabs are such an obvious improvement for modern web browsing that it is difficult for me to understand what's taken so long.  Fortunately, I've had vertical tabs in Firefox thanks to the browser's sidebar that can be used to contain browser tabs.  I use the add-on Tree Style Tabs, which works wonderfully.  And then I tweaked the browser's UI, the CSS style sheet, to hide the tabs across the top, which are still there.  So although I've found a solution to place tabs to the side, where they should have been immediately moved once our screens moved away from their original 4:3 aspect ratio, it will be wonderful for Firefox to offer them natively.  So let's hope that happens because, like, what's the problem?  It's just so obvious.



LEO:  You raise an excellent point.  We have wide, wide screens.  There's lots of room.



STEVE:  Yes.



LEO:  On the left.  I hadn't really thought about it that way.  Yeah.



STEVE:  Yes, exactly.  And many sites, in fact, my site looks weird because it sets the width to 85% of the browser window, which on today's browsers is wrong.  So as I'm rewriting pages, I'm changing the way that works.  As a consequence, there's a lot of empty space on the sides.  So anyway.



LEO:  Yeah, yeah.  I put my dock on the left for that reason on my Mac, yeah.



STEVE:  Right.  And we're an hour in, Leo.  Let's take our third break.



LEO:  Let's go to work.  Let me do some work while you relax.  It's my turn.



STEVE:  I've got coffee.



LEO:  The world's largest mug of coffee.  Hey, you're going to like - whoa.  Steve?



STEVE:  So Tal in Israel, he said:  "Back at Episode 970 you read a listener's feedback about a SOHO router that requires you to press a button on the router in order for configuration changes to be applied."  We've talked about this is now recommended behavior for future routers, in order to minimize the ability of attackers to do a purely electronic, non-present change of configurations.  Anyway, he said, speaking of the listener feedback, said that "A well-known router manufacturer named FRITZ!Box has been creating such routers, where configuration changes require the press of a button on the router.



"I've been looking for a new router, as my old Xiaomi router stopped receiving updates in 2021.  Xiaomi is notoriously known for not providing many updates to devices after they've been sold.  Also, that router was always underpowered, dropping WiFi connections and being generally unable to handle my needs.  I remembered the name FRITZ!Box, looking around a little, and it seems the company who manufactures them is very security-aware, and the performance of them is very good.  I was happy to discover an Israeli seller, and bought the FRITZ!Box 5530, which seems to be what is most suitable for me.



"After I've been using it, I think it's the best router I've ever had.  It does not even break a sweat with multiple video streaming and downloading and anything else I do.  And I think it can serve as an example of how SoHo routers should be.  First, it comes with automatic updates turned on by default.  Second, both wireless key and router admin passwords are randomized when you get one.  And if you reset it to factory default, those passwords will be reverted back.  There's a very durable sticker on the bottom of the router with them so you should not worry about losing them.



"Changing some configurations like DNS will require you to go and press a button on the router.  But since it can also serve as a telephony hub, if you have a phone directly connected to it you can pick it up and dial some number it tells you to dial in order to apply the configurations.  Or you can define an authenticator app and then use the six-digit token to apply changes.  Other nice things it supports" - oh, by the way, that's a cool feature, right, because now you've got a way where you don't have to physically be present, but you do have another authentication token that is changing dynamically to prevent someone from making changes electronically.  That's something we hadn't talked about.  That's brilliant.



And he says:  "Fourth, other nice things it supports is DNS over TLS so your ISP will know nothing of your DNS queries."  And he says:  "I use both Google and Cloudflare OpenDNS resolvers, which I trust way more than my ISP provider.  And finally, fifth, FRITZ!Box is well known for supporting their devices for a long time."  He finishes:  "It has many other features where you can definitely see that security awareness went into the design.  So whoever mentions FRITZ!Box in Episode 970, thank you.  Unfortunately," he said, "I could not find your name in the transcripts."



Well, okay.  Since this is all about listener feedback, I wanted to keep this thread alive by sharing this listener's very positive experience with FRITZ!Box.  I brought up a site web search of GRC some time ago, so I went to GRC and put "FRITZ!Box" into the search field at the upper right of every page.  That brought up all of our many mentions of FRITZ!Box through the years, as well as some comments over in GRC's forums.  The listener who tweeted the news to us in Episode 970 used the Twitter handle "ndom91."  So we still don't know his name or who he or she is, but thank you again for the mention.  And thank you, Tal, for sharing your impressions.



I went over and looked at their lineup.  It's a German company.  They've got, fortunately, an English language website.  And it is very impressive, I have to say.  I especially liked the integrated DOCSIS 3.1 cable modems and routers.



LEO:  Oh, nice.



STEVE:  They have several devices.  Yes.  And I've been sort of unhappy with domestic cable modem suppliers.  They, you know, they really don't seem to be doing a great job.  This German firm, you know, really does seem to have their act together.  So if we ever get fiber in our area, they also have an integrated fiber modem router.  I might take a look at that.  And they're all WiFi 6 and even 7.  So they're keeping up to date with the standards, yeah.  Look at that thing.  That's just beautiful.



LEO:  Yeah, it really is.



STEVE:  And somehow they're doing a good job without lots of antennas sticking out everywhere.



LEO:  Well, I think that's - the antennas are a marketing thing, I think, yeah.



STEVE:  Uh-huh, exactly.



LEO:  Oh, this is cool.



STEVE:  Yup.  And there's a WiFi 7-enabled cable modem router that I'm seeing there.



LEO:  Yeah.  And with Zigbee onboard so you can control your smart home. 



STEVE:  Yup.



LEO:  Oh, these are nice.  This is really nice.  This is for LTE, so you use mobile broadband.



STEVE:  They are solid-looking devices.



LEO:  Of course, you've got to make sure that your cable company will support their...



STEVE:  Yes, that's the thought I had was that the Cox does, you know, they make a point of saying, well, these are the ones we support. 



LEO:  Yeah.  This is the one I would get, though, if I - interesting.  3.1, WiFi 7.



STEVE:  And apparently really strong.



LEO:  Yeah.



STEVE:  As he said, you know, they're not, like, they didn't cheap out the processor and RAM.



LEO:  I like that, yeah.  Look at that.  Okay.  FRITZ!Box.



STEVE:  And a telephone system.  It does telephony built-in, too.



LEO:  Oh, yeah, with DECT Base Station for cordless telephony.  Wow.  Wow.  They put everything in here but the kitchen sink.  Oh, wait a minute, here's a kitchen sink.



STEVE:  Oh.



LEO:  No.  At least use a kitchen.  And their operating system is called FRITZ!OS.  No.  FRITZ!OS.



STEVE:  Too bad it's not Fritos.



LEO:  It's so close to Fritos, I want to get it, yeah.  So it's not - you can't put WRT or something else on there.  This is their...



STEVE:  No, I don't think you - but I would bet that they've got a beautiful-looking built-in router.



LEO:  Well, it's made in Germany, so it's got to be good; right?



STEVE:  Jawohl.



LEO:  Look at that, yeah.  Wow.  Okay.



STEVE:  Okay.  So Richard Green in Lethbridge, Alberta, Canada, his subject was "Four-digit PINs in a corporate environment."  Okay.  Get a load of this, Leo.  He said:  "Hi, Steve.  Absolutely love the show.  Thanks for doing it.  I thought you might enjoy this story.  I'm a physical security installer, as in physical alarm systems, and I was asked to do a system audit and upgrade on a major chain grocery store.  So we came out and gave everything a physical check up and upgraded their equipment.  We then asked for their list of current users so we could verify and remove any old and unused alarm-disarming PINs.  At first, they didn't want to do this, and I figured it was a corporate policy.  But then they relented and started printing off pages of names and PINs.  Pages and pages of PINs.



"Apparently, some higher-up decided it would be a great idea to have every manager, assistant manager, or anyone else of importance nationwide, programmed into every store's alarm system just in case they might travel.  We started out, of course, with 10,000 possible PINs, but their list was nearly 7,000 PINs long.  This meant that any random guess would have a 70% chance of disarming their alarm system at any facility."  He said:  "I flat-out refused to be the guy to set up a system that was so insecure.  Luckily for me they finally relented, and we only added about 60" - six zero - "user PINs, local to our region.  I wouldn't have believed it had I not seen it for myself."



Amazing.  So Richard, thank you for sharing that horror story from the field.  It's helpful, I think, to see, like, the way things are actually being done out there in the real world, you know, way far from the ivory tower.



Manuel Schmerber, sorry for messing up your last name, Manuel, in St. Louis.  He said:  "Hi, Steve.  I noticed that the UDM value just selects from the menu of search options:  15 is attractions, 12 is news, 14 is web, and so on.  I also noticed that an easier way for me to get to the simpler Web results is to just select from the menu of offerings below my search phrase.  I select the More dropdown and then Web."  And he says:  "Thanks for the lowdown on search."



So I just wanted to thank Manuel for demystifying the "magic 14" of the UDM value.  I didn't spend any time digging around, and I'd been wondering where the 14 came from, you know, why was it UDM=14.  And yes, it is certainly possible to select the Web menu item from Google's already displayed results.  But the various hacks that are emerging allow us to get those same "web only" results right from the start with our browser's default search.



Vern Mastel in Mandan, North Dakota.  Oh, I got a kick out of this.  The subject was "SN-975 Windows XP Test."  He said:  "The Windows XP report is misleading.  The test was not fair.  What Parker did was test a 1935 Chevrolet sedan on a modern eight-lane superhighway at rush hour.  He should repeat the test with a new out-of-the-box configuration Windows 10 or 11 machine on the same, no router, open Internet connection.  That would be very interesting."



Vern said:  "Windows always has come out of the box with EVERYTHING" - that's all caps - "turned on.  I claim that the biggest holes in that test XP machine were Windows File and Print Sharing and Windows Remote Desktop.  Both are wide open in a fresh Windows XP install.  Such a machine should be dead meat on today's Internet.  So that begs the question, what ISP was used for the test?



"When XP was new in 2001, ISPs did not do any active protocol blocking.  Windows NetBEUI/NetBIOS ports 137, 138, 139, and 445, along with many others, were open to the world.  For example, with File and Printer Sharing turned on, the default, you could see and easily access other Windows XP machines in the vicinity.  For many years, when I set up a new Windows XP machine on the networks I administrated, I spent an extra hour changing network and system settings to close security holes, and shut down or remove the many unneeded features.



"Now things are pretty well locked down at the ISP level.  Old LAN protocols are blocked by default, you cannot run your own mail server out of your house, and other server protocols like FTP are monitored or blocked outright.  Properly configured, XP is/was a stable, reliable, and reasonably safe version of Windows."



Okay.  I agree with everything Vern said, except for his thesis that this was in some way not a fair test.  I have a problem with that characterization only because it wasn't meant to be a test of fairness.  It was a test of reality, or perhaps a test of yesterday's reality versus today's reality.  And of course everything Vern noted about the way he would first spend his first hour with any new Windows XP machine was the reason I created the ShieldsUP! port probing facility.  It was precisely because these early machines from Microsoft were such a disaster on the Internet.



That said, I also agree with him that it would indeed be interesting to place a currently fully patched Windows 10 or 11 machine directly on the 'Net to see how it would fare.  You know, direct exposure to the Internet.  Given that all Windows machines have a very competent application-driven firewall that is up and running before the rest of the vulnerable networking behind it comes to life, I would expect it to do well.



But in any event, Parker's whole point was to get some sense for the malicious crap that is circulating out on the Internet right this moment.  We're all so well insulated and so well shielded behind our NAT routers and firewalls that it's possible to sort of forget just what's out there constantly pounding away at those defenses.  These defenses that we have today are absolutely not optional.



Okay.  And lastly, Jeff Smock says as his subject:  "I offer you my very own First Law of Cloud Data Security."  He said:  "Forget about all the bluster and jazz hands the cloud service providers give us regarding the security of our data."  He says:  "Here is the simple truth:  'The security of cloud data is inversely proportional to its potential value as perceived by a hacker or rogue staff member."  So, yes, the more they want it, the bigger problem we're going to have keeping them from it.  So I completely agree with that characterization.  And Leo, let's take our final break, and then we're going to talk about the 50 Gigabyte Privacy Bomb.



LEO:  Can't wait.



STEVE:  Which Microsoft plans to drop into everyone's lap.



LEO:  Can't wait.  Hey, I just wanted to mention, because that FRITZ! thing got me really excited, and I went to the website, it's a German company.  As far as I could tell they do not sell in the United States.  Your correspondent was in Israel.  



STEVE:  Ah.



LEO:  If somebody who's listening tells me where I can get FRITZ! stuff in the U.S., and if it's U.S. compliant, I'd be very interested.  But right now it looks like, I mean, it's Germany.



STEVE:  I wonder why.



LEO:  Well, we have an FCC.  I mean, they'd have to get approval in the U.S.



STEVE:  But they're Germans.



LEO:  How hard can that be?



STEVE:  I mean, I'm driving - we're each driving one of their cars, Leo.



LEO:  I know.  I like...



STEVE:  They work just great.



LEO:  German engineering, okay.  But as far as I can tell I couldn't find anywhere I could buy it in the U.S. 



STEVE:  Wow.



LEO:  And I checked, and your correspondent was in Israel, which might explain how they could get it.



STEVE:  Yes, he was.



LEO:  Yeah.  Okay, 50 gigabyte pile of nonsense. 



STEVE:  Uh-huh.  So since we began the podcast with a general theme of how AI, which is not even close to being intelligent, is being misapplied during these early days, I feel as though a security and privacy-focused podcast like this one ought to take note of the new "Recall" feature that will be part of the next-generation ARM-based Windows 11, what's known as Copilot+ laptop PCs.



First of all, yes, it does appear that ARM processors have finally come far enough along to be able to carry the weight of Windows on their processors.  And while having Windows on ARM will certainly create a new array of challenges, like for example the lack of specific hardware drivers that only exist for Intel kernels, in the more self-contained applications, you know, where drivers are much less used, such as laptops, where power consumption and battery life trumps pretty much any other consideration, it's foreseeable that Windows may finally be able to find a home on ARM.  Today, laptop and tablet form-factor machines containing Qualcomm Snapdragon ARM processors, running Windows 11, have been announced and are in some cases available for pre-order from Acer, Asus, Dell, HP, Lenovo, Microsoft, and Samsung.



It's also worth noting that Intel PCs will also be getting Copilot+ at some time in the future.  But they will need to have a neural processing engine.  Answering the question "What makes Copilot+ PCs unique," Microsoft writes:  "Copilot+ PCs are a new class of Windows 11 PCs that are powered by a turbocharged neural processing unit (NPU), a specialized computer chip for AI-intensive processes like real-time translations and image generation, that can perform more than 40 trillion operations per second."  So we have TOPS, trillion operations per second.  So more than 40 TOPS.  And later, Microsoft writes:  "We are partnering with Intel and AMD to bring Copilot+ PC experiences to PCs with their processors in the future."  So potentially everybody's going to be able to get this.



Okay.  So what is Recall?  Microsoft explains.  They said:  "You can use Recall on Copilot+ PCs to find the content you have viewed on your device.  Recall is currently in preview status.  During this phase we will collect customer feedback, develop more controls for enterprise customers to manage and govern Recall data, and improve the overall experience for users.  On devices that are not powered by a Snapdragon X Series processor, installation of a Windows update will be required to run Recall.



"Recall is currently optimized for select languages, including English, simplified Chinese, French, German, Japanese, and Spanish.  This means Recall is able to retrieve snapshots from your PC's timeline based on more sophisticated searches in these languages.  During the preview phase, we will enhance optimization for additional languages.  Recall can also retrieve snapshots from your PC's timeline based on text-to-text searches in more than 160 languages."



Okay.  Fortunately, they then ask themselves "How does Recall work?"  To which they reply:  "Recall uses Copilot+ PC advanced processing capabilities to take images of your active screen every few seconds.  The snapshots are encrypted and saved on your PC's hard drive.  You can use Recall to locate the content you have viewed on your PC using search, or on a timeline bar that allows you to scroll through your snapshots.  Once you find the snapshot that you were looking for in Recall, it will be analyzed and offer you options to interact with the content.



"Recall will also enable you to open the snapshot in the original application in which it was created."  Whoa.  Really?  Okay.  "And, as Recall is refined over time, it will open the actual source document, website, or email in a screenshot."  Which, okay, is mindboggling.  But they said:  "This functionality will be improved during Recall's preview phase."  So before they let it loose.



They said:  "Copilot+ PC storage size determines the number of snapshots that Recall can take and store.  The minimum hard drive space needed to run Recall is 250GB, and 50GB of space must be available.  The default allocation for Recall on a device with 256GB will be 25GB, which can store approximately three months of snapshots.  You can increase the storage allocation for Recall in your PC Settings.  Old snapshots will be deleted once you use up your allocated storage, allowing new ones to be stored."  Okay, so it's sort of a rolling 90-day window, the most recent 90 days of screen snapshots taken every few seconds.



Okay.  Then they ask:  "What privacy controls does Recall offer?"  They respond:  "Recall is a key part of what makes Copilot+ PCs special, and Microsoft built privacy into Recall's design from the ground up."  Which of course we all recognize as standard boilerplate, which we all hope is true.  They said:  "On Copilot+ PCs powered by a Snapdragon X Series processor, you will see the Recall taskbar icon after you first activate your device.  You can use that icon to open Recall's settings and make choices about what snapshots Recall collects and stores on your device.  You can limit which snapshots Recall collects; for example, you can select specific apps or websites visited in a supported browser to filter out of your snapshots.  In addition, you can pause snapshots on demand from the Recall icon in the system tray, clear some or all snapshots that have been stored, or delete all the snapshots from your device."



LEO:  We call that "I'm going to watch porn button now."



STEVE:  Yeah.



LEO:  Press the porn button.



STEVE:  Yeah.



LEO:  Yeah.



STEVE:  And it occurs to me that I'd later talk about how snapshots of the Windows-based Signal app would be a problem.



LEO:  Oh.  Because that's in the clear; right?



STEVE:  Right, right.  I mean, it's what the user sees.  Maybe this allows you to say "Don't take snapshots of that window."  And we should also remember that what we see is a graphic user interface.  But Windows knows the text behind the actual controls that it's displaying.  So it doesn't actually have to be, I mean, I guess it's - who knows what it's doing in detail.  But my point is that, while we see graphics, there's actual text which is being mapped into bitmapped fonts which is then being displayed on the screen.  So behind the screens, so to speak, Microsoft actually has the raw text which was used to generate the screen.



LEO:  Yeah, yeah.  That makes sense.



STEVE:  Okay.  So they said:  "Recall also does not take snapshots of certain kinds of content, including InPrivate web browsing sessions in Microsoft Edge."  And by the way, they only said Edge, but I saw elsewhere that it's any of the browsers that have a well-defined private browsing mode.  You know, they do not record that.  And they said:  "It also treats material protected under digital rights management," you know, DRM stuff, "similarly.  Like other Windows apps such as the Snipping Tool, Recall will not store DRM content."



And they said:  "Note that Recall does not perform content moderation.  It will not hide information such as passwords or financial account numbers.  That data may be in snapshots that are stored on your device, especially when sites do not follow standard Internet protocols like cloaking password entry."



Okay.  So we're rolling toward an entirely new capability for Windows PCs, where we'll be able to store data which I presume is somehow indexed first, then encrypted for storage and later access.  And unless otherwise instructed and proscribed, this system is indiscriminately taking snapshots of our PC screen content every few seconds and is, by Microsoft's own admission, potentially capturing and saving, for later retrieval, financial account numbers, monetary balances, contract language, proprietary corporate memos and communications, and who knows what private things we'd really rather never have recorded, or whatever else the user might assume will never go any further.  This is where our much beloved and overworked phrase "What could possibly go wrong?" comes to mind.



Does anyone not imagine for an instant that having searchable access to the previous 90 or more days of a PC's screen might be hugely interesting to all manner of both legal and illegal investigators?  Corporate espionage is a very real thing.  China is moving their enterprises away from Windows as rapidly as they can.  But you have to know that cyberattackers, many of the most skillful and persistent who seem to be persistently based in China, must be beside themselves with delight over this new prospect that we decadent capitalists in the West are going to start having our PCs recording everything that's displayed on their screens.  What a great idea.  If history teaches us anything, it's that we still have not figured out how to keep a secret, and especially not Microsoft.  So what Microsoft is proposing to plant inside all next-generation PCs is tantamount to a 50 Gigabyte Privacy Bomb.  Maybe it will never go off, but it will certainly be sitting there trying to.



And just ask yourself whether law enforcement and intelligence agencies don't also think this sounds like a terrific idea?  Oh, you betcha.  With great power comes great responsibility.  And here, clearly, there's much to go wrong.  Microsoft understands this perception, and so they ask:  "How is your data protected when using Recall?"  They explain:  "Recall snapshots are kept on Copilot+ PCs themselves, on the local hard disk, and are protected using data encryption on your device and, if you have Windows 11 Pro or an enterprise Windows 11 SKU, BitLocker.  Recall screenshots are only linked to a specific user profile, and Recall does not share them with other users, make them available for Microsoft to view, or use them for targeting advertisements.



"Screenshots are only available to the person whose profile was used to sign into the device.  If two people share a device with different profiles, they'll not be able to access each other's screenshots.  If they use the same profile to sign into the device, then they will share a screenshot history."  And thus, you know, be able to scroll back to see what the other person has been doing.  "Otherwise, Recall screenshots are not available to other users or accessed by other applications or services."



Okay.  So all that really means there is they've done the obvious thing; right?  Is that they've, you know, they've divided the machine in the same way they do currently now, you know, with things like apps that you install for only one profile.  So, okay.  That's what Microsoft had to say.



The guys from Ars Technica watched Microsoft's presentation of this last Monday and gave their write up an impressively factual and neutral headline.  They said:  "New Windows AI feature records everything you've done on your PC."  And then they said:  "Recall uses AI features" - okay - "to 'take images of your active screen every few seconds.'"  So Ars wrote:  "At a Build conference event on Monday, Microsoft revealed a new AI-powered feature called Recall for Copilot+ PCs that will allow Windows 11 users to search and retrieve their past activities on their PC.  To make it work, Recall records everything users do on their PC, including activities in apps, communications in live meetings, and websites visited for research.  Despite encryption and local storage, the new feature raises privacy concerns for certain Windows users.



"Microsoft says on its website:  'Recall uses Copilot+ PC advanced processing capabilities to take images of your active screen every few seconds.  The snapshots are encrypted and saved on your PC's hard drive.  You can use Recall to locate the contents you have viewed on your PC using search or on a timeline bar that allows you to scroll through your snapshots,'" quotes Ars Technica.



Ars wrote:  "By performing a Recall action, users can access a snapshot from a specific time period, providing context for the event or moment they're searching for.  It also allows users to search through teleconference meetings they've participated in and videos watched using an AI-powered feature that transcribes and translates speech.  At first glance, the Recall feature seems like it may set the stage for potential gross violations of user privacy.  Despite reassurances from Microsoft, that impression persists for second and third glances, as well."  They said:  "For example, someone with access to your Windows account could potentially use Recall to see everything you've been doing recently on your PC, which might extend beyond the embarrassing implications of pornography viewing and actually threaten the lives of journalists or perceived enemies of the state."  And I'll interject to say, in other words, this puts examining someone's web browser history to shame.  How quaint that becomes.



Ars continues:  "Despite the privacy concerns, Microsoft says that the Recall index remains local and private on device, encrypted in a way that is linked to a particular user's account.  Microsoft says:  'Recall screenshots are only linked to specific user profile, and Recall does not share them with other users, make them available for Microsoft to view,'" anyway, blah blah blah, what I just wrote about that.



So they said:  "Users can pause, stop, or delete captured content, and can exclude specific apps or websites.  Recall won't take snapshots of InPrivate web browsing sessions in Microsoft Edge or DRM-protected content.  However, Recall won't actively hide sensitive information like passwords and financial account numbers that appear onscreen.  Microsoft previously explored a somewhat similar functionality with the Timeline feature in Windows 10, which the company discontinued in 2021, but it didn't take continuous snapshots.  Recall also shares some obvious similarities to Rewind, a third-party app for Mac we covered in 2022 that logs user activities for later playback."



They said:  "As you might imagine, all this snapshot recording comes at a hardware penalty.  To use Recall, users will need to purchase one of the new 'Copilot Plus PCs' powered by Qualcomm's Snapdragon X Elite chips, which include the necessary neural processing unit.  There are also minimum storage requirements for running Recall, with a minimum of 256GB of hard drive space and 50GB of available space.  The default allocation for Recall on a 256GB device is 25GB, which can store approximately three months of snapshots.  Users can adjust the allocation in their PC settings.



"As far as availability goes," they conclude, "Microsoft says that Recall is still undergoing testing.  Microsoft says on its website:  'Recall is currently in preview status.  During this phase we'll collect customer feedback, develop more controls for enterprise customers to manage and govern Recall data, and improve the overall experience for users.'"



Okay.  I just should note that the amount of storage Recall uses does scales upward with the size of the system's mass storage.  And presumably the duration of the scroll back increases similarly.  It'll take 25GB when 256 is available, 75GB on a 512GB drive, and 150GB from a system with a 1TB drive of primary mass storage.  So presumably, the more storage the system is able to commandeer, the further it's possible to scroll back through the system's display history.



Okay, now, while trying to be objective about this, the first question that leaps into the foreground for me is whether anyone actually needs or wants this?  Is this a big, previously unappreciated problem that everyone has?  Okay.  But trying to be objective.  First of all, compared to the static contents of a hard drive, Recall would be objectively a goldmine of additional new information about the past 90-plus days of someone's life, as viewed through their computer activities.  And more than ever before, people's entire lives, and their private lives, are reflected in what's shown on the screens of their computers.  Maybe that makes scrolling back through their recorded lives compelling.  I don't know.



But we know from Microsoft that it will be snapping video conference content on the fly.  And as I mentioned, the Windows Signal app that goes to extremes to protect the contents of its chats would presumably be captured, unless you're able, as I mentioned before, and they sort of suggest, you can tell Recall don't record specific applications.  So you probably want to turn that off, or maybe you trust Microsoft, and it'll be part of your scroll back.



But, you know, email screens and nearly everything that happens on a PC would be captured.  And of course that's the point; right?  But the vast majority of that content will not have been stored on the machine's hard drive ever, until now.  So objectively, the presence of Recall clearly introduces a new, never before existing liability.  And that's what everyone who talks about this sees as a potential for creating havoc where none existed before.



So the question, it seems to me, is whether the new value that's created and is returned by Recall's scrolling usage history justifies whatever new risk might also be created by its retention of that data.  How useful will having all that actually be?  I've tried to imagine an instance where I wish I could look back in time at my computer screen.  I suppose I don't feel the need since I've never had the option.



So if I knew I could scroll my computer's screens back in time, I suppose it might be an interesting curiosity.  But it really doesn't feel like a feature I've been needing and missing until now.  I suppose an analogy would be that the world had no idea what it was missing before the creation of social media.  And hasn't that been a big boon to mankind?  Now, unfortunately, we seem unable to live without it.  Perhaps this will be the same.



The bottom line is this I think we're just going to need to live with this thing for a while.  We're going to need to see whether this is a capability desperately searching for a need; or whether, once people get used to having this new thing, they start thinking, how did I ever live without this?  However, one thing that is also absolutely objectively true is that everyone will be carrying around a 50GB privacy bomb that they never had before.  Maybe it'll be worth the risk.  Only time will tell.



Oh, and Simon Zerafa posted a tweet from someone who has been poking into Recall's storage.  He's detective@mastodon.social, who wrote:  "Can confirm that Recall data is indeed stored in a SQLite3 database.  The folder it's in is fully accessible only by system and the administrators group.  Attempting to access it as a normal user yields the usual 'You don't currently have permission' error."  And he said:  "Here's how the database is laid out for those curious."  And he said:  "Figured you might appreciate a few screenshots."



So I've put one in the show notes.  And sure enough, it's got a DB browser for SQLite and shows the layout of the table with all of the various components, you know, window capture text index content, window capture text index data, window capture text doc size and relations and all kinds of stuff.  So anyway, I guess what this means is that, if nothing else, if that data should ever escape from anyone's PC, it will not be difficult for anybody who gets it to open it up and browse around in it because it's just a SQLite3 database.



And Leo, you know, I guess, you know, if search really worked, and you were able to search on something that you remember, but you didn't write down or didn't record, didn't save, but it was just like right there at your fingertips, and bang, it popped up and showed it to you, I guess I could see that that could be compelling.



LEO:  Yeah.  I mean, I want to have - the late Gordon Bell passed away last week.  He used to wear around a camera.  His wife Gwen, I knew them both, wonderful people, had severe Alzheimer's, so he became very aware of the idea of remembering things.  And I can't remember what he called it, the "mem-it," the "meme-it" or something.  But this was '96.  This was way before there really was the technology to do this.  But it would take a picture every 20 seconds.  And his theory was I would like to have, I mean, it's not just recall of your Windows desktop, but of everything, that you could then search and query.



And now there are, you know, I just bought something called the Limitless Pin that should come in August, it's a little lapel pin that records all your audio and then feeds it to an AI.  So you can query it of things like that.  You know, Steve and I were talking, and he mentioned a router from a German company, I can't remember the name of it.  What was the name of that?  "The name was Fritz."  You could see that that kind of might be useful.  There are absolutely privacy issues with this.  In fact, that Limitless Pin won't record somebody's voice unless you get explicit spoken approval to do so, which is very interesting.  It uses voice printing.



STEVE:  So it's doing voice recognition.



LEO:  Yeah, yeah.



STEVE:  Instead of just being a generic audio recorder.



LEO:  Right.  But then, once they say yes, then it will say, "Steve said," "Leo said," that kind of thing.  I don't - you know what?  This is very early days.  But you nailed it.  There is potentially some use for this, but there's also a downside, many downsides.



STEVE:  Yeah.  And so I think it's a tradeoff, like anything else.  Is this so useful that it's worth carrying around the last 90 days plus of everything that your computer screen showed?  And that's the other thing, Leo.  You're not going to want to not record, like, chunks of your screen, like you would probably not want to not record - I'm sorry.  You would want to record Signal because you'd want to be able to...



LEO:  You want to be able to query it, yeah.



STEVE:  Exactly.  Exactly.  So the tendency will be to, you know, record everything and trust the Force.  Unfortunately, that's Microsoft.



LEO:  It's a challenge, I mean, this is really a challenge.  I would not turn on Recall, partly because of the burden, the strain it puts on the system seems like a bad idea.



STEVE:  Well, yeah.  I have a feeling that our audience will not be among the first adopters.



LEO:  Right.



STEVE:  I mean, some will.  I'm curious.  And as I was thinking about this, I thought, I will be interested in hearing.  Like, you know, to hear Paul - lord knows he's not a pushover.  So if Paul Thurrott says, hey, this is the greatest thing since, you know, bananas, sliced bread, you know, whatever, then wow.



LEO:  Yeah, we'll watch what Paul - exactly.  I mean, you've searched through your - I have - searched through your browser history; right?  I can't remember, what was that site?  And you go through your browser history.  That's what browser history does, except it's just recording websites you visit, just the URLs.  Might be more useful if it recorded the content.  And then maybe if all your apps did the same, and you can see how you can slide into this.



STEVE:  Yeah.  Content, there was an app back in the DOS days, and I've tried to remember what it was.  It would take little notes.  And so you could easily create a little text window and type some text in, and it just went into a big pile.



LEO:  Was it Sidekick?



STEVE:  Well, as you typed, I think it was a TSR, and you would bring it up full screen, and it would be this blizzard of little overlapping, like, Post-its.



LEO:  Yeah, that sounds familiar, yeah.



STEVE:  But then, as you type a few characters, all the ones that did not contain that substring would - they disappeared.  And it was compelling to be able, like anything you thought you remembered, you could just type a few characters, and it would, like, whittle it right down. 



LEO:  There was "Ex."  Do you remember "Ex"?  That was the idea of "Ex" was a superfast - but that wasn't like what you just described.  But the idea was it indexes your - just like Windows does, but it faster and better indexes everything on your drive.  And then you type one letter, it finds everything that matches that, two letters, three letters, it's that progressive search.  So you could very quickly - "Ex" was very, very fast.  It was very cool.



STEVE:  What happened to it?



LEO:  It's still around.  I think they went - they became an enterprise tool.  



STEVE:  Okay.  Well, and it's built into Windows now.  So, you know...



LEO:  Yeah.  Yeah, but Windows doesn't do it as well as "Ex" did.  It was [crosstalk] even when "Ex" did it.  It was a smart search tool.  What was the name of that DOS program?  I know exactly what you're talking about.



STEVE:  Yeah.  It was, and I've tried to remember it, and like Instant Recall or something like that.



LEO:  Oh, that sounds familiar, yeah.  Oh, this is "Ex."  That's the wrong "Ex."  This is the problem with the word "Ex."  It's  not a good search term.  It's not easy. 



STEVE:  No, it was a dumb thing for Twitter to get renamed to.



LEO:  Not the only dumb thing Elon's ever done.  That's interesting.  I want to know this Instant Recall thing.



STEVE:  I want to think - I think it was written by Phil Katz.



LEO:  Oh, well, there you go.  Yeah, he was a genius.



STEVE:  The PKZIP guy.



LEO:  The PKZIP guy.



STEVE:  Yeah.



LEO:  Yeah, Recall 11.  It was called Recall.



STEVE:  Just Recall.



LEO:  I think so.



STEVE:  Wow.



LEO:  I think so.  Memory resident, no, it's command line editor and history utility.  That might be it.  It's TSR.



STEVE:  No.  This thing was definitely - it was little notes.  It was not a command line history.



LEO:  Okay.



STEVE:  That does sound like a DOS command line history.  



LEO:  Yeah.  Huh.  Somebody will remember, and they'll message you on your new email platform.



STEVE:  Yay.



LEO:  How do they do that again?  They go to GRC.com.



STEVE:  /mail.



LEO:  /mail.



STEVE:  Or just GRC.com, and there's a little white envelope up at the top of the screen.



LEO:  They have to get whitelisted, though.



STEVE:  Yes.  Exactly.  So GRC.com/mail.  Okay.  And there it is.



LEO:  I put in my email.



STEVE:  Yup.  And I just sent you a link.



LEO:  And then you're going to send me an email.  Okay.  And then I confirm that that's my email, which no spammer would ever do.



STEVE:  Correct.



LEO:  So right there you've eliminated spammers.



STEVE:  Correct.  And so you click on - the email that comes is very attractive.  And you click on the little button that you get, and it takes you to your subscriptions page.  And you can put your name in if you want so that I address email to you by name.



LEO:  Well, there it is. 



STEVE:  There it is.



LEO:  Very attractive.  Nice choice of colors, Steve.  Confirm.



STEVE:  Well, I need to think about that, though, the black on white.  It looks much better on a white background.



LEO:  Ah, yes.  I'm dark mode, man.  Okay.  So now I'm - you know better than clicking a button in email.  So what I'm going to do is the smart thing, which is copy it.



STEVE:  That's right, I gave you a link.



LEO:  And paste it in.



STEVE:  I gave you a link because I know who our...



LEO:  Yeah, GRC.com/manage, yeah, that looks good, okay.  And now I can choose, if I wish, to subscribe, but I don't need to.  Oh, look, you can subscribe to Security Now!.  Oh, I definitely want product news.  Oh, and GRC news.  Okay.  And my name, Leo.



STEVE:  Is Leo.



LEO:  Yes.  And I'm going to update my subscriptions.  So now I can email you from this address.



STEVE:  Correct.  And now, if you get the mail again, you'll see the confirmation that was sent, which is also very pretty.



LEO:  Okay.  Look at that.  Also, you know, let me get out of dark mode so I can enjoy the fresh...



STEVE:  Yeah, it's very pretty.



LEO:  ...beauty of what you're doing here.  Of course I've been in dark mode so long I have no idea how to get out of it.



STEVE:  How to turn it off.



LEO:  How do you turn this off here?  I don't know.  I think I have to go to system settings.  And I go, let's be light.  Light mail.  Oh, look how pretty that is, Steve.



STEVE:  Yup.  So I show - I address it by name, show you your email address, and which list that you're subscribed to.



LEO:  Nice.  Very nice.



STEVE:  Yup, that's all there is to it.  And so now you are whitelisted, and you can send email to securitynow@grc.com.



LEO:  And that's where you should do your feedback.  Go through those steps, and you can have a nice conversation, a nice chat with Steve.



STEVE:  Yeah, and I don't know what's going to happen with Twitter.  As I said, it's easy for me to post the weekly notes there. 



LEO:  Oh, you should keep doing that.



STEVE:  I'm beginning to get a lot of spam.  So I guess I don't see any reason for it any longer.



LEO:  I've been gone more than a year, and I don't miss it.  Good.  Good.  Very nice.  Now, while you're at GRC.com, don't stop there.  Do that, GRC.com/mail.  But you can also find Security Now! there.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#977

DATE:		June 4, 2024

TITLE:		A Large Language Model in Every Pot

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-977.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  When is a simpler application better than something complex?  How did the first week of GRC's new email system go?  Have you been pwned?  And if so, how worried should you be?  What's the latest new supply-chain attack vector?  What certificate authority just lost all their TLS server business?  And remember that early messaging service ICQ?  Whatever became of it?  Finally, after I share a tip about a perfect science fiction movie, two pieces of listener feedback and one user's happiness over SpinRite, we're going to look at what a prominent security researcher learned after using Microsoft's Recall for 10 days, and why I think Microsoft is willing to bet the farm and risk the dire warnings of the entire security community over this unasked-for capability.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Have you ever been pwned?  Well, here's a way to know and whether you should worry about it.  What certificate authority just lost their TLS server business?  We'll talk about that, the end of ICQ, and Microsoft's new Recall feature that's coming to all Copilot+ PCs.  Steve explains why it is not as secure as Microsoft has said, why it's in fact a real danger.  He also has a theory, and I agree with it 100%, of why Microsoft is doing this.  So a very interesting play for your information, next on Security Now!. 



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 977, recorded Tuesday, June 4th, 2024:  A Large Language Model in Every Pot.



It's time for Security Now!.  Yes, adjust your spectacles and put your beanie on straight because this guy, Steve Gibson, is going to challenge you, he's going to excite you, he's going to thrill you, he's going to make you a geek just by proximity.  Hello, Steve Gibson.



STEVE GIBSON:  I think if you've survived more than a couple of these podcasts...	



LEO:  You qualify.



STEVE:  ...your geek status has been already established.



LEO:  You more than qualify.



STEVE:  If you haven't gone running for the hills.  It's like, agh.  I got a piece of mail from one listener who said, okay, so I think I understand about 5% of what you're talking about.



LEO:  That's pretty good.  You're doing well.



STEVE:  But I do come away with something useful every week, so I keep coming back for more abuse.  No, for more edification.



LEO:  Well, and it's like lifting a heifer.  Like when a cow, a baby cow is first born, you can lift it.  If you lift it every day, you'll be able to lift a full-grown cow. 



STEVE:  This is the analogy you've come up?  We're lifting - lifting cows.



LEO:  Keep listening every week, and in a year or two you'll be able to lift a cow.  How about that?



STEVE:  And maybe you'll be able to throw a honeypot.



LEO:  There you go.



STEVE:  Oh, actually there's a really interesting piece that Microsoft just revealed, the details of a honeypot they had been running for a long time.  Anyway, I may be talking about that next week, if nothing more interesting comes along.  But speaking of pots, today's title is "A Large Language Model in Every Pot."  And we're going to go back and talk about Recall again.  Well, okay.  I'm stepping on my own sequence here.  So we've got a lot of things to talk about.



When is a simpler application better than something complex?  How did the first week of GRC's new email system turn out?  Have you been pwned?  And, if so, how worried should you be?  What's the latest new supply-chain attack vector?  What certificate authority just lost all their TLS server business?  Whoops.  And remember that early messaging service ICQ?  Whatever became of it?



Finally, after I share a tip about what I consider to be a perfect science fiction movie, two pieces of listener feedback, and one user's happiness over SpinRite, we're going to look at what a prominent security researcher learned after using Microsoft's Recall for 10 days, and why I think Microsoft is willing to bet the farm and risk the dire warnings of the entire security community over this unasked-for capability.  I think I know where they're headed.  And it's very exciting, if I'm right.  And it's also very troubling.  And it's really a shame that they've been screwing around with Windows, adding features nobody wanted instead of making it more secure because they really can't do what they want to do.  So we're going to have fun today.



LEO:  Very interesting.



STEVE:  Unlike all of the other 976 podcasts that came before.



LEO:  So boring.  So boring.



STEVE:  Yeah.



LEO:  No.  We're going to have fun today, I promise you.



STEVE:  And we do have a great Picture of the Week.



LEO:  Oh, haven't read it, I just know the caption.  All right.  All right.  I am ready for the Picture of the Week, Mr. Gibson.



STEVE:  So I gave this picture the title, "But Officer..."



LEO:  Okay.  Does it need no explanation?



STEVE:  It really doesn't, once you see the picture.



LEO:  All right.  It's going to take me a minute to get it up on this computer.  Here it comes.  All right.  I'm prepared.  Are you ready?  I'm going to scroll up.  We shall enjoy it together.  But Officer...  There's a one-way street sign, a stop sign, and a no right-turn sign.  What the - what am I supposed to do?



STEVE:  You know, Leo, you just have to wonder, like...



LEO:  What the heck?



STEVE:  I know.  Okay.  So for people who aren't seeing this, we have a picture where a public street has come up to a T intersection.  So you have to turn left or right.  Well, there's a stop sign, so you certainly need to consider your options, thus stopping.  The problem is that the street that you are intersecting with has been labeled as one-way, where all the traffic is moving from left to right.  But below the stop sign, it's also very clearly marked that you must not turn right.  There's the right-turn arrow with a big red slash through it.  So I don't know.  Do you back up?  You know, like backing up would be the only thing you can do.



LEO:  I think it's all you can do.  But notice there's no outlet.  You're in a cul-de-sac.  So you're really dead in the water.



STEVE:  So you're right.  Is that what the yellow sign says?  



LEO:  Yeah, it says "No Outlet."



STEVE:  It says "No Outlet"?  I thought, yeah, I thought so.  So now that...



LEO:  So this is the worst.



STEVE:  That's something that would be seen by people going down the street waving at you because you're stuck, and you can't go anywhere.



LEO:  I think this is a prank being played on self-driving cars.  Whoever lives on this street added that sign knowing that a self-driving vehicle would then be complete stuck.



STEVE:  It would just explode, Leo.



LEO:  It can't do anything.



STEVE:  It would just say, okay, I quit.



LEO:  I can't do anything.  I'm stuck.  Oh, my god, that's hysterical.



STEVE:  Welcome to America.  Okay.  So I wanted to thank all of our listeners who correctly recalled that the random notes DOS app we were trying to remember last week was "Tornado Notes."



LEO:  And I don't even remember that one, so I wouldn't have gotten it.



STEVE:  Yeah, it was not well - Leo, it was DOS.  But you used DOS back in the day.



LEO:  Oh, I used Sidekick.  I used a lot of [crosstalk] DOS, yeah.



STEVE:  Yeah, yeah, yeah.  So it was not Phil Katz of PKZIP fame.  It was a guy named Jim Lewis of Micro Logic Corporation. And when I first encountered Tornado Notes from a company named Micro Logic Corporation of Hackensack, New Jersey, I wondered, why is that name so familiar?  And it turned out it was because the same guy had created one of the most useful sets of 8.5 x 11" double-sided plastic sheet processor instruction reference cards the world had ever encountered.  I have a picture of them in the show notes.



Now, upon the event of my death, my plan is for cremation, after first having whatever organs may still be functioning and useful to anyone removed. But if my plan were burial, I would want these processor instruction reference cards...



LEO:  Bury them with you.



STEVE:  ...buried alongside me.



LEO:  This is a 6502, a Z80, and an 8086.  It's all in there.



STEVE:  And there is a 68000, as well.  I cannot begin to express how important they were back when I was writing assembly code...



LEO:  Wow, look at this.



STEVE:  ...first for Apple's and later Atari's 6502-based machines.



LEO:  This is so cool.



STEVE:  And Leo, I've got links on the next page to the PDFs of them.  I mean, these things were significant to so many people.



LEO:  Yeah.



STEVE:  I ran across someone over on Reddit who commented that it was a good thing these were 100% plastic or he would have worn his out.  You know, they were indispensable.  And I don't know where mine are.  I'm sure they're here somewhere because I would have never thrown them out.  They were just perfect.  Now, you have on the screen now the 6502 card.  And notice all the blank boxes.  Those are missing opcodes.  So that was important.  You had to know, you know, what was available and what wasn't.  And one of the reasons the 6502 microprocessor was so well used - Apple chose it, Atari chose it, Commodore chose it - was because it was so inexpensive.  And the reason it was inexpensive...



LEO:  It didn't do diddly.



STEVE:  It didn't do much, exactly.  It, you know, it transferred all the burden to the programmer, and like most of those opcodes are empty in there.



LEO:  Wow.



STEVE:  But it did just enough in order to get the job done.  But this was just - it was so - so this guy named Jim Lewis, who later gave us Tornado Notes for DOS, a TSR, you know, the reason I knew his name when Tornado Notes came along is like, wait a minute, I've got these Instruction Reference Cards that I've been using forever.  But anyway.  Tornado Notes for DOS was utterly unique.  When Windows happened, Jim tried to recreate the success of Tornado Notes with a product he named Info Select.  But Info Select was the victim of its own featuritis.



The sublime beauty of Tornado Notes was that it was so simple. It did exactly and only one thing perfectly and - and this was the other thing - instantaneously.  It began as a massively overwhelming disorganized pile of rectangular notes.  Didn't matter, you could just put anything, just random text in, didn't matter what shape or size they were.  But then, as you typed successive characters of a string, all those notes that did not contain the substring that had been entered thus far would instantly disappear.  So you got this very satisfying, almost animated, real-time winnowing of your entire pile until you could see the note you knew was there somewhere.  And notice that you also saw all the notes that contained that same substring, which was often surprisingly useful at times.



Unfortunately, Jim, for all his brilliance, did not understand that Tornado Notes succeeded due to the constraints imposed upon it by its DOS environment.  So when he created its successor, which was Info Select for Windows, he gave it hierarchies and categories and menus and formatted printing and everything else you can imagine that Windows made possible.  I think there was even a kitchen sink tucked in there somewhere.  And, you know, we wanted the same thing for Windows that we had for DOS.  But what we got was a monstrosity that required all manner of configuration and thought.  Yes, it could do so much more than Tornado Notes could.  But the very thing that was so beautiful about Tornado Notes was everything it did not do.  So as it turned out in retrospect, you know, the thing, I mean, it being so minimal was what made it so compelling and useful.



And I'm mentioning this because there's a larger lesson here.  One of the things the original designers of Unix also got exactly right was the idea of creating many simple commands that took some input, did something to it, and then produced some output.  And then to that you add the simple ability to interconnect these individual small building blocks into a chain by piping the output of one into the input of another, and you're able to interactively create and assemble a much more complex ad hoc function.



And, Leo, while I'm not a LISP programmer, I have the sense that the same sort of approach can be used there, where you kind of incrementally build up...



LEO:  Exactly, yes.



STEVE:  ...a much more complex solution that's assembled from many smaller pieces interacting.



LEO:  They call it "composable" because you compose a larger program out of pieces of smaller programs.  And to my mind it makes it so much easier because you can bite off a little bite, figure out how it works, and because it's basically functional, you know, it's always going to give you the same result with the same input.  You could slow put those together and build something out of it.



STEVE:  Yeah.



LEO:  It feels to me like woodworking almost, like assembling a machine.  It's great.



STEVE:  Like crafting a solution.



LEO:  Crafting, exactly, yes.



STEVE:  So anyway, the point I hope to make here is that more is not always better.  And, you know, for example, this is a lesson that the people who design the remote controls for A/V equipment appear to have never learned.  Oh, my goodness, it's a joke that those things are so crazy.  And I did notice that, you know, when I was thinking about this, that my freeware all just does one thing.  You know?  I create a little program.  It just does one thing.  If you want that one thing, that's the program you use.  It's, you know, 23K.  It does its job.  And then you're done.  And actually through the years people have been asking for many, many more features from SpinRite, and I've just said no, you know, SpinRite does what it's supposed to do.  And that's what it's for.



So anyway, I just - I want to thank all of our listeners who said "I think you guys were thinking about Tornado Notes."  And sure enough.  And I wouldn't be surprised, I mean, there are DOS boxes around that could run Tornado Notes.  I haven't run across a copy of it, but I probably have one on a hard disk around here somewhere.  Anyway, I also wanted to follow up on last week's announcement of GRC's new email system, which has been a resounding success.  If you missed last week's episode, that is, if you don't listen to them all and don't know about it yet, you could go to our old GRC.com/feedback page which we've been talking about for 20 years which explains a bit about the nature of web form spam, which unfortunately is a thing, and it contains a pointer over to our new page, GRC.com/mail.



Anyway, the only post-announcement glitch we encountered was from users mostly using Gmail, but also a few other ISPs, I think Virgin Media was one, that use their own domains backed by those services, like Gmail.  But since the email they send comes from that underlying service, like Gmail, rather than from their domain alias, and since the incoming filter that's in front of the securitynow@grc.com mailbox looks to see whether the sender is known to us, listeners need to register their underlying Gmail account at GRC, not their aliased account, which is the one that's, you know, shown in the email From: header of their email.



So some people were going over to the GRC.com/mail page and putting in their account name and their own domain, even though it's a front for Gmail.  It turns out that the mail that they send actually comes from Gmail, so that was not an account that we'd ever seen before and so their mail was bouncing.  As soon as I understood what was going on, I added a little comment on the form just to say, you know, for Gmail people, that was like by far the majority of users who were having a bounce problem, that that was what they had to do, and that problem went away.  So people are paying attention to that.



Oh, also, anyone using an anonymizing email service will have a problem.  I received an email from a listener who was using the SimpleLogin email anonymizing service by Proton, which by the way appears to be a very nice service.  When that listener sent email to GRC, the sender's email was this bizarre long one-time 54-character random account name in front of the @simplelogin.com domain name.  So again, GRC's filter had never seen that before, probably will never see it again.  And it bounced that mail back.  So we're not compatible, our approach is not compatible with email anonymizing services.



And I didn't mention it last week, but I actually have at the GRC.com/mail page what I call "The Prime Directive," which is nobody will ever get mail from us that they don't want.  I mean, and I'm serious about that.  We will also never divulge anyone's email address.  Since sending email is a pain, you know, please unsubscribe if you're ever not happy and so forth.  Anyway, to make a long story short, our listeners love the simple solution.  You just register one time.  You optionally subscribe to whatever announcement lists, if any, you may wish.  And then from then on you can simply send email to securitynow@grc.com.  I have been overwhelmed with notes of thanks and congratulations from listeners, and people I've never heard from before who were never going to sign up to Twitter just to maybe send me a note.



You know, and in fairness, Twitter is about so much more than that.  You know, it's about building a community and a following, and following people, and networking.  I had been just using it as a point-to-point instant messaging service, which after all is exactly what email is.  So anyway, needless to say, as I said, I will never share anyone's email address.  Oh, and I did want to say, if somebody writes to me, I will never share your email address when I share your feedback.  And anyone requesting anonymity for their name, of course I will honor that.



Now, I should mention, and Leo I remember you mentioning this, too, when we first talked about it, one of the nice things about GRC's now-retired web form was that it solicited our listeners' location.  And it was nice being able to include that when sharing feedback, you know, since it made the email feel a little bit more personal.  So if you happen to think of it, let me know where you're writing from when you send me a note, and I'll just sort of toss that in when I share your feedback.



LEO:  I'm wondering, you said it has to have the same domain as the server.  So most email clients will let you choose a personality that says - so, for instance, I might be running on Gmail, but my email, I would like it to be leo@leoville.com.  I can choose leo@leoville.com as my personality in Gmail.  And even though it's originating from the Gmail server, it should look to you, to your server, like leo@leoville.  You don't look at the underlying outbound server; do you?  Maybe you do.



STEVE:  Yes.  I actually do.



LEO:  You do.



STEVE:  Yes, because...



LEO:  Instead of just the email address, the reply-to address, in other words.



STEVE:  Yeah.  The problem is the reply-to address is trivially spoofable.



LEO:  Of course.



STEVE:  And so I wanted something that is a little less spoofable.



LEO:  Okay.



STEVE:  I have a thread that I've not yet caught up in over in the newsgroups to do some brainstorming about whether I ought to change that because it would be easier if I just use the From: address.  And I'm not sure that it really matters because any spammer could certainly be spoofing the Receipt-To address, as well.



LEO:  Right, right.



STEVE:  So I may rethink that and change that, just to make it a little bit easier for...



LEO:  That's a good advisory.  You have to use the email address that your service provides.



STEVE:  Correct.



LEO:  As opposed to any personality, any identity that you use.



STEVE:  Correct.  And we ran across that with Gmail people and also, as I mentioned, SimpleLogin people.  It's an anonymizing service from Proton.  They also had to do that.  But really, after I explained it, we stopped having anymore problems with signup.  So my current work, this moment, you know, this evening, is to finish up automating and catching real-time email bounces.  So I could immediately inform someone when GRC is able to detect that it was unable to successfully deliver their authentication loop email.  Once that's in place, I'll stick my toe in the water to begin actually sending email in today's spam-conscious climate.  You've got to be careful.  And so we'll ramp up from there.



So anyway, I wanted to thank everybody for their support.  You know, everyone's interest is the reason I became convinced that we need to keep this going past 999.  And, you know, here we are, already at 977, with our 20th birthday coming up in August.



LEO:  Yeah, see?  Yeah.



STEVE:  Yeah.



LEO:  Old doesn't mean in the way.



STEVE:  While I was writing the note above yesterday, I received an email alert from Troy Hunt's "Have I Been Pwned?" email breach monitoring service.  The email's Subject was:  "16 emails on GRC.com have been pwned in the Telegram Combolists data breach."  Okay.  The breach occurred one week ago on May 28th.  In the breached data - get this, Leo - 361,468,099 email accounts were found.  And HIBP (Have I Been Pwned) sent this email because 16 of those 361 plus million belonged to GRC.com.



The description of the breach that Troy included said:  "In May 2024, two billion rows of data with 361 million unique email addresses were collated from malicious Telegram channels.  The data contained 122GB across 1,700 files with email addresses, usernames, passwords and in many cases, the website they were entered into."



LEO:  Does Troy email every one of those addresses?  How did you get that?  He must; right?  Or do you sign up for some?



STEVE:  No, no.  Yes.  So I subscribed to a domain-wide free - it doesn't cost anybody, so I would recommend this.  It's domain-wide.  So you would, you know, do Leoville.com and TWiT.tv.  And then you have to prove ownership of the domain.  And once you do, anytime Troy gets a hold of any new breach data, he'll scan the email addresses in the breach content and then notify you of any hits which may be one of your active email addresses having just been disclosed.



Okay.  So he said:  "In this case the data contained from this Telegram Combolists data breach, 122GB across 17,000 files, with email addresses, usernames, passwords, and in many cases the website they were entered into."  He said:  "The data appears to have been sourced from a combination of existing combolists and info-stealer malware."  And we'll be hearing a little bit more about info-stealer malware because that comes up when we're talking about Recall again. 



Okay.  So naturally I went over, after I received this email from him, to see whether any of those 16 addresses which HIBP reported were of concern.  Okay.  The short version is none were.  The longer version is the only two that were ever valid were greg@grc.com and offices@grc.com, neither of which we have used for decades.  I once watched a spammer's server connect to GRC's email server and just run down a list of first names, just, you know, abigail@grc.com, amantha@grc.com, and so forth, A through Z, hoping to get lucky.  Immediately after that we retired our original and, you know, oh-so-very-innocent use of our first names for email.  That just became impractical.



The wonderful open source email server I've been using for years is known as hMailServer.  Anyone looking for an utterly solid, feature-packed, no nonsense, free, Windows-hosted email server should look no further.  There really is nothing comparable.  I know lots of people run, you know, Sendmail and Postfix and so forth over on Linux.  And I get that.  Those are certainly mature platforms, too.  But Windows hMailServer.  It's another of those rare software creations that has no bugs.  Just like John Dvorak gets no spam, this thing has no bugs.



The only time it's been updated for years is to keep up with improvements in the OpenSSL library which it uses to make its TLS client and server connections.  And in fact I updated it just last week after many years of trouble-free service only to obtain support for TLS 1.3, which I did not have in my previous instance.  And remember, 1.2 appears to be fine.  You know, 1.3 exists.  It's real.  People should support it.  But 1.2 ain't going away anytime soon because it's still, what is it, 86% of connections or something like that.



Anyway, hMailServer has a dynamic blocklist feature that will block for a configurable period of time any remote server by IP address that attempts to deliver email to any nonexistent address, in my case at GRC.  I just checked the server when I was writing this yesterday.  I currently have the blocklist expiration set for two hours.  And at the moment I checked, 473 individual IP addresses were currently being blocked.  So within the previous two hours, 473 different spamming SMTP servers had connected to GRC and attempted to send spam.  Not to actually, you know, not even to any valid email address, but just to throw crap at the wall, hoping to get lucky.



Now, GRC's been around a long time.  The domain is well-known.  But we're certainly not particularly high-profile.  And it so saddens me, Leo, to see, sadly, I mean, really, what a sewer our beloved Internet has become.  I'm unsure what it teaches us about humanity, but I'm pretty sure I don't want to know.



LEO:  Yeah.  I think it just reflects humanity.  That's the problem.



STEVE:  Yeah.



LEO:  As we go along, it's more and more like the people who make it.



STEVE:  Yes, yes.  The trifecta of the Internet being anonymous, global, and free, those three things, enables every last miscreant on Earth to attempt to have their way with everyone else.  Fortunately, the rest of us are far from powerless, and we have this podcast to help us stay ahead of the tidal wave of incoming crap that's out there pounding on the door, trying to get in.  You know, we're not going to let any of that in.



LEO:  No.



STEVE:  Okay.  Okay.  So I want to talk about a new supply chain attack vector.  But let's take a break first, and then we will get into some security news of the week.



LEO:  All right.  I think you should write a manifesto, Steve.  We're mad as hell, and we're not going to take it anymore.



STEVE:  Well, we're going to stamp on it.  We're going to hide behind our NAT routers and hope that all that junk out there - I mean, come on, 473 servers just hooking up to GRC in the course of two hours, spewing junk at it.



LEO:  It's mindboggling; isn't it.



STEVE:  It's just...



LEO:  It's just amazing, yeah.



STEVE:  It's really sad.



LEO:  Yeah.  It's the world we live in, I'm sorry to say.  Well, you know, one good thing about doing this show is because you focus on all this stuff, we have the best sponsors when it comes to security; right?  They flock to us.  In fact, we talk to people all the time that say, hey, can I be on Security Now!?  And most of the time I'm happy to say we have to say, no, it's sold out for the next quarter.  All right, Mr. G.  On we go with the show.



STEVE:  And speaking of what a sad mess the greater Internet has become...



LEO:  Yes.



STEVE:  ...and of not letting any of that mess into our lives, one of our listeners, Terence Kam, pointed me to a recent piece in BleepingComputer titled "Cybercriminals pose as 'helpful' [in air quotes] Stack Overflow users to push malware."  Okay, now, for those who have never encountered it, Stack Overflow is a forum community of developers of widely ranging skill.  It's essentially a place where coders can help one another.  When I've been struggling with a programming problem, such as when I was working to get server-side on-the-fly code signing to work remotely with a certificate stored in an HSM, which as far as I know no one has ever done before, the Stack Overflow site would often be listed among Google's search results.  And I'm a member there, since I've enjoyed answering questions and giving back when I can.



So BleepingComputer writes:  "Cybercriminals are abusing Stack Overflow in an interesting approach to spreading malware - answering users' questions by promoting a malicious PyPI package that installs Windows information-stealing malware.  Sonatype researcher Ax Sharma (who's also a writer at BleepingComputer) discovered this new PyPI package is part of a previously known 'Cool package' campaign, named after a string in the package's metadata, that targeted Windows users last year.



"This PyPI package is named 'pytoileur' and was uploaded by threat actors to the PyPI repository over the weekend, claiming to be an API management tool.  Malicious packages like this," they write, "are usually promoted using names similar to other popular packages," you know, a process we've talked about before known as typo-squatting.  "However, with this package, the threat actors took a more novel approach by answering questions on Stack Overflow and promoting the package as a solution.  As Stack Overflow," Bleeping Computer writes, "is a widely used platform for developers of all skill sets to ask and answer questions, it provides a perfect environment to spread malware disguised as programming interfaces and libraries.



"Sonatype's Ax Sharma said in their report:  'We further noticed that a Stack Overflow account' - it had a nonsense name of  EstAYA G - 'created roughly two days ago is now exploiting the platform's community members who are seeking debugging.  It's directing them to install this malicious package as a "solution" - again in air quotes - 'to their issue, even though the "solution" is unrelated to the questions being posed by developers.'



"In this case, the pytoileur package contains a setup.py" - you know, Python - "file that pads a Base64 encoded command which executes with spaces, so that unless you enable word wrapping  in your IDE, you know, your Integrated Development Environment, or text file editor, this Base64 blob will be pushed all the way out past the right margin and offscreen so you'll never see it.  When that blob of Base64 is deobfuscated, the command will download an executable named 'runtime.exe' from a remote site and run it."



They write:  "This executable is a Python program converted into an .exe that acts as an information-stealing malware to harvest cookies, passwords, browser history, credit cards, and other data from the users' web browsers.  It also appears to search through documents for specific phrases and, if found, steals the data in them, as well.  All of this information is then sent back to the attacker, who can sell it on the dark web markets or use it to breach further accounts that are owned by the victim."



They said:  "While malicious PyPI packages and information-stealers are nothing new, the cybercriminals' strategy now to pose as helpful contributors on Stack Overflow is an interesting new approach as it allows them to exploit the site's trust and authority within the coding community.  This approach serves as a reminder of the constantly changing tactics of cybercriminals and, unfortunately, illustrates why you can never blindly trust what someone shares online.  Instead, developers must verify the source of all packages they add to their projects and, even if it feels trustworthy, check the code" - and they said "with word wrap enabled" - "for unusual or obfuscated commands which will be executed."



I have a picture in the show notes of the window.  And you can where there is a Python class named "install command," and then a definition of run which is going to print something, and then you can see a big bunch of white space.  Well, that's all spaces that will push this huge green blob of Base64 encoded code far off to the right so that, if someone did not have word wrap enabled, they'd never see this.  They would look at it and go, huh.  Well, okay.  I don't quite get what it's doing, but looks fine.  Nothing bad there.  When in fact there's a big blob of badness which the exec function will deobfuscate and then run.



So anyway, I'll just note that before the end of today's podcast, the security researcher Kevin Beaumont is going to show us, despite Microsoft's claims to the contrary, that the database underlying Microsoft's new Recall system can, in fact, be exfiltrated remotely, does not require system privilege, and can be accessed by any other user on the same machine.  That means that Recall's SQLite database is 100% vulnerable to exactly this sort of info-stealing malware.  So it's not like Microsoft has created some miracle that is going to protect this database.  And we'll be talking about more of that in a minute.



So in other news, we have another certificate authority in the doghouse.  Google has announced that it will be removing its trust of all new TLS certificates issued by the Austrian certificate authority GlobalTrust.  Rather than yanking GlobalTrust's root certificate, which would invalidate all previously-issued GlobalTrust certs, Google will be using a recently added new feature that allows it to manage certificate trust based on certificate issue dates.  So Chrome will not be trusting any new certificates issued by GlobalTrust after the end of this month, June 30th.



Now, through the nearly 20 years of this podcast we've seen and discussed a range of misbehavior on the part of those who have been given the privilege of essentially printing money.  Certificate authorities charge their customers hundreds of dollars in return for encrypting a hash of a small block of bits that the customer presents.  But in return for this money-printing privilege, the CA must abide by a significant code of conduct.  When that code is broken, and only after bending over backwards with more than ample warnings, the industry can and has summarily withdrawn its trust from the signatures of those CAs on the grounds that, if the CA cannot be trusted, neither can anything they have signed.



In this case GlobalTrust has established a multi - well, "established" is an interesting choice of my words - a multi-year history of misconduct, and they've lost the trust of the industry.  Google will be enforcing a ban retroactively on all Chrome versions down to 124.  So lots of previous Chrome versions.  I don't know who would not be keeping their version of Chrome up to date, but okay.  And the other browser makers have not yet announced a similar decision, although Mozilla appears to be aware of the problems with GlobalTrust and is concerned.



On the other hand, since no customer would purchase a certificate for a web server which anyone visiting with Chrome would be unable to connect to securely, this immediately puts GlobalTrust out of the business of selling web server certificates.  In other words, whether or not Apple and Mozilla should choose to follow, GlobalTrust is done for now, at least on the TLS web server certificate business.  They may be selling lots of certificates for other purposes, but not for any Chrome browsers in the future.



Those of us who have been around since the dawn of the Internet will likely remember the first successful instant messaging app known as ICQ.  It was meant to be short for "I seek you."  The system was originally developed back in 1996 by an Israeli company named Mirabilis.  I practiced pronouncing it earlier, and now I can't do it.  Mirabilis.



LEO:  No, Mirabilis.



STEVE:  Mirabilis.  I thought - okay, right.  Mirabilis.



LEO:  Mirabilis.



STEVE:  Two years after it was created - ICQ was created by AOL in 1998, and then by the Russian Mail.ru Group in 2010.  It had a neat kind of funky flower petal logo, and I've sort of thought of it like through the years, wondering whatever became of it.  At its peak around 2001, it had more than 100 million accounts registered.



LEO:  Wow.



STEVE:  And nine years later, when AOL sold it to Mail.ru, it had around 42 million daily users.  And it has been puttering along in the background ever since.  Two years ago it had dropped to around 11 million monthly users.



And finally, the reason the subject came up is that a week and a half ago, on May 24th, the website of ICQ.com announced that the service would be shut down about three weeks from now, on June 26th, 2024.  So it had a pretty good 28-year run for an instant messaging service that was largely passed by when smartphones and other major social media service got into the game.  But it was there from the beginning and kind of cool.



Okay.  Now, completely off topic, but this has been something that I've been wanting to just make sure everybody knew about for a while.  My wife recently agreed to join me in watching one of my favorite science fiction movies of all time.  We know I'm a pushover for science fiction.  But unfortunately, far more horrible science fiction movies have been made than good ones, and even more rare is the perfect science fiction movie.  So we settled down to watch "Dj Vu" which stars...



LEO:  I feel like I've seen it before.  I don't...



STEVE:  You probably have, Leo.  It's not new.  And yes, I get your...



LEO:  Okay, just checking.  Actually, I don't feel like I've ever seen it.



STEVE:  Oh, no kidding?



LEO:  I don't usually think of Val Kilmer and Denzel Washington as being sci-fi stalwarts.



STEVE:  Oh, Leo.



LEO:  Oh, all right.



STEVE:  Okay.  So listen to - okay.  So Denzel Washington, Val Kilmer, and some other recognizable actors from Hollywood's inventory.  As I was watching it for maybe the fourth time, I kept thinking over and over, you know, it is - as I was watching this perfectly and often leisurely paced two-hour movie unfold scene by scene, and everything was happening exactly the way it should, that I was sitting here watching one of the all too rare perfect movies.  This movie offers convincing acting that's not distracting, a brand new and perfect concept, a perfect script, and a plot that's both surprising and where what happens is better than someone steeped in science could have ever hoped for.  The writers enlisted the help of Brian Greene, a Cornell and Columbia University physicist, to get the science right.  And boy, did they.  You know, that's part of what's so gratifying about this movie.  Now, as I said, it's not a new movie.  It was released 18 years ago, back in 2006.  But it stands up, and it feels 100% contemporary.



I realized that since this podcast is closing in on its 20th birthday, every time I've seen this movie I've done this podcast a few days later, yet somehow I've never thought to mention it.  I searched our transcripts, and there was no mention of it.  So, you know, that's my bad, and that's fixed now.  I know quite well that not everyone's taste is the same.  Not everyone will feel as I do about this.  But if you don't already know this movie - and Leo, I guess you don't...



LEO:  Lisa said she's seen it.  So it'll be dj vu for her, but it will be whatever it is, premiere view for me.



STEVE:  It is just so good.  I just...



LEO:  I'm watching it tonight.  I need something to watch.



STEVE:  It is wonderful sci-fi.



LEO:  I love Denzel, of course.



STEVE:  And, yes, I do, too.  And it will not disappoint you.



LEO:  Okay.  Thank you.  Finally, something to watch tonight.



STEVE:  So our listener Jeff Price, he wrote and said:  "Leo touched on this, but Fastmail allows you to create these unique random email addresses.  What most people forget is Apple lets you create these, as well.  They call it Hide My Email."  So I just wanted to share Jeff's note since I have the feeling email aliasing services are going to become increasingly popular as websites turn to collecting and sharing whatever they can about their visitors as a means of increasing their advertising revenue, you know, as third-party cookies and as Google tries to promote their sandbox anti-tracking technologies.



Kirk Sexton wrote:  "Hi, Steve.  Great work on the new email system.  I never miss a show.  I listen on my morning runs and in the car on my way to work.  Sometimes I have to run a little further or sit in my car for a few minutes longer after arriving so I don't interrupt a point before hitting pause.



"I may have missed this point, but I don't recall hearing anything about those users who sync their accounts on Microsoft OneDrive, or for that matter use other cloud-based backup services."  And he's talking about Recall.  He says:  "Backing up files is one thing.  It would be expected that anything committed to local storage will be backed up to the subscribed cloud storage.  However, temporary information that is used just for the moment will now be stored locally - think passwords, credit cards, or other sensitive information - within the screen grabs.  Microsoft has said it will only be stored locally, but what about cloud-syncing with OneDrive or other services?  I see it as the problem just mushrooming into multiple attack vectors.  Am I missing something?"  And he finished:  "To 999 and beyond!  All the best, Kirk Sexton."



So Kirk raised a great point, I think.  We're about to spend the rest of the podcast looking at what one security researcher found and also about what may be Microsoft's significantly greater plan beyond what they've announced.  But everything we know now suggests that the Recall data are just SQLite files stored under the user's AppData directory in a new folder called "CoreAIPlatform."  Microsoft has indicated that BitLocker will be used to encrypt the data at rest.  But online backups are made of live unencrypted data so that they can later be retrieved.  And there's nothing we know so far that would prevent anything that was backing up a user's machine from also backing up their machine's Recall history.  So, you know, there just seems to be so many things that have not been well thought through here.



Okay.  And then just one piece of feedback.  I'm way far behind, just so everybody knows.  The first week of listener feedback email was intense, with many listeners, you know, wanting to say hi, to express their happiness there's now a way to send me thoughts without engaging in social media.  So, yeah, as I said, I'm way behind.  But I figured I'd share one piece of feedback that's primarily about a SpinRite owner's experience, first with SpinRite 6.0, or by comparison with SpinRite 6, and then with 6.1.



Our listener Mark Jones sent email with the subject "Wow!  SpinRite 6.1 is amazing."  He wrote:  "Dear Steve.  Long-time listener, occasional source of feedback."  He says:  "(I was @mjphd on Twitter.)  I'm so happy to be using email. I only kept my X account for Security Now! feedback."  He said:  "I've listened to you discuss both the speed of 6.1 and the magic it does on an SSD.  Ever the experimentalist, I thought I would put it through its paces.  I have two drives, a 1TB spinner and a 250GB SSD that seemed to have slowed.  The results are nothing short of remarkable on both drives.  In only four hours, the 1TB was rejuvenated.  That would have taken days using SpinRite 6.  The boot into Windows 10 is now seconds instead of minutes, and the random slowdowns that were plaguing the system are gone.



"The real miracle was on the SSD.  The new drive test showed I was at 19 MB/s at the front and middle, and 80 MB/s at the end."  So 19 front and middle, 80 MB at the end.  "The whole drive is now over 546 MB/s after a level 3 scan.  Saying computer performance has returned feels inadequate.  It's mind-blowingly fast compared to yesterday.  Truly amazing.  Thanks for the great work, and I'm happy there will be a future past 999.  Regards, Mark Jones."



Okay.  So let's talk about Recall again because we have additional information.  And Leo, I'll find a point to pause here.



LEO:  Sure.



STEVE:  For our final.



LEO:  Yeah.



STEVE:  Okay.  So I think that a data-driven theory about Microsoft's future plans for this technology emerged after I read a recent posting by a well-known and well-informed security researcher named Kevin Beaumont.  Since last week's episode, which I titled, as we know, "The 50 Gigabyte Privacy Bomb," Kevin, whom we often quote and refer to, has again weighed in on Microsoft's new Recall facility.  His first posting on the subject, which he made on May 21st, immediately following Microsoft's announcement, was titled "How the new Microsoft Recall feature fundamentally undermines Windows security."



As a mature, seasoned, and experienced security researcher, his immediate "What could possibly go wrong?" reaction to the idea of having Windows continually recording and storing our PCs' screens echoes my own.  It's immediately obvious to anyone who's been around the block a few times that this is, indeed, a 50GB privacy bomb.  What wasn't clear to me until just yesterday was why Microsoft may be doing this, and what they probably have planned for the future.  We'll get to that.



Ever since his immediate posting in reaction to the announcement of Recall, Kevin has been playing with it.  After reading what Kevin wrote, a light bulb went off for me.  So I'm first going to share Kevin's follow-up piece which further describes Recall in much more detail.  Then I'll share what I think it really means.  Kevin titled his follow-up piece, which he posted four days ago, after spending a week and a half with Recall:  "Stealing everything you've ever typed or viewed on your own Windows PC is now possible with two lines of code inside the Copilot+ Recall disaster."



Okay.  Now, before switching into Q&A mode, which he does later, Kevin began his newly informed discussions of Recall by writing this.  He said:  "I wrote a piece recently about Copilot+ Recall, a new Microsoft Windows 11 feature which  in the words of Microsoft CEO Satya Nadella  takes 'screenshots' of your PC constantly, and makes it into an instantly searchable database of everything you've ever seen.  As he says, it is a photographic memory of your PC life.  I got hold of the Copilot+ software and got it working on a system without an NPU about a week ago, and I've been exploring how this thing works in practice.  So we'll have a look into that shortly.  First, I want to look at how this feature was received as I think it is important to understand the context.



"The overwhelmingly negative reaction has probably taken Microsoft leadership by surprise.  For almost everybody else, it wouldn't have.  This was like watching Microsoft become an Apple Mac marketing department.  At a surface level, it is great if you're a manager at a company with much to do and too little time as you can instantly search what you were doing about a subject a month ago.  In practice, that audience's needs are a very small - tiny, in fact - portion of Windows overall user base.  And frankly, talking about screenshotting the things people in the real world, not executive world, are doing is basically like punching customers in the face.  The echo chamber effect inside Microsoft is real here, and oh, boy.  Just oh, boy.  It's a rare misfire, I think," Kevin wrote.



He said:  "I think Recall is an interesting, entirely optional feature with a niche initial user base that would require incredibly careful communication, cybersecurity, engineering, and implementation.  Copilot+ Recall does not have any of these.  The work has clearly not been done to properly package it together.  A lot of Windows users just want their PCs so they can play games, watch porn, and live their lives as human beings who make mistakes that they don't always want to remember.  And the idea other people with access to the device could see a photographic memory is very scary to a great many people on a deeply personal level.  Windows is a personal experience.  This shatters that belief."



Okay, now, I thought Kevin's take on this was interesting.  His observation that Microsoft appears to be oblivious to the fact that not all users of PCs are even close to being the same.  That a manager in a corporate environment might indeed find it useful to be able to look a month back for some specific work subject, but that for the common user - the rest of us - the idea that our machines are watching and recording everything we do, even if it would only be for our own later access, is mostly just creepy.  You know, we don't know the future.  We don't know what's going to happen a month or two from now.  But Recall would make what's happening on our machines now available to that unknown future.



Anyway, Kevin finishes his lead-in by writing:  "I think they're probably going to set fire to the entire Copilot brand due to how poorly this has been implemented and rolled out.  It's an act of self-harm at Microsoft in the name of AI; and, by proxy, real customer harm.  More importantly, as I pointed out at the time, this fundamentally breaks the promise of security in Windows.  I'd like to now detail why."  He said:  "Strap in.  This is crazy.  I'm going to structure this as a Q&A with myself now, sourced from comments I've seen online, as it's really interesting seeing how some people hand-wave the issues away."



Okay.  So now Kevin switches into Q&A format.  He asks himself a question.  So the question is someone saying, "Well, the data is processed entirely locally on your laptop; right?"  Answer:  "Yes.  They made some smart decisions here.  There's a whole subsystem of Azure AI, et cetera, code that processes on the device."  Okay, question:  "Cool, so hackers and malware can't access it; right?"  And he says, "No, they can."  



Q:  "But it's encrypted."  A:  "When you're logged into a PC and run software, things are decrypted for you.  Encryption at rest only helps if somebody comes to your house and physically steals your laptop.  That's not what criminal hackers do.  For example, info-stealer trojans, which automatically steal usernames and passwords, have been a major problem for well over a decade.  Now these can be easily modified to support Recall."



Q:  "But the BBC said data cannot be accessed remotely by hackers."  A:  "They were quoting Microsoft, but this is wrong.  Data can be accessed remotely.  This is what the journalist was told for some reason."  And then he has a snippet from the journalist that says:  "That is what Microsoft told me, that attackers would have to get physical access to your laptop and sign into it to get hold of the screenshots."  Kevin says:  "Not true."  The questioner says:  "Microsoft say only that user can access the data."  Kevin:  "That is not true.  I can demonstrate another user account on the same device accessing the database."



Okay.  The question:  "So how does this work?"  Kevin answers:  "Every few seconds, screenshots are taken.  These are automatically OCR'd by Azure AI, running on your device, and written into an SQLite database in the user's folder.  This database file has a record of everything you've ever viewed on your PC in plaintext.  OCR is a process of looking an image and extracting the letters."  Question:  "What does the database look like?"  And Kevin shows some screenshots like those that we saw last week.  Just looking like, you know, a SQLite database with rows and columns, recognizable filenames.



Question:  "How do you obtain the database files?"  Answer:  "They're just files in AppData, in the new CoreAIPlatform folder."  Q:  "But it's highly encrypted, and nobody can access them; right?"  A:  "Here's a few seconds of video of two Microsoft engineers accessing the folder."  And then Kevin quotes an earlier Mastodon post of his at cyberplace.social where he notes that the Risky Business episode on Recall is good, but with one small correction:  Recall does not need system rights.  He notes that since it's just a SQLite database, it is trivial to access.  And he finishes by saying:  "I'm not being hyperbolic when I say this is the dumbest cybersecurity move in a decade.  Good luck to my parents safely using their PC."



Question:  "But normal users don't run as admins."  Answer:  "According to Microsoft's own website, in their Recall rollout page, they do."  And then he has a snippet from Microsoft.com where it says:  "Making admin users more secure:  Most people," says Microsoft, "run as full admins on their devices, which means..."



So Kevin says:  "In fact, you don't even need to be an admin to read the database.  More on that in a later blog."  Question:  "But a UAC prompt appeared in that video, that's a security boundary."  Kevin replies:  "According to Microsoft's own website and MSRC, UAC is not a security boundary."  And he quotes Microsoft saying:  "More important, Same-desktop Elevation in UAC is not a security boundary.  It can be hijacked by unprivileged software that runs on the same desktop.  Same-desktop Elevation should be considered a convenience feature."  So now Microsoft is saying, oh, well, you know, that's just for convenience.



So the questioner asks:  "So where's the security here?"  Answer:  "They've tried to do a bunch of things, but none of it actually works properly in the real world due to gaps you can fly a plane through."  Question:  "Does it automatically not screenshot and OCR things like financial information?"  A:  "No."  We know that it does.  Q:  "How large is the database?"  Kevin says - and here was one of the first ahas that hit me.  Kevin says:  "It compresses well.  Several days working is around 90KB," nine zero kilobytes for several days of work.  He says:  "You can exfiltrate several months of documents and key presses in the space of a few seconds with an average broadband connection."



Question:  "How fast is search?"  He says:  "On device is really fast."  Question:  "Have you exfiltrated your own Recall database?"  A:  "Yes. I have automated exfiltration, and made a website where you can upload a database and instantly search it.  I am deliberately holding back technical details until Microsoft ship the feature as I want to give them time to do something."  He said:  "I actually have a whole bunch of things to show, and I think the wider cyber community will have so much fun with this once it's generally available.  But I also think that's really sad, as real-world harm will ensue."



So question is "What kind of things are in the database?"  A:  "Everything a user has ever seen, organized by application.  Every bit of text the user has seen, with some minor exceptions," he says, for example, "Microsoft Edge InPrivate mode is excluded, but Google Chrome isn't."  He said:  "Every user interaction, for example minimizing a window.  There is an API for user activity, and third-party apps can plug in to enrich data and also view stored data."  Well, that's news, and interesting.  He says:  "It also stores all websites you visit, even if third party."



Question:  "If I delete an email/WhatsApp/Signal/Teams message, is it deleted from Recall?"  A:  "No, it stays in the database indefinitely."  Question:  "Are auto-deleting messages in messaging apps removed from Recall?"  A:  "No, they are scraped by Recall and available."  Q:  "But if a hacker gains access to run code on your PC, it's already game over."  Kevin says:  "If you run something like an info-stealer, at present they will automatically scrape things like credential stores.  At scale, hackers scrape rather than touch every victim, because there are so many, and resell them in online marketplaces.  Recall enables threat actors to automate scraping everything you've ever looked at within seconds.



"While testing this with an off-the-shelf info-stealer," he said, "I used Microsoft Defender for Endpoint, which detected the off-the-shelf info-stealer.  But by the time the automated remediation kicked in, which took over 10 minutes," he notes, "my Recall data was already long gone."



Question:  "Does this enable mass data breaches of website?"  A:  "Yes.  The next time you see a major data breach where customer data is clearly visible in the breach, you're going to presume the company who processes the data is at fault; right?  But if people have used a Windows device with Recall to access the service/app/whatever, hackers can see everything" - he means that the people offering the service have seen, he said - "and assemble data dumps without the company who runs the service even being aware.  The data is already consistently structured in the Recall database for attackers.  So prepare for AI-powered super breaches.  Currently credential marketplaces exist where you can buy stolen passwords.  Soon you'll be able to buy stolen customer data from insurance companies, et cetera, because all code required to do this has been pre-installed and enabled on Windows by Microsoft."



Q:  "So did Microsoft mislead the BBC about the security of Copilot?"  A:  "Yes."  Q:  "Have Microsoft misled customers about the security of Copilot?"  A:  "Yes.  For example," he says, "they describe it as an optional experience, but it is enabled by default, and people can optionally disable it.  That's," Kevin says, "wordsmithing.  Microsoft's CEO referred to 'screenshots' in an interview about the product, but the product itself only refers to 'snapshots.'  A snapshot is actually a screenshot.  It's, again, wordsmithing for whatever reason.  Microsoft just need to be super clear about what this is so customers can make an informed choice."



And of course I need to note here that the tyranny of the default will be at work.  We know that whatever is the default setting is what 99.99% of all Windows users will leave active.  I don't know if any of you have seen people using Windows computers, but for some reason they always leave those stickers all over the keyboard.  And I just, I can't believe it.  It's like you realize the computer will still work if you peel those stickers off the keyboard.  You don't need to be, you know, advertising the crap that came from the manufacturer.  But anyway, the tyranny of the default.



So question:  "Recall only applies to one hardware device."  Kevin replies:  "That's not true.  There are currently 10 Copilot+ devices available to order right now from every major manufacturer.  Additionally, Microsoft's website says they're working on support for AMD and Intel chipsets.  Recall is coming to Windows 11."  Q:  "How do I disable Recall?"  A:  "In initial device setup for compatible Copilot+ devices out of the box, you have to click through options to disable Recall.  In enterprise, you have to turn off Recall as it is enabled by default."



Q:  "What are the privacy implications?  Isn't this against GDPR?"  Kevin replies:  "I'm not a privacy person or a legal person.  I will say that privacy people I have talked to are extremely worried about the impacts on households in domestic abuse situations and such.  Obviously, from a corporate point of view, organizations should absolutely consider the risk of processing customer data like this.  Microsoft won't be held responsible as the data processor, as it is done at the edge on your devices.  You are responsible here."



The question:  "Are Microsoft a big, evil company?"  Kevin:  "No."



LEO:  Hell, yes.  Oh.



STEVE:  "That's insanely reductive."  He says:  "They're super smart people, and sometimes super smart people make mistakes.  What matters is what they do with knowledge of mistakes."  So the question:  "Aren't you the former employee who hates Microsoft?"  Kevin says:  "No.  I just wrote a blog this month praising them.  It was 'Breaking down Microsoft's pivot to placing cybersecurity as a top priority.'  My thoughts on Microsoft's 'last chance saloon' moment on security."



So we have a couple, just two more.  Question:  "Is this really as harmful as you think?"  Answer:  "Go to your parents' house, your grandparents' house, et cetera, and look at their Windows PC.  Look at the installed software in the past year.  Try to use their device.  Run some AV scans.  There's no way this implementation does not end in tears.  There's a reason there's a trillion dollar security industry, and that most problems revolve around malware and endpoints."  Q:  "What should Microsoft do?"  Answer:  "In my opinion, they should recall Recall and rework it to be the feature it deserves to be, delivered at a later date.  They also need to review the internal decision-making that led to this situation."



He says:  "This kind of thing should not happen.  Earlier this month, Microsoft's CEO emailed all their staff, saying:  'If you're faced with the tradeoff between security and another priority, your answer is clear:  Do security.'"  He said:  "We will find out if he was serious about that email.  They need to eat some humble pie and just take the hit now, or risk customer trust in their Copilot and security brands.  Frankly, few if any customers are going to cry about Recall not being immediately available, but they are absolutely going to be seriously concerned if Microsoft's reaction is to do nothing, ship the product, slightly tinker, or try to wordsmith around the problem in the media."  Okay.



LEO:  Seems like a great piece.  I mean, I've read it, and I was very impressed.



STEVE:  Yup.



LEO:  And he makes a strong case.  The one thing that's a question mark, a lot of the things he describes sounds like you had to be on the physical PC.  But he says you don't.  So malware would be able to escalate the UAC and do all those things, look across accounts.



STEVE:  Yup.



LEO:  All of that stuff.  Okay.  So the real issue is, if malware gets in your system, they've got access to everything you've done.



STEVE:  Right.  There is now much more that it has access to.  Let's take our final break, and then I'm going to talk about what I think is really going on.



LEO:  Yeah.  Why would Microsoft do all this?



STEVE:  Yup.



LEO:  What's the plan here?  Hmm.



STEVE:  I think there is one.



LEO:  All right.  Steve.  You've set us up well.  Obviously this is a bad idea.



STEVE:  Why would they do it?



LEO:  But Microsoft's going full speed ahead with it.  Why?



STEVE:  Okay.  So we now know that Microsoft currently plans to enable this whole PC history recording by default.  They also know that unless Windows ships with it enabled and running, no one will use it.  So they want to blow everyone's mind by AI-enabling Windows PCs somehow, and this is what they've come up with.  I doubt there's an informed security-minded technologist anywhere who doesn't think this is a very bad idea.  Yet until we learn otherwise, this is exactly what Microsoft intends to do.  Now, I have to say I have some personal experience with endeavoring, and failing, to get Microsoft to change its plans.



LEO:  Can anybody say "raw sockets"?



STEVE:  Uh-huh.  Before their release of Windows XP, which grew out of Windows 2000, I tried to keep Microsoft from shipping XP with the totally unnecessary access to raw sockets available to the operating system's client software.  They ignored me until the MSBlast worm would have taken them off the Internet had it  not been targeted at the wrong domain.  After that near-death brush with being attacked by an entirely unnecessary feature of their own operating system, XP's Service Pack 3 removed unprivileged access to raw sockets, and no one cared.  The fact that no one cared demonstrated that the unnecessary feature should have never been present in a consumer OS.  Raw sockets never came back because they just beg to be abused.



Okay, now, I learned my lesson from that experience.  I have no interest in lobbying Microsoft to change its behavior.  You know, Microsoft is like Godzilla.  It does whatever it wants to do.  All anyone can do is stay out of its way.  But what's so odd about this moment where we find ourselves is that they have just made all this noise about how security is now job number one.  And Kevin quoted Satya Nadella saying:  "If you're faced with the tradeoff between security and another priority, your answer is clear:  Do security."  Except they're not.  The entire security industry is jumping up and down, waving their arms, and saying "Don't do it," exactly as I once did before with XP.  Yet Microsoft is certain that they know better.



Now, it's interesting that Kevin believes that the screen is being OCR'd.  I strongly doubt that's actually the case, at least not unless an actual JPEG or PNG-style graphic image is being displayed, in which case OCRing the image would be the only choice.  As I noted last week, hooking into the Windows API that paints text onto the screen would be far more efficient.  Behind every character glyph, what we see on the screen is a 16-bit Unicode character which was rendered through a chosen font and turned into clear-type colorized pixel text.  There's just no reason to look at the pixels of a screen that was just rendered from Unicode and try to determine which characters they are.  So my assumption would be that the textual output graphic API is being hooked and intercepted by Recall.



It was also very interesting to learn how economical Recall's storage is.  This makes sense if it's storing and compressing text, since we know how much redundancy exists in linguistic text.  But Kevin said that several days' worth of work compresses to around 90KB of database storage.  If we take Kevin's "several days" to mean two, then that's around 45K of storage required per day.  That means that 50GB of storage allocation, consumed at the rate of 45K per day, would yield 3,042 years' worth of storage.  I'm sure we'll learn more going forward, but I don't think Recall will be storing the past 90 days of a PC's use.  It appears that it will always be recording the PC's entire life of use.



That's why the title of Kevin's second post makes far more sense.  His title began "Stealing everything you've ever typed or viewed on your own Windows PC."  And I think that's exactly what Microsoft is actually planning to do.  If they're able to capture and compress all the text displayed on Windows 11 screens, and given the explosion in local mass storage capacity and the efficiency of text compression, they clearly have the storage capacity to capture everything for all time.



And this brings us to the title I gave today's podcast:  "A Large Language Model in Every Pot."  Why would Microsoft want to be capturing every single thing a user types and views on their own PC throughout its entire lifetime of use?  I have a theory.  Microsoft wants to make a big splash in AI.  So how about using all of that data to train an entirely personal local large language model?  What if a future local large language model was not just used to index and search your PC's history timeline, but was continually being trained across your entire corpus of personal data so that it would be possible to conversationally interact with your own personal AI that has grown to know you intimately because it has been watching and learning everything you've been doing for years?  It would "know," and I have "know" in air quotes, everything you had ever entered into its keyboard and displayed on its screen.  The entire history of that machine's use would become an ever-growing corpus that is continually training the model.



That would completely and profoundly forever alter a user's interactive experience with their PC.  It would be a true game changer.  It would be transformative of the PC experience.  And if Microsoft has that up its sleeve, I can see how and why they would be super excited about Recall, even though Recall would be just the beginning.  Even if the local large language model technology is not yet ready for delivery, the time to begin capturing all of a user's use of their machine is as soon as possible.  That begins creating the corpus that will be used to train a future personal local large language model.



If this view of the future is correct, there's one large and glaring problem with this, which Kevin highlights and which Microsoft is conveniently ignoring because they have no choice but to ignore it.  What Microsoft must ignore is that the actual security of today's Windows is a catastrophe.  Microsoft has not been paying more than begrudging and passing attention to security while they've been busily adding trivial new feature after new feature and never getting ahead of the game.



Last month's Patch Tuesday saw Microsoft patching 61 newly recognized vulnerabilities, 47 of them in Windows, and another 25 for anyone paying for extended security updates.  44% of those were remote code execution, 11% were information disclosure, and 28% were elevation of privilege - none of which suggests that Windows would be a safe place to store the data that will be used to drive an entity that can be queried about nearly any aspect of you and your life which it has observed throughout the entire history of your use of that machine.



If this is indeed what Microsoft is planning, and having voiced it now it's difficult to imagine that it's not exactly what they're planning, then this really is a double-edged sword.  The world stumbled upon the startling power of large language models, which Microsoft just so happens to own a big chunk of, and someone inside Microsoft realized that by leveraging the power of next-generation neural processing units, it would be possible to train a local model on the user's entire usage history of their computer.  And that would create a personal assistant of unprecedented scope and power.



I would wager that, today, the smarter people within Microsoft are wishing, more than anything else, that instead of screwing around with endless unnecessary features and new unwanted versions of Windows, they had been taking the security of their existing system seriously.  Because if they had, they would own a secure foundation and would stand a far greater chance of successfully protecting the crown jewels of a user's computer usage legacy.  Instead, what they have today is a Swiss cheese operating system that is secure only so long as no one really cares what its user has stored.  Depending upon who the user is, the data that will be accumulated by Recall will represent a treasure that is certain to dramatically increase the pressure to penetrate Windows.  The entire professional security community understands this, which is why it's going batshit over Recall, while Microsoft has no choice other than to deny the problem because they're desperate to begin the data aggregation of their users so that it can be used to train tomorrow's personal PC assistant AIs.



So Microsoft will declare, as they always do, that Windows is more secure than it's ever been, even though history always shows us afterward that's never been true.  Microsoft is going to have Recall installed, running, and collecting its users' data in all forthcoming qualifying Copilot+ Windows 11 PCs.  And don't get me wrong.  The idea of being able to ask a built-in autonomous personal AI assistant about absolutely anything we've ever typed into or seen on our computer is intoxicatingly powerful.  For many of us who live much of our lives through our computers, it would be like having a neural-link extension of our brain with flawless perfect recall.  But it also represents a security and privacy threat the likes of which has never existed before.



When you consider the amount of digital storage that anyone can now easily own, it seems pretty obvious that this is going to happen sooner or later.  Unfortunately, Microsoft has not proven itself to be a trustworthy caretaker of such information.



LEO:  Wow.  I think you're exactly right.  I mean, that's almost what they're proposing anyway is you can always query the machine about everything you've done.



STEVE:  Well, they're saying "timeline," that you can query a timeline.  But if this thing, if they're capturing text from the screen, Leo, and Kevin saw 90KB was stored after several days of use, that means that that 50GB that they want to set aside, this is not a 90-day rolling window which I thought last week.  They're going to store everything you ever do for your entire life of your use of that machine.



LEO:  Right, right.



STEVE:  And in fact you're going to want that to be portable to the next machine you move to so that you're able to take that accrued data with you from one, you know, three years from now when you need to buy a new Windows 13 machine.



LEO:  It could be secured; right?  You could do this right; couldn't you?



STEVE:  Yes.  And what they're doing - I think you could.  I think you could, I mean, you would need new hardware because you need some sort of the equivalent of an HSM.  Basically you'd want this super Jeeves to be in its own enclave that could not be exfiltrated from.



LEO:  Yes, that's right.



STEVE:  Where data goes in, and nothing comes out.  And then, I mean, but it would - imagine that, Leo.  It would be compelling to be able to ask your computer anything that you ever did with it.



LEO:  I'm well aware of that.  That's the...



STEVE:  It's perfect recall.



LEO:  Yeah, the endgame for all of this.  I've even referred back...



STEVE:  And you've been talking about your own local smaller corpuses or corpi and how useful that is. 



LEO:  Right.  And I've talked about...



STEVE:  This would be that.



LEO:  ...the founder of DEC, not the founder, one of the designers of DEC just passed away recently.



STEVE:  Gordon Bell.



LEO:  Gordon Bell, who had the same idea.  He had a camera around his neck.  He wanted to record everything he ever did.  This is ever before we had these powerful LLMs.



STEVE:  And the storage capacity to record our life.



LEO:  Right, right.  Well, the issue always was, and with Gordon's database, is well, okay, I've got it.  What do I do with it?



STEVE:  Right.



LEO:  I can't in a reasonable way parse it.  Well, now we can.



STEVE:  Yes, yes.



LEO:  And so I'm very interested.  I ordered the Limitless Pin which records all our conversations, the idea, same thing, being to allow you to query that.  You know, what did I say to Steve?  I think this is the single most useful persuasive use of AI is as an assistant that knows everything about you.  But, boy, that poses some big problems.  It's almost as if we need an initiative to create a way.  It also solves other problems because data privacy's a huge issue.  We need a way, something that you can - Stacey Higginbotham used to call it The Blob, a place where you could securely, in Secure Enclave, store all your data for your own personal use, not so that other people could invade your privacy, but for your own personal use.  And this is the best possible use.



So I think we're on the right track.  I think this Microsoft implementation could kill it in its tracks.  It could actually have a - this is what worries me is people are moving so fast, with so little regard for safety, that they could have the opposite effect.  They could get people so scared about their security and privacy that they give up entirely on AI.



STEVE:  Well, and they're frankly lying...



LEO:  Yeah.



STEVE:  ...about the security.



LEO:  They're misrepresenting it, yes.



STEVE:  Yes.  I mean, all this is is some files under the user's app directory.



LEO:  Right.



STEVE:  This is not some hocus pocus.  And so everybody knows how to exfiltrate files.  Kevin did it.  There's now a GitHub project that is able to display all your Recall data.



LEO:  Well, I'm glad that he published this paper.  I'm glad you did this show because up to now the press, not knowing any better, and I include myself, we've parroted Microsoft's assertions that, well, it's all on device.  It's all local.  It's all safe.  It's encrypted.  It's only available to you.  I always - I have pointed out in the past that it's only encrypted as long as you don't log in.  This is the second part of that.  Once you're logged in, it's decrypted, and then available to any malware on your system.  Yeah, I think people will - I hope the press will start to come around and say, hey, wait a minute, this isn't as secure as you said it was.



STEVE:  Well, our listeners are preemptively protected; right?  I mean, they're going to turn this off.



LEO:  Like that.



STEVE:  But unfortunately, there's no reach.  Well, there's minimal reach.  But, you know, there's a bazillion Windows 10 or Windows 11 users, and they're going to think, hey, this is cool.  I can scroll back in history.  And this is Microsoft getting ready for something that comes next.



LEO:  Yeah, I agree.  You know, Apple has a solution called Timeline.  It's a backup solution that keeps everything you do in a timeline database, a vault.  Hard links to every version of every document.  So they're kind of doing something similar.  Nobody's ever questioned the usefulness or the security of it.  I don't know how different it is.  But, yeah, this is a problem.  This really is a problem.



Steve's done it again, hasn't he, kids.  This is why we wait for Tuesday with bated breath.  Steve is the man in charge of GRC.com, the Gibson Research Corporation dot com.  And it is the place you can email him.  Now, what should they do again?  They email...



STEVE:  So first you need to register.  Otherwise your email will not get through.  So just go to GRC.com/mail.



LEO:  Okay, there you go.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




